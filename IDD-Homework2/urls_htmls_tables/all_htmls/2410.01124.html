<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Synthetic imagery for fuzzy object detection: A comparative study</title>
<!--Generated on Tue Oct  1 23:11:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.01124v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S1" title="In Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S2" title="In Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S3" title="In Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4" title="In Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Iterations of synthetic data generation method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1" title="In 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Method 1: Process establishment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1.SSS1" title="In 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Setting up virtual scenes using 3D models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1.SSS2" title="In 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Defining camera movement rules</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1.SSS3" title="In 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Generating and arranging flame/fire image planes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1.SSS4" title="In 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Calculating annotations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS2" title="In 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Method 2: Process improvements</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS2.SSS1" title="In 4.2 Method 2: Process improvements ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Arranging flame/fire image planes to improve annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS2.SSS2" title="In 4.2 Method 2: Process improvements ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Diversification of the scenes and fires</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5" title="In Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>ML experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS1" title="In 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS2" title="In 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison of synthetic data generation methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS3" title="In 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Comparison of various training combinations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S6" title="In Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S6.SS1" title="In 6 Conclusion ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Future research</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Synthetic imagery for fuzzy object detection: A comparative study</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Siavash H. Khajavi 
<br class="ltx_break"/>Department of Industrial Engineering and Management
<br class="ltx_break"/>Aalto University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">siavash.khajavi@aalto.fi
<br class="ltx_break"/></span>&amp;Mehdi Moshtaghi 
<br class="ltx_break"/>Department of Computer Science 
<br class="ltx_break"/>Aalto University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">mehdi.moshtaghi@aalto.fi</span>
<br class="ltx_break"/>&amp;Dikai Yu 
<br class="ltx_break"/>Department of Design 
<br class="ltx_break"/>Aalto University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">dikai.yu@aalto.fi</span>
<br class="ltx_break"/>&amp;Zixuan Liu 
<br class="ltx_break"/>Department of Computer Science 
<br class="ltx_break"/>Tulane University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">zliu41@tulane.edu</span>
<br class="ltx_break"/>&amp;Kary Främling 
<br class="ltx_break"/>Department of Industrial Engineering and Management 
<br class="ltx_break"/>Aalto University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">kary.framling@aalto.fi</span>
&amp;Jan Holmström 
<br class="ltx_break"/>Department of Industrial Engineering and Management
<br class="ltx_break"/>Aalto University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.6.id6">jan.holmstrom@aalto.fi</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">The fuzzy object detection is a challenging field of research in computer vision (CV). Distinguishing between fuzzy and non-fuzzy object detection in CV is important. Fuzzy objects such as fire, smoke, mist, and steam present significantly greater complexities in terms of visual features, blurred edges, varying shapes, opacity, and volume compared to non-fuzzy objects such as trees and cars. Collection of a balanced and diverse dataset and accurate annotation is crucial to achieve better ML models for fuzzy objects, however, the task of collection and annotation is still highly manual. In this research, we propose and leverage an alternative method of generating and automatically annotating fully synthetic fire images based on 3D models for training an object detection model. Moreover, the performance, and efficiency of the trained ML models on synthetic images is compared with ML models trained on real imagery and mixed imagery. Findings proved the effectiveness of the synthetic data for fire detection, while the performance improves as the test dataset covers a broader spectrum of real fires. Our findings illustrates that when synthetic imagery and real imagery is utilized in a mixed training set the resulting ML model outperforms models trained on real imagery as well as models trained on synthetic imagery for detection of a broad spectrum of fires. The proposed method for automating the annotation of synthetic fuzzy objects imagery carries substantial implications for reducing both time and cost in creating computer vision models specifically tailored for detecting fuzzy objects.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Fuzzy objects, such as fire, smoke, are common in natural physical life. Detecting and locating such objects in the images remains difficult in the computer vision field due to their blurred edges, variable shapes and varying intensity levels compared to well-defined objects.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The success of AlexNet <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib1" title="">2012</a>)</cite> on ImageNet <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib2" title="">2009</a>)</cite> led to the use of deep convolutional networks (CNNs) for fuzzy object detection. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib3" title="">2016</a>)</cite> proposed a fine-tuned pre-trained AlexNet model for forest fire detection. Muhammad et al. fine-tuned different variants of CNNs like AlexNet <cite class="ltx_cite ltx_citemacro_cite">Muhammad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib4" title="">2018a</a>)</cite>, SqueezeNet <cite class="ltx_cite ltx_citemacro_cite">Muhammad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib5" title="">2018b</a>)</cite>,GoogleNet <cite class="ltx_cite ltx_citemacro_cite">Muhammad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib6" title="">2018c</a>)</cite>, and MobileNetV2 <cite class="ltx_cite ltx_citemacro_cite">Muhammad et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib7" title="">2019</a>)</cite> for detection. However, the performance of their methods is not promising because the datasets they used is small and limited. They used Foggia’s dataset <cite class="ltx_cite ltx_citemacro_cite">Foggia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib8" title="">2015</a>)</cite> which only contains 14 fire and 17 non-fire videos, and the frames of the videos are highly similar, which greatly restricts the performance of the model. It is known that CNNs methods are only effective when training on large scale datasets. For example, the well-used ImageNet ILSRVC dataset <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib2" title="">2009</a>)</cite> contains 1 million images for training. Obtaining such vast amounts of training datasets with real-world data of fuzzy objects, especially for fire and smoke, is challenging because of their sporadic nature and the difficulty in capturing high-quality data under hazardous conditions. Moreover, publicly-available and high-quality datasets of fire and smoke are limited; for example, the D-Fire dataset <cite class="ltx_cite ltx_citemacro_cite">de Venancio et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib9" title="">2022</a>)</cite>, the largest published fire dataset, comprises 21,000 annotated fire and smoke images, which is not comparable with ImageNet. Another issue with D-Fire is that a significant portion of the dataset contains sequential frames of surveillance videos, which are highly similar images. After removing duplicate and similar images, based on their VGG <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib10" title="">2015</a>)</cite> embeddings, approximately half of the original dataset remains. This also makes D-Fire an inappropriate dataset as a benchmark to test on. As a result, current methods perform poorly on fuzzy object detection. Generating large, realistic, and diverse data of fire and smoke is vital for developing effective detection algorithms to detect fires.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In order to mitigate the requirement of large datasets, synthetic data has been proposed to solve this challenge and be used for training CNN models. Researchers at OpenAI <cite class="ltx_cite ltx_citemacro_cite">Tobin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib11" title="">2017</a>)</cite> use domain randomization, which is a technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. Their experiments show that, with enough variability in the simulator, the deep neural network trained only on simulated images can be successfully transferred to real-world for object localization. In addition, researchers at Google <cite class="ltx_cite ltx_citemacro_cite">Hinterstoisser et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib12" title="">2019</a>)</cite> have successfully demonstrated the efficacy of synthetic data for non-fuzzy object detection. They leverage a large dataset of 3D background models and densely render them using full domain randomization. This yields background images with realistic shapes and texture, on top of which they render the objects of interest. Moreover, Unity <cite class="ltx_cite ltx_citemacro_cite">Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib13" title="">2021</a>)</cite> published free tools called “unity computer vision” and “unity simulation” for generating large-scale datasets with labels, randomizers, samples, and rendering capabilities. However, all their works focus on rendering limited non-fuzzy objects with well-defined boundaries or characteristics, such as boxes, bottles, books. In this research, we intend to expand this to fuzzy objects and propose a method of 3D model-based synthetic imagery generation for this purpose. Our research questions are as follows:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Is the fully synthetic data generated by 3D model-based synthetic imagery generation method effective in enabling a computer vision ML model to detect fire?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">How to implement automated annotation for fuzzy objects in the 3D model-based synthetic imagery data?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">What is the comparative performance of the ML model trained on fully synthetic imagery, the ML model trained on real imagery, and the ML model trained on mixed imagery in the detection of fire?</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The computer vision field has allocated substantial resources towards the development of datasets like PASCAL VOC <cite class="ltx_cite ltx_citemacro_cite">Everingham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib14" title="">2010</a>)</cite>, MS COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib15" title="">2015</a>)</cite>, and ImageNet <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib2" title="">2009</a>)</cite>, which have significantly promoted the research forward in complex areas such as image classification <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib16" title="">2016</a>)</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib17" title="">2015</a>); Redmon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib18" title="">2016</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib19" title="">2017</a>)</cite>, and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib20" title="">2017</a>); Long et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib21" title="">2015</a>)</cite>. However, these datasets do not encompass all scenarios of interest to researchers, nor do they facilitate the creation of new datasets, especially some domain-specific datasets. Notice that acquiring and manually annotating such domain-specific datasets can be costly, labor-intensive, and prone to mistakes <cite class="ltx_cite ltx_citemacro_cite">Hinterstoisser et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib12" title="">2019</a>)</cite>. A widely adopted strategy to enhance the efficacy of detection algorithms involves augmenting real datasets with synthetic data <cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib22" title="">2015</a>); Georgakis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib23" title="">2017</a>)</cite>. To bridge the gap between synthetic and real image characteristics, various techniques have been investigated <cite class="ltx_cite ltx_citemacro_cite">Shrivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib24" title="">2017</a>); Hinterstoisser et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib12" title="">2019</a>); Inoue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib25" title="">2018</a>)</cite>. Shrivastava et al. <cite class="ltx_cite ltx_citemacro_cite">Shrivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib24" title="">2017</a>)</cite> propose a Simulated+Unsupervised (S+U) learning method, employing adopted Generative Adversarial Networks (GANs) to align the distribution of synthetic and real images more closely. Hinterstoisser et al. <cite class="ltx_cite ltx_citemacro_cite">Hinterstoisser et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib12" title="">2019</a>)</cite> deploy a technique where background scenes are rendered with complete domain randomization <cite class="ltx_cite ltx_citemacro_cite">Tobin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib11" title="">2017</a>)</cite>, incorporating realistic shapes and textures. On top of these backgrounds, the target objects of interest are then rendered, encompassing a comprehensive array of poses and conditions to ensure a thorough representation in training datasets. Furthermore, Inoue et al. <cite class="ltx_cite ltx_citemacro_cite">Inoue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib25" title="">2018</a>)</cite> employ transfer learning techniques to mitigate the domain disparity between synthetic and real domain. By leveraging a substantial dataset of synthetic images processed through variational auto-encoders, their approach consistently yields strong performance across diverse conditions.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Constructing simulation environments has been identified as another major challenge, as the settings employed in the aforementioned studies are often not publicly available, which makes it difficult to achieve widespread applicability. Efforts have been made towards developing a versatile simulator to address this issue, facilitating broader generalization and reproducibility <cite class="ltx_cite ltx_citemacro_cite">Denninger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib26" title="">2019</a>); Morrical et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib27" title="">2021</a>); Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib13" title="">2021</a>)</cite>. For example, BlenderProc <cite class="ltx_cite ltx_citemacro_cite">Denninger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib26" title="">2019</a>)</cite>, which is built on Blender <cite class="ltx_cite ltx_citemacro_cite">Blender Online Community (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib28" title="">2019</a>)</cite>, provides a flexible procedural framework capable of producing photorealistic scene images, alongside their respective segmentation masks, depth maps, or normal images. NViSII <cite class="ltx_cite ltx_citemacro_cite">Morrical et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib27" title="">2021</a>)</cite> presents a Python-based rendering tool leveraging NVIDIA’s OptiX ray tracing engine and its AI denoiser. This tool is designed for the creation of high-quality synthetic images, accompanied by diverse metadata types for various computer vision tasks, including bounding boxes, segmentation masks, depth maps, normal maps, material properties, and optical flow vectors. Moreover, Unity <cite class="ltx_cite ltx_citemacro_cite">Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib13" title="">2021</a>)</cite> advances these initiatives by introducing the Unity Perception package, designed to streamline and expedite synthetic dataset generation. This package provides a user-friendly and highly customizable suite of tools, enabling the production of perfectly annotated data samples. It also features a comprehensive randomization framework to introduce diversity into the datasets created. However, these simulators fall short of integrating with specific domains or tasks, particularly when dealing with objects that have inconsistent shapes and indistinct edges.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In contrast, our approach leverages 3D model of digital twins as well as spacial models of fictional spaces as the simulation framework. A digital twin is a virtual replica of a physical object, process, or system, bridging the physical and digital worlds through real-time data collection and analysis <cite class="ltx_cite ltx_citemacro_cite">Grieves and Vickers (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib29" title="">2017</a>)</cite>. This innovative concept has found effective applications across numerous sectors, notably in the manufacturing industry <cite class="ltx_cite ltx_citemacro_cite">Kritzinger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib30" title="">2018</a>)</cite>, healthcare <cite class="ltx_cite ltx_citemacro_cite">Erol et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib31" title="">2020</a>)</cite>, and smart cities <cite class="ltx_cite ltx_citemacro_cite">White et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib32" title="">2021</a>)</cite>. To the best of our knowledge, we are the first to employ digital twins for producing synthetic data for fuzzy object detection, resulting in models that surpass those trained on real datasets in performance.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The present method involves utilizing a fully synthetic 3D model of environments and spaces in conjunction with synthetically generated fire 3D models, and an algorithm for automated annotation to create fully synthetic training data for computer vision machine learning (ML) algorithms.
We utilized EmberGen software, to create fire and flames in visible light spectrum for the synthetic data generation (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:obnecol</span>). Moreover, multiple 3D models created based on real world or fictional environments were utilized to generate the frames that then were fused with synthetically generated fire and flame data. The resulting fully synthetic images are then used for ML model training. The spacial models used to generate the synthetic image backgrounds were collected from the open 3D model libraries of Sketchfab on the web.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We conducted this study in two subsequent iterations; In Method 1, we utilized four 3D models extracted from real-world digital twins as well as fictional worlds as the backgrounds. The background models were used as a carrier of the synthetic fire frames while randomizing the placement of the fire frames inside the 3D model and only keeping the orientation of the fires constant. We also developed a first version of a code for annotation automation. Subsequently, in Method 1, 1000 fully synthetic images were generate and used to train a CV ML model in Experiment 1. Afterwards, the model performance in fire detection was evaluated to identify improvements. After the analysis of the results of the Method 1, in the subsequent step, efforts were focused on the improvements in the synthetic frames diversity, and annotation accuracy. In Method 2, the diversity of fires and spacial models were significantly increased by using larger number of 3D models and also rendering many different fire and flames separately. Moreover, the annotation accuracy was improved by revising the initial code with a new approach.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">The details of the improved methodology for the fully synthetic data generation used for machine learning algorithm training for fire detection is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S3.F1" title="Figure 1 ‣ 3 Methodology ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="322" id="S3.F1.g1" src="extracted/5894210/image/flowchart.png" width="359"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;"> The Method 2 flowchart for generating and automatically annotating the fully synthetic fire imagery</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Iterations of synthetic data generation method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To answer the research questions and examine the viability of our proposed synthetic generation method, we conducted two consecutive iterations. In this section, initially, we explain the method proposed for synthetic data generation and automated annotation in Method 1 and complementary Method 2.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Method 1: Process establishment</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Setting up virtual scenes using 3D models </h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">As a first step for our method, we selected 3D scenes reconstructed based on field scans, as well as some open-source scenes from Sketchfab, including residential, commercial, and industrial spaces.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Defining camera movement rules</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">After selecting the 3D environments, we defined the movement limitations of the camera within each scene, including the area within which the camera should move, rotation angles, and track-to targets. This enables the camera to cover as many different corners of the scene as possible while maintaining a horizontal view, thereby generating a variety of environmental visualizations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Generating and arranging flame/fire image planes</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In this step, the fire and flame images are randomly added to the 3D environmental models and further adjustments are performed. Based on the camera’s position, we arranged the flame image planes. Here, flame images are used instead of particle-generated flames because using image flames not only significantly reduces performance overhead but also allows for the use of different flame textures in each frame. The flame images are sourced from EmberGen-generated photo-realistic flame model renders. By adjusting different parameters, various colors, burning shapes, and smoke states the flames can be created and differentiated.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Comparison of background layers, flame object layers, annotation layers, and their relation to camera position used in Method 1 and Method 2. Method 1 (left): Random fire image plane overlay. Method 2 (right): Parallel with background overlay of fire image plane. </span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S4.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Comparison of annotation bounding box accuracy between 3D flame plane placement in Method 1 (left) and 2D layer flame generation in Method 2 (right) </span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Calculating annotations</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">After setting up the camera and fires, a code was written in Blender Python to calculate the flame and fire annotations. The logic of the code is to obtain the world coordinates of the flame and fires in the camera plane and use perspective algorithms to translate them into two-dimensional coordinates within the camera plane. On this basis, we utilized the bounding box algorithm and the size of the flame and fire planes to calculate the size of the flame and fires in the camera plane. We output this size and position as a rectangle’s center and dimensions into the frame’s corresponding JSON file. In cases where the flame or fire are at the edge of the screen, We only calculated the visible part that enters the camera plane (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:short1</span>).</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="226" id="S4.F4.g1" src="extracted/5894210/image/synthetic_samples.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Samples of SynthData 2 (synthetic images generated and annotated in Method 2) used in Experiment 2</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Method 2: Process improvements</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">After utilizing the synthetic data generated in Method 1 for the training of a CV ML model, relatively weak results were achieved, which indicated two major issues in the first method:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Inaccuracy in the automatic annotation algorithm,</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Lack of diversity and randomization in the 3D scenes and the fire and flames generated by EmberGen software.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">In Method 2, we addressed the issues, which are explained as follows;</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Arranging flame/fire image planes to improve annotations</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Step <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1.SSS3" title="4.1.3 Generating and arranging flame/fire image planes ‣ 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">4.1.3</span></a> of Method 1, caused challenges with the accuracy of the automated annotation of the synthetic fires and flames in the synthetic scenes (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:short1</span>). To resolve this issue in the Method 2, we imported the EmberGen-generated flame images into the Blender scene in the form of ’image to plane’ and set these planes’ constraints to ’always face the camera’ (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.F2" title="Figure 2 ‣ 4.1.3 Generating and arranging flame/fire image planes ‣ 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">2</span></a>). Subsequently, we randomized the positions and sizes of these flame planes according to the camera’s location.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">The details of the improvements made to the annotation process in Method 2 are outlined as follows;</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">To enhance the precision of flame annotation within 3D scenes, an alternative approach was adopted that involved directly overlaying fire and flame images of varying sizes and positions onto a pre-rendered background image. The process of defining camera movement rules explained in section <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1.SSS2" title="4.1.2 Defining camera movement rules ‣ 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">4.1.2</span></a> was repeated, but this time the 3D scene did not contain preset fire and flames. After obtaining the same rendered images of the 3D scene, we extracted these images and used them as the background for the next step. In the subsequent step, fire and flame frames were randomly placed in the foreground. This method consolidates the steps of placing fire and flame planes within a 3D environment and projecting them onto the camera plane from the previous method. In this way, as the flames were directly placed on the 2D plane, we could achieve very accurate annotations and effectively eliminate errors caused by perspective distortion due to camera focus (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.F3" title="Figure 3 ‣ 4.1.3 Generating and arranging flame/fire image planes ‣ 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Diversification of the scenes and fires</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Additionally, we improved the logic for generating fire and flames with EmberGen, allowing us to produce a rich variety of flame shapes and colors. The new fire and flame images were the result of rendering a fire 3D model with varying criteria and then converting the video of the burning fire into 2D frames. We selected flame images from a continuous set of 1024 image files, picking one every 12 frames to ensure the distinctiveness of the flame shapes (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:onecol66</span>). Criteria changed for each fire and flame model were particle emission method, emission shape, and color. Thus, we could easily obtain thousands of different fire and flame images. After rendering the fire and flame models and obtaining the frames, we used a Photoshop script for batch processing to remove the transparent background and crop to the size of the colored pixel part, to ensure the accuracy of the annotations.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">In addition to fire and flame diversification, we also increased the number of 3D environmental models used for generating the scene backgrounds. While in Method 1, we only used 4 environmental models, in Method 2, 48 environments and distinct scenes were utilized assisting with the improved diversity and randomization of the synthetic imagery (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.F4" title="Figure 4 ‣ 4.1.4 Calculating annotations ‣ 4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>ML experiments </h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To examine the viability of our fuzzy object synthetic data generation methods, we conducted a series of ML experiments.
Our experiments were inspired by previous research <cite class="ltx_cite ltx_citemacro_cite">Hinterstoisser et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib12" title="">2019</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib13" title="">2021</a>)</cite>.
We describe our experimental setup in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS1" title="5.1 Experimental setup ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">5.1</span></a>, followed by a comparison of the performance results of our two synthetic image generation methods in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS2" title="5.2 Comparison of synthetic data generation methods ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">5.2</span></a>. Finally, we compare the performance of different training combinations of Experiment 2 on two different test benchmarks in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS3" title="5.3 Comparison of various training combinations ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental setup</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p1.1.1">Datasets.</span> We introduce two synthetic fire datasets: SynthData 1 and SynthData 2, each containing 1,000 images and 2D bounding box annotations generated using Method 1 (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS1" title="4.1 Method 1: Process establishment ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">4.1</span></a>) and Method 2 (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS2" title="4.2 Method 2: Process improvements ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">4.2</span></a>) respectively. We compare the performance of these two synthetic fire datasets (Experiment 1 vs. Experiment 2) in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS2" title="5.2 Comparison of synthetic data generation methods ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">5.2</span></a>.
Experiment 1 is training of YOLOv5 mini using SynthData 1 while Experiment 2 is the training of YOLOv5 mini using SynthData 2.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We also collected a real-world diverse dataset, named "Fire in The Wild" FiIW, containing 1400 images of real instances of fire scenes. To curate this dicverse dataset, we first extended the D-Fire <cite class="ltx_cite ltx_citemacro_cite">de Venancio et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib9" title="">2022</a>)</cite> dataset with more web-source fire images, then removed all exact-duplicate and near-duplicate images, based on the Euclidean distance of their VGG embeddings, and finally selected the top 1,400 unique images, based on the ranking of their relative distance to other samples. This dataset is annotated manually by us and with the guidelines from the VOC2011 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib33" title="">voc </a></cite>. It is randomly shuffled and split into a training set of 1000 (71.4%) images, a validation set of 200 (14.3%) images, and a test set of 200 (14.3%) images. These validation and test sets are used in both experiments.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">We also randomly selected a few subsets of size 250 (25%), 500 (50%), and 750 (75%) from both the FiIW training set and the SynthData 2, to build multiple mixtures of synthetic and real images for training in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.SS3" title="5.3 Comparison of various training combinations ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">5.3</span></a>. The total number of images in each training mixture is kept constant 1,000.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">To assess the generalizability of the model in identifying rare instances of fires, an additional real-world web-source test dataset was curated, denoted as "RealRareFire", which also is manually annotated by us and with the guidelines from the VOC2011 <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib33" title="">voc </a></cite>. This dataset comprises 69 images depicting uncommon fire occurrences, distinct from those encompassed within the FiIW dataset. Although RealRareFire is relatively small as a dataset, it is noteworthy that the acquisition of each image in that demanded considerably more time compared to that of the FiIW dataset. This is attributable to the scarcity of such images exhibiting distinctive visual attributes, namely color and shape, across online sources. The comparative evaluation of the model’s performance across these two test datasets is instrumental in discerning potential instances of overfitting to the FiIW dataset, which predominantly comprises fire images readily accessible on the internet.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.2.1">Evaluation Metrics.</span>
We consider two evaluation metrics of mean Average Precision averaged across IoU thresholds of [0.5:0.95] (<math alttext="\mathbf{AP}" class="ltx_Math" display="inline" id="S5.SS1.p5.1.m1.1"><semantics id="S5.SS1.p5.1.m1.1a"><mi id="S5.SS1.p5.1.m1.1.1" xref="S5.SS1.p5.1.m1.1.1.cmml">𝐀𝐏</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.1.m1.1b"><ci id="S5.SS1.p5.1.m1.1.1.cmml" xref="S5.SS1.p5.1.m1.1.1">𝐀𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.1.m1.1c">\mathbf{AP}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p5.1.m1.1d">bold_AP</annotation></semantics></math>), and mean Average Precision with a single intersection of union (IoU) threshold of 0.5 (<math alttext="\mathbf{AP_{50}}" class="ltx_Math" display="inline" id="S5.SS1.p5.2.m2.1"><semantics id="S5.SS1.p5.2.m2.1a"><msub id="S5.SS1.p5.2.m2.1.1" xref="S5.SS1.p5.2.m2.1.1.cmml"><mi id="S5.SS1.p5.2.m2.1.1.2" xref="S5.SS1.p5.2.m2.1.1.2.cmml">𝐀𝐏</mi><mn id="S5.SS1.p5.2.m2.1.1.3" xref="S5.SS1.p5.2.m2.1.1.3.cmml">𝟓𝟎</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p5.2.m2.1b"><apply id="S5.SS1.p5.2.m2.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p5.2.m2.1.1.1.cmml" xref="S5.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p5.2.m2.1.1.2.cmml" xref="S5.SS1.p5.2.m2.1.1.2">𝐀𝐏</ci><cn id="S5.SS1.p5.2.m2.1.1.3.cmml" type="integer" xref="S5.SS1.p5.2.m2.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p5.2.m2.1c">\mathbf{AP_{50}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p5.2.m2.1d">bold_AP start_POSTSUBSCRIPT bold_50 end_POSTSUBSCRIPT</annotation></semantics></math>). These are based on MS COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib15" title="">2015</a>)</cite> definition, which are extensively used across the object detection literature <cite class="ltx_cite ltx_citemacro_cite">Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib13" title="">2021</a>); Bochkovskiy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib34" title="">2020</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib35" title="">2022</a>); Ge et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib36" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.3"><span class="ltx_text ltx_font_bold" id="S5.SS1.p6.3.1">Training Setup.</span>
In both experiments, we perform the default data augmentation methods in YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">Jocher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib37" title="">2022</a>)</cite>, including Mosaic introduced by <cite class="ltx_cite ltx_citemacro_cite">Bochkovskiy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib34" title="">2020</a>)</cite>, image translation, horizontal flipping, HSV (hue saturation value) augmentation. All models are trained with a minibatch size of 64 on one NVIDIA-A100 GPU for 100 epochs with SGD (stochastic gradient descent) for optimization. Other default hyper-parameters in YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">Jocher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib37" title="">2022</a>)</cite> are unchanged, including "fitness", which is the metric to select the best model over training epochs, and is computed on the validation set of the FiIW. <cite class="ltx_cite ltx_citemacro_cite">Jocher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib37" title="">2022</a>)</cite> defined it as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Fitness=0.1\times AP_{50}+0.9\times AP" class="ltx_Math" display="block" id="S5.Ex1.m1.1"><semantics id="S5.Ex1.m1.1a"><mrow id="S5.Ex1.m1.1.1" xref="S5.Ex1.m1.1.1.cmml"><mrow id="S5.Ex1.m1.1.1.2" xref="S5.Ex1.m1.1.1.2.cmml"><mi id="S5.Ex1.m1.1.1.2.2" xref="S5.Ex1.m1.1.1.2.2.cmml">F</mi><mo id="S5.Ex1.m1.1.1.2.1" xref="S5.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.2.3" xref="S5.Ex1.m1.1.1.2.3.cmml">i</mi><mo id="S5.Ex1.m1.1.1.2.1a" xref="S5.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.2.4" xref="S5.Ex1.m1.1.1.2.4.cmml">t</mi><mo id="S5.Ex1.m1.1.1.2.1b" xref="S5.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.2.5" xref="S5.Ex1.m1.1.1.2.5.cmml">n</mi><mo id="S5.Ex1.m1.1.1.2.1c" xref="S5.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.2.6" xref="S5.Ex1.m1.1.1.2.6.cmml">e</mi><mo id="S5.Ex1.m1.1.1.2.1d" xref="S5.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.2.7" xref="S5.Ex1.m1.1.1.2.7.cmml">s</mi><mo id="S5.Ex1.m1.1.1.2.1e" xref="S5.Ex1.m1.1.1.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.2.8" xref="S5.Ex1.m1.1.1.2.8.cmml">s</mi></mrow><mo id="S5.Ex1.m1.1.1.1" xref="S5.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S5.Ex1.m1.1.1.3" xref="S5.Ex1.m1.1.1.3.cmml"><mrow id="S5.Ex1.m1.1.1.3.2" xref="S5.Ex1.m1.1.1.3.2.cmml"><mrow id="S5.Ex1.m1.1.1.3.2.2" xref="S5.Ex1.m1.1.1.3.2.2.cmml"><mn id="S5.Ex1.m1.1.1.3.2.2.2" xref="S5.Ex1.m1.1.1.3.2.2.2.cmml">0.1</mn><mo id="S5.Ex1.m1.1.1.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S5.Ex1.m1.1.1.3.2.2.1.cmml">×</mo><mi id="S5.Ex1.m1.1.1.3.2.2.3" xref="S5.Ex1.m1.1.1.3.2.2.3.cmml">A</mi></mrow><mo id="S5.Ex1.m1.1.1.3.2.1" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><msub id="S5.Ex1.m1.1.1.3.2.3" xref="S5.Ex1.m1.1.1.3.2.3.cmml"><mi id="S5.Ex1.m1.1.1.3.2.3.2" xref="S5.Ex1.m1.1.1.3.2.3.2.cmml">P</mi><mn id="S5.Ex1.m1.1.1.3.2.3.3" xref="S5.Ex1.m1.1.1.3.2.3.3.cmml">50</mn></msub></mrow><mo id="S5.Ex1.m1.1.1.3.1" xref="S5.Ex1.m1.1.1.3.1.cmml">+</mo><mrow id="S5.Ex1.m1.1.1.3.3" xref="S5.Ex1.m1.1.1.3.3.cmml"><mrow id="S5.Ex1.m1.1.1.3.3.2" xref="S5.Ex1.m1.1.1.3.3.2.cmml"><mn id="S5.Ex1.m1.1.1.3.3.2.2" xref="S5.Ex1.m1.1.1.3.3.2.2.cmml">0.9</mn><mo id="S5.Ex1.m1.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S5.Ex1.m1.1.1.3.3.2.1.cmml">×</mo><mi id="S5.Ex1.m1.1.1.3.3.2.3" xref="S5.Ex1.m1.1.1.3.3.2.3.cmml">A</mi></mrow><mo id="S5.Ex1.m1.1.1.3.3.1" xref="S5.Ex1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.3.3" xref="S5.Ex1.m1.1.1.3.3.3.cmml">P</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.1b"><apply id="S5.Ex1.m1.1.1.cmml" xref="S5.Ex1.m1.1.1"><eq id="S5.Ex1.m1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1"></eq><apply id="S5.Ex1.m1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.2"><times id="S5.Ex1.m1.1.1.2.1.cmml" xref="S5.Ex1.m1.1.1.2.1"></times><ci id="S5.Ex1.m1.1.1.2.2.cmml" xref="S5.Ex1.m1.1.1.2.2">𝐹</ci><ci id="S5.Ex1.m1.1.1.2.3.cmml" xref="S5.Ex1.m1.1.1.2.3">𝑖</ci><ci id="S5.Ex1.m1.1.1.2.4.cmml" xref="S5.Ex1.m1.1.1.2.4">𝑡</ci><ci id="S5.Ex1.m1.1.1.2.5.cmml" xref="S5.Ex1.m1.1.1.2.5">𝑛</ci><ci id="S5.Ex1.m1.1.1.2.6.cmml" xref="S5.Ex1.m1.1.1.2.6">𝑒</ci><ci id="S5.Ex1.m1.1.1.2.7.cmml" xref="S5.Ex1.m1.1.1.2.7">𝑠</ci><ci id="S5.Ex1.m1.1.1.2.8.cmml" xref="S5.Ex1.m1.1.1.2.8">𝑠</ci></apply><apply id="S5.Ex1.m1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.3"><plus id="S5.Ex1.m1.1.1.3.1.cmml" xref="S5.Ex1.m1.1.1.3.1"></plus><apply id="S5.Ex1.m1.1.1.3.2.cmml" xref="S5.Ex1.m1.1.1.3.2"><times id="S5.Ex1.m1.1.1.3.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2.1"></times><apply id="S5.Ex1.m1.1.1.3.2.2.cmml" xref="S5.Ex1.m1.1.1.3.2.2"><times id="S5.Ex1.m1.1.1.3.2.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2.2.1"></times><cn id="S5.Ex1.m1.1.1.3.2.2.2.cmml" type="float" xref="S5.Ex1.m1.1.1.3.2.2.2">0.1</cn><ci id="S5.Ex1.m1.1.1.3.2.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.2.3">𝐴</ci></apply><apply id="S5.Ex1.m1.1.1.3.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.3.2.3.1.cmml" xref="S5.Ex1.m1.1.1.3.2.3">subscript</csymbol><ci id="S5.Ex1.m1.1.1.3.2.3.2.cmml" xref="S5.Ex1.m1.1.1.3.2.3.2">𝑃</ci><cn id="S5.Ex1.m1.1.1.3.2.3.3.cmml" type="integer" xref="S5.Ex1.m1.1.1.3.2.3.3">50</cn></apply></apply><apply id="S5.Ex1.m1.1.1.3.3.cmml" xref="S5.Ex1.m1.1.1.3.3"><times id="S5.Ex1.m1.1.1.3.3.1.cmml" xref="S5.Ex1.m1.1.1.3.3.1"></times><apply id="S5.Ex1.m1.1.1.3.3.2.cmml" xref="S5.Ex1.m1.1.1.3.3.2"><times id="S5.Ex1.m1.1.1.3.3.2.1.cmml" xref="S5.Ex1.m1.1.1.3.3.2.1"></times><cn id="S5.Ex1.m1.1.1.3.3.2.2.cmml" type="float" xref="S5.Ex1.m1.1.1.3.3.2.2">0.9</cn><ci id="S5.Ex1.m1.1.1.3.3.2.3.cmml" xref="S5.Ex1.m1.1.1.3.3.2.3">𝐴</ci></apply><ci id="S5.Ex1.m1.1.1.3.3.3.cmml" xref="S5.Ex1.m1.1.1.3.3.3">𝑃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.1c">Fitness=0.1\times AP_{50}+0.9\times AP</annotation><annotation encoding="application/x-llamapun" id="S5.Ex1.m1.1d">italic_F italic_i italic_t italic_n italic_e italic_s italic_s = 0.1 × italic_A italic_P start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPT + 0.9 × italic_A italic_P</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.p6.2">We decided to use the same coefficient weights for <math alttext="\mathbf{AP_{50}}" class="ltx_Math" display="inline" id="S5.SS1.p6.1.m1.1"><semantics id="S5.SS1.p6.1.m1.1a"><msub id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml"><mi id="S5.SS1.p6.1.m1.1.1.2" xref="S5.SS1.p6.1.m1.1.1.2.cmml">𝐀𝐏</mi><mn id="S5.SS1.p6.1.m1.1.1.3" xref="S5.SS1.p6.1.m1.1.1.3.cmml">𝟓𝟎</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><apply id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p6.1.m1.1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p6.1.m1.1.1.2.cmml" xref="S5.SS1.p6.1.m1.1.1.2">𝐀𝐏</ci><cn id="S5.SS1.p6.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.p6.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">\mathbf{AP_{50}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.1.m1.1d">bold_AP start_POSTSUBSCRIPT bold_50 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{AP}" class="ltx_Math" display="inline" id="S5.SS1.p6.2.m2.1"><semantics id="S5.SS1.p6.2.m2.1a"><mi id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml">𝐀𝐏</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.1b"><ci id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1">𝐀𝐏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.1c">\mathbf{AP}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.2.m2.1d">bold_AP</annotation></semantics></math> to facilitate the reproducibility of our work, although YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">Jocher et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib37" title="">2022</a>)</cite> lacks a clear explanation for choosing these weights.</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p7.1.1">Random Seeds.</span> As there is inherent randomness in choosing the subsets and in training neural networks, we report the mean and standard deviation of each metric over five seeds.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison of synthetic data generation methods</h3>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.16.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S5.T1.17.2" style="font-size:90%;">Comparison of Experiments 1 and 2 mean and standard deviation over five seeds</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T1.2.2.3"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T1.1.1.1"><math alttext="\mathbf{FiIW_{test}}" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><msub id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml"><mi id="S5.T1.1.1.1.m1.1.1.2" mathsize="80%" xref="S5.T1.1.1.1.m1.1.1.2.cmml">𝐅𝐢𝐈𝐖</mi><mi id="S5.T1.1.1.1.m1.1.1.3" mathsize="80%" xref="S5.T1.1.1.1.m1.1.1.3.cmml">𝐭𝐞𝐬𝐭</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.m1.1.1.2">𝐅𝐢𝐈𝐖</ci><ci id="S5.T1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.m1.1.1.3">𝐭𝐞𝐬𝐭</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\mathbf{FiIW_{test}}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">bold_FiIW start_POSTSUBSCRIPT bold_test end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T1.2.2.2"><math alttext="\mathbf{RealRareFire}" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.m1.1a"><mi id="S5.T1.2.2.2.m1.1.1" mathsize="80%" xref="S5.T1.2.2.2.m1.1.1.cmml">𝐑𝐞𝐚𝐥𝐑𝐚𝐫𝐞𝐅𝐢𝐫𝐞</mi><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">𝐑𝐞𝐚𝐥𝐑𝐚𝐫𝐞𝐅𝐢𝐫𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">\mathbf{RealRareFire}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m1.1d">bold_RealRareFire</annotation></semantics></math></th>
</tr>
<tr class="ltx_tr" id="S5.T1.6.6">
<th class="ltx_td ltx_th ltx_th_row" id="S5.T1.6.6.5"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.3.3.1"><math alttext="\mathbf{AP_{50}(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T1.3.3.1.m1.1"><semantics id="S5.T1.3.3.1.m1.1a"><mrow id="S5.T1.3.3.1.m1.1b"><msub id="S5.T1.3.3.1.m1.1.1"><mi id="S5.T1.3.3.1.m1.1.1.2" mathsize="80%">𝐀𝐏</mi><mn id="S5.T1.3.3.1.m1.1.1.3" mathsize="80%">𝟓𝟎</mn></msub><mrow id="S5.T1.3.3.1.m1.1.2"><mo id="S5.T1.3.3.1.m1.1.2.1" maxsize="80%" minsize="80%">(</mo><mo id="S5.T1.3.3.1.m1.1.2.2" mathsize="80%">%</mo><mo id="S5.T1.3.3.1.m1.1.2.3" maxsize="80%" minsize="80%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T1.3.3.1.m1.1c">\mathbf{AP_{50}(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.1.m1.1d">bold_AP start_POSTSUBSCRIPT bold_50 end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.2"><math alttext="\mathbf{AP(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T1.4.4.2.m1.1"><semantics id="S5.T1.4.4.2.m1.1a"><mrow id="S5.T1.4.4.2.m1.1b"><mi id="S5.T1.4.4.2.m1.1.1" mathsize="80%">𝐀𝐏</mi><mrow id="S5.T1.4.4.2.m1.1.2"><mo id="S5.T1.4.4.2.m1.1.2.1" maxsize="80%" minsize="80%">(</mo><mo id="S5.T1.4.4.2.m1.1.2.2" mathsize="80%">%</mo><mo id="S5.T1.4.4.2.m1.1.2.3" maxsize="80%" minsize="80%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T1.4.4.2.m1.1c">\mathbf{AP(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.2.m1.1d">bold_AP ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.5.5.3"><math alttext="\mathbf{AP_{50}(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T1.5.5.3.m1.1"><semantics id="S5.T1.5.5.3.m1.1a"><mrow id="S5.T1.5.5.3.m1.1b"><msub id="S5.T1.5.5.3.m1.1.1"><mi id="S5.T1.5.5.3.m1.1.1.2" mathsize="80%">𝐀𝐏</mi><mn id="S5.T1.5.5.3.m1.1.1.3" mathsize="80%">𝟓𝟎</mn></msub><mrow id="S5.T1.5.5.3.m1.1.2"><mo id="S5.T1.5.5.3.m1.1.2.1" maxsize="80%" minsize="80%">(</mo><mo id="S5.T1.5.5.3.m1.1.2.2" mathsize="80%">%</mo><mo id="S5.T1.5.5.3.m1.1.2.3" maxsize="80%" minsize="80%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T1.5.5.3.m1.1c">\mathbf{AP_{50}(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.3.m1.1d">bold_AP start_POSTSUBSCRIPT bold_50 end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.6.6.4"><math alttext="\mathbf{AP(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T1.6.6.4.m1.1"><semantics id="S5.T1.6.6.4.m1.1a"><mrow id="S5.T1.6.6.4.m1.1b"><mi id="S5.T1.6.6.4.m1.1.1" mathsize="80%">𝐀𝐏</mi><mrow id="S5.T1.6.6.4.m1.1.2"><mo id="S5.T1.6.6.4.m1.1.2.1" maxsize="80%" minsize="80%">(</mo><mo id="S5.T1.6.6.4.m1.1.2.2" mathsize="80%">%</mo><mo id="S5.T1.6.6.4.m1.1.2.3" maxsize="80%" minsize="80%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T1.6.6.4.m1.1c">\mathbf{AP(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.6.6.4.m1.1d">bold_AP ( % )</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T1.10.10.5"><span class="ltx_text" id="S5.T1.10.10.5.1" style="font-size:80%;">Experiment 1</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.7.7.1">
<span class="ltx_text" id="S5.T1.7.7.1.1" style="font-size:80%;">3.64 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.7.7.1.m1.1"><semantics id="S5.T1.7.7.1.m1.1a"><mo id="S5.T1.7.7.1.m1.1.1" mathsize="80%" xref="S5.T1.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.1b"><csymbol cd="latexml" id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.7.7.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T1.7.7.1.2" style="font-size:80%;"> 1.26</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.8.8.2">
<span class="ltx_text" id="S5.T1.8.8.2.1" style="font-size:80%;">0.94 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.8.8.2.m1.1"><semantics id="S5.T1.8.8.2.m1.1a"><mo id="S5.T1.8.8.2.m1.1.1" mathsize="80%" xref="S5.T1.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.2.m1.1b"><csymbol cd="latexml" id="S5.T1.8.8.2.m1.1.1.cmml" xref="S5.T1.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.8.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T1.8.8.2.2" style="font-size:80%;"> 0.37</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.9.9.3">
<span class="ltx_text" id="S5.T1.9.9.3.1" style="font-size:80%;">4.9 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.9.9.3.m1.1"><semantics id="S5.T1.9.9.3.m1.1a"><mo id="S5.T1.9.9.3.m1.1.1" mathsize="80%" xref="S5.T1.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.3.m1.1b"><csymbol cd="latexml" id="S5.T1.9.9.3.m1.1.1.cmml" xref="S5.T1.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.9.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T1.9.9.3.2" style="font-size:80%;"> 1.14</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.10.10.4">
<span class="ltx_text" id="S5.T1.10.10.4.1" style="font-size:80%;">1.35 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.10.10.4.m1.1"><semantics id="S5.T1.10.10.4.m1.1a"><mo id="S5.T1.10.10.4.m1.1.1" mathsize="80%" xref="S5.T1.10.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.4.m1.1b"><csymbol cd="latexml" id="S5.T1.10.10.4.m1.1.1.cmml" xref="S5.T1.10.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.10.10.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T1.10.10.4.2" style="font-size:80%;"> 0.33</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T1.14.14.5"><span class="ltx_text" id="S5.T1.14.14.5.1" style="font-size:80%;">Experiment 2</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.11.11.1"><span class="ltx_text ltx_font_bold" id="S5.T1.11.11.1.1" style="font-size:80%;">15.14 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.11.11.1.1.m1.1"><semantics id="S5.T1.11.11.1.1.m1.1a"><mo id="S5.T1.11.11.1.1.m1.1.1" xref="S5.T1.11.11.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.1.1.m1.1b"><csymbol cd="latexml" id="S5.T1.11.11.1.1.m1.1.1.cmml" xref="S5.T1.11.11.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.11.11.1.1.m1.1d">±</annotation></semantics></math> 1.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.12.12.2"><span class="ltx_text ltx_font_bold" id="S5.T1.12.12.2.1" style="font-size:80%;">5.65 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.12.12.2.1.m1.1"><semantics id="S5.T1.12.12.2.1.m1.1a"><mo id="S5.T1.12.12.2.1.m1.1.1" xref="S5.T1.12.12.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.2.1.m1.1b"><csymbol cd="latexml" id="S5.T1.12.12.2.1.m1.1.1.cmml" xref="S5.T1.12.12.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.12.12.2.1.m1.1d">±</annotation></semantics></math> 0.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.13.13.3"><span class="ltx_text ltx_font_bold" id="S5.T1.13.13.3.1" style="font-size:80%;">24.38 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.13.13.3.1.m1.1"><semantics id="S5.T1.13.13.3.1.m1.1a"><mo id="S5.T1.13.13.3.1.m1.1.1" xref="S5.T1.13.13.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.3.1.m1.1b"><csymbol cd="latexml" id="S5.T1.13.13.3.1.m1.1.1.cmml" xref="S5.T1.13.13.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.13.13.3.1.m1.1d">±</annotation></semantics></math> 2.09</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T1.14.14.4"><span class="ltx_text ltx_font_bold" id="S5.T1.14.14.4.1" style="font-size:80%;">8.87 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T1.14.14.4.1.m1.1"><semantics id="S5.T1.14.14.4.1.m1.1a"><mo id="S5.T1.14.14.4.1.m1.1.1" xref="S5.T1.14.14.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T1.14.14.4.1.m1.1b"><csymbol cd="latexml" id="S5.T1.14.14.4.1.m1.1.1.cmml" xref="S5.T1.14.14.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.14.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T1.14.14.4.1.m1.1d">±</annotation></semantics></math> 0.52</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F5.sf1.g1" src="extracted/5894210/image/FiIW_AP.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="337" id="S5.F5.sf2.g1" src="extracted/5894210/image/RareFire_AP.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.6.3.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.4.2" style="font-size:90%;">Comparison of training combinations tested on (a) <math alttext="\mathbf{FiIW_{test}}" class="ltx_Math" display="inline" id="S5.F5.3.1.m1.1"><semantics id="S5.F5.3.1.m1.1b"><msub id="S5.F5.3.1.m1.1.1" xref="S5.F5.3.1.m1.1.1.cmml"><mi id="S5.F5.3.1.m1.1.1.2" xref="S5.F5.3.1.m1.1.1.2.cmml">𝐅𝐢𝐈𝐖</mi><mi id="S5.F5.3.1.m1.1.1.3" xref="S5.F5.3.1.m1.1.1.3.cmml">𝐭𝐞𝐬𝐭</mi></msub><annotation-xml encoding="MathML-Content" id="S5.F5.3.1.m1.1c"><apply id="S5.F5.3.1.m1.1.1.cmml" xref="S5.F5.3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.F5.3.1.m1.1.1.1.cmml" xref="S5.F5.3.1.m1.1.1">subscript</csymbol><ci id="S5.F5.3.1.m1.1.1.2.cmml" xref="S5.F5.3.1.m1.1.1.2">𝐅𝐢𝐈𝐖</ci><ci id="S5.F5.3.1.m1.1.1.3.cmml" xref="S5.F5.3.1.m1.1.1.3">𝐭𝐞𝐬𝐭</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.3.1.m1.1d">\mathbf{FiIW_{test}}</annotation><annotation encoding="application/x-llamapun" id="S5.F5.3.1.m1.1e">bold_FiIW start_POSTSUBSCRIPT bold_test end_POSTSUBSCRIPT</annotation></semantics></math> and (b) <math alttext="\mathbf{RealRareFire}" class="ltx_Math" display="inline" id="S5.F5.4.2.m2.1"><semantics id="S5.F5.4.2.m2.1b"><mi id="S5.F5.4.2.m2.1.1" xref="S5.F5.4.2.m2.1.1.cmml">𝐑𝐞𝐚𝐥𝐑𝐚𝐫𝐞𝐅𝐢𝐫𝐞</mi><annotation-xml encoding="MathML-Content" id="S5.F5.4.2.m2.1c"><ci id="S5.F5.4.2.m2.1.1.cmml" xref="S5.F5.4.2.m2.1.1">𝐑𝐞𝐚𝐥𝐑𝐚𝐫𝐞𝐅𝐢𝐫𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.4.2.m2.1d">\mathbf{RealRareFire}</annotation><annotation encoding="application/x-llamapun" id="S5.F5.4.2.m2.1e">bold_RealRareFire</annotation></semantics></math></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.T1" title="Table 1 ‣ 5.2 Comparison of synthetic data generation methods ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">1</span></a> shows the detection performance result obtained when using different iterations in our method. Experiment 1 uses synthetic data generated and annotated using Method 1 with low level of scene and fire randomization (SynthData 1). Experiment 2 uses synthetic data generated and annotated using Method 2 with high level of scene and fire randomization (SynthData 2).</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">In addressing our initial research question, it becomes evident that ML models trained on both iterations of the fully synthetic fire data generation method produce fire detection. However, we observe that the SynthData 2 outperforms SynthData 1 based on the results reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.T1" title="Table 1 ‣ 5.2 Comparison of synthetic data generation methods ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">1</span></a>. Therefore, Method 2 outperforms Method 1 by high margins in all metrics and benchmarks. This relates to the challenges identified in the Method 1 and addressed in Method 2 (see <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S4.SS2" title="4.2 Method 2: Process improvements ‣ 4 Iterations of synthetic data generation method ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">4.2</span></a>)</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Comparison of various training combinations</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">Combination Setup.</span> Three training strategies with different combinations of SynthData 2 and real-world data are used in this part:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">In Strategy 1, four subsets (250, 500, 750, 1000) of the training set of FiIW is used to fine-tune the model.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1">In Strategy 2, we control for the size of training dataset to be 1000, and build three mixtures of real-synthetic training sets using (250, 500, 750) subsets of each real and synthetic dataset.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1">In Strategy 3, we use four subsets (250, 500, 750, 1000) of SynthData 2 for ML model training.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.60.3.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S5.T2.4.2" style="font-size:90%;">Comparison of three strategies and their training set combinations in R(m)_S(n) format, where m and n indicate the number of images from Real training and Synthetic sets respectively. Comparison metrics are <math alttext="AP_{50}" class="ltx_Math" display="inline" id="S5.T2.3.1.m1.1"><semantics id="S5.T2.3.1.m1.1b"><mrow id="S5.T2.3.1.m1.1.1" xref="S5.T2.3.1.m1.1.1.cmml"><mi id="S5.T2.3.1.m1.1.1.2" xref="S5.T2.3.1.m1.1.1.2.cmml">A</mi><mo id="S5.T2.3.1.m1.1.1.1" xref="S5.T2.3.1.m1.1.1.1.cmml">⁢</mo><msub id="S5.T2.3.1.m1.1.1.3" xref="S5.T2.3.1.m1.1.1.3.cmml"><mi id="S5.T2.3.1.m1.1.1.3.2" xref="S5.T2.3.1.m1.1.1.3.2.cmml">P</mi><mn id="S5.T2.3.1.m1.1.1.3.3" xref="S5.T2.3.1.m1.1.1.3.3.cmml">50</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.3.1.m1.1c"><apply id="S5.T2.3.1.m1.1.1.cmml" xref="S5.T2.3.1.m1.1.1"><times id="S5.T2.3.1.m1.1.1.1.cmml" xref="S5.T2.3.1.m1.1.1.1"></times><ci id="S5.T2.3.1.m1.1.1.2.cmml" xref="S5.T2.3.1.m1.1.1.2">𝐴</ci><apply id="S5.T2.3.1.m1.1.1.3.cmml" xref="S5.T2.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.T2.3.1.m1.1.1.3.1.cmml" xref="S5.T2.3.1.m1.1.1.3">subscript</csymbol><ci id="S5.T2.3.1.m1.1.1.3.2.cmml" xref="S5.T2.3.1.m1.1.1.3.2">𝑃</ci><cn id="S5.T2.3.1.m1.1.1.3.3.cmml" type="integer" xref="S5.T2.3.1.m1.1.1.3.3">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.1.m1.1d">AP_{50}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.1.m1.1e">italic_A italic_P start_POSTSUBSCRIPT 50 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="AP" class="ltx_Math" display="inline" id="S5.T2.4.2.m2.1"><semantics id="S5.T2.4.2.m2.1b"><mrow id="S5.T2.4.2.m2.1.1" xref="S5.T2.4.2.m2.1.1.cmml"><mi id="S5.T2.4.2.m2.1.1.2" xref="S5.T2.4.2.m2.1.1.2.cmml">A</mi><mo id="S5.T2.4.2.m2.1.1.1" xref="S5.T2.4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.T2.4.2.m2.1.1.3" xref="S5.T2.4.2.m2.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.2.m2.1c"><apply id="S5.T2.4.2.m2.1.1.cmml" xref="S5.T2.4.2.m2.1.1"><times id="S5.T2.4.2.m2.1.1.1.cmml" xref="S5.T2.4.2.m2.1.1.1"></times><ci id="S5.T2.4.2.m2.1.1.2.cmml" xref="S5.T2.4.2.m2.1.1.2">𝐴</ci><ci id="S5.T2.4.2.m2.1.1.3.cmml" xref="S5.T2.4.2.m2.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.2.m2.1d">AP</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.2.m2.1e">italic_A italic_P</annotation></semantics></math> (the higher, the better). Mean and standard deviation over 5 seeds is shown for all combinations. <span class="ltx_text ltx_font_bold" id="S5.T2.4.2.1">Best</span> results are shown in <span class="ltx_text ltx_font_bold" id="S5.T2.4.2.2">bold</span>, <span class="ltx_text ltx_font_italic" id="S5.T2.4.2.3">second best</span> results are shown in <span class="ltx_text ltx_font_italic" id="S5.T2.4.2.4">italic</span>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.54">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.6.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T2.6.2.3"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.6.2.4"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.5.1.1"><math alttext="\mathbf{FiIW_{test}}" class="ltx_Math" display="inline" id="S5.T2.5.1.1.m1.1"><semantics id="S5.T2.5.1.1.m1.1a"><msub id="S5.T2.5.1.1.m1.1.1" xref="S5.T2.5.1.1.m1.1.1.cmml"><mi id="S5.T2.5.1.1.m1.1.1.2" mathsize="90%" xref="S5.T2.5.1.1.m1.1.1.2.cmml">𝐅𝐢𝐈𝐖</mi><mi id="S5.T2.5.1.1.m1.1.1.3" mathsize="90%" xref="S5.T2.5.1.1.m1.1.1.3.cmml">𝐭𝐞𝐬𝐭</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T2.5.1.1.m1.1b"><apply id="S5.T2.5.1.1.m1.1.1.cmml" xref="S5.T2.5.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.5.1.1.m1.1.1.1.cmml" xref="S5.T2.5.1.1.m1.1.1">subscript</csymbol><ci id="S5.T2.5.1.1.m1.1.1.2.cmml" xref="S5.T2.5.1.1.m1.1.1.2">𝐅𝐢𝐈𝐖</ci><ci id="S5.T2.5.1.1.m1.1.1.3.cmml" xref="S5.T2.5.1.1.m1.1.1.3">𝐭𝐞𝐬𝐭</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.5.1.1.m1.1c">\mathbf{FiIW_{test}}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.5.1.1.m1.1d">bold_FiIW start_POSTSUBSCRIPT bold_test end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T2.6.2.2"><math alttext="\mathbf{RealRareFire}" class="ltx_Math" display="inline" id="S5.T2.6.2.2.m1.1"><semantics id="S5.T2.6.2.2.m1.1a"><mi id="S5.T2.6.2.2.m1.1.1" mathsize="90%" xref="S5.T2.6.2.2.m1.1.1.cmml">𝐑𝐞𝐚𝐥𝐑𝐚𝐫𝐞𝐅𝐢𝐫𝐞</mi><annotation-xml encoding="MathML-Content" id="S5.T2.6.2.2.m1.1b"><ci id="S5.T2.6.2.2.m1.1.1.cmml" xref="S5.T2.6.2.2.m1.1.1">𝐑𝐞𝐚𝐥𝐑𝐚𝐫𝐞𝐅𝐢𝐫𝐞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.2.2.m1.1c">\mathbf{RealRareFire}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.2.2.m1.1d">bold_RealRareFire</annotation></semantics></math></th>
</tr>
<tr class="ltx_tr" id="S5.T2.10.6">
<th class="ltx_td ltx_th ltx_th_row" id="S5.T2.10.6.5"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row" id="S5.T2.10.6.6"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.7.3.1"><math alttext="\mathbf{AP_{50}(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T2.7.3.1.m1.1"><semantics id="S5.T2.7.3.1.m1.1a"><mrow id="S5.T2.7.3.1.m1.1b"><msub id="S5.T2.7.3.1.m1.1.1"><mi id="S5.T2.7.3.1.m1.1.1.2" mathsize="90%">𝐀𝐏</mi><mn id="S5.T2.7.3.1.m1.1.1.3" mathsize="90%">𝟓𝟎</mn></msub><mrow id="S5.T2.7.3.1.m1.1.2"><mo id="S5.T2.7.3.1.m1.1.2.1" maxsize="90%" minsize="90%">(</mo><mo id="S5.T2.7.3.1.m1.1.2.2" mathsize="90%">%</mo><mo id="S5.T2.7.3.1.m1.1.2.3" maxsize="90%" minsize="90%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T2.7.3.1.m1.1c">\mathbf{AP_{50}(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.7.3.1.m1.1d">bold_AP start_POSTSUBSCRIPT bold_50 end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.8.4.2"><math alttext="\mathbf{AP(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T2.8.4.2.m1.1"><semantics id="S5.T2.8.4.2.m1.1a"><mrow id="S5.T2.8.4.2.m1.1b"><mi id="S5.T2.8.4.2.m1.1.1" mathsize="90%">𝐀𝐏</mi><mrow id="S5.T2.8.4.2.m1.1.2"><mo id="S5.T2.8.4.2.m1.1.2.1" maxsize="90%" minsize="90%">(</mo><mo id="S5.T2.8.4.2.m1.1.2.2" mathsize="90%">%</mo><mo id="S5.T2.8.4.2.m1.1.2.3" maxsize="90%" minsize="90%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T2.8.4.2.m1.1c">\mathbf{AP(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.8.4.2.m1.1d">bold_AP ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.9.5.3"><math alttext="\mathbf{AP_{50}(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T2.9.5.3.m1.1"><semantics id="S5.T2.9.5.3.m1.1a"><mrow id="S5.T2.9.5.3.m1.1b"><msub id="S5.T2.9.5.3.m1.1.1"><mi id="S5.T2.9.5.3.m1.1.1.2" mathsize="90%">𝐀𝐏</mi><mn id="S5.T2.9.5.3.m1.1.1.3" mathsize="90%">𝟓𝟎</mn></msub><mrow id="S5.T2.9.5.3.m1.1.2"><mo id="S5.T2.9.5.3.m1.1.2.1" maxsize="90%" minsize="90%">(</mo><mo id="S5.T2.9.5.3.m1.1.2.2" mathsize="90%">%</mo><mo id="S5.T2.9.5.3.m1.1.2.3" maxsize="90%" minsize="90%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T2.9.5.3.m1.1c">\mathbf{AP_{50}(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.9.5.3.m1.1d">bold_AP start_POSTSUBSCRIPT bold_50 end_POSTSUBSCRIPT ( % )</annotation></semantics></math></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.10.6.4"><math alttext="\mathbf{AP(\%)}" class="ltx_math_unparsed" display="inline" id="S5.T2.10.6.4.m1.1"><semantics id="S5.T2.10.6.4.m1.1a"><mrow id="S5.T2.10.6.4.m1.1b"><mi id="S5.T2.10.6.4.m1.1.1" mathsize="90%">𝐀𝐏</mi><mrow id="S5.T2.10.6.4.m1.1.2"><mo id="S5.T2.10.6.4.m1.1.2.1" maxsize="90%" minsize="90%">(</mo><mo id="S5.T2.10.6.4.m1.1.2.2" mathsize="90%">%</mo><mo id="S5.T2.10.6.4.m1.1.2.3" maxsize="90%" minsize="90%">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S5.T2.10.6.4.m1.1c">\mathbf{AP(\%)}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.10.6.4.m1.1d">bold_AP ( % )</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.14.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.14.10.5" rowspan="4"><span class="ltx_text" id="S5.T2.14.10.5.1" style="font-size:90%;color:#616EC4;">Strategy 1</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.14.10.6"><span class="ltx_text ltx_font_bold" id="S5.T2.14.10.6.1" style="font-size:90%;">R250_S0</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.11.7.1">
<span class="ltx_text" id="S5.T2.11.7.1.1" style="font-size:90%;">51.68 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.11.7.1.m1.1"><semantics id="S5.T2.11.7.1.m1.1a"><mo id="S5.T2.11.7.1.m1.1.1" mathsize="90%" xref="S5.T2.11.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.11.7.1.m1.1b"><csymbol cd="latexml" id="S5.T2.11.7.1.m1.1.1.cmml" xref="S5.T2.11.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.11.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.11.7.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.11.7.1.2" style="font-size:90%;"> 1.11</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.12.8.2">
<span class="ltx_text" id="S5.T2.12.8.2.1" style="font-size:90%;">23.92 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.12.8.2.m1.1"><semantics id="S5.T2.12.8.2.m1.1a"><mo id="S5.T2.12.8.2.m1.1.1" mathsize="90%" xref="S5.T2.12.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.12.8.2.m1.1b"><csymbol cd="latexml" id="S5.T2.12.8.2.m1.1.1.cmml" xref="S5.T2.12.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.12.8.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.12.8.2.2" style="font-size:90%;"> 0.31</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.13.9.3">
<span class="ltx_text" id="S5.T2.13.9.3.1" style="font-size:90%;">35.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.13.9.3.m1.1"><semantics id="S5.T2.13.9.3.m1.1a"><mo id="S5.T2.13.9.3.m1.1.1" mathsize="90%" xref="S5.T2.13.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.13.9.3.m1.1b"><csymbol cd="latexml" id="S5.T2.13.9.3.m1.1.1.cmml" xref="S5.T2.13.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.9.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.13.9.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.13.9.3.2" style="font-size:90%;"> 2.57</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.14.10.4">
<span class="ltx_text" id="S5.T2.14.10.4.1" style="font-size:90%;">16.41 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.14.10.4.m1.1"><semantics id="S5.T2.14.10.4.m1.1a"><mo id="S5.T2.14.10.4.m1.1.1" mathsize="90%" xref="S5.T2.14.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.14.10.4.m1.1b"><csymbol cd="latexml" id="S5.T2.14.10.4.m1.1.1.cmml" xref="S5.T2.14.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.10.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.14.10.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.14.10.4.2" style="font-size:90%;"> 1.3</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.18.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.18.14.5"><span class="ltx_text ltx_font_bold" id="S5.T2.18.14.5.1" style="font-size:90%;">R500_S0</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.15.11.1">
<span class="ltx_text" id="S5.T2.15.11.1.1" style="font-size:90%;">56.05 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.15.11.1.m1.1"><semantics id="S5.T2.15.11.1.m1.1a"><mo id="S5.T2.15.11.1.m1.1.1" mathsize="90%" xref="S5.T2.15.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.15.11.1.m1.1b"><csymbol cd="latexml" id="S5.T2.15.11.1.m1.1.1.cmml" xref="S5.T2.15.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.11.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.15.11.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.15.11.1.2" style="font-size:90%;"> 0.82</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.16.12.2">
<span class="ltx_text" id="S5.T2.16.12.2.1" style="font-size:90%;">27.49 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.16.12.2.m1.1"><semantics id="S5.T2.16.12.2.m1.1a"><mo id="S5.T2.16.12.2.m1.1.1" mathsize="90%" xref="S5.T2.16.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.16.12.2.m1.1b"><csymbol cd="latexml" id="S5.T2.16.12.2.m1.1.1.cmml" xref="S5.T2.16.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.12.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.16.12.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.16.12.2.2" style="font-size:90%;"> 0.95</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.17.13.3">
<span class="ltx_text" id="S5.T2.17.13.3.1" style="font-size:90%;">26.69 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.17.13.3.m1.1"><semantics id="S5.T2.17.13.3.m1.1a"><mo id="S5.T2.17.13.3.m1.1.1" mathsize="90%" xref="S5.T2.17.13.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.17.13.3.m1.1b"><csymbol cd="latexml" id="S5.T2.17.13.3.m1.1.1.cmml" xref="S5.T2.17.13.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.13.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.17.13.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.17.13.3.2" style="font-size:90%;"> 5.25</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.18.14.4">
<span class="ltx_text" id="S5.T2.18.14.4.1" style="font-size:90%;">13.2 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.18.14.4.m1.1"><semantics id="S5.T2.18.14.4.m1.1a"><mo id="S5.T2.18.14.4.m1.1.1" mathsize="90%" xref="S5.T2.18.14.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.18.14.4.m1.1b"><csymbol cd="latexml" id="S5.T2.18.14.4.m1.1.1.cmml" xref="S5.T2.18.14.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.18.14.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.18.14.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.18.14.4.2" style="font-size:90%;"> 1.91</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.22.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.22.18.5"><span class="ltx_text ltx_font_bold" id="S5.T2.22.18.5.1" style="font-size:90%;">R750_S0</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.19.15.1">
<span class="ltx_text" id="S5.T2.19.15.1.1" style="font-size:90%;">55.51 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.19.15.1.m1.1"><semantics id="S5.T2.19.15.1.m1.1a"><mo id="S5.T2.19.15.1.m1.1.1" mathsize="90%" xref="S5.T2.19.15.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.19.15.1.m1.1b"><csymbol cd="latexml" id="S5.T2.19.15.1.m1.1.1.cmml" xref="S5.T2.19.15.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.19.15.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.19.15.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.19.15.1.2" style="font-size:90%;"> 1.22</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.20.16.2">
<span class="ltx_text" id="S5.T2.20.16.2.1" style="font-size:90%;">27.29 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.20.16.2.m1.1"><semantics id="S5.T2.20.16.2.m1.1a"><mo id="S5.T2.20.16.2.m1.1.1" mathsize="90%" xref="S5.T2.20.16.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.20.16.2.m1.1b"><csymbol cd="latexml" id="S5.T2.20.16.2.m1.1.1.cmml" xref="S5.T2.20.16.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.20.16.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.20.16.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.20.16.2.2" style="font-size:90%;"> 1.13</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.21.17.3">
<span class="ltx_text" id="S5.T2.21.17.3.1" style="font-size:90%;">25.37 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.21.17.3.m1.1"><semantics id="S5.T2.21.17.3.m1.1a"><mo id="S5.T2.21.17.3.m1.1.1" mathsize="90%" xref="S5.T2.21.17.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.21.17.3.m1.1b"><csymbol cd="latexml" id="S5.T2.21.17.3.m1.1.1.cmml" xref="S5.T2.21.17.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.21.17.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.21.17.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.21.17.3.2" style="font-size:90%;"> 4.32</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.22.18.4">
<span class="ltx_text" id="S5.T2.22.18.4.1" style="font-size:90%;">12.99 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.22.18.4.m1.1"><semantics id="S5.T2.22.18.4.m1.1a"><mo id="S5.T2.22.18.4.m1.1.1" mathsize="90%" xref="S5.T2.22.18.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.22.18.4.m1.1b"><csymbol cd="latexml" id="S5.T2.22.18.4.m1.1.1.cmml" xref="S5.T2.22.18.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.22.18.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.22.18.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.22.18.4.2" style="font-size:90%;"> 2.34</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.26.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.26.22.5"><span class="ltx_text ltx_font_bold" id="S5.T2.26.22.5.1" style="font-size:90%;">R1000_S0</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.23.19.1"><span class="ltx_text ltx_font_bold" id="S5.T2.23.19.1.1" style="font-size:90%;">58.13 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.23.19.1.1.m1.1"><semantics id="S5.T2.23.19.1.1.m1.1a"><mo id="S5.T2.23.19.1.1.m1.1.1" xref="S5.T2.23.19.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.23.19.1.1.m1.1b"><csymbol cd="latexml" id="S5.T2.23.19.1.1.m1.1.1.cmml" xref="S5.T2.23.19.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.23.19.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.23.19.1.1.m1.1d">±</annotation></semantics></math> 1.05</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.24.20.2"><span class="ltx_text ltx_font_bold" id="S5.T2.24.20.2.1" style="font-size:90%;">29.17 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.24.20.2.1.m1.1"><semantics id="S5.T2.24.20.2.1.m1.1a"><mo id="S5.T2.24.20.2.1.m1.1.1" xref="S5.T2.24.20.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.24.20.2.1.m1.1b"><csymbol cd="latexml" id="S5.T2.24.20.2.1.m1.1.1.cmml" xref="S5.T2.24.20.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.24.20.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.24.20.2.1.m1.1d">±</annotation></semantics></math> 0.79</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.25.21.3">
<span class="ltx_text" id="S5.T2.25.21.3.1" style="font-size:90%;">26.18 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.25.21.3.m1.1"><semantics id="S5.T2.25.21.3.m1.1a"><mo id="S5.T2.25.21.3.m1.1.1" mathsize="90%" xref="S5.T2.25.21.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.25.21.3.m1.1b"><csymbol cd="latexml" id="S5.T2.25.21.3.m1.1.1.cmml" xref="S5.T2.25.21.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.25.21.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.25.21.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.25.21.3.2" style="font-size:90%;"> 2.61</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.26.22.4">
<span class="ltx_text" id="S5.T2.26.22.4.1" style="font-size:90%;">13.71 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.26.22.4.m1.1"><semantics id="S5.T2.26.22.4.m1.1a"><mo id="S5.T2.26.22.4.m1.1.1" mathsize="90%" xref="S5.T2.26.22.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.26.22.4.m1.1b"><csymbol cd="latexml" id="S5.T2.26.22.4.m1.1.1.cmml" xref="S5.T2.26.22.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.26.22.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.26.22.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.26.22.4.2" style="font-size:90%;"> 1.6</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.30.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.30.26.5" rowspan="3"><span class="ltx_text" id="S5.T2.30.26.5.1" style="font-size:90%;color:#80FF00;">Strategy 2</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.30.26.6"><span class="ltx_text ltx_font_bold" id="S5.T2.30.26.6.1" style="font-size:90%;">R750_S250</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.27.23.1"><span class="ltx_text ltx_font_italic" id="S5.T2.27.23.1.1" style="font-size:90%;">56.8 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.27.23.1.1.m1.1"><semantics id="S5.T2.27.23.1.1.m1.1a"><mo id="S5.T2.27.23.1.1.m1.1.1" xref="S5.T2.27.23.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.27.23.1.1.m1.1b"><csymbol cd="latexml" id="S5.T2.27.23.1.1.m1.1.1.cmml" xref="S5.T2.27.23.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.27.23.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.27.23.1.1.m1.1d">±</annotation></semantics></math> 1.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.28.24.2"><span class="ltx_text ltx_font_italic" id="S5.T2.28.24.2.1" style="font-size:90%;">28.5 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.28.24.2.1.m1.1"><semantics id="S5.T2.28.24.2.1.m1.1a"><mo id="S5.T2.28.24.2.1.m1.1.1" xref="S5.T2.28.24.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.28.24.2.1.m1.1b"><csymbol cd="latexml" id="S5.T2.28.24.2.1.m1.1.1.cmml" xref="S5.T2.28.24.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.28.24.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.28.24.2.1.m1.1d">±</annotation></semantics></math> 1.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.29.25.3"><span class="ltx_text ltx_font_italic" id="S5.T2.29.25.3.1" style="font-size:90%;">41.54 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.29.25.3.1.m1.1"><semantics id="S5.T2.29.25.3.1.m1.1a"><mo id="S5.T2.29.25.3.1.m1.1.1" xref="S5.T2.29.25.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.29.25.3.1.m1.1b"><csymbol cd="latexml" id="S5.T2.29.25.3.1.m1.1.1.cmml" xref="S5.T2.29.25.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.29.25.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.29.25.3.1.m1.1d">±</annotation></semantics></math> 5.24</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.30.26.4"><span class="ltx_text ltx_font_italic" id="S5.T2.30.26.4.1" style="font-size:90%;">22.28 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.30.26.4.1.m1.1"><semantics id="S5.T2.30.26.4.1.m1.1a"><mo id="S5.T2.30.26.4.1.m1.1.1" xref="S5.T2.30.26.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.30.26.4.1.m1.1b"><csymbol cd="latexml" id="S5.T2.30.26.4.1.m1.1.1.cmml" xref="S5.T2.30.26.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.30.26.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.30.26.4.1.m1.1d">±</annotation></semantics></math> 3.08</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.34.30">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.34.30.5"><span class="ltx_text ltx_font_bold" id="S5.T2.34.30.5.1" style="font-size:90%;">R500_S500</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.31.27.1">
<span class="ltx_text" id="S5.T2.31.27.1.1" style="font-size:90%;">56.74 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.31.27.1.m1.1"><semantics id="S5.T2.31.27.1.m1.1a"><mo id="S5.T2.31.27.1.m1.1.1" mathsize="90%" xref="S5.T2.31.27.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.31.27.1.m1.1b"><csymbol cd="latexml" id="S5.T2.31.27.1.m1.1.1.cmml" xref="S5.T2.31.27.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.31.27.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.31.27.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.31.27.1.2" style="font-size:90%;"> 1.38</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.32.28.2">
<span class="ltx_text" id="S5.T2.32.28.2.1" style="font-size:90%;">27.93 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.32.28.2.m1.1"><semantics id="S5.T2.32.28.2.m1.1a"><mo id="S5.T2.32.28.2.m1.1.1" mathsize="90%" xref="S5.T2.32.28.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.32.28.2.m1.1b"><csymbol cd="latexml" id="S5.T2.32.28.2.m1.1.1.cmml" xref="S5.T2.32.28.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.32.28.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.32.28.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.32.28.2.2" style="font-size:90%;"> 0.8</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.33.29.3"><span class="ltx_text ltx_font_bold" id="S5.T2.33.29.3.1" style="font-size:90%;">42.3 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.33.29.3.1.m1.1"><semantics id="S5.T2.33.29.3.1.m1.1a"><mo id="S5.T2.33.29.3.1.m1.1.1" xref="S5.T2.33.29.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.33.29.3.1.m1.1b"><csymbol cd="latexml" id="S5.T2.33.29.3.1.m1.1.1.cmml" xref="S5.T2.33.29.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.33.29.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.33.29.3.1.m1.1d">±</annotation></semantics></math> 3.62</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.34.30.4"><span class="ltx_text ltx_font_bold" id="S5.T2.34.30.4.1" style="font-size:90%;">22.63 <math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.34.30.4.1.m1.1"><semantics id="S5.T2.34.30.4.1.m1.1a"><mo id="S5.T2.34.30.4.1.m1.1.1" xref="S5.T2.34.30.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.34.30.4.1.m1.1b"><csymbol cd="latexml" id="S5.T2.34.30.4.1.m1.1.1.cmml" xref="S5.T2.34.30.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.34.30.4.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.34.30.4.1.m1.1d">±</annotation></semantics></math> 1.49</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.38.34">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.38.34.5"><span class="ltx_text ltx_font_bold" id="S5.T2.38.34.5.1" style="font-size:90%;">R250_S750</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.35.31.1">
<span class="ltx_text" id="S5.T2.35.31.1.1" style="font-size:90%;">52.37 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.35.31.1.m1.1"><semantics id="S5.T2.35.31.1.m1.1a"><mo id="S5.T2.35.31.1.m1.1.1" mathsize="90%" xref="S5.T2.35.31.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.35.31.1.m1.1b"><csymbol cd="latexml" id="S5.T2.35.31.1.m1.1.1.cmml" xref="S5.T2.35.31.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.35.31.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.35.31.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.35.31.1.2" style="font-size:90%;"> 1.47</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.36.32.2">
<span class="ltx_text" id="S5.T2.36.32.2.1" style="font-size:90%;">24.54 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.36.32.2.m1.1"><semantics id="S5.T2.36.32.2.m1.1a"><mo id="S5.T2.36.32.2.m1.1.1" mathsize="90%" xref="S5.T2.36.32.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.36.32.2.m1.1b"><csymbol cd="latexml" id="S5.T2.36.32.2.m1.1.1.cmml" xref="S5.T2.36.32.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.36.32.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.36.32.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.36.32.2.2" style="font-size:90%;"> 1.21</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.37.33.3">
<span class="ltx_text" id="S5.T2.37.33.3.1" style="font-size:90%;">39.77 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.37.33.3.m1.1"><semantics id="S5.T2.37.33.3.m1.1a"><mo id="S5.T2.37.33.3.m1.1.1" mathsize="90%" xref="S5.T2.37.33.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.37.33.3.m1.1b"><csymbol cd="latexml" id="S5.T2.37.33.3.m1.1.1.cmml" xref="S5.T2.37.33.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.37.33.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.37.33.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.37.33.3.2" style="font-size:90%;"> 3.92</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.38.34.4">
<span class="ltx_text" id="S5.T2.38.34.4.1" style="font-size:90%;">19.55 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.38.34.4.m1.1"><semantics id="S5.T2.38.34.4.m1.1a"><mo id="S5.T2.38.34.4.m1.1.1" mathsize="90%" xref="S5.T2.38.34.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.38.34.4.m1.1b"><csymbol cd="latexml" id="S5.T2.38.34.4.m1.1.1.cmml" xref="S5.T2.38.34.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.38.34.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.38.34.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.38.34.4.2" style="font-size:90%;"> 2.35</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.42.38">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.42.38.5" rowspan="4"><span class="ltx_text" id="S5.T2.42.38.5.1" style="font-size:90%;color:#FF0000;">Strategy 3</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.42.38.6"><span class="ltx_text ltx_font_bold" id="S5.T2.42.38.6.1" style="font-size:90%;">R0_S1000</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.39.35.1">
<span class="ltx_text" id="S5.T2.39.35.1.1" style="font-size:90%;">15.14 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.39.35.1.m1.1"><semantics id="S5.T2.39.35.1.m1.1a"><mo id="S5.T2.39.35.1.m1.1.1" mathsize="90%" xref="S5.T2.39.35.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.39.35.1.m1.1b"><csymbol cd="latexml" id="S5.T2.39.35.1.m1.1.1.cmml" xref="S5.T2.39.35.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.39.35.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.39.35.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.39.35.1.2" style="font-size:90%;"> 1.16</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.40.36.2">
<span class="ltx_text" id="S5.T2.40.36.2.1" style="font-size:90%;">5.65 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.40.36.2.m1.1"><semantics id="S5.T2.40.36.2.m1.1a"><mo id="S5.T2.40.36.2.m1.1.1" mathsize="90%" xref="S5.T2.40.36.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.40.36.2.m1.1b"><csymbol cd="latexml" id="S5.T2.40.36.2.m1.1.1.cmml" xref="S5.T2.40.36.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.40.36.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.40.36.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.40.36.2.2" style="font-size:90%;"> 0.61</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.41.37.3">
<span class="ltx_text" id="S5.T2.41.37.3.1" style="font-size:90%;">24.38 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.41.37.3.m1.1"><semantics id="S5.T2.41.37.3.m1.1a"><mo id="S5.T2.41.37.3.m1.1.1" mathsize="90%" xref="S5.T2.41.37.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.41.37.3.m1.1b"><csymbol cd="latexml" id="S5.T2.41.37.3.m1.1.1.cmml" xref="S5.T2.41.37.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.41.37.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.41.37.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.41.37.3.2" style="font-size:90%;"> 2.09</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.42.38.4">
<span class="ltx_text" id="S5.T2.42.38.4.1" style="font-size:90%;">8.87 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.42.38.4.m1.1"><semantics id="S5.T2.42.38.4.m1.1a"><mo id="S5.T2.42.38.4.m1.1.1" mathsize="90%" xref="S5.T2.42.38.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.42.38.4.m1.1b"><csymbol cd="latexml" id="S5.T2.42.38.4.m1.1.1.cmml" xref="S5.T2.42.38.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.42.38.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.42.38.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.42.38.4.2" style="font-size:90%;"> 0.52</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.46.42">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.46.42.5"><span class="ltx_text ltx_font_bold" id="S5.T2.46.42.5.1" style="font-size:90%;">R0_S750</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.43.39.1">
<span class="ltx_text" id="S5.T2.43.39.1.1" style="font-size:90%;">14.43 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.43.39.1.m1.1"><semantics id="S5.T2.43.39.1.m1.1a"><mo id="S5.T2.43.39.1.m1.1.1" mathsize="90%" xref="S5.T2.43.39.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.43.39.1.m1.1b"><csymbol cd="latexml" id="S5.T2.43.39.1.m1.1.1.cmml" xref="S5.T2.43.39.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.43.39.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.43.39.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.43.39.1.2" style="font-size:90%;"> 1.61</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.44.40.2">
<span class="ltx_text" id="S5.T2.44.40.2.1" style="font-size:90%;">5.5 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.44.40.2.m1.1"><semantics id="S5.T2.44.40.2.m1.1a"><mo id="S5.T2.44.40.2.m1.1.1" mathsize="90%" xref="S5.T2.44.40.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.44.40.2.m1.1b"><csymbol cd="latexml" id="S5.T2.44.40.2.m1.1.1.cmml" xref="S5.T2.44.40.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.44.40.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.44.40.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.44.40.2.2" style="font-size:90%;"> 0.38</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.45.41.3">
<span class="ltx_text" id="S5.T2.45.41.3.1" style="font-size:90%;">20.01 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.45.41.3.m1.1"><semantics id="S5.T2.45.41.3.m1.1a"><mo id="S5.T2.45.41.3.m1.1.1" mathsize="90%" xref="S5.T2.45.41.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.45.41.3.m1.1b"><csymbol cd="latexml" id="S5.T2.45.41.3.m1.1.1.cmml" xref="S5.T2.45.41.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.45.41.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.45.41.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.45.41.3.2" style="font-size:90%;"> 5.32</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.46.42.4">
<span class="ltx_text" id="S5.T2.46.42.4.1" style="font-size:90%;">7.7 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.46.42.4.m1.1"><semantics id="S5.T2.46.42.4.m1.1a"><mo id="S5.T2.46.42.4.m1.1.1" mathsize="90%" xref="S5.T2.46.42.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.46.42.4.m1.1b"><csymbol cd="latexml" id="S5.T2.46.42.4.m1.1.1.cmml" xref="S5.T2.46.42.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.46.42.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.46.42.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.46.42.4.2" style="font-size:90%;"> 2.2</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.50.46">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.50.46.5"><span class="ltx_text ltx_font_bold" id="S5.T2.50.46.5.1" style="font-size:90%;">R0_S500</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.47.43.1">
<span class="ltx_text" id="S5.T2.47.43.1.1" style="font-size:90%;">12.87 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.47.43.1.m1.1"><semantics id="S5.T2.47.43.1.m1.1a"><mo id="S5.T2.47.43.1.m1.1.1" mathsize="90%" xref="S5.T2.47.43.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.47.43.1.m1.1b"><csymbol cd="latexml" id="S5.T2.47.43.1.m1.1.1.cmml" xref="S5.T2.47.43.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.47.43.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.47.43.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.47.43.1.2" style="font-size:90%;"> 1.57</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.48.44.2">
<span class="ltx_text" id="S5.T2.48.44.2.1" style="font-size:90%;">4.75 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.48.44.2.m1.1"><semantics id="S5.T2.48.44.2.m1.1a"><mo id="S5.T2.48.44.2.m1.1.1" mathsize="90%" xref="S5.T2.48.44.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.48.44.2.m1.1b"><csymbol cd="latexml" id="S5.T2.48.44.2.m1.1.1.cmml" xref="S5.T2.48.44.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.48.44.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.48.44.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.48.44.2.2" style="font-size:90%;"> 0.55</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T2.49.45.3">
<span class="ltx_text" id="S5.T2.49.45.3.1" style="font-size:90%;">22.18 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.49.45.3.m1.1"><semantics id="S5.T2.49.45.3.m1.1a"><mo id="S5.T2.49.45.3.m1.1.1" mathsize="90%" xref="S5.T2.49.45.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.49.45.3.m1.1b"><csymbol cd="latexml" id="S5.T2.49.45.3.m1.1.1.cmml" xref="S5.T2.49.45.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.49.45.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.49.45.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.49.45.3.2" style="font-size:90%;"> 7.03</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.50.46.4">
<span class="ltx_text" id="S5.T2.50.46.4.1" style="font-size:90%;">8.03 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.50.46.4.m1.1"><semantics id="S5.T2.50.46.4.m1.1a"><mo id="S5.T2.50.46.4.m1.1.1" mathsize="90%" xref="S5.T2.50.46.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.50.46.4.m1.1b"><csymbol cd="latexml" id="S5.T2.50.46.4.m1.1.1.cmml" xref="S5.T2.50.46.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.50.46.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.50.46.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.50.46.4.2" style="font-size:90%;"> 2.22</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.54.50">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.54.50.5"><span class="ltx_text ltx_font_bold" id="S5.T2.54.50.5.1" style="font-size:90%;">R0_S250</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.51.47.1">
<span class="ltx_text" id="S5.T2.51.47.1.1" style="font-size:90%;">10.56 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.51.47.1.m1.1"><semantics id="S5.T2.51.47.1.m1.1a"><mo id="S5.T2.51.47.1.m1.1.1" mathsize="90%" xref="S5.T2.51.47.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.51.47.1.m1.1b"><csymbol cd="latexml" id="S5.T2.51.47.1.m1.1.1.cmml" xref="S5.T2.51.47.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.51.47.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.51.47.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.51.47.1.2" style="font-size:90%;"> 1.96</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.52.48.2">
<span class="ltx_text" id="S5.T2.52.48.2.1" style="font-size:90%;">3.77 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.52.48.2.m1.1"><semantics id="S5.T2.52.48.2.m1.1a"><mo id="S5.T2.52.48.2.m1.1.1" mathsize="90%" xref="S5.T2.52.48.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.52.48.2.m1.1b"><csymbol cd="latexml" id="S5.T2.52.48.2.m1.1.1.cmml" xref="S5.T2.52.48.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.52.48.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.52.48.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.52.48.2.2" style="font-size:90%;"> 0.72</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.53.49.3">
<span class="ltx_text" id="S5.T2.53.49.3.1" style="font-size:90%;">21.99 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.53.49.3.m1.1"><semantics id="S5.T2.53.49.3.m1.1a"><mo id="S5.T2.53.49.3.m1.1.1" mathsize="90%" xref="S5.T2.53.49.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.53.49.3.m1.1b"><csymbol cd="latexml" id="S5.T2.53.49.3.m1.1.1.cmml" xref="S5.T2.53.49.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.53.49.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.53.49.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.53.49.3.2" style="font-size:90%;"> 2.45</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T2.54.50.4">
<span class="ltx_text" id="S5.T2.54.50.4.1" style="font-size:90%;">7.94 </span><math alttext="\pm" class="ltx_Math" display="inline" id="S5.T2.54.50.4.m1.1"><semantics id="S5.T2.54.50.4.m1.1a"><mo id="S5.T2.54.50.4.m1.1.1" mathsize="90%" xref="S5.T2.54.50.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S5.T2.54.50.4.m1.1b"><csymbol cd="latexml" id="S5.T2.54.50.4.m1.1.1.cmml" xref="S5.T2.54.50.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.54.50.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S5.T2.54.50.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S5.T2.54.50.4.2" style="font-size:90%;"> 0.72</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Findings.</span> Table <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.T2" title="Table 2 ‣ 5.3 Comparison of various training combinations ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">2</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#S5.F5" title="Figure 5 ‣ 5.2 Comparison of synthetic data generation methods ‣ 5 ML experiments ‣ Synthetic imagery for fuzzy object detection: A comparative study"><span class="ltx_text ltx_ref_tag">5</span></a> present the detection performance of different combinations of training set on the two test sets. The findings of the these experiments are as follows:</p>
<ul class="ltx_itemize" id="S5.I2">
<li class="ltx_item" id="S5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i1.p1">
<p class="ltx_p" id="S5.I2.i1.p1.1">In Strategy 1, we observe that with more real data, the model performance is improved under FiIW setup, and diminished in the RealRareFire setup. This indicates that increasing the number of real data in a purely real training set harms model generalization to real rare fire cases. We also observe that all models in this strategy perform better under FiIW setting, for example, R1000_S0 has <math alttext="\sim" class="ltx_Math" display="inline" id="S5.I2.i1.p1.1.m1.1"><semantics id="S5.I2.i1.p1.1.m1.1a"><mo id="S5.I2.i1.p1.1.m1.1.1" xref="S5.I2.i1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.I2.i1.p1.1.m1.1.1.cmml" xref="S5.I2.i1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i1.p1.1.m1.1d">∼</annotation></semantics></math>15.5% lower AP under RealRareFire setting. This indicates that the models are not able to generalize to real rare fire cases in terms of color and shape, and are over-fitted to common fire cases.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i2.p1">
<p class="ltx_p" id="S5.I2.i2.p1.1">In Strategy 2, R750_S250 and R500_S500 show approximately similar performance while both having higher performance than R250_S750 under both test sets. Nevertheless, R500_S500 having the best performance overall under RealRareFire test set, and, unlike Strategy 1, adding more real data to a mixed real-synthetic training set does not harm the performance on RealRareFire, which is another benefit of using both types of data. Furthermore, we observe a smaller gap in range of <math alttext="\sim" class="ltx_Math" display="inline" id="S5.I2.i2.p1.1.m1.1"><semantics id="S5.I2.i2.p1.1.m1.1a"><mo id="S5.I2.i2.p1.1.m1.1.1" xref="S5.I2.i2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.1.m1.1b"><csymbol cd="latexml" id="S5.I2.i2.p1.1.m1.1.1.cmml" xref="S5.I2.i2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i2.p1.1.m1.1d">∼</annotation></semantics></math>[5,6]% between the AP score under both test sets.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i3.p1">
<p class="ltx_p" id="S5.I2.i3.p1.1">In Strategy 3, we see that with more synthetic data, the model performance increases under FiIW setup; while, having no substantial improvement under RealRareFire setup. Unlike Strategy 1, all four models in Strategy 3 do not show substantial performance improvements under RealRareFire setting. For instance, R0_S1000 has only <math alttext="\sim" class="ltx_Math" display="inline" id="S5.I2.i3.p1.1.m1.1"><semantics id="S5.I2.i3.p1.1.m1.1a"><mo id="S5.I2.i3.p1.1.m1.1.1" xref="S5.I2.i3.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.I2.i3.p1.1.m1.1b"><csymbol cd="latexml" id="S5.I2.i3.p1.1.m1.1.1.cmml" xref="S5.I2.i3.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i3.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.I2.i3.p1.1.m1.1d">∼</annotation></semantics></math>3% higher AP under RealRareFire setting. This indicates that increasing the number of fully synthetic data in a purely synthetic training set does not have substantial effect on model generalization to real rare fire cases.</p>
</div>
</li>
<li class="ltx_item" id="S5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I2.i4.p1">
<p class="ltx_p" id="S5.I2.i4.p1.1">Comparing all strategies, first, we observe that all three combinations of Strategy 2 outperform the Strategies 1 and 3 under RealRareFire test set. Secondly, although R1000_S0 from Strategy 1 performs the best under FiIW test set, Strategy 2 is on a par with that, where R750_S250 and R500_S500 are the <span class="ltx_text ltx_font_bold" id="S5.I2.i4.p1.1.1">second</span> and the <span class="ltx_text ltx_font_bold" id="S5.I2.i4.p1.1.2">third best</span> overall performers respectively, while potentially having <span class="ltx_text ltx_font_bold" id="S5.I2.i4.p1.1.3">lower cost</span> in terms of collection and annotation.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS3.p2.2">It is worth noting that, similar to Borkman et al. <cite class="ltx_cite ltx_citemacro_cite">Borkman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib13" title="">2021</a>)</cite>, since all experiments are conducted with one model and a limited training size (1000 images or smaller). Therefore, the experimental results might not generalize to other models or training dataset sizes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The contribution of this research is to overcome the shortcomings of the present fuzzy object computer vision training methods by proposing a synthetic data generation and annotation method and to disclose its comparative performance and shortcomings. Our proposed method showed promising results for the detection of fire in both common and rare fire test sets. Moreover, we illustrated that a mixed model trained on a fully synthetic and real training sets can outperform a model only trained on real data and better generalize in detection of real fire instances with various visual features. We clearly illustrated that, randomization and diversity of the synthetic data is necessary for the method to work, while only generating realistic imagery alone is not effective for generalization. The limitations of this research was the use of limited number of synthetic images that should be increased in the future research.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Future research</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The lack of large datasets is only one of the reasons the current CNN based method performs poorly on fuzzy object detection. Most of the popular detection models, such as YOLO <cite class="ltx_cite ltx_citemacro_cite">Redmon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib18" title="">2016</a>)</cite>, ViT <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib38" title="">2020</a>)</cite>, focus on detecting static objects like cats, dogs, and people, which have fixed shape and the shape will not change over time. However, fuzzy objects have variable shapes and move over time, which are different from static objects and these dynamic characteristics cannot be described in a single image. Therefore, it is harder for the current CNN based method to capture the features of these types of objects. However, a model trained directly on video may be capable of detecting temporal patterns of shape shift and movement behaviors of fuzzy objects in a video, which are not available in an static image. Therefore, training on video may be a better choice than training on image for fuzzy objects. In literature, video classification methods have been widely used to analyze human actions <cite class="ltx_cite ltx_citemacro_cite">Carreira and Zisserman (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib39" title="">2017</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Carreira et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib40" title="">2018</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Bertasius et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.01124v1#bib.bib41" title="">2021</a>)</cite>. However, to our knowledge, there is no research about training video classification models for fuzzy objects and the use of synthetic videos for this purpose.
Moreover, future research should investigate the use of our proposed method for other fuzzy objects such as smoke. This should be studied in relation to the sensitivity, confidence level, and accuracy of a computer vision model trained on real smoke imagery.
The future research may also study the use of custom synthetic data generated in environmental digital twins for custom retraining of the generic fuzzy object detection models for improved performance.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. [2012]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in neural information processing systems</em>, 25, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">2009 IEEE conference on computer vision and pattern recognition</em>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2016]</span>
<span class="ltx_bibblock">
Q. Zhang, J. Xu, L. Xu, and H. Guo.

</span>
<span class="ltx_bibblock">Deep convolutional neural networks for forest fire detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2016 International Forum on Management, Education and Information Technology Application</em>. Atlantis Press, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al. [2018a]</span>
<span class="ltx_bibblock">
K. Muhammad, J. Ahmad, and S. W. Baik.

</span>
<span class="ltx_bibblock">Early fire detection using convolutional neural networks during surveillance for effective disaster management.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Neurocomputing</em>, 288:30–42, 2018a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al. [2018b]</span>
<span class="ltx_bibblock">
K. Muhammad, J. Ahmad, Z. Lv, P. Bellavista, P. Yang, and S. W. Baik.

</span>
<span class="ltx_bibblock">Efficient deep cnn-based fire detection and localization in video surveillance applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Systems, Man, and Cybernetics: Systems</em>, (99):1–16, 2018b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al. [2018c]</span>
<span class="ltx_bibblock">
K. Muhammad, J. Ahmad, I. Mehmood, S. Rho, and S. W. Baik.

</span>
<span class="ltx_bibblock">Convolutional neural networks based fire detection in surveillance videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Access</em>, 6:18174–18183, 2018c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al. [2019]</span>
<span class="ltx_bibblock">
K. Muhammad, S. Khan, M. Elhoseny, S. H. Ahmed, and S. W. Baik.

</span>
<span class="ltx_bibblock">Efficient fire detection for uncertain surveillance environment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE Transactions on Industrial Informatics</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Foggia et al. [2015]</span>
<span class="ltx_bibblock">
P. Foggia, A. Saggese, and M. Vento.

</span>
<span class="ltx_bibblock">Real-time fire detection for videosurveillance applications using a combination of experts based on color, shape, and motion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">IEEE TRANSACTIONS on circuits and systems for video technology</em>, 25(9):1545–1556, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Venancio et al. [2022]</span>
<span class="ltx_bibblock">
Pedro Vinicius AB de Venancio, Adriano C Lisboa, and Adriano V Barbosa.

</span>
<span class="ltx_bibblock">An automatic fire detection system based on deep convolutional neural networks for low-power, resource-constrained devices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Neural Computing and Applications</em>, 34(18):15349–15368, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman [2015]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobin et al. [2017]</span>
<span class="ltx_bibblock">
Josh Tobin et al.

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from simulation to the real world.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>. IEEE, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinterstoisser et al. [2019]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser et al.

</span>
<span class="ltx_bibblock">An annotation saved is an annotation earned: Using fully synthetic training for object instance detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:1902.09967</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borkman et al. [2021]</span>
<span class="ltx_bibblock">
Steve Borkman, Adam Crespi, Saurav Dhakad, Sujoy Ganguly, Jonathan Hogins, You-Cyuan Jhang, Mohsen Kamalzadeh, Bowen Li, Steven Leal, Pete Parisi, Cesar Romero, Wesley Smith, Alex Thaman, Samuel Warren, and Nupur Yadav.

</span>
<span class="ltx_bibblock">Unity perception: Generate synthetic data for computer vision, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Everingham et al. [2010]</span>
<span class="ltx_bibblock">
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman.

</span>
<span class="ltx_bibblock">The pascal visual object classes (voc) challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">International journal of computer vision</em>, 88:303–338, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2015]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2016]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770–778, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2015]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in neural information processing systems</em>, 28, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Redmon et al. [2016]</span>
<span class="ltx_bibblock">
Joseph Redmon et al.

</span>
<span class="ltx_bibblock">You only look once: Unified, real-time object detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2017]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.

</span>
<span class="ltx_bibblock">Focal loss for dense object detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 2980–2988, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2017]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 2961–2969, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. [2015]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 3431–3440, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2015]</span>
<span class="ltx_bibblock">
Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas.

</span>
<span class="ltx_bibblock">Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 2686–2694, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Georgakis et al. [2017]</span>
<span class="ltx_bibblock">
Georgios Georgakis, Arsalan Mousavian, Alexander C Berg, and Jana Kosecka.

</span>
<span class="ltx_bibblock">Synthesizing training data for object detection in indoor scenes.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1702.07836</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shrivastava et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang, and Russell Webb.

</span>
<span class="ltx_bibblock">Learning from simulated and unsupervised images through adversarial training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 2107–2116, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inoue et al. [2018]</span>
<span class="ltx_bibblock">
Tadanobu Inoue, Subhajit Choudhury, Giovanni De Magistris, and Sakyasingha Dasgupta.

</span>
<span class="ltx_bibblock">Transfer learning from synthetic to real images using variational autoencoders for precise position detection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">2018 25th IEEE International Conference on Image Processing (ICIP)</em>, pages 2725–2729. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Denninger et al. [2019]</span>
<span class="ltx_bibblock">
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.

</span>
<span class="ltx_bibblock">Blenderproc.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:1911.01911</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morrical et al. [2021]</span>
<span class="ltx_bibblock">
Nathan Morrical, Jonathan Tremblay, Yunzhi Lin, Stephen Tyree, Stan Birchfield, Valerio Pascucci, and Ingo Wald.

</span>
<span class="ltx_bibblock">Nvisii: A scriptable tool for photorealistic image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2105.13962</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blender Online Community [2019]</span>
<span class="ltx_bibblock">
Blender Online Community.

</span>
<span class="ltx_bibblock">Blender - a 3d modelling and rendering package, 2019.

</span>
<span class="ltx_bibblock">[Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.blender.org" title="">http://www.blender.org</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grieves and Vickers [2017]</span>
<span class="ltx_bibblock">
Michael Grieves and John Vickers.

</span>
<span class="ltx_bibblock">Digital twin: Mitigating unpredictable, undesirable emergent behavior in complex systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Transdisciplinary perspectives on complex systems: New findings and approaches</em>, pages 85–113, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kritzinger et al. [2018]</span>
<span class="ltx_bibblock">
Werner Kritzinger, Matthias Karner, Georg Traar, Jan Henjes, and Wilfried Sihn.

</span>
<span class="ltx_bibblock">Digital twin in manufacturing: A categorical literature review and classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Ifac-PapersOnline</em>, 51(11):1016–1022, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erol et al. [2020]</span>
<span class="ltx_bibblock">
Tolga Erol, Arif Furkan Mendi, and Dilara Doğan.

</span>
<span class="ltx_bibblock">The digital twin revolution in healthcare.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)</em>, pages 1–7, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1109/ISMSIT50672.2020.9255249</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al. [2021]</span>
<span class="ltx_bibblock">
Gary White, Anna Zink, Lara Codecá, and Siobhán Clarke.

</span>
<span class="ltx_bibblock">A digital twin smart city for citizen feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Cities</em>, 110:103064, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
VOC2011 Annotation Guidelines.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/guidelines.html" title="">http://host.robots.ox.ac.uk/pascal/VOC/voc2012/guidelines.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bochkovskiy et al. [2020]</span>
<span class="ltx_bibblock">
Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">Yolov4: Optimal speed and accuracy of object detection, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. [2021]</span>
<span class="ltx_bibblock">
Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun.

</span>
<span class="ltx_bibblock">Yolox: Exceeding yolo series in 2021, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jocher et al. [2022]</span>
<span class="ltx_bibblock">
Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, NanoCode012, Yonghye Kwon, Kalen Michael, TaoXie, Jiacong Fang, imyhxy, Lorna, Zeng Yifu(Zeng Yifu), Colin Wong, Abhiram V, Diego Montes, Zhiqiang Wang, Cristi Fati, Jebastin Nadar, Laughing, UnglvKitDe, Victor Sonck, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Dhruv Nair, Max Strobel, and Mrinal Jain.

</span>
<span class="ltx_bibblock">ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentation, November 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/7347926" title="">https://zenodo.org/records/7347926</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2020]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira and Zisserman [2017]</span>
<span class="ltx_bibblock">
J. Carreira and A. Zisserman.

</span>
<span class="ltx_bibblock">Quo vadis, action recognition? a new model and the kinetics dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira et al. [2018]</span>
<span class="ltx_bibblock">
J. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman.

</span>
<span class="ltx_bibblock">A short note about kinetics-600.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">CoRR</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bertasius et al. [2021]</span>
<span class="ltx_bibblock">
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.

</span>
<span class="ltx_bibblock">Is space-time attention all you need for video understanding?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">ICML</em>, volume 2, 2021.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 23:11:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
