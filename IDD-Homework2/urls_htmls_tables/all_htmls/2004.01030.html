<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2004.01030] Objects of violence: synthetic data for practical ML in human rights investigations</title><meta property="og:description" content="We introduce a machine learning workflow to search for, identify, and meaningfully triage videos and images of munitions, weapons, and military equipment, even when limited training data exists for the object of intere…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Objects of violence: synthetic data for practical ML in human rights investigations">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Objects of violence: synthetic data for practical ML in human rights investigations">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2004.01030">

<!--Generated on Sat Mar  2 12:01:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Objects of violence: synthetic data for practical ML in human rights investigations</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lachlan Kermode  
<br class="ltx_break">Forensic Architecture
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">lk@forensic-architecture.org</span> 
<br class="ltx_break">Jan Freyberg <span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break">Element AI 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">jan.freyberg@elementai.com</span> 
<br class="ltx_break">Alican Akturk 
<br class="ltx_break">Forensic Architecture 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">aa@forensic-architecture.org</span> 
<br class="ltx_break">Robert Trafford 
<br class="ltx_break">Forensic Architecture
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">rt@forensic-architecture.org</span> 
<br class="ltx_break">Denis Kochetkov 
<br class="ltx_break">Element AI 
<br class="ltx_break"><span id="id5.5.id5" class="ltx_text ltx_font_typewriter">denis.kochetkov@elementai.com</span> 
<br class="ltx_break">Rafael Pardinas 
<br class="ltx_break">Element AI 
<br class="ltx_break"><span id="id6.6.id6" class="ltx_text ltx_font_typewriter">rafael.pardinas@elementai.com</span> 
<br class="ltx_break">Eyal Weizman 
<br class="ltx_break">Forensic Architecture
<br class="ltx_break"><span id="id7.7.id7" class="ltx_text ltx_font_typewriter">ew@forensic-architecture.org</span> 
<br class="ltx_break">Julien Cornebise 
<br class="ltx_break">Element AI and University College London 
<br class="ltx_break"><span id="id8.8.id8" class="ltx_text ltx_font_typewriter">j.cornebise@ucl.ac.uk</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Equal contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">We introduce a machine learning workflow to search for, identify, and meaningfully triage videos and images of munitions, weapons, and military equipment, even when limited training data exists for the object of interest. This workflow is designed to expedite the work of OSINT ("open source intelligence") researchers in human rights investigations. It consists of three components: automatic rendering and annotating of synthetic datasets that make up for a lack of training data; training image classifiers from combined sets of photographic and synthetic data; and <span id="id9.id1.1" class="ltx_text ltx_font_typewriter">mtriage</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, an open source software that orchestrates these classifiers’ deployment to triage public domain media, and visualise predictions in a web interface. We show that synthetic data helps to train classifiers more effectively, and that certain approaches yield better results for different architectures. We then demonstrate our workflow in two real-world human rights investigations: the use of the Triple-Chaser tear gas grenade against civilians, and the verification of allegations of military presence in Ukraine in 2014.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Motivation</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Many recent human rights investigations have relied upon finding a specific object (or object class) among a large number of freely available videos or images. Effective manual search-and-analysis of freely available image material has led to remarkable results in human rights research, including the first charges brought at the International Criminal Court based on public domain evidence <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In the Safariland case, although the export of military equipment from the US is commonly a matter of public record, the sale and export of tear gas is not. As a result, it is only when images of tear gas grenades appear online that monitoring organizations and the public can know where they have been sold, and who is using them. The discovery, identification, and archiving of munitions that appear in online images is therefore essential for the pursuit of corporate and government accountability in the global arms trade.</p>
</div>
<figure id="S1.F1" class="ltx_figure ltx_align_floatright">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/synth-tank.jpg" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="426" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/x1.jpg" id="S1.F1.2.g1" class="ltx_graphics ltx_img_landscape" width="664" height="425" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/cropped-synth.png" id="S1.F1.3.g1" class="ltx_graphics ltx_img_square" width="598" height="530" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S1.F1.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/x2.png" id="S1.F1.4.g1" class="ltx_graphics ltx_img_square" width="664" height="654" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.7.2" class="ltx_text" style="font-size:90%;">Our 3D model of a Russian tank (top left) and real turret of a destroyed tank (top right); Our 3D model of the Triple-Chaser grenade (bottom left) and photo of real parts (bottom right).</span></figcaption>
</figure>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Yet for all its potential impact, searching for images of tear gas online manually is laborious and time-consuming. Automating discovery, identification, or archiving, could be hugely beneficial to human rights monitors. The deployment of computer vision classifiers holds substantial promise for the field of human rights investigation and monitoring at large.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">While deep learning is a state-of-the-art technique for computer vision <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, it is extremely data-intensive. Effective classifiers usually require thousands of annotated training images, even when using pretrained models. Not only is this annotation of real-world images costly and laborious; building such a set relies on there being enough images available (with appropriate licensing) in the first place. When investigating the use of Triple-Chaser, a tear gas canister manufacted by a group called Safariland, we found that public images of the Triple-Chaser are relatively rare. Synthetic datasets of different flavours have been used in various domains to create supplemental training data <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. The authors know of one other initiative to use synthetic data for the purpose of documenting human rights violations, VFRAME <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Contribution</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">To supplement a lack of training data in various cases, we developed an approach to generate annotated synthetic training data sets. Our synthetic data is generated using both Unity and Unreal Engine. We use a combination of purely artificial ’low-fidelity’ images, and ’high-fidelity’ ones that recreate the visual contexts in which the object of interest has been previously documented. We illustrate the potential of synthetic data by looking in detail at the Triple-Chaser case, providing a comparative study of the advantage of synthetic data with various model architectures and training methods.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">To scale inference across a range of public domain media, a requirement for a classifier’s meaningful use in human rights research, we developed a workflow to apply classifiers on the frames of videos scraped from Youtube. We also developed a web interface to allow human rights researchers to interactively visualise predictions, and thus streamline results as a part of OSINT research. We release the tool <span id="S1.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">mtriage</span> as open-source <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Finally, we report on the impact this work has had on our human rights investigations.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Generating synthetic data</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">By carefully studying images collected in the field, we used architectural modelling and game engine software software to generate annotated, high-fidelity synthetic datasets.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Domain randomisation: parametric modification of 3D models and render perspective</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Referencing a photogrammetric reconstruction of the Triple Chaser (see <a href="#A1.SS1" title="A.1 Photogrammetry ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>) alongside other photographs, we used Adobe’s Substance Designer <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to create parametric photo-realistic textures for the grenade. In <a href="#A1.SS2" title="A.2 Unreal Engine ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>, we outline our reasons for choosing Unreal. In the Substance texture, the branding color and content for the grenade, which varies around the world, can be modified at render time to produce a range of variations. We can similarly modify weathering and wear effects on the canister, such as dust, dirt, grime, scratches, and bends.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We then insert the 3D model into a scene that randomly varies the render camera’s position, as well as modifying settings such as exposure, focal length, and depth of field, between frames. After rendering each frame, we also apply post-effects such as LUTs for color correction, variations in the film grain, and different vignettes. See Appendix <a href="#A1.SS3" title="A.3 Texture variations ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a> for demonstrative images. In a parallel stream of work, we also developed a Unity 3D pipeline that renders images at lower fidelity (see <a href="#A1.F5" title="Figure 5 ‣ A.5 Mask annotations ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> but contains distractor objects that further randomise the viewing conditions the Triple Chaser grenade appears in.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Producing annotated synthetic datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Using Unreal, we produced thousands of synthetic images depicting the Triple-Chaser in various conditions. Following Moore’s approach <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, the grenade was depicted both against "decontextualized" patterned backgrounds as well as simulated real-world environments. In addition to each rendered image, we also rendered an additional ‘mask’ for each image that contains pixel-level annotations. By comparing this mask with the original image, we then generated bounding boxes and bitmap masks for each rendered image (see <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>). See Appendix <a href="#A1.SS5" title="A.5 Mask annotations ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.5</span></a> for demonstrative images.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Using synthetic data</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Numerical evaluation on Triple Chaser</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We evaluate the utility of synthesised images containing the Triple Chaser on a small data set of real, labelled images containing the grenade. We trained three separate models: a classification network, a semantic segmentation network, and an instance segmentation network. For details of the training data and the model architectures, see <a href="#A1.SS7" title="A.7 Details of training architectures of varying complexity on synthetic images. ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.7</span></a>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Training with synthetic data increased performance of the model. In particular, training with synthetic data allowed models to train significantly faster. We observed that training with domain adversarial methods provided significant performance increases when using classification models, while that increase was smaller when using a semantic segmentation model, and nonexistent with a mask model.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We believe that this could be alleviated with a more sophisticated domain adaptation approach. The domain adaptation achieved with our architecture only affected roughly half the parameters in the U-Net architecture, and an even smaller proportion in MaskRCNN, which may explain why it didn’t produce a larger up-lift in those models. On the other hand, mask predictions encourage models to process information locally and retain visual information necessary to identify the Triple Chaser grenades, which may mean that domain adaptation is less necessary. This might be because the distance between synthetic and real distributions of pixels on the canister is much lower than the distance between distribution of pixels in the background.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2004.01030/assets/images/training-results.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Area under ROC curve of models trained for 10 epochs. For details of the training procedures and data used, see <a href="#A1.SS7" title="A.7 Details of training architectures of varying complexity on synthetic images. ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.7</span></a>. Synthetic data provides faster learning, and effects of domain difference can be alleviated with domain adaptation.</span></figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">It is important to note the different standards by which such applications are useful. Because there are direct legal implications related to the discovery of material, neural network output must be verified by human experts. Our image classifier’s task is therefore to narrow down the number of images for the human expert. Evaluation of this work purely with classifier accuracy does not assess the usefulness of the workflow in practice. Thresholds for network predictions are set dynamically: after ranking based on network scores, humans simply start at the top and work their way down until they realise the predictions are no longer accurate. For this reason, we believe that area under either ROC or P/R curves are informative; but that user studies of OSINT researchers interacting with the model output surfaced via mtriage would produce better evaluation metrics.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analysing media at scale with Mtriage</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The bulk of media used during human rights investigations derive from major platforms such as Youtube, Twitter, and Facebook. These platforms often have robust APIs, but they differ in various ways: in the authentication they require, the search parameters they make available, and the media formats they support. We introduce Mtriage, a modular open source software, to produce a standardised media format across various platforms, and to effectively orchestrate analysis at scale across this media. Mtriage contains two types of components: <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">selectors</em> to search for and download media, and <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">analysers</em> to derive data from selected media. See Appendix <a href="#A1.SS6" title="A.6 Mtriage ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.6</span></a> for more detail regarding Mtriage’s architecture and use-case.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">In this work, we use Mtriage to run inference at scale, utilising a NVIDIA-RTX 2080Ti GPU and running in parallel on 16 CPUs, using classifiers trained with synthetic data to make predictions on videos scraped from Youtube. The workflow splits each video into frames sampled at a rate of one frame per second, and then runs inference with the classifier for each sampled frame, predicting the likelihood of a grenade in each image. The predictions are then visualized in a web interface (also shown in Appendix <a href="#A1.SS6" title="A.6 Mtriage ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.6</span></a>).</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Real-world use in human rights investigations</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In the Triple-Chaser investigation, a fully trained human rights researcher spent one full-time week to crawl online sources and surface merely 173 images of the grenades. Our pipeline Mtriage scanned the same sources in a couple of hours.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In the investigation "The Battle of Ilovaisk" <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, we experimentally ran Mtriage to collect images of tanks using a ResNet50 classifier trained on ImageNet <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Previously, an experienced open source researcher who is an expert in Ukraine/Russia relations had worked full-time for three weeks to collect some 37 videos from Youtube that he deemed relevant.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We ran the Mtriage workflow using three or four search terms related to the conflict (a far narrower net than that which the researcher had cast while searching manually), which yielded approximately 600 videos. Download, frame extraction, and inference on these took approximately 4 hours, running on a single computer, leveraging GPU capabilities via CUDA where appropriate. Workflows in this paper were run in serial, as Mtriage operations had not been parallelized at the time. After sorting these videos by the relative volume of frames with a positive prediction, in less than 20 minutes the same researcher found two previously undiscovered videos that were instrumental in the research more generally.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">While these two single cases cannot substitute a varied and more extensive study, Mtriage’s use in these investigations does indicate our pipeline’s powerful potential in supporting specialised researchers to expedite media discovery in human rights research.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Future work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This research shows that synthetic data is a valuable addition to a researcher’s toolkit when training classifiers. It will be exciting to further qualify the potential of synthetic data by experimenting with varied and additional domain randomization techniques in Unity, Unreal, and other graphics engines.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The early enthusiasm for this research from our partners in the fields of OSINT research and human rights investigation and monitoring has been very encouraging. Through workshops and ongoing consultation with organisations such as Amnesty International, its Citizen Evidence Lab <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and Digital Verification Corps, Bellingcat, Human Rights Watch, the UN’s Office for the High Commissioner of Human Rights, and the Syrian Archive, we are in the process of co-conceiving new use cases through which both this research can be furthered, and the workflow further tested and improved. We are actively developing a project to create an AI model zoo for human rights research, so that techniques in computer vision can be further democratised across many flavours of human rights research using Mtriage and other tools.</p>
</div>
<section id="S4.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S4.SS0.SSSx1.p1" class="ltx_para">
<p id="S4.SS0.SSSx1.p1.1" class="ltx_p">We thank Emily Jacir, Sarit Michaeli, Oren Ziv, Zuhal Altan, Micol (@_EN_Mexico on Twitter), Phoebe Cottam, and Giedre Steikunate for contributing images of the Triple Chaser. We would also like to acknowledge Praxis Films for working closely with Forensic Architecture to produce the film that was shown in the 2019 Whitney Biennial documenting this research, as well as David Byrne, Anna Feigenbaum, Neil Cormey and Adam Harvey. We thank Alex Kuefler from Element AI for some of his computer vision code. Frank Longford conducted a six-week study on the efficacy of synthetic data as a research fellow at Forensic Architecture in 2018, thanks to the generous support of Faculty data science; that work was a prelude to the much of the research presented in this paper.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
mtriage github repository.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/forensic-architecture/mtriage" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/forensic-architecture/mtriage</a>.

</span>
<span class="ltx_bibblock">Accessed: 2019-09-06.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costello [2018]</span>
<span class="ltx_bibblock">
Róisín A Costello.

</span>
<span class="ltx_bibblock">International criminal law and the role of non-state actors in
preserving open source evidence.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Cambridge International Law Journal</em>, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al. [2015]</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">nature</em>, 521(7553):436, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miglio [2019]</span>
<span class="ltx_bibblock">
Sebastien Miglio.

</span>
<span class="ltx_bibblock">AI in Unreal Engine: Learning Through Virtual
Simulations.

</span>
<span class="ltx_bibblock">https://www.unrealengine.com/en-US/tech-blog/ai-in-unreal-engine-learning-through-virtual-simulations,
2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay et al. [2018a]</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Training Deep Networks with Synthetic Data: Bridging the
Reality Gap by Domain Randomization.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv:1804.06516 [cs]</em>, April 2018a.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ros et al. [2016]</span>
<span class="ltx_bibblock">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M.
Lopez.

</span>
<span class="ltx_bibblock">The SYNTHIA Dataset: A Large Collection of Synthetic
Images for Semantic Segmentation of Urban Scenes.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>, pages 3234–3243, 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tremblay et al. [2018b]</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Thang To, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Falling Things: A Synthetic Dataset for 3D Object
Detection and Pose Estimation.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv:1804.06534 [cs]</em>, April 2018b.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harvey [2019]</span>
<span class="ltx_bibblock">
Adam Harvey.

</span>
<span class="ltx_bibblock">Vframe.

</span>
<span class="ltx_bibblock">https://vframe.io/, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Substance Designer product web page.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.allegorithmic.com/products/substance-designer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.allegorithmic.com/products/substance-designer</a>.

</span>
<span class="ltx_bibblock">Accessed: 2019-09-06.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moore [2018]</span>
<span class="ltx_bibblock">
Matt Moore.

</span>
<span class="ltx_bibblock">Generating large, synthetic, annotated, &amp; photorealistic datasets
for computer vision.

</span>
<span class="ltx_bibblock">https://towardsdatascience.com/generating-large-synthetic-annotated-photorealistic-datasets-for-computer-vision-ffac6f50a29c,
2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ue [4]</span>
<span class="ltx_bibblock">
ue4-supervisely github repository.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/forensic-architecture/scripts/tree/master/ue4-supervisely" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/forensic-architecture/scripts/tree/master/ue4-supervisely</a>.

</span>
<span class="ltx_bibblock">Accessed: 2019-09-06.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
The Battle of Ilovaisk investigation description web page.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ilovaisk.forensic-architecture.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ilovaisk.forensic-architecture.org/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2019-09-06.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</em>, pages 248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Citizen Evidence Lab web page.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://citizenevidence.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://citizenevidence.org/</a>.

</span>
<span class="ltx_bibblock">Accessed: 2019-09-06.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Flickr8k dataset.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2015]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1512.03385, 2015.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1512.03385" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1512.03385</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. [2015]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1505.04597, 2015.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1505.04597" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1505.04597</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2017]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">The IEEE International Conference on Computer Vision
(ICCV)</em>, Oct 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganin et al. [2016]</span>
<span class="ltx_bibblock">
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky.

</span>
<span class="ltx_bibblock">Domain-adversarial training of neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, 17(1):2096–2030, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pei et al. [2018]</span>
<span class="ltx_bibblock">
Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang.

</span>
<span class="ltx_bibblock">Multi-adversarial domain adaptation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Thirty-Second AAAI Conference on Artificial Intelligence</em>,
2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ciga et al. [2019]</span>
<span class="ltx_bibblock">
Ozan Ciga, Jianan Chen, and Anne Martel.

</span>
<span class="ltx_bibblock">Multi-layer domain adaptation for deep convolutional networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, page 20–27, 2019.

</span>
<span class="ltx_bibblock">ISSN 1611-3349.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-030-33391-1_3</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://dx.doi.org/10.1007/978-3-030-33391-1_3" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1007/978-3-030-33391-1_3</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Photogrammetry</h3>

<figure id="A1.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F3.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image15.png" id="A1.F3.1.g1" class="ltx_graphics ltx_img_square" width="598" height="624" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F3.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image13.png" id="A1.F3.2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="597" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="A1.F3.5.2" class="ltx_text" style="font-size:90%;">Photogrammetry is a Structure-from-Motion algorithm that allows the reconstruction of 3D assets given multiple photographic perspectives of an object as input.</span></figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Unreal Engine</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">We could have worked in a number of 3D frameworks, but we chose Unreal for a number of reasons:</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<ol id="A1.I1" class="ltx_enumerate">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Purpose-built for dynamic perspective</span>. In
previous
research<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://engineering.forensic-architecture.org/experiments-in-synthetic-data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://engineering.forensic-architecture.org/experiments-in-synthetic-data</a></span></span></span>, we built an experimental framework in
Maxon’s
Cinema4D<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.maxon.net/en/products/cinema-4d/overview/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.maxon.net/en/products/cinema-4d/overview/</a></span></span></span> for this same purpose. Cinema4D is a general purpose
modelling and animating suite, similar in principle to
Blender<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.blender.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.blender.org/</a></span></span></span> or
Maya<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.autodesk.com/products/maya/overview" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.autodesk.com/products/maya/overview</a></span></span></span>. While
there are certain advantages using it, when using it we found that we
were required to script many camera, texture, and model variations
from scratch. Indeed, many of the functions we require for a synthetic
data generator– such as domain randomization, parametric textures,
and variable lighting– are more common in game development than in
animation. Unreal offers a lot of relevant functionality ‘out of the
box’, and has a rich online support community.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Powerful visual scripting</span>. Unreal’s node-based visual
scripting language
blueprints<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://docs.unrealengine.com/en-us/Engine/Blueprints" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.unrealengine.com/en-us/Engine/Blueprints</a></span></span></span>
is more accessible than code for the architects, game designers, and
filmmakers with whom we work. We used Unreal blueprints to programmatically modify camera position, camera settings (exposure, focal length and depth of field), as well as image post-effects (LUT’s for color correction variations, film grain, and vignette).</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Full source code access</span>. Unreal’s source code is available in
its entirety (although you need an Epic Games account in order to view
Github repository.) It’s not exactly ‘open source’, as the license
under which it is released does not allow for redistribution or
adaptation, but source code access allows us to understand performance
bottlenecks, and to see the full scope of what is possible in the
software.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Real-time raytracing</span>. Unreal’s capability for real-time
rendering is the cherry on the top of Unreal’s suitability for
synthetic data generation. The real-time quality of rendering means
that, in contrast to a workflow using Cinema4D, there are much
shorter turnarounds in the ongoing conversation between generating and
training phases.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Texture variations</h3>

<figure id="A1.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F4.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image.jpeg" id="A1.F4.1.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F4.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_1.jpeg" id="A1.F4.2.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F4.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_2.jpeg" id="A1.F4.3.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F4.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_3.jpeg" id="A1.F4.4.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="A1.F4.5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_4.jpeg" id="A1.F4.5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F4.6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_5.jpeg" id="A1.F4.6.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F4.7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_6.jpeg" id="A1.F4.7.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F4.8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image_7.jpeg" id="A1.F4.8.g1" class="ltx_graphics ltx_img_square" width="598" height="499" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F4.10.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="A1.F4.11.2" class="ltx_text" style="font-size:90%;">Various amounts of grit and dirt on a rendered model of the Triple Chaser (increasing in intensity from left top to right bottom). The model of the grenade is represented with no label (first image from left top) and with a varied label (third image from left top).</span></figcaption>
</figure>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Rendering less photorealistic images in Unity</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">In addition to data rendered in UE4, we also rendered data with the Unity 3D engine. We used the same 3D models and the same parametric variations of the Triple Chaser grenade in Unity. However, we added domain randomisation features to our rendering engine. In particular, we sampled background images from the Flicker 8K dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and added "flying distractors" to the scene. These geometric objects (cubes, spheres, cylinders) added additional lighting complexity to the dataset and allowed for canisters to be partially obscured.</p>
</div>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Mask annotations</h3>

<figure id="A1.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F5.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image16.jpeg" id="A1.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F5.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image12.jpeg" id="A1.F5.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F5.3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/image1.jpeg" id="A1.F5.3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="326" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F5.4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2004.01030/assets/images/img0.jpeg" id="A1.F5.4.g1" class="ltx_graphics ltx_img_square" width="598" height="598" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F5.6.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A1.F5.7.2" class="ltx_text" style="font-size:90%;">A rendered synthetic image (left top), the corresponding pixel-level instance mask annotation (right top), and the training image with bounding boxes overlaid (bottom). Images were also rendered with Unity at less high-resolution, and with less photorealistic textures and render settings, but with flying distractors and randomised photographic backgrounds.</span></figcaption>
</figure>
</section>
<section id="A1.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Mtriage</h3>

<figure id="A1.F6" class="ltx_figure ltx_align_center"><img src="/html/2004.01030/assets/images/Phase-04_Overview.jpg" id="A1.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="354" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A1.F6.4.2" class="ltx_text" style="font-size:90%;">The web interface presented by mtriage-viewer to the visualise results of object detection inference on the frames of Youtube videos, processed in mtriage. Each grey cell represents the timeline of a video. Red frames in the videos timeline represent positive predictions, with brighter red indicating a higher confidence, and darker red the opposite. When the user hovers over a section in the timeline, the exact confidence is displayed in a tooltip. Controls to apply a minimum threshold at which to display a prediction (i.e. 25%) and to rank videos by different metrics are available on the right.</span></figcaption>
</figure>
<div id="A1.SS6.p1" class="ltx_para">
<p id="A1.SS6.p1.1" class="ltx_p">As we increasingly found ourselves writing repetitive logic to scrape, download, preprocess, and process media, we developed Mtriage. At a high level, Mtriage is a command-line interface to orchestrate computations that read and write to the local filesystem. Computations are implemented in Python, and are inserted into Mtriage’s lifecycle through two related abstractions: <span id="A1.SS6.p1.1.1" class="ltx_text ltx_font_bold">selectors</span>, which sample media from various platforms and comport them as Mtriage ’elements’ (media with well-defined types on the filesystem); and <span id="A1.SS6.p1.1.2" class="ltx_text ltx_font_bold">analysers</span>, which process pre-selected elements, and create corresponding elements.</p>
</div>
<div id="A1.SS6.p2" class="ltx_para">
<p id="A1.SS6.p2.1" class="ltx_p">The base requirement for an <code id="A1.SS6.p2.1.1" class="ltx_verbatim ltx_font_typewriter">Analyser</code> is an implementation for the <code id="A1.SS6.p2.1.2" class="ltx_verbatim ltx_font_typewriter">analyse_element</code> function, which expresses the computation performed to read an element and produce a new one. The analysis performed in an analyser is conceived as a one-to-one function that reads an element from the filesystem, and eventually writes another. Each analyser has self-contained dependencies, as each is run in a separate Docker container, with Mtriage mounting the appropriate parts of the filesystem as volumes during runtime. Analyser code is arbitrarily complex and, though it must be invoked via Python, can execute logic in any language that can be virtualized in Docker (all languages). This architecture allows developers to architect complex orchestration workflows, treating each analyser as a self-contained microservice. When running analysis, Mtriage scales <code id="A1.SS6.p2.1.3" class="ltx_verbatim ltx_font_typewriter">analyse_element</code> to operate across N elements, running operations in parallel according to the number of CPUs available on the host computer. Other class functions such as <code id="A1.SS6.p2.1.4" class="ltx_verbatim ltx_font_typewriter">pre_analyse</code> and <code id="A1.SS6.p2.1.5" class="ltx_verbatim ltx_font_typewriter">post_analyse</code> can be optionally implemented if setup or teardown (logic that does not need to be run for every element, but only once) is necessary.</p>
</div>
<div id="A1.SS6.p3" class="ltx_para">
<p id="A1.SS6.p3.1" class="ltx_p">The <code id="A1.SS6.p3.1.1" class="ltx_verbatim ltx_font_typewriter">Selector</code> class is quite similar to <code id="A1.SS6.p3.1.2" class="ltx_verbatim ltx_font_typewriter">Analyser</code>, but differs in that it does not take elements as input, but instead generates elements by retrieving media based on the configuration variables passed. The Youtube selector, for example, requires a search term and start/finish dates, and generates elements that contain the videos uploaded between those dates that are returned from a Youtube search. Mtriage assumes that the <code id="A1.SS6.p3.1.3" class="ltx_verbatim ltx_font_typewriter">retrieve_element</code> function contains some asynchronous logic, and thus both parallelizes operations across muliple CPUs, and cooperative multitasking on each CPU.</p>
</div>
<div id="A1.SS6.p4" class="ltx_para">
<p id="A1.SS6.p4.1" class="ltx_p">The selector or analyser to be used, as well as the configuration specific to that component, is written in a single YAML configuration file that is passed to Mtriage via its path from the command line. Logs are both printed to the console and kept on the local filesystem for reference once a component’s process is complete. If a process is interrupted for any reason, and Mtriage run again with the same configuration file, the process will continue from where it left off, only running logic on elements that have not yet been processed. Selectors and analysers can be chained together by a unique <code id="A1.SS6.p4.1.1" class="ltx_verbatim ltx_font_typewriter">meta</code> analyser that works as a higher-order function to chain the logic of multiple components into one.</p>
</div>
<div id="A1.SS6.p5" class="ltx_para">
<p id="A1.SS6.p5.1" class="ltx_p">Once a process has been run to completion using Mtriage, the result can be effectively visualized using <span id="A1.SS6.p5.1.1" class="ltx_text ltx_font_bold">mtriage-viewer</span>, an auxiliary tool we developed and which is available at https://github.com/forensic-architecture/mtriage-viewer<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/forensic-architecture/mtriage-viewer" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/forensic-architecture/mtriage-viewer</a></span></span></span>.</p>
</div>
<div id="A1.SS6.p6" class="ltx_para">
<p id="A1.SS6.p6.1" class="ltx_p">For more detail and instructions on how to use mtriage in your own research, see https://github.com/forensic-architecture/mtriage<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/forensic-architecture/mtriage" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/forensic-architecture/mtriage</a></span></span></span>. Mtriage is available under the DoNoHarm license on Github. We welcome community pull requests and new contributors.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A1.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.7 </span>Details of training architectures of varying complexity on synthetic images.</h3>

<div id="A1.SS7.p1" class="ltx_para">
<p id="A1.SS7.p1.1" class="ltx_p">During our model training process, our synthetic training data consisted of images rendered using different parameters and tools. In total, we used 700 high-fidelity images rendered with UE4, and 20,000 images rendered with low-fidelity using Unity. For parametric variations in rendering, see <a href="#A1.SS3" title="A.3 Texture variations ‣ Appendix A Appendix ‣ Objects of violence: synthetic data for practical ML in human rights investigations" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
<div id="A1.SS7.p2" class="ltx_para">
<p id="A1.SS7.p2.1" class="ltx_p">We combined our synthetic data with images not relevant to the task from the Pascal VOC dataset. Each model was trained to predict the Triple Chaser class, as well as the 20 "nuisance" classes in Pascal VOC which were ignored during evaluation. Using these real images enabled us to train a model using domain adversarial training.</p>
</div>
<div id="A1.SS7.p3" class="ltx_para">
<p id="A1.SS7.p3.1" class="ltx_p">During training, images were sampled equally from the low-fidelity, high-fidelity, and Pascal VOC datasets. In practice, this meant that we significantly under-sampled from the low-fidelity images.</p>
</div>
<div id="A1.SS7.p4" class="ltx_para">
<p id="A1.SS7.p4.1" class="ltx_p">We trained three different architectures on this task: a resnet 50 classifier <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, a u-net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> with Resnet 50 as the encoder, and a mask-rcnn model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> with resnet 50 as the backbone in the feature pyramid network. For full implementation details, see the citations for each architecture. When calculating performance, to obtain image-level class predictions, we used the softmax probabilities from the classifier, the average of the top-5% of pixelwise softmax probabilities from the semantic segmentation network, and the maximum class probability of the MaskRCNN after non-max suppression.</p>
</div>
<div id="A1.SS7.p5" class="ltx_para">
<p id="A1.SS7.p5.1" class="ltx_p">In all cases, we trained networks using the Adam optimizer with a learning rate of 1e-4 and L2 regularisation of 1e-6. Domain adversarial training was achieved by extracting intermediate layer output (layer four of Resnet 50) from the model and processing it using a simple fully connected classifier trained to discriminate between 3 domains: real-world photography, low-fidelity synthetic images (i.e. generated with Unity), and high-fidelity synthetic images (i.e. UE4). This discriminator uses linear layers with ReLU activation. Since Resnet50 was used in all our architectures, it processed the output of layer 4 (2048 feature maps), with 3 fully connected layers (with 512, and 256 units in hidden layers and 3 output units, respectively). We optimized cross-entropy of class predictions in these fully connected layers, but reversed the gradients backpropagated into the main, task-specific network. Moving weights in the opposite direction to the gradient that optimises domain classification leads to domain adaptation: parameters become optimised to produce intermediate features that are indistinguishable between domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. To stabilise training, we multiplied the domain adversarial loss with a weighting alpha, which we varied between 0.001, 0.05, and 0.1.</p>
</div>
<div id="A1.SS7.p6" class="ltx_para">
<p id="A1.SS7.p6.1" class="ltx_p">This implementation of domain adversarial training was originally designed for classification models. The reversed gradients only flow through parts of the feature representation in the segmentation networks and therefore only affect a small proportion of the parameters. Recent approaches to domain adversarial training in segmentation models have used multi-level domain confusion approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> which attempt to match intermediate representations of the data at different stages of the network. This may help adapt segmentation networks better.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2004.01030/assets/images/model-architectures.jpg" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A1.F7.3.2" class="ltx_text" style="font-size:90%;">The three architectures we used. In all cases, we used an ImageNet pre-trained Resnet 50 as the initial encoder. The output of this is where domain adversarial training, using reverse gradients, was applied. For details on the remaining components of the architecture, we refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</span></figcaption>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2004.01027" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2004.01030" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2004.01030">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2004.01030" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2004.01031" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 12:01:48 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
