<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2406.00093] Bootstrap3D: Improving 3D Content Creation with Synthetic Data</title><meta property="og:description" content="Recent years have witnessed remarkable progress in multi-view diffusion models for 3D content creation. However, there remains a significant gap in image quality and prompt-following ability compared to 2D diffusion mo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bootstrap3D: Improving 3D Content Creation with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bootstrap3D: Improving 3D Content Creation with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2406.00093">

<!--Generated on Fri Jul  5 21:08:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Bootstrap3D: Improving 3D Content Creation with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zeyi Sun<sup id="id13.13.id1" class="ltx_sup"><span id="id13.13.id1.1" class="ltx_text ltx_font_italic">1,3</span></sup>, Tong Wu<sup id="id14.14.id2" class="ltx_sup"><span id="id14.14.id2.1" class="ltx_text ltx_font_italic">2</span></sup><sup id="id15.15.id3" class="ltx_sup">🖂</sup>, Pan Zhang<sup id="id16.16.id4" class="ltx_sup"><span id="id16.16.id4.1" class="ltx_text ltx_font_italic">3</span></sup>, Yuhang Zang<sup id="id17.17.id5" class="ltx_sup"><span id="id17.17.id5.1" class="ltx_text ltx_font_italic">3</span></sup>, Xiaoyi Dong<sup id="id18.18.id6" class="ltx_sup"><span id="id18.18.id6.1" class="ltx_text ltx_font_italic">3</span></sup> 
<br class="ltx_break"><span id="id8.8.3" class="ltx_text ltx_font_bold">Yuanjun Xiong<sup id="id8.8.3.1" class="ltx_sup"><span id="id8.8.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">4</span></sup>, Dahua Lin<sup id="id8.8.3.2" class="ltx_sup"><span id="id8.8.3.2.1" class="ltx_text ltx_font_medium ltx_font_italic">2,3</span></sup>, Jiaqi Wang<sup id="id8.8.3.3" class="ltx_sup"><span id="id8.8.3.3.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup><sup id="id8.8.3.4" class="ltx_sup">🖂</sup></span> 
<br class="ltx_break"><sup id="id19.19.id7" class="ltx_sup">1</sup>Shanghai Jiao Tong University  <sup id="id20.20.id8" class="ltx_sup">2</sup>The Chinese University of Hong Kong  
<br class="ltx_break"><sup id="id21.21.id9" class="ltx_sup">3</sup>Shanghai AI Laboratory  <sup id="id22.22.id10" class="ltx_sup">4</sup>MThreads, Inc.
<br class="ltx_break"><a target="_blank" href="https://SunzeY.github.io/Bootstrap3D/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://SunzeY.github.io/Bootstrap3D/</a>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id23.id1" class="ltx_p">Recent years have witnessed remarkable progress in multi-view diffusion models for 3D content creation. However, there remains a significant gap in image quality and prompt-following ability compared to 2D diffusion models. A critical bottleneck is the scarcity of high-quality 3D assets with detailed captions. To address this challenge, we propose <span id="id23.id1.1" class="ltx_text ltx_font_bold">Bootstrap3D</span>, a novel framework that automatically generates an arbitrary quantity of multi-view images to assist in training multi-view diffusion models. Specifically, we introduce a data generation pipeline that employs (1) 2D and video diffusion models to generate multi-view images based on constructed text prompts, and (2) our fine-tuned 3D-aware <span id="id23.id1.2" class="ltx_text ltx_font_bold">MV-LLaVA</span> for filtering high-quality data and rewriting inaccurate captions. Leveraging this pipeline, we have generated 1 million high-quality synthetic multi-view images with dense descriptive captions to address the shortage of high-quality 3D data. Furthermore, we present a <span id="id23.id1.3" class="ltx_text ltx_font_bold">Training Timestep Reschedule (TTR)</span> strategy that leverages the denoising process to learn multi-view consistency while maintaining the original 2D diffusion prior. Extensive experiments demonstrate that Bootstrap3D can generate high-quality multi-view images with superior aesthetic quality, image-text alignment, and maintained view consistency.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">3D content creation stands as a fundamental challenge within the generative domain, boasting widespread applications in augmented reality (AR) and game modeling. Unlike 2D image generation, the dearth of high-quality 3D models persists as a significant hurdle to overcome. In the realm of 2D image generation, the pivotal role of training on billion-scale image-text pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> has been firmly established <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. However, in 3D content generation, the scarcity of high-quality 3D models often compels reliance on the priors of 2D diffusion models. The predominant methodologies in this domain can be categorized into two main streams: 1) Gaining optimized neural representations from fixed 2D diffusion models via Score Distillation Sampling (SDS) loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, which are time-intensive, lacking diversity and suffer from low robustness although capable of producing high-quality 3D objects.
2) Fine-tuning 2D diffusion models to achieve multi-view generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
, directly synthesizing 3D objects through sparse reconstruction models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. With recent improvements in large-scale sparse view reconstruction models and 3D representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, the second stream is garnering increasing attention.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Fine-tuning 2D diffusion models for multi-view generation remains challenging owing to the insufficiency in both data quality and quantity. Previous methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> primarily train on a filtered subset of high-quality data from Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and Objaverse-XL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The scarcity of high-quality data often introduces various shortcomings. In single-view based novel view synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, if the input images deviate from the distribution of the training data, it can induce issues such as motion blurring, object distortion and deformation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.
Moreover, in direct text-to-multi-view image generation, the pursuit of enhancing view consistency compromises the aesthetic and photo-realistic quality. For instance, Intant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> fine-tunes SDXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> using only 10K high-quality Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> data with a small learning rate for 10K steps, which does not fundamentally prevent the catastrophic forgetting problem of losing 2D diffusion prior, leading to compromised image quality. Recent endeavors have predominantly focused on alleviating data scarcity and improving view consistency from a model-centric perspective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, with limited exploration into the improvement of training data itself.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recent Multimodal Large Language Models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> like GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>, possess image understanding capabilities and rudimentary 3D world awareness, has enabled automatic quality assessment of multi-view images and dense caption generation. Furthermore, notable advancements in video diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> have improved the generalizability of novel view synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Employing these advancements, we propose Bootstrap3D to generate synthetic data to counteract the data deficiencies inherent in training multi-view diffusion models.
To be specific, we introduce the Bootstrap3D data generation pipeline for producing high-quality multi-view images with dense descriptive captions. Subsequently, we fine-tune a multi-view-aware MLLM model, dubbed as MV-LLaVA. MV-LLaVA achieves fully automated high-quality data generation with both efficiency and accuracy. To mitigate catastrophic forgetting of 2D diffusion prior, we introduce a training timestep reschedule (TTR) strategy when fine-tuning multi-view diffusion models. Specifically, we use the phased nature of the denoising process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and limit different training time steps for synthetic data and real data to achieve enhanced image quality with maintained view consistency.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Through extensive experiments, we demonstrate that our method significantly enhances the adherence of the multi-view diffusion model to text prompts and image quality while ensuring view consistency. Integrated with the reconstruction model and Gaussian representations, our approach facilitates the creation of 3D models with superior quality.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions are summarized into the following points:</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">1)</span> We present an automated <span id="S1.p6.1.2" class="ltx_text ltx_font_bold">Bootstrap3D</span> data generation pipeline that uses the video diffusion model and our fine-tuned 3D-aware MV-LLaVA to synthesize an arbitrary number of high-quality 3D data.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">2)</span> We propose a Training Time-step Reschedule (TTR) strategy for fine-tuning the multi-view diffusion model that employs both synthetic data and real data to enhance image quality and image-text alignment while maintaining view consistency.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_bold">3)</span> We generate <span id="S1.p8.1.2" class="ltx_text ltx_font_bold">1 million</span> multi-view images with dense descriptive captions suitable for training the multi-view diffusion model, which improves the 3D generation quality and mitigates the gap with the 2D diffusion model from a data perspective.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Existing 3D datasets and data pre-processing.</span> Existing object level 3D datasets, sourced either from CAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> or scan from real objects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, are still small in size. Most state-of-the-art open-sourced 3D content creation models are trained on Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> (800k) and Objaverse-XL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (10M). However, there still exists a huge gap compared to data used for training 2D diffusion models like Laion5B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. In addition to quantity, quality is also an important problem remains to be solved as many methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> trained on Objaverse or Objaverse-XL rely on simple methods like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> score to filter out low-quality data, making the precious 3D data even less. This gap poses a significant obstacle to achieving high-quality 3D content creation compared to its 2D diffusion counterpart. Another critical gap that requires attention is the quality of the 3D object’s caption. Previous work Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> propose to apply BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite> to generate caption based on multi-view images. However, this approach, without direct input image into GPT, can lead to severe hallucination. Given recent breakthroughs in improving text-image alignment through caption rewriting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, there is a pressing need to rewrite denser and more accurate captions for 3D objects with the assistance of advanced Multimodal Large Language Models (MLLMs) compared to what Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> have accomplished. In this work, we propose a new data generation pipeline to synthesize multi-view images and rewrite captions for 3D objects incorporating additional quality scoring mechanisms to address the aforementioned issues.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Text-to-3D content creation.</span>
The field of 3D content creation has been a vibrant area of research over the past years. One prominent research direction explores the use of Score Distillation Sampling (SDS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> and its variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, using the priors of 2D diffusion models to optimize 3D representations. While these methods have demonstrated success in producing high-quality 3D generations, they often require prolonged optimization time to converge. In contrast, recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite> have proposed the direct inference of 3D representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite> conditioned by images. Among these approaches, Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> stands out by utilizing multi-view images of the same object to directly deduce the Triplane <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> representation. This approach effectively addresses the issue of ambiguous unseen areas inherent in the single image to 3D conversions, as encountered in LRM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and TripoSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>. Instant3D, along with subsequent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, efficiently decomposes 3D generation into two processes: the generation of multi-view images using multi-view diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and large reconstruction model to generate 3D representations conditioned on these multi-view images. In this work, we introduce a method that significantly enhances the scalability of training and data generation for multi-view image generation.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Video diffusion for novel view synthesis.</span>
Recent advancements in video diffusion have marked a significant breakthrough, with models such as Sora <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and SVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> scaling up the direct generation process from images to videos. Following these developments, a series of works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> represented by SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, have fine-tuned these video diffusion models for 3D content creation. Despite these groundbreaking developments, the new perspective images generated based on video priors still suffer from issues like motion blur, leading to inconsistencies with natural images. In this work, we propose to utilize SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> as a data generator to produce novel views of given images with additional quality checks to leave only high-quality data.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Multimodal Large Language Models.</span> With the development of large language models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>, multimodal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, such as GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, have demonstrated groundbreaking 2D comprehension capabilities and open-world knowledge. As is discovered in GPTEval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, GPT-4V can achieve human-aligned evaluation for multi-view images rendered from 3D objects. In this work, we fine-tune the LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, a lightweight MLLM for quality judgment and descriptive caption generation based on multi-view images. This integration uses the strengths of MLLMs in understanding visual content to enhance the quality assessment and captioning process, contributing to improved 3D content generation.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2406.00093/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Bootstrap3D data generation pipeline<span id="S2.F1.4.2.1" class="ltx_text ltx_font_medium"> that consists of 1) using LLM to generate diverse text prompts 2) employing the T2I model to generate single-view images 3) synthesizing arbitrary number of multi-view images by applying the video diffusion model, 4) employing MV-LLaVA to filter and select only high-quality data, and rewrite captions to be dense and descriptive.</span></span></figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Due to the scarcity of high-quality 3D data, we develop the Bootstrap3D data generation pipeline to efficiently construct an arbitrary number of training data (Sec. <a href="#S3.SS1" title="3.1 Bootstrap3D Data Generation Pipeline ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Subsequently, the quality of generated multi-view images is assessed using the powerful GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> or our proposed MV-LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> model to generate dense descriptive captions efficiency and faithfully (Sec. <a href="#S3.SS2" title="3.2 Multi-View LLaVA (MV-LLaVA) ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>). We also design a training timestep reschedule (Sec. <a href="#S3.SS3" title="3.3 Training Timestep Reschedule (TTR) ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>) when fine-tuning the multi-view diffusion model with our synthetic and real data.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Bootstrap3D Data Generation Pipeline</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As illustrated in Fig.<a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, our data generation pipeline initially employs GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> to generate a multitude of imaginative and varied text prompts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. Subsequently, to generate 2D images that closely align with the text prompts, we utilize the PixArt-Alpha <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> model use FlanT5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> text encoder with DiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> architecture for text-to-image (T2I) generation. Thereafter, we use SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> for novel view synthesis. Given the significant motion blur and distortion often present in SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> outputs, we further employ Multimodal Large Language Models(MLLM) to evaluate the quality of multi-view images. To rectify mismatches between multi-view images and the original text prompts induced by novel view synthesis and provide more precise captions, we further propose MV-LLaVA to generate dense descriptive captions for multi-view images.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Multi-View LLaVA (MV-LLaVA)</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To efficiently generate captions and label quality scores for both generated multi-view images and 3D assets in Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we propose the Multi-View LLaVA (MV-LLaVA) that fine-tune LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> based on our instructive conversation pairs generated by the powerful GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Preparing the instruction tuning data.</span> As shown in Fig.<a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we use GPT-4 to generate 20k varied text prompts based on prompts designed in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> and use PixArt-alpha <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to generate single view image and use SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> or Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> to generate multi-view images. For this 20k generated multi-view images, we prompt GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> to generate comments on view consistency, image quality and generate dense descriptive captions. For the additional 10K rendered multi-view images from Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, we prompt GPT-4V (detailed prompts in Sup. <a href="#A4.SS1" title="D.1 Prompts for GPT-4V for Quality Check ‣ Appendix D Details of Prompt Design ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.1</span></a>) to offer feedback on the quality and aesthetic appeal of 3D objects, along with style judgments. We utilize these 30K high-quality multi-view image text pairs (prompts detailed in Sup. <a href="#A4.SS2" title="D.2 Prompts for MV-LLaVA Instruct Tuning ‣ Appendix D Details of Prompt Design ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D.2</span></a>) as the instruction tuning data for LLaVA.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2406.00093/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">MV-LLaVA.<span id="S3.F2.4.2.1" class="ltx_text ltx_font_medium"> We use GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> to generate long descriptive captions, quality scoring, and reasoning processes for multi-view images to construct the instruction tuning dataset. Then we fine-tune our MV-LLaVA based on LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> to serve as the human-aligned quality checker and captioner for multi-view images.</span></span></figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Instruction tuning.</span> As presented in the left part of Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 Multi-View LLaVA (MV-LLaVA) ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, due to the LLaVA’s maximum training context length constraints of 2048, we input four images separately into CLIP-L/14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and generate 4<math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mo id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><times id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\times</annotation></semantics></math>256 image tokens. Inspired by ShareGPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we freeze only a portion of layers of CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> in the first stage of pre-training to enhance multi-view awareness and texture perception of vision encoder (detailed in Sup. <a href="#A3.SS1" title="C.1 Choice of number of unfrozen layers of vision encoder. ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C.1</span></a>).
As shown in the right part of Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 Multi-View LLaVA (MV-LLaVA) ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we first ask the model to generate descriptions, then let the model score the quality based on multi-view images and captions. Our approach encourages LLM to deduct more reasonable quality scores through chain-of-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>. A mixture of original training data of LLaVA is adopted to mitigate over-fitting. As a result, we obtain MV-LLaVA, which efficiently filters and re-captions both synthetic data and 3D assets. As detailed in Sup.<a href="#A3" title="Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, MV-LLaVA can not only generate more accurate, less hallucinated dense captions that faithfully describe 3D objects compared to Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> but also assign the human-aligned quality score on both synthetic data and Objaverse assets. The filtered high-quality multi-view images with rewritten dense captions served as training data for the diffusion model.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training Timestep Reschedule (TTR)</h3>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2406.00093/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="265" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.5.2.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training Timestep Reschedule (TTR).<span id="S3.F3.2.1.1" class="ltx_text ltx_font_medium"> For different types of training data, we restrict the training time step <math id="S3.F3.2.1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.F3.2.1.1.m1.1b"><mi id="S3.F3.2.1.1.m1.1.1" xref="S3.F3.2.1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.1.m1.1c"><ci id="S3.F3.2.1.1.m1.1.1.cmml" xref="S3.F3.2.1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.1.m1.1d">t</annotation></semantics></math> accordingly to achieve the balance between varied high aesthetic images that are better aligned with text prompt, photo-realistic texture, and view consistency for 3D generation.</span></span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Despite retaining only relatively high-quality synthetic data with minimal motion blur from SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> through MV-LLaVA, small areas of blurring persist, stemming from both motion and out-of-distribution scenarios for SV3D and SVD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. These blurred data can potentially compromise the final performance of the multi-view diffusion model. To restrict the training time step for synthetic data, we proposed a simple yet effective Training Timestep Reschedule (TTR) method.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Background.</span> Before delving into TTR, we briefly review some basic concepts needed to understand diffusion models (DDPMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Gaussian diffusion models assume a forward noising process which
gradually applies noise to real data <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">x</mi><mn id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝑥</ci><cn type="integer" id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">x_{0}</annotation></semantics></math></p>
<table id="A7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\displaystyle q(x_{t}|x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})\mathbf{I})" display="inline"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S3.E1.m1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo fence="false" id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">x</mi><mn id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">0</mn></msub></mrow><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.5" xref="S3.E1.m1.4.4.5.cmml">=</mo><mrow id="S3.E1.m1.4.4.4" xref="S3.E1.m1.4.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.4.5" xref="S3.E1.m1.4.4.4.5.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.4" xref="S3.E1.m1.4.4.4.4.cmml">​</mo><mrow id="S3.E1.m1.4.4.4.3.3" xref="S3.E1.m1.4.4.4.3.4.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.4.3.3.4" xref="S3.E1.m1.4.4.4.3.4.cmml">(</mo><msub id="S3.E1.m1.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.1.1.1.3.cmml">t</mi></msub><mo id="S3.E1.m1.4.4.4.3.3.5" xref="S3.E1.m1.4.4.4.3.4.cmml">;</mo><mrow id="S3.E1.m1.3.3.3.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.cmml"><msqrt id="S3.E1.m1.3.3.3.2.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.2.cmml"><msub id="S3.E1.m1.3.3.3.2.2.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.2.2.cmml"><mover accent="true" id="S3.E1.m1.3.3.3.2.2.2.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.2.2.2.cmml"><mi id="S3.E1.m1.3.3.3.2.2.2.2.2.2.2" xref="S3.E1.m1.3.3.3.2.2.2.2.2.2.2.cmml">α</mi><mo id="S3.E1.m1.3.3.3.2.2.2.2.2.2.1" xref="S3.E1.m1.3.3.3.2.2.2.2.2.2.1.cmml">¯</mo></mover><mi id="S3.E1.m1.3.3.3.2.2.2.2.2.3" xref="S3.E1.m1.3.3.3.2.2.2.2.2.3.cmml">t</mi></msub></msqrt><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.3.2.2.2.1" xref="S3.E1.m1.3.3.3.2.2.2.1.cmml">​</mo><msub id="S3.E1.m1.3.3.3.2.2.2.3" xref="S3.E1.m1.3.3.3.2.2.2.3.cmml"><mi id="S3.E1.m1.3.3.3.2.2.2.3.2" xref="S3.E1.m1.3.3.3.2.2.2.3.2.cmml">x</mi><mn id="S3.E1.m1.3.3.3.2.2.2.3.3" xref="S3.E1.m1.3.3.3.2.2.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.E1.m1.4.4.4.3.3.6" xref="S3.E1.m1.4.4.4.3.4.cmml">,</mo><mrow id="S3.E1.m1.4.4.4.3.3.3" xref="S3.E1.m1.4.4.4.3.3.3.cmml"><mrow id="S3.E1.m1.4.4.4.3.3.3.1.1" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.4.3.3.3.1.1.2" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.4.3.3.3.1.1.1" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.cmml"><mn id="S3.E1.m1.4.4.4.3.3.3.1.1.1.2" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.2.cmml">1</mn><mo id="S3.E1.m1.4.4.4.3.3.3.1.1.1.1" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.1.cmml">−</mo><msub id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.cmml"><mover accent="true" id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.cmml"><mi id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.2" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.2.cmml">α</mi><mo id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.1" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.1.cmml">¯</mo></mover><mi id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.3" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S3.E1.m1.4.4.4.3.3.3.1.1.3" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.4.3.3.3.2" xref="S3.E1.m1.4.4.4.3.3.3.2.cmml">​</mo><mi id="S3.E1.m1.4.4.4.3.3.3.3" xref="S3.E1.m1.4.4.4.3.3.3.3.cmml">𝐈</mi></mrow><mo stretchy="false" id="S3.E1.m1.4.4.4.3.3.7" xref="S3.E1.m1.4.4.4.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.5.cmml" xref="S3.E1.m1.4.4.5"></eq><apply id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><times id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.3">𝑞</ci><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S3.E1.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑥</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">0</cn></apply></apply></apply><apply id="S3.E1.m1.4.4.4.cmml" xref="S3.E1.m1.4.4.4"><times id="S3.E1.m1.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4"></times><ci id="S3.E1.m1.4.4.4.5.cmml" xref="S3.E1.m1.4.4.4.5">𝒩</ci><list id="S3.E1.m1.4.4.4.3.4.cmml" xref="S3.E1.m1.4.4.4.3.3"><apply id="S3.E1.m1.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.1.1.1.3">𝑡</ci></apply><apply id="S3.E1.m1.3.3.3.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2"><times id="S3.E1.m1.3.3.3.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.1"></times><apply id="S3.E1.m1.3.3.3.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2"><root id="S3.E1.m1.3.3.3.2.2.2.2a.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2"></root><apply id="S3.E1.m1.3.3.3.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2.2">subscript</csymbol><apply id="S3.E1.m1.3.3.3.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2.2.2"><ci id="S3.E1.m1.3.3.3.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2.2.2.1">¯</ci><ci id="S3.E1.m1.3.3.3.2.2.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2.2.2.2">𝛼</ci></apply><ci id="S3.E1.m1.3.3.3.2.2.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.2.2.3">𝑡</ci></apply></apply><apply id="S3.E1.m1.3.3.3.2.2.2.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.2.2.3.1.cmml" xref="S3.E1.m1.3.3.3.2.2.2.3">subscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.2.3.2.cmml" xref="S3.E1.m1.3.3.3.2.2.2.3.2">𝑥</ci><cn type="integer" id="S3.E1.m1.3.3.3.2.2.2.3.3.cmml" xref="S3.E1.m1.3.3.3.2.2.2.3.3">0</cn></apply></apply><apply id="S3.E1.m1.4.4.4.3.3.3.cmml" xref="S3.E1.m1.4.4.4.3.3.3"><times id="S3.E1.m1.4.4.4.3.3.3.2.cmml" xref="S3.E1.m1.4.4.4.3.3.3.2"></times><apply id="S3.E1.m1.4.4.4.3.3.3.1.1.1.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1"><minus id="S3.E1.m1.4.4.4.3.3.3.1.1.1.1.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.1"></minus><cn type="integer" id="S3.E1.m1.4.4.4.3.3.3.1.1.1.2.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.2">1</cn><apply id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3">subscript</csymbol><apply id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2"><ci id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.1.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.1">¯</ci><ci id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.2.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.2.2">𝛼</ci></apply><ci id="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.3.cmml" xref="S3.E1.m1.4.4.4.3.3.3.1.1.1.3.3">𝑡</ci></apply></apply><ci id="S3.E1.m1.4.4.4.3.3.3.3.cmml" xref="S3.E1.m1.4.4.4.3.3.3.3">𝐈</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\displaystyle q(x_{t}|x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{\alpha}_{t})\mathbf{I})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.2" class="ltx_p">here constants <math id="S3.SS3.p2.2.m1.1" class="ltx_Math" alttext="\bar{\alpha}_{t}" display="inline"><semantics id="S3.SS3.p2.2.m1.1a"><msub id="S3.SS3.p2.2.m1.1.1" xref="S3.SS3.p2.2.m1.1.1.cmml"><mover accent="true" id="S3.SS3.p2.2.m1.1.1.2" xref="S3.SS3.p2.2.m1.1.1.2.cmml"><mi id="S3.SS3.p2.2.m1.1.1.2.2" xref="S3.SS3.p2.2.m1.1.1.2.2.cmml">α</mi><mo id="S3.SS3.p2.2.m1.1.1.2.1" xref="S3.SS3.p2.2.m1.1.1.2.1.cmml">¯</mo></mover><mi id="S3.SS3.p2.2.m1.1.1.3" xref="S3.SS3.p2.2.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m1.1b"><apply id="S3.SS3.p2.2.m1.1.1.cmml" xref="S3.SS3.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m1.1.1.1.cmml" xref="S3.SS3.p2.2.m1.1.1">subscript</csymbol><apply id="S3.SS3.p2.2.m1.1.1.2.cmml" xref="S3.SS3.p2.2.m1.1.1.2"><ci id="S3.SS3.p2.2.m1.1.1.2.1.cmml" xref="S3.SS3.p2.2.m1.1.1.2.1">¯</ci><ci id="S3.SS3.p2.2.m1.1.1.2.2.cmml" xref="S3.SS3.p2.2.m1.1.1.2.2">𝛼</ci></apply><ci id="S3.SS3.p2.2.m1.1.1.3.cmml" xref="S3.SS3.p2.2.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m1.1c">\bar{\alpha}_{t}</annotation></semantics></math> are hyperparameters. By applying the reparameterization trick, we can sample</p>
<table id="A7.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon_{t}" display="inline"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">x</mi><mi id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">t</mi></msub><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mrow id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><msqrt id="S3.E2.m1.1.1.3.2.2" xref="S3.E2.m1.1.1.3.2.2.cmml"><msub id="S3.E2.m1.1.1.3.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.cmml"><mover accent="true" id="S3.E2.m1.1.1.3.2.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.2.cmml"><mi id="S3.E2.m1.1.1.3.2.2.2.2.2" xref="S3.E2.m1.1.1.3.2.2.2.2.2.cmml">α</mi><mo id="S3.E2.m1.1.1.3.2.2.2.2.1" xref="S3.E2.m1.1.1.3.2.2.2.2.1.cmml">¯</mo></mover><mi id="S3.E2.m1.1.1.3.2.2.2.3" xref="S3.E2.m1.1.1.3.2.2.2.3.cmml">t</mi></msub></msqrt><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.2.1" xref="S3.E2.m1.1.1.3.2.1.cmml">​</mo><msub id="S3.E2.m1.1.1.3.2.3" xref="S3.E2.m1.1.1.3.2.3.cmml"><mi id="S3.E2.m1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.3.2.3.2.cmml">x</mi><mn id="S3.E2.m1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.3.2.3.3.cmml">0</mn></msub></mrow><mo id="S3.E2.m1.1.1.3.1" xref="S3.E2.m1.1.1.3.1.cmml">+</mo><mrow id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><msqrt id="S3.E2.m1.1.1.3.3.2" xref="S3.E2.m1.1.1.3.3.2.cmml"><mrow id="S3.E2.m1.1.1.3.3.2.2" xref="S3.E2.m1.1.1.3.3.2.2.cmml"><mn id="S3.E2.m1.1.1.3.3.2.2.2" xref="S3.E2.m1.1.1.3.3.2.2.2.cmml">1</mn><mo id="S3.E2.m1.1.1.3.3.2.2.1" xref="S3.E2.m1.1.1.3.3.2.2.1.cmml">−</mo><msub id="S3.E2.m1.1.1.3.3.2.2.3" xref="S3.E2.m1.1.1.3.3.2.2.3.cmml"><mover accent="true" id="S3.E2.m1.1.1.3.3.2.2.3.2" xref="S3.E2.m1.1.1.3.3.2.2.3.2.cmml"><mi id="S3.E2.m1.1.1.3.3.2.2.3.2.2" xref="S3.E2.m1.1.1.3.3.2.2.3.2.2.cmml">α</mi><mo id="S3.E2.m1.1.1.3.3.2.2.3.2.1" xref="S3.E2.m1.1.1.3.3.2.2.3.2.1.cmml">¯</mo></mover><mi id="S3.E2.m1.1.1.3.3.2.2.3.3" xref="S3.E2.m1.1.1.3.3.2.2.3.3.cmml">t</mi></msub></mrow></msqrt><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.3.3.1" xref="S3.E2.m1.1.1.3.3.1.cmml">​</mo><msub id="S3.E2.m1.1.1.3.3.3" xref="S3.E2.m1.1.1.3.3.3.cmml"><mi id="S3.E2.m1.1.1.3.3.3.2" xref="S3.E2.m1.1.1.3.3.3.2.cmml">ϵ</mi><mi id="S3.E2.m1.1.1.3.3.3.3" xref="S3.E2.m1.1.1.3.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">𝑥</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">𝑡</ci></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><plus id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><times id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2.1"></times><apply id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2"><root id="S3.E2.m1.1.1.3.2.2a.cmml" xref="S3.E2.m1.1.1.3.2.2"></root><apply id="S3.E2.m1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2.2">subscript</csymbol><apply id="S3.E2.m1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2"><ci id="S3.E2.m1.1.1.3.2.2.2.2.1.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2.1">¯</ci><ci id="S3.E2.m1.1.1.3.2.2.2.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2.2.2.2">𝛼</ci></apply><ci id="S3.E2.m1.1.1.3.2.2.2.3.cmml" xref="S3.E2.m1.1.1.3.2.2.2.3">𝑡</ci></apply></apply><apply id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.3.2.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.3.2.3.2">𝑥</ci><cn type="integer" id="S3.E2.m1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.3.2.3.3">0</cn></apply></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><times id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.1"></times><apply id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2"><root id="S3.E2.m1.1.1.3.3.2a.cmml" xref="S3.E2.m1.1.1.3.3.2"></root><apply id="S3.E2.m1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2"><minus id="S3.E2.m1.1.1.3.3.2.2.1.cmml" xref="S3.E2.m1.1.1.3.3.2.2.1"></minus><cn type="integer" id="S3.E2.m1.1.1.3.3.2.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2.2">1</cn><apply id="S3.E2.m1.1.1.3.3.2.2.3.cmml" xref="S3.E2.m1.1.1.3.3.2.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.2.2.3.1.cmml" xref="S3.E2.m1.1.1.3.3.2.2.3">subscript</csymbol><apply id="S3.E2.m1.1.1.3.3.2.2.3.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2.3.2"><ci id="S3.E2.m1.1.1.3.3.2.2.3.2.1.cmml" xref="S3.E2.m1.1.1.3.3.2.2.3.2.1">¯</ci><ci id="S3.E2.m1.1.1.3.3.2.2.3.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2.3.2.2">𝛼</ci></apply><ci id="S3.E2.m1.1.1.3.3.2.2.3.3.cmml" xref="S3.E2.m1.1.1.3.3.2.2.3.3">𝑡</ci></apply></apply></apply><apply id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3.3.2">italic-ϵ</ci><ci id="S3.E2.m1.1.1.3.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle x_{t}=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon_{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS3.p2.12" class="ltx_p">During training, <math id="S3.SS3.p2.3.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p2.3.m1.1a"><mi id="S3.SS3.p2.3.m1.1.1" xref="S3.SS3.p2.3.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m1.1b"><ci id="S3.SS3.p2.3.m1.1.1.cmml" xref="S3.SS3.p2.3.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m1.1c">t</annotation></semantics></math> is randomly sampled in <math id="S3.SS3.p2.4.m2.2" class="ltx_Math" alttext="[0,N]" display="inline"><semantics id="S3.SS3.p2.4.m2.2a"><mrow id="S3.SS3.p2.4.m2.2.3.2" xref="S3.SS3.p2.4.m2.2.3.1.cmml"><mo stretchy="false" id="S3.SS3.p2.4.m2.2.3.2.1" xref="S3.SS3.p2.4.m2.2.3.1.cmml">[</mo><mn id="S3.SS3.p2.4.m2.1.1" xref="S3.SS3.p2.4.m2.1.1.cmml">0</mn><mo id="S3.SS3.p2.4.m2.2.3.2.2" xref="S3.SS3.p2.4.m2.2.3.1.cmml">,</mo><mi id="S3.SS3.p2.4.m2.2.2" xref="S3.SS3.p2.4.m2.2.2.cmml">N</mi><mo stretchy="false" id="S3.SS3.p2.4.m2.2.3.2.3" xref="S3.SS3.p2.4.m2.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.4.m2.2b"><interval closure="closed" id="S3.SS3.p2.4.m2.2.3.1.cmml" xref="S3.SS3.p2.4.m2.2.3.2"><cn type="integer" id="S3.SS3.p2.4.m2.1.1.cmml" xref="S3.SS3.p2.4.m2.1.1">0</cn><ci id="S3.SS3.p2.4.m2.2.2.cmml" xref="S3.SS3.p2.4.m2.2.2">𝑁</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.4.m2.2c">[0,N]</annotation></semantics></math> (<math id="S3.SS3.p2.5.m3.1" class="ltx_Math" alttext="N=1000" display="inline"><semantics id="S3.SS3.p2.5.m3.1a"><mrow id="S3.SS3.p2.5.m3.1.1" xref="S3.SS3.p2.5.m3.1.1.cmml"><mi id="S3.SS3.p2.5.m3.1.1.2" xref="S3.SS3.p2.5.m3.1.1.2.cmml">N</mi><mo id="S3.SS3.p2.5.m3.1.1.1" xref="S3.SS3.p2.5.m3.1.1.1.cmml">=</mo><mn id="S3.SS3.p2.5.m3.1.1.3" xref="S3.SS3.p2.5.m3.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.5.m3.1b"><apply id="S3.SS3.p2.5.m3.1.1.cmml" xref="S3.SS3.p2.5.m3.1.1"><eq id="S3.SS3.p2.5.m3.1.1.1.cmml" xref="S3.SS3.p2.5.m3.1.1.1"></eq><ci id="S3.SS3.p2.5.m3.1.1.2.cmml" xref="S3.SS3.p2.5.m3.1.1.2">𝑁</ci><cn type="integer" id="S3.SS3.p2.5.m3.1.1.3.cmml" xref="S3.SS3.p2.5.m3.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.5.m3.1c">N=1000</annotation></semantics></math> in  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>) for the model to predict the added noise <math id="S3.SS3.p2.6.m4.1" class="ltx_Math" alttext="\epsilon_{t}" display="inline"><semantics id="S3.SS3.p2.6.m4.1a"><msub id="S3.SS3.p2.6.m4.1.1" xref="S3.SS3.p2.6.m4.1.1.cmml"><mi id="S3.SS3.p2.6.m4.1.1.2" xref="S3.SS3.p2.6.m4.1.1.2.cmml">ϵ</mi><mi id="S3.SS3.p2.6.m4.1.1.3" xref="S3.SS3.p2.6.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.6.m4.1b"><apply id="S3.SS3.p2.6.m4.1.1.cmml" xref="S3.SS3.p2.6.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.6.m4.1.1.1.cmml" xref="S3.SS3.p2.6.m4.1.1">subscript</csymbol><ci id="S3.SS3.p2.6.m4.1.1.2.cmml" xref="S3.SS3.p2.6.m4.1.1.2">italic-ϵ</ci><ci id="S3.SS3.p2.6.m4.1.1.3.cmml" xref="S3.SS3.p2.6.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.6.m4.1c">\epsilon_{t}</annotation></semantics></math>, where <math id="S3.SS3.p2.7.m5.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="S3.SS3.p2.7.m5.1a"><msub id="S3.SS3.p2.7.m5.1.1" xref="S3.SS3.p2.7.m5.1.1.cmml"><mi id="S3.SS3.p2.7.m5.1.1.2" xref="S3.SS3.p2.7.m5.1.1.2.cmml">x</mi><mn id="S3.SS3.p2.7.m5.1.1.3" xref="S3.SS3.p2.7.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.7.m5.1b"><apply id="S3.SS3.p2.7.m5.1.1.cmml" xref="S3.SS3.p2.7.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.7.m5.1.1.1.cmml" xref="S3.SS3.p2.7.m5.1.1">subscript</csymbol><ci id="S3.SS3.p2.7.m5.1.1.2.cmml" xref="S3.SS3.p2.7.m5.1.1.2">𝑥</ci><cn type="integer" id="S3.SS3.p2.7.m5.1.1.3.cmml" xref="S3.SS3.p2.7.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.7.m5.1c">x_{0}</annotation></semantics></math> denotes for the clear nature image and <math id="S3.SS3.p2.8.m6.1" class="ltx_Math" alttext="x_{N}" display="inline"><semantics id="S3.SS3.p2.8.m6.1a"><msub id="S3.SS3.p2.8.m6.1.1" xref="S3.SS3.p2.8.m6.1.1.cmml"><mi id="S3.SS3.p2.8.m6.1.1.2" xref="S3.SS3.p2.8.m6.1.1.2.cmml">x</mi><mi id="S3.SS3.p2.8.m6.1.1.3" xref="S3.SS3.p2.8.m6.1.1.3.cmml">N</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.8.m6.1b"><apply id="S3.SS3.p2.8.m6.1.1.cmml" xref="S3.SS3.p2.8.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.8.m6.1.1.1.cmml" xref="S3.SS3.p2.8.m6.1.1">subscript</csymbol><ci id="S3.SS3.p2.8.m6.1.1.2.cmml" xref="S3.SS3.p2.8.m6.1.1.2">𝑥</ci><ci id="S3.SS3.p2.8.m6.1.1.3.cmml" xref="S3.SS3.p2.8.m6.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.8.m6.1c">x_{N}</annotation></semantics></math> denotes for pure random noise. As depicted in Fig.<a href="#S3.F3" title="Figure 3 ‣ 3.3 Training Timestep Reschedule (TTR) ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, when <math id="S3.SS3.p2.9.m7.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p2.9.m7.1a"><mi id="S3.SS3.p2.9.m7.1.1" xref="S3.SS3.p2.9.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.9.m7.1b"><ci id="S3.SS3.p2.9.m7.1.1.cmml" xref="S3.SS3.p2.9.m7.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.9.m7.1c">t</annotation></semantics></math> is large, the denoising process primarily focuses on determining the global
low frequency(<math id="S3.SS3.p2.10.m8.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS3.p2.10.m8.1a"><mi id="S3.SS3.p2.10.m8.1.1" xref="S3.SS3.p2.10.m8.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.10.m8.1b"><ci id="S3.SS3.p2.10.m8.1.1.cmml" xref="S3.SS3.p2.10.m8.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.10.m8.1c">f</annotation></semantics></math>) content such as overall structure and shape. Conversely, when <math id="S3.SS3.p2.11.m9.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p2.11.m9.1a"><mi id="S3.SS3.p2.11.m9.1.1" xref="S3.SS3.p2.11.m9.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.11.m9.1b"><ci id="S3.SS3.p2.11.m9.1.1.cmml" xref="S3.SS3.p2.11.m9.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.11.m9.1c">t</annotation></semantics></math> is small, the denoising process is predominantly responsible for generating high <math id="S3.SS3.p2.12.m10.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS3.p2.12.m10.1a"><mi id="S3.SS3.p2.12.m10.1.1" xref="S3.SS3.p2.12.m10.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.12.m10.1b"><ci id="S3.SS3.p2.12.m10.1.1.cmml" xref="S3.SS3.p2.12.m10.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.12.m10.1c">f</annotation></semantics></math>
components such as texture.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.2" class="ltx_p"><span id="S3.SS3.p3.2.1" class="ltx_text ltx_font_bold">Simple implementation.</span> When adapting Stable Diffision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> for multi-view generation, the previous approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> changes the default scaled linear schedule into the linear schedule to emphasize more on early denoising stage for structural variation and view consistency. Inspired by this, we propose restricting the denoising time step of synthetic data during training. As small yet observable blur still exists in synthetic data with novel view generated by SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, we limit them to training diffusion model only with large <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">t</annotation></semantics></math>. This restricts the backpropagation of these synthetic data to focus on the low-frequency component of the image like the overall structure and shape that faithfully follow text prompts and consistency between different views. Small <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">t</annotation></semantics></math> values are only sampled on clear and physically consistent multi-view images rendered from Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and supplemented high-quality 2D images from SA-1B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, help model outcome high-quality images with more photo-realistic and varied texture details.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Settings</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Training data.</span>
For each set of 4-view images depicting the same object, obtained from both Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and generated by SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> or Zero123++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, we use MV-LLaVA to generate long descriptive captions with predicted quality score. Detailed quality check of MV-LLaVA is supplied in Sup. <a href="#A3" title="Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> and data analysis in Sup. <a href="#A2" title="Appendix B Data Statistics ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. In the end, we generate 200K 4-view image-text pairs on Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, 1000K 4-view image-text pairs from synthetic data from SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and Zero123++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. We also sample 35K HQ SA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> data with captions from ShareGPT4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.6" class="ltx_p"><span id="S4.SS1.p2.6.1" class="ltx_text ltx_font_bold">Training details.</span>

We test our framework directly on the text-to-multi-view diffusion model. We fine-tune PixArt-<math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\alpha</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> with backbone DiT-XL/2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> model on the data as mentioned earlier. Similar to Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, we train the diffusion model directly on 4-view images naturally arranged in a 2<math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mo id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><times id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">\times</annotation></semantics></math>2 grid. For 4 same view images from SA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we limit training time step <math id="S4.SS1.p2.3.m3.2" class="ltx_Math" alttext="t\in[0,50]" display="inline"><semantics id="S4.SS1.p2.3.m3.2a"><mrow id="S4.SS1.p2.3.m3.2.3" xref="S4.SS1.p2.3.m3.2.3.cmml"><mi id="S4.SS1.p2.3.m3.2.3.2" xref="S4.SS1.p2.3.m3.2.3.2.cmml">t</mi><mo id="S4.SS1.p2.3.m3.2.3.1" xref="S4.SS1.p2.3.m3.2.3.1.cmml">∈</mo><mrow id="S4.SS1.p2.3.m3.2.3.3.2" xref="S4.SS1.p2.3.m3.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.2.3.3.2.1" xref="S4.SS1.p2.3.m3.2.3.3.1.cmml">[</mo><mn id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml">0</mn><mo id="S4.SS1.p2.3.m3.2.3.3.2.2" xref="S4.SS1.p2.3.m3.2.3.3.1.cmml">,</mo><mn id="S4.SS1.p2.3.m3.2.2" xref="S4.SS1.p2.3.m3.2.2.cmml">50</mn><mo stretchy="false" id="S4.SS1.p2.3.m3.2.3.3.2.3" xref="S4.SS1.p2.3.m3.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.2b"><apply id="S4.SS1.p2.3.m3.2.3.cmml" xref="S4.SS1.p2.3.m3.2.3"><in id="S4.SS1.p2.3.m3.2.3.1.cmml" xref="S4.SS1.p2.3.m3.2.3.1"></in><ci id="S4.SS1.p2.3.m3.2.3.2.cmml" xref="S4.SS1.p2.3.m3.2.3.2">𝑡</ci><interval closure="closed" id="S4.SS1.p2.3.m3.2.3.3.1.cmml" xref="S4.SS1.p2.3.m3.2.3.3.2"><cn type="integer" id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">0</cn><cn type="integer" id="S4.SS1.p2.3.m3.2.2.cmml" xref="S4.SS1.p2.3.m3.2.2">50</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.2c">t\in[0,50]</annotation></semantics></math>. We limit synthetic multi-view images <math id="S4.SS1.p2.4.m4.2" class="ltx_Math" alttext="t\in[200,1000]" display="inline"><semantics id="S4.SS1.p2.4.m4.2a"><mrow id="S4.SS1.p2.4.m4.2.3" xref="S4.SS1.p2.4.m4.2.3.cmml"><mi id="S4.SS1.p2.4.m4.2.3.2" xref="S4.SS1.p2.4.m4.2.3.2.cmml">t</mi><mo id="S4.SS1.p2.4.m4.2.3.1" xref="S4.SS1.p2.4.m4.2.3.1.cmml">∈</mo><mrow id="S4.SS1.p2.4.m4.2.3.3.2" xref="S4.SS1.p2.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.4.m4.2.3.3.2.1" xref="S4.SS1.p2.4.m4.2.3.3.1.cmml">[</mo><mn id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml">200</mn><mo id="S4.SS1.p2.4.m4.2.3.3.2.2" xref="S4.SS1.p2.4.m4.2.3.3.1.cmml">,</mo><mn id="S4.SS1.p2.4.m4.2.2" xref="S4.SS1.p2.4.m4.2.2.cmml">1000</mn><mo stretchy="false" id="S4.SS1.p2.4.m4.2.3.3.2.3" xref="S4.SS1.p2.4.m4.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.2b"><apply id="S4.SS1.p2.4.m4.2.3.cmml" xref="S4.SS1.p2.4.m4.2.3"><in id="S4.SS1.p2.4.m4.2.3.1.cmml" xref="S4.SS1.p2.4.m4.2.3.1"></in><ci id="S4.SS1.p2.4.m4.2.3.2.cmml" xref="S4.SS1.p2.4.m4.2.3.2">𝑡</ci><interval closure="closed" id="S4.SS1.p2.4.m4.2.3.3.1.cmml" xref="S4.SS1.p2.4.m4.2.3.3.2"><cn type="integer" id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">200</cn><cn type="integer" id="S4.SS1.p2.4.m4.2.2.cmml" xref="S4.SS1.p2.4.m4.2.2">1000</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.2c">t\in[200,1000]</annotation></semantics></math>. Regarding 3D object-rendered images, we do not limit <math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mi id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><ci id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">t</annotation></semantics></math> but sample more frequently in the range <math id="S4.SS1.p2.6.m6.2" class="ltx_Math" alttext="[50,200]" display="inline"><semantics id="S4.SS1.p2.6.m6.2a"><mrow id="S4.SS1.p2.6.m6.2.3.2" xref="S4.SS1.p2.6.m6.2.3.1.cmml"><mo stretchy="false" id="S4.SS1.p2.6.m6.2.3.2.1" xref="S4.SS1.p2.6.m6.2.3.1.cmml">[</mo><mn id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml">50</mn><mo id="S4.SS1.p2.6.m6.2.3.2.2" xref="S4.SS1.p2.6.m6.2.3.1.cmml">,</mo><mn id="S4.SS1.p2.6.m6.2.2" xref="S4.SS1.p2.6.m6.2.2.cmml">200</mn><mo stretchy="false" id="S4.SS1.p2.6.m6.2.3.2.3" xref="S4.SS1.p2.6.m6.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.2b"><interval closure="closed" id="S4.SS1.p2.6.m6.2.3.1.cmml" xref="S4.SS1.p2.6.m6.2.3.2"><cn type="integer" id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1">50</cn><cn type="integer" id="S4.SS1.p2.6.m6.2.2.cmml" xref="S4.SS1.p2.6.m6.2.2">200</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.2c">[50,200]</annotation></semantics></math> as a complement. We set the total batch size to 1024 with the learning rate set to 8e-5 for 20K steps. Training is conducted on 32 NVIDIA A100-80G GPUs for 20 hours with Flan-T5-XXL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> text features and VAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> features pre-extracted.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Evaluation metrics.</span>

We primarily benchmark the quantitative results of our approach and other methods from two main dimensions: 1). <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_bold">Image-text alignment</span> measured by CLIP score and CLIP-R score indicating the prompt follow ability of text-to-multi-view (T2MV) diffusion model. 2). <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_bold">Quality of generated images</span> measured by FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Given the trend of decoupling multi-view image generation and sparse view reconstruction, we conduct tests separately on multi-view images by T2MV and rerendered images from generated 3D objects. To test the robustness and diversity of Bootstrap3D beyond prompts generated by GPT, we also collect real user prompts from public website, the details and test results are available in Sup. <a href="#A1" title="Appendix A Evaluation on wild prompts from real users ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Evaluation details.</span>
For CLIP-R Score and CLIP Score, we test on 110 text prompts from GPTeval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> using different CLIP models (i.e., CLIP-L/14 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and CLIP-bigG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>) following the same setting of Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Regarding the FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> test, as there is no golden standard for HQ 3D objects, we follow the similar evaluation idea of PlayGround2.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> (PG2.5) to use powerful T2I model generated images to form ground truth (GT) distribution. We use curated prompts to guide powerful PixArt and PG2.5 to generate high-quality CAD-style images with a single object in the pure background. Rembg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is adopted to create white background object-centric images. We use the method proposed in GPTeval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> to generate 3K prompts. For both PG-2.5 and PixArt, we generate 10 images for each prompt with different seeds, resulting in 30K images to form the GT distribution of high-quality CAD-style objects.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Comparing methods.</span> In addition to Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> and MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> as direct text-to-multi-view (T2MV) methods, we also adopt edge-cutting single image to multi-view (I2MV) methods SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and Zero123++<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>. For these methods, we condition the diffusion model on the single view image generated by PixArt (prompted to generate CAD-style single object-centric image). The result of the CLIP score is 3 times averaged with different seeds. For FID, we use 3 different seeds for each of the 3K prompts to generate 9K images to test the distance with GT high-quality images.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation of Multi-view Images</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.11.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Benchmark of CLIP and FID score of text-to-multi-view (T2MV) models<span id="S4.T1.2.1.1" class="ltx_text ltx_font_medium"> on generated 4 view images, CLIP score tests on 110 text prompts from GPTeval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> while FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> is measured with the distribution of 30K object-centric images generated by SOTA T2I models. For text-to-image-to-multi-view(T2I2MV), we input I2MV models with single view images generated by Pixart-<math id="S4.T1.2.1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T1.2.1.1.m1.1b"><mi id="S4.T1.2.1.1.m1.1.1" xref="S4.T1.2.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.1.1.m1.1c"><ci id="S4.T1.2.1.1.m1.1.1.cmml" xref="S4.T1.2.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.1.1.m1.1d">\alpha</annotation></semantics></math>, which superior single view image score is marked in <span id="S4.T1.2.1.1.1" class="ltx_text" style="background-color:#E6F8E0;">green</span>.</span></span></figcaption>
<div id="S4.T1.7" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:507.6pt;height:118.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.7pt,13.0pt) scale(0.82,0.82) ;">
<table id="S4.T1.7.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.5.3.3" class="ltx_tr">
<th id="S4.T1.5.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.5.3.3.4.1" class="ltx_text ltx_font_bold">Domain</span></th>
<th id="S4.T1.5.3.3.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.5.3.3.5.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S4.T1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">CLIP-R Score <math id="S4.T1.3.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.3.1.1.1.m1.1a"><mo stretchy="false" id="S4.T1.3.1.1.1.m1.1.1" xref="S4.T1.3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.1.1.1.m1.1b"><ci id="S4.T1.3.1.1.1.m1.1.1.cmml" xref="S4.T1.3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.4.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">CLIP Score <math id="S4.T1.4.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.4.2.2.2.m1.1a"><mo stretchy="false" id="S4.T1.4.2.2.2.m1.1.1" xref="S4.T1.4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.2.2.2.m1.1b"><ci id="S4.T1.4.2.2.2.m1.1.1.cmml" xref="S4.T1.4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</td>
<td id="S4.T1.5.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> <math id="S4.T1.5.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.5.3.3.3.m1.1a"><mo stretchy="false" id="S4.T1.5.3.3.3.m1.1.1" xref="S4.T1.5.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.3.3.3.m1.1b"><ci id="S4.T1.5.3.3.3.m1.1.1.cmml" xref="S4.T1.5.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.6.4.4" class="ltx_tr">
<td id="S4.T1.6.4.4.2" class="ltx_td ltx_align_center">CLIP-L/14</td>
<td id="S4.T1.6.4.4.3" class="ltx_td ltx_align_center">CLIP-bigG</td>
<td id="S4.T1.6.4.4.4" class="ltx_td ltx_align_center">CLIP-L/14</td>
<td id="S4.T1.6.4.4.5" class="ltx_td ltx_align_center">CLIP-bigG</td>
<td id="S4.T1.6.4.4.6" class="ltx_td ltx_align_center">PG2.5</td>
<td id="S4.T1.6.4.4.1" class="ltx_td ltx_align_center">PixArt-<math id="S4.T1.6.4.4.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T1.6.4.4.1.m1.1a"><mi id="S4.T1.6.4.4.1.m1.1.1" xref="S4.T1.6.4.4.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T1.6.4.4.1.m1.1b"><ci id="S4.T1.6.4.4.1.m1.1.1.cmml" xref="S4.T1.6.4.4.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.4.4.1.m1.1c">\alpha</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T1.7.5.5" class="ltx_tr" style="background-color:#E6F8E0;">
<th id="S4.T1.7.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.7.5.5.2.1" class="ltx_text" style="background-color:#E6F8E0;">T2I</span></th>
<th id="S4.T1.7.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.7.5.5.1.1" class="ltx_text" style="background-color:#E6F8E0;">PixArt-<math id="S4.T1.7.5.5.1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.T1.7.5.5.1.1.m1.1a"><mi mathbackground="#E6F8E0" id="S4.T1.7.5.5.1.1.m1.1.1" xref="S4.T1.7.5.5.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.T1.7.5.5.1.1.m1.1b"><ci id="S4.T1.7.5.5.1.1.m1.1.1.cmml" xref="S4.T1.7.5.5.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.5.5.1.1.m1.1c">\alpha</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span></th>
<td id="S4.T1.7.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.5.5.3.1" class="ltx_text" style="color:#A6A6A6;background-color:#E6F8E0;">96.1</span></td>
<td id="S4.T1.7.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.5.5.4.1" class="ltx_text" style="color:#A6A6A6;background-color:#E6F8E0;">94.7</span></td>
<td id="S4.T1.7.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.5.5.5.1" class="ltx_text" style="color:#A6A6A6;background-color:#E6F8E0;">25.9</span></td>
<td id="S4.T1.7.5.5.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.5.5.6.1" class="ltx_text" style="color:#A6A6A6;background-color:#E6F8E0;">41.5</span></td>
<td id="S4.T1.7.5.5.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.5.5.7.1" class="ltx_text" style="color:#A6A6A6;background-color:#E6F8E0;">20.7</span></td>
<td id="S4.T1.7.5.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.7.5.5.8.1" class="ltx_text" style="color:#A6A6A6;background-color:#E6F8E0;">5.4</span></td>
</tr>
<tr id="S4.T1.7.5.6.1" class="ltx_tr">
<th id="S4.T1.7.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S4.T1.7.5.6.1.1.1" class="ltx_text">T2I2MV</span></th>
<th id="S4.T1.7.5.6.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</th>
<td id="S4.T1.7.5.6.1.3" class="ltx_td ltx_align_center ltx_border_t">78.8</td>
<td id="S4.T1.7.5.6.1.4" class="ltx_td ltx_align_center ltx_border_t">81.3</td>
<td id="S4.T1.7.5.6.1.5" class="ltx_td ltx_align_center ltx_border_t">24.7</td>
<td id="S4.T1.7.5.6.1.6" class="ltx_td ltx_align_center ltx_border_t">37.3</td>
<td id="S4.T1.7.5.6.1.7" class="ltx_td ltx_align_center ltx_border_t">55.7</td>
<td id="S4.T1.7.5.6.1.8" class="ltx_td ltx_align_center ltx_border_t">54.2</td>
</tr>
<tr id="S4.T1.7.5.7.2" class="ltx_tr">
<th id="S4.T1.7.5.7.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</th>
<td id="S4.T1.7.5.7.2.2" class="ltx_td ltx_align_center">78.0</td>
<td id="S4.T1.7.5.7.2.3" class="ltx_td ltx_align_center">84.5</td>
<td id="S4.T1.7.5.7.2.4" class="ltx_td ltx_align_center">24.2</td>
<td id="S4.T1.7.5.7.2.5" class="ltx_td ltx_align_center">36.9</td>
<td id="S4.T1.7.5.7.2.6" class="ltx_td ltx_align_center">53.2</td>
<td id="S4.T1.7.5.7.2.7" class="ltx_td ltx_align_center">49.3</td>
</tr>
<tr id="S4.T1.7.5.8.3" class="ltx_tr">
<th id="S4.T1.7.5.8.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T1.7.5.8.3.1.1" class="ltx_text">T2MV</span></th>
<th id="S4.T1.7.5.8.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (unofficial)</th>
<td id="S4.T1.7.5.8.3.3" class="ltx_td ltx_align_center ltx_border_t">83.6</td>
<td id="S4.T1.7.5.8.3.4" class="ltx_td ltx_align_center ltx_border_t">91.1</td>
<td id="S4.T1.7.5.8.3.5" class="ltx_td ltx_align_center ltx_border_t">25.6</td>
<td id="S4.T1.7.5.8.3.6" class="ltx_td ltx_align_center ltx_border_t">39.2</td>
<td id="S4.T1.7.5.8.3.7" class="ltx_td ltx_align_center ltx_border_t">83.2</td>
<td id="S4.T1.7.5.8.3.8" class="ltx_td ltx_align_center ltx_border_t">77.9</td>
</tr>
<tr id="S4.T1.7.5.9.4" class="ltx_tr">
<th id="S4.T1.7.5.9.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>
</th>
<td id="S4.T1.7.5.9.4.2" class="ltx_td ltx_align_center">84.8</td>
<td id="S4.T1.7.5.9.4.3" class="ltx_td ltx_align_center">89.3</td>
<td id="S4.T1.7.5.9.4.4" class="ltx_td ltx_align_center">25.5</td>
<td id="S4.T1.7.5.9.4.5" class="ltx_td ltx_align_center">38.4</td>
<td id="S4.T1.7.5.9.4.6" class="ltx_td ltx_align_center">60.2</td>
<td id="S4.T1.7.5.9.4.7" class="ltx_td ltx_align_center">59.2</td>
</tr>
<tr id="S4.T1.7.5.10.5" class="ltx_tr">
<th id="S4.T1.7.5.10.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.7.5.10.5.1.1" class="ltx_text" style="background-color:#D4E6F1;">Bootstrap3D</span></th>
<td id="S4.T1.7.5.10.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.7.5.10.5.2.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">88.8</span></td>
<td id="S4.T1.7.5.10.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.7.5.10.5.3.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">92.5</span></td>
<td id="S4.T1.7.5.10.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.7.5.10.5.4.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">25.8</span></td>
<td id="S4.T1.7.5.10.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.7.5.10.5.5.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">40.1</span></td>
<td id="S4.T1.7.5.10.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.7.5.10.5.6.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">42.4</span></td>
<td id="S4.T1.7.5.10.5.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.7.5.10.5.7.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">31.0</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">As illustrated in Tab.<a href="#S4.T1" title="Table 1 ‣ 4.2 Evaluation of Multi-view Images ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, in addition to 4 view images generated by the multi-view diffusion model. Compared to other methods, the T2MV diffusion model trained by our framework yields the best results both according to image-text alignment and image quality. It is worth noting that in the T2MV task, T2I2MV suffers a huge drop due to the domain gap between T2I-generated images and training images as well as the use of Rembg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Qualitative results of real user cases are available in Sup. <a href="#A1" title="Appendix A Evaluation on wild prompts from real users ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2406.00093/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="327" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Bootstrap3D generates 3D objects compared to other edge-cutting methods<span id="S4.F4.4.2.1" class="ltx_text ltx_font_medium"> given text prompt. More results with higher resolution are available in Sup.<a href="#A6.SS1" title="F.1 Comparison with Other Methods ‣ Appendix F More Results Visualization ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F.1</span></a>.</span></span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation of Generated 3D Objects</h3>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.6.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Benchmark of CLIP and FID score of generated 3D objects<span id="S4.T2.7.2.1" class="ltx_text ltx_font_medium"> based on rendered 9 view images. *While MVDream is tested on 200 generated objects for FID test using SDS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>, other methods are tested on 1000 objects using GRM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> as sparse view reconstruction model.</span></span></figcaption>
<div id="S4.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:696.9pt;height:107.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.5pt,9.4pt) scale(0.85,0.85) ;">
<table id="S4.T2.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.3.3" class="ltx_tr">
<th id="S4.T2.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T2.3.3.3.4.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">CLIP-R Score <math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">CLIP Score <math id="S4.T2.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.2.m1.1b"><ci id="S4.T2.2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T2.3.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> <math id="S4.T2.3.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T2.3.3.3.3.m1.1.1" xref="S4.T2.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.3.m1.1b"><ci id="S4.T2.3.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.3.m1.1c">\downarrow</annotation></semantics></math>
</th>
</tr>
<tr id="S4.T2.3.3.4.1" class="ltx_tr">
<th id="S4.T2.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIP-L/14</th>
<th id="S4.T2.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIP-bigG</th>
<th id="S4.T2.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIP-L/14</th>
<th id="S4.T2.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CLIP-bigG</th>
<th id="S4.T2.3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PG2.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>
</th>
<th id="S4.T2.3.3.4.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">PixArt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.3.5.1" class="ltx_tr">
<th id="S4.T2.3.3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (unofficial)</th>
<td id="S4.T2.3.3.5.1.2" class="ltx_td ltx_align_center ltx_border_t">81.7</td>
<td id="S4.T2.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_t">89.4</td>
<td id="S4.T2.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_t">24.8</td>
<td id="S4.T2.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_t">37.1</td>
<td id="S4.T2.3.3.5.1.6" class="ltx_td ltx_align_center ltx_border_t">85.4</td>
<td id="S4.T2.3.3.5.1.7" class="ltx_td ltx_align_center ltx_border_t">80.3</td>
</tr>
<tr id="S4.T2.3.3.6.2" class="ltx_tr">
<th id="S4.T2.3.3.6.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>*</th>
<td id="S4.T2.3.3.6.2.2" class="ltx_td ltx_align_center">85.2</td>
<td id="S4.T2.3.3.6.2.3" class="ltx_td ltx_align_center">90.8</td>
<td id="S4.T2.3.3.6.2.4" class="ltx_td ltx_align_center"><span id="S4.T2.3.3.6.2.4.1" class="ltx_text ltx_font_bold">26.1</span></td>
<td id="S4.T2.3.3.6.2.5" class="ltx_td ltx_align_center">39.4</td>
<td id="S4.T2.3.3.6.2.6" class="ltx_td ltx_align_center">57.8</td>
<td id="S4.T2.3.3.6.2.7" class="ltx_td ltx_align_center">56.7</td>
</tr>
<tr id="S4.T2.3.3.7.3" class="ltx_tr">
<th id="S4.T2.3.3.7.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>
</th>
<td id="S4.T2.3.3.7.3.2" class="ltx_td ltx_align_center">74.1</td>
<td id="S4.T2.3.3.7.3.3" class="ltx_td ltx_align_center">82.8</td>
<td id="S4.T2.3.3.7.3.4" class="ltx_td ltx_align_center">23.4</td>
<td id="S4.T2.3.3.7.3.5" class="ltx_td ltx_align_center">34.1</td>
<td id="S4.T2.3.3.7.3.6" class="ltx_td ltx_align_center">68.4</td>
<td id="S4.T2.3.3.7.3.7" class="ltx_td ltx_align_center">69.1</td>
</tr>
<tr id="S4.T2.3.3.8.4" class="ltx_tr">
<th id="S4.T2.3.3.8.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</th>
<td id="S4.T2.3.3.8.4.2" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T2.3.3.8.4.3" class="ltx_td ltx_align_center">80.3</td>
<td id="S4.T2.3.3.8.4.4" class="ltx_td ltx_align_center">22.3</td>
<td id="S4.T2.3.3.8.4.5" class="ltx_td ltx_align_center">34.5</td>
<td id="S4.T2.3.3.8.4.6" class="ltx_td ltx_align_center">69.3</td>
<td id="S4.T2.3.3.8.4.7" class="ltx_td ltx_align_center">72.4</td>
</tr>
<tr id="S4.T2.3.3.9.5" class="ltx_tr" style="background-color:#D4E6F1;">
<th id="S4.T2.3.3.9.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.3.3.9.5.1.1" class="ltx_text" style="background-color:#D4E6F1;">Bootstrap3D</span></th>
<td id="S4.T2.3.3.9.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.9.5.2.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">86.3</span></td>
<td id="S4.T2.3.3.9.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.9.5.3.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">91.6</span></td>
<td id="S4.T2.3.3.9.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.9.5.4.1" class="ltx_text" style="background-color:#D4E6F1;">25.9</span></td>
<td id="S4.T2.3.3.9.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.9.5.5.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">39.7</span></td>
<td id="S4.T2.3.3.9.5.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.9.5.6.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">51.2</span></td>
<td id="S4.T2.3.3.9.5.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.3.3.9.5.7.1" class="ltx_text ltx_font_bold" style="background-color:#D4E6F1;">50.7</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2406.00093/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="300" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Bootstrap3D can generate 3D objects with different style with more precise prompt control<span id="S4.F5.4.2.1" class="ltx_text ltx_font_medium"> compared to other edge-cutting methods. More results with thorough render views are available in Sup.<a href="#A6.SS2" title="F.2 Visualization of Generated objects with Different Styles ‣ Appendix F More Results Visualization ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F.2</span></a>.</span></span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">View consistency is another crucial factor in reconstructing reasonable 3D objects. Miss alignment between different views can lead to blurred areas in reconstructed objects by large reconstruction model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>. To assess the final quality of the generated 3D object, we employ GRM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> to reconstruct the object given sparse view images generated in Sec. <a href="#S4.SS2" title="4.2 Evaluation of Multi-view Images ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>. We render 9 view images evenly in orbit for each object and evaluate the image-text alignment and image quality. As reported in Tab. <a href="#S4.T2" title="Table 2 ‣ 4.3 Evaluation of Generated 3D Objects ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Bootstrap3D, after conditioning GRM on 4 view images, can generate the best 3D objects both according to image-text alignment and image quality. GPT-4V based human-aligned evaluation based on GPTeval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> is supplied in Sup. <a href="#A5" title="Appendix E GPT-4V based 3D Object Generation Evaluation. ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We also present visualizations of some results in Fig.<a href="#S4.F4" title="Figure 4 ‣ 4.2 Evaluation of Multi-view Images ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Compared with GRM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, Bootstrap3D can generate objects with more diversity and prompt follow ability. Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> here uses the text-to-image-to-multi-view pipeline, with the first single view image generated by PixArt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. As shown in the first column of Fig.<a href="#S4.F4" title="Figure 4 ‣ 4.2 Evaluation of Multi-view Images ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Although the first image may be well aligned with the given text prompt. The final 3D object may be compromised due to the limitations of its multi-view diffusion model as it is also fine-tuned on Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> only. The domain gap between the rendered Objaverse image and images generated by the T2I model limits the outcome of the text-to-single image, single-view-to-3D pipeline.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.6" class="ltx_p"><span id="S4.SS4.p1.6.1" class="ltx_text ltx_font_bold">Training Timestep Reschedule (TTR)</span> is proposed in <a href="#S3.SS3" title="3.3 Training Timestep Reschedule (TTR) ‣ 3 Methods ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> to better integrate different types of data. The training time step of synthetic data is restricted in <math id="S4.SS4.p1.1.m1.2" class="ltx_Math" alttext="[T,1000]" display="inline"><semantics id="S4.SS4.p1.1.m1.2a"><mrow id="S4.SS4.p1.1.m1.2.3.2" xref="S4.SS4.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS4.p1.1.m1.2.3.2.1" xref="S4.SS4.p1.1.m1.2.3.1.cmml">[</mo><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">T</mi><mo id="S4.SS4.p1.1.m1.2.3.2.2" xref="S4.SS4.p1.1.m1.2.3.1.cmml">,</mo><mn id="S4.SS4.p1.1.m1.2.2" xref="S4.SS4.p1.1.m1.2.2.cmml">1000</mn><mo stretchy="false" id="S4.SS4.p1.1.m1.2.3.2.3" xref="S4.SS4.p1.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.2b"><interval closure="closed" id="S4.SS4.p1.1.m1.2.3.1.cmml" xref="S4.SS4.p1.1.m1.2.3.2"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑇</ci><cn type="integer" id="S4.SS4.p1.1.m1.2.2.cmml" xref="S4.SS4.p1.1.m1.2.2">1000</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.2c">[T,1000]</annotation></semantics></math>, where <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">T</annotation></semantics></math> is a hyper-parameter to be set in training. We demonstrate the effect of the time-step limit in Fig.<a href="#S4.F6" title="Figure 6 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, where the bar in the middle is the value of <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mi id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><ci id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">T</annotation></semantics></math>. When <math id="S4.SS4.p1.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS4.p1.4.m4.1a"><mi id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><ci id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">T</annotation></semantics></math> is large, namely synthetic data won’t affect more time-step at the end of the denoising process, Synthetic data has less influence on the denoising process towards the end, which leads to better view consistency but lower prompt-following ability. Conversely, if <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><mi id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><ci id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">T</annotation></semantics></math> is small, the denoised result better follows the given text prompt but blurring becomes much more severe. In summary, there is a trade-off in injecting synthetic data into the training process: better image-text alignment comes at the cost of worse view consistency and increased blurring. Ultimately, we set <math id="S4.SS4.p1.6.m6.1" class="ltx_Math" alttext="T=200" display="inline"><semantics id="S4.SS4.p1.6.m6.1a"><mrow id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml"><mi id="S4.SS4.p1.6.m6.1.1.2" xref="S4.SS4.p1.6.m6.1.1.2.cmml">T</mi><mo id="S4.SS4.p1.6.m6.1.1.1" xref="S4.SS4.p1.6.m6.1.1.1.cmml">=</mo><mn id="S4.SS4.p1.6.m6.1.1.3" xref="S4.SS4.p1.6.m6.1.1.3.cmml">200</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><apply id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1"><eq id="S4.SS4.p1.6.m6.1.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1.1"></eq><ci id="S4.SS4.p1.6.m6.1.1.2.cmml" xref="S4.SS4.p1.6.m6.1.1.2">𝑇</ci><cn type="integer" id="S4.SS4.p1.6.m6.1.1.3.cmml" xref="S4.SS4.p1.6.m6.1.1.3">200</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">T=200</annotation></semantics></math> based on empirical study.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study of proposed components and quantity of synthetic data.<span id="S4.T3.4.2.1" class="ltx_text ltx_font_medium"> with CLIP-R Score represents image-text alignment and FID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> represents image quality.</span></span></figcaption>
<div id="S4.T3.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:452.4pt;height:126.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.8pt,8.6pt) scale(0.88,0.88) ;">
<table id="S4.T3.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.1.1.1" class="ltx_tr">
<th id="S4.T3.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.5.1.1.1.1.1" class="ltx_text">Methods</span></th>
<th id="S4.T3.5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Multi-view Image</th>
<th id="S4.T3.5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Generated Object</th>
</tr>
<tr id="S4.T3.5.1.2.2" class="ltx_tr">
<th id="S4.T3.5.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">CLIP-R Score</th>
<th id="S4.T3.5.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">FID PG-2.5</th>
<th id="S4.T3.5.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">CLIP-R Score</th>
<th id="S4.T3.5.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">FID PG-2.5</th>
</tr>
<tr id="S4.T3.5.1.3.3" class="ltx_tr">
<th id="S4.T3.5.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (unofficial)</th>
<th id="S4.T3.5.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">83.6</th>
<th id="S4.T3.5.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">83.2</th>
<th id="S4.T3.5.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">81.7</th>
<th id="S4.T3.5.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">85.4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.5.1.4.1" class="ltx_tr">
<th id="S4.T3.5.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cap3D only</th>
<td id="S4.T3.5.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t">77.9</td>
<td id="S4.T3.5.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t">101.3</td>
<td id="S4.T3.5.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t">74.6</td>
<td id="S4.T3.5.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t">120.4</td>
</tr>
<tr id="S4.T3.5.1.5.2" class="ltx_tr">
<th id="S4.T3.5.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Cap3D + Synthetic Image (100k) w/o TTR</th>
<td id="S4.T3.5.1.5.2.2" class="ltx_td ltx_align_center">81.5</td>
<td id="S4.T3.5.1.5.2.3" class="ltx_td ltx_align_center">92.0</td>
<td id="S4.T3.5.1.5.2.4" class="ltx_td ltx_align_center">71.2</td>
<td id="S4.T3.5.1.5.2.5" class="ltx_td ltx_align_center">134.6</td>
</tr>
<tr id="S4.T3.5.1.6.3" class="ltx_tr">
<th id="S4.T3.5.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Cap3D + Synthetic Image (100k) w/ TTR</th>
<td id="S4.T3.5.1.6.3.2" class="ltx_td ltx_align_center">83.3</td>
<td id="S4.T3.5.1.6.3.3" class="ltx_td ltx_align_center">60.8</td>
<td id="S4.T3.5.1.6.3.4" class="ltx_td ltx_align_center">80.2</td>
<td id="S4.T3.5.1.6.3.5" class="ltx_td ltx_align_center">70.6</td>
</tr>
<tr id="S4.T3.5.1.7.4" class="ltx_tr">
<th id="S4.T3.5.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Dense recaption + Synthetic Image (100k)</th>
<td id="S4.T3.5.1.7.4.2" class="ltx_td ltx_align_center">87.4</td>
<td id="S4.T3.5.1.7.4.3" class="ltx_td ltx_align_center">50.2</td>
<td id="S4.T3.5.1.7.4.4" class="ltx_td ltx_align_center">85.1</td>
<td id="S4.T3.5.1.7.4.5" class="ltx_td ltx_align_center">50.9</td>
</tr>
<tr id="S4.T3.5.1.8.5" class="ltx_tr">
<th id="S4.T3.5.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Dense recaption + Synthetic Image (500k)</th>
<td id="S4.T3.5.1.8.5.2" class="ltx_td ltx_align_center ltx_border_bb">88.8</td>
<td id="S4.T3.5.1.8.5.3" class="ltx_td ltx_align_center ltx_border_bb">42.4</td>
<td id="S4.T3.5.1.8.5.4" class="ltx_td ltx_align_center ltx_border_bb">86.3</td>
<td id="S4.T3.5.1.8.5.5" class="ltx_td ltx_align_center ltx_border_bb">51.2</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2406.00093/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="255" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.5.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablation study of training time reschedule (TTR)<span id="S4.F6.2.1.1" class="ltx_text ltx_font_medium"> demonstrates a trade-off between image-text alignment and image quality with different <math id="S4.F6.2.1.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S4.F6.2.1.1.m1.1b"><mi id="S4.F6.2.1.1.m1.1.1" xref="S4.F6.2.1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.F6.2.1.1.m1.1c"><ci id="S4.F6.2.1.1.m1.1.1.cmml" xref="S4.F6.2.1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F6.2.1.1.m1.1d">t</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Synthetic data and dense captioning</span> are proposed in our work to achieve high-quality images and better image-text alignment. We ablate their effects and the importance of data quantity in Tab. <a href="#S4.T3" title="Table 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Direct use of synthetic data without Training Timestep Reschedule (TTR) can cause severe blurs and deformation in final outcome. With the help of TTR, the mixture of data can not only improve image-text alignment but also maintain view consistency. Replacing Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>’s caption with MV-LLaVA’s dense descriptive caption further improves the model’s capability of following prompts faithfully. Improvement through increasing volume of data also proves the scalability of our framework.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we introduce a novel framework that employs MLLMs and diffusion models to synthesize high-quality data for bootstrapping multi-view diffusion models. With a powerful fine-tuned 3D-aware MLLM serving as the dense captioner and quality filter, the generated synthetic data addresses the issue of insufficient high-quality 3D data. The proposed strategy of injecting different data at different training time steps uses the property of the denoising process to further achieve higher image quality while maintaining view consistency. We believe this work will contribute to the goal of achieving 3D content creation with each rendered view comparable with the single view diffusion model, with more advanced MLLMs and diffusion models on the horizon.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Limitations and future work.</span> Despite its promise, our work still faces several unresolved challenges. Firstly, the multi-view diffusion model is only the first step of the 3D content creation pipeline. Sparse view reconstruction models also need improvement as most edge-cutting sparse view reconstruction models are also trained on Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> only. Secondly, Although MLLMs can estimate general quality and view consistency, subtle view inconsistency is hard to detect until ambiguity leads to blurred areas in reconstructed 3D content. While the proposed Training Timestep Reschedule can mitigate this problem, it cannot solve the problem fundamentally. Using synthetic data to train sparse view reconstruction models and quality estimation directly based on the reconstructed object are thus interesting future directions for improving 3D content creation.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Henrik Aanæs, Rasmus Ramsbøl Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl.

</span>
<span class="ltx_bibblock">Large-scale data for multiple-view stereopsis.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 120:153–168, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2204.14198, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Crépy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D’iaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier García, Sebastian Gehrmann, Lucas González, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2305.10403, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openflamingo: An open-source framework for training large autoregressive vision-language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2308.01390, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al.

</span>
<span class="ltx_bibblock">Improving image generation with better captions.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf</span>, 2(3), 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al.

</span>
<span class="ltx_bibblock">Stable video diffusion: Scaling latent video diffusion models to large datasets.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.15127</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh.

</span>
<span class="ltx_bibblock">Video generation models as world simulators.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2005.14165, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al.

</span>
<span class="ltx_bibblock">Efficient geometry-aware 3d generative adversarial networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 16123–16133, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al.

</span>
<span class="ltx_bibblock">Shapenet: An information-rich 3d model repository.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1512.03012</span>, 2015.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li.

</span>
<span class="ltx_bibblock">Pixart-<math id="bib.bib11.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="bib.bib11.1.m1.1a"><mi id="bib.bib11.1.m1.1.1" xref="bib.bib11.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="bib.bib11.1.m1.1b"><ci id="bib.bib11.1.m1.1.1.cmml" xref="bib.bib11.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib11.1.m1.1c">\sigma</annotation></semantics></math>: Weak-to-strong training of diffusion transformer for 4k text-to-image generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.04692</span>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al.

</span>
<span class="ltx_bibblock">Pixart-<math id="bib.bib12.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="bib.bib12.1.m1.1a"><mi id="bib.bib12.1.m1.1.1" xref="bib.bib12.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="bib.bib12.1.m1.1b"><ci id="bib.bib12.1.m1.1.1.cmml" xref="bib.bib12.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib12.1.m1.1c">\alpha</annotation></semantics></math>: Fast training of diffusion transformer for photorealistic text-to-image synthesis.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.00426</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.

</span>
<span class="ltx_bibblock">Sharegpt4v: Improving large multi-modal models with better captions.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.12793</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.

</span>
<span class="ltx_bibblock">Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 22246–22256, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu.

</span>
<span class="ltx_bibblock">V3d: Video diffusion models are effective 3d generators.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.06738</span>, 2024.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, 24:240:1–240:113, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 25(70):1–53, 2024.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee.

</span>
<span class="ltx_bibblock">Luciddreamer: Domain-free generation of 3d gaussian splatting scenes.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.13384</span>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.

</span>
<span class="ltx_bibblock">Objaverse-xl: A universe of 10m+ 3d objects.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.

</span>
<span class="ltx_bibblock">Objaverse: A universe of annotated 3d objects.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 13142–13153, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model, 2024.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B McHugh, and Vincent Vanhoucke.

</span>
<span class="ltx_bibblock">Google scanned objects: A high-quality dataset of 3d scanned household items.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">2022 International Conference on Robotics and Automation (ICRA)</span>, pages 2553–2560. IEEE, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence.

</span>
<span class="ltx_bibblock">Palm-e: An embodied multimodal language model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.03206</span>, 2024.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Daniel Gatis etc.

</span>
<span class="ltx_bibblock">Rembg.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/danielgatis/rembg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/danielgatis/rembg</a>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, and Dahua Lin.

</span>
<span class="ltx_bibblock">Make-it-real: Unleashing large multimodal model’s ability for painting 3d objects with realistic materials, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Junlin Han, Filippos Kokkinos, and Philip Torr.

</span>
<span class="ltx_bibblock">Vfusion3d: Learning scalable 3d generative models from video diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.12034</span>, 2024.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Amir Hertz, Kfir Aberman, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">Delta denoising score.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 2328–2337, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.

</span>
<span class="ltx_bibblock">Gans trained by a two time-scale update rule converge to a local nash equilibrium.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Denoising diffusion probabilistic models.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:6840–6851, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2203.15556, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan.

</span>
<span class="ltx_bibblock">Lrm: Large reconstruction model for single image to 3d.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.04400</span>, 2023.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei.

</span>
<span class="ltx_bibblock">Language is not all you need: Aligning perception with language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2302.14045, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openclip.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/mlfoundations/open_clip" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mlfoundations/open_clip</a>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, and Aliaksandr Siarohin.

</span>
<span class="ltx_bibblock">Spad: Spatially aware multiview diffusers.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.05235</span>, 2024.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis.

</span>
<span class="ltx_bibblock">3d gaussian splatting for real-time radiance field rendering.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics</span>, 42(4):1–14, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Diederik P Kingma and Max Welling.

</span>
<span class="ltx_bibblock">Auto-encoding variational bayes.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1312.6114</span>, 2013.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4015–4026, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi.

</span>
<span class="ltx_bibblock">Vivid-1-to-3: Novel view synthesis with video diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.01305</span>, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi.

</span>
<span class="ltx_bibblock">Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi.

</span>
<span class="ltx_bibblock">Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.06214</span>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2301.12597, 2023.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi.

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, 2022.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen.

</span>
<span class="ltx_bibblock">Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.11284</span>, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin.

</span>
<span class="ltx_bibblock">Magic3d: High-resolution text-to-3d content creation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 300–309, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 36, 2024.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su.

</span>
<span class="ltx_bibblock">One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.07885</span>, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su.

</span>
<span class="ltx_bibblock">One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.

</span>
<span class="ltx_bibblock">Zero-1-to-3: Zero-shot one image to 3d object.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 9298–9309, 2023.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.

</span>
<span class="ltx_bibblock">Syncdreamer: Generating multiview-consistent images from a single-view image.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.03453</span>, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al.

</span>
<span class="ltx_bibblock">Wonder3d: Single image to 3d using cross-domain diffusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.15008</span>, 2023.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson.

</span>
<span class="ltx_bibblock">Scalable 3d captioning with pretrained models.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, and Filippos Kokkinos.

</span>
<span class="ltx_bibblock">Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.08682</span>, 2024.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.

</span>
<span class="ltx_bibblock">Nerf: Representing scenes as neural radiance fields for view synthesis.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 65(1):99–106, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4v(ision) system card.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">OpenAI</span>, 2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
R OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">arXiv</span>, pages 2303–08774, 2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4195–4205, 2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">Sdxl: Improving latent diffusion models for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.01952</span>, 2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.

</span>
<span class="ltx_bibblock">Dreamfusion: Text-to-3d using 2d diffusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2209.14988</span>, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han.

</span>
<span class="ltx_bibblock">Richdreamer: A generalizable normal-depth diffusion model for detail richness in text-to-3d.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2311.16918</span>, 2023.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 10684–10695, 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Tim Salimans and Jonathan Ho.

</span>
<span class="ltx_bibblock">Progressive distillation for fast sampling of diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.00512</span>, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 35:25278–25294, 2022.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia, Dmitry Lagun, Fredo Durand, William T Freeman, and Mark Matthews.

</span>
<span class="ltx_bibblock">Alchemist: Parametric control of material properties with diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.02970</span>, 2023.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su.

</span>
<span class="ltx_bibblock">Zero123++: a single image to consistent multi-view diffusion base model.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2310.15110</span>, 2023.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang.

</span>
<span class="ltx_bibblock">Mvdream: Multi-view diffusion for 3d generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2308.16512</span>, 2023.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.

</span>
<span class="ltx_bibblock">Deep unsupervised learning using nonequilibrium thermodynamics.

</span>
<span class="ltx_bibblock">In <span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages 2256–2265. PMLR, 2015.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Alpha-clip: A clip model focusing on wherever you want, 2023.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu.

</span>
<span class="ltx_bibblock">Lgm: Large multi-view gaussian model for high-resolution 3d content creation.

</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.05054</span>, 2024.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng.

</span>
<span class="ltx_bibblock">Dreamgaussian: Generative gaussian splatting for efficient 3d content creation.

</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2309.16653</span>, 2023.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Shitao Tang, Jiacheng Chen, Dilin Wang, Chengzhou Tang, Fuyang Zhang, Yuchen Fan, Vikas Chandra, Yasutaka Furukawa, and Rakesh Ranjan.

</span>
<span class="ltx_bibblock">Mvdiffusion++: A dense high-resolution multi-view diffusion model for single or sparse-view 3d object reconstruction.

</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.12712</span>, 2024.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.11805</span>, 2023.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao.

</span>
<span class="ltx_bibblock">Triposr: Fast 3d object reconstruction from a single image.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.02151</span>, 2024.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2302.13971, 2023.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani.

</span>
<span class="ltx_bibblock">Sv3d: Novel multi-view synthesis and 3d generation from a single image using latent video diffusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.12008</span>, 2024.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich.

</span>
<span class="ltx_bibblock">Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 12619–12629, 2023.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Peng Wang and Yichun Shi.

</span>
<span class="ltx_bibblock">Imagedream: Image-prompt multi-view diffusion for 3d generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.02201</span>, 2023.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang.

</span>
<span class="ltx_bibblock">Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction, 2023.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu.

</span>
<span class="ltx_bibblock">Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu.

</span>
<span class="ltx_bibblock">Crm: Single image to 3d textured mesh with convolutional reconstruction model.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.05034</span>, 2024.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan.

</span>
<span class="ltx_bibblock">Motionctrl: A unified and flexible motion controller for video generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.03641</span>, 2023.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 35:24824–24837, 2022.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Xinyue Wei, Kai Zhang, Sai Bi, Hao Tan, Fujun Luan, Valentin Deschaintre, Kalyan Sunkavalli, Hao Su, and Zexiang Xu.

</span>
<span class="ltx_bibblock">Meshlrm: Large reconstruction model for high-quality mesh, 2024.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein.

</span>
<span class="ltx_bibblock">Gpt-4v (ision) is a human-aligned evaluator for text-to-3d generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2401.04092</span>, 2024.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al.

</span>
<span class="ltx_bibblock">Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 803–814, 2023.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.

</span>
<span class="ltx_bibblock">3d shapenets: A deep representation for volumetric shapes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 1912–1920, 2015.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan.

</span>
<span class="ltx_bibblock">Instantmesh: Efficient 3d mesh generation from a single image with sparse-view large reconstruction models.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.07191</span>, 2024.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein.

</span>
<span class="ltx_bibblock">Grm: Large gaussian reconstruction model for efficient 3d reconstruction and generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2403.14621</span>, 2024.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, and Guosheng Lin.

</span>
<span class="ltx_bibblock">Magic-boost: Boost 3d generation with mutli-view conditioned diffusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.06429</span>, 2024.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.

</span>
<span class="ltx_bibblock">Blendedmvs: A large-scale dataset for generalized multi-view stereo networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 1790–1799, 2020.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka.

</span>
<span class="ltx_bibblock">3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 42(4):1–16, 2023.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition, 2023.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, and Yang Liu.

</span>
<span class="ltx_bibblock">Mvd<sup id="bib.bib94.2.1" class="ltx_sup"><span id="bib.bib94.2.1.1" class="ltx_text ltx_font_italic">2</span></sup>: Efficient multiview 3d reconstruction for multiview diffusion.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.3.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.14253</span>, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Evaluation on wild prompts from real users</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The results of the main part of the paper are only tested on GPT generated prompts. To test our work’s capability in wild cases, we also collect real user prompts and compare our method with Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. specifically, we randomly collect 100 prompts from <a target="_blank" href="https://www.meshy.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.meshy.ai/</a> and test the CLIP-R precision as well as GPT based evaluation (detailed in Sup. <a href="#A5" title="Appendix E GPT-4V based 3D Object Generation Evaluation. ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>). Results and some qualitative cases are shown in Tab. <a href="#A1.T4" title="Table 4 ‣ Appendix A Evaluation on wild prompts from real users ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and Fig. <a href="#A1.F7" title="Figure 7 ‣ Appendix A Evaluation on wild prompts from real users ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We highlight that our Bootstrap3D excels Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
when tested on real user prompts through training on synthetic data.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A1.T4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Test results of in the wild cases.<span id="A1.T4.4.2.1" class="ltx_text ltx_font_medium"> Bootstrap3D also excels Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> in generating high quality images according to real user prompts.</span></span></figcaption>
<table id="A1.T4.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T4.5.1.1" class="ltx_tr">
<th id="A1.T4.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="A1.T4.5.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<th id="A1.T4.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">CLIP based metric</th>
<th id="A1.T4.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">GPTEval3D</th>
</tr>
<tr id="A1.T4.5.2.2" class="ltx_tr">
<td id="A1.T4.5.2.2.1" class="ltx_td ltx_align_center">CLIP-R score</td>
<td id="A1.T4.5.2.2.2" class="ltx_td ltx_align_center">image-text alignment</td>
<td id="A1.T4.5.2.2.3" class="ltx_td ltx_align_center">texture detail</td>
</tr>
<tr id="A1.T4.5.3.3" class="ltx_tr">
<th id="A1.T4.5.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (unofficial)</th>
<th id="A1.T4.5.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">77.0</th>
<th id="A1.T4.5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">22.0%</th>
<th id="A1.T4.5.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">24.5%</th>
</tr>
<tr id="A1.T4.5.4.4" class="ltx_tr">
<th id="A1.T4.5.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Bootstrap3D</th>
<td id="A1.T4.5.4.4.2" class="ltx_td ltx_align_center ltx_border_bb">83.5</td>
<td id="A1.T4.5.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">78.0%</td>
<td id="A1.T4.5.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">75.5%</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2406.00093/assets/x7.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="518" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A1.F7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Real user prompt cases<span id="A1.F7.4.2.1" class="ltx_text ltx_font_medium"> visualization compared to Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite></span></span></figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Data Statistics</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Caption Analysis</h3>

<figure id="A2.F8" class="ltx_figure"><img src="/html/2406.00093/assets/x8.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="396" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A2.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualized analysis of dense reasoning descriptions generated by GPT4-Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite><span id="A2.F8.4.2.1" class="ltx_text ltx_font_medium"> of the root noun-verb pairs (occurring over 1%) of the descriptions</span></span></figcaption>
</figure>
<figure id="A2.F9" class="ltx_figure"><img src="/html/2406.00093/assets/x9.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="396" height="283" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A2.F9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualized analysis of dense reasoning descriptions generated by our MV-LLaVA<span id="A2.F9.4.2.1" class="ltx_text ltx_font_medium"> of the root noun-verb pairs (occurring over 1%) of the descriptions</span></span></figcaption>
</figure>
<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">Fig. <a href="#A2.F8" title="Figure 8 ‣ B.1 Caption Analysis ‣ Appendix B Data Statistics ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and  <a href="#A2.F9" title="Figure 9 ‣ B.1 Caption Analysis ‣ Appendix B Data Statistics ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> provide a visualization of the root noun-verb pairs for the captions generated by GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and MV-LLaVA. It’s clear to see that the diversity and linguistic
expression of the captions produced by MV-LLaVA are
highly matched with those of GPT-4V. We believe the highly detailed description focusing on object’s texture, shape and color have potential usage beyond training multi-view diffusion model in the field like object texturing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and stylization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> in Computer Graphics. MV-LLaVA can also serve as free and efficient 3D object assistant comparable with GPT-4V for future research of 3D content creation.</p>
</div>
<figure id="A2.F10" class="ltx_figure"><img src="/html/2406.00093/assets/x10.png" id="A2.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="A2.F10.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Histogram Visualization of the Caption Length<span id="A2.F10.4.2.1" class="ltx_text ltx_font_medium"> compared with Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></span></span></figcaption>
</figure>
<figure id="A2.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T5.21.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="A2.T5.22.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparison of lexical composition of the captions<span id="A2.T5.22.2.1" class="ltx_text ltx_font_medium"> generated by GPT4-Vision and Share-Captioner.</span></span></figcaption>
<table id="A2.T5.18" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T5.18.19.1" class="ltx_tr">
<th id="A2.T5.18.19.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Lexical</th>
<th id="A2.T5.18.19.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">n.</th>
<th id="A2.T5.18.19.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">adj.</th>
<th id="A2.T5.18.19.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">adv.</th>
<th id="A2.T5.18.19.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">v.</th>
<th id="A2.T5.18.19.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">num.</th>
<th id="A2.T5.18.19.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">prep.</th>
</tr>
<tr id="A2.T5.6.6" class="ltx_tr">
<th id="A2.T5.6.6.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<th id="A2.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">29.1<math id="A2.T5.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.1.1.1.m1.1a"><mo id="A2.T5.1.1.1.m1.1.1" xref="A2.T5.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.1.1.1.m1.1b"><csymbol cd="latexml" id="A2.T5.1.1.1.m1.1.1.cmml" xref="A2.T5.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.1.1.1.m1.1c">\%</annotation></semantics></math>
</th>
<th id="A2.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">16.0<math id="A2.T5.2.2.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.2.2.2.m1.1a"><mo id="A2.T5.2.2.2.m1.1.1" xref="A2.T5.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.2.2.2.m1.1b"><csymbol cd="latexml" id="A2.T5.2.2.2.m1.1.1.cmml" xref="A2.T5.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.2.2.2.m1.1c">\%</annotation></semantics></math>
</th>
<th id="A2.T5.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">1.5<math id="A2.T5.3.3.3.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.3.3.3.m1.1a"><mo id="A2.T5.3.3.3.m1.1.1" xref="A2.T5.3.3.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.3.3.3.m1.1b"><csymbol cd="latexml" id="A2.T5.3.3.3.m1.1.1.cmml" xref="A2.T5.3.3.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.3.3.3.m1.1c">\%</annotation></semantics></math>
</th>
<th id="A2.T5.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">11.1<math id="A2.T5.4.4.4.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.4.4.4.m1.1a"><mo id="A2.T5.4.4.4.m1.1.1" xref="A2.T5.4.4.4.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.4.4.4.m1.1b"><csymbol cd="latexml" id="A2.T5.4.4.4.m1.1.1.cmml" xref="A2.T5.4.4.4.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.4.4.4.m1.1c">\%</annotation></semantics></math>
</th>
<th id="A2.T5.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">0.5<math id="A2.T5.5.5.5.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.5.5.5.m1.1a"><mo id="A2.T5.5.5.5.m1.1.1" xref="A2.T5.5.5.5.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.5.5.5.m1.1b"><csymbol cd="latexml" id="A2.T5.5.5.5.m1.1.1.cmml" xref="A2.T5.5.5.5.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.5.5.5.m1.1c">\%</annotation></semantics></math>
</th>
<th id="A2.T5.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">9.0<math id="A2.T5.6.6.6.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.6.6.6.m1.1a"><mo id="A2.T5.6.6.6.m1.1.1" xref="A2.T5.6.6.6.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.6.6.6.m1.1b"><csymbol cd="latexml" id="A2.T5.6.6.6.m1.1.1.cmml" xref="A2.T5.6.6.6.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.6.6.6.m1.1c">\%</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T5.12.12" class="ltx_tr">
<th id="A2.T5.12.12.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">BS-Description</th>
<td id="A2.T5.7.7.1" class="ltx_td ltx_align_center ltx_border_t">28.5<math id="A2.T5.7.7.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.7.7.1.m1.1a"><mo id="A2.T5.7.7.1.m1.1.1" xref="A2.T5.7.7.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.7.7.1.m1.1b"><csymbol cd="latexml" id="A2.T5.7.7.1.m1.1.1.cmml" xref="A2.T5.7.7.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.7.7.1.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.8.8.2" class="ltx_td ltx_align_center ltx_border_t">16.0<math id="A2.T5.8.8.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.8.8.2.m1.1a"><mo id="A2.T5.8.8.2.m1.1.1" xref="A2.T5.8.8.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.8.8.2.m1.1b"><csymbol cd="latexml" id="A2.T5.8.8.2.m1.1.1.cmml" xref="A2.T5.8.8.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.8.8.2.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.9.9.3" class="ltx_td ltx_align_center ltx_border_t">1.4<math id="A2.T5.9.9.3.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.9.9.3.m1.1a"><mo id="A2.T5.9.9.3.m1.1.1" xref="A2.T5.9.9.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.9.9.3.m1.1b"><csymbol cd="latexml" id="A2.T5.9.9.3.m1.1.1.cmml" xref="A2.T5.9.9.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.9.9.3.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.10.10.4" class="ltx_td ltx_align_center ltx_border_t">10.8<math id="A2.T5.10.10.4.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.10.10.4.m1.1a"><mo id="A2.T5.10.10.4.m1.1.1" xref="A2.T5.10.10.4.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.10.10.4.m1.1b"><csymbol cd="latexml" id="A2.T5.10.10.4.m1.1.1.cmml" xref="A2.T5.10.10.4.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.10.10.4.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.11.11.5" class="ltx_td ltx_align_center ltx_border_t">0.3<math id="A2.T5.11.11.5.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.11.11.5.m1.1a"><mo id="A2.T5.11.11.5.m1.1.1" xref="A2.T5.11.11.5.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.11.11.5.m1.1b"><csymbol cd="latexml" id="A2.T5.11.11.5.m1.1.1.cmml" xref="A2.T5.11.11.5.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.11.11.5.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.12.12.6" class="ltx_td ltx_align_center ltx_border_t">8.6<math id="A2.T5.12.12.6.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.12.12.6.m1.1a"><mo id="A2.T5.12.12.6.m1.1.1" xref="A2.T5.12.12.6.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.12.12.6.m1.1b"><csymbol cd="latexml" id="A2.T5.12.12.6.m1.1.1.cmml" xref="A2.T5.12.12.6.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.12.12.6.m1.1c">\%</annotation></semantics></math>
</td>
</tr>
<tr id="A2.T5.18.18" class="ltx_tr">
<th id="A2.T5.18.18.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">BS-Caption</th>
<td id="A2.T5.13.13.1" class="ltx_td ltx_align_center ltx_border_bb">30.2<math id="A2.T5.13.13.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.13.13.1.m1.1a"><mo id="A2.T5.13.13.1.m1.1.1" xref="A2.T5.13.13.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.13.13.1.m1.1b"><csymbol cd="latexml" id="A2.T5.13.13.1.m1.1.1.cmml" xref="A2.T5.13.13.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.13.13.1.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.14.14.2" class="ltx_td ltx_align_center ltx_border_bb">23.0<math id="A2.T5.14.14.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.14.14.2.m1.1a"><mo id="A2.T5.14.14.2.m1.1.1" xref="A2.T5.14.14.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.14.14.2.m1.1b"><csymbol cd="latexml" id="A2.T5.14.14.2.m1.1.1.cmml" xref="A2.T5.14.14.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.14.14.2.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.15.15.3" class="ltx_td ltx_align_center ltx_border_bb">0.3<math id="A2.T5.15.15.3.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.15.15.3.m1.1a"><mo id="A2.T5.15.15.3.m1.1.1" xref="A2.T5.15.15.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.15.15.3.m1.1b"><csymbol cd="latexml" id="A2.T5.15.15.3.m1.1.1.cmml" xref="A2.T5.15.15.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.15.15.3.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.16.16.4" class="ltx_td ltx_align_center ltx_border_bb">5.6<math id="A2.T5.16.16.4.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.16.16.4.m1.1a"><mo id="A2.T5.16.16.4.m1.1.1" xref="A2.T5.16.16.4.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.16.16.4.m1.1b"><csymbol cd="latexml" id="A2.T5.16.16.4.m1.1.1.cmml" xref="A2.T5.16.16.4.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.16.16.4.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.17.17.5" class="ltx_td ltx_align_center ltx_border_bb">0.1<math id="A2.T5.17.17.5.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.17.17.5.m1.1a"><mo id="A2.T5.17.17.5.m1.1.1" xref="A2.T5.17.17.5.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.17.17.5.m1.1b"><csymbol cd="latexml" id="A2.T5.17.17.5.m1.1.1.cmml" xref="A2.T5.17.17.5.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.17.17.5.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A2.T5.18.18.6" class="ltx_td ltx_align_center ltx_border_bb">8.9<math id="A2.T5.18.18.6.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A2.T5.18.18.6.m1.1a"><mo id="A2.T5.18.18.6.m1.1.1" xref="A2.T5.18.18.6.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A2.T5.18.18.6.m1.1b"><csymbol cd="latexml" id="A2.T5.18.18.6.m1.1.1.cmml" xref="A2.T5.18.18.6.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A2.T5.18.18.6.m1.1c">\%</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">Fig. <a href="#A2.F10" title="Figure 10 ‣ B.1 Caption Analysis ‣ Appendix B Data Statistics ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> visualizes the histogram of caption length compared with Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. We fine-tune MV-LLaVA to generate two different lengths suitable for different diffusion architecture, namely CLIP-based text encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> with 77 token length and T5 based text encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> with 120 token length. Both excel the length of Cap3D with less hallucinations.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Estimated Quality Analysis</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">For direct grasp of the quality of objaverse data and synthetic data used to train diffusion model, we randomly picked some of multi-view images from different score rank. Results are shown in Fig. <a href="#A3.F14" title="Figure 14 ‣ C.3 Qualitative caption quality study ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, Fig. <a href="#A3.F15" title="Figure 15 ‣ C.3 Qualitative caption quality study ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> and Fig. <a href="#A3.F16" title="Figure 16 ‣ C.3 Qualitative caption quality study ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. We use high quality data with score 4 and 5 for the training of multi-view diffusion model.</p>
</div>
<figure id="A2.F11" class="ltx_figure"><img src="/html/2406.00093/assets/x11.png" id="A2.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A2.F11.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Quality score statistics of different data source.</span></figcaption>
</figure>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.1" class="ltx_p">We count the number of multi-view images from different data sources, namely 660K from Objaverse, 500K from SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and 500K from Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> generated by our Bootstrap3D pipeline. Result are shown in Fig.<a href="#A2.F11" title="Figure 11 ‣ B.2 Estimated Quality Analysis ‣ Appendix B Data Statistics ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. For Objaverse and SV3D, the assigned score is normal and we use score 4 and score 5 multi-view images as high quality data for training. However, for Zero123++, most objects are assigned with score greater than 3. We attribute this phenomenon to the fact that Zero123++ tend to generate objects with less motion blurring but more stretching and deformation compared to SV3D. Joint training of MV-LLaVA on three different data source lead to higher and more focused distribution for Zero123++’s multi-view images. For this part of synthetic data, we leave only score 5 multi-view images as high quality data.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Quality of MV-LLaVA</h2>

<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Choice of number of unfrozen layers of vision encoder.</h3>

<figure id="A3.F12" class="ltx_figure"><img src="/html/2406.00093/assets/x12.png" id="A3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="456" height="243" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A3.F12.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Qualitative results of unfreeze final layers of CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> vision encoder<span id="A3.F12.4.2.1" class="ltx_text ltx_font_medium"> compared to original fixed vision encoder setting in LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.</span></span></figcaption>
</figure>
<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">Inspired by ShareGPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we unfreeze selected final layers of the CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> vision encoder during the initial phase of vision language alignment. The CLIP-L/14 model used for LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> contains 24 transformer layers. We selectively unfreeze some of final layers to enable the CLIP model to focus more on details such as texture of multi-view images. After qualitative manual screening, we select to unfreeze eight layers to yield better results. Fig. <a href="#A3.F12" title="Figure 12 ‣ C.1 Choice of number of unfrozen layers of vision encoder. ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> illustrates the differences between unfreezing eight layers and not unfreezing any (the original training setting of LLaVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>). The red sections highlight the erroneous hallucinations occurring when the vision encoder remains fully unchanged, while the green sections indicate accurate descriptions of the image content. This demonstrates that partially unfreezing the vision encoder can produce more precise captions and reduce some hallucinations.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Quantitative quality study</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">To test the quality of our MV-LLaVA. We propose two quantitative study over the quality of captions and the alignment of quality estimation with human experts. In first study, we randomly picked 200 object from Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and exclude training data of MV-LLaVA. We use GPT4-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> and MV-LLaVA to generate descriptive captions for each object. We invite human volunteers to choose their preference over shuffled captions. Results are shown in Tab. <a href="#A3.T6" title="Table 6 ‣ C.2 Quantitative quality study ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, where MV-LLaVA shows comparable captioning ability with powerful GPT4-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, which is essential to generate millions of high quality image-text pairs for the training of text to multi-view image diffusion model.</p>
</div>
<div id="A3.SS2.p2" class="ltx_para">
<p id="A3.SS2.p2.1" class="ltx_p">Second experiment studies MV-LLaVA’s ability in quality estimation of both 3D assets and generated multi-view images. We invite human volunteers to estimate the quality of multi-view images rendered from Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> or generated by SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. As there is no golden standard for multi quality classification, We ask them to separate the randomly select multi-view images into approximately two half and serve as GT quality. We use MV-LLaVA to estimate the quality of these images and generate confusion matrix. Results are shown in Tab.<a href="#A3.T7" title="Table 7 ‣ C.2 Quantitative quality study ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Given the great amount of source data of 3D assets and infinite synthetic data, we care more about the false positive rate, as these data will be mixed into training data. In this observation, we highlight the false positive rate of over 20% for SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> generated multi-view images. This result align with the observation of inevitable motion blurring of SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. To leverage this part of data source for data diversity without hurting the final quality. We propose Training Noise Reschedule to avoid samplings from these synthetic data when time step is small.</p>
</div>
<figure id="A3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A3.T6.6.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="A3.T6.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Human evaluation<span id="A3.T6.7.2.1" class="ltx_text ltx_font_medium"> on the quality of generated captions from MV-LLaVA vs. GPT4-Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> over 200 validation samples from Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</span></span></figcaption>
<table id="A3.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T6.3.4.1" class="ltx_tr">
<th id="A3.T6.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Preference</th>
<th id="A3.T6.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">GPT4-Vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>
</th>
<th id="A3.T6.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">MV-LLaVA</th>
<th id="A3.T6.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Comparable</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T6.3.3" class="ltx_tr">
<th id="A3.T6.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">Percentage</th>
<td id="A3.T6.1.1.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">39.5<math id="A3.T6.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A3.T6.1.1.1.m1.1a"><mo id="A3.T6.1.1.1.m1.1.1" xref="A3.T6.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A3.T6.1.1.1.m1.1b"><csymbol cd="latexml" id="A3.T6.1.1.1.m1.1.1.cmml" xref="A3.T6.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.1.1.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A3.T6.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">34.5<math id="A3.T6.2.2.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A3.T6.2.2.2.m1.1a"><mo id="A3.T6.2.2.2.m1.1.1" xref="A3.T6.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A3.T6.2.2.2.m1.1b"><csymbol cd="latexml" id="A3.T6.2.2.2.m1.1.1.cmml" xref="A3.T6.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.2.2.2.m1.1c">\%</annotation></semantics></math>
</td>
<td id="A3.T6.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">26.0<math id="A3.T6.3.3.3.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A3.T6.3.3.3.m1.1a"><mo id="A3.T6.3.3.3.m1.1.1" xref="A3.T6.3.3.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A3.T6.3.3.3.m1.1b"><csymbol cd="latexml" id="A3.T6.3.3.3.m1.1.1.cmml" xref="A3.T6.3.3.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.3.3.3.m1.1c">\%</annotation></semantics></math>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A3.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="A3.T7.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Confusion matrix<span id="A3.T7.4.2.1" class="ltx_text ltx_font_medium"> of mutli-view images quality estimation.</span></span></figcaption>
<table id="A3.T7.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T7.5.1.1" class="ltx_tr">
<th id="A3.T7.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="3">Objaverse quality check</th>
<th id="A3.T7.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3">Synthetic quality check</th>
</tr>
<tr id="A3.T7.5.2.2" class="ltx_tr">
<th id="A3.T7.5.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="A3.T7.5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">HQ-gt</th>
<th id="A3.T7.5.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">LQ-gt</th>
<th id="A3.T7.5.2.2.4" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="A3.T7.5.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">HQ-gt</th>
<th id="A3.T7.5.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">LQ-gt</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T7.5.3.1" class="ltx_tr">
<th id="A3.T7.5.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">HQ by model</th>
<td id="A3.T7.5.3.1.2" class="ltx_td ltx_align_center ltx_border_t">31.0%</td>
<td id="A3.T7.5.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.5%</td>
<td id="A3.T7.5.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HQ by model</td>
<td id="A3.T7.5.3.1.5" class="ltx_td ltx_align_center ltx_border_t">34.5%</td>
<td id="A3.T7.5.3.1.6" class="ltx_td ltx_align_center ltx_border_t">11.5%</td>
</tr>
<tr id="A3.T7.5.4.2" class="ltx_tr">
<th id="A3.T7.5.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">LQ by model</th>
<td id="A3.T7.5.4.2.2" class="ltx_td ltx_align_center ltx_border_bb">11.0%</td>
<td id="A3.T7.5.4.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">53.5%</td>
<td id="A3.T7.5.4.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">LQ by model</td>
<td id="A3.T7.5.4.2.5" class="ltx_td ltx_align_center ltx_border_bb">17.0%</td>
<td id="A3.T7.5.4.2.6" class="ltx_td ltx_align_center ltx_border_bb">37.0%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Qualitative caption quality study</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">We selective compare some of the captions generated by Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> and MV-LLaVA in Fig. <a href="#A3.F13" title="Figure 13 ‣ C.3 Qualitative caption quality study ‣ Appendix C Quality of MV-LLaVA ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. Our MV-LLaVA can generate more detailed descriptive captions with less hallucinations.</p>
</div>
<figure id="A3.F13" class="ltx_figure"><img src="/html/2406.00093/assets/x13.png" id="A3.F13.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="456" height="567" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A3.F13.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Caption comparison with Cap3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.<span id="A3.F13.4.2.1" class="ltx_text ltx_font_medium"> Our MV-LLaVA can generate long captions that faithfully describing 3D assets from different perspectives like color, geometry and texture.</span></span></figcaption>
</figure>
<figure id="A3.F14" class="ltx_figure"><img src="/html/2406.00093/assets/x14.png" id="A3.F14.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="456" height="675" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F14.3.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A3.F14.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Randomly picked multi-view images with different scores from 500k synthetic data generated by SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>.</span></figcaption>
</figure>
<figure id="A3.F15" class="ltx_figure"><img src="/html/2406.00093/assets/x15.png" id="A3.F15.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="456" height="675" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F15.3.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A3.F15.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Randomly picked multi-view images with different scores from 500k synthetic data generated by Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>.</span></figcaption>
</figure>
<figure id="A3.F16" class="ltx_figure"><img src="/html/2406.00093/assets/x16.png" id="A3.F16.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="456" height="675" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A3.F16.3.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="A3.F16.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Randomly picked multi-view images with different scores from 660k Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> 3D assets.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of Prompt Design</h2>

<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Prompts for GPT-4V for Quality Check</h3>

<figure id="A4.F17" class="ltx_figure"><img src="/html/2406.00093/assets/x17.png" id="A4.F17.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="456" height="575" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F17.3.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="A4.F17.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Prompt for GPT-4V to generate caption and estimate quality of multi-view images from SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</span></figcaption>
</figure>
<div id="A4.SS1.p1" class="ltx_para">
<p id="A4.SS1.p1.1" class="ltx_p">Detailed prompts are shown in Fig.<a href="#A4.F17" title="Figure 17 ‣ D.1 Prompts for GPT-4V for Quality Check ‣ Appendix D Details of Prompt Design ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Prompts for MV-LLaVA Instruct Tuning</h3>

<figure id="A4.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A4.T8.3.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="A4.T8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Instruct tuning prompt for SV3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> and Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> multi-view images</span></figcaption>
<table id="A4.T8.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T8.5.1.1" class="ltx_tr">
<th id="A4.T8.5.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="A4.T8.5.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.1.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T8.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">prompt type</span></span>
</span>
</th>
<th id="A4.T8.5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.T8.5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.1.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A4.T8.5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">prompt</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T8.5.2.1" class="ltx_tr">
<td id="A4.T8.5.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="3">
<span id="A4.T8.5.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T8.5.2.1.1.1.1.1" class="ltx_text">generate caption</span></span>
</span>
</td>
<td id="A4.T8.5.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.5.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.2.1.2.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nWhat is this multi-view photo about? generate a short caption for me.</span>
</span>
</td>
</tr>
<tr id="A4.T8.5.3.2" class="ltx_tr">
<td id="A4.T8.5.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T8.5.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.3.2.1.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nGenerate a short caption of the following multi-view image.</span>
</span>
</td>
</tr>
<tr id="A4.T8.5.4.3" class="ltx_tr">
<td id="A4.T8.5.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T8.5.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.4.3.1.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nCan you describe the main features of this multi-view image for me by a short caption?</span>
</span>
</td>
</tr>
<tr id="A4.T8.5.5.4" class="ltx_tr">
<td id="A4.T8.5.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="3">
<span id="A4.T8.5.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.5.4.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T8.5.5.4.1.1.1.1" class="ltx_text">reasoning</span></span>
</span>
</td>
<td id="A4.T8.5.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T8.5.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.5.4.2.1.1" class="ltx_p" style="width:284.5pt;">How about the view consistency of this synthesized multi-view image?</span>
</span>
</td>
</tr>
<tr id="A4.T8.5.6.5" class="ltx_tr">
<td id="A4.T8.5.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T8.5.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.6.5.1.1.1" class="ltx_p" style="width:284.5pt;">Do some comments about the view consistency of this synthesized multi-view image.</span>
</span>
</td>
</tr>
<tr id="A4.T8.5.7.6" class="ltx_tr">
<td id="A4.T8.5.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T8.5.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.7.6.1.1.1" class="ltx_p" style="width:284.5pt;">What do you think about the view consistency of this synthesized multi-view image?</span>
</span>
</td>
</tr>
<tr id="A4.T8.5.8.7" class="ltx_tr">
<td id="A4.T8.5.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T8.5.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.8.7.1.1.1" class="ltx_p" style="width:85.4pt;">quality estimation</span>
</span>
</td>
<td id="A4.T8.5.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.T8.5.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T8.5.8.7.2.1.1" class="ltx_p" style="width:284.5pt;">What do you think about the overall quality of view consistency of three synthesized novel views? Choosing from "poor", "relatively poor", "boardline", "relatively good", "good", "perfect".</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A4.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A4.T9.3.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>: </span><span id="A4.T9.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Instruct tuning prompt for Objaverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> rendered multi-view images</span></figcaption>
<table id="A4.T9.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T9.5.1.1" class="ltx_tr">
<th id="A4.T9.5.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt">
<span id="A4.T9.5.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.1.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T9.5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">prompt type</span></span>
</span>
</th>
<th id="A4.T9.5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="A4.T9.5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.1.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="A4.T9.5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">prompt</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T9.5.2.1" class="ltx_tr">
<td id="A4.T9.5.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="3">
<span id="A4.T9.5.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.2.1.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T9.5.2.1.1.1.1.1" class="ltx_text">long description</span></span>
</span>
</td>
<td id="A4.T9.5.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T9.5.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.2.1.2.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nWhat is this multi-view photo about? generate a long descriptive caption for me.</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.3.2" class="ltx_tr">
<td id="A4.T9.5.3.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T9.5.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.3.2.1.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nGenerate a long descriptive caption of the following multi-view image.</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.4.3" class="ltx_tr">
<td id="A4.T9.5.4.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T9.5.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.4.3.1.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nCan you describe the main features of this multi-view image for me by a long descriptive caption caption?</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.5.4" class="ltx_tr">
<td id="A4.T9.5.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" rowspan="3">
<span id="A4.T9.5.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.5.4.1.1.1" class="ltx_p" style="width:85.4pt;"><span id="A4.T9.5.5.4.1.1.1.1" class="ltx_text">caption</span></span>
</span>
</td>
<td id="A4.T9.5.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T9.5.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.5.4.2.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nWhat is this multi-view photo about? generate a short caption for me.</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.6.5" class="ltx_tr">
<td id="A4.T9.5.6.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T9.5.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.6.5.1.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nGenerate a short caption of the following multi-view image.</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.7.6" class="ltx_tr">
<td id="A4.T9.5.7.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A4.T9.5.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.7.6.1.1.1" class="ltx_p" style="width:284.5pt;">&lt;image&gt;&lt;image&gt;&lt;image&gt;&lt;image&gt;\nCan you describe the main features of this multi-view image for me by a short caption?</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.8.7" class="ltx_tr">
<td id="A4.T9.5.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T9.5.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.8.7.1.1.1" class="ltx_p" style="width:85.4pt;">quality estimation</span>
</span>
</td>
<td id="A4.T9.5.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T9.5.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.8.7.2.1.1" class="ltx_p" style="width:284.5pt;">What do you think about the overall quality of this 3D model? Choosing from "poor", "relatively poor", "boardline", "relatively good", "good", "perfect".</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.9.8" class="ltx_tr">
<td id="A4.T9.5.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="A4.T9.5.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.9.8.1.1.1" class="ltx_p" style="width:85.4pt;">scale tag</span>
</span>
</td>
<td id="A4.T9.5.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A4.T9.5.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.9.8.2.1.1" class="ltx_p" style="width:284.5pt;">What do you think about the scale of the 3D model represents? Choosing from "single_object", "multi-object", "small_scene", "large_scene".</span>
</span>
</td>
</tr>
<tr id="A4.T9.5.10.9" class="ltx_tr">
<td id="A4.T9.5.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="A4.T9.5.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.10.9.1.1.1" class="ltx_p" style="width:85.4pt;">style tag</span>
</span>
</td>
<td id="A4.T9.5.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="A4.T9.5.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T9.5.10.9.2.1.1" class="ltx_p" style="width:284.5pt;">What do you think about the overall style of the 3D model? Choosing from "CAD", "Cartoon", "Photo_realistic".</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A4.F18" class="ltx_figure"><img src="/html/2406.00093/assets/x18.png" id="A4.F18.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="627" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A4.F18.3.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="A4.F18.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">A test conversation with GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite><span id="A4.F18.4.2.1" class="ltx_text ltx_font_medium"> of evaluating generated objects.</span></span></figcaption>
</figure>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>GPT-4V based 3D Object Generation Evaluation.</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">We adopt method proposed in GPTeval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> for more thorough and human-aligned evaluation of the quality of generated object by different methods. A full test case is shown in Fig. <a href="#A4.F18" title="Figure 18 ‣ D.2 Prompts for MV-LLaVA Instruct Tuning ‣ Appendix D Details of Prompt Design ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>. Left 9-view image is rendered from object generated by Bootstrap3D and the right one generated by Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. We ask GPT-4V to mainly evaluate through comparison based on three dimensions: text-image alignment, low-level texture quality and 3D plausibility. The answer of GPT-4V shows its in depth perception ability of given reasonable comparison well aligned with human preference. We thus choose to use GPT-4V rather than human volunteers to give reasonable evaluation.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">We adopt the 110 test prompts proposed in GPTeval3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> to test Bootstrap3D generated object comparing with Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> and MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. For each methods, we conditioned model based on 110 test prompts with 4 different seeds, with each methods generates 440 objects, we make 1-to-1 comparison following aforementioned test setting. Results are reported in Tab. <a href="#A5.T10" title="Table 10 ‣ Appendix E GPT-4V based 3D Object Generation Evaluation. ‣ Bootstrap3D: Improving 3D Content Creation with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. Except MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> (SDS) (which generates single object consuming 30 mins while Bootstrap3D only need 5 seconds.). Bootstrap3D excels in all three evaluation dimensions, which proves the ability of Bootstrap3D in creating high quality 3D objects.</p>
</div>
<figure id="A5.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A5.T10.3.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>: </span><span id="A5.T10.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">GPT-4V based evaluation result.<span id="A5.T10.4.2.1" class="ltx_text ltx_font_medium"> the result is in format of "number of objects preferred geneated by Bootstrap3D/ that of other methods". Cases when GPT cannot answer the question or generates "cannot decide" answer are excluded.</span></span></figcaption>
<div id="A5.T10.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:491.5pt;height:79.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.5pt,5.4pt) scale(0.88,0.88) ;">
<table id="A5.T10.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A5.T10.5.1.1.1" class="ltx_tr">
<th id="A5.T10.5.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A5.T10.5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Image-text alignment</th>
<th id="A5.T10.5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Texture quality</th>
<th id="A5.T10.5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">3D plausibility</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A5.T10.5.1.2.1" class="ltx_tr">
<th id="A5.T10.5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Compared to Instant3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (unofficial)</th>
<td id="A5.T10.5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">247 / 116</td>
<td id="A5.T10.5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">202 / 162</td>
<td id="A5.T10.5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">259 / 110</td>
</tr>
<tr id="A5.T10.5.1.3.2" class="ltx_tr">
<th id="A5.T10.5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Compared to Zero123++ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>
</th>
<td id="A5.T10.5.1.3.2.2" class="ltx_td ltx_align_center">192 / 143</td>
<td id="A5.T10.5.1.3.2.3" class="ltx_td ltx_align_center">210 / 161</td>
<td id="A5.T10.5.1.3.2.4" class="ltx_td ltx_align_center">231 / 139</td>
</tr>
<tr id="A5.T10.5.1.4.3" class="ltx_tr">
<th id="A5.T10.5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Compared to MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> (GRM)</th>
<td id="A5.T10.5.1.4.3.2" class="ltx_td ltx_align_center">290 / 71</td>
<td id="A5.T10.5.1.4.3.3" class="ltx_td ltx_align_center">245 / 131</td>
<td id="A5.T10.5.1.4.3.4" class="ltx_td ltx_align_center">284 / 102</td>
</tr>
<tr id="A5.T10.5.1.5.4" class="ltx_tr">
<th id="A5.T10.5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Compared to MVDream <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> (SDS)</th>
<td id="A5.T10.5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">188 / 155</td>
<td id="A5.T10.5.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">173 / 190</td>
<td id="A5.T10.5.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">192 / 150</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>More Results Visualization</h2>

<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Comparison with Other Methods</h3>

<figure id="A6.F19" class="ltx_figure"><img src="/html/2406.00093/assets/x19.png" id="A6.F19.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F19.3.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span id="A6.F19.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods</span></figcaption>
</figure>
<figure id="A6.F20" class="ltx_figure"><img src="/html/2406.00093/assets/x20.png" id="A6.F20.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F20.3.1.1" class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span id="A6.F20.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods</span></figcaption>
</figure>
<figure id="A6.F21" class="ltx_figure"><img src="/html/2406.00093/assets/x21.png" id="A6.F21.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F21.3.1.1" class="ltx_text" style="font-size:90%;">Figure 21</span>: </span><span id="A6.F21.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods</span></figcaption>
</figure>
<figure id="A6.F22" class="ltx_figure"><img src="/html/2406.00093/assets/x22.png" id="A6.F22.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F22.3.1.1" class="ltx_text" style="font-size:90%;">Figure 22</span>: </span><span id="A6.F22.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods</span></figcaption>
</figure>
<figure id="A6.F23" class="ltx_figure"><img src="/html/2406.00093/assets/x23.png" id="A6.F23.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F23.3.1.1" class="ltx_text" style="font-size:90%;">Figure 23</span>: </span><span id="A6.F23.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods</span></figcaption>
</figure>
</section>
<section id="A6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Visualization of Generated objects with Different Styles</h3>

<figure id="A6.F24" class="ltx_figure"><img src="/html/2406.00093/assets/x24.png" id="A6.F24.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F24.3.1.1" class="ltx_text" style="font-size:90%;">Figure 24</span>: </span><span id="A6.F24.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods with different style control.</span></figcaption>
</figure>
<figure id="A6.F25" class="ltx_figure"><img src="/html/2406.00093/assets/x25.png" id="A6.F25.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F25.3.1.1" class="ltx_text" style="font-size:90%;">Figure 25</span>: </span><span id="A6.F25.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods with different style control.</span></figcaption>
</figure>
<figure id="A6.F26" class="ltx_figure"><img src="/html/2406.00093/assets/x26.png" id="A6.F26.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F26.3.1.1" class="ltx_text" style="font-size:90%;">Figure 26</span>: </span><span id="A6.F26.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods with different style control.</span></figcaption>
</figure>
<figure id="A6.F27" class="ltx_figure"><img src="/html/2406.00093/assets/x27.png" id="A6.F27.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F27.3.1.1" class="ltx_text" style="font-size:90%;">Figure 27</span>: </span><span id="A6.F27.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods with different style control.</span></figcaption>
</figure>
<figure id="A6.F28" class="ltx_figure"><img src="/html/2406.00093/assets/x28.png" id="A6.F28.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F28.3.1.1" class="ltx_text" style="font-size:90%;">Figure 28</span>: </span><span id="A6.F28.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods with different style control.</span></figcaption>
</figure>
<figure id="A6.F29" class="ltx_figure"><img src="/html/2406.00093/assets/x29.png" id="A6.F29.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="559" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A6.F29.3.1.1" class="ltx_text" style="font-size:90%;">Figure 29</span>: </span><span id="A6.F29.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualization of generated objects compared to other edge-cutting methods with different style control.</span></figcaption>
</figure>
</section>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Broader Impacts</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p"><span id="A7.p1.1.1" class="ltx_text ltx_font_bold">Potential positive societal impacts:</span> The proposed framework, Bootstrap3D, enhances the quality and consistency of 3D models, which can benefit various industries such as entertainment, education, virtual reality, and digital art. By generating and sharing a large synthetic dataset of high-quality synthetic multi-view images, We will promotes open access to resources that can accelerate progress in the field. The model and data can serve as educational tools for students and researchers, fostering learning and innovation in machine learning and 3D modeling.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p"><span id="A7.p2.1.1" class="ltx_text ltx_font_bold">Potential negative societal impacts:</span> High-quality 3D models could be used to create deepfakes or misleading content, which may contribute to disinformation or malicious activities. Monitoring and Defense Mechanisms: Developing tools to detect and prevent the misuse of the generated 3D models, particularly in contexts like disinformation and surveillance. There may be unintended biases in the generated data or models, leading to unfair treatment of specific groups if the technology is deployed in applications affecting societal decision-making.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2406.00092" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2406.00093" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2406.00093">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2406.00093" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2406.00094" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Jul  5 21:08:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
