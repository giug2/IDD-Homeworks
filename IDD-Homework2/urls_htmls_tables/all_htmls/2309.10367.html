<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.10367] Toward efficient resource utilization at edge nodes in federated learning</title><meta property="og:description" content="Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Toward efficient resource utilization at edge nodes in federated learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Toward efficient resource utilization at edge nodes in federated learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.10367">

<!--Generated on Wed Feb 28 04:13:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Toward efficient resource utilization at edge nodes in federated learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sadi Alawadi
<br class="ltx_break">Department of Computer Science 
<br class="ltx_break">Blekinge Institute of Technology, Karlskrona, Sweden
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">sadi.alawadi@bth.se </span>
&amp;Addi Ait-Mlouk 
<br class="ltx_break">School of Informatics, University of Skövde,
<br class="ltx_break">Skövde, Sweden
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">addi.ait-mlouk@his.se </span>
&amp;Salman Toor
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Division of Scientific Computing
<br class="ltx_break">Uppsala University, Sweden
<br class="ltx_break">Scaleout Systems, Sweden
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">salman.toor@it.uu.se</span>
&amp;Andreas Hellander 
<br class="ltx_break">Department of Information Technology
<br class="ltx_break">Division of Scientific Computing
<br class="ltx_break">Uppsala University, Sweden
<br class="ltx_break">Scaleout Systems, Sweden
<br class="ltx_break"><span id="id4.4.id4" class="ltx_text ltx_font_typewriter">andreas.hellander@it.uu.se</span>
</span><span class="ltx_author_notes"><span id="id5.5.id1" class="ltx_text ltx_font_typewriter">Corresponding author.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Federated learning (FL) enables edge nodes to collaboratively contribute to constructing a global model without sharing their data. This is accomplished by devices computing local, private model updates that are then aggregated by a server. However, computational resource constraints and network communication can become a severe bottleneck for larger model sizes typical for deep learning applications. Edge nodes tends to have limited hardware resources (RAM, CPU), and the network bandwidth and reliability at the edge is a concern for scaling federated fleet applications. In this paper, we propose and evaluate a FL strategy inspired by transfer learning in order to reduce resource utilization on devices, as well as the load on the server and network in each global training round. For each local model update, we randomly select layers to train, freezing the remaining part of the model. In doing so, we can reduce both server load and communication costs per round by excluding all untrained layer weights from being transferred to the server. The goal of this study is to empirically explore the potential trade-off between resource utilization on devices and global model convergence under the proposed strategy. We implement the approach using the federated learning framework FEDn. A number of experiments were carried out over different datasets (CIFAR-10, CASA, and IMDB), performing different tasks using different deep-learning model architectures. Our results show that training the model partially can accelerate the training process, efficiently utilizes resources on-device, and reduce the data transmission by around 75% and 53% when we train 25%, and 50% of the model layers, respectively, without harming the resulting global model accuracy. Furthermore, our results demonstrate a negative correlation between the number of participating clients in the training process and the number of layers that need to be trained on each client’s side. As the number of clients increases, there is a decrease in the required number of layers. This observation highlights the potential of the approach, particularly in cross-device use cases.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.5" class="ltx_p"><em id="p1.5.1" class="ltx_emph ltx_font_bold ltx_font_italic">Keywords</em> Distributed Training  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Data Privacy <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Federated Learning <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Machine Learning <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
Training Parallelization <math id="p1.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation></semantics></math>
Partial Training.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a privacy-preserving machine learning (ML) training strategy introduced by McMahan et al <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>.
In FL, edge nodes contribute to a global model by locally computing partial model updates, which are then exchanged with a server and combined/aggregated into a global model. By iterating this process, we avoid sharing or transferring private data <cite class="ltx_cite ltx_citemacro_cite">Konečnỳ et al. (<a href="#bib.bib2" title="" class="ltx_ref">2016a</a>)</cite> – instead of moving data to a central server, the model implementation is transferred to the data owners’ local sites, where model training occurs. In this sense, Federated Learning falls in the category of decentralized optimization.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In the most basic FL architecture, a single central server constructs a global model in each communication (training) round by aggregating model parameters sent by the edge nodes. The limited internet connection bandwidth makes the model weight transfer between the edge nodes and server a bottleneck which contributes significantly to the training time of each round <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>. In addition,
training a large and complex Neural Network (NN) model at the edge node requires non-trivial time and computational resources (memory, network and CPU). Taken together, it is a challenge to accommodate increasingly complex models in cases where the network connection and the edge hardware have limitations.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">Federated learning often involves edge devices that are not homogeneous and have varying computational capabilities. As a result, the research community has focused on addressing the challenges posed by weak computing capacity devices, commonly referred to as stragglers <cite class="ltx_cite ltx_citemacro_cite">Tandon et al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>); Xu et al. (<a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>. These devices can significantly slow the training process and may need to be excluded from the round. Consequently, training a complex model over massive data in federated settings is challenging due to the presence of stragglers, which can slow down the process and limit the node’s contribution to the round <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Therefore, Different approaches have been proposed that can reduce resource utilization during the training of a deep-learning (DL) model, both in centralized settings and decentralized settings. One such strategy is to freeze pre-trained model layers and to add a new output layer to be trained over the new task data, where its weights will be adapted based on the weights of the prior layers (transfer learning or fine-tuning) <cite class="ltx_cite ltx_citemacro_cite">Too et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>); Liu et al. (<a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite>. This approach is applicable in a centralized paradigm. For distributed settings, two main parallel strategies are in use to speed up training: 1) replicating a copy of the whole model overall cluster nodes and then using data mini-batches broadcasted by the cluster head (data parallelism) <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>); Zou et al. (<a href="#bib.bib10" title="" class="ltx_ref">2014</a>)</cite>, and 2) distributing the model’s layers over the cluster nodes, where each layer will be trained using the entire data set, and a cluster coordinator is responsible for parallel communication (model parallelism) <cite class="ltx_cite ltx_citemacro_cite">Vishnu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2016</a>); Hewett and Grady II (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>); Jia et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>); Shoeybi et al. (<a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Recently, a new approach inspired by transfer learning aims to selectively freeze a set of layers and only update the remaining layers in each iteration <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019a</a>)</cite>. In this paper, we explore the same strategy for local model training in a federated learning setting. The goal is to a) reduce the resource needs on the edge device and, in this way, fit larger models by reducing the training memory and CPU footprint for each update, and b) reduce network transfer costs by reducing the number of updated parameters in each iteration. We are specifically interested in investigating the feasibility of this approach for scenarios where the hardware on the device is too limited to train the entire model effectively (resource starvation). To summarize, the main contributions of this article are:</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Our proposed approach draws inspiration from transfer learning and aims to decrease the amount of resources used on edge nodes/clients. By doing so, we aim to enhance efficiency and dependability, thereby minimizing the occurrence of stragglers in a round. Additionally, this approach is intended to enable edge devices/nodes with constrained resources to operate more effectively and provide a stable training environment.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Reduce the amount of transferred data (gradients, weights) over the network between the edge nodes and the model aggregator.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">Systematically evaluates the potential of the approach for varying resource availability on the client side, including the Jetson Nano <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://developer.nvidia.com/embedded/jetson-nano-developer-kit</span></span></span> as a constrained edge device.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p">The rest of this paper is organized as follows. Section <a href="#S3" title="3 Federated machine learning and relevant schemes ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides background on federated learning, model fine-tuning and transfer learning, as well as how the proposed approach relates to general parallel training strategies. Next, we introduce the proposed approach in Section <a href="#S4" title="4 Proposed Approach ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Section <a href="#S5" title="5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> describes the experimental settings and the results, followed by a summary of related work in Section <a href="#S2" title="2 Related Work ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Finally, we conclude the paper in Section <a href="#S6" title="6 Conclusion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Several studies have been investigated speeding up and reducing the cost of training DL models. These studies are oriented to solve different problems before and during the training process, such as complex and large models, shortage in the training sample, a vast amount of training data that demand high computational resources and time. Transfer Learning has been proposed to tackle lack of training data via transferring the knowledge from a related pre-trained model to a new task <cite class="ltx_cite ltx_citemacro_cite">Pan and Yang (<a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite> which is widely used in image processing and natural language processing (NLP).</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p">Different approaches have been proposed to distribute the training process workload across a group of machines (Cluster). For large models Dean et. el <cite class="ltx_cite ltx_citemacro_cite">Dean et al. (<a href="#bib.bib17" title="" class="ltx_ref">2012</a>)</cite> have developed a DistBelief framework that supports DL model parallelism. The framework trains large models over a computing cluster with thousands of machines. It comprises two main algorithms, Downpour SGD, responsible for a large number of model replica and adaptive learning rate, while Sandblaster is accountable for the parallelization process. In this study, <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> authors have investigated the space of parallelization strategies (i.e. SOAP), which includes Sample, Operation, Attribute, and Parameter dimensions to parallelize a DL model. Furthermore, they proposed FlexFlow framework <cite class="ltx_cite ltx_citemacro_cite">Jia et al. (<a href="#bib.bib13" title="" class="ltx_ref">2018</a>)</cite> that uses the SOAP space to search randomly for a fast parallelization strategy for a specific machine. In addition to the frameworks mentioned above, more research has been conducted on model parallelism, such as <cite class="ltx_cite ltx_citemacro_cite">Sergeev and Del Balso (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>); Park et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); Gaunt et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>); Chen et al. (<a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p">For a large training data sample, Valiant <cite class="ltx_cite ltx_citemacro_cite">Valiant (<a href="#bib.bib21" title="" class="ltx_ref">1990</a>)</cite> has introduced the bulk-synchronous parallel (BSP) model, which parallelises the training process by two main steps. Firstly, place a replica of the whole DL model on each device. Secondly, split the training dataset into mini-batches then distribute them among multiple workers to train the DL model.
Finally, each worker synchronises model parameters with a different worker at the end of each iteration <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a href="#bib.bib22" title="" class="ltx_ref">2012</a>)</cite>. Moreover, Tensorflow <cite class="ltx_cite ltx_citemacro_cite">ten (<a href="#bib.bib23" title="" class="ltx_ref">2017</a>)</cite>, Coffe2 <cite class="ltx_cite ltx_citemacro_cite">caf (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>)</cite>, Pytorch <cite class="ltx_cite ltx_citemacro_cite">pyt (<a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite> frameworks have been used both data and model parallelisem to parallelise DL training process. Nevertheless, Data parallelism is an efficient technique that can train a small DL model with few parameters. While, in a large model’s case, this becomes an inefficient strategy that caus a scalability jam in large scale distributed training environments.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p">The authors <cite class="ltx_cite ltx_citemacro_cite">Brock et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite> proposed a Freezeout approach to accelerate the training process by training each hidden layer in the model for a set part of the training schedule, "freezing out layers" progressively and the back-propagation of these layers is avoided. Also, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">Xu-hui et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> have followed the same strategy proposed in <cite class="ltx_cite ltx_citemacro_cite">Brock et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite> to train the model by freezing the hidden layers out one by one.
A new approach has been proposed by Xueli et al. <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a href="#bib.bib28" title="" class="ltx_ref">2019b</a>)</cite> to freeze layers intelligently during the training phase, where the differences of the normalized gradient for all weighted layers have been computed to identify the number of layers that should be frozen. This approach has been developed on top of stochastic gradient descent (SGD) and evaluated using large models (i.e. VGG, ResNets, and DenseNets) in a centralized fashion.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p">Identifying the number of the freezing layers has been investigated in <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite> during the transformer fine-tuning process for well know pre-trained models (i.e. BERT, and RoBERTa) in the NLP field. Moreover, Yuhan et al.<cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021b</a>)</cite> were proposed AutoFreeze framework
for automatically freezing layers to speed up fine-tuning by applying an adaptive approach to identify all layers that need to be trained while maintaining accuracy. Also, multiple mechanisms have been developed to decrease the forward computation time while conducting model fine-tuning by enabling client caching of intermediate activation’s.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p">Based on the fact that the internal layer’s training progress differ significantly, a knowledge-guided training system (KGT) <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite> has been proposed to focus more on those layers. Sub consequently, KGT skips part of the computations and communications in the deep neural network’s (DNN) internal layers (hidden layers) to accelerate the training process while maintaining accuracy.</p>
</div>
<div id="S2.p7" class="ltx_para ltx_noindent">
<p id="S2.p7.1" class="ltx_p">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> were proposed a new scheme named Adaptive Parameter
Freezing (APF)
tackle the communication bottleneck in federated learning settings. The APF is responsible for freezing and unfreezing converged parameters during the training rounds for intermittent periods. The model was fully trained to identify the stable and unstable parameters for several rounds, and then APF freezes the stable parameters based on threshold increase gradually. In addition, the authors have introduced a mechanism that dynamically adjusts the stability threshold at runtime when most of the parameters have been classified as stable and decreases the stability threshold by one half. The results reveal that this scheme has reduced the communication volume without compromising the model accuracy. However, this approach still relies on memory to cash the prior parameters to check their stability and also requires both CPUs and RAMs to train the entire model at the beginning and each unfreezes period. While our approach train sub-layers of the model selected randomly, which significantly impacts the resources, training time, transferred data, and the final model accuracy, as shown in our results.
</p>
</div>
<div id="S2.p8" class="ltx_para ltx_noindent">
<p id="S2.p8.1" class="ltx_p">Our approach stands out from existing methods in this context due to its unique training strategy in Federated Learning (FL) settings. Our strategy focuses on training different parts of the model every communication round, aiming to achieve several key goals. These goals include, <span id="S2.p8.1.1" class="ltx_text ltx_font_italic">reducing resource utilization, minimizing the communication flow, and enabling the
restricted edge devices to participate in the training process without sharing the raw data</span>. Unlike other approaches reported in the literature, where a complete model must be shared to train the model, our approach avoids the need to share a complete model. Instead, we adopt a more efficient technique where each client randomly selects a different model layer every communication round to be trained, which enables stragglers (constrained devices) to effectively participate in each round of the federated training.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Federated machine learning and relevant schemes</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Federated Machine Learning (FL) is a new solution proposed by google <cite class="ltx_cite ltx_citemacro_cite">Konečnỳ et al. (<a href="#bib.bib2" title="" class="ltx_ref">2016a</a>, <a href="#bib.bib32" title="" class="ltx_ref">b</a>)</cite> to preserve data privacy that aliens with General Data Protection Regulation (GDPR). FL enables users or organizations to collaboratively train a machine learning model without transferring their own data to a central storage system. Instead, the code is moved to the data owners’ local sites in such a paradigm; incremental local updates are combined into a global model every communication round. Typically, a server or aggregator is responsible for managing the client nodes, communication, distributing the global model weight, controlling the training rounds, and generating a global model from received models using FEDAvg<cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>, FEDProx <cite class="ltx_cite ltx_citemacro_cite">Sahu et al. (<a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite> and many other aggregation methods. Firstly, the server starts the training by distributing the ML model architecture with random weight. Secondly, each client will receive the shared model, initiate the training process using local data, and share the weight updates with the server. Third, in each training round, the server combines all obtained weight updates using FEDAvg, or other aggregation methods to update the global model weight. Finally, new weights are distributed across the clients to start a new training round; in this sense, all clients have shared their knowledge without sharing their data <cite class="ltx_cite ltx_citemacro_cite">Ekmefjord et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.
Hence, FL maintains data privacy, reduces data transferring costs, and shares different client or organization knowledge, especially in healthcare scenarios where the data is more sensitive. In addition, learning from different sources is required to capture more knowledge about a given problem.<cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Transfer Learning and Model Fine-tuning</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">The shortage of the data samples needed to train a ML model enables transfer learning and fine-tuning to reduce the training cost through transferring the knowledge from a large pre-trained model (source) to
a new model (destination) using the small number of new samples that will perform new tasks <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib36" title="" class="ltx_ref">2019</a>); Vrbančič and Podgorelec (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">In transfer learning, only the last few layers of a large source model will be trained over a new dataset for a specific task and adapt their weights based on the prior layers <cite class="ltx_cite ltx_citemacro_cite">Vrbančič and Podgorelec (<a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>. While in the fine-tuning case, the whole pre-trained model is trained over a new dataset and new task <cite class="ltx_cite ltx_citemacro_cite">Too et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. Despite their difference, both techniques share the common aim of enabling the use of DL in situations where there is a dearth of training data. This is achieved by transferring knowledge across related domains, substantially reducing training time and resource consumption in a centralized fashion.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Training Parallelization Techniques</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">Here we discuss the two main approaches that have been introduced to address challenges in large-scale model training. In both cases, the focus is on parallelizing the training process by utilizing large computational resources</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i1.p1.1" class="ltx_p">Training deep learning models requires a massive amount of data and computational resources, which can be time-consuming. The data parallelism <cite class="ltx_cite ltx_citemacro_cite">Valiant (<a href="#bib.bib21" title="" class="ltx_ref">1990</a>)</cite> approach has been widely used to address this challenge, using a parameter server architecture to distribute the training workload across multiple workers and speed up the process. In this approach, the training dataset is divided into mini-batches and each worker is assigned a different subset. The parameter server maintains a full copy of the DL model and communicates with workers to synchronize gradients or weights <cite class="ltx_cite ltx_citemacro_cite">Shallue et al. (<a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite>. Each worker receives a copy of the DL model and a mini-batch of data, computes local gradients, and shares them with the server parameter. The server then updates the model parameters based on the gradients received from the workers, using negative gradient direction or parameter averaging. The latest values are shared with the workers <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); Shazeer et al. (<a href="#bib.bib39" title="" class="ltx_ref">2018</a>)</cite>. However, the communication channel bandwidth capacity can become a bottleneck as more workers join the training process, leading to slower processing times. To address this issue, a new architecture called AllReduce operations was proposed, which does not depend on the number of workers in the training process to maintain communication channel capacity <cite class="ltx_cite ltx_citemacro_cite">Thakur et al. (<a href="#bib.bib40" title="" class="ltx_ref">2005</a>)</cite>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i2.p1.1" class="ltx_p">To tackle complex tasks that require large DL models with millions of parameters, training these models can be very computationally demanding and resource-intensive. Model parallelism is a useful approach to efficiently train these models by creating a cluster of worker nodes with a coordinator to parallelize the training process <cite class="ltx_cite ltx_citemacro_cite">Dean et al. (<a href="#bib.bib17" title="" class="ltx_ref">2012</a>)</cite>. The DL model is split into sub-layers and distributed among multiple workers, with the coordinator maintaining the model layers in sequential order, managing data flow, and communicating between all workers. Each worker is assigned a mini-batch of data that is shared among all workers to update the worker-assigned model layers’ gradients. Finally, the coordinator combines all the layers received from workers to produce the final model. This approach can be implemented in multi-GPU cores, where each core acts as an independent worker <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky (<a href="#bib.bib41" title="" class="ltx_ref">2014</a>); Park et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p">In this context, we propose a new approach inspired by transfer learning and parallel training to randomly train a deep-learning model based on freezing particular trainable layers randomly. As shown in the next section, this approach reduces the amount of transferred updated local parameters, improving communication efficiency, model training performance, and efficiently utilising the device resources.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Approach</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">The approach we propose involves selectively freezing layers during client updates. This technique can be adapted to work with any federated training strategy or aggregation scheme, and it only requires modifications to be made on the client side. As a result, it offers a high degree of flexibility and versatility, making it easy to integrate into existing federated learning workflows.
While there are many different aggregation schemes with different properties, we have chosen to exemplify with the Federated Averaging (FedAvg) strategy in this work. FedAvg is an established FL aggregation algorithm proposed in <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.7" class="ltx_p">The FedAvg algorithm is a decentralized version of Stochastic Gradient Descent (SGD). In each training round, a subset of <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">C</annotation></semantics></math> clients receive a copy of the recent global model. These clients execute <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.p2.2.m2.1a"><mi id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><ci id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">E</annotation></semantics></math> epochs over their local dataset <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="D_{k}" display="inline"><semantics id="S4.p2.3.m3.1a"><msub id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">D</mi><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1">subscript</csymbol><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">𝐷</ci><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">D_{k}</annotation></semantics></math> to update the model weights <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="W_{k}" display="inline"><semantics id="S4.p2.4.m4.1a"><msub id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">W</mi><mi id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1">subscript</csymbol><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">𝑊</ci><ci id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">W_{k}</annotation></semantics></math>.
A new global model is constructed once the updated weights <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="W_{k}" display="inline"><semantics id="S4.p2.5.m5.1a"><msub id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml"><mi id="S4.p2.5.m5.1.1.2" xref="S4.p2.5.m5.1.1.2.cmml">W</mi><mi id="S4.p2.5.m5.1.1.3" xref="S4.p2.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><apply id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m5.1.1.1.cmml" xref="S4.p2.5.m5.1.1">subscript</csymbol><ci id="S4.p2.5.m5.1.1.2.cmml" xref="S4.p2.5.m5.1.1.2">𝑊</ci><ci id="S4.p2.5.m5.1.1.3.cmml" xref="S4.p2.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">W_{k}</annotation></semantics></math> have been computed and sent back to the server. This new global model is built by averaging all the received updated weights <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="W_{k}" display="inline"><semantics id="S4.p2.6.m6.1a"><msub id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml"><mi id="S4.p2.6.m6.1.1.2" xref="S4.p2.6.m6.1.1.2.cmml">W</mi><mi id="S4.p2.6.m6.1.1.3" xref="S4.p2.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><apply id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p2.6.m6.1.1.1.cmml" xref="S4.p2.6.m6.1.1">subscript</csymbol><ci id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.2">𝑊</ci><ci id="S4.p2.6.m6.1.1.3.cmml" xref="S4.p2.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">W_{k}</annotation></semantics></math> contributed by all participating clients <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p2.7.m7.1a"><mi id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><ci id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">n</annotation></semantics></math> every round.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">. This weighted average is then utilized to generate a new version of the global model <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">M</annotation></semantics></math>. This process is repeated until the model converges. The FedAvg algorithm, which is unchanged in this paper, is outlined in Algorithm <a href="#alg1" title="In 4 Proposed Approach ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<table id="Sx1.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\displaystyle M=\sum_{k=1}^{C}\frac{n_{k}}{n}W_{k}^{(i)}." display="inline"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2.1" xref="S4.E1.m1.2.2.1.1.cmml"><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.1.cmml"><mi id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.1.2.cmml">M</mi><mo id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.1.3.cmml"><mstyle displaystyle="true" id="S4.E1.m1.2.2.1.1.3.1" xref="S4.E1.m1.2.2.1.1.3.1.cmml"><munderover id="S4.E1.m1.2.2.1.1.3.1a" xref="S4.E1.m1.2.2.1.1.3.1.cmml"><mo movablelimits="false" id="S4.E1.m1.2.2.1.1.3.1.2.2" xref="S4.E1.m1.2.2.1.1.3.1.2.2.cmml">∑</mo><mrow id="S4.E1.m1.2.2.1.1.3.1.2.3" xref="S4.E1.m1.2.2.1.1.3.1.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.3.1.2.3.2" xref="S4.E1.m1.2.2.1.1.3.1.2.3.2.cmml">k</mi><mo id="S4.E1.m1.2.2.1.1.3.1.2.3.1" xref="S4.E1.m1.2.2.1.1.3.1.2.3.1.cmml">=</mo><mn id="S4.E1.m1.2.2.1.1.3.1.2.3.3" xref="S4.E1.m1.2.2.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E1.m1.2.2.1.1.3.1.3" xref="S4.E1.m1.2.2.1.1.3.1.3.cmml">C</mi></munderover></mstyle><mrow id="S4.E1.m1.2.2.1.1.3.2" xref="S4.E1.m1.2.2.1.1.3.2.cmml"><mstyle displaystyle="true" id="S4.E1.m1.2.2.1.1.3.2.2" xref="S4.E1.m1.2.2.1.1.3.2.2.cmml"><mfrac id="S4.E1.m1.2.2.1.1.3.2.2a" xref="S4.E1.m1.2.2.1.1.3.2.2.cmml"><msub id="S4.E1.m1.2.2.1.1.3.2.2.2" xref="S4.E1.m1.2.2.1.1.3.2.2.2.cmml"><mi id="S4.E1.m1.2.2.1.1.3.2.2.2.2" xref="S4.E1.m1.2.2.1.1.3.2.2.2.2.cmml">n</mi><mi id="S4.E1.m1.2.2.1.1.3.2.2.2.3" xref="S4.E1.m1.2.2.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="S4.E1.m1.2.2.1.1.3.2.2.3" xref="S4.E1.m1.2.2.1.1.3.2.2.3.cmml">n</mi></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.3.2.1" xref="S4.E1.m1.2.2.1.1.3.2.1.cmml">​</mo><msubsup id="S4.E1.m1.2.2.1.1.3.2.3" xref="S4.E1.m1.2.2.1.1.3.2.3.cmml"><mi id="S4.E1.m1.2.2.1.1.3.2.3.2.2" xref="S4.E1.m1.2.2.1.1.3.2.3.2.2.cmml">W</mi><mi id="S4.E1.m1.2.2.1.1.3.2.3.2.3" xref="S4.E1.m1.2.2.1.1.3.2.3.2.3.cmml">k</mi><mrow id="S4.E1.m1.1.1.1.3" xref="S4.E1.m1.2.2.1.1.3.2.3.cmml"><mo stretchy="false" id="S4.E1.m1.1.1.1.3.1" xref="S4.E1.m1.2.2.1.1.3.2.3.cmml">(</mo><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S4.E1.m1.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.3.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow><mo lspace="0em" id="S4.E1.m1.2.2.1.2" xref="S4.E1.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.1.1.cmml" xref="S4.E1.m1.2.2.1"><eq id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"></eq><ci id="S4.E1.m1.2.2.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.2">𝑀</ci><apply id="S4.E1.m1.2.2.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3"><apply id="S4.E1.m1.2.2.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.1.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.3.1.2.cmml" xref="S4.E1.m1.2.2.1.1.3.1"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1">subscript</csymbol><sum id="S4.E1.m1.2.2.1.1.3.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.1.2.2"></sum><apply id="S4.E1.m1.2.2.1.1.3.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.1.2.3"><eq id="S4.E1.m1.2.2.1.1.3.1.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.1.2.3.1"></eq><ci id="S4.E1.m1.2.2.1.1.3.1.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.3.1.2.3.3.cmml" xref="S4.E1.m1.2.2.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E1.m1.2.2.1.1.3.1.3.cmml" xref="S4.E1.m1.2.2.1.1.3.1.3">𝐶</ci></apply><apply id="S4.E1.m1.2.2.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2"><times id="S4.E1.m1.2.2.1.1.3.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2.1"></times><apply id="S4.E1.m1.2.2.1.1.3.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2"><divide id="S4.E1.m1.2.2.1.1.3.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2"></divide><apply id="S4.E1.m1.2.2.1.1.3.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.2.2.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2.2">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.3.2.2.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2.2.2">𝑛</ci><ci id="S4.E1.m1.2.2.1.1.3.2.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2.2.3">𝑘</ci></apply><ci id="S4.E1.m1.2.2.1.1.3.2.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.2.2.3">𝑛</ci></apply><apply id="S4.E1.m1.2.2.1.1.3.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.2.3.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3">superscript</csymbol><apply id="S4.E1.m1.2.2.1.1.3.2.3.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.3.2.3.2.1.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3">subscript</csymbol><ci id="S4.E1.m1.2.2.1.1.3.2.3.2.2.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3.2.2">𝑊</ci><ci id="S4.E1.m1.2.2.1.1.3.2.3.2.3.cmml" xref="S4.E1.m1.2.2.1.1.3.2.3.2.3">𝑘</ci></apply><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\displaystyle M=\sum_{k=1}^{C}\frac{n_{k}}{n}W_{k}^{(i)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p">Each iteration on the client side is outlined in Algorithm <a href="#alg2" title="In 4 Proposed Approach ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The only difference from standard FedAvg is the layer selection step (line 3). In our approach, a random layer selection strategy is used, although future work could explore more advanced selection strategies.</p>
</div>
<figure id="alg1" class="ltx_float ltx_algorithm">
<div id="alg1.12" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="alg1.1.1" class="ltx_listingline">
<span id="alg1.1.1.1" class="ltx_text"><span id="alg1.1.1.1.1" class="ltx_text ltx_font_bold">Input:</span> </span><math id="alg1.1.1.m1.1" class="ltx_Math" alttext="W_{t}" display="inline"><semantics id="alg1.1.1.m1.1a"><msub id="alg1.1.1.m1.1.1" xref="alg1.1.1.m1.1.1.cmml"><mi id="alg1.1.1.m1.1.1.2" xref="alg1.1.1.m1.1.1.2.cmml">W</mi><mi id="alg1.1.1.m1.1.1.3" xref="alg1.1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.1.1.m1.1b"><apply id="alg1.1.1.m1.1.1.cmml" xref="alg1.1.1.m1.1.1"><csymbol cd="ambiguous" id="alg1.1.1.m1.1.1.1.cmml" xref="alg1.1.1.m1.1.1">subscript</csymbol><ci id="alg1.1.1.m1.1.1.2.cmml" xref="alg1.1.1.m1.1.1.2">𝑊</ci><ci id="alg1.1.1.m1.1.1.3.cmml" xref="alg1.1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.1.1.m1.1c">W_{t}</annotation></semantics></math>
</div>
<div id="alg1.2.2" class="ltx_listingline">
<span id="alg1.2.2.1" class="ltx_text"><span id="alg1.2.2.1.1" class="ltx_text ltx_font_bold">Output:</span> </span><math id="alg1.2.2.m1.1" class="ltx_Math" alttext="M(W_{t})" display="inline"><semantics id="alg1.2.2.m1.1a"><mrow id="alg1.2.2.m1.1.1" xref="alg1.2.2.m1.1.1.cmml"><mi id="alg1.2.2.m1.1.1.3" xref="alg1.2.2.m1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="alg1.2.2.m1.1.1.2" xref="alg1.2.2.m1.1.1.2.cmml">​</mo><mrow id="alg1.2.2.m1.1.1.1.1" xref="alg1.2.2.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.2.2.m1.1.1.1.1.2" xref="alg1.2.2.m1.1.1.1.1.1.cmml">(</mo><msub id="alg1.2.2.m1.1.1.1.1.1" xref="alg1.2.2.m1.1.1.1.1.1.cmml"><mi id="alg1.2.2.m1.1.1.1.1.1.2" xref="alg1.2.2.m1.1.1.1.1.1.2.cmml">W</mi><mi id="alg1.2.2.m1.1.1.1.1.1.3" xref="alg1.2.2.m1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.2.2.m1.1.1.1.1.3" xref="alg1.2.2.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.2.2.m1.1b"><apply id="alg1.2.2.m1.1.1.cmml" xref="alg1.2.2.m1.1.1"><times id="alg1.2.2.m1.1.1.2.cmml" xref="alg1.2.2.m1.1.1.2"></times><ci id="alg1.2.2.m1.1.1.3.cmml" xref="alg1.2.2.m1.1.1.3">𝑀</ci><apply id="alg1.2.2.m1.1.1.1.1.1.cmml" xref="alg1.2.2.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.2.2.m1.1.1.1.1.1.1.cmml" xref="alg1.2.2.m1.1.1.1.1">subscript</csymbol><ci id="alg1.2.2.m1.1.1.1.1.1.2.cmml" xref="alg1.2.2.m1.1.1.1.1.1.2">𝑊</ci><ci id="alg1.2.2.m1.1.1.1.1.1.3.cmml" xref="alg1.2.2.m1.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.2.m1.1c">M(W_{t})</annotation></semantics></math>
</div>
<div id="alg1.12.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>
<span id="alg1.12.13.1" class="ltx_text ltx_font_bold">Server executes:</span>
</div>
<div id="alg1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>
initialized <math id="alg1.3.3.m1.1" class="ltx_Math" alttext="W_{0}" display="inline"><semantics id="alg1.3.3.m1.1a"><msub id="alg1.3.3.m1.1.1" xref="alg1.3.3.m1.1.1.cmml"><mi id="alg1.3.3.m1.1.1.2" xref="alg1.3.3.m1.1.1.2.cmml">W</mi><mn id="alg1.3.3.m1.1.1.3" xref="alg1.3.3.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.3.3.m1.1b"><apply id="alg1.3.3.m1.1.1.cmml" xref="alg1.3.3.m1.1.1"><csymbol cd="ambiguous" id="alg1.3.3.m1.1.1.1.cmml" xref="alg1.3.3.m1.1.1">subscript</csymbol><ci id="alg1.3.3.m1.1.1.2.cmml" xref="alg1.3.3.m1.1.1.2">𝑊</ci><cn type="integer" id="alg1.3.3.m1.1.1.3.cmml" xref="alg1.3.3.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.3.3.m1.1c">W_{0}</annotation></semantics></math>

</div>
<div id="alg1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>

<span id="alg1.4.4.2" class="ltx_text ltx_font_bold">Function</span> <em id="alg1.4.4.1" class="ltx_emph ltx_font_typewriter">FedAVG(<em id="alg1.4.4.1.1.1" class="ltx_emph ltx_font_serif ltx_font_italic"><math id="alg1.4.4.1.1.1.m1.3" class="ltx_Math" alttext="k,W_{t-1},W_{t}" display="inline"><semantics id="alg1.4.4.1.1.1.m1.3a"><mrow id="alg1.4.4.1.1.1.m1.3.3.2" xref="alg1.4.4.1.1.1.m1.3.3.3.cmml"><mi id="alg1.4.4.1.1.1.m1.1.1" xref="alg1.4.4.1.1.1.m1.1.1.cmml">k</mi><mo id="alg1.4.4.1.1.1.m1.3.3.2.3" xref="alg1.4.4.1.1.1.m1.3.3.3.cmml">,</mo><msub id="alg1.4.4.1.1.1.m1.2.2.1.1" xref="alg1.4.4.1.1.1.m1.2.2.1.1.cmml"><mi id="alg1.4.4.1.1.1.m1.2.2.1.1.2" xref="alg1.4.4.1.1.1.m1.2.2.1.1.2.cmml">W</mi><mrow id="alg1.4.4.1.1.1.m1.2.2.1.1.3" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.cmml"><mi id="alg1.4.4.1.1.1.m1.2.2.1.1.3.2" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.2.cmml">t</mi><mo id="alg1.4.4.1.1.1.m1.2.2.1.1.3.1" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.1.cmml">−</mo><mn id="alg1.4.4.1.1.1.m1.2.2.1.1.3.3" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.3.cmml">1</mn></mrow></msub><mo id="alg1.4.4.1.1.1.m1.3.3.2.4" xref="alg1.4.4.1.1.1.m1.3.3.3.cmml">,</mo><msub id="alg1.4.4.1.1.1.m1.3.3.2.2" xref="alg1.4.4.1.1.1.m1.3.3.2.2.cmml"><mi id="alg1.4.4.1.1.1.m1.3.3.2.2.2" xref="alg1.4.4.1.1.1.m1.3.3.2.2.2.cmml">W</mi><mi id="alg1.4.4.1.1.1.m1.3.3.2.2.3" xref="alg1.4.4.1.1.1.m1.3.3.2.2.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.4.4.1.1.1.m1.3b"><list id="alg1.4.4.1.1.1.m1.3.3.3.cmml" xref="alg1.4.4.1.1.1.m1.3.3.2"><ci id="alg1.4.4.1.1.1.m1.1.1.cmml" xref="alg1.4.4.1.1.1.m1.1.1">𝑘</ci><apply id="alg1.4.4.1.1.1.m1.2.2.1.1.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="alg1.4.4.1.1.1.m1.2.2.1.1.1.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1">subscript</csymbol><ci id="alg1.4.4.1.1.1.m1.2.2.1.1.2.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1.2">𝑊</ci><apply id="alg1.4.4.1.1.1.m1.2.2.1.1.3.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3"><minus id="alg1.4.4.1.1.1.m1.2.2.1.1.3.1.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.1"></minus><ci id="alg1.4.4.1.1.1.m1.2.2.1.1.3.2.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.2">𝑡</ci><cn type="integer" id="alg1.4.4.1.1.1.m1.2.2.1.1.3.3.cmml" xref="alg1.4.4.1.1.1.m1.2.2.1.1.3.3">1</cn></apply></apply><apply id="alg1.4.4.1.1.1.m1.3.3.2.2.cmml" xref="alg1.4.4.1.1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="alg1.4.4.1.1.1.m1.3.3.2.2.1.cmml" xref="alg1.4.4.1.1.1.m1.3.3.2.2">subscript</csymbol><ci id="alg1.4.4.1.1.1.m1.3.3.2.2.2.cmml" xref="alg1.4.4.1.1.1.m1.3.3.2.2.2">𝑊</ci><ci id="alg1.4.4.1.1.1.m1.3.3.2.2.3.cmml" xref="alg1.4.4.1.1.1.m1.3.3.2.2.3">𝑡</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.4.4.1.1.1.m1.3c">k,W_{t-1},W_{t}</annotation></semantics></math></em>)</em><span id="alg1.4.4.3" class="ltx_text ltx_font_bold">:</span> 
</div>
<div id="alg1.12.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="alg1.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span id="alg1.6.6.3" class="ltx_text ltx_font_bold">foreach</span> <em id="alg1.6.6.2" class="ltx_emph ltx_font_italic"><math id="alg1.5.5.1.m1.1" class="ltx_Math" alttext="t\leftarrow 1" display="inline"><semantics id="alg1.5.5.1.m1.1a"><mrow id="alg1.5.5.1.m1.1.1" xref="alg1.5.5.1.m1.1.1.cmml"><mi id="alg1.5.5.1.m1.1.1.2" xref="alg1.5.5.1.m1.1.1.2.cmml">t</mi><mo stretchy="false" id="alg1.5.5.1.m1.1.1.1" xref="alg1.5.5.1.m1.1.1.1.cmml">←</mo><mn id="alg1.5.5.1.m1.1.1.3" xref="alg1.5.5.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.5.5.1.m1.1b"><apply id="alg1.5.5.1.m1.1.1.cmml" xref="alg1.5.5.1.m1.1.1"><ci id="alg1.5.5.1.m1.1.1.1.cmml" xref="alg1.5.5.1.m1.1.1.1">←</ci><ci id="alg1.5.5.1.m1.1.1.2.cmml" xref="alg1.5.5.1.m1.1.1.2">𝑡</ci><cn type="integer" id="alg1.5.5.1.m1.1.1.3.cmml" xref="alg1.5.5.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.5.5.1.m1.1c">t\leftarrow 1</annotation></semantics></math> <span id="alg1.6.6.2.1" class="ltx_text ltx_font_bold ltx_font_upright">to</span> <math id="alg1.6.6.2.m2.1" class="ltx_Math" alttext="r" display="inline"><semantics id="alg1.6.6.2.m2.1a"><mi id="alg1.6.6.2.m2.1.1" xref="alg1.6.6.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="alg1.6.6.2.m2.1b"><ci id="alg1.6.6.2.m2.1.1.cmml" xref="alg1.6.6.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.6.6.2.m2.1c">r</annotation></semantics></math></em> <span id="alg1.6.6.4" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="alg1.12.15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="alg1.7.7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <math id="alg1.7.7.m1.1" class="ltx_Math" alttext="S_{t}\leftarrow" display="inline"><semantics id="alg1.7.7.m1.1a"><mrow id="alg1.7.7.m1.1.1" xref="alg1.7.7.m1.1.1.cmml"><msub id="alg1.7.7.m1.1.1.2" xref="alg1.7.7.m1.1.1.2.cmml"><mi id="alg1.7.7.m1.1.1.2.2" xref="alg1.7.7.m1.1.1.2.2.cmml">S</mi><mi id="alg1.7.7.m1.1.1.2.3" xref="alg1.7.7.m1.1.1.2.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.7.7.m1.1.1.1" xref="alg1.7.7.m1.1.1.1.cmml">←</mo><mi id="alg1.7.7.m1.1.1.3" xref="alg1.7.7.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.7.7.m1.1b"><apply id="alg1.7.7.m1.1.1.cmml" xref="alg1.7.7.m1.1.1"><ci id="alg1.7.7.m1.1.1.1.cmml" xref="alg1.7.7.m1.1.1.1">←</ci><apply id="alg1.7.7.m1.1.1.2.cmml" xref="alg1.7.7.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.7.7.m1.1.1.2.1.cmml" xref="alg1.7.7.m1.1.1.2">subscript</csymbol><ci id="alg1.7.7.m1.1.1.2.2.cmml" xref="alg1.7.7.m1.1.1.2.2">𝑆</ci><ci id="alg1.7.7.m1.1.1.2.3.cmml" xref="alg1.7.7.m1.1.1.2.3">𝑡</ci></apply><csymbol cd="latexml" id="alg1.7.7.m1.1.1.3.cmml" xref="alg1.7.7.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.7.7.m1.1c">S_{t}\leftarrow</annotation></semantics></math> (sample a random set of clients)
</div>
<div id="alg1.8.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg1.8.8.2" class="ltx_text ltx_font_bold">foreach</span> <em id="alg1.8.8.1" class="ltx_emph ltx_font_italic">client <math id="alg1.8.8.1.m1.1" class="ltx_Math" alttext="k\in S_{t}" display="inline"><semantics id="alg1.8.8.1.m1.1a"><mrow id="alg1.8.8.1.m1.1.1" xref="alg1.8.8.1.m1.1.1.cmml"><mi id="alg1.8.8.1.m1.1.1.2" xref="alg1.8.8.1.m1.1.1.2.cmml">k</mi><mo id="alg1.8.8.1.m1.1.1.1" xref="alg1.8.8.1.m1.1.1.1.cmml">∈</mo><msub id="alg1.8.8.1.m1.1.1.3" xref="alg1.8.8.1.m1.1.1.3.cmml"><mi id="alg1.8.8.1.m1.1.1.3.2" xref="alg1.8.8.1.m1.1.1.3.2.cmml">S</mi><mi id="alg1.8.8.1.m1.1.1.3.3" xref="alg1.8.8.1.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.8.8.1.m1.1b"><apply id="alg1.8.8.1.m1.1.1.cmml" xref="alg1.8.8.1.m1.1.1"><in id="alg1.8.8.1.m1.1.1.1.cmml" xref="alg1.8.8.1.m1.1.1.1"></in><ci id="alg1.8.8.1.m1.1.1.2.cmml" xref="alg1.8.8.1.m1.1.1.2">𝑘</ci><apply id="alg1.8.8.1.m1.1.1.3.cmml" xref="alg1.8.8.1.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.8.8.1.m1.1.1.3.1.cmml" xref="alg1.8.8.1.m1.1.1.3">subscript</csymbol><ci id="alg1.8.8.1.m1.1.1.3.2.cmml" xref="alg1.8.8.1.m1.1.1.3.2">𝑆</ci><ci id="alg1.8.8.1.m1.1.1.3.3.cmml" xref="alg1.8.8.1.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.8.8.1.m1.1c">k\in S_{t}</annotation></semantics></math> <span id="alg1.8.8.1.1" class="ltx_text ltx_font_bold">in parallel</span></em> <span id="alg1.8.8.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="alg1.9.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="alg1.9.9.m1.3" class="ltx_Math" alttext="W_{t}^{k}\leftarrow ClientUpdate(k,W_{t},N_{l})" display="inline"><semantics id="alg1.9.9.m1.3a"><mrow id="alg1.9.9.m1.3.3" xref="alg1.9.9.m1.3.3.cmml"><msubsup id="alg1.9.9.m1.3.3.4" xref="alg1.9.9.m1.3.3.4.cmml"><mi id="alg1.9.9.m1.3.3.4.2.2" xref="alg1.9.9.m1.3.3.4.2.2.cmml">W</mi><mi id="alg1.9.9.m1.3.3.4.2.3" xref="alg1.9.9.m1.3.3.4.2.3.cmml">t</mi><mi id="alg1.9.9.m1.3.3.4.3" xref="alg1.9.9.m1.3.3.4.3.cmml">k</mi></msubsup><mo stretchy="false" id="alg1.9.9.m1.3.3.3" xref="alg1.9.9.m1.3.3.3.cmml">←</mo><mrow id="alg1.9.9.m1.3.3.2" xref="alg1.9.9.m1.3.3.2.cmml"><mi id="alg1.9.9.m1.3.3.2.4" xref="alg1.9.9.m1.3.3.2.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.5" xref="alg1.9.9.m1.3.3.2.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3a" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.6" xref="alg1.9.9.m1.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3b" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.7" xref="alg1.9.9.m1.3.3.2.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3c" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.8" xref="alg1.9.9.m1.3.3.2.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3d" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.9" xref="alg1.9.9.m1.3.3.2.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3e" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.10" xref="alg1.9.9.m1.3.3.2.10.cmml">U</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3f" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.11" xref="alg1.9.9.m1.3.3.2.11.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3g" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.12" xref="alg1.9.9.m1.3.3.2.12.cmml">d</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3h" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.13" xref="alg1.9.9.m1.3.3.2.13.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3i" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.14" xref="alg1.9.9.m1.3.3.2.14.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3j" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mi id="alg1.9.9.m1.3.3.2.15" xref="alg1.9.9.m1.3.3.2.15.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.9.9.m1.3.3.2.3k" xref="alg1.9.9.m1.3.3.2.3.cmml">​</mo><mrow id="alg1.9.9.m1.3.3.2.2.2" xref="alg1.9.9.m1.3.3.2.2.3.cmml"><mo stretchy="false" id="alg1.9.9.m1.3.3.2.2.2.3" xref="alg1.9.9.m1.3.3.2.2.3.cmml">(</mo><mi id="alg1.9.9.m1.1.1" xref="alg1.9.9.m1.1.1.cmml">k</mi><mo id="alg1.9.9.m1.3.3.2.2.2.4" xref="alg1.9.9.m1.3.3.2.2.3.cmml">,</mo><msub id="alg1.9.9.m1.2.2.1.1.1.1" xref="alg1.9.9.m1.2.2.1.1.1.1.cmml"><mi id="alg1.9.9.m1.2.2.1.1.1.1.2" xref="alg1.9.9.m1.2.2.1.1.1.1.2.cmml">W</mi><mi id="alg1.9.9.m1.2.2.1.1.1.1.3" xref="alg1.9.9.m1.2.2.1.1.1.1.3.cmml">t</mi></msub><mo id="alg1.9.9.m1.3.3.2.2.2.5" xref="alg1.9.9.m1.3.3.2.2.3.cmml">,</mo><msub id="alg1.9.9.m1.3.3.2.2.2.2" xref="alg1.9.9.m1.3.3.2.2.2.2.cmml"><mi id="alg1.9.9.m1.3.3.2.2.2.2.2" xref="alg1.9.9.m1.3.3.2.2.2.2.2.cmml">N</mi><mi id="alg1.9.9.m1.3.3.2.2.2.2.3" xref="alg1.9.9.m1.3.3.2.2.2.2.3.cmml">l</mi></msub><mo stretchy="false" id="alg1.9.9.m1.3.3.2.2.2.6" xref="alg1.9.9.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.9.9.m1.3b"><apply id="alg1.9.9.m1.3.3.cmml" xref="alg1.9.9.m1.3.3"><ci id="alg1.9.9.m1.3.3.3.cmml" xref="alg1.9.9.m1.3.3.3">←</ci><apply id="alg1.9.9.m1.3.3.4.cmml" xref="alg1.9.9.m1.3.3.4"><csymbol cd="ambiguous" id="alg1.9.9.m1.3.3.4.1.cmml" xref="alg1.9.9.m1.3.3.4">superscript</csymbol><apply id="alg1.9.9.m1.3.3.4.2.cmml" xref="alg1.9.9.m1.3.3.4"><csymbol cd="ambiguous" id="alg1.9.9.m1.3.3.4.2.1.cmml" xref="alg1.9.9.m1.3.3.4">subscript</csymbol><ci id="alg1.9.9.m1.3.3.4.2.2.cmml" xref="alg1.9.9.m1.3.3.4.2.2">𝑊</ci><ci id="alg1.9.9.m1.3.3.4.2.3.cmml" xref="alg1.9.9.m1.3.3.4.2.3">𝑡</ci></apply><ci id="alg1.9.9.m1.3.3.4.3.cmml" xref="alg1.9.9.m1.3.3.4.3">𝑘</ci></apply><apply id="alg1.9.9.m1.3.3.2.cmml" xref="alg1.9.9.m1.3.3.2"><times id="alg1.9.9.m1.3.3.2.3.cmml" xref="alg1.9.9.m1.3.3.2.3"></times><ci id="alg1.9.9.m1.3.3.2.4.cmml" xref="alg1.9.9.m1.3.3.2.4">𝐶</ci><ci id="alg1.9.9.m1.3.3.2.5.cmml" xref="alg1.9.9.m1.3.3.2.5">𝑙</ci><ci id="alg1.9.9.m1.3.3.2.6.cmml" xref="alg1.9.9.m1.3.3.2.6">𝑖</ci><ci id="alg1.9.9.m1.3.3.2.7.cmml" xref="alg1.9.9.m1.3.3.2.7">𝑒</ci><ci id="alg1.9.9.m1.3.3.2.8.cmml" xref="alg1.9.9.m1.3.3.2.8">𝑛</ci><ci id="alg1.9.9.m1.3.3.2.9.cmml" xref="alg1.9.9.m1.3.3.2.9">𝑡</ci><ci id="alg1.9.9.m1.3.3.2.10.cmml" xref="alg1.9.9.m1.3.3.2.10">𝑈</ci><ci id="alg1.9.9.m1.3.3.2.11.cmml" xref="alg1.9.9.m1.3.3.2.11">𝑝</ci><ci id="alg1.9.9.m1.3.3.2.12.cmml" xref="alg1.9.9.m1.3.3.2.12">𝑑</ci><ci id="alg1.9.9.m1.3.3.2.13.cmml" xref="alg1.9.9.m1.3.3.2.13">𝑎</ci><ci id="alg1.9.9.m1.3.3.2.14.cmml" xref="alg1.9.9.m1.3.3.2.14">𝑡</ci><ci id="alg1.9.9.m1.3.3.2.15.cmml" xref="alg1.9.9.m1.3.3.2.15">𝑒</ci><vector id="alg1.9.9.m1.3.3.2.2.3.cmml" xref="alg1.9.9.m1.3.3.2.2.2"><ci id="alg1.9.9.m1.1.1.cmml" xref="alg1.9.9.m1.1.1">𝑘</ci><apply id="alg1.9.9.m1.2.2.1.1.1.1.cmml" xref="alg1.9.9.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg1.9.9.m1.2.2.1.1.1.1.1.cmml" xref="alg1.9.9.m1.2.2.1.1.1.1">subscript</csymbol><ci id="alg1.9.9.m1.2.2.1.1.1.1.2.cmml" xref="alg1.9.9.m1.2.2.1.1.1.1.2">𝑊</ci><ci id="alg1.9.9.m1.2.2.1.1.1.1.3.cmml" xref="alg1.9.9.m1.2.2.1.1.1.1.3">𝑡</ci></apply><apply id="alg1.9.9.m1.3.3.2.2.2.2.cmml" xref="alg1.9.9.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="alg1.9.9.m1.3.3.2.2.2.2.1.cmml" xref="alg1.9.9.m1.3.3.2.2.2.2">subscript</csymbol><ci id="alg1.9.9.m1.3.3.2.2.2.2.2.cmml" xref="alg1.9.9.m1.3.3.2.2.2.2.2">𝑁</ci><ci id="alg1.9.9.m1.3.3.2.2.2.2.3.cmml" xref="alg1.9.9.m1.3.3.2.2.2.2.3">𝑙</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.9.9.m1.3c">W_{t}^{k}\leftarrow ClientUpdate(k,W_{t},N_{l})</annotation></semantics></math>
</div>
<div id="alg1.10.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="alg1.10.10.m1.1" class="ltx_Math" alttext="W_{t}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}W_{t}^{k}" display="inline"><semantics id="alg1.10.10.m1.1a"><mrow id="alg1.10.10.m1.1.1" xref="alg1.10.10.m1.1.1.cmml"><msub id="alg1.10.10.m1.1.1.2" xref="alg1.10.10.m1.1.1.2.cmml"><mi id="alg1.10.10.m1.1.1.2.2" xref="alg1.10.10.m1.1.1.2.2.cmml">W</mi><mi id="alg1.10.10.m1.1.1.2.3" xref="alg1.10.10.m1.1.1.2.3.cmml">t</mi></msub><mo rspace="0.111em" stretchy="false" id="alg1.10.10.m1.1.1.1" xref="alg1.10.10.m1.1.1.1.cmml">←</mo><mrow id="alg1.10.10.m1.1.1.3" xref="alg1.10.10.m1.1.1.3.cmml"><msubsup id="alg1.10.10.m1.1.1.3.1" xref="alg1.10.10.m1.1.1.3.1.cmml"><mo id="alg1.10.10.m1.1.1.3.1.2.2" xref="alg1.10.10.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="alg1.10.10.m1.1.1.3.1.2.3" xref="alg1.10.10.m1.1.1.3.1.2.3.cmml"><mi id="alg1.10.10.m1.1.1.3.1.2.3.2" xref="alg1.10.10.m1.1.1.3.1.2.3.2.cmml">k</mi><mo id="alg1.10.10.m1.1.1.3.1.2.3.1" xref="alg1.10.10.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="alg1.10.10.m1.1.1.3.1.2.3.3" xref="alg1.10.10.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="alg1.10.10.m1.1.1.3.1.3" xref="alg1.10.10.m1.1.1.3.1.3.cmml">K</mi></msubsup><mrow id="alg1.10.10.m1.1.1.3.2" xref="alg1.10.10.m1.1.1.3.2.cmml"><mfrac id="alg1.10.10.m1.1.1.3.2.2" xref="alg1.10.10.m1.1.1.3.2.2.cmml"><msub id="alg1.10.10.m1.1.1.3.2.2.2" xref="alg1.10.10.m1.1.1.3.2.2.2.cmml"><mi id="alg1.10.10.m1.1.1.3.2.2.2.2" xref="alg1.10.10.m1.1.1.3.2.2.2.2.cmml">n</mi><mi id="alg1.10.10.m1.1.1.3.2.2.2.3" xref="alg1.10.10.m1.1.1.3.2.2.2.3.cmml">k</mi></msub><mi id="alg1.10.10.m1.1.1.3.2.2.3" xref="alg1.10.10.m1.1.1.3.2.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="alg1.10.10.m1.1.1.3.2.1" xref="alg1.10.10.m1.1.1.3.2.1.cmml">​</mo><msubsup id="alg1.10.10.m1.1.1.3.2.3" xref="alg1.10.10.m1.1.1.3.2.3.cmml"><mi id="alg1.10.10.m1.1.1.3.2.3.2.2" xref="alg1.10.10.m1.1.1.3.2.3.2.2.cmml">W</mi><mi id="alg1.10.10.m1.1.1.3.2.3.2.3" xref="alg1.10.10.m1.1.1.3.2.3.2.3.cmml">t</mi><mi id="alg1.10.10.m1.1.1.3.2.3.3" xref="alg1.10.10.m1.1.1.3.2.3.3.cmml">k</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.10.10.m1.1b"><apply id="alg1.10.10.m1.1.1.cmml" xref="alg1.10.10.m1.1.1"><ci id="alg1.10.10.m1.1.1.1.cmml" xref="alg1.10.10.m1.1.1.1">←</ci><apply id="alg1.10.10.m1.1.1.2.cmml" xref="alg1.10.10.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.10.10.m1.1.1.2.1.cmml" xref="alg1.10.10.m1.1.1.2">subscript</csymbol><ci id="alg1.10.10.m1.1.1.2.2.cmml" xref="alg1.10.10.m1.1.1.2.2">𝑊</ci><ci id="alg1.10.10.m1.1.1.2.3.cmml" xref="alg1.10.10.m1.1.1.2.3">𝑡</ci></apply><apply id="alg1.10.10.m1.1.1.3.cmml" xref="alg1.10.10.m1.1.1.3"><apply id="alg1.10.10.m1.1.1.3.1.cmml" xref="alg1.10.10.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.10.10.m1.1.1.3.1.1.cmml" xref="alg1.10.10.m1.1.1.3.1">superscript</csymbol><apply id="alg1.10.10.m1.1.1.3.1.2.cmml" xref="alg1.10.10.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg1.10.10.m1.1.1.3.1.2.1.cmml" xref="alg1.10.10.m1.1.1.3.1">subscript</csymbol><sum id="alg1.10.10.m1.1.1.3.1.2.2.cmml" xref="alg1.10.10.m1.1.1.3.1.2.2"></sum><apply id="alg1.10.10.m1.1.1.3.1.2.3.cmml" xref="alg1.10.10.m1.1.1.3.1.2.3"><eq id="alg1.10.10.m1.1.1.3.1.2.3.1.cmml" xref="alg1.10.10.m1.1.1.3.1.2.3.1"></eq><ci id="alg1.10.10.m1.1.1.3.1.2.3.2.cmml" xref="alg1.10.10.m1.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="alg1.10.10.m1.1.1.3.1.2.3.3.cmml" xref="alg1.10.10.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="alg1.10.10.m1.1.1.3.1.3.cmml" xref="alg1.10.10.m1.1.1.3.1.3">𝐾</ci></apply><apply id="alg1.10.10.m1.1.1.3.2.cmml" xref="alg1.10.10.m1.1.1.3.2"><times id="alg1.10.10.m1.1.1.3.2.1.cmml" xref="alg1.10.10.m1.1.1.3.2.1"></times><apply id="alg1.10.10.m1.1.1.3.2.2.cmml" xref="alg1.10.10.m1.1.1.3.2.2"><divide id="alg1.10.10.m1.1.1.3.2.2.1.cmml" xref="alg1.10.10.m1.1.1.3.2.2"></divide><apply id="alg1.10.10.m1.1.1.3.2.2.2.cmml" xref="alg1.10.10.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="alg1.10.10.m1.1.1.3.2.2.2.1.cmml" xref="alg1.10.10.m1.1.1.3.2.2.2">subscript</csymbol><ci id="alg1.10.10.m1.1.1.3.2.2.2.2.cmml" xref="alg1.10.10.m1.1.1.3.2.2.2.2">𝑛</ci><ci id="alg1.10.10.m1.1.1.3.2.2.2.3.cmml" xref="alg1.10.10.m1.1.1.3.2.2.2.3">𝑘</ci></apply><ci id="alg1.10.10.m1.1.1.3.2.2.3.cmml" xref="alg1.10.10.m1.1.1.3.2.2.3">𝑛</ci></apply><apply id="alg1.10.10.m1.1.1.3.2.3.cmml" xref="alg1.10.10.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="alg1.10.10.m1.1.1.3.2.3.1.cmml" xref="alg1.10.10.m1.1.1.3.2.3">superscript</csymbol><apply id="alg1.10.10.m1.1.1.3.2.3.2.cmml" xref="alg1.10.10.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="alg1.10.10.m1.1.1.3.2.3.2.1.cmml" xref="alg1.10.10.m1.1.1.3.2.3">subscript</csymbol><ci id="alg1.10.10.m1.1.1.3.2.3.2.2.cmml" xref="alg1.10.10.m1.1.1.3.2.3.2.2">𝑊</ci><ci id="alg1.10.10.m1.1.1.3.2.3.2.3.cmml" xref="alg1.10.10.m1.1.1.3.2.3.2.3">𝑡</ci></apply><ci id="alg1.10.10.m1.1.1.3.2.3.3.cmml" xref="alg1.10.10.m1.1.1.3.2.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.10.10.m1.1c">W_{t}\leftarrow\sum_{k=1}^{K}\frac{n_{k}}{n}W_{t}^{k}</annotation></semantics></math> 
</div>
<div id="alg1.12.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end foreach
</div>
<div id="alg1.11.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <math id="alg1.11.11.m1.1" class="ltx_Math" alttext="W_{t}\leftarrow(W_{t-1}+(W_{t}-W_{t-1})/t)" display="inline"><semantics id="alg1.11.11.m1.1a"><mrow id="alg1.11.11.m1.1.1" xref="alg1.11.11.m1.1.1.cmml"><msub id="alg1.11.11.m1.1.1.3" xref="alg1.11.11.m1.1.1.3.cmml"><mi id="alg1.11.11.m1.1.1.3.2" xref="alg1.11.11.m1.1.1.3.2.cmml">W</mi><mi id="alg1.11.11.m1.1.1.3.3" xref="alg1.11.11.m1.1.1.3.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.11.11.m1.1.1.2" xref="alg1.11.11.m1.1.1.2.cmml">←</mo><mrow id="alg1.11.11.m1.1.1.1.1" xref="alg1.11.11.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.11.11.m1.1.1.1.1.2" xref="alg1.11.11.m1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.11.11.m1.1.1.1.1.1" xref="alg1.11.11.m1.1.1.1.1.1.cmml"><msub id="alg1.11.11.m1.1.1.1.1.1.3" xref="alg1.11.11.m1.1.1.1.1.1.3.cmml"><mi id="alg1.11.11.m1.1.1.1.1.1.3.2" xref="alg1.11.11.m1.1.1.1.1.1.3.2.cmml">W</mi><mrow id="alg1.11.11.m1.1.1.1.1.1.3.3" xref="alg1.11.11.m1.1.1.1.1.1.3.3.cmml"><mi id="alg1.11.11.m1.1.1.1.1.1.3.3.2" xref="alg1.11.11.m1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="alg1.11.11.m1.1.1.1.1.1.3.3.1" xref="alg1.11.11.m1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="alg1.11.11.m1.1.1.1.1.1.3.3.3" xref="alg1.11.11.m1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo id="alg1.11.11.m1.1.1.1.1.1.2" xref="alg1.11.11.m1.1.1.1.1.1.2.cmml">+</mo><mrow id="alg1.11.11.m1.1.1.1.1.1.1" xref="alg1.11.11.m1.1.1.1.1.1.1.cmml"><mrow id="alg1.11.11.m1.1.1.1.1.1.1.1.1" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.11.11.m1.1.1.1.1.1.1.1.1.2" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.2" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">W</mi><mi id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.3" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.1" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.2" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">W</mi><mrow id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.2" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml">t</mi><mo id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.1" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><mn id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.3" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><mo stretchy="false" id="alg1.11.11.m1.1.1.1.1.1.1.1.1.3" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="alg1.11.11.m1.1.1.1.1.1.1.2" xref="alg1.11.11.m1.1.1.1.1.1.1.2.cmml">/</mo><mi id="alg1.11.11.m1.1.1.1.1.1.1.3" xref="alg1.11.11.m1.1.1.1.1.1.1.3.cmml">t</mi></mrow></mrow><mo stretchy="false" id="alg1.11.11.m1.1.1.1.1.3" xref="alg1.11.11.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.11.11.m1.1b"><apply id="alg1.11.11.m1.1.1.cmml" xref="alg1.11.11.m1.1.1"><ci id="alg1.11.11.m1.1.1.2.cmml" xref="alg1.11.11.m1.1.1.2">←</ci><apply id="alg1.11.11.m1.1.1.3.cmml" xref="alg1.11.11.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.11.11.m1.1.1.3.1.cmml" xref="alg1.11.11.m1.1.1.3">subscript</csymbol><ci id="alg1.11.11.m1.1.1.3.2.cmml" xref="alg1.11.11.m1.1.1.3.2">𝑊</ci><ci id="alg1.11.11.m1.1.1.3.3.cmml" xref="alg1.11.11.m1.1.1.3.3">𝑡</ci></apply><apply id="alg1.11.11.m1.1.1.1.1.1.cmml" xref="alg1.11.11.m1.1.1.1.1"><plus id="alg1.11.11.m1.1.1.1.1.1.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.2"></plus><apply id="alg1.11.11.m1.1.1.1.1.1.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.11.11.m1.1.1.1.1.1.3.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3">subscript</csymbol><ci id="alg1.11.11.m1.1.1.1.1.1.3.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3.2">𝑊</ci><apply id="alg1.11.11.m1.1.1.1.1.1.3.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3.3"><minus id="alg1.11.11.m1.1.1.1.1.1.3.3.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3.3.1"></minus><ci id="alg1.11.11.m1.1.1.1.1.1.3.3.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="alg1.11.11.m1.1.1.1.1.1.3.3.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="alg1.11.11.m1.1.1.1.1.1.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1"><divide id="alg1.11.11.m1.1.1.1.1.1.1.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.2"></divide><apply id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1"><minus id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.2">𝑊</ci><ci id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.2">𝑊</ci><apply id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3"><minus id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.1"></minus><ci id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply><ci id="alg1.11.11.m1.1.1.1.1.1.1.3.cmml" xref="alg1.11.11.m1.1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.11.11.m1.1c">W_{t}\leftarrow(W_{t-1}+(W_{t}-W_{t-1})/t)</annotation></semantics></math> 
</div>
<div id="alg1.12.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end foreach
</div>
<div id="alg1.12.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span id="alg1.12.12.1" class="ltx_text ltx_font_bold">return</span> <math id="alg1.12.12.m1.1" class="ltx_Math" alttext="M(W_{t})" display="inline"><semantics id="alg1.12.12.m1.1a"><mrow id="alg1.12.12.m1.1.1" xref="alg1.12.12.m1.1.1.cmml"><mi id="alg1.12.12.m1.1.1.3" xref="alg1.12.12.m1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="alg1.12.12.m1.1.1.2" xref="alg1.12.12.m1.1.1.2.cmml">​</mo><mrow id="alg1.12.12.m1.1.1.1.1" xref="alg1.12.12.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.12.12.m1.1.1.1.1.2" xref="alg1.12.12.m1.1.1.1.1.1.cmml">(</mo><msub id="alg1.12.12.m1.1.1.1.1.1" xref="alg1.12.12.m1.1.1.1.1.1.cmml"><mi id="alg1.12.12.m1.1.1.1.1.1.2" xref="alg1.12.12.m1.1.1.1.1.1.2.cmml">W</mi><mi id="alg1.12.12.m1.1.1.1.1.1.3" xref="alg1.12.12.m1.1.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="alg1.12.12.m1.1.1.1.1.3" xref="alg1.12.12.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.12.12.m1.1b"><apply id="alg1.12.12.m1.1.1.cmml" xref="alg1.12.12.m1.1.1"><times id="alg1.12.12.m1.1.1.2.cmml" xref="alg1.12.12.m1.1.1.2"></times><ci id="alg1.12.12.m1.1.1.3.cmml" xref="alg1.12.12.m1.1.1.3">𝑀</ci><apply id="alg1.12.12.m1.1.1.1.1.1.cmml" xref="alg1.12.12.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.12.12.m1.1.1.1.1.1.1.cmml" xref="alg1.12.12.m1.1.1.1.1">subscript</csymbol><ci id="alg1.12.12.m1.1.1.1.1.1.2.cmml" xref="alg1.12.12.m1.1.1.1.1.1.2">𝑊</ci><ci id="alg1.12.12.m1.1.1.1.1.1.3.cmml" xref="alg1.12.12.m1.1.1.1.1.1.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.12.12.m1.1c">M(W_{t})</annotation></semantics></math>

</div>
<div id="alg1.12.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span> 
</div>
<div id="alg1.12.19" class="ltx_listingline">

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.19.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span>FedAVG algorithm. <span id="alg1.20.2" class="ltx_text ltx_font_bold">C</span>: Number of clients, <span id="alg1.21.3" class="ltx_text ltx_font_bold">r</span>: Number of rounds, <math id="alg1.14.m1.1" class="ltx_Math" alttext="W_{i}" display="inline"><semantics id="alg1.14.m1.1b"><msub id="alg1.14.m1.1.1" xref="alg1.14.m1.1.1.cmml"><mi id="alg1.14.m1.1.1.2" xref="alg1.14.m1.1.1.2.cmml">W</mi><mi id="alg1.14.m1.1.1.3" xref="alg1.14.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.14.m1.1c"><apply id="alg1.14.m1.1.1.cmml" xref="alg1.14.m1.1.1"><csymbol cd="ambiguous" id="alg1.14.m1.1.1.1.cmml" xref="alg1.14.m1.1.1">subscript</csymbol><ci id="alg1.14.m1.1.1.2.cmml" xref="alg1.14.m1.1.1.2">𝑊</ci><ci id="alg1.14.m1.1.1.3.cmml" xref="alg1.14.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.14.m1.1d">W_{i}</annotation></semantics></math>: Local model weights and <span id="alg1.22.4" class="ltx_text ltx_font_bold">M</span>: Global model weights</figcaption>
</figure>
<figure id="alg2" class="ltx_float ltx_algorithm">
<div id="alg2.14" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="alg2.1.1" class="ltx_listingline">
<span id="alg2.1.1.1" class="ltx_text"><span id="alg2.1.1.1.1" class="ltx_text ltx_font_bold">Output:</span> </span><math id="alg2.1.1.m1.1" class="ltx_Math" alttext="W_{t}" display="inline"><semantics id="alg2.1.1.m1.1a"><msub id="alg2.1.1.m1.1.1" xref="alg2.1.1.m1.1.1.cmml"><mi id="alg2.1.1.m1.1.1.2" xref="alg2.1.1.m1.1.1.2.cmml">W</mi><mi id="alg2.1.1.m1.1.1.3" xref="alg2.1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.1.1.m1.1b"><apply id="alg2.1.1.m1.1.1.cmml" xref="alg2.1.1.m1.1.1"><csymbol cd="ambiguous" id="alg2.1.1.m1.1.1.1.cmml" xref="alg2.1.1.m1.1.1">subscript</csymbol><ci id="alg2.1.1.m1.1.1.2.cmml" xref="alg2.1.1.m1.1.1.2">𝑊</ci><ci id="alg2.1.1.m1.1.1.3.cmml" xref="alg2.1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.1.1.m1.1c">W_{t}</annotation></semantics></math>
</div>
<div id="alg2.14.15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>

</div>
<div id="alg2.14.16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span> // Run on client k
</div>
<div id="alg2.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>

<span id="alg2.2.2.2" class="ltx_text ltx_font_bold">Function</span> <em id="alg2.2.2.1" class="ltx_emph ltx_font_typewriter">ClientUpdate(<em id="alg2.2.2.1.1.1" class="ltx_emph ltx_font_serif ltx_font_italic"><math id="alg2.2.2.1.1.1.m1.3" class="ltx_Math" alttext="k,W_{t},N_{l}" display="inline"><semantics id="alg2.2.2.1.1.1.m1.3a"><mrow id="alg2.2.2.1.1.1.m1.3.3.2" xref="alg2.2.2.1.1.1.m1.3.3.3.cmml"><mi id="alg2.2.2.1.1.1.m1.1.1" xref="alg2.2.2.1.1.1.m1.1.1.cmml">k</mi><mo id="alg2.2.2.1.1.1.m1.3.3.2.3" xref="alg2.2.2.1.1.1.m1.3.3.3.cmml">,</mo><msub id="alg2.2.2.1.1.1.m1.2.2.1.1" xref="alg2.2.2.1.1.1.m1.2.2.1.1.cmml"><mi id="alg2.2.2.1.1.1.m1.2.2.1.1.2" xref="alg2.2.2.1.1.1.m1.2.2.1.1.2.cmml">W</mi><mi id="alg2.2.2.1.1.1.m1.2.2.1.1.3" xref="alg2.2.2.1.1.1.m1.2.2.1.1.3.cmml">t</mi></msub><mo id="alg2.2.2.1.1.1.m1.3.3.2.4" xref="alg2.2.2.1.1.1.m1.3.3.3.cmml">,</mo><msub id="alg2.2.2.1.1.1.m1.3.3.2.2" xref="alg2.2.2.1.1.1.m1.3.3.2.2.cmml"><mi id="alg2.2.2.1.1.1.m1.3.3.2.2.2" xref="alg2.2.2.1.1.1.m1.3.3.2.2.2.cmml">N</mi><mi id="alg2.2.2.1.1.1.m1.3.3.2.2.3" xref="alg2.2.2.1.1.1.m1.3.3.2.2.3.cmml">l</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="alg2.2.2.1.1.1.m1.3b"><list id="alg2.2.2.1.1.1.m1.3.3.3.cmml" xref="alg2.2.2.1.1.1.m1.3.3.2"><ci id="alg2.2.2.1.1.1.m1.1.1.cmml" xref="alg2.2.2.1.1.1.m1.1.1">𝑘</ci><apply id="alg2.2.2.1.1.1.m1.2.2.1.1.cmml" xref="alg2.2.2.1.1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="alg2.2.2.1.1.1.m1.2.2.1.1.1.cmml" xref="alg2.2.2.1.1.1.m1.2.2.1.1">subscript</csymbol><ci id="alg2.2.2.1.1.1.m1.2.2.1.1.2.cmml" xref="alg2.2.2.1.1.1.m1.2.2.1.1.2">𝑊</ci><ci id="alg2.2.2.1.1.1.m1.2.2.1.1.3.cmml" xref="alg2.2.2.1.1.1.m1.2.2.1.1.3">𝑡</ci></apply><apply id="alg2.2.2.1.1.1.m1.3.3.2.2.cmml" xref="alg2.2.2.1.1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="alg2.2.2.1.1.1.m1.3.3.2.2.1.cmml" xref="alg2.2.2.1.1.1.m1.3.3.2.2">subscript</csymbol><ci id="alg2.2.2.1.1.1.m1.3.3.2.2.2.cmml" xref="alg2.2.2.1.1.1.m1.3.3.2.2.2">𝑁</ci><ci id="alg2.2.2.1.1.1.m1.3.3.2.2.3.cmml" xref="alg2.2.2.1.1.1.m1.3.3.2.2.3">𝑙</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="alg2.2.2.1.1.1.m1.3c">k,W_{t},N_{l}</annotation></semantics></math></em>)</em><span id="alg2.2.2.3" class="ltx_text ltx_font_bold">:</span> 
</div>
<div id="alg2.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="alg2.3.3.m1.1" class="ltx_Math" alttext="W_{t}(l)" display="inline"><semantics id="alg2.3.3.m1.1a"><mrow id="alg2.3.3.m1.1.2" xref="alg2.3.3.m1.1.2.cmml"><msub id="alg2.3.3.m1.1.2.2" xref="alg2.3.3.m1.1.2.2.cmml"><mi id="alg2.3.3.m1.1.2.2.2" xref="alg2.3.3.m1.1.2.2.2.cmml">W</mi><mi id="alg2.3.3.m1.1.2.2.3" xref="alg2.3.3.m1.1.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="alg2.3.3.m1.1.2.1" xref="alg2.3.3.m1.1.2.1.cmml">​</mo><mrow id="alg2.3.3.m1.1.2.3.2" xref="alg2.3.3.m1.1.2.cmml"><mo stretchy="false" id="alg2.3.3.m1.1.2.3.2.1" xref="alg2.3.3.m1.1.2.cmml">(</mo><mi id="alg2.3.3.m1.1.1" xref="alg2.3.3.m1.1.1.cmml">l</mi><mo stretchy="false" id="alg2.3.3.m1.1.2.3.2.2" xref="alg2.3.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.3.3.m1.1b"><apply id="alg2.3.3.m1.1.2.cmml" xref="alg2.3.3.m1.1.2"><times id="alg2.3.3.m1.1.2.1.cmml" xref="alg2.3.3.m1.1.2.1"></times><apply id="alg2.3.3.m1.1.2.2.cmml" xref="alg2.3.3.m1.1.2.2"><csymbol cd="ambiguous" id="alg2.3.3.m1.1.2.2.1.cmml" xref="alg2.3.3.m1.1.2.2">subscript</csymbol><ci id="alg2.3.3.m1.1.2.2.2.cmml" xref="alg2.3.3.m1.1.2.2.2">𝑊</ci><ci id="alg2.3.3.m1.1.2.2.3.cmml" xref="alg2.3.3.m1.1.2.2.3">𝑡</ci></apply><ci id="alg2.3.3.m1.1.1.cmml" xref="alg2.3.3.m1.1.1">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.3.3.m1.1c">W_{t}(l)</annotation></semantics></math> <math id="alg2.4.4.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg2.4.4.m2.1a"><mo stretchy="false" id="alg2.4.4.m2.1.1" xref="alg2.4.4.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg2.4.4.m2.1b"><ci id="alg2.4.4.m2.1.1.cmml" xref="alg2.4.4.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.4.4.m2.1c">\leftarrow</annotation></semantics></math> Select <math id="alg2.5.5.m3.1" class="ltx_Math" alttext="N_{l}" display="inline"><semantics id="alg2.5.5.m3.1a"><msub id="alg2.5.5.m3.1.1" xref="alg2.5.5.m3.1.1.cmml"><mi id="alg2.5.5.m3.1.1.2" xref="alg2.5.5.m3.1.1.2.cmml">N</mi><mi id="alg2.5.5.m3.1.1.3" xref="alg2.5.5.m3.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.5.5.m3.1b"><apply id="alg2.5.5.m3.1.1.cmml" xref="alg2.5.5.m3.1.1"><csymbol cd="ambiguous" id="alg2.5.5.m3.1.1.1.cmml" xref="alg2.5.5.m3.1.1">subscript</csymbol><ci id="alg2.5.5.m3.1.1.2.cmml" xref="alg2.5.5.m3.1.1.2">𝑁</ci><ci id="alg2.5.5.m3.1.1.3.cmml" xref="alg2.5.5.m3.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.5.5.m3.1c">N_{l}</annotation></semantics></math> layers from full model <math id="alg2.6.6.m4.1" class="ltx_Math" alttext="W_{t}" display="inline"><semantics id="alg2.6.6.m4.1a"><msub id="alg2.6.6.m4.1.1" xref="alg2.6.6.m4.1.1.cmml"><mi id="alg2.6.6.m4.1.1.2" xref="alg2.6.6.m4.1.1.2.cmml">W</mi><mi id="alg2.6.6.m4.1.1.3" xref="alg2.6.6.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.6.6.m4.1b"><apply id="alg2.6.6.m4.1.1.cmml" xref="alg2.6.6.m4.1.1"><csymbol cd="ambiguous" id="alg2.6.6.m4.1.1.1.cmml" xref="alg2.6.6.m4.1.1">subscript</csymbol><ci id="alg2.6.6.m4.1.1.2.cmml" xref="alg2.6.6.m4.1.1.2">𝑊</ci><ci id="alg2.6.6.m4.1.1.3.cmml" xref="alg2.6.6.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.6.6.m4.1c">W_{t}</annotation></semantics></math> randomly to train
</div>
<div id="alg2.14.17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="alg2.9.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <math id="alg2.7.7.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="alg2.7.7.m1.1a"><mi id="alg2.7.7.m1.1.1" xref="alg2.7.7.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="alg2.7.7.m1.1b"><ci id="alg2.7.7.m1.1.1.cmml" xref="alg2.7.7.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.7.7.m1.1c">\beta</annotation></semantics></math> <math id="alg2.8.8.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg2.8.8.m2.1a"><mo stretchy="false" id="alg2.8.8.m2.1.1" xref="alg2.8.8.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg2.8.8.m2.1b"><ci id="alg2.8.8.m2.1.1.cmml" xref="alg2.8.8.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.8.8.m2.1c">\leftarrow</annotation></semantics></math> (split <math id="alg2.9.9.m3.1" class="ltx_Math" alttext="D^{k}" display="inline"><semantics id="alg2.9.9.m3.1a"><msup id="alg2.9.9.m3.1.1" xref="alg2.9.9.m3.1.1.cmml"><mi id="alg2.9.9.m3.1.1.2" xref="alg2.9.9.m3.1.1.2.cmml">D</mi><mi id="alg2.9.9.m3.1.1.3" xref="alg2.9.9.m3.1.1.3.cmml">k</mi></msup><annotation-xml encoding="MathML-Content" id="alg2.9.9.m3.1b"><apply id="alg2.9.9.m3.1.1.cmml" xref="alg2.9.9.m3.1.1"><csymbol cd="ambiguous" id="alg2.9.9.m3.1.1.1.cmml" xref="alg2.9.9.m3.1.1">superscript</csymbol><ci id="alg2.9.9.m3.1.1.2.cmml" xref="alg2.9.9.m3.1.1.2">𝐷</ci><ci id="alg2.9.9.m3.1.1.3.cmml" xref="alg2.9.9.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.9.9.m3.1c">D^{k}</annotation></semantics></math> into mini batches)
</div>
<div id="alg2.10.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg2.10.10.2" class="ltx_text ltx_font_bold">for</span> <em id="alg2.10.10.1" class="ltx_emph ltx_font_italic"><math id="alg2.10.10.1.m1.2" class="ltx_Math" alttext="local\leavevmode\nobreak\ epoch\leavevmode\nobreak\ e_{i}\in 1,\dots e" display="inline"><semantics id="alg2.10.10.1.m1.2a"><mrow id="alg2.10.10.1.m1.2.2" xref="alg2.10.10.1.m1.2.2.cmml"><mrow id="alg2.10.10.1.m1.2.2.3" xref="alg2.10.10.1.m1.2.2.3.cmml"><mi id="alg2.10.10.1.m1.2.2.3.2" xref="alg2.10.10.1.m1.2.2.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.3" xref="alg2.10.10.1.m1.2.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1a" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.4" xref="alg2.10.10.1.m1.2.2.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1b" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.5" xref="alg2.10.10.1.m1.2.2.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1c" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.6" xref="alg2.10.10.1.m1.2.2.3.6.cmml">l</mi><mo lspace="0.500em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1d" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.7" xref="alg2.10.10.1.m1.2.2.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1e" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.8" xref="alg2.10.10.1.m1.2.2.3.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1f" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.9" xref="alg2.10.10.1.m1.2.2.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1g" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.10" xref="alg2.10.10.1.m1.2.2.3.10.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1h" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.3.11" xref="alg2.10.10.1.m1.2.2.3.11.cmml">h</mi><mo lspace="0.500em" rspace="0em" id="alg2.10.10.1.m1.2.2.3.1i" xref="alg2.10.10.1.m1.2.2.3.1.cmml">​</mo><msub id="alg2.10.10.1.m1.2.2.3.12" xref="alg2.10.10.1.m1.2.2.3.12.cmml"><mi id="alg2.10.10.1.m1.2.2.3.12.2" xref="alg2.10.10.1.m1.2.2.3.12.2.cmml">e</mi><mi id="alg2.10.10.1.m1.2.2.3.12.3" xref="alg2.10.10.1.m1.2.2.3.12.3.cmml">i</mi></msub></mrow><mo id="alg2.10.10.1.m1.2.2.2" xref="alg2.10.10.1.m1.2.2.2.cmml">∈</mo><mrow id="alg2.10.10.1.m1.2.2.1.1" xref="alg2.10.10.1.m1.2.2.1.2.cmml"><mn id="alg2.10.10.1.m1.1.1" xref="alg2.10.10.1.m1.1.1.cmml">1</mn><mo id="alg2.10.10.1.m1.2.2.1.1.2" xref="alg2.10.10.1.m1.2.2.1.2.cmml">,</mo><mrow id="alg2.10.10.1.m1.2.2.1.1.1" xref="alg2.10.10.1.m1.2.2.1.1.1.cmml"><mi mathvariant="normal" id="alg2.10.10.1.m1.2.2.1.1.1.2" xref="alg2.10.10.1.m1.2.2.1.1.1.2.cmml">…</mi><mo lspace="0em" rspace="0em" id="alg2.10.10.1.m1.2.2.1.1.1.1" xref="alg2.10.10.1.m1.2.2.1.1.1.1.cmml">​</mo><mi id="alg2.10.10.1.m1.2.2.1.1.1.3" xref="alg2.10.10.1.m1.2.2.1.1.1.3.cmml">e</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.10.10.1.m1.2b"><apply id="alg2.10.10.1.m1.2.2.cmml" xref="alg2.10.10.1.m1.2.2"><in id="alg2.10.10.1.m1.2.2.2.cmml" xref="alg2.10.10.1.m1.2.2.2"></in><apply id="alg2.10.10.1.m1.2.2.3.cmml" xref="alg2.10.10.1.m1.2.2.3"><times id="alg2.10.10.1.m1.2.2.3.1.cmml" xref="alg2.10.10.1.m1.2.2.3.1"></times><ci id="alg2.10.10.1.m1.2.2.3.2.cmml" xref="alg2.10.10.1.m1.2.2.3.2">𝑙</ci><ci id="alg2.10.10.1.m1.2.2.3.3.cmml" xref="alg2.10.10.1.m1.2.2.3.3">𝑜</ci><ci id="alg2.10.10.1.m1.2.2.3.4.cmml" xref="alg2.10.10.1.m1.2.2.3.4">𝑐</ci><ci id="alg2.10.10.1.m1.2.2.3.5.cmml" xref="alg2.10.10.1.m1.2.2.3.5">𝑎</ci><ci id="alg2.10.10.1.m1.2.2.3.6.cmml" xref="alg2.10.10.1.m1.2.2.3.6">𝑙</ci><ci id="alg2.10.10.1.m1.2.2.3.7.cmml" xref="alg2.10.10.1.m1.2.2.3.7">𝑒</ci><ci id="alg2.10.10.1.m1.2.2.3.8.cmml" xref="alg2.10.10.1.m1.2.2.3.8">𝑝</ci><ci id="alg2.10.10.1.m1.2.2.3.9.cmml" xref="alg2.10.10.1.m1.2.2.3.9">𝑜</ci><ci id="alg2.10.10.1.m1.2.2.3.10.cmml" xref="alg2.10.10.1.m1.2.2.3.10">𝑐</ci><ci id="alg2.10.10.1.m1.2.2.3.11.cmml" xref="alg2.10.10.1.m1.2.2.3.11">ℎ</ci><apply id="alg2.10.10.1.m1.2.2.3.12.cmml" xref="alg2.10.10.1.m1.2.2.3.12"><csymbol cd="ambiguous" id="alg2.10.10.1.m1.2.2.3.12.1.cmml" xref="alg2.10.10.1.m1.2.2.3.12">subscript</csymbol><ci id="alg2.10.10.1.m1.2.2.3.12.2.cmml" xref="alg2.10.10.1.m1.2.2.3.12.2">𝑒</ci><ci id="alg2.10.10.1.m1.2.2.3.12.3.cmml" xref="alg2.10.10.1.m1.2.2.3.12.3">𝑖</ci></apply></apply><list id="alg2.10.10.1.m1.2.2.1.2.cmml" xref="alg2.10.10.1.m1.2.2.1.1"><cn type="integer" id="alg2.10.10.1.m1.1.1.cmml" xref="alg2.10.10.1.m1.1.1">1</cn><apply id="alg2.10.10.1.m1.2.2.1.1.1.cmml" xref="alg2.10.10.1.m1.2.2.1.1.1"><times id="alg2.10.10.1.m1.2.2.1.1.1.1.cmml" xref="alg2.10.10.1.m1.2.2.1.1.1.1"></times><ci id="alg2.10.10.1.m1.2.2.1.1.1.2.cmml" xref="alg2.10.10.1.m1.2.2.1.1.1.2">…</ci><ci id="alg2.10.10.1.m1.2.2.1.1.1.3.cmml" xref="alg2.10.10.1.m1.2.2.1.1.1.3">𝑒</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.10.10.1.m1.2c">local\leavevmode\nobreak\ epoch\leavevmode\nobreak\ e_{i}\in 1,\dots e</annotation></semantics></math></em> <span id="alg2.10.10.3" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="alg2.12.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="alg2.12.12.3" class="ltx_text ltx_font_bold">for</span> <em id="alg2.12.12.2" class="ltx_emph ltx_font_italic">batch b <math id="alg2.11.11.1.m1.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="alg2.11.11.1.m1.1a"><mo id="alg2.11.11.1.m1.1.1" xref="alg2.11.11.1.m1.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="alg2.11.11.1.m1.1b"><in id="alg2.11.11.1.m1.1.1.cmml" xref="alg2.11.11.1.m1.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="alg2.11.11.1.m1.1c">\in</annotation></semantics></math> <math id="alg2.12.12.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="alg2.12.12.2.m2.1a"><mi id="alg2.12.12.2.m2.1.1" xref="alg2.12.12.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="alg2.12.12.2.m2.1b"><ci id="alg2.12.12.2.m2.1.1.cmml" xref="alg2.12.12.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.12.12.2.m2.1c">\beta</annotation></semantics></math></em> <span id="alg2.12.12.4" class="ltx_text ltx_font_bold">do</span> 
</div>
<div id="alg2.13.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<math id="alg2.13.13.m1.4" class="ltx_Math" alttext="W_{t}\leftarrow W_{t}(l)-\eta\nabla l(W_{t}(l),b)" display="inline"><semantics id="alg2.13.13.m1.4a"><mrow id="alg2.13.13.m1.4.4" xref="alg2.13.13.m1.4.4.cmml"><msub id="alg2.13.13.m1.4.4.3" xref="alg2.13.13.m1.4.4.3.cmml"><mi id="alg2.13.13.m1.4.4.3.2" xref="alg2.13.13.m1.4.4.3.2.cmml">W</mi><mi id="alg2.13.13.m1.4.4.3.3" xref="alg2.13.13.m1.4.4.3.3.cmml">t</mi></msub><mo stretchy="false" id="alg2.13.13.m1.4.4.2" xref="alg2.13.13.m1.4.4.2.cmml">←</mo><mrow id="alg2.13.13.m1.4.4.1" xref="alg2.13.13.m1.4.4.1.cmml"><mrow id="alg2.13.13.m1.4.4.1.3" xref="alg2.13.13.m1.4.4.1.3.cmml"><msub id="alg2.13.13.m1.4.4.1.3.2" xref="alg2.13.13.m1.4.4.1.3.2.cmml"><mi id="alg2.13.13.m1.4.4.1.3.2.2" xref="alg2.13.13.m1.4.4.1.3.2.2.cmml">W</mi><mi id="alg2.13.13.m1.4.4.1.3.2.3" xref="alg2.13.13.m1.4.4.1.3.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="alg2.13.13.m1.4.4.1.3.1" xref="alg2.13.13.m1.4.4.1.3.1.cmml">​</mo><mrow id="alg2.13.13.m1.4.4.1.3.3.2" xref="alg2.13.13.m1.4.4.1.3.cmml"><mo stretchy="false" id="alg2.13.13.m1.4.4.1.3.3.2.1" xref="alg2.13.13.m1.4.4.1.3.cmml">(</mo><mi id="alg2.13.13.m1.1.1" xref="alg2.13.13.m1.1.1.cmml">l</mi><mo stretchy="false" id="alg2.13.13.m1.4.4.1.3.3.2.2" xref="alg2.13.13.m1.4.4.1.3.cmml">)</mo></mrow></mrow><mo id="alg2.13.13.m1.4.4.1.2" xref="alg2.13.13.m1.4.4.1.2.cmml">−</mo><mrow id="alg2.13.13.m1.4.4.1.1" xref="alg2.13.13.m1.4.4.1.1.cmml"><mi id="alg2.13.13.m1.4.4.1.1.3" xref="alg2.13.13.m1.4.4.1.1.3.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="alg2.13.13.m1.4.4.1.1.2" xref="alg2.13.13.m1.4.4.1.1.2.cmml">​</mo><mrow id="alg2.13.13.m1.4.4.1.1.4" xref="alg2.13.13.m1.4.4.1.1.4.cmml"><mo rspace="0.167em" id="alg2.13.13.m1.4.4.1.1.4.1" xref="alg2.13.13.m1.4.4.1.1.4.1.cmml">∇</mo><mi id="alg2.13.13.m1.4.4.1.1.4.2" xref="alg2.13.13.m1.4.4.1.1.4.2.cmml">l</mi></mrow><mo lspace="0em" rspace="0em" id="alg2.13.13.m1.4.4.1.1.2a" xref="alg2.13.13.m1.4.4.1.1.2.cmml">​</mo><mrow id="alg2.13.13.m1.4.4.1.1.1.1" xref="alg2.13.13.m1.4.4.1.1.1.2.cmml"><mo stretchy="false" id="alg2.13.13.m1.4.4.1.1.1.1.2" xref="alg2.13.13.m1.4.4.1.1.1.2.cmml">(</mo><mrow id="alg2.13.13.m1.4.4.1.1.1.1.1" xref="alg2.13.13.m1.4.4.1.1.1.1.1.cmml"><msub id="alg2.13.13.m1.4.4.1.1.1.1.1.2" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2.cmml"><mi id="alg2.13.13.m1.4.4.1.1.1.1.1.2.2" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2.2.cmml">W</mi><mi id="alg2.13.13.m1.4.4.1.1.1.1.1.2.3" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="alg2.13.13.m1.4.4.1.1.1.1.1.1" xref="alg2.13.13.m1.4.4.1.1.1.1.1.1.cmml">​</mo><mrow id="alg2.13.13.m1.4.4.1.1.1.1.1.3.2" xref="alg2.13.13.m1.4.4.1.1.1.1.1.cmml"><mo stretchy="false" id="alg2.13.13.m1.4.4.1.1.1.1.1.3.2.1" xref="alg2.13.13.m1.4.4.1.1.1.1.1.cmml">(</mo><mi id="alg2.13.13.m1.2.2" xref="alg2.13.13.m1.2.2.cmml">l</mi><mo stretchy="false" id="alg2.13.13.m1.4.4.1.1.1.1.1.3.2.2" xref="alg2.13.13.m1.4.4.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="alg2.13.13.m1.4.4.1.1.1.1.3" xref="alg2.13.13.m1.4.4.1.1.1.2.cmml">,</mo><mi id="alg2.13.13.m1.3.3" xref="alg2.13.13.m1.3.3.cmml">b</mi><mo stretchy="false" id="alg2.13.13.m1.4.4.1.1.1.1.4" xref="alg2.13.13.m1.4.4.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.13.13.m1.4b"><apply id="alg2.13.13.m1.4.4.cmml" xref="alg2.13.13.m1.4.4"><ci id="alg2.13.13.m1.4.4.2.cmml" xref="alg2.13.13.m1.4.4.2">←</ci><apply id="alg2.13.13.m1.4.4.3.cmml" xref="alg2.13.13.m1.4.4.3"><csymbol cd="ambiguous" id="alg2.13.13.m1.4.4.3.1.cmml" xref="alg2.13.13.m1.4.4.3">subscript</csymbol><ci id="alg2.13.13.m1.4.4.3.2.cmml" xref="alg2.13.13.m1.4.4.3.2">𝑊</ci><ci id="alg2.13.13.m1.4.4.3.3.cmml" xref="alg2.13.13.m1.4.4.3.3">𝑡</ci></apply><apply id="alg2.13.13.m1.4.4.1.cmml" xref="alg2.13.13.m1.4.4.1"><minus id="alg2.13.13.m1.4.4.1.2.cmml" xref="alg2.13.13.m1.4.4.1.2"></minus><apply id="alg2.13.13.m1.4.4.1.3.cmml" xref="alg2.13.13.m1.4.4.1.3"><times id="alg2.13.13.m1.4.4.1.3.1.cmml" xref="alg2.13.13.m1.4.4.1.3.1"></times><apply id="alg2.13.13.m1.4.4.1.3.2.cmml" xref="alg2.13.13.m1.4.4.1.3.2"><csymbol cd="ambiguous" id="alg2.13.13.m1.4.4.1.3.2.1.cmml" xref="alg2.13.13.m1.4.4.1.3.2">subscript</csymbol><ci id="alg2.13.13.m1.4.4.1.3.2.2.cmml" xref="alg2.13.13.m1.4.4.1.3.2.2">𝑊</ci><ci id="alg2.13.13.m1.4.4.1.3.2.3.cmml" xref="alg2.13.13.m1.4.4.1.3.2.3">𝑡</ci></apply><ci id="alg2.13.13.m1.1.1.cmml" xref="alg2.13.13.m1.1.1">𝑙</ci></apply><apply id="alg2.13.13.m1.4.4.1.1.cmml" xref="alg2.13.13.m1.4.4.1.1"><times id="alg2.13.13.m1.4.4.1.1.2.cmml" xref="alg2.13.13.m1.4.4.1.1.2"></times><ci id="alg2.13.13.m1.4.4.1.1.3.cmml" xref="alg2.13.13.m1.4.4.1.1.3">𝜂</ci><apply id="alg2.13.13.m1.4.4.1.1.4.cmml" xref="alg2.13.13.m1.4.4.1.1.4"><ci id="alg2.13.13.m1.4.4.1.1.4.1.cmml" xref="alg2.13.13.m1.4.4.1.1.4.1">∇</ci><ci id="alg2.13.13.m1.4.4.1.1.4.2.cmml" xref="alg2.13.13.m1.4.4.1.1.4.2">𝑙</ci></apply><interval closure="open" id="alg2.13.13.m1.4.4.1.1.1.2.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1"><apply id="alg2.13.13.m1.4.4.1.1.1.1.1.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1.1"><times id="alg2.13.13.m1.4.4.1.1.1.1.1.1.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1.1.1"></times><apply id="alg2.13.13.m1.4.4.1.1.1.1.1.2.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2"><csymbol cd="ambiguous" id="alg2.13.13.m1.4.4.1.1.1.1.1.2.1.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2">subscript</csymbol><ci id="alg2.13.13.m1.4.4.1.1.1.1.1.2.2.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2.2">𝑊</ci><ci id="alg2.13.13.m1.4.4.1.1.1.1.1.2.3.cmml" xref="alg2.13.13.m1.4.4.1.1.1.1.1.2.3">𝑡</ci></apply><ci id="alg2.13.13.m1.2.2.cmml" xref="alg2.13.13.m1.2.2">𝑙</ci></apply><ci id="alg2.13.13.m1.3.3.cmml" xref="alg2.13.13.m1.3.3">𝑏</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.13.13.m1.4c">W_{t}\leftarrow W_{t}(l)-\eta\nabla l(W_{t}(l),b)</annotation></semantics></math>
</div>
<div id="alg2.14.18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="alg2.14.19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end for
</div>
<div id="alg2.14.20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
</div>
<div id="alg2.14.21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>    end for
</div>
<div id="alg2.14.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span id="alg2.14.14.2" class="ltx_text ltx_font_bold">return</span> <em id="alg2.14.14.1" class="ltx_emph ltx_font_italic"><math id="alg2.14.14.1.m1.1" class="ltx_Math" alttext="W_{t}" display="inline"><semantics id="alg2.14.14.1.m1.1a"><msub id="alg2.14.14.1.m1.1.1" xref="alg2.14.14.1.m1.1.1.cmml"><mi id="alg2.14.14.1.m1.1.1.2" xref="alg2.14.14.1.m1.1.1.2.cmml">W</mi><mi id="alg2.14.14.1.m1.1.1.3" xref="alg2.14.14.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.14.14.1.m1.1b"><apply id="alg2.14.14.1.m1.1.1.cmml" xref="alg2.14.14.1.m1.1.1"><csymbol cd="ambiguous" id="alg2.14.14.1.m1.1.1.1.cmml" xref="alg2.14.14.1.m1.1.1">subscript</csymbol><ci id="alg2.14.14.1.m1.1.1.2.cmml" xref="alg2.14.14.1.m1.1.1.2">𝑊</ci><ci id="alg2.14.14.1.m1.1.1.3.cmml" xref="alg2.14.14.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.14.14.1.m1.1c">W_{t}</annotation></semantics></math></em>
</div>
<div id="alg2.14.22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">15</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="alg2.14.23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">16</span> 
</div>
<div id="alg2.14.24" class="ltx_listingline">

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg2.24.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span>Local client update, <span id="alg2.25.2" class="ltx_text ltx_font_bold">k</span>: Number of clients, <math id="alg2.18.m1.1" class="ltx_Math" alttext="N_{l}" display="inline"><semantics id="alg2.18.m1.1b"><msub id="alg2.18.m1.1.1" xref="alg2.18.m1.1.1.cmml"><mi id="alg2.18.m1.1.1.2" xref="alg2.18.m1.1.1.2.cmml">N</mi><mi id="alg2.18.m1.1.1.3" xref="alg2.18.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="alg2.18.m1.1c"><apply id="alg2.18.m1.1.1.cmml" xref="alg2.18.m1.1.1"><csymbol cd="ambiguous" id="alg2.18.m1.1.1.1.cmml" xref="alg2.18.m1.1.1">subscript</csymbol><ci id="alg2.18.m1.1.1.2.cmml" xref="alg2.18.m1.1.1.2">𝑁</ci><ci id="alg2.18.m1.1.1.3.cmml" xref="alg2.18.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.18.m1.1d">N_{l}</annotation></semantics></math> : Number of trained layers, <math id="alg2.19.m2.1" class="ltx_Math" alttext="D^{k}" display="inline"><semantics id="alg2.19.m2.1b"><msup id="alg2.19.m2.1.1" xref="alg2.19.m2.1.1.cmml"><mi id="alg2.19.m2.1.1.2" xref="alg2.19.m2.1.1.2.cmml">D</mi><mi id="alg2.19.m2.1.1.3" xref="alg2.19.m2.1.1.3.cmml">k</mi></msup><annotation-xml encoding="MathML-Content" id="alg2.19.m2.1c"><apply id="alg2.19.m2.1.1.cmml" xref="alg2.19.m2.1.1"><csymbol cd="ambiguous" id="alg2.19.m2.1.1.1.cmml" xref="alg2.19.m2.1.1">superscript</csymbol><ci id="alg2.19.m2.1.1.2.cmml" xref="alg2.19.m2.1.1.2">𝐷</ci><ci id="alg2.19.m2.1.1.3.cmml" xref="alg2.19.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.19.m2.1d">D^{k}</annotation></semantics></math>: Client k local dataset, <span id="alg2.26.3" class="ltx_text ltx_font_bold">e</span>: Number of local epochs, and <math id="alg2.20.m3.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg2.20.m3.1b"><mi id="alg2.20.m3.1.1" xref="alg2.20.m3.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="alg2.20.m3.1c"><ci id="alg2.20.m3.1.1.cmml" xref="alg2.20.m3.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.20.m3.1d">\eta</annotation></semantics></math> is the learning rate </figcaption>
</figure>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p">In all experiments that follow we have used the FedAvg implementation in the FEDn federated learning framework <cite class="ltx_cite ltx_citemacro_cite">Ekmefjord et al. (<a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>. FEDn is highly scalable and fully distributed and can be used without modification for the evaluated strategy. This also serves to illustrate that the approach can be embedded in a production-grade distributed system in a straightforward manner to reduce resource utilization on the edge device. It should be noted however, that to fully benefit from the potential network transfer reduction, some minor modifications to the FEDn aggregation server would be needed.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Our carefully designed experiments have yielded valuable insights: in a federated training setting with restricted resources, choosing a limited number of layers at each client location can achieve results comparable to training the complete model at each site. A noteworthy observation we made was that a <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">negative correlation</span> exists between the number of clients and the number of layers that need to be trained at each client location. As the number of clients increases, the required number of layers trained per client decreases, indicating that this approach can be especially effective for cross-device use cases. These findings provide valuable insights into federated learning optimization and offer practical recommendations for limited resource edge device deployment.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">We outline our experiments’ datasets, models, and configurations in the following subsections. We also introduce the evaluation metrics we used to assess the performance of our models (subsection <a href="#S5.SS1" title="5.1 Experimental Settings ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>). Afterwards, we provide a comprehensive discussion of the results we obtained (subsection <a href="#S5.SS2" title="5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>).</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental Settings</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">To explore the viability of our proposed approach, we utilized three open-source datasets and their associated machine learning models, specifically selected to represent a diverse range of application domains (computer vision, natural language processing (NLP), and Internet-of-Things). Our investigation focused on evaluating model convergence, network load and communication cost, impact on training time, and resource utilization.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i1.p1.9" class="ltx_p"><span id="S5.I1.i1.p1.9.1" class="ltx_text ltx_font_bold">Experiment 1</span>: This experiment focuses on a computer vision task that utilizes CIFAR-10 dataset<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="http://www.cs.toronto.edu/~kriz/cifar.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.cs.toronto.edu/~kriz/cifar.html</a></span></span></span>. The dataset consists of <math id="S5.I1.i1.p1.1.m1.2" class="ltx_Math" alttext="60,000" display="inline"><semantics id="S5.I1.i1.p1.1.m1.2a"><mrow id="S5.I1.i1.p1.1.m1.2.3.2" xref="S5.I1.i1.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml">60</mn><mo id="S5.I1.i1.p1.1.m1.2.3.2.1" xref="S5.I1.i1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i1.p1.1.m1.2.2" xref="S5.I1.i1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.2b"><list id="S5.I1.i1.p1.1.m1.2.3.1.cmml" xref="S5.I1.i1.p1.1.m1.2.3.2"><cn type="integer" id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1">60</cn><cn type="integer" id="S5.I1.i1.p1.1.m1.2.2.cmml" xref="S5.I1.i1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.2c">60,000</annotation></semantics></math> colour images with dimensions of <math id="S5.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S5.I1.i1.p1.2.m2.1a"><mrow id="S5.I1.i1.p1.2.m2.1.1" xref="S5.I1.i1.p1.2.m2.1.1.cmml"><mn id="S5.I1.i1.p1.2.m2.1.1.2" xref="S5.I1.i1.p1.2.m2.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S5.I1.i1.p1.2.m2.1.1.1" xref="S5.I1.i1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S5.I1.i1.p1.2.m2.1.1.3" xref="S5.I1.i1.p1.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.2.m2.1b"><apply id="S5.I1.i1.p1.2.m2.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1"><times id="S5.I1.i1.p1.2.m2.1.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1.1"></times><cn type="integer" id="S5.I1.i1.p1.2.m2.1.1.2.cmml" xref="S5.I1.i1.p1.2.m2.1.1.2">32</cn><cn type="integer" id="S5.I1.i1.p1.2.m2.1.1.3.cmml" xref="S5.I1.i1.p1.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.2.m2.1c">32\times 32</annotation></semantics></math> pixels, grouped into <math id="S5.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.I1.i1.p1.3.m3.1a"><mn id="S5.I1.i1.p1.3.m3.1.1" xref="S5.I1.i1.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.3.m3.1b"><cn type="integer" id="S5.I1.i1.p1.3.m3.1.1.cmml" xref="S5.I1.i1.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.3.m3.1c">10</annotation></semantics></math> classes, with <math id="S5.I1.i1.p1.4.m4.2" class="ltx_Math" alttext="6,000" display="inline"><semantics id="S5.I1.i1.p1.4.m4.2a"><mrow id="S5.I1.i1.p1.4.m4.2.3.2" xref="S5.I1.i1.p1.4.m4.2.3.1.cmml"><mn id="S5.I1.i1.p1.4.m4.1.1" xref="S5.I1.i1.p1.4.m4.1.1.cmml">6</mn><mo id="S5.I1.i1.p1.4.m4.2.3.2.1" xref="S5.I1.i1.p1.4.m4.2.3.1.cmml">,</mo><mn id="S5.I1.i1.p1.4.m4.2.2" xref="S5.I1.i1.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.4.m4.2b"><list id="S5.I1.i1.p1.4.m4.2.3.1.cmml" xref="S5.I1.i1.p1.4.m4.2.3.2"><cn type="integer" id="S5.I1.i1.p1.4.m4.1.1.cmml" xref="S5.I1.i1.p1.4.m4.1.1">6</cn><cn type="integer" id="S5.I1.i1.p1.4.m4.2.2.cmml" xref="S5.I1.i1.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.4.m4.2c">6,000</annotation></semantics></math> samples per each. The dataset is divided into two subsets: <math id="S5.I1.i1.p1.5.m5.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S5.I1.i1.p1.5.m5.2a"><mrow id="S5.I1.i1.p1.5.m5.2.3.2" xref="S5.I1.i1.p1.5.m5.2.3.1.cmml"><mn id="S5.I1.i1.p1.5.m5.1.1" xref="S5.I1.i1.p1.5.m5.1.1.cmml">50</mn><mo id="S5.I1.i1.p1.5.m5.2.3.2.1" xref="S5.I1.i1.p1.5.m5.2.3.1.cmml">,</mo><mn id="S5.I1.i1.p1.5.m5.2.2" xref="S5.I1.i1.p1.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.5.m5.2b"><list id="S5.I1.i1.p1.5.m5.2.3.1.cmml" xref="S5.I1.i1.p1.5.m5.2.3.2"><cn type="integer" id="S5.I1.i1.p1.5.m5.1.1.cmml" xref="S5.I1.i1.p1.5.m5.1.1">50</cn><cn type="integer" id="S5.I1.i1.p1.5.m5.2.2.cmml" xref="S5.I1.i1.p1.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.5.m5.2c">50,000</annotation></semantics></math> training images and <math id="S5.I1.i1.p1.6.m6.2" class="ltx_Math" alttext="10,000" display="inline"><semantics id="S5.I1.i1.p1.6.m6.2a"><mrow id="S5.I1.i1.p1.6.m6.2.3.2" xref="S5.I1.i1.p1.6.m6.2.3.1.cmml"><mn id="S5.I1.i1.p1.6.m6.1.1" xref="S5.I1.i1.p1.6.m6.1.1.cmml">10</mn><mo id="S5.I1.i1.p1.6.m6.2.3.2.1" xref="S5.I1.i1.p1.6.m6.2.3.1.cmml">,</mo><mn id="S5.I1.i1.p1.6.m6.2.2" xref="S5.I1.i1.p1.6.m6.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.6.m6.2b"><list id="S5.I1.i1.p1.6.m6.2.3.1.cmml" xref="S5.I1.i1.p1.6.m6.2.3.2"><cn type="integer" id="S5.I1.i1.p1.6.m6.1.1.cmml" xref="S5.I1.i1.p1.6.m6.1.1">10</cn><cn type="integer" id="S5.I1.i1.p1.6.m6.2.2.cmml" xref="S5.I1.i1.p1.6.m6.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.6.m6.2c">10,000</annotation></semantics></math> test images. Also, we used the VGG16 model <cite class="ltx_cite ltx_citemacro_cite">Krizhevsky et al. (<a href="#bib.bib42" title="" class="ltx_ref">2009</a>)</cite> in this experiment. Table <a href="#S5.T1" title="Table 1 ‣ 5.1 Experimental Settings ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the model architecture, including layer types, output dimensions, and the number of trainable parameters per layer. The model has a total of <math id="S5.I1.i1.p1.7.m7.3" class="ltx_Math" alttext="14,736,714" display="inline"><semantics id="S5.I1.i1.p1.7.m7.3a"><mrow id="S5.I1.i1.p1.7.m7.3.4.2" xref="S5.I1.i1.p1.7.m7.3.4.1.cmml"><mn id="S5.I1.i1.p1.7.m7.1.1" xref="S5.I1.i1.p1.7.m7.1.1.cmml">14</mn><mo id="S5.I1.i1.p1.7.m7.3.4.2.1" xref="S5.I1.i1.p1.7.m7.3.4.1.cmml">,</mo><mn id="S5.I1.i1.p1.7.m7.2.2" xref="S5.I1.i1.p1.7.m7.2.2.cmml">736</mn><mo id="S5.I1.i1.p1.7.m7.3.4.2.2" xref="S5.I1.i1.p1.7.m7.3.4.1.cmml">,</mo><mn id="S5.I1.i1.p1.7.m7.3.3" xref="S5.I1.i1.p1.7.m7.3.3.cmml">714</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.7.m7.3b"><list id="S5.I1.i1.p1.7.m7.3.4.1.cmml" xref="S5.I1.i1.p1.7.m7.3.4.2"><cn type="integer" id="S5.I1.i1.p1.7.m7.1.1.cmml" xref="S5.I1.i1.p1.7.m7.1.1">14</cn><cn type="integer" id="S5.I1.i1.p1.7.m7.2.2.cmml" xref="S5.I1.i1.p1.7.m7.2.2">736</cn><cn type="integer" id="S5.I1.i1.p1.7.m7.3.3.cmml" xref="S5.I1.i1.p1.7.m7.3.3">714</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.7.m7.3c">14,736,714</annotation></semantics></math> parameters and <math id="S5.I1.i1.p1.8.m8.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.I1.i1.p1.8.m8.1a"><mn id="S5.I1.i1.p1.8.m8.1.1" xref="S5.I1.i1.p1.8.m8.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.8.m8.1b"><cn type="integer" id="S5.I1.i1.p1.8.m8.1.1.cmml" xref="S5.I1.i1.p1.8.m8.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.8.m8.1c">14</annotation></semantics></math> trainable layers, including the output layer. The generated model size is <math id="S5.I1.i1.p1.9.m9.1" class="ltx_Math" alttext="53.5MB" display="inline"><semantics id="S5.I1.i1.p1.9.m9.1a"><mrow id="S5.I1.i1.p1.9.m9.1.1" xref="S5.I1.i1.p1.9.m9.1.1.cmml"><mn id="S5.I1.i1.p1.9.m9.1.1.2" xref="S5.I1.i1.p1.9.m9.1.1.2.cmml">53.5</mn><mo lspace="0em" rspace="0em" id="S5.I1.i1.p1.9.m9.1.1.1" xref="S5.I1.i1.p1.9.m9.1.1.1.cmml">​</mo><mi id="S5.I1.i1.p1.9.m9.1.1.3" xref="S5.I1.i1.p1.9.m9.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.I1.i1.p1.9.m9.1.1.1a" xref="S5.I1.i1.p1.9.m9.1.1.1.cmml">​</mo><mi id="S5.I1.i1.p1.9.m9.1.1.4" xref="S5.I1.i1.p1.9.m9.1.1.4.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.9.m9.1b"><apply id="S5.I1.i1.p1.9.m9.1.1.cmml" xref="S5.I1.i1.p1.9.m9.1.1"><times id="S5.I1.i1.p1.9.m9.1.1.1.cmml" xref="S5.I1.i1.p1.9.m9.1.1.1"></times><cn type="float" id="S5.I1.i1.p1.9.m9.1.1.2.cmml" xref="S5.I1.i1.p1.9.m9.1.1.2">53.5</cn><ci id="S5.I1.i1.p1.9.m9.1.1.3.cmml" xref="S5.I1.i1.p1.9.m9.1.1.3">𝑀</ci><ci id="S5.I1.i1.p1.9.m9.1.1.4.cmml" xref="S5.I1.i1.p1.9.m9.1.1.4">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.9.m9.1c">53.5MB</annotation></semantics></math>. For more information, please refer to the client source code available on GitHub.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/saadiabadi/cifar_updated.git" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/saadiabadi/cifar_updated.git</a></span></span></span>.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i2.p1.2" class="ltx_p"><span id="S5.I1.i2.p1.2.1" class="ltx_text ltx_font_bold">Experiment 2</span>:
This experiment is centred around sentiment analysis tasks using the IMDB dataset v1.0<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://ai.stanford.edu/~amaas/data/sentiment/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.stanford.edu/~amaas/data/sentiment/</a></span></span></span>. The dataset consists of <math id="S5.I1.i2.p1.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S5.I1.i2.p1.1.m1.2a"><mrow id="S5.I1.i2.p1.1.m1.2.3.2" xref="S5.I1.i2.p1.1.m1.2.3.1.cmml"><mn id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">50</mn><mo id="S5.I1.i2.p1.1.m1.2.3.2.1" xref="S5.I1.i2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.I1.i2.p1.1.m1.2.2" xref="S5.I1.i2.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.2b"><list id="S5.I1.i2.p1.1.m1.2.3.1.cmml" xref="S5.I1.i2.p1.1.m1.2.3.2"><cn type="integer" id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1">50</cn><cn type="integer" id="S5.I1.i2.p1.1.m1.2.2.cmml" xref="S5.I1.i2.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.2c">50,000</annotation></semantics></math> reviews, with a maximum of <math id="S5.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.I1.i2.p1.2.m2.1a"><mn id="S5.I1.i2.p1.2.m2.1.1" xref="S5.I1.i2.p1.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.2.m2.1b"><cn type="integer" id="S5.I1.i2.p1.2.m2.1.1.cmml" xref="S5.I1.i2.p1.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.2.m2.1c">30</annotation></semantics></math> reviews per movie, equally divided between positive and negative reviews. In <cite class="ltx_cite ltx_citemacro_cite">Maas et al. (<a href="#bib.bib43" title="" class="ltx_ref">2011</a>)</cite>, the dataset has been used for sentiment analysis tasks, where a machine learning model predicts whether a given review is positive or negative based on the review text. We used a deep learning model to predict the review decision for this task. Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Experimental Settings ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the detailed architecture of the model, including the layer types, dimensions and parameters used to construct and generate the initial model. Further technical details can be found on the client source code GitHub<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/saadiabadi/IMDB_Example.git" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/saadiabadi/IMDB_Example.git</a></span></span></span>.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i3.p1.6" class="ltx_p"><span id="S5.I1.i3.p1.6.1" class="ltx_text ltx_font_bold">Experiment 3</span>: In this experiment, the focus is on human activity recognition (HAR) using the CASA dataset<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+from+Continuous+Ambient+Sensor+Data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+from+Continuous+Ambient+Sensor+Data</a></span></span></span>. The dataset comprises <math id="S5.I1.i3.p1.1.m1.3" class="ltx_Math" alttext="13,956,534" display="inline"><semantics id="S5.I1.i3.p1.1.m1.3a"><mrow id="S5.I1.i3.p1.1.m1.3.4.2" xref="S5.I1.i3.p1.1.m1.3.4.1.cmml"><mn id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml">13</mn><mo id="S5.I1.i3.p1.1.m1.3.4.2.1" xref="S5.I1.i3.p1.1.m1.3.4.1.cmml">,</mo><mn id="S5.I1.i3.p1.1.m1.2.2" xref="S5.I1.i3.p1.1.m1.2.2.cmml">956</mn><mo id="S5.I1.i3.p1.1.m1.3.4.2.2" xref="S5.I1.i3.p1.1.m1.3.4.1.cmml">,</mo><mn id="S5.I1.i3.p1.1.m1.3.3" xref="S5.I1.i3.p1.1.m1.3.3.cmml">534</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.3b"><list id="S5.I1.i3.p1.1.m1.3.4.1.cmml" xref="S5.I1.i3.p1.1.m1.3.4.2"><cn type="integer" id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1">13</cn><cn type="integer" id="S5.I1.i3.p1.1.m1.2.2.cmml" xref="S5.I1.i3.p1.1.m1.2.2">956</cn><cn type="integer" id="S5.I1.i3.p1.1.m1.3.3.cmml" xref="S5.I1.i3.p1.1.m1.3.3">534</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.3c">13,956,534</annotation></semantics></math> patterns collected over two months from 30 homes using continuous ambient and PIR sensors. Each pattern consists of a set of <math id="S5.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="37" display="inline"><semantics id="S5.I1.i3.p1.2.m2.1a"><mn id="S5.I1.i3.p1.2.m2.1.1" xref="S5.I1.i3.p1.2.m2.1.1.cmml">37</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.2.m2.1b"><cn type="integer" id="S5.I1.i3.p1.2.m2.1.1.cmml" xref="S5.I1.i3.p1.2.m2.1.1">37</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.2.m2.1c">37</annotation></semantics></math> features linked to different sensors distributed throughout the home, representing daily human activities such as sleep, eating, reading, and watching TV. The goal of this experiment is to classify the output into <math id="S5.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.I1.i3.p1.3.m3.1a"><mn id="S5.I1.i3.p1.3.m3.1.1" xref="S5.I1.i3.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.3.m3.1b"><cn type="integer" id="S5.I1.i3.p1.3.m3.1.1.cmml" xref="S5.I1.i3.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.3.m3.1c">10</annotation></semantics></math> different daily activities for each user. We use a Long Short-Term Memory (LSTM) model with an input layer dimension of <math id="S5.I1.i3.p1.4.m4.3" class="ltx_Math" alttext="(100,1,36)" display="inline"><semantics id="S5.I1.i3.p1.4.m4.3a"><mrow id="S5.I1.i3.p1.4.m4.3.4.2" xref="S5.I1.i3.p1.4.m4.3.4.1.cmml"><mo stretchy="false" id="S5.I1.i3.p1.4.m4.3.4.2.1" xref="S5.I1.i3.p1.4.m4.3.4.1.cmml">(</mo><mn id="S5.I1.i3.p1.4.m4.1.1" xref="S5.I1.i3.p1.4.m4.1.1.cmml">100</mn><mo id="S5.I1.i3.p1.4.m4.3.4.2.2" xref="S5.I1.i3.p1.4.m4.3.4.1.cmml">,</mo><mn id="S5.I1.i3.p1.4.m4.2.2" xref="S5.I1.i3.p1.4.m4.2.2.cmml">1</mn><mo id="S5.I1.i3.p1.4.m4.3.4.2.3" xref="S5.I1.i3.p1.4.m4.3.4.1.cmml">,</mo><mn id="S5.I1.i3.p1.4.m4.3.3" xref="S5.I1.i3.p1.4.m4.3.3.cmml">36</mn><mo stretchy="false" id="S5.I1.i3.p1.4.m4.3.4.2.4" xref="S5.I1.i3.p1.4.m4.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.4.m4.3b"><vector id="S5.I1.i3.p1.4.m4.3.4.1.cmml" xref="S5.I1.i3.p1.4.m4.3.4.2"><cn type="integer" id="S5.I1.i3.p1.4.m4.1.1.cmml" xref="S5.I1.i3.p1.4.m4.1.1">100</cn><cn type="integer" id="S5.I1.i3.p1.4.m4.2.2.cmml" xref="S5.I1.i3.p1.4.m4.2.2">1</cn><cn type="integer" id="S5.I1.i3.p1.4.m4.3.3.cmml" xref="S5.I1.i3.p1.4.m4.3.3">36</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.4.m4.3c">(100,1,36)</annotation></semantics></math>, four dense layers, and one output layer. The model has <math id="S5.I1.i3.p1.5.m5.2" class="ltx_Math" alttext="68,884" display="inline"><semantics id="S5.I1.i3.p1.5.m5.2a"><mrow id="S5.I1.i3.p1.5.m5.2.3.2" xref="S5.I1.i3.p1.5.m5.2.3.1.cmml"><mn id="S5.I1.i3.p1.5.m5.1.1" xref="S5.I1.i3.p1.5.m5.1.1.cmml">68</mn><mo id="S5.I1.i3.p1.5.m5.2.3.2.1" xref="S5.I1.i3.p1.5.m5.2.3.1.cmml">,</mo><mn id="S5.I1.i3.p1.5.m5.2.2" xref="S5.I1.i3.p1.5.m5.2.2.cmml">884</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.5.m5.2b"><list id="S5.I1.i3.p1.5.m5.2.3.1.cmml" xref="S5.I1.i3.p1.5.m5.2.3.2"><cn type="integer" id="S5.I1.i3.p1.5.m5.1.1.cmml" xref="S5.I1.i3.p1.5.m5.1.1">68</cn><cn type="integer" id="S5.I1.i3.p1.5.m5.2.2.cmml" xref="S5.I1.i3.p1.5.m5.2.2">884</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.5.m5.2c">68,884</annotation></semantics></math> trainable parameters, and the compiled model size is <math id="S5.I1.i3.p1.6.m6.1" class="ltx_Math" alttext="254KB" display="inline"><semantics id="S5.I1.i3.p1.6.m6.1a"><mrow id="S5.I1.i3.p1.6.m6.1.1" xref="S5.I1.i3.p1.6.m6.1.1.cmml"><mn id="S5.I1.i3.p1.6.m6.1.1.2" xref="S5.I1.i3.p1.6.m6.1.1.2.cmml">254</mn><mo lspace="0em" rspace="0em" id="S5.I1.i3.p1.6.m6.1.1.1" xref="S5.I1.i3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.I1.i3.p1.6.m6.1.1.3" xref="S5.I1.i3.p1.6.m6.1.1.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.I1.i3.p1.6.m6.1.1.1a" xref="S5.I1.i3.p1.6.m6.1.1.1.cmml">​</mo><mi id="S5.I1.i3.p1.6.m6.1.1.4" xref="S5.I1.i3.p1.6.m6.1.1.4.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.6.m6.1b"><apply id="S5.I1.i3.p1.6.m6.1.1.cmml" xref="S5.I1.i3.p1.6.m6.1.1"><times id="S5.I1.i3.p1.6.m6.1.1.1.cmml" xref="S5.I1.i3.p1.6.m6.1.1.1"></times><cn type="integer" id="S5.I1.i3.p1.6.m6.1.1.2.cmml" xref="S5.I1.i3.p1.6.m6.1.1.2">254</cn><ci id="S5.I1.i3.p1.6.m6.1.1.3.cmml" xref="S5.I1.i3.p1.6.m6.1.1.3">𝐾</ci><ci id="S5.I1.i3.p1.6.m6.1.1.4.cmml" xref="S5.I1.i3.p1.6.m6.1.1.4">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.6.m6.1c">254KB</annotation></semantics></math>. For further technical details, please refer to the client source code available on GitHub<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://github.com/saadiabadi/Casa_IoT_Example.git" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/saadiabadi/Casa_IoT_Example.git</a></span></span></span>.</p>
</div>
</li>
</ul>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>The VGG16 model architecture details used in computer vision experiment.</figcaption>
<div id="S5.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:709.9pt;height:276.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.4pt,15.3pt) scale(0.9,0.9) ;">
<table id="S5.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.1.1" class="ltx_tr">
<td id="S5.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Layer type</td>
<td id="S5.T1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Output dimension</td>
<td id="S5.T1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Param #</td>
<td id="S5.T1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Layer type</td>
<td id="S5.T1.1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Output dimension</td>
<td id="S5.T1.1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">Param #</td>
<td id="S5.T1.1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Layer type</td>
<td id="S5.T1.1.1.1.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Output dimension</td>
<td id="S5.T1.1.1.1.1.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Param #</td>
</tr>
<tr id="S5.T1.1.1.2.2" class="ltx_tr">
<td id="S5.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">conv2d</td>
<td id="S5.T1.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(32, 32, 64)</td>
<td id="S5.T1.1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">1792</td>
<td id="S5.T1.1.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_4</td>
<td id="S5.T1.1.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.2.2.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.2.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_9</td>
<td id="S5.T1.1.1.2.2.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.2.2.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.3.3" class="ltx_tr">
<td id="S5.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">batch_normalization</td>
<td id="S5.T1.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(32, 32, 64)</td>
<td id="S5.T1.1.1.3.3.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">256</td>
<td id="S5.T1.1.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_5</td>
<td id="S5.T1.1.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.3.3.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">590080</td>
<td id="S5.T1.1.1.3.3.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">max_pooling2d_3</td>
<td id="S5.T1.1.1.3.3.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.3.3.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.4.4" class="ltx_tr">
<td id="S5.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">activation</td>
<td id="S5.T1.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(32, 32, 64)</td>
<td id="S5.T1.1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_5</td>
<td id="S5.T1.1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.4.4.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">1024</td>
<td id="S5.T1.1.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_10</td>
<td id="S5.T1.1.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2359808</td>
</tr>
<tr id="S5.T1.1.1.5.5" class="ltx_tr">
<td id="S5.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">conv2d_1</td>
<td id="S5.T1.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(32, 32, 64)</td>
<td id="S5.T1.1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">36928</td>
<td id="S5.T1.1.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_5</td>
<td id="S5.T1.1.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.5.5.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_10</td>
<td id="S5.T1.1.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2048</td>
</tr>
<tr id="S5.T1.1.1.6.6" class="ltx_tr">
<td id="S5.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">batch_normalization_1</td>
<td id="S5.T1.1.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(32, 32, 64)</td>
<td id="S5.T1.1.1.6.6.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">256</td>
<td id="S5.T1.1.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_6</td>
<td id="S5.T1.1.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.6.6.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">590080</td>
<td id="S5.T1.1.1.6.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_10</td>
<td id="S5.T1.1.1.6.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.6.6.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.7.7" class="ltx_tr">
<td id="S5.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">activation</td>
<td id="S5.T1.1.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(32, 32, 64)</td>
<td id="S5.T1.1.1.7.7.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_6</td>
<td id="S5.T1.1.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.7.7.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">1024</td>
<td id="S5.T1.1.1.7.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_11</td>
<td id="S5.T1.1.1.7.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.7.7.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2359808</td>
</tr>
<tr id="S5.T1.1.1.8.8" class="ltx_tr">
<td id="S5.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">max_pooling2d</td>
<td id="S5.T1.1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 64)</td>
<td id="S5.T1.1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_6</td>
<td id="S5.T1.1.1.8.8.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.8.8.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.8.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_11</td>
<td id="S5.T1.1.1.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2048</td>
</tr>
<tr id="S5.T1.1.1.9.9" class="ltx_tr">
<td id="S5.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">conv2d_2</td>
<td id="S5.T1.1.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 128)</td>
<td id="S5.T1.1.1.9.9.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">73856</td>
<td id="S5.T1.1.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">max_pooling2d_2</td>
<td id="S5.T1.1.1.9.9.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 256)</td>
<td id="S5.T1.1.1.9.9.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.9.9.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_11</td>
<td id="S5.T1.1.1.9.9.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.9.9.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.10.10" class="ltx_tr">
<td id="S5.T1.1.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">batch_normalization_2</td>
<td id="S5.T1.1.1.10.10.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 128)</td>
<td id="S5.T1.1.1.10.10.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">512</td>
<td id="S5.T1.1.1.10.10.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_7</td>
<td id="S5.T1.1.1.10.10.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.10.10.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">1180160</td>
<td id="S5.T1.1.1.10.10.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_12</td>
<td id="S5.T1.1.1.10.10.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.10.10.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2359808</td>
</tr>
<tr id="S5.T1.1.1.11.11" class="ltx_tr">
<td id="S5.T1.1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">activation_2</td>
<td id="S5.T1.1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 128)</td>
<td id="S5.T1.1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.11.11.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_7</td>
<td id="S5.T1.1.1.11.11.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.11.11.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">2048</td>
<td id="S5.T1.1.1.11.11.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_12</td>
<td id="S5.T1.1.1.11.11.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.11.11.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2048</td>
</tr>
<tr id="S5.T1.1.1.12.12" class="ltx_tr">
<td id="S5.T1.1.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">conv2d_3</td>
<td id="S5.T1.1.1.12.12.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 128)</td>
<td id="S5.T1.1.1.12.12.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">147584</td>
<td id="S5.T1.1.1.12.12.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_7</td>
<td id="S5.T1.1.1.12.12.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.12.12.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.12.12.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_12</td>
<td id="S5.T1.1.1.12.12.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(2, 2, 512)</td>
<td id="S5.T1.1.1.12.12.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.13.13" class="ltx_tr">
<td id="S5.T1.1.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">batch_normalization_3</td>
<td id="S5.T1.1.1.13.13.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 128)</td>
<td id="S5.T1.1.1.13.13.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">512</td>
<td id="S5.T1.1.1.13.13.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_8</td>
<td id="S5.T1.1.1.13.13.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.13.13.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">2359808</td>
<td id="S5.T1.1.1.13.13.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">max_pooling2d_4</td>
<td id="S5.T1.1.1.13.13.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(1, 1, 512)</td>
<td id="S5.T1.1.1.13.13.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.14.14" class="ltx_tr">
<td id="S5.T1.1.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">activation_3</td>
<td id="S5.T1.1.1.14.14.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(16, 16, 128)</td>
<td id="S5.T1.1.1.14.14.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.14.14.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">batch_normalization_8</td>
<td id="S5.T1.1.1.14.14.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.14.14.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">2048</td>
<td id="S5.T1.1.1.14.14.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">average_pooling2d</td>
<td id="S5.T1.1.1.14.14.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(1, 1, 512)</td>
<td id="S5.T1.1.1.14.14.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.15.15" class="ltx_tr">
<td id="S5.T1.1.1.15.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">max_pooling2d_1</td>
<td id="S5.T1.1.1.15.15.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 128)</td>
<td id="S5.T1.1.1.15.15.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.15.15.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">activation_8</td>
<td id="S5.T1.1.1.15.15.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.15.15.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">0</td>
<td id="S5.T1.1.1.15.15.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">flatten</td>
<td id="S5.T1.1.1.15.15.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(512)</td>
<td id="S5.T1.1.1.15.15.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">0</td>
</tr>
<tr id="S5.T1.1.1.16.16" class="ltx_tr">
<td id="S5.T1.1.1.16.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">conv2d_4</td>
<td id="S5.T1.1.1.16.16.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.16.16.3" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">295168</td>
<td id="S5.T1.1.1.16.16.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">conv2d_9</td>
<td id="S5.T1.1.1.16.16.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.16.16.6" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">2359808</td>
<td id="S5.T1.1.1.16.16.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">dense</td>
<td id="S5.T1.1.1.16.16.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">(10)</td>
<td id="S5.T1.1.1.16.16.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">5130</td>
</tr>
<tr id="S5.T1.1.1.17.17" class="ltx_tr">
<td id="S5.T1.1.1.17.17.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">batch_normalization_4</td>
<td id="S5.T1.1.1.17.17.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">(8, 8, 256)</td>
<td id="S5.T1.1.1.17.17.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t">1024</td>
<td id="S5.T1.1.1.17.17.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">batch_normalization_9</td>
<td id="S5.T1.1.1.17.17.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">(4, 4, 512)</td>
<td id="S5.T1.1.1.17.17.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t">2048</td>
<td id="S5.T1.1.1.17.17.7" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.1.17.17.8" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S5.T1.1.1.17.17.9" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model specification for the sentiments analysis architecture.</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:342.4pt;height:95pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.3pt,6.5pt) scale(0.88,0.88) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<td id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Layers</span></td>
<td id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Parameters characteristic</span></td>
</tr>
<tr id="S5.T2.1.1.2.2" class="ltx_tr">
<td id="S5.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Embedding layer</span></td>
<td id="S5.T2.1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">max features= 20000, maxlen= 100, embedding size= 128</td>
</tr>
<tr id="S5.T2.1.1.3.3" class="ltx_tr">
<td id="S5.T2.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Convolutional layer</span></td>
<td id="S5.T2.1.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">kernel size=5, filters= 64, pool size= 4</td>
</tr>
<tr id="S5.T2.1.1.4.4" class="ltx_tr">
<td id="S5.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.4.4.1.1" class="ltx_text ltx_font_bold">LSTM layer</span></td>
<td id="S5.T2.1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">lstm outputsize= 70</td>
</tr>
<tr id="S5.T2.1.1.5.5" class="ltx_tr">
<td id="S5.T2.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.1.1.5.5.1.1" class="ltx_text ltx_font_bold">One output Dense layer</span></td>
<td id="S5.T2.1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 outputs (positive or negative review)</td>
</tr>
<tr id="S5.T2.1.1.6.6" class="ltx_tr">
<td id="S5.T2.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" colspan="2"><span id="S5.T2.1.1.6.6.1.1" class="ltx_text ltx_font_bold">The generated model size = 10MB</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">For all conducted experiments, we used one local epoch, batch size 32, learning rate 0.01, and ADAM optimizer as the local training parameters settings. Moreover, Python and TensorFlow were used to implement the models and local model updates. The experiments were performed on the Swedish OpenStack Infrastructure as-a Service, SNIC Science Cloud (SSC)<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://cloud.snic.se/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cloud.snic.se/</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Toor et al. (<a href="#bib.bib44" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p">To evaluate model performance, we used the accuracy function provided by Tensorflow, whose return value that falls between (0, 100), as shown in Equation <a href="#S5.E2" title="In 5.1 Experimental Settings ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, as well as the loss function using categorical cross-entropy as shown in Equation <a href="#S5.E3" title="In 5.1 Experimental Settings ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.1" class="ltx_Math" alttext="Accuracy=\frac{TP+TN}{TP+TN+FN+FP}*100\%" display="block"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml"><mrow id="S5.E2.m1.1.1.2" xref="S5.E2.m1.1.1.2.cmml"><mi id="S5.E2.m1.1.1.2.2" xref="S5.E2.m1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.3" xref="S5.E2.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1a" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.4" xref="S5.E2.m1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1b" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.5" xref="S5.E2.m1.1.1.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1c" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.6" xref="S5.E2.m1.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1d" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.7" xref="S5.E2.m1.1.1.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1e" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.8" xref="S5.E2.m1.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.2.1f" xref="S5.E2.m1.1.1.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.2.9" xref="S5.E2.m1.1.1.2.9.cmml">y</mi></mrow><mo id="S5.E2.m1.1.1.1" xref="S5.E2.m1.1.1.1.cmml">=</mo><mrow id="S5.E2.m1.1.1.3" xref="S5.E2.m1.1.1.3.cmml"><mfrac id="S5.E2.m1.1.1.3.2" xref="S5.E2.m1.1.1.3.2.cmml"><mrow id="S5.E2.m1.1.1.3.2.2" xref="S5.E2.m1.1.1.3.2.2.cmml"><mrow id="S5.E2.m1.1.1.3.2.2.2" xref="S5.E2.m1.1.1.3.2.2.2.cmml"><mi id="S5.E2.m1.1.1.3.2.2.2.2" xref="S5.E2.m1.1.1.3.2.2.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.3.2.2.2.1" xref="S5.E2.m1.1.1.3.2.2.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.3.2.2.2.3" xref="S5.E2.m1.1.1.3.2.2.2.3.cmml">P</mi></mrow><mo id="S5.E2.m1.1.1.3.2.2.1" xref="S5.E2.m1.1.1.3.2.2.1.cmml">+</mo><mrow id="S5.E2.m1.1.1.3.2.2.3" xref="S5.E2.m1.1.1.3.2.2.3.cmml"><mi id="S5.E2.m1.1.1.3.2.2.3.2" xref="S5.E2.m1.1.1.3.2.2.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.3.2.2.3.1" xref="S5.E2.m1.1.1.3.2.2.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.3.2.2.3.3" xref="S5.E2.m1.1.1.3.2.2.3.3.cmml">N</mi></mrow></mrow><mrow id="S5.E2.m1.1.1.3.2.3" xref="S5.E2.m1.1.1.3.2.3.cmml"><mrow id="S5.E2.m1.1.1.3.2.3.2" xref="S5.E2.m1.1.1.3.2.3.2.cmml"><mi id="S5.E2.m1.1.1.3.2.3.2.2" xref="S5.E2.m1.1.1.3.2.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.3.2.3.2.1" xref="S5.E2.m1.1.1.3.2.3.2.1.cmml">​</mo><mi id="S5.E2.m1.1.1.3.2.3.2.3" xref="S5.E2.m1.1.1.3.2.3.2.3.cmml">P</mi></mrow><mo id="S5.E2.m1.1.1.3.2.3.1" xref="S5.E2.m1.1.1.3.2.3.1.cmml">+</mo><mrow id="S5.E2.m1.1.1.3.2.3.3" xref="S5.E2.m1.1.1.3.2.3.3.cmml"><mi id="S5.E2.m1.1.1.3.2.3.3.2" xref="S5.E2.m1.1.1.3.2.3.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.3.2.3.3.1" xref="S5.E2.m1.1.1.3.2.3.3.1.cmml">​</mo><mi id="S5.E2.m1.1.1.3.2.3.3.3" xref="S5.E2.m1.1.1.3.2.3.3.3.cmml">N</mi></mrow><mo id="S5.E2.m1.1.1.3.2.3.1a" xref="S5.E2.m1.1.1.3.2.3.1.cmml">+</mo><mrow id="S5.E2.m1.1.1.3.2.3.4" xref="S5.E2.m1.1.1.3.2.3.4.cmml"><mi id="S5.E2.m1.1.1.3.2.3.4.2" xref="S5.E2.m1.1.1.3.2.3.4.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.3.2.3.4.1" xref="S5.E2.m1.1.1.3.2.3.4.1.cmml">​</mo><mi id="S5.E2.m1.1.1.3.2.3.4.3" xref="S5.E2.m1.1.1.3.2.3.4.3.cmml">N</mi></mrow><mo id="S5.E2.m1.1.1.3.2.3.1b" xref="S5.E2.m1.1.1.3.2.3.1.cmml">+</mo><mrow id="S5.E2.m1.1.1.3.2.3.5" xref="S5.E2.m1.1.1.3.2.3.5.cmml"><mi id="S5.E2.m1.1.1.3.2.3.5.2" xref="S5.E2.m1.1.1.3.2.3.5.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.1.3.2.3.5.1" xref="S5.E2.m1.1.1.3.2.3.5.1.cmml">​</mo><mi id="S5.E2.m1.1.1.3.2.3.5.3" xref="S5.E2.m1.1.1.3.2.3.5.3.cmml">P</mi></mrow></mrow></mfrac><mo lspace="0.222em" rspace="0.222em" id="S5.E2.m1.1.1.3.1" xref="S5.E2.m1.1.1.3.1.cmml">∗</mo><mrow id="S5.E2.m1.1.1.3.3" xref="S5.E2.m1.1.1.3.3.cmml"><mn id="S5.E2.m1.1.1.3.3.2" xref="S5.E2.m1.1.1.3.3.2.cmml">100</mn><mo id="S5.E2.m1.1.1.3.3.1" xref="S5.E2.m1.1.1.3.3.1.cmml">%</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1"><eq id="S5.E2.m1.1.1.1.cmml" xref="S5.E2.m1.1.1.1"></eq><apply id="S5.E2.m1.1.1.2.cmml" xref="S5.E2.m1.1.1.2"><times id="S5.E2.m1.1.1.2.1.cmml" xref="S5.E2.m1.1.1.2.1"></times><ci id="S5.E2.m1.1.1.2.2.cmml" xref="S5.E2.m1.1.1.2.2">𝐴</ci><ci id="S5.E2.m1.1.1.2.3.cmml" xref="S5.E2.m1.1.1.2.3">𝑐</ci><ci id="S5.E2.m1.1.1.2.4.cmml" xref="S5.E2.m1.1.1.2.4">𝑐</ci><ci id="S5.E2.m1.1.1.2.5.cmml" xref="S5.E2.m1.1.1.2.5">𝑢</ci><ci id="S5.E2.m1.1.1.2.6.cmml" xref="S5.E2.m1.1.1.2.6">𝑟</ci><ci id="S5.E2.m1.1.1.2.7.cmml" xref="S5.E2.m1.1.1.2.7">𝑎</ci><ci id="S5.E2.m1.1.1.2.8.cmml" xref="S5.E2.m1.1.1.2.8">𝑐</ci><ci id="S5.E2.m1.1.1.2.9.cmml" xref="S5.E2.m1.1.1.2.9">𝑦</ci></apply><apply id="S5.E2.m1.1.1.3.cmml" xref="S5.E2.m1.1.1.3"><times id="S5.E2.m1.1.1.3.1.cmml" xref="S5.E2.m1.1.1.3.1"></times><apply id="S5.E2.m1.1.1.3.2.cmml" xref="S5.E2.m1.1.1.3.2"><divide id="S5.E2.m1.1.1.3.2.1.cmml" xref="S5.E2.m1.1.1.3.2"></divide><apply id="S5.E2.m1.1.1.3.2.2.cmml" xref="S5.E2.m1.1.1.3.2.2"><plus id="S5.E2.m1.1.1.3.2.2.1.cmml" xref="S5.E2.m1.1.1.3.2.2.1"></plus><apply id="S5.E2.m1.1.1.3.2.2.2.cmml" xref="S5.E2.m1.1.1.3.2.2.2"><times id="S5.E2.m1.1.1.3.2.2.2.1.cmml" xref="S5.E2.m1.1.1.3.2.2.2.1"></times><ci id="S5.E2.m1.1.1.3.2.2.2.2.cmml" xref="S5.E2.m1.1.1.3.2.2.2.2">𝑇</ci><ci id="S5.E2.m1.1.1.3.2.2.2.3.cmml" xref="S5.E2.m1.1.1.3.2.2.2.3">𝑃</ci></apply><apply id="S5.E2.m1.1.1.3.2.2.3.cmml" xref="S5.E2.m1.1.1.3.2.2.3"><times id="S5.E2.m1.1.1.3.2.2.3.1.cmml" xref="S5.E2.m1.1.1.3.2.2.3.1"></times><ci id="S5.E2.m1.1.1.3.2.2.3.2.cmml" xref="S5.E2.m1.1.1.3.2.2.3.2">𝑇</ci><ci id="S5.E2.m1.1.1.3.2.2.3.3.cmml" xref="S5.E2.m1.1.1.3.2.2.3.3">𝑁</ci></apply></apply><apply id="S5.E2.m1.1.1.3.2.3.cmml" xref="S5.E2.m1.1.1.3.2.3"><plus id="S5.E2.m1.1.1.3.2.3.1.cmml" xref="S5.E2.m1.1.1.3.2.3.1"></plus><apply id="S5.E2.m1.1.1.3.2.3.2.cmml" xref="S5.E2.m1.1.1.3.2.3.2"><times id="S5.E2.m1.1.1.3.2.3.2.1.cmml" xref="S5.E2.m1.1.1.3.2.3.2.1"></times><ci id="S5.E2.m1.1.1.3.2.3.2.2.cmml" xref="S5.E2.m1.1.1.3.2.3.2.2">𝑇</ci><ci id="S5.E2.m1.1.1.3.2.3.2.3.cmml" xref="S5.E2.m1.1.1.3.2.3.2.3">𝑃</ci></apply><apply id="S5.E2.m1.1.1.3.2.3.3.cmml" xref="S5.E2.m1.1.1.3.2.3.3"><times id="S5.E2.m1.1.1.3.2.3.3.1.cmml" xref="S5.E2.m1.1.1.3.2.3.3.1"></times><ci id="S5.E2.m1.1.1.3.2.3.3.2.cmml" xref="S5.E2.m1.1.1.3.2.3.3.2">𝑇</ci><ci id="S5.E2.m1.1.1.3.2.3.3.3.cmml" xref="S5.E2.m1.1.1.3.2.3.3.3">𝑁</ci></apply><apply id="S5.E2.m1.1.1.3.2.3.4.cmml" xref="S5.E2.m1.1.1.3.2.3.4"><times id="S5.E2.m1.1.1.3.2.3.4.1.cmml" xref="S5.E2.m1.1.1.3.2.3.4.1"></times><ci id="S5.E2.m1.1.1.3.2.3.4.2.cmml" xref="S5.E2.m1.1.1.3.2.3.4.2">𝐹</ci><ci id="S5.E2.m1.1.1.3.2.3.4.3.cmml" xref="S5.E2.m1.1.1.3.2.3.4.3">𝑁</ci></apply><apply id="S5.E2.m1.1.1.3.2.3.5.cmml" xref="S5.E2.m1.1.1.3.2.3.5"><times id="S5.E2.m1.1.1.3.2.3.5.1.cmml" xref="S5.E2.m1.1.1.3.2.3.5.1"></times><ci id="S5.E2.m1.1.1.3.2.3.5.2.cmml" xref="S5.E2.m1.1.1.3.2.3.5.2">𝐹</ci><ci id="S5.E2.m1.1.1.3.2.3.5.3.cmml" xref="S5.E2.m1.1.1.3.2.3.5.3">𝑃</ci></apply></apply></apply><apply id="S5.E2.m1.1.1.3.3.cmml" xref="S5.E2.m1.1.1.3.3"><csymbol cd="latexml" id="S5.E2.m1.1.1.3.3.1.cmml" xref="S5.E2.m1.1.1.3.3.1">percent</csymbol><cn type="integer" id="S5.E2.m1.1.1.3.3.2.cmml" xref="S5.E2.m1.1.1.3.3.2">100</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">Accuracy=\frac{TP+TN}{TP+TN+FN+FP}*100\%</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS1.p6" class="ltx_para ltx_noindent">
<p id="S5.SS1.p6.4" class="ltx_p">where <math id="S5.SS1.p6.1.m1.1" class="ltx_Math" alttext="TP" display="inline"><semantics id="S5.SS1.p6.1.m1.1a"><mrow id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml"><mi id="S5.SS1.p6.1.m1.1.1.2" xref="S5.SS1.p6.1.m1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.3" xref="S5.SS1.p6.1.m1.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><apply id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1"><times id="S5.SS1.p6.1.m1.1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1.1"></times><ci id="S5.SS1.p6.1.m1.1.1.2.cmml" xref="S5.SS1.p6.1.m1.1.1.2">𝑇</ci><ci id="S5.SS1.p6.1.m1.1.1.3.cmml" xref="S5.SS1.p6.1.m1.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">TP</annotation></semantics></math>, <math id="S5.SS1.p6.2.m2.1" class="ltx_Math" alttext="TN" display="inline"><semantics id="S5.SS1.p6.2.m2.1a"><mrow id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml"><mi id="S5.SS1.p6.2.m2.1.1.2" xref="S5.SS1.p6.2.m2.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.3" xref="S5.SS1.p6.2.m2.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.1b"><apply id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1"><times id="S5.SS1.p6.2.m2.1.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1.1"></times><ci id="S5.SS1.p6.2.m2.1.1.2.cmml" xref="S5.SS1.p6.2.m2.1.1.2">𝑇</ci><ci id="S5.SS1.p6.2.m2.1.1.3.cmml" xref="S5.SS1.p6.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.1c">TN</annotation></semantics></math>,<math id="S5.SS1.p6.3.m3.1" class="ltx_Math" alttext="FP" display="inline"><semantics id="S5.SS1.p6.3.m3.1a"><mrow id="S5.SS1.p6.3.m3.1.1" xref="S5.SS1.p6.3.m3.1.1.cmml"><mi id="S5.SS1.p6.3.m3.1.1.2" xref="S5.SS1.p6.3.m3.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.3.m3.1.1.1" xref="S5.SS1.p6.3.m3.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.3.m3.1.1.3" xref="S5.SS1.p6.3.m3.1.1.3.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.3.m3.1b"><apply id="S5.SS1.p6.3.m3.1.1.cmml" xref="S5.SS1.p6.3.m3.1.1"><times id="S5.SS1.p6.3.m3.1.1.1.cmml" xref="S5.SS1.p6.3.m3.1.1.1"></times><ci id="S5.SS1.p6.3.m3.1.1.2.cmml" xref="S5.SS1.p6.3.m3.1.1.2">𝐹</ci><ci id="S5.SS1.p6.3.m3.1.1.3.cmml" xref="S5.SS1.p6.3.m3.1.1.3">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.3.m3.1c">FP</annotation></semantics></math>, and <math id="S5.SS1.p6.4.m4.1" class="ltx_Math" alttext="FN" display="inline"><semantics id="S5.SS1.p6.4.m4.1a"><mrow id="S5.SS1.p6.4.m4.1.1" xref="S5.SS1.p6.4.m4.1.1.cmml"><mi id="S5.SS1.p6.4.m4.1.1.2" xref="S5.SS1.p6.4.m4.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.4.m4.1.1.1" xref="S5.SS1.p6.4.m4.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.4.m4.1.1.3" xref="S5.SS1.p6.4.m4.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.4.m4.1b"><apply id="S5.SS1.p6.4.m4.1.1.cmml" xref="S5.SS1.p6.4.m4.1.1"><times id="S5.SS1.p6.4.m4.1.1.1.cmml" xref="S5.SS1.p6.4.m4.1.1.1"></times><ci id="S5.SS1.p6.4.m4.1.1.2.cmml" xref="S5.SS1.p6.4.m4.1.1.2">𝐹</ci><ci id="S5.SS1.p6.4.m4.1.1.3.cmml" xref="S5.SS1.p6.4.m4.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.4.m4.1c">FN</annotation></semantics></math> are the True Positives, True Negatives, False Positives and False Negatives respectively.</p>
<table id="S5.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E3.m1.2" class="ltx_Math" alttext="Loss=-\sum_{i=1}^{N}y_{i}.\log\hat{y}_{i}" display="block"><semantics id="S5.E3.m1.2a"><mrow id="S5.E3.m1.2.2.2" xref="S5.E3.m1.2.2.3.cmml"><mrow id="S5.E3.m1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.cmml"><mrow id="S5.E3.m1.1.1.1.1.2" xref="S5.E3.m1.1.1.1.1.2.cmml"><mi id="S5.E3.m1.1.1.1.1.2.2" xref="S5.E3.m1.1.1.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.1.1.2.1" xref="S5.E3.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E3.m1.1.1.1.1.2.3" xref="S5.E3.m1.1.1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.1.1.2.1a" xref="S5.E3.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E3.m1.1.1.1.1.2.4" xref="S5.E3.m1.1.1.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.E3.m1.1.1.1.1.2.1b" xref="S5.E3.m1.1.1.1.1.2.1.cmml">​</mo><mi id="S5.E3.m1.1.1.1.1.2.5" xref="S5.E3.m1.1.1.1.1.2.5.cmml">s</mi></mrow><mo id="S5.E3.m1.1.1.1.1.1" xref="S5.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S5.E3.m1.1.1.1.1.3" xref="S5.E3.m1.1.1.1.1.3.cmml"><mo id="S5.E3.m1.1.1.1.1.3a" xref="S5.E3.m1.1.1.1.1.3.cmml">−</mo><mrow id="S5.E3.m1.1.1.1.1.3.2" xref="S5.E3.m1.1.1.1.1.3.2.cmml"><munderover id="S5.E3.m1.1.1.1.1.3.2.1" xref="S5.E3.m1.1.1.1.1.3.2.1.cmml"><mo movablelimits="false" id="S5.E3.m1.1.1.1.1.3.2.1.2.2" xref="S5.E3.m1.1.1.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S5.E3.m1.1.1.1.1.3.2.1.2.3" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.cmml"><mi id="S5.E3.m1.1.1.1.1.3.2.1.2.3.2" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.2.cmml">i</mi><mo id="S5.E3.m1.1.1.1.1.3.2.1.2.3.1" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S5.E3.m1.1.1.1.1.3.2.1.2.3.3" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="S5.E3.m1.1.1.1.1.3.2.1.3" xref="S5.E3.m1.1.1.1.1.3.2.1.3.cmml">N</mi></munderover><msub id="S5.E3.m1.1.1.1.1.3.2.2" xref="S5.E3.m1.1.1.1.1.3.2.2.cmml"><mi id="S5.E3.m1.1.1.1.1.3.2.2.2" xref="S5.E3.m1.1.1.1.1.3.2.2.2.cmml">y</mi><mi id="S5.E3.m1.1.1.1.1.3.2.2.3" xref="S5.E3.m1.1.1.1.1.3.2.2.3.cmml">i</mi></msub></mrow></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S5.E3.m1.2.2.2.3" xref="S5.E3.m1.2.2.3a.cmml">.</mo><mrow id="S5.E3.m1.2.2.2.2" xref="S5.E3.m1.2.2.2.2.cmml"><mi id="S5.E3.m1.2.2.2.2.1" xref="S5.E3.m1.2.2.2.2.1.cmml">log</mi><mo lspace="0.167em" id="S5.E3.m1.2.2.2.2a" xref="S5.E3.m1.2.2.2.2.cmml">⁡</mo><msub id="S5.E3.m1.2.2.2.2.2" xref="S5.E3.m1.2.2.2.2.2.cmml"><mover accent="true" id="S5.E3.m1.2.2.2.2.2.2" xref="S5.E3.m1.2.2.2.2.2.2.cmml"><mi id="S5.E3.m1.2.2.2.2.2.2.2" xref="S5.E3.m1.2.2.2.2.2.2.2.cmml">y</mi><mo id="S5.E3.m1.2.2.2.2.2.2.1" xref="S5.E3.m1.2.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S5.E3.m1.2.2.2.2.2.3" xref="S5.E3.m1.2.2.2.2.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E3.m1.2b"><apply id="S5.E3.m1.2.2.3.cmml" xref="S5.E3.m1.2.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.3a.cmml" xref="S5.E3.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S5.E3.m1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1"><eq id="S5.E3.m1.1.1.1.1.1.cmml" xref="S5.E3.m1.1.1.1.1.1"></eq><apply id="S5.E3.m1.1.1.1.1.2.cmml" xref="S5.E3.m1.1.1.1.1.2"><times id="S5.E3.m1.1.1.1.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.2.1"></times><ci id="S5.E3.m1.1.1.1.1.2.2.cmml" xref="S5.E3.m1.1.1.1.1.2.2">𝐿</ci><ci id="S5.E3.m1.1.1.1.1.2.3.cmml" xref="S5.E3.m1.1.1.1.1.2.3">𝑜</ci><ci id="S5.E3.m1.1.1.1.1.2.4.cmml" xref="S5.E3.m1.1.1.1.1.2.4">𝑠</ci><ci id="S5.E3.m1.1.1.1.1.2.5.cmml" xref="S5.E3.m1.1.1.1.1.2.5">𝑠</ci></apply><apply id="S5.E3.m1.1.1.1.1.3.cmml" xref="S5.E3.m1.1.1.1.1.3"><minus id="S5.E3.m1.1.1.1.1.3.1.cmml" xref="S5.E3.m1.1.1.1.1.3"></minus><apply id="S5.E3.m1.1.1.1.1.3.2.cmml" xref="S5.E3.m1.1.1.1.1.3.2"><apply id="S5.E3.m1.1.1.1.1.3.2.1.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.3.2.1.1.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1">superscript</csymbol><apply id="S5.E3.m1.1.1.1.1.3.2.1.2.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.3.2.1.2.1.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1">subscript</csymbol><sum id="S5.E3.m1.1.1.1.1.3.2.1.2.2.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1.2.2"></sum><apply id="S5.E3.m1.1.1.1.1.3.2.1.2.3.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3"><eq id="S5.E3.m1.1.1.1.1.3.2.1.2.3.1.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.1"></eq><ci id="S5.E3.m1.1.1.1.1.3.2.1.2.3.2.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.2">𝑖</ci><cn type="integer" id="S5.E3.m1.1.1.1.1.3.2.1.2.3.3.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="S5.E3.m1.1.1.1.1.3.2.1.3.cmml" xref="S5.E3.m1.1.1.1.1.3.2.1.3">𝑁</ci></apply><apply id="S5.E3.m1.1.1.1.1.3.2.2.cmml" xref="S5.E3.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.1.1.1.1.3.2.2.1.cmml" xref="S5.E3.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S5.E3.m1.1.1.1.1.3.2.2.2.cmml" xref="S5.E3.m1.1.1.1.1.3.2.2.2">𝑦</ci><ci id="S5.E3.m1.1.1.1.1.3.2.2.3.cmml" xref="S5.E3.m1.1.1.1.1.3.2.2.3">𝑖</ci></apply></apply></apply></apply><apply id="S5.E3.m1.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2"><log id="S5.E3.m1.2.2.2.2.1.cmml" xref="S5.E3.m1.2.2.2.2.1"></log><apply id="S5.E3.m1.2.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S5.E3.m1.2.2.2.2.2.1.cmml" xref="S5.E3.m1.2.2.2.2.2">subscript</csymbol><apply id="S5.E3.m1.2.2.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2.2.2"><ci id="S5.E3.m1.2.2.2.2.2.2.1.cmml" xref="S5.E3.m1.2.2.2.2.2.2.1">^</ci><ci id="S5.E3.m1.2.2.2.2.2.2.2.cmml" xref="S5.E3.m1.2.2.2.2.2.2.2">𝑦</ci></apply><ci id="S5.E3.m1.2.2.2.2.2.3.cmml" xref="S5.E3.m1.2.2.2.2.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E3.m1.2c">Loss=-\sum_{i=1}^{N}y_{i}.\log\hat{y}_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S5.SS1.p7" class="ltx_para ltx_noindent">
<p id="S5.SS1.p7.2" class="ltx_p">where <math id="S5.SS1.p7.1.m1.1" class="ltx_Math" alttext="\hat{y}_{i}" display="inline"><semantics id="S5.SS1.p7.1.m1.1a"><msub id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml"><mover accent="true" id="S5.SS1.p7.1.m1.1.1.2" xref="S5.SS1.p7.1.m1.1.1.2.cmml"><mi id="S5.SS1.p7.1.m1.1.1.2.2" xref="S5.SS1.p7.1.m1.1.1.2.2.cmml">y</mi><mo id="S5.SS1.p7.1.m1.1.1.2.1" xref="S5.SS1.p7.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S5.SS1.p7.1.m1.1.1.3" xref="S5.SS1.p7.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.1b"><apply id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p7.1.m1.1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">subscript</csymbol><apply id="S5.SS1.p7.1.m1.1.1.2.cmml" xref="S5.SS1.p7.1.m1.1.1.2"><ci id="S5.SS1.p7.1.m1.1.1.2.1.cmml" xref="S5.SS1.p7.1.m1.1.1.2.1">^</ci><ci id="S5.SS1.p7.1.m1.1.1.2.2.cmml" xref="S5.SS1.p7.1.m1.1.1.2.2">𝑦</ci></apply><ci id="S5.SS1.p7.1.m1.1.1.3.cmml" xref="S5.SS1.p7.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.1c">\hat{y}_{i}</annotation></semantics></math> is the model prediction for <span id="S5.SS1.p7.2.1" class="ltx_text ltx_font_italic">i-th</span> pattern, <math id="S5.SS1.p7.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S5.SS1.p7.2.m2.1a"><msub id="S5.SS1.p7.2.m2.1.1" xref="S5.SS1.p7.2.m2.1.1.cmml"><mi id="S5.SS1.p7.2.m2.1.1.2" xref="S5.SS1.p7.2.m2.1.1.2.cmml">y</mi><mi id="S5.SS1.p7.2.m2.1.1.3" xref="S5.SS1.p7.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.2.m2.1b"><apply id="S5.SS1.p7.2.m2.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p7.2.m2.1.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1">subscript</csymbol><ci id="S5.SS1.p7.2.m2.1.1.2.cmml" xref="S5.SS1.p7.2.m2.1.1.2">𝑦</ci><ci id="S5.SS1.p7.2.m2.1.1.3.cmml" xref="S5.SS1.p7.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.2.m2.1c">y_{i}</annotation></semantics></math> represent the corresponding real value, and <span id="S5.SS1.p7.2.2" class="ltx_text ltx_font_italic">N</span> is the total number of samples.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results and discussion</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">The results are organized to evaluate various aspects of our approach: model performance, the impact of scaling the number of the edge nodes (clients) on the model performance, the possible minimal size of data exchange through the network (network workload), the trained layers distribution, and finally, the resources utilization efficiency using both VMs and actual device (Jetson Nano).</p>
</div>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Model performance</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Our goal is to attain accuracy similar to conventional federated learning methods while implementing a new strategy that maximizes the utilization of edge resources. This approach not only upholds data privacy, which is crucial in any federated training environment but also minimizes training costs and reduces data transfer. To demonstrate the effectiveness of this strategy, we conducted a series of experiments where we trained different numbers of randomly selected model layers. Through these experiments, we systematically increased the number of layers to assess their impact on the overall performance of the model.
To compare the accuracy of centrally trained and FL global models, we varied the number of trainable layers for the FL settings. Figure <a href="#S5.F1" title="Figure 1 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the accuracy of the VGG16 model trained on the CIFAR-10 dataset, both centrally and in FL settings (10 clients and the data split partition was equally across 10 clients), with different trainable layers randomly selected per round.</p>
</div>
<figure id="S5.F1" class="ltx_figure"><img src="/html/2309.10367/assets/figures/model_accuracy.png" id="S5.F1.g1" class="ltx_graphics ltx_img_landscape" width="354" height="178" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>VGG16 model accuracy for CIFAR-10 dataset using different numbers of trainable layers.</figcaption>
</figure>
<div id="S5.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p2.3" class="ltx_p">The centralized model achieved an accuracy of <math id="S5.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="87.00\%" display="inline"><semantics id="S5.SS2.SSS1.p2.1.m1.1a"><mrow id="S5.SS2.SSS1.p2.1.m1.1.1" xref="S5.SS2.SSS1.p2.1.m1.1.1.cmml"><mn id="S5.SS2.SSS1.p2.1.m1.1.1.2" xref="S5.SS2.SSS1.p2.1.m1.1.1.2.cmml">87.00</mn><mo id="S5.SS2.SSS1.p2.1.m1.1.1.1" xref="S5.SS2.SSS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.1.m1.1b"><apply id="S5.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.SSS1.p2.1.m1.1.1.2.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1.2">87.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.1.m1.1c">87.00\%</annotation></semantics></math>, while the FL model, with all layers included in the training process, achieved a slightly lower accuracy of <math id="S5.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="86.08\%" display="inline"><semantics id="S5.SS2.SSS1.p2.2.m2.1a"><mrow id="S5.SS2.SSS1.p2.2.m2.1.1" xref="S5.SS2.SSS1.p2.2.m2.1.1.cmml"><mn id="S5.SS2.SSS1.p2.2.m2.1.1.2" xref="S5.SS2.SSS1.p2.2.m2.1.1.2.cmml">86.08</mn><mo id="S5.SS2.SSS1.p2.2.m2.1.1.1" xref="S5.SS2.SSS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.2.m2.1b"><apply id="S5.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p2.2.m2.1.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.SSS1.p2.2.m2.1.1.2.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1.2">86.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.2.m2.1c">86.08\%</annotation></semantics></math>, a difference of only <math id="S5.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="0.92\%" display="inline"><semantics id="S5.SS2.SSS1.p2.3.m3.1a"><mrow id="S5.SS2.SSS1.p2.3.m3.1.1" xref="S5.SS2.SSS1.p2.3.m3.1.1.cmml"><mn id="S5.SS2.SSS1.p2.3.m3.1.1.2" xref="S5.SS2.SSS1.p2.3.m3.1.1.2.cmml">0.92</mn><mo id="S5.SS2.SSS1.p2.3.m3.1.1.1" xref="S5.SS2.SSS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.3.m3.1b"><apply id="S5.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p2.3.m3.1.1.1.cmml" xref="S5.SS2.SSS1.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.SSS1.p2.3.m3.1.1.2.cmml" xref="S5.SS2.SSS1.p2.3.m3.1.1.2">0.92</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.3.m3.1c">0.92\%</annotation></semantics></math>. However, FL with slightly lower accuracy offers a privacy-preserving training environment.</p>
</div>
<div id="S5.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p3.3" class="ltx_p">Our experiments revealed that training <math id="S5.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS2.SSS1.p3.1.m1.1a"><mn id="S5.SS2.SSS1.p3.1.m1.1.1" xref="S5.SS2.SSS1.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p3.1.m1.1b"><cn type="integer" id="S5.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p3.1.m1.1c">10</annotation></semantics></math> randomly selected layers in each round of the model was enough to capture and learn the data behaviour, resulting around 85.70% in accuracy. Figure <a href="#S5.F1" title="Figure 1 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> indicates that the model began to converge from the first round of training. As we reduced the number of trained layers to approximately <math id="S5.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS2.SSS1.p3.2.m2.1a"><mrow id="S5.SS2.SSS1.p3.2.m2.1.1" xref="S5.SS2.SSS1.p3.2.m2.1.1.cmml"><mn id="S5.SS2.SSS1.p3.2.m2.1.1.2" xref="S5.SS2.SSS1.p3.2.m2.1.1.2.cmml">50</mn><mo id="S5.SS2.SSS1.p3.2.m2.1.1.1" xref="S5.SS2.SSS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p3.2.m2.1b"><apply id="S5.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p3.2.m2.1.1.1.cmml" xref="S5.SS2.SSS1.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS1.p3.2.m2.1.1.2.cmml" xref="S5.SS2.SSS1.p3.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p3.2.m2.1c">50\%</annotation></semantics></math> (<math id="S5.SS2.SSS1.p3.3.m3.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.SS2.SSS1.p3.3.m3.1a"><mn id="S5.SS2.SSS1.p3.3.m3.1.1" xref="S5.SS2.SSS1.p3.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p3.3.m3.1b"><cn type="integer" id="S5.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p3.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p3.3.m3.1c">7</annotation></semantics></math> layers) of the model, the accuracy gap increased. Nevertheless, the accuracy remained 84.79% compared to the baseline, given the reduction in cost achieved through this approach, as elaborated in the subsequent sections.</p>
</div>
<div id="S5.SS2.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p4.3" class="ltx_p">We also observed oscillations in the model’s performance at the outset of training with only <math id="S5.SS2.SSS1.p4.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S5.SS2.SSS1.p4.1.m1.1a"><mn id="S5.SS2.SSS1.p4.1.m1.1.1" xref="S5.SS2.SSS1.p4.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p4.1.m1.1b"><cn type="integer" id="S5.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p4.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p4.1.m1.1c">4</annotation></semantics></math> layers. The model faced difficulties in capturing the complete data behavior using the small number of layers in each round.
However, the model eventually achieves an accuracy of <math id="S5.SS2.SSS1.p4.2.m2.1" class="ltx_Math" alttext="79.00\%" display="inline"><semantics id="S5.SS2.SSS1.p4.2.m2.1a"><mrow id="S5.SS2.SSS1.p4.2.m2.1.1" xref="S5.SS2.SSS1.p4.2.m2.1.1.cmml"><mn id="S5.SS2.SSS1.p4.2.m2.1.1.2" xref="S5.SS2.SSS1.p4.2.m2.1.1.2.cmml">79.00</mn><mo id="S5.SS2.SSS1.p4.2.m2.1.1.1" xref="S5.SS2.SSS1.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p4.2.m2.1b"><apply id="S5.SS2.SSS1.p4.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p4.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p4.2.m2.1.1.1.cmml" xref="S5.SS2.SSS1.p4.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.SSS1.p4.2.m2.1.1.2.cmml" xref="S5.SS2.SSS1.p4.2.m2.1.1.2">79.00</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p4.2.m2.1c">79.00\%</annotation></semantics></math> in the later stages, which is approximately <math id="S5.SS2.SSS1.p4.3.m3.1" class="ltx_Math" alttext="7\%" display="inline"><semantics id="S5.SS2.SSS1.p4.3.m3.1a"><mrow id="S5.SS2.SSS1.p4.3.m3.1.1" xref="S5.SS2.SSS1.p4.3.m3.1.1.cmml"><mn id="S5.SS2.SSS1.p4.3.m3.1.1.2" xref="S5.SS2.SSS1.p4.3.m3.1.1.2.cmml">7</mn><mo id="S5.SS2.SSS1.p4.3.m3.1.1.1" xref="S5.SS2.SSS1.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p4.3.m3.1b"><apply id="S5.SS2.SSS1.p4.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p4.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p4.3.m3.1.1.1.cmml" xref="S5.SS2.SSS1.p4.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS1.p4.3.m3.1.1.2.cmml" xref="S5.SS2.SSS1.p4.3.m3.1.1.2">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p4.3.m3.1c">7\%</annotation></semantics></math> lower than the baseline accuracy achieved through conventional federated training.</p>
</div>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.10367/assets/figures/casa_acc.png" id="S5.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="354" height="179" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Model accuracy trained over CASA dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.10367/assets/figures/imdb_acc.png" id="S5.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="354" height="175" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Model accuracy trained over IMDB dataset</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Evaluating two different DL architectures to perform distinct tasks in terms of accuracy (a) human activity recognition task using CASA dataset. (b) Sentiment analysis task using IMDB dataset</figcaption>
</figure>
<div id="S5.SS2.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p5.5" class="ltx_p">To highlight the advantages of our proposed approach, we evaluated its adaptability and robustness across two distinct domains, utilizing both CASA and IMDB datasets in federated learning settings, involving 10 clients, and equal subset of data amount split from the original dataset. Figure <a href="#S5.F2" title="Figure 2 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates a convergence in terms of model performance for both datasets. As depicted in Figure <a href="#S5.F2" title="Figure 2 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>-a, for the HAR task, training only <math id="S5.SS2.SSS1.p5.1.m1.1" class="ltx_Math" alttext="33\%" display="inline"><semantics id="S5.SS2.SSS1.p5.1.m1.1a"><mrow id="S5.SS2.SSS1.p5.1.m1.1.1" xref="S5.SS2.SSS1.p5.1.m1.1.1.cmml"><mn id="S5.SS2.SSS1.p5.1.m1.1.1.2" xref="S5.SS2.SSS1.p5.1.m1.1.1.2.cmml">33</mn><mo id="S5.SS2.SSS1.p5.1.m1.1.1.1" xref="S5.SS2.SSS1.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.1.m1.1b"><apply id="S5.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p5.1.m1.1.1.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS1.p5.1.m1.1.1.2.cmml" xref="S5.SS2.SSS1.p5.1.m1.1.1.2">33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.1.m1.1c">33\%</annotation></semantics></math> (<math id="S5.SS2.SSS1.p5.2.m2.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S5.SS2.SSS1.p5.2.m2.1a"><mn id="S5.SS2.SSS1.p5.2.m2.1.1" xref="S5.SS2.SSS1.p5.2.m2.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.2.m2.1b"><cn type="integer" id="S5.SS2.SSS1.p5.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p5.2.m2.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.2.m2.1c">2</annotation></semantics></math> randomly selected layers every round) of the model resulted in good accuracy (around 79.01%) compared to the fully trained (<math id="S5.SS2.SSS1.p5.3.m3.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S5.SS2.SSS1.p5.3.m3.1a"><mn id="S5.SS2.SSS1.p5.3.m3.1.1" xref="S5.SS2.SSS1.p5.3.m3.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.3.m3.1b"><cn type="integer" id="S5.SS2.SSS1.p5.3.m3.1.1.cmml" xref="S5.SS2.SSS1.p5.3.m3.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.3.m3.1c">6</annotation></semantics></math> layers, 80.20%) model with a small gap. In contrast, training <math id="S5.SS2.SSS1.p5.4.m4.1" class="ltx_Math" alttext="66\%" display="inline"><semantics id="S5.SS2.SSS1.p5.4.m4.1a"><mrow id="S5.SS2.SSS1.p5.4.m4.1.1" xref="S5.SS2.SSS1.p5.4.m4.1.1.cmml"><mn id="S5.SS2.SSS1.p5.4.m4.1.1.2" xref="S5.SS2.SSS1.p5.4.m4.1.1.2.cmml">66</mn><mo id="S5.SS2.SSS1.p5.4.m4.1.1.1" xref="S5.SS2.SSS1.p5.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.4.m4.1b"><apply id="S5.SS2.SSS1.p5.4.m4.1.1.cmml" xref="S5.SS2.SSS1.p5.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p5.4.m4.1.1.1.cmml" xref="S5.SS2.SSS1.p5.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS1.p5.4.m4.1.1.2.cmml" xref="S5.SS2.SSS1.p5.4.m4.1.1.2">66</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.4.m4.1c">66\%</annotation></semantics></math> (<math id="S5.SS2.SSS1.p5.5.m5.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S5.SS2.SSS1.p5.5.m5.1a"><mn id="S5.SS2.SSS1.p5.5.m5.1.1" xref="S5.SS2.SSS1.p5.5.m5.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.5.m5.1b"><cn type="integer" id="S5.SS2.SSS1.p5.5.m5.1.1.cmml" xref="S5.SS2.SSS1.p5.5.m5.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.5.m5.1c">4</annotation></semantics></math> layers) of the model nearly achieved the same level of accuracy as training all the model layers. Interestingly, similar trends were observed in the NLP experiments, as demonstrated in Figure <a href="#S5.F2" title="Figure 2 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>-b.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Trainable layer distribution</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p1.6" class="ltx_p">As the number of clients participating in a training round increases, the probability of engaging all layers of a model in the global training process also rises. Simultaneously, the volume of training data expands. This heightened participation and increased training data elevate the likelihood of training all model layers over each client’s data as multiple rounds progress. Consequently, this phenomenon significantly influences the model’s convergence.This effect is particularly noticeable when we randomly select different trainable layers of the model (<math id="S5.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="25\%" display="inline"><semantics id="S5.SS2.SSS2.p1.1.m1.1a"><mrow id="S5.SS2.SSS2.p1.1.m1.1.1" xref="S5.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S5.SS2.SSS2.p1.1.m1.1.1.2" xref="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml">25</mn><mo id="S5.SS2.SSS2.p1.1.m1.1.1.1" xref="S5.SS2.SSS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.1.m1.1b"><apply id="S5.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.1.m1.1c">25\%</annotation></semantics></math>, <math id="S5.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS2.SSS2.p1.2.m2.1a"><mrow id="S5.SS2.SSS2.p1.2.m2.1.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.cmml"><mn id="S5.SS2.SSS2.p1.2.m2.1.1.2" xref="S5.SS2.SSS2.p1.2.m2.1.1.2.cmml">50</mn><mo id="S5.SS2.SSS2.p1.2.m2.1.1.1" xref="S5.SS2.SSS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.2.m2.1b"><apply id="S5.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.SSS2.p1.2.m2.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.2.m2.1c">50\%</annotation></semantics></math>, and <math id="S5.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="75\%" display="inline"><semantics id="S5.SS2.SSS2.p1.3.m3.1a"><mrow id="S5.SS2.SSS2.p1.3.m3.1.1" xref="S5.SS2.SSS2.p1.3.m3.1.1.cmml"><mn id="S5.SS2.SSS2.p1.3.m3.1.1.2" xref="S5.SS2.SSS2.p1.3.m3.1.1.2.cmml">75</mn><mo id="S5.SS2.SSS2.p1.3.m3.1.1.1" xref="S5.SS2.SSS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.3.m3.1b"><apply id="S5.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.SSS2.p1.3.m3.1.1.2">75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.3.m3.1c">75\%</annotation></semantics></math>) for training. During the training process, we noticed that each client has the opportunity to train every layer of the model at least once. Moreover, the distribution of layers among all clients is equitable, ensuring a balanced contribution to the training process
, as demonstrated in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.2.2 Trainable layer distribution ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The equal distribution of model layers has been used across all client’s with different training settings (i.e. <math id="S5.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S5.SS2.SSS2.p1.4.m4.1a"><mn id="S5.SS2.SSS2.p1.4.m4.1.1" xref="S5.SS2.SSS2.p1.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.4.m4.1b"><cn type="integer" id="S5.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S5.SS2.SSS2.p1.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.4.m4.1c">4</annotation></semantics></math>, <math id="S5.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.SS2.SSS2.p1.5.m5.1a"><mn id="S5.SS2.SSS2.p1.5.m5.1.1" xref="S5.SS2.SSS2.p1.5.m5.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.5.m5.1b"><cn type="integer" id="S5.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S5.SS2.SSS2.p1.5.m5.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.5.m5.1c">7</annotation></semantics></math>, and <math id="S5.SS2.SSS2.p1.6.m6.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS2.SSS2.p1.6.m6.1a"><mn id="S5.SS2.SSS2.p1.6.m6.1.1" xref="S5.SS2.SSS2.p1.6.m6.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.6.m6.1b"><cn type="integer" id="S5.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S5.SS2.SSS2.p1.6.m6.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.6.m6.1c">10</annotation></semantics></math> layers).</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.10367/assets/figures/4l_Distribution.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="236" height="164" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Train 25% (4 Layers) of the model </figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.10367/assets/figures/7l_Distribution.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="236" height="165" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Train 50% (7 Layers) of the model </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.10367/assets/figures/10l_Distribution.png" id="S5.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="236" height="165" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Train 75% (10 Layers) of the model </figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>VGG16 layers distribution across 10 clients during 100 training rounds using different parts of the model</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.3 </span>The impact of scaling the number of clients (edge nodes) on the model accuracy</h4>

<div id="S5.SS2.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p1.1" class="ltx_p">We conducted two more experiments to estimate the impact by using different settings, such as the number of clients, and the number of trainable layers.These experiments aimed to determine how these factors affect model accuracy while keeping the amount of data fixed.</p>
</div>
<div id="S5.SS2.SSS3.p2" class="ltx_para ltx_noindent">
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i1.p1.2" class="ltx_p"><span id="S5.I2.i1.p1.2.1" class="ltx_text ltx_font_bold">Exp. 1</span>: In this experiment, we partitioned the dataset among <math id="S5.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.I2.i1.p1.1.m1.1a"><mn id="S5.I2.i1.p1.1.m1.1.1" xref="S5.I2.i1.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.1.m1.1b"><cn type="integer" id="S5.I2.i1.p1.1.m1.1.1.cmml" xref="S5.I2.i1.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.1.m1.1c">10</annotation></semantics></math> distinct clients to train the entire model, which consisted of <math id="S5.I2.i1.p1.2.m2.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.I2.i1.p1.2.m2.1a"><mn id="S5.I2.i1.p1.2.m2.1.1" xref="S5.I2.i1.p1.2.m2.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i1.p1.2.m2.1b"><cn type="integer" id="S5.I2.i1.p1.2.m2.1.1.cmml" xref="S5.I2.i1.p1.2.m2.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i1.p1.2.m2.1c">14</annotation></semantics></math> trainable layers, within the context of federated learning settings.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i2.p1.3" class="ltx_p"><span id="S5.I2.i2.p1.3.1" class="ltx_text ltx_font_bold">Exp. 2</span>: In this particular experiment, we divided the dataset into <math id="S5.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.I2.i2.p1.1.m1.1a"><mn id="S5.I2.i2.p1.1.m1.1.1" xref="S5.I2.i2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.1.m1.1b"><cn type="integer" id="S5.I2.i2.p1.1.m1.1.1.cmml" xref="S5.I2.i2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.1.m1.1c">20</annotation></semantics></math> partitions, distributed across <math id="S5.I2.i2.p1.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.I2.i2.p1.2.m2.1a"><mn id="S5.I2.i2.p1.2.m2.1.1" xref="S5.I2.i2.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.2.m2.1b"><cn type="integer" id="S5.I2.i2.p1.2.m2.1.1.cmml" xref="S5.I2.i2.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.2.m2.1c">20</annotation></semantics></math> clients. The objective was to train <math id="S5.I2.i2.p1.3.m3.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.I2.i2.p1.3.m3.1a"><mn id="S5.I2.i2.p1.3.m3.1.1" xref="S5.I2.i2.p1.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.I2.i2.p1.3.m3.1b"><cn type="integer" id="S5.I2.i2.p1.3.m3.1.1.cmml" xref="S5.I2.i2.p1.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I2.i2.p1.3.m3.1c">7</annotation></semantics></math> trainable layers, which were randomly selected during each training round in the FL context. This corresponds to training half of the entire model in each round.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS2.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p3.1" class="ltx_p">Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2.3 The impact of scaling the number of clients (edge nodes) on the model accuracy ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the impact of scaling the number of clients, using different numbers of trainable layers of the VGG16 model while keeping the data amount fixed. According to the results, both experimental models demonstrated high accuracy (86.08% and 86.28%) in comparison to the centralized model’s accuracy, with only a minor difference of approximately 0.92% and 0.72%, respectively.</p>
</div>
<div id="S5.SS2.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p4.2" class="ltx_p">We found that using more clients (with fewer resources) and fewer trainable layers can achieve the same model performance as training the entire model with fewer nodes. This was particularly evident in the last <math id="S5.SS2.SSS3.p4.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.SS2.SSS3.p4.1.m1.1a"><mn id="S5.SS2.SSS3.p4.1.m1.1.1" xref="S5.SS2.SSS3.p4.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p4.1.m1.1b"><cn type="integer" id="S5.SS2.SSS3.p4.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p4.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p4.1.m1.1c">20</annotation></semantics></math> rounds, where double the number of nodes were used to train <math id="S5.SS2.SSS3.p4.2.m2.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.SS2.SSS3.p4.2.m2.1a"><mn id="S5.SS2.SSS3.p4.2.m2.1.1" xref="S5.SS2.SSS3.p4.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p4.2.m2.1b"><cn type="integer" id="S5.SS2.SSS3.p4.2.m2.1.1.cmml" xref="S5.SS2.SSS3.p4.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p4.2.m2.1c">7</annotation></semantics></math> layers. That leads to the conclusion, that with more clients, each layer had more opportunities to be trained at least once per communication round, resulting in better accuracy.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2309.10367/assets/figures/model_accuracy_double_clients.png" id="S5.F4.g1" class="ltx_graphics ltx_img_landscape" width="354" height="171" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Comparing the impact of reducing the number of trainable layers to half (<math id="S5.F4.5.m1.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.F4.5.m1.1b"><mn id="S5.F4.5.m1.1.1" xref="S5.F4.5.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.F4.5.m1.1c"><cn type="integer" id="S5.F4.5.m1.1.1.cmml" xref="S5.F4.5.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.5.m1.1d">7</annotation></semantics></math> layers) while scaling the number of clients (<math id="S5.F4.6.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.F4.6.m2.1b"><mn id="S5.F4.6.m2.1.1" xref="S5.F4.6.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.F4.6.m2.1c"><cn type="integer" id="S5.F4.6.m2.1.1.cmml" xref="S5.F4.6.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.6.m2.1d">20</annotation></semantics></math> layers) to double the number of clients (<math id="S5.F4.7.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.F4.7.m3.1b"><mn id="S5.F4.7.m3.1.1" xref="S5.F4.7.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.F4.7.m3.1c"><cn type="integer" id="S5.F4.7.m3.1.1.cmml" xref="S5.F4.7.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.7.m3.1d">10</annotation></semantics></math> layers) used to train the whole model (<math id="S5.F4.8.m4.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.F4.8.m4.1b"><mn id="S5.F4.8.m4.1.1" xref="S5.F4.8.m4.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.F4.8.m4.1c"><cn type="integer" id="S5.F4.8.m4.1.1.cmml" xref="S5.F4.8.m4.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.8.m4.1d">14</annotation></semantics></math> layers). This setting change was carried out while maintaining the same amount of CIFAR-10 data for both scenarios. The objective was to evaluate how these modifications influenced the global model’s accuracy.
</figcaption>
</figure>
<div id="S5.SS2.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p5.5" class="ltx_p">Another experiment has been conducted to evaluate how scaling the number of clients, data ratio and trainable layers affected model accuracy. As depicted in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.3 The impact of scaling the number of clients (edge nodes) on the model accuracy ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the bar chart demonstrates a consistent enhancement in model accuracy as the number of clients are increased across all training settings. Specifically, when training the model with seven layers, scaling the contributors from <math id="S5.SS2.SSS3.p5.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S5.SS2.SSS3.p5.1.m1.1a"><mn id="S5.SS2.SSS3.p5.1.m1.1.1" xref="S5.SS2.SSS3.p5.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p5.1.m1.1b"><cn type="integer" id="S5.SS2.SSS3.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS3.p5.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p5.1.m1.1c">5</annotation></semantics></math> to <math id="S5.SS2.SSS3.p5.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.SS2.SSS3.p5.2.m2.1a"><mn id="S5.SS2.SSS3.p5.2.m2.1.1" xref="S5.SS2.SSS3.p5.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p5.2.m2.1b"><cn type="integer" id="S5.SS2.SSS3.p5.2.m2.1.1.cmml" xref="S5.SS2.SSS3.p5.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p5.2.m2.1c">20</annotation></semantics></math>, and expanding the training data from a quarter to all the data, an accuracy gain of approximately <math id="S5.SS2.SSS3.p5.3.m3.1" class="ltx_Math" alttext="15\%" display="inline"><semantics id="S5.SS2.SSS3.p5.3.m3.1a"><mrow id="S5.SS2.SSS3.p5.3.m3.1.1" xref="S5.SS2.SSS3.p5.3.m3.1.1.cmml"><mn id="S5.SS2.SSS3.p5.3.m3.1.1.2" xref="S5.SS2.SSS3.p5.3.m3.1.1.2.cmml">15</mn><mo id="S5.SS2.SSS3.p5.3.m3.1.1.1" xref="S5.SS2.SSS3.p5.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p5.3.m3.1b"><apply id="S5.SS2.SSS3.p5.3.m3.1.1.cmml" xref="S5.SS2.SSS3.p5.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.SSS3.p5.3.m3.1.1.1.cmml" xref="S5.SS2.SSS3.p5.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S5.SS2.SSS3.p5.3.m3.1.1.2.cmml" xref="S5.SS2.SSS3.p5.3.m3.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p5.3.m3.1c">15\%</annotation></semantics></math> was observed. The same scenario was followed for <math id="S5.SS2.SSS3.p5.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.SS2.SSS3.p5.4.m4.1a"><mn id="S5.SS2.SSS3.p5.4.m4.1.1" xref="S5.SS2.SSS3.p5.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p5.4.m4.1b"><cn type="integer" id="S5.SS2.SSS3.p5.4.m4.1.1.cmml" xref="S5.SS2.SSS3.p5.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p5.4.m4.1c">10</annotation></semantics></math> and <math id="S5.SS2.SSS3.p5.5.m5.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.SS2.SSS3.p5.5.m5.1a"><mn id="S5.SS2.SSS3.p5.5.m5.1.1" xref="S5.SS2.SSS3.p5.5.m5.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS3.p5.5.m5.1b"><cn type="integer" id="S5.SS2.SSS3.p5.5.m5.1.1.cmml" xref="S5.SS2.SSS3.p5.5.m5.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS3.p5.5.m5.1c">14</annotation></semantics></math> trainable layers.</p>
</div>
<div id="S5.SS2.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p6.1" class="ltx_p">Training the model with different numbers of layers using varying numbers of clients had minimal impact on model performance, even with an increased amount of training data. Comparing the accuracy achieved using 20 clients to train 7 and 10 layers of the model revealed only a borderline difference of approximately 1% in accuracy, as shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.3 The impact of scaling the number of clients (edge nodes) on the model accuracy ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. A slightly more significant gap was observed when comparing the accuracy obtained by training 7 and 14 layers of the model using 20 clients, with a difference of around 2%.</p>
</div>
<div id="S5.SS2.SSS3.p7" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p7.1" class="ltx_p">These findings indicate that training a model with fewer layers can be a more resource-efficient alternative without significantly compromising accuracy. The advantage is that fewer layers requires less computational resources client side. This is particularly important while dealing with restricted devices or limited computational capabilities.</p>
</div>
<div id="S5.SS2.SSS3.p8" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p8.1" class="ltx_p">The results also highlight the scalability of the approach, as increasing the number of clients can compensate for the reduced model depth, resulting in comparable accuracy to training with more clients and fewer layers.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2309.10367/assets/figures/scale_clients_n.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="354" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Show the impact of scaling the number of clients, adjusting the number of model layers, and increasing the dataset size on the global model’s performance. Training approximately 75% of the model yielded a high accuracy level, closely approaching the fully trained model, with only a tiny performance gap. Furthermore, training 50% (half) of the model still achieved a high level of accuracy, with negligible differences observed in the model’s performance as the number of clients increased, as demonstrated in the case involving 20 clients.
</figcaption>
</figure>
<div id="S5.SS2.SSS3.p9" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS3.p9.1" class="ltx_p">Moreover, we evaluate the effectiveness of our approach by increasing the number of clients while keeping the data amount constant to train 7 layers. As shown in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2.3 The impact of scaling the number of clients (edge nodes) on the model accuracy ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, Despite employing the same model architecture and total data size, we have observed similar performance when using either 20 or 10 clients. It is important to note that we divided the dataset into 10 and 20 partitions, which were assigned to 10 and 20 clients, respectively. By increasing the number of clients involved in constructing the global model, we noticed an improvement in the model’s performance. Therefore, our findings indicate that the global model generated by 20 clients outperformed the one built by 10 clients. Furthermore, we observed that the model trained with 20 clients achieved good accuracy, albeit slightly different from the baseline. This can be attributed to the model’s capability to learn hidden patterns from data with sufficient samples more efficiently, even when the client has a smaller sample size. These findings highlight the significance of involving more contributors (clients) and selecting an appropriate number of trainable layers in the federated learning (FL) settings, as they have a substantial impact on the model’s performance. Additionally, this outcome allows for a more precise estimation of the training budget and optimizing the utilization of available resources.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2309.10367/assets/figures/scale3_clients_fixed_data.png" id="S5.F6.g1" class="ltx_graphics ltx_img_landscape" width="354" height="174" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The impact of training a consistent number of model layers (7) over a fixed amount of data while scaling the number of clients on model performance. Notably, the model’s accuracy demonstrated improvement as the number of clients increased, ultimately outperforming the model trained with fewer clients.
</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.4 </span>Training time</h4>

<div id="S5.SS2.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p1.1" class="ltx_p">In the previous sections, we demonstrated the ability to maintain accuracy while freezing parts of the model. Nonetheless, it is crucial to consider the effect on training time, mainly when dealing with larger models. Nevertheless, our approach demonstrates its effectiveness in accelerating the training process. This is achieved by distributing sub-layers of the entire model among the clients to be trained over clients’ local data within the Federated Learning (FL) network, ultimately reducing training time.</p>
</div>
<div id="S5.SS2.SSS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p2.1" class="ltx_p">Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2.4 Training time ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the total training time (in minutes) required by 10 clients to complete 100 training rounds. Training the entire model takes approximately 331 minutes while training 75% of the model saves approximately 21 minutes (a 7% reduction in time). There is a significant difference of 63 minutes between training 4 layers (25% of the model) compared to training 14 layers. By training approximately 50% of the model, we can save around 36 minutes (a 10% reduction compared to training the entire model) while still maintaining accuracy (refer to Figure <a href="#S5.F1" title="Figure 1 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S5.SS2.SSS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS4.p3.1" class="ltx_p">Additionally, Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2.4 Training time ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> demonstrates the time required for a single client to complete 100 training rounds. As the number of trained layers increases, the training time grows linearly, impacting computational resources in terms of cost and availability.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2309.10367/assets/figures/training_time_client.png" id="S5.F7.g1" class="ltx_graphics ltx_img_landscape" width="354" height="174" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Total time required for 10 clients to complete 100 global rounds when training different parts of VGG16 model over CIFAR-10 dataset.</figcaption>
</figure>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2309.10367/assets/figures/training_time_r_10C.png" id="S5.F8.g1" class="ltx_graphics ltx_img_landscape" width="354" height="166" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The time spent by one client to train the VGG16 model over CIFAR-10 dataset using different trainable layers (cost per client per global communication round).
</figcaption>
</figure>
</section>
<section id="S5.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.5 </span>Transferred data size and number of trainable parameters </h4>

<div id="S5.SS2.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p1.1" class="ltx_p">The size of transferred parameters (weights) in Federated Learning (FL) settings naturally depends on the number of trainable layers in the neural network. In addition to the benefits mentioned above, layer sub-selection has the potential to reduce the amount of resources required on the client side. This can include computing power, storage space, and network capacity needed for communication.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>The average transferred data size for different numbers of trained layers during a communication round with 10 participating clients</figcaption>
<div id="S5.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:303.0pt;height:46.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.7pt,3.8pt) scale(0.86,0.86) ;">
<table id="S5.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th id="S5.T3.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">4 Layers</span></th>
<th id="S5.T3.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">7 Layers</span></th>
<th id="S5.T3.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">10 Layers</span></th>
<th id="S5.T3.1.1.1.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S5.T3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">14 Layers</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.2.1" class="ltx_tr">
<td id="S5.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">No. of training parameters</span></td>
<td id="S5.T3.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">34.88 M</td>
<td id="S5.T3.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">67.92 M</td>
<td id="S5.T3.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">101.3 M</td>
<td id="S5.T3.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">147.2 M</td>
</tr>
<tr id="S5.T3.1.1.3.2" class="ltx_tr">
<td id="S5.T3.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T3.1.1.3.2.1.1" class="ltx_text ltx_font_bold">Transferred data size</span></td>
<td id="S5.T3.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">133.1 MB</td>
<td id="S5.T3.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">259.1 MB</td>
<td id="S5.T3.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">386.5 MB</td>
<td id="S5.T3.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">561.6 MB</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS2.SSS5.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p2.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 ‣ 5.2.5 Transferred data size and number of trainable parameters ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the number of trainable parameters and transferred data size for different training settings over 100 training rounds with 10 clients, highlighting the linear correlation between the number of trainable layers and serialized data size.</p>
</div>
<div id="S5.SS2.SSS5.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS5.p3.1" class="ltx_p">Additionally, we observed a significant time difference (see Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2.4 Training time ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) between training a model with 4 layers (268 minutes) and 14 layers (331 minutes), with a difference of 63 minutes. When considering factors such as model accuracy (refer to Figure <a href="#S5.F1" title="Figure 1 ‣ 5.2.1 Model performance ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), transferred updated gradients, number of trainable parameters (as shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.2.5 Transferred data size and number of trainable parameters ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), and training time (see Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2.4 Training time ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>), these effects will be discussed in more detail in the upcoming section.</p>
</div>
</section>
<section id="S5.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.6 </span>Resources utilization based on VM’s</h4>

<div id="S5.SS2.SSS6.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS6.p1.1" class="ltx_p">In this section we empirically study practical resource constraints by varying the computational resources available clients side by using different VM flavors in the SNIC Science Cloud. Table <a href="#S5.T4" title="Table 4 ‣ 5.2.6 Resources utilization based on VM’s ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports the measured percentage of CPU and RAM utilization for local training when varying the number of trainable layers. We conducted experiments starting from the ssc.xsmall flavor, which simulates devices with restricted resources, and gradually scaled up the client resources to the ssc.xlarge flavor. As shown in Table <a href="#S5.T4" title="Table 4 ‣ 5.2.6 Resources utilization based on VM’s ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, using the ssc.xsmall flavor allowed us to train only 4 layers of the VGG16 model, utilizing both CPU and RAM fully, and requiring 1119.49 seconds. Due to resource limitations, the client cannot train additional layers using the available resources. However, by scaling up to the ssc.small flavor, we were able to train up to 10 layers without any issues. Training the entire model required the essential resource of the ssc.small.highcpu flavor (2 VCPUs, 2 RAM). When training 4 layers, 90% of the CPU and 64.64% of the RAM were allocated, with a training time of 535.24 seconds compared to the ssc.xsmall flavor. Training half of the model resulted in a 4% increase in CPU and RAM utilization, slowing down the training process by approximately 11 seconds. Comparing the local training process for 14 and 4 layers, we observed a significant resource utilization gap (CPU: 8%, RAM: 12.14%) and an increase in training time by 84.41 seconds, highlighting the need for more computational power to perform the task.</p>
</div>
<div id="S5.SS2.SSS6.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS6.p2.1" class="ltx_p">To expedite the training process for a single client, it often demands an increase in computational resources, such as CPU and RAM. However, this can lead to higher overall training costs and limitations in scaling up these resources, potentially resulting in a single point of failure.
As a solution, scaling out the number of clients with lower resources can mitigate these challenges. Training the model partially while increasing the number of clients, we’ve shown in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.2.3 The impact of scaling the number of clients (edge nodes) on the model accuracy ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> that it doesn’t adversely affect the model’s accuracy. Consequently, this strategy allows for the model training process to involve constrained devices like Jetson Nano or VM’s with limited configurations (e.g., ssc.small). This approach enables the establishment of a cluster incorporating restricted nodes to train the ML model in a FL setting. The primary goal is to reduce training costs and enhance resilience against failures.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>The local training cost in terms of time (seconds), CPU, and RAM per round may vary for different clients with different flavors deployed on the SNIC Science Cloud. <span id="S5.T4.3.1" class="ltx_text ltx_font_bold">Flavors:</span> <span id="S5.T4.4.2" class="ltx_text ltx_font_italic">ssc.xsmall (1 VCPU, 1 GB RAM),ssc.small (1 VCPU, 2 GB RAM), ssc.small.highcpu (2 VCPU, 2 GB RAM), ssc.medium (2 VCPU, 4 GB RAM), ssc.medium.highcpu (4 VCPU, 4 GB RAM), ssc.large (4 VCPU, 8 GB RAM), ssc.xlarge (8 VCPU, 16 GB RAM)</span></figcaption>
<div id="S5.T4.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:605.3pt;height:148.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-66.4pt,16.2pt) scale(0.82,0.82) ;">
<table id="S5.T4.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.5.1.1.1" class="ltx_tr">
<th id="S5.T4.5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="3"><span id="S5.T4.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Flavour</span></th>
<td id="S5.T4.5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="4"><span id="S5.T4.5.1.1.1.2.1" class="ltx_text ltx_font_bold">Training time (Sec.)</span></td>
<td id="S5.T4.5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="8"><span id="S5.T4.5.1.1.1.3.1" class="ltx_text ltx_font_bold">Resources Consumption</span></td>
</tr>
<tr id="S5.T4.5.1.2.2" class="ltx_tr">
<td id="S5.T4.5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.5.1.2.2.1.1" class="ltx_text ltx_font_bold">4 Layers</span></td>
<td id="S5.T4.5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.5.1.2.2.2.1" class="ltx_text ltx_font_bold">7 Layers</span></td>
<td id="S5.T4.5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T4.5.1.2.2.3.1" class="ltx_text ltx_font_bold">10 Layers</span></td>
<td id="S5.T4.5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" rowspan="2"><span id="S5.T4.5.1.2.2.4.1" class="ltx_text ltx_font_bold">14 Layers</span></td>
<td id="S5.T4.5.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.5.1.2.2.5.1" class="ltx_text ltx_font_bold">4 Layers</span></td>
<td id="S5.T4.5.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.5.1.2.2.6.1" class="ltx_text ltx_font_bold">7 Layers</span></td>
<td id="S5.T4.5.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.5.1.2.2.7.1" class="ltx_text ltx_font_bold">10 Layers</span></td>
<td id="S5.T4.5.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T4.5.1.2.2.8.1" class="ltx_text ltx_font_bold">14 Layers</span></td>
</tr>
<tr id="S5.T4.5.1.3.3" class="ltx_tr">
<td id="S5.T4.5.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPU</td>
<td id="S5.T4.5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RAM (MB)</td>
<td id="S5.T4.5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPU</td>
<td id="S5.T4.5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RAM (MB)</td>
<td id="S5.T4.5.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPU</td>
<td id="S5.T4.5.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RAM (MB)</td>
<td id="S5.T4.5.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPU</td>
<td id="S5.T4.5.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RAM (MB)</td>
</tr>
<tr id="S5.T4.5.1.4.4" class="ltx_tr" style="background-color:#CCE6BF;">
<th id="S5.T4.5.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.1.1" class="ltx_text" style="background-color:#CCE6BF;">ssc.xsmall</span></th>
<td id="S5.T4.5.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.2.1" class="ltx_text" style="background-color:#CCE6BF;">1119.49</span></td>
<td id="S5.T4.5.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.3.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.4.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t"><span id="S5.T4.5.1.4.4.5.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.6.1" class="ltx_text" style="background-color:#CCE6BF;">99%</span></td>
<td id="S5.T4.5.1.4.4.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.7.1" class="ltx_text" style="background-color:#CCE6BF;">988 (95.70%)</span></td>
<td id="S5.T4.5.1.4.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.8.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.9.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.10.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.11.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.12" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.12.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.4.4.13" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.4.4.13.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
</tr>
<tr id="S5.T4.5.1.5.5" class="ltx_tr" style="background-color:#CCE6BF;">
<th id="S5.T4.5.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.1.1" class="ltx_text" style="background-color:#CCE6BF;">ssc.small</span></th>
<td id="S5.T4.5.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.2.1" class="ltx_text" style="background-color:#CCE6BF;">894.0</span></td>
<td id="S5.T4.5.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.3.1" class="ltx_text" style="background-color:#CCE6BF;">954.0</span></td>
<td id="S5.T4.5.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.4.1" class="ltx_text" style="background-color:#CCE6BF;">1086.0</span></td>
<td id="S5.T4.5.1.5.5.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t"><span id="S5.T4.5.1.5.5.5.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.6.1" class="ltx_text" style="background-color:#CCE6BF;">99%</span></td>
<td id="S5.T4.5.1.5.5.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.7.1" class="ltx_text" style="background-color:#CCE6BF;">1312 (66.21%)</span></td>
<td id="S5.T4.5.1.5.5.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.8.1" class="ltx_text" style="background-color:#CCE6BF;">99%</span></td>
<td id="S5.T4.5.1.5.5.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.9.1" class="ltx_text" style="background-color:#CCE6BF;">1490 (72.75%)</span></td>
<td id="S5.T4.5.1.5.5.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.10.1" class="ltx_text" style="background-color:#CCE6BF;">99%</span></td>
<td id="S5.T4.5.1.5.5.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.11.1" class="ltx_text" style="background-color:#CCE6BF;">1758 (85.83%)</span></td>
<td id="S5.T4.5.1.5.5.12" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.12.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
<td id="S5.T4.5.1.5.5.13" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S5.T4.5.1.5.5.13.1" class="ltx_text" style="background-color:#CCE6BF;">-</span></td>
</tr>
<tr id="S5.T4.5.1.6.6" class="ltx_tr">
<th id="S5.T4.5.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">ssc.small.highcpu</th>
<td id="S5.T4.5.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">535.24</td>
<td id="S5.T4.5.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">546.23</td>
<td id="S5.T4.5.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">604.65</td>
<td id="S5.T4.5.1.6.6.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">619.65</td>
<td id="S5.T4.5.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">90%</td>
<td id="S5.T4.5.1.6.6.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1284 (64.64%)</td>
<td id="S5.T4.5.1.6.6.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">94%</td>
<td id="S5.T4.5.1.6.6.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1373 (69.10%)</td>
<td id="S5.T4.5.1.6.6.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">96%</td>
<td id="S5.T4.5.1.6.6.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1438 (72.39%)</td>
<td id="S5.T4.5.1.6.6.12" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">98%</td>
<td id="S5.T4.5.1.6.6.13" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1525 (76.78%)</td>
</tr>
<tr id="S5.T4.5.1.7.7" class="ltx_tr">
<th id="S5.T4.5.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">ssc.medium</th>
<td id="S5.T4.5.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">467.16</td>
<td id="S5.T4.5.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">533.75</td>
<td id="S5.T4.5.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">555.92</td>
<td id="S5.T4.5.1.7.7.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">582.22</td>
<td id="S5.T4.5.1.7.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">80%</td>
<td id="S5.T4.5.1.7.7.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1410 (35.85%)</td>
<td id="S5.T4.5.1.7.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">87%</td>
<td id="S5.T4.5.1.7.7.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1486 (37.77%)</td>
<td id="S5.T4.5.1.7.7.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">90%</td>
<td id="S5.T4.5.1.7.7.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1559 (39.64%)</td>
<td id="S5.T4.5.1.7.7.12" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">93%</td>
<td id="S5.T4.5.1.7.7.13" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1678 (42.63%)</td>
</tr>
<tr id="S5.T4.5.1.8.8" class="ltx_tr">
<th id="S5.T4.5.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">ssc.medium.highcpu</th>
<td id="S5.T4.5.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">280.87</td>
<td id="S5.T4.5.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">310.19</td>
<td id="S5.T4.5.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">323.15</td>
<td id="S5.T4.5.1.8.8.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">357.6</td>
<td id="S5.T4.5.1.8.8.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">71%</td>
<td id="S5.T4.5.1.8.8.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1519 (38.63%)</td>
<td id="S5.T4.5.1.8.8.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">77%</td>
<td id="S5.T4.5.1.8.8.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1660 (42.20%)</td>
<td id="S5.T4.5.1.8.8.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">82%</td>
<td id="S5.T4.5.1.8.8.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1662 (42.20%)</td>
<td id="S5.T4.5.1.8.8.12" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">88%</td>
<td id="S5.T4.5.1.8.8.13" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1698 (43.18%)</td>
</tr>
<tr id="S5.T4.5.1.9.9" class="ltx_tr">
<th id="S5.T4.5.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">ssc.large</th>
<td id="S5.T4.5.1.9.9.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">270.26</td>
<td id="S5.T4.5.1.9.9.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">303.13</td>
<td id="S5.T4.5.1.9.9.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">315.37</td>
<td id="S5.T4.5.1.9.9.5" class="ltx_td ltx_align_left ltx_border_rr ltx_border_t">342.66</td>
<td id="S5.T4.5.1.9.9.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">68%</td>
<td id="S5.T4.5.1.9.9.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1541 (19.38%)</td>
<td id="S5.T4.5.1.9.9.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">76%</td>
<td id="S5.T4.5.1.9.9.9" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1602 (20.13%)</td>
<td id="S5.T4.5.1.9.9.10" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">79%</td>
<td id="S5.T4.5.1.9.9.11" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1661 (20.86%)</td>
<td id="S5.T4.5.1.9.9.12" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">86%</td>
<td id="S5.T4.5.1.9.9.13" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">1723 (21.64%)</td>
</tr>
<tr id="S5.T4.5.1.10.10" class="ltx_tr">
<th id="S5.T4.5.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">ssc.xlarge</th>
<td id="S5.T4.5.1.10.10.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">160.24</td>
<td id="S5.T4.5.1.10.10.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">180.08</td>
<td id="S5.T4.5.1.10.10.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">187.04</td>
<td id="S5.T4.5.1.10.10.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_rr ltx_border_t">195.86</td>
<td id="S5.T4.5.1.10.10.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">55%</td>
<td id="S5.T4.5.1.10.10.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">1795 (11.22%)</td>
<td id="S5.T4.5.1.10.10.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">60%</td>
<td id="S5.T4.5.1.10.10.9" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">1934 (12.09%)</td>
<td id="S5.T4.5.1.10.10.10" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">64%</td>
<td id="S5.T4.5.1.10.10.11" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">1988 (12.42%)</td>
<td id="S5.T4.5.1.10.10.12" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">78%</td>
<td id="S5.T4.5.1.10.10.13" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">2059 (12.86%)</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.7 </span>Resources utilization based on Jetson Nano</h4>

<div id="S5.SS2.SSS7.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS7.p1.1" class="ltx_p">We conducted an additional experiment using an actual restricted device, the Jetson Nano 2GB kit <span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a target="_blank" href="https://developer.nvidia.com/embedded/jetson-nano-2gb-developer-kit" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.nvidia.com/embedded/jetson-nano-2gb-developer-kit</a></span></span></span>. This experiment aimed to observe the behaviour of the device in terms of resource utilization during the local model training process for different subsets of layers <span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a target="_blank" href="https://github.com/saadiabadi/Jetson-Nano-Setup-Fedn.git" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/saadiabadi/Jetson-Nano-Setup-Fedn.git</a></span></span></span>. We utilized a lighter version of the VGG16 model by reducing the dimensions of the previous model’s layers by half and setting the batch size to 4. This adjustment aimed to avoid out-of-memory issues.</p>
</div>
<div id="S5.SS2.SSS7.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS7.p2.1" class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ 5.2.7 Resources utilization based on Jetson Nano ‣ 5.2 Results and discussion ‣ 5 Results and Discussion ‣ Toward efficient resource utilization at edge nodes in federated learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reports the training time and resource consumption per training round for different trainable layers on the Jetson Nano. It can be observed that training 4 layers requires 191.4 seconds per round and demands 46.55% and 88.61% of the CPU and RAM, respectively. These resource demands are still manageable for the device. However, as we increase the number of trained layers, these values increase accordingly. Comparing the training time for 4 and 10 layers, we noticed a difference of approximately 66 seconds, which can be significant in critical scenarios. Furthermore, training 10 layers resulted in a 4% and 7% increase in both CPU and RAM consumption, respectively.</p>
</div>
<div id="S5.SS2.SSS7.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS7.p3.1" class="ltx_p">However, during the training process of the entire model, the Jetson Nano crashed (cannot finish the training) due to a lack of memory.
Finally, this experiment on the Jetson Nano device further supports the benefits of training the model partially on restricted devices, as it allows for efficient resource utilization and avoids memory limitations.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>The local training cost in terms of time (Seconds), CPU and RAM per round using Jetson Nano 2GB client (Quad-core ARM CPU, 2GB RAM)</figcaption>
<table id="S5.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.1" class="ltx_tr">
<td id="S5.T5.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="8"><span id="S5.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Training time (Sec.)</span></td>
</tr>
<tr id="S5.T5.1.2.2" class="ltx_tr">
<td id="S5.T5.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><span id="S5.T5.1.2.2.1.1" class="ltx_text ltx_font_bold">4 Layers</span></td>
<td id="S5.T5.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T5.1.2.2.2.1" class="ltx_text ltx_font_bold">7 Layers</span></td>
<td id="S5.T5.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T5.1.2.2.3.1" class="ltx_text ltx_font_bold">10 Layers</span></td>
<td id="S5.T5.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2"><span id="S5.T5.1.2.2.4.1" class="ltx_text ltx_font_bold">14 Layers</span></td>
</tr>
<tr id="S5.T5.1.3.3" class="ltx_tr">
<td id="S5.T5.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">191.4</td>
<td id="S5.T5.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">235.6</td>
<td id="S5.T5.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2">257.8</td>
<td id="S5.T5.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#BFBFFF;" colspan="2"><span id="S5.T5.1.3.3.4.1" class="ltx_text" style="background-color:#BFBFFF;">-</span></td>
</tr>
<tr id="S5.T5.1.4.4" class="ltx_tr">
<td id="S5.T5.1.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="8"><span id="S5.T5.1.4.4.1.1" class="ltx_text ltx_font_bold">Resources Consumption</span></td>
</tr>
<tr id="S5.T5.1.5.5" class="ltx_tr">
<td id="S5.T5.1.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.1.1" class="ltx_text ltx_font_bold">CPU</span></td>
<td id="S5.T5.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.2.1" class="ltx_text ltx_font_bold">RAM (MB)</span></td>
<td id="S5.T5.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.3.1" class="ltx_text ltx_font_bold">CPU</span></td>
<td id="S5.T5.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.4.1" class="ltx_text ltx_font_bold">RAM (MB)</span></td>
<td id="S5.T5.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.5.1" class="ltx_text ltx_font_bold">CPU</span></td>
<td id="S5.T5.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.6.1" class="ltx_text ltx_font_bold">RAM (MB)</span></td>
<td id="S5.T5.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.7.1" class="ltx_text ltx_font_bold">CPU</span></td>
<td id="S5.T5.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T5.1.5.5.8.1" class="ltx_text ltx_font_bold">RAM (MB)</span></td>
</tr>
<tr id="S5.T5.1.6.6" class="ltx_tr">
<td id="S5.T5.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">46.55%</td>
<td id="S5.T5.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1747 (88.61%)</td>
<td id="S5.T5.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">48.23%</td>
<td id="S5.T5.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1821 (92.38%)</td>
<td id="S5.T5.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">50.63%</td>
<td id="S5.T5.1.6.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1881 (95.68%)</td>
<td id="S5.T5.1.6.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#BFBFFF;"><span id="S5.T5.1.6.6.7.1" class="ltx_text" style="background-color:#BFBFFF;">-</span></td>
<td id="S5.T5.1.6.6.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#BFBFFF;"><span id="S5.T5.1.6.6.8.1" class="ltx_text" style="background-color:#BFBFFF;">-</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">This paper introduces a novel approach for training deep learning models in a federated learning setting, aiming to efficiently utilize edge node resources and reduce network workload. The proposed approach trains a specific number of the model’s layers, randomly selected every training round, and freezes the remaining layers. This approach can enable IoT devices with restricted resources (as examplified here with the Jetson Nano) to participate in training larger models.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">The approach was evaluated for three tasks: sentiment analysis using the IMDB dataset, object detection using the CIFAR-10 dataset, and human activity recognition using the CASA dataset, each with different model architectures. The experimental results demonstrate that training only a part of the model in the federated learning setting has a significant impact on resource utilization, communication, training budget, and model performance.
Furthermore, increasing the number of contributors also considerably affects model performance while keeping the amount of data fixed as we demonstrated in the results.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Overall, the study demonstrates the potential of our approach to train deep learning models in a federated learning setting efficiently, enabling participation from a diverse range of devices and clients without sacrificing model performance.</p>
</div>
<div id="S6.p4" class="ltx_para ltx_noindent">
<p id="S6.p4.1" class="ltx_p">In future work, the authors plan to investigate strategies for selecting layers based on the available resources of each client and the number of trainable parameters per layer, considering the expected heterogeneity in real-life use cases. Additionally, exploring measures of each layer’s importance for model improvement could be an interesting avenue for further research.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">This work was funded by the eSSENCE
strategic collaboration on eScience (Alawadi, Ait-Mlouk, Toor, and Hellander), and supported by Halmstad University. The authors also would like to thank SNIC for providing cloud resources.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence and Statistics</em>, pages 1273–1282,
2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. [2016a]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Daniel Ramage, and Peter
Richtárik.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine learning for on-device
intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016a.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021]</span>
<span class="ltx_bibblock">
Chen Chen, Hong Xu, Wei Wang, Baochun Li, Bo Li, Li Chen, and Gong Zhang.

</span>
<span class="ltx_bibblock">Communication-efficient federated learning with adaptive parameter
freezing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 41st International Conference on Distributed
Computing Systems (ICDCS)</em>, pages 1–11. IEEE, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tandon et al. [2017]</span>
<span class="ltx_bibblock">
Rashish Tandon, Qi Lei, Alexandros G Dimakis, and Nikos Karampatziakis.

</span>
<span class="ltx_bibblock">Gradient coding: Avoiding stragglers in distributed learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
3368–3376. PMLR, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2019]</span>
<span class="ltx_bibblock">
Zirui Xu, Zhao Yang, Jinjun Xiong, Janlei Yang, and Xiang Chen.

</span>
<span class="ltx_bibblock">Elfish: Resource-aware federated learning on heterogeneous edge
devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Ratio</em>, 2(r1):r2, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE signal processing magazine</em>, 37(3):50–60, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Too et al. [2019]</span>
<span class="ltx_bibblock">
Edna Chebet Too, Li Yujian, Sam Njuki, and Liu Yingchun.

</span>
<span class="ltx_bibblock">A comparative study of fine-tuning deep learning models for plant
disease identification.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Computers and Electronics in Agriculture</em>, 161:272–279, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021a]</span>
<span class="ltx_bibblock">
Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman.

</span>
<span class="ltx_bibblock">Autofreeze: Automatically freezing model blocks to accelerate
fine-tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.01386</em>, 2021a.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2018]</span>
<span class="ltx_bibblock">
Chi-Chung Chen, Chia-Lin Yang, and Hsiang-Yun Cheng.

</span>
<span class="ltx_bibblock">Efficient and robust parallel dnn training through model parallelism
on multi-gpu platform.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.02839</em>, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al. [2014]</span>
<span class="ltx_bibblock">
Yongqiang Zou, Xing Jin, Yi Li, Zhimao Guo, Eryu Wang, and Bin Xiao.

</span>
<span class="ltx_bibblock">Mariana: Tencent deep learning platform and its applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment</em>, 7(13):1772–1777, 2014.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vishnu et al. [2016]</span>
<span class="ltx_bibblock">
Abhinav Vishnu, Charles Siegel, and Jeffrey Daily.

</span>
<span class="ltx_bibblock">Distributed tensorflow with mpi.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1603.02339</em>, 2016.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewett and Grady II [2020]</span>
<span class="ltx_bibblock">
Russell J Hewett and Thomas J Grady II.

</span>
<span class="ltx_bibblock">A linear algebraic approach to model parallelism in deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.03108</em>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. [2018]</span>
<span class="ltx_bibblock">
Zhihao Jia, Matei Zaharia, and Alex Aiken.

</span>
<span class="ltx_bibblock">Beyond data and model parallelism for deep neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.05358</em>, 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et al. [2019]</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-lm: Training multi-billion parameter language models using
model parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2019a]</span>
<span class="ltx_bibblock">
Xueli Xiao, Thosini Bamunu Mudiyanselage, Chunyan Ji, Jie Hu, and Yi Pan.

</span>
<span class="ltx_bibblock">Fast deep learning training through intelligently freezing layers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Internet of Things
(iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE
Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data
(SmartData)</em>, pages 1225–1232. IEEE, 2019a.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan and Yang [2009]</span>
<span class="ltx_bibblock">
Sinno Jialin Pan and Qiang Yang.

</span>
<span class="ltx_bibblock">A survey on transfer learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on knowledge and data engineering</em>,
22(10):1345–1359, 2009.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dean et al. [2012]</span>
<span class="ltx_bibblock">
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al.

</span>
<span class="ltx_bibblock">Large scale distributed deep networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 25, 2012.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sergeev and Del Balso [2018]</span>
<span class="ltx_bibblock">
Alexander Sergeev and Mike Del Balso.

</span>
<span class="ltx_bibblock">Horovod: fast and easy distributed deep learning in tensorflow.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1802.05799</em>, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. [2020]</span>
<span class="ltx_bibblock">
Jay H Park, Gyeongchan Yun, M Yi Chang, Nguyen T Nguyen, Seungmin Lee, Jaesik
Choi, Sam H Noh, and Young-ri Choi.

</span>
<span class="ltx_bibblock"><math id="bib.bib19.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib19.1.m1.1a"><mo stretchy="false" id="bib.bib19.1.m1.1.1" xref="bib.bib19.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib19.1.m1.1b"><ci id="bib.bib19.1.m1.1.1.cmml" xref="bib.bib19.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.1.m1.1c">\{</annotation></semantics></math>HetPipe<math id="bib.bib19.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib19.2.m2.1a"><mo stretchy="false" id="bib.bib19.2.m2.1.1" xref="bib.bib19.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib19.2.m2.1b"><ci id="bib.bib19.2.m2.1.1.cmml" xref="bib.bib19.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.2.m2.1c">\}</annotation></semantics></math>: Enabling large <math id="bib.bib19.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib19.3.m3.1a"><mo stretchy="false" id="bib.bib19.3.m3.1.1" xref="bib.bib19.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib19.3.m3.1b"><ci id="bib.bib19.3.m3.1.1.cmml" xref="bib.bib19.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.3.m3.1c">\{</annotation></semantics></math>DNN<math id="bib.bib19.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib19.4.m4.1a"><mo stretchy="false" id="bib.bib19.4.m4.1.1" xref="bib.bib19.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib19.4.m4.1b"><ci id="bib.bib19.4.m4.1.1.cmml" xref="bib.bib19.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.4.m4.1c">\}</annotation></semantics></math> training on (whimpy)
heterogeneous <math id="bib.bib19.5.m5.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib19.5.m5.1a"><mo stretchy="false" id="bib.bib19.5.m5.1.1" xref="bib.bib19.5.m5.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib19.5.m5.1b"><ci id="bib.bib19.5.m5.1.1.cmml" xref="bib.bib19.5.m5.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.5.m5.1c">\{</annotation></semantics></math>GPU<math id="bib.bib19.6.m6.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib19.6.m6.1a"><mo stretchy="false" id="bib.bib19.6.m6.1.1" xref="bib.bib19.6.m6.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib19.6.m6.1b"><ci id="bib.bib19.6.m6.1.1.cmml" xref="bib.bib19.6.m6.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib19.6.m6.1c">\}</annotation></semantics></math> clusters through integration of pipelined model
parallelism and data parallelism.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.7.1" class="ltx_emph ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</em>,
pages 307–321, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaunt et al. [2017]</span>
<span class="ltx_bibblock">
Alexander L Gaunt, Matthew A Johnson, Maik Riechert, Daniel Tarlow, Ryota
Tomioka, Dimitrios Vytiniotis, and Sam Webster.

</span>
<span class="ltx_bibblock">Ampnet: Asynchronous model-parallel training for dynamic neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.09786</em>, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valiant [1990]</span>
<span class="ltx_bibblock">
Leslie G Valiant.

</span>
<span class="ltx_bibblock">A bridging model for parallel computation.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 33(8):103–111,
1990.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. [2012]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 25, 2012.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ten [2017]</span>
<span class="ltx_bibblock">
Tensorflow benchmarks., 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.tensorflow.org/performance/benchmarks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/performance/benchmarks</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">caf [2016]</span>
<span class="ltx_bibblock">
A new lightweight, modular, and scalable deep learning framework, 2016.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://caffe2.ai/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://caffe2.ai/</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">pyt [2017]</span>
<span class="ltx_bibblock">
Tensors and dynamic neural networks in python with strong gpu acceleration.,
2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://pytorch.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brock et al. [2017]</span>
<span class="ltx_bibblock">
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston.

</span>
<span class="ltx_bibblock">Freezeout: Accelerate training by progressively freezing layers.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.04983</em>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu-hui et al. [2019]</span>
<span class="ltx_bibblock">
CHEN Xu-hui, Ejaz UI Haq, and ZHOU Chengyu.

</span>
<span class="ltx_bibblock">Notice of violation of ieee publication principles: Efficient
technique to accelerate neural network training by freezing hidden layers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2019 IEEE/ACIS 18th International Conference on Computer and
Information Science (ICIS)</em>, pages 542–546. IEEE, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. [2019b]</span>
<span class="ltx_bibblock">
Xueli Xiao, Thosini Bamunu Mudiyanselage, Chunyan Ji, Jie Hu, and Yi Pan.

</span>
<span class="ltx_bibblock">Fast deep learning training through intelligently freezing layers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">2019 International Conference on Internet of Things
(iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE
Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data
(SmartData)</em>, pages 1225–1232. IEEE, 2019b.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2019]</span>
<span class="ltx_bibblock">
Jaejun Lee, Raphael Tang, and Jimmy Lin.

</span>
<span class="ltx_bibblock">What would elsa do? freezing layers during transformer fine-tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.03090</em>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021b]</span>
<span class="ltx_bibblock">
Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman.

</span>
<span class="ltx_bibblock">Autofreeze: Automatically freezing model blocks to accelerate
fine-tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.01386</em>, 2021b.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Yiding Wang, Decang Sun, Kai Chen, Fan Lai, and Mosharaf Chowdhury.

</span>
<span class="ltx_bibblock">Efficient dnn training with knowledge-guided layer freezing.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.06227</em>, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. [2016b]</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016b.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu et al. [2018]</span>
<span class="ltx_bibblock">
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">On the convergence of federated optimization in heterogeneous
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.06127</em>, 3:3, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekmefjord et al. [2021]</span>
<span class="ltx_bibblock">
Morgan Ekmefjord, Addi Ait-Mlouk, Sadi Alawadi, Mattias Åkesson, Desislava
Stoyanova, Ola Spjuth, Salman Toor, and Andreas Hellander.

</span>
<span class="ltx_bibblock">Scalable federated machine learning with fedn.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.00148</em>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2019]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2019]</span>
<span class="ltx_bibblock">
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and
Rogerio Feris.

</span>
<span class="ltx_bibblock">Spottune: transfer learning through adaptive fine-tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 4805–4814, 2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vrbančič and Podgorelec [2020]</span>
<span class="ltx_bibblock">
Grega Vrbančič and Vili Podgorelec.

</span>
<span class="ltx_bibblock">Transfer learning with adaptive fine-tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8:196197–196211, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shallue et al. [2018]</span>
<span class="ltx_bibblock">
Christopher J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
Roy Frostig, and George E Dahl.

</span>
<span class="ltx_bibblock">Measuring the effects of data parallelism on neural network training.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.03600</em>, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et al. [2018]</span>
<span class="ltx_bibblock">
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn
Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young,
et al.

</span>
<span class="ltx_bibblock">Mesh-tensorflow: Deep learning for supercomputers.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 31, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et al. [2005]</span>
<span class="ltx_bibblock">
Rajeev Thakur, Rolf Rabenseifner, and William Gropp.

</span>
<span class="ltx_bibblock">Optimization of collective communication operations in mpich.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">The International Journal of High Performance Computing
Applications</em>, 19(1):49–66, 2005.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky [2014]</span>
<span class="ltx_bibblock">
Alex Krizhevsky.

</span>
<span class="ltx_bibblock">One weird trick for parallelizing convolutional neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1404.5997</em>, 2014.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky et al. [2009]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maas et al. [2011]</span>
<span class="ltx_bibblock">
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and
Christopher Potts.

</span>
<span class="ltx_bibblock">Learning word vectors for sentiment analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies</em>, pages 142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://www.aclweb.org/anthology/P11-1015" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.aclweb.org/anthology/P11-1015</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toor et al. [2017]</span>
<span class="ltx_bibblock">
Salman Toor, Mathias Lindberg, Ingemar Falman, Andreas Vallin, Olof Mohill,
Pontus Freyhult, Linus Nilsson, Martin Agback, Lars Viklund, Henric Zazzik,
Ola Spjuth, Marco Capuccini, Joakim Möller, Donal Murtagh, and Andreas
Hellander.

</span>
<span class="ltx_bibblock">Snic science cloud (ssc): A national-scale cloud infrastructure for
swedish academia.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2017 IEEE 13th International Conference on e-Science
(e-Science)</em>, pages 219–227, 2017.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https://doi.org/10.1109/eScience.2017.35" title="" class="ltx_ref ltx_href">10.1109/eScience.2017.35</a>.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="David S. Hippocampus, Elias D. Striatum"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="First keyword, Second keyword, More"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="q-bio.NC, q-bio.QM"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="A template for the arxiv style"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.10366" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.10367" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.10367">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.10367" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.10368" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 04:13:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
