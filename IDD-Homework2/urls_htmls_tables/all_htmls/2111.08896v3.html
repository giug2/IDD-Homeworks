<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2111.08896] Achieving Human Parity on Visual Question Answering</title><meta property="og:description" content="The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-wo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Achieving Human Parity on Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Achieving Human Parity on Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2111.08896">

<!--Generated on Tue Mar 19 16:34:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %Uncomment␣to␣remove␣the␣date .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Achieving Human Parity on Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ming Yan  , Haiyang Xu<sup id="id5.5.id1" class="ltx_sup">∗</sup>, Chenliang Li<sup id="id6.6.id2" class="ltx_sup">∗</sup>, Junfeng Tian<sup id="id7.7.id3" class="ltx_sup">∗</sup>, Bin Bi<sup id="id8.8.id4" class="ltx_sup">∗</sup>, Wei Wang, Weihua Chen, Xianzhe Xu,

<br class="ltx_break"><span id="id9.9.id5" class="ltx_text ltx_font_bold">Fan Wang, Zheng Cao, Zhicheng Zhang, Qiyu Zhang, Ji Zhang, Songfang Huang, Fei Huang, Luo Si, Rong Jin</span>

<br class="ltx_break">Alibaba Group
<br class="ltx_break">{ym119608, shuofeng.xhy, lcl193798, tjf141457, b.bi, hebian.ww, kugang.cwh, xianzhe.xxz, 
<br class="ltx_break">fan.w, zhengzhi.cz, zhangzhicheng.zzc, qiyu.zhang, zj122146, songfang.hsf, f.huang, luo.si, jinrong.jr}@alibaba-inc.com
</span><span class="ltx_author_notes">Equal contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id10.id1" class="ltx_p">The Visual Question Answering (VQA) task utilizes both visual image and language analysis to answer a textual question with respect to an image. It has been a popular research topic with an increasing number of real-world applications in the last decade. This paper describes our recent research of AliceMind-MMU (ALIbaba’s Collection of Encoder-decoders from Machine IntelligeNce lab of Damo academy - MultiMedia Understanding) that obtains similar or even slightly better results than human being does on VQA. This is achieved by systematically improving the VQA pipeline including: (1) pre-training with comprehensive visual and textual feature representation; (2) effective cross-modal interaction with learning to attend; and (3) A novel knowledge mining framework with specialized expert modules for the complex VQA task. Treating different types of visual questions with corresponding expertise needed plays an important role in boosting the performance of our VQA architecture up to the human level. An extensive set of experiments and analysis are conducted to demonstrate the effectiveness of the new research work.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Recent years have witnessed human-level performance reached or surpassed by well trained computer programs in tasks ranging from games such as Go <cite class="ltx_cite ltx_citemacro_citep">(Silver et al., <a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>
to classification of images in ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib2" title="" class="ltx_ref">2009a</a>)</cite> to natural language understanding on the GLUE benchmark <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib3" title="" class="ltx_ref">2018</a>)</cite>. Vision and language are two fundamental capabilities of human intelligence. We have seen dramatic progress in the area of representation learning across these two modalities. Inspired by the success of pre-training in both computer vision (CV) <cite class="ltx_cite ltx_citemacro_citep">(Sharif Razavian et al., <a href="#bib.bib4" title="" class="ltx_ref">2014</a>)</cite>
and natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, a number of vision-and-language (V&amp;L) models have been proposed in the last couple of years to tackle challenges at the intersection of these two key areas of AI. Despite superhuman performance achieved respectively in some vision (e.g., ImageNet) and natural language tasks (e.g., GLUE), joint learning across these two modalities, which is essential to human cognition, has demonstrated limited human-level performance by prevalent V&amp;L approaches.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">A compelling reason to study vision and language jointly is the promise of language as a universal and natural interface for visual reasoning problems - useful both in specifying a wide range of problems and in communicating AI responses. Visual reasoning has long been recognized most challenging for cross-modal learning owing to its requirement of higher-order cognition and commonsense reasoning intelligence. With the systematic design of our reasoning architecture, this research work unprecedentedly surpasses human performance in the popular Visual Question Answering (VQA) task <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">This paper summarizes how we achieve the human parity in VQA. Most of the presented techniques are not specific to VQA and can be transferable to tackling other visual reasoning tasks. Our work builds upon the significant progress made on CV and NLP over the past few decades.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2111.08896/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of free-form, open-ended questions and images in the VQA dataset</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">A VQA system takes as input an image and a free-form, open-ended, natural-language question about the image, and produces a natural-language answer as the output. Example questions are shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The open-ended questions require a potentially vast set of AI capabilities to answer, including question understanding, commonsense reasoning, activity recognition, object counting, and visually-grounded language understanding, etc.. Therefore, achieving human performance in VQA would be an important milestone in artificial intelligence. In pursuit of this goal, a new VQA architecture is designed by improving the individual capabilities of visual reasoning. Different from the prevalent VQA methods that rely on a single kind of features with standard Transformer, the new VQA architecture exploits more comprehensive visual and textual feature representation with pre-training, and more effective cross-modal interaction with <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">learning to attend</span>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">The key to our success in VQA is tackling the diverse challenges with different capabilities. In particular, we introduce a novel knowledge mining framework with the Mixture-of-Experts (MoE) model for the complex VQA task. Most existing methods for VQA treat different types of visual questions in the same manner. Different from these methods, following the divide-and-conquer strategy, our new framework first decomposes the complex VQA task into different sub-tasks by a clustering-based method, which allows to identify the types of questions difficult to address. Each type of these questions are then resolved by a specialized expert module. All these expert modules are put together by the MoE paradigm. Beyond a patchwork of models like ensemble, the MoE paradigm learns which module to call upon by exploiting the expertise of each module, and thus intelligently delegates every question to a proper expert module. According to our quantitative analysis, the new knowledge mining with MoE plays an important role in boosting the performance of our VQA architecture up to the human level, significantly outperforming existing methods without explicit task decomposition by a large margin.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">The rest of this paper is organized as follows. Section <a href="#S2" title="2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents the design of our new VQA architecture with the knowledge mining framework. The empirical evaluation and quantitative analysis are given in Section <a href="#S3" title="3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Section <a href="#S4" title="4 Related Work ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> introduces the prior work related to VQA. Finally, the paper is concluded in Section <a href="#S5" title="5 Discussion and Limitation ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> by discussing our findings and limitations.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>VQA Modeling</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Reaching Human Parity</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Despite immense progress on VQA in the research community over the past years, human parity has remained out of reach even though the gap is reduced significantly over the last a few years. This paper describes our efforts to achieve the unprecedented human-level performance on the VQA task by systematically improving the components of the VQA architecture, which is illustrated in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1 Reaching Human Parity ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This work addresses a number of limitations of existing VQA studies, and make the following major contributions:</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2111.08896/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="67" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overview of the new VQA architecture.</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i1.p1.1" class="ltx_p">Most existing VQA approaches rely on a single class of features to represent visual signals. The homogeneous feature representation is insufficient to capture the diversity of visual signal needed to answer open-ended questions as illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. To address this limitation, our VQA architecture exploits a diverse set of visual representations: <span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">region features</span>, <span id="S2.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">grid features</span> and <span id="S2.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">patch features</span>, each of which is used to capture visual signals of a specific type. Specifically, <span id="S2.I1.i1.p1.1.4" class="ltx_text ltx_font_italic">region features</span> are good at locating the salient objects in the image, e.g., slices of pizza in the second case of Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, which is more suitable in tasks like object counting. <span id="S2.I1.i1.p1.1.5" class="ltx_text ltx_font_italic">Grid features</span> are more skilled in the global or background information in the image, e.g., in the fourth case of Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where background information of weather is identified. With the heterogeneous feature set, the model is able to answer different types of questions by leveraging desired visual signals.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i2.p1.1" class="ltx_p">The semantic gap between visual and textual modalities has always been treated as one of the most important challenges in cross-modality research. There exist two families of V&amp;L models that bridge the cross-modal semantic gap: <span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">single-stream architecture</span> <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> and <span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">dual-stream architecture</span> <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Yu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. The single-stream architecture essentially treats the two input modalities equally, and thus does not make full use of the signal from each modality. On the other hand, the dual-stream architecture is insufficient to capture the fine-grained interaction between visual and textual hints. To address the limitations of these architectures, our VQA architecture fuses visual and textual modalities by learning how the features should attend to each other. To allow for fine-grained cross-modal interaction, our model is built upon the single-stream architecture. The original self-attention in Transformer is replaced with a weighted self-attention, where intra-modal attention and inter-modal attention are dynamically adjusted. This leads to effective alignment of the cross-modal semantics.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.i3.p1.1" class="ltx_p">In the VQA task, visual questions are open-ended and require various kinds of knowledge and capabilities to answer. We, therefore, propose a new knowledge mining framework with MoE to address the diverse questions. Besides the general V&amp;L understanding and reasoning in VQA, the new framework is able to identify two types of questions (text-reading questions and clock-reading questions) that are difficult to address by the general-purpose VQA techniques, due to the lack of text reading and clock reading abilities. A specialized expert module is introduced for each of the two question types: 1) Text Reading Expert: answering questions by reasoning about text in images. 2) Clock Reading Expert: answering questions on the time shown by clocks.
An outstanding VQA architecture needs to appropriately mix the multiple experts to delegate each question to a proper expert module. Hence our VQA architecture employs the MoE paradigm to distill the expert knowledge required to answer each type of questions and mix the results of the experts to derive final answers. In the future, we will exploit techniques that automatically discover challenging case sets. New expert modules devoted to the discovered cases are then autonomously learned from specialized data. This learn-and-evolve process will lead to evolutionary intelligence that can rapidly adapt to any new task.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2111.08896/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="195" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The framework of <span id="S2.F3.2.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Comprehensive Feature Representation</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Feature representation for vision and text is fundamental for cross-modal learning. Different kinds of features can help capture diverse data characteristics, which complements with each other.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Visual Features</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">For comprehensive visual feature understanding, three kinds of visual features are considered: <span id="S2.SS2.SSS1.p1.1.1" class="ltx_text ltx_font_italic">region feature</span>, <span id="S2.SS2.SSS1.p1.1.2" class="ltx_text ltx_font_italic">grid feature</span> and <span id="S2.SS2.SSS1.p1.1.3" class="ltx_text ltx_font_italic">patch feature</span>.</p>
</div>
<section id="S2.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Region Feature</h5>

<div id="S2.SS2.SSS1.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.Px1.p1.6" class="ltx_p">With the discovery of ‘bottom-up’ attention <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, region-based visual features have been the <span id="S2.SS2.SSS1.Px1.p1.6.1" class="ltx_text ltx_font_italic">de facto</span> standard for vision and language tasks. Unlike normal ‘top-down’ attention that directly focuses on semantically irrelevant parts of visual input, bottom-up attention uses pre-trained object detectors <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite> to identify salient regions based on the visual input. As a result, images are represented by a collection of region-based features, which provide better localization of individual objects and capture the detailed semantics within the image content. Generally, region-based visual encoders such as BUTD <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite> are pre-trained with detection data like Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>. Recently, VinVL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> has been built on a large-scale pre-trained object-attribute detection model with much larger amounts of data on four public object detection datasets, which helps better capture both coarse-level and fine-grained visual semantic information in images. The object detector from VinVL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> is used in this work to extract a collection of object-level region features with more detailed visual semantics, where each object <math id="S2.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="o_{j}" display="inline"><semantics id="S2.SS2.SSS1.Px1.p1.1.m1.1a"><msub id="S2.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS1.Px1.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml">o</mi><mi id="S2.SS2.SSS1.Px1.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px1.p1.1.m1.1b"><apply id="S2.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1.2">𝑜</ci><ci id="S2.SS2.SSS1.Px1.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.Px1.p1.1.m1.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px1.p1.1.m1.1c">o_{j}</annotation></semantics></math> is represented as a 2048-dimensional feature vector <math id="S2.SS2.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="r_{j}" display="inline"><semantics id="S2.SS2.SSS1.Px1.p1.2.m2.1a"><msub id="S2.SS2.SSS1.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS1.Px1.p1.2.m2.1.1.2" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml">r</mi><mi id="S2.SS2.SSS1.Px1.p1.2.m2.1.1.3" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1.2">𝑟</ci><ci id="S2.SS2.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS1.Px1.p1.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px1.p1.2.m2.1c">r_{j}</annotation></semantics></math>. To capture the spatial information of the object, box-level location features for each object are also encoded via a 4-dimensional vector <math id="S2.SS2.SSS1.Px1.p1.3.m3.4" class="ltx_Math" alttext="l_{j}=(\frac{x_{1}}{W},\frac{y_{1}}{H},\frac{x_{2}}{W},\frac{y_{2}}{H})" display="inline"><semantics id="S2.SS2.SSS1.Px1.p1.3.m3.4a"><mrow id="S2.SS2.SSS1.Px1.p1.3.m3.4.5" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.cmml"><msub id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.cmml"><mi id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.2.cmml">l</mi><mi id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.3.cmml">j</mi></msub><mo id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.1" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.1.cmml">=</mo><mrow id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2.1" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml">(</mo><mfrac id="S2.SS2.SSS1.Px1.p1.3.m3.1.1" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.cmml"><msub id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.2.cmml">x</mi><mn id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.3.cmml">1</mn></msub><mi id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.3.cmml">W</mi></mfrac><mo id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml">,</mo><mfrac id="S2.SS2.SSS1.Px1.p1.3.m3.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.cmml"><msub id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.cmml"><mi id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.2.cmml">y</mi><mn id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.3.cmml">1</mn></msub><mi id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.3.cmml">H</mi></mfrac><mo id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml">,</mo><mfrac id="S2.SS2.SSS1.Px1.p1.3.m3.3.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.cmml"><msub id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.cmml"><mi id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.2.cmml">x</mi><mn id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.3.cmml">2</mn></msub><mi id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.3.cmml">W</mi></mfrac><mo id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2.4" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml">,</mo><mfrac id="S2.SS2.SSS1.Px1.p1.3.m3.4.4" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.cmml"><msub id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.cmml"><mi id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.2" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.2.cmml">y</mi><mn id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.3.cmml">2</mn></msub><mi id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.3" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.3.cmml">H</mi></mfrac><mo stretchy="false" id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2.5" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px1.p1.3.m3.4b"><apply id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5"><eq id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.1"></eq><apply id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.2">𝑙</ci><ci id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.2.3">𝑗</ci></apply><vector id="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.5.3.2"><apply id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1"><divide id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1"></divide><apply id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.2">𝑥</ci><cn type="integer" id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.2.3">1</cn></apply><ci id="S2.SS2.SSS1.Px1.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.1.1.3">𝑊</ci></apply><apply id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2"><divide id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2"></divide><apply id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.2">𝑦</ci><cn type="integer" id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.2.3">1</cn></apply><ci id="S2.SS2.SSS1.Px1.p1.3.m3.2.2.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.2.2.3">𝐻</ci></apply><apply id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3"><divide id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3"></divide><apply id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.2">𝑥</ci><cn type="integer" id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.2.3">2</cn></apply><ci id="S2.SS2.SSS1.Px1.p1.3.m3.3.3.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.3.3.3">𝑊</ci></apply><apply id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4"><divide id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4"></divide><apply id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.1.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.2.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.2">𝑦</ci><cn type="integer" id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.2.3">2</cn></apply><ci id="S2.SS2.SSS1.Px1.p1.3.m3.4.4.3.cmml" xref="S2.SS2.SSS1.Px1.p1.3.m3.4.4.3">𝐻</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px1.p1.3.m3.4c">l_{j}=(\frac{x_{1}}{W},\frac{y_{1}}{H},\frac{x_{2}}{W},\frac{y_{2}}{H})</annotation></semantics></math> as in SemVLP <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2021a</a>)</cite> and LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>. The <math id="S2.SS2.SSS1.Px1.p1.4.m4.1" class="ltx_Math" alttext="r_{j}" display="inline"><semantics id="S2.SS2.SSS1.Px1.p1.4.m4.1a"><msub id="S2.SS2.SSS1.Px1.p1.4.m4.1.1" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1.cmml"><mi id="S2.SS2.SSS1.Px1.p1.4.m4.1.1.2" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1.2.cmml">r</mi><mi id="S2.SS2.SSS1.Px1.p1.4.m4.1.1.3" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px1.p1.4.m4.1b"><apply id="S2.SS2.SSS1.Px1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.4.m4.1.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1.2">𝑟</ci><ci id="S2.SS2.SSS1.Px1.p1.4.m4.1.1.3.cmml" xref="S2.SS2.SSS1.Px1.p1.4.m4.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px1.p1.4.m4.1c">r_{j}</annotation></semantics></math> and <math id="S2.SS2.SSS1.Px1.p1.5.m5.1" class="ltx_Math" alttext="l_{j}" display="inline"><semantics id="S2.SS2.SSS1.Px1.p1.5.m5.1a"><msub id="S2.SS2.SSS1.Px1.p1.5.m5.1.1" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1.cmml"><mi id="S2.SS2.SSS1.Px1.p1.5.m5.1.1.2" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1.2.cmml">l</mi><mi id="S2.SS2.SSS1.Px1.p1.5.m5.1.1.3" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px1.p1.5.m5.1b"><apply id="S2.SS2.SSS1.Px1.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1.2">𝑙</ci><ci id="S2.SS2.SSS1.Px1.p1.5.m5.1.1.3.cmml" xref="S2.SS2.SSS1.Px1.p1.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px1.p1.5.m5.1c">l_{j}</annotation></semantics></math> are concatenated to form a position-sensitive object feature vector, which is further transformed to a lower dimension of <math id="S2.SS2.SSS1.Px1.p1.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.SSS1.Px1.p1.6.m6.1a"><mi id="S2.SS2.SSS1.Px1.p1.6.m6.1.1" xref="S2.SS2.SSS1.Px1.p1.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px1.p1.6.m6.1b"><ci id="S2.SS2.SSS1.Px1.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS1.Px1.p1.6.m6.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px1.p1.6.m6.1c">D</annotation></semantics></math> using a linear projection to ensure that it has the same vector dimension as that of token embeddings.</p>
</div>
<div id="S2.SS2.SSS1.Px1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.Px1.p2.1" class="ltx_p">Despite the superior performance obtained via region-based visual features, this kind of features suffer from several problems. Firstly, the region-based methods heavily rely on a pre-trained object detector, where the performance may be bounded by the capability of the object detector and its predefined visual vocabulary. Besides, only salient regions of image are used in region-based methods, where the global or background information may be missing.</p>
</div>
</section>
<section id="S2.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Grid Feature</h5>

<div id="S2.SS2.SSS1.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.Px2.p1.9" class="ltx_p">To address the limitations of region-based features like locality, some work such as PixelBERT <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>, E2E-VLP <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, Grid-VLP <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>
and <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> have been proposed to revisit grid-based convolutional features for multi-modal learning, skipping the expensive region-related steps. The advantage lies in that: 1) the grid-based feature allows to introduce more flexible architectural designs for vision and language tasks, which makes it possible to support end-to-end training and efficient online inference; 2) it operates on a full image instead of a collection of semantic regions, so it can better capture global information of an image such as the background information; 3) it does not rely on a pre-trained object detector with limited visual vocabulary. Specifically, starting from the raw image <math id="S2.SS2.SSS1.Px2.p1.1.m1.1" class="ltx_Math" alttext="I_{img}\in R^{3\times H_{0}\times W_{0}}" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.1.m1.1a"><mrow id="S2.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml"><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.2.cmml">I</mi><mrow id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.1" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.1a" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.4" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.4.cmml">g</mi></mrow></msub><mo id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.1" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.2.cmml">R</mi><mrow id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.cmml"><mn id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.1" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.2.cmml">H</mi><mn id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.3.cmml">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.1a" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.cmml"><mi id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.2" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.2.cmml">W</mi><mn id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.3" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.3.cmml">0</mn></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1"><in id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.1"></in><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.2">𝐼</ci><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3"><times id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.1"></times><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.2">𝑖</ci><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.3">𝑚</ci><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.4.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.2.3.4">𝑔</ci></apply></apply><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.2">𝑅</ci><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3"><times id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.2">3</cn><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.2">𝐻</ci><cn type="integer" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.3.3">0</cn></apply><apply id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.2.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.2">𝑊</ci><cn type="integer" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.3.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.3.3.4.3">0</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.1.m1.1c">I_{img}\in R^{3\times H_{0}\times W_{0}}</annotation></semantics></math> with 3 color channels, a fixed CNN-based image encoder such as ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> generates a lower-resolution activation map <math id="S2.SS2.SSS1.Px2.p1.2.m2.1" class="ltx_Math" alttext="F_{img}\in R^{C\times H\times W}" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.2.m2.1a"><mrow id="S2.SS2.SSS1.Px2.p1.2.m2.1.1" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.cmml"><msub id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.cmml"><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.2" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.2.cmml">F</mi><mrow id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.2" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.1" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.3" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.1a" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.4" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.4.cmml">g</mi></mrow></msub><mo id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.1" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.2" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.2.cmml">R</mi><mrow id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.2" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.1" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.3" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.1a" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.4" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.4.cmml">W</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.2.m2.1b"><apply id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1"><in id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.1"></in><apply id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.1.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.2.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.2">𝐹</ci><apply id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3"><times id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.1"></times><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.2">𝑖</ci><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.3">𝑚</ci><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.4.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.2.3.4">𝑔</ci></apply></apply><apply id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.2">𝑅</ci><apply id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3"><times id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.1"></times><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.2">𝐶</ci><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.3">𝐻</ci><ci id="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.4.cmml" xref="S2.SS2.SSS1.Px2.p1.2.m2.1.1.3.3.4">𝑊</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.2.m2.1c">F_{img}\in R^{C\times H\times W}</annotation></semantics></math>, where <math id="S2.SS2.SSS1.Px2.p1.3.m3.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.3.m3.1a"><mi id="S2.SS2.SSS1.Px2.p1.3.m3.1.1" xref="S2.SS2.SSS1.Px2.p1.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.3.m3.1b"><ci id="S2.SS2.SSS1.Px2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.3.m3.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.3.m3.1c">C</annotation></semantics></math> is the channel width and <math id="S2.SS2.SSS1.Px2.p1.4.m4.1" class="ltx_Math" alttext="H=\frac{H_{0}}{32}" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.4.m4.1a"><mrow id="S2.SS2.SSS1.Px2.p1.4.m4.1.1" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.2" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.2.cmml">H</mi><mo id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.1" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.1.cmml">=</mo><mfrac id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.cmml"><msub id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.cmml"><mi id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.2" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.2.cmml">H</mi><mn id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.3" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.3.cmml">0</mn></msub><mn id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.3" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.3.cmml">32</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.4.m4.1b"><apply id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1"><eq id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.1"></eq><ci id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.2">𝐻</ci><apply id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3"><divide id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3"></divide><apply id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.1.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.2.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.2">𝐻</ci><cn type="integer" id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.3.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.2.3">0</cn></apply><cn type="integer" id="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.4.m4.1.1.3.3">32</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.4.m4.1c">H=\frac{H_{0}}{32}</annotation></semantics></math>, <math id="S2.SS2.SSS1.Px2.p1.5.m5.1" class="ltx_Math" alttext="W=\frac{W_{0}}{32}" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.5.m5.1a"><mrow id="S2.SS2.SSS1.Px2.p1.5.m5.1.1" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.2" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.2.cmml">W</mi><mo id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.1" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.1.cmml">=</mo><mfrac id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.cmml"><msub id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.cmml"><mi id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.2" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.2.cmml">W</mi><mn id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.3" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.3.cmml">0</mn></msub><mn id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.3" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.3.cmml">32</mn></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.5.m5.1b"><apply id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1"><eq id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.1"></eq><ci id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.2">𝑊</ci><apply id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3"><divide id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3"></divide><apply id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.1.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.2.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.2">𝑊</ci><cn type="integer" id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.3.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.2.3">0</cn></apply><cn type="integer" id="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.5.m5.1.1.3.3">32</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.5.m5.1c">W=\frac{W_{0}}{32}</annotation></semantics></math>. As the cross-modal fusion network expects a sequence as input, the spatial dimensions of <math id="S2.SS2.SSS1.Px2.p1.6.m6.1" class="ltx_Math" alttext="F_{img}" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.6.m6.1a"><msub id="S2.SS2.SSS1.Px2.p1.6.m6.1.1" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.2" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.2.cmml">F</mi><mrow id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.2" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.1" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.3" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.1a" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.4" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.6.m6.1b"><apply id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.2">𝐹</ci><apply id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3"><times id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.1"></times><ci id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.2">𝑖</ci><ci id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.3">𝑚</ci><ci id="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.4.cmml" xref="S2.SS2.SSS1.Px2.p1.6.m6.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.6.m6.1c">F_{img}</annotation></semantics></math> are collapsed into one dimension, resulting in a <math id="S2.SS2.SSS1.Px2.p1.7.m7.1" class="ltx_Math" alttext="HW\times C" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.7.m7.1a"><mrow id="S2.SS2.SSS1.Px2.p1.7.m7.1.1" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.cmml"><mrow id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.cmml"><mi id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.2" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.1" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.3" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.3.cmml">W</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.1" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.1.cmml">×</mo><mi id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.3" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.7.m7.1b"><apply id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1"><times id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.1"></times><apply id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2"><times id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.1.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.1"></times><ci id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.2.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.2">𝐻</ci><ci id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.3.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.2.3">𝑊</ci></apply><ci id="S2.SS2.SSS1.Px2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.SSS1.Px2.p1.7.m7.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.7.m7.1c">HW\times C</annotation></semantics></math> feature map. Finally, a linear projection layer is used to reduce the channel dimension of the high-level feature map from <math id="S2.SS2.SSS1.Px2.p1.8.m8.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.8.m8.1a"><mi id="S2.SS2.SSS1.Px2.p1.8.m8.1.1" xref="S2.SS2.SSS1.Px2.p1.8.m8.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.8.m8.1b"><ci id="S2.SS2.SSS1.Px2.p1.8.m8.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.8.m8.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.8.m8.1c">C</annotation></semantics></math> to a smaller dimension <math id="S2.SS2.SSS1.Px2.p1.9.m9.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.SSS1.Px2.p1.9.m9.1a"><mi id="S2.SS2.SSS1.Px2.p1.9.m9.1.1" xref="S2.SS2.SSS1.Px2.p1.9.m9.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.9.m9.1b"><ci id="S2.SS2.SSS1.Px2.p1.9.m9.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.9.m9.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.9.m9.1c">D</annotation></semantics></math> for matching the dimension of token embeddings. To distinguish between different modalities, the grid feature map is supplemented with a learnable modal type embedding which is added to the output of linear projection layer.</p>
</div>
<div id="S2.SS2.SSS1.Px2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.Px2.p2.1" class="ltx_p">To generate good grid features, it is very important to pre-train a strong CNN-based image encoder, to which the visual semantic information is incorporated. In terms of the data used to pre-train the image encoder, there are mainly two ways along this line: 1) <span id="S2.SS2.SSS1.Px2.p2.1.1" class="ltx_text ltx_font_italic">Supervised Pre-training</span>: the image encoder is pre-trained with image classification data such as ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib21" title="" class="ltx_ref">2009b</a>)</cite> or detection data such as Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>. As found in <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>, the large-scale object and attribute annotations collected in the Visual Genome are very helpful to provide the grid feature with visual semantics incorporated; 2) <span id="S2.SS2.SSS1.Px2.p2.1.2" class="ltx_text ltx_font_italic">Unsupervised Pre-training</span>: the image encoder is pre-trained with a large amount of unlabeled image-text pairs without human supervision such as CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>, where about 400M aligned image-text pairs are used. It belongs to the line of research that learns visual representations from natural language supervision <cite class="ltx_cite ltx_citemacro_citep">(Jia et al., <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Desai and Johnson, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>. In this way, the image encoder is naturally aligned with the textual semantics to facilitate the cross-modal fusion. It is well recognized that fully supervised pre-trained CNN model shows promising performance on in-domain or near-domain datasets, while it may not yield best performance when coming to transfer learning on out-domain datasets. Features derived from different ways can well complement with each other, which adapts to different kinds of questions.</p>
</div>
</section>
<section id="S2.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Patch Feature</h5>

<div id="S2.SS2.SSS1.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.Px3.p1.7" class="ltx_p">Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite> has achieved outstanding performance in various visual tasks <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Touvron et al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Chen et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite>. It firstly splits an image into fixed-size patches, then uses a simple linear projection of a patch before feeding them into transformers. ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> is the first to explore patch-based features for multi-modal learning, and achieves up to dozens of times faster inference than previous region-based VLP methods. The advantages of patch-based features are following: 1) its simple framework can be more efficient than grid-based convolutional features in the online inference phrase; 2) it is more effective in capturing the global structure of a full image with the self-attention mechanism, which can provide complementary visual features different from region-based and grid-based ones. Specifically, the 2D image <math id="S2.SS2.SSS1.Px3.p1.1.m1.1" class="ltx_Math" alttext="I_{img}\in R^{3\times H_{0}\times W_{0}}" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.1.m1.1a"><mrow id="S2.SS2.SSS1.Px3.p1.1.m1.1.1" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.cmml"><msub id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.cmml"><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.2.cmml">I</mi><mrow id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.cmml"><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.1" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.1a" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.4" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.4.cmml">g</mi></mrow></msub><mo id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.cmml"><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.2.cmml">R</mi><mrow id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.cmml"><mn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.1" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.cmml"><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.2.cmml">H</mi><mn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.3.cmml">0</mn></msub><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.1a" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.cmml"><mi id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.2" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.2.cmml">W</mi><mn id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.3" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.3.cmml">0</mn></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.1.m1.1b"><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1"><in id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.1"></in><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.2">𝐼</ci><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3"><times id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.1"></times><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.2">𝑖</ci><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.3">𝑚</ci><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.4.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.2.3.4">𝑔</ci></apply></apply><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.2">𝑅</ci><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3"><times id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.1"></times><cn type="integer" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.2">3</cn><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.2">𝐻</ci><cn type="integer" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.3.3">0</cn></apply><apply id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.1.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4">subscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.2.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.2">𝑊</ci><cn type="integer" id="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.3.cmml" xref="S2.SS2.SSS1.Px3.p1.1.m1.1.1.3.3.4.3">0</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.1.m1.1c">I_{img}\in R^{3\times H_{0}\times W_{0}}</annotation></semantics></math> is reshaped into a sequence of flattened 2D patches <math id="S2.SS2.SSS1.Px3.p1.2.m2.1" class="ltx_Math" alttext="x_{p}\in R^{N\times(P^{2}\cdot C)}" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.2.m2.1a"><mrow id="S2.SS2.SSS1.Px3.p1.2.m2.1.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.cmml"><msub id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.cmml"><mi id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.2.cmml">x</mi><mi id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.3" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.3.cmml">p</mi></msub><mo id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.1" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.1.cmml">∈</mo><msup id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.cmml"><mi id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.2.cmml">R</mi><mrow id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.cmml"><mi id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.3" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.3.cmml">N</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.2.cmml">×</mo><mrow id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.cmml"><msup id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.cmml"><mi id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.2" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.2.cmml">P</mi><mn id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.3" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.1" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.1.cmml">⋅</mo><mi id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.3" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.3.cmml">C</mi></mrow><mo stretchy="false" id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.3" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.2.m2.1b"><apply id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2"><in id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.1"></in><apply id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.2">𝑥</ci><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.2.3">𝑝</ci></apply><apply id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3">superscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.2.3.2">𝑅</ci><apply id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1"><times id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.2"></times><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.3.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.3">𝑁</ci><apply id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1"><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.1">⋅</ci><apply id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.1.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2">superscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.2">𝑃</ci><cn type="integer" id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.2.3">2</cn></apply><ci id="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.Px3.p1.2.m2.1.1.1.1.1.1.3">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.2.m2.1c">x_{p}\in R^{N\times(P^{2}\cdot C)}</annotation></semantics></math>, where <math id="S2.SS2.SSS1.Px3.p1.3.m3.2" class="ltx_Math" alttext="(H_{0},W_{0})" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.3.m3.2a"><mrow id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.3.cmml"><mo stretchy="false" id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.3" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.3.cmml">(</mo><msub id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.cmml"><mi id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.2" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.2.cmml">H</mi><mn id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.3" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.4" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.3.cmml">,</mo><msub id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.cmml"><mi id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.2" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.2.cmml">W</mi><mn id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.3" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.3.cmml">0</mn></msub><mo stretchy="false" id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.5" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.3.m3.2b"><interval closure="open" id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2"><apply id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.2">𝐻</ci><cn type="integer" id="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.1.1.1.1.3">0</cn></apply><apply id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.2">𝑊</ci><cn type="integer" id="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.3.m3.2.2.2.2.3">0</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.3.m3.2c">(H_{0},W_{0})</annotation></semantics></math> is the resolution of the original image, <math id="S2.SS2.SSS1.Px3.p1.4.m4.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.4.m4.1a"><mi id="S2.SS2.SSS1.Px3.p1.4.m4.1.1" xref="S2.SS2.SSS1.Px3.p1.4.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.4.m4.1b"><ci id="S2.SS2.SSS1.Px3.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.4.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.4.m4.1c">C</annotation></semantics></math> is the number of channels,<math id="S2.SS2.SSS1.Px3.p1.5.m5.2" class="ltx_Math" alttext="(P,P)" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.5.m5.2a"><mrow id="S2.SS2.SSS1.Px3.p1.5.m5.2.3.2" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.3.1.cmml"><mo stretchy="false" id="S2.SS2.SSS1.Px3.p1.5.m5.2.3.2.1" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.3.1.cmml">(</mo><mi id="S2.SS2.SSS1.Px3.p1.5.m5.1.1" xref="S2.SS2.SSS1.Px3.p1.5.m5.1.1.cmml">P</mi><mo id="S2.SS2.SSS1.Px3.p1.5.m5.2.3.2.2" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.3.1.cmml">,</mo><mi id="S2.SS2.SSS1.Px3.p1.5.m5.2.2" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.2.cmml">P</mi><mo stretchy="false" id="S2.SS2.SSS1.Px3.p1.5.m5.2.3.2.3" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.5.m5.2b"><interval closure="open" id="S2.SS2.SSS1.Px3.p1.5.m5.2.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.3.2"><ci id="S2.SS2.SSS1.Px3.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.5.m5.1.1">𝑃</ci><ci id="S2.SS2.SSS1.Px3.p1.5.m5.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.5.m5.2.2">𝑃</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.5.m5.2c">(P,P)</annotation></semantics></math> is the resolution of each image patch, and <math id="S2.SS2.SSS1.Px3.p1.6.m6.1" class="ltx_Math" alttext="N=HW/P^{2}" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.6.m6.1a"><mrow id="S2.SS2.SSS1.Px3.p1.6.m6.1.1" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.cmml"><mi id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.2" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.2.cmml">N</mi><mo id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.1" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.1.cmml">=</mo><mrow id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.cmml"><mrow id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.cmml"><mi id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.2" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.1" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.1.cmml">​</mo><mi id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.3" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.3.cmml">W</mi></mrow><mo id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.1" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.1.cmml">/</mo><msup id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.cmml"><mi id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.2" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.2.cmml">P</mi><mn id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.3" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.3.cmml">2</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.6.m6.1b"><apply id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1"><eq id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.1"></eq><ci id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.2">𝑁</ci><apply id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3"><divide id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.1"></divide><apply id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2"><times id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.1.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.1"></times><ci id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.2.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.2">𝐻</ci><ci id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.3.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.2.3">𝑊</ci></apply><apply id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.1.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3">superscript</csymbol><ci id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.2.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.2">𝑃</ci><cn type="integer" id="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.3.cmml" xref="S2.SS2.SSS1.Px3.p1.6.m6.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.6.m6.1c">N=HW/P^{2}</annotation></semantics></math> is the resulting number of patches and also serves as the input sequence length for the Transformer. Then, the patches are flatten and embedded to <math id="S2.SS2.SSS1.Px3.p1.7.m7.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.SS2.SSS1.Px3.p1.7.m7.1a"><mi id="S2.SS2.SSS1.Px3.p1.7.m7.1.1" xref="S2.SS2.SSS1.Px3.p1.7.m7.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px3.p1.7.m7.1b"><ci id="S2.SS2.SSS1.Px3.p1.7.m7.1.1.cmml" xref="S2.SS2.SSS1.Px3.p1.7.m7.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px3.p1.7.m7.1c">D</annotation></semantics></math> dimensions with a trainable linear projectionm, and an appropriate position encoding is introduced to capture the geometric relationship among different patches. Finally, the sequence of patch embeddings serves as input of the visual transformer encoder to pretrain.</p>
</div>
<div id="S2.SS2.SSS1.Px3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.Px3.p2.1" class="ltx_p">With the rapid development of various ViT variants, there are also different ways to generate diverse patch features as in grid feature extraction: 1) <span id="S2.SS2.SSS1.Px3.p2.1.1" class="ltx_text ltx_font_italic">Supervised Pre-training</span>: the image patch encoder is pre-trained with image classification data such as in ViT <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite> or object detection data such as in Swin Transformer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>; 2) <span id="S2.SS2.SSS1.Px3.p2.1.2" class="ltx_text ltx_font_italic">Unsupervised Pre-training</span>: the image patch encoder is pre-trained with a large amount of unlabeled image-text pairs. For example, CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite> pretrains the image patch encoder of ViT with 400M aligned image-text pairs and <cite class="ltx_cite ltx_citemacro_citep">(Changpinyo et al., <a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite> provides a large dataset of 12M image-text pairs CC12M and conducts image-text pre-training to recognize long-tail visual concepts.</p>
</div>
</section>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Textual Features</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p1.5" class="ltx_p">This research work utilizes the method in BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> with the WordPiece tokenizer to tokenize the input text sentence into a sequence of sub-word tokens <math id="S2.SS2.SSS2.p1.1.m1.3" class="ltx_Math" alttext="\{w_{1},\cdots,w_{m}\}" display="inline"><semantics id="S2.SS2.SSS2.p1.1.m1.3a"><mrow id="S2.SS2.SSS2.p1.1.m1.3.3.2" xref="S2.SS2.SSS2.p1.1.m1.3.3.3.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.3.3.2.3" xref="S2.SS2.SSS2.p1.1.m1.3.3.3.cmml">{</mo><msub id="S2.SS2.SSS2.p1.1.m1.2.2.1.1" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.2" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.2.cmml">w</mi><mn id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.3" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS2.p1.1.m1.3.3.2.4" xref="S2.SS2.SSS2.p1.1.m1.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">⋯</mi><mo id="S2.SS2.SSS2.p1.1.m1.3.3.2.5" xref="S2.SS2.SSS2.p1.1.m1.3.3.3.cmml">,</mo><msub id="S2.SS2.SSS2.p1.1.m1.3.3.2.2" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.cmml"><mi id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml">w</mi><mi id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.3" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.3.cmml">m</mi></msub><mo stretchy="false" id="S2.SS2.SSS2.p1.1.m1.3.3.2.6" xref="S2.SS2.SSS2.p1.1.m1.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.3b"><set id="S2.SS2.SSS2.p1.1.m1.3.3.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2"><apply id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.2">𝑤</ci><cn type="integer" id="S2.SS2.SSS2.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.2.2.1.1.3">1</cn></apply><ci id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">⋯</ci><apply id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.2">𝑤</ci><ci id="S2.SS2.SSS2.p1.1.m1.3.3.2.2.3.cmml" xref="S2.SS2.SSS2.p1.1.m1.3.3.2.2.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.3c">\{w_{1},\cdots,w_{m}\}</annotation></semantics></math>. Then each token <math id="S2.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S2.SS2.SSS2.p1.2.m2.1a"><msub id="S2.SS2.SSS2.p1.2.m2.1.1" xref="S2.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS2.p1.2.m2.1.1.2" xref="S2.SS2.SSS2.p1.2.m2.1.1.2.cmml">w</mi><mi id="S2.SS2.SSS2.p1.2.m2.1.1.3" xref="S2.SS2.SSS2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.2.m2.1b"><apply id="S2.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1.2">𝑤</ci><ci id="S2.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.2.m2.1c">w_{i}</annotation></semantics></math> is assigned three kinds of learnable embeddings: token, modal type and position embeddings. The three embeddings are summed and layer-normalized to represent input sentence as a sequence of embedding vectors <math id="S2.SS2.SSS2.p1.3.m3.5" class="ltx_Math" alttext="E_{emb}=\{e_{CLS},e_{1},\cdots,e_{m},e_{SEP}\}" display="inline"><semantics id="S2.SS2.SSS2.p1.3.m3.5a"><mrow id="S2.SS2.SSS2.p1.3.m3.5.5" xref="S2.SS2.SSS2.p1.3.m3.5.5.cmml"><msub id="S2.SS2.SSS2.p1.3.m3.5.5.6" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.5.5.6.2" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.2.cmml">E</mi><mrow id="S2.SS2.SSS2.p1.3.m3.5.5.6.3" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.2" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.2.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.1" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.3" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.1a" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.4" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.4.cmml">b</mi></mrow></msub><mo id="S2.SS2.SSS2.p1.3.m3.5.5.5" xref="S2.SS2.SSS2.p1.3.m3.5.5.5.cmml">=</mo><mrow id="S2.SS2.SSS2.p1.3.m3.5.5.4.4" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.5" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml">{</mo><msub id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.2" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.2.cmml">e</mi><mrow id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.2" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.1" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.3" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.1a" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.4" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.4.cmml">S</mi></mrow></msub><mo id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.6" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.2" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.2.cmml">e</mi><mn id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.3" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.7" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml">,</mo><mi mathvariant="normal" id="S2.SS2.SSS2.p1.3.m3.1.1" xref="S2.SS2.SSS2.p1.3.m3.1.1.cmml">⋯</mi><mo id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.8" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.2" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.2.cmml">e</mi><mi id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.3" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.9" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml">,</mo><msub id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.2" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.2.cmml">e</mi><mrow id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.cmml"><mi id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.2" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.1" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.3" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.1a" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.4" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.4.cmml">P</mi></mrow></msub><mo stretchy="false" id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.10" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.3.m3.5b"><apply id="S2.SS2.SSS2.p1.3.m3.5.5.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5"><eq id="S2.SS2.SSS2.p1.3.m3.5.5.5.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.5"></eq><apply id="S2.SS2.SSS2.p1.3.m3.5.5.6.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.3.m3.5.5.6.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6">subscript</csymbol><ci id="S2.SS2.SSS2.p1.3.m3.5.5.6.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.2">𝐸</ci><apply id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3"><times id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.1"></times><ci id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.2">𝑒</ci><ci id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.3">𝑚</ci><ci id="S2.SS2.SSS2.p1.3.m3.5.5.6.3.4.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.6.3.4">𝑏</ci></apply></apply><set id="S2.SS2.SSS2.p1.3.m3.5.5.4.5.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4"><apply id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.2">𝑒</ci><apply id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3"><times id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.1"></times><ci id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.2">𝐶</ci><ci id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.3">𝐿</ci><ci id="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.4.cmml" xref="S2.SS2.SSS2.p1.3.m3.2.2.1.1.1.3.4">𝑆</ci></apply></apply><apply id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.2">𝑒</ci><cn type="integer" id="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.3.3.2.2.2.3">1</cn></apply><ci id="S2.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.1.1">⋯</ci><apply id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.2">𝑒</ci><ci id="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.4.4.3.3.3.3">𝑚</ci></apply><apply id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.2">𝑒</ci><apply id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3"><times id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.1.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.1"></times><ci id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.2.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.2">𝑆</ci><ci id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.3.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.3">𝐸</ci><ci id="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.4.cmml" xref="S2.SS2.SSS2.p1.3.m3.5.5.4.4.4.3.4">𝑃</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.3.m3.5c">E_{emb}=\{e_{CLS},e_{1},\cdots,e_{m},e_{SEP}\}</annotation></semantics></math>, where <math id="S2.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="[CLS]" display="inline"><semantics id="S2.SS2.SSS2.p1.4.m4.1a"><mrow id="S2.SS2.SSS2.p1.4.m4.1.1.1" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.4.m4.1.1.1.2" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.1.cmml">[</mo><mrow id="S2.SS2.SSS2.p1.4.m4.1.1.1.1" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.2" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.1" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.3" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.1a" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.4" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.4.cmml">S</mi></mrow><mo stretchy="false" id="S2.SS2.SSS2.p1.4.m4.1.1.1.3" xref="S2.SS2.SSS2.p1.4.m4.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.4.m4.1b"><apply id="S2.SS2.SSS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS2.p1.4.m4.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1"><times id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.1"></times><ci id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.2">𝐶</ci><ci id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.3">𝐿</ci><ci id="S2.SS2.SSS2.p1.4.m4.1.1.1.1.4.cmml" xref="S2.SS2.SSS2.p1.4.m4.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.4.m4.1c">[CLS]</annotation></semantics></math> and <math id="S2.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="[SEP]" display="inline"><semantics id="S2.SS2.SSS2.p1.5.m5.1a"><mrow id="S2.SS2.SSS2.p1.5.m5.1.1.1" xref="S2.SS2.SSS2.p1.5.m5.1.1.2.cmml"><mo stretchy="false" id="S2.SS2.SSS2.p1.5.m5.1.1.1.2" xref="S2.SS2.SSS2.p1.5.m5.1.1.2.1.cmml">[</mo><mrow id="S2.SS2.SSS2.p1.5.m5.1.1.1.1" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.cmml"><mi id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.2" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.1" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.3" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.1a" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.1.cmml">​</mo><mi id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.4" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.4.cmml">P</mi></mrow><mo stretchy="false" id="S2.SS2.SSS2.p1.5.m5.1.1.1.3" xref="S2.SS2.SSS2.p1.5.m5.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.5.m5.1b"><apply id="S2.SS2.SSS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1"><csymbol cd="latexml" id="S2.SS2.SSS2.p1.5.m5.1.1.2.1.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1"><times id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.1.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.1"></times><ci id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.2.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.2">𝑆</ci><ci id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.3.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.3">𝐸</ci><ci id="S2.SS2.SSS2.p1.5.m5.1.1.1.1.4.cmml" xref="S2.SS2.SSS2.p1.5.m5.1.1.1.1.4">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.5.m5.1c">[SEP]</annotation></semantics></math> are the special tokens in BERT.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">To provide better textual features, text stream parameters were initialized with three different pre-trained language models: BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> and StructBERT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite>. RoBERTa trains on a larger corpus for more steps, and StructBERT incorporates more word ordering and sentence ordering information into pre-training language structures.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Cross-Modal Interaction with Learning to Attend</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Vision-and-Language Pre-training (VLP)</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">The interaction of multiple modalities has always been treated as one of the most significant problems in cross-modality research. In VLP literature, there are two mainstream architectures to bridge the cross-modal feature gap: <span id="S2.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_italic">single-stream architecture</span> and <span id="S2.SS3.SSS1.p1.1.2" class="ltx_text ltx_font_italic">dual-stream architecture</span>. The former such as VL-BERT <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> and UNITER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> assume simple and clear underlying semantics behind the two modalities and thus simply concatenate image-region features and text features as input to a single Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite> network for early fusion in a straightforward manner. This paradigm learns the cross-modal semantic alignment from a bottom feature level by using the self-attention mechanism. Nevertheless, the design of single-stream structure treats both modality inputs equally, leaving the inherent different peculiarity of each modality not fully exploited. In contrast, the latter like LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> and ERNIE-ViL <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> first use separate Transformer encoders to learn high-level abstraction of image and sentence representation respectively, and then combine the two modalities together with a cross-modal Transformer. This kind of design explicitly distinguishes between different modality inputs and aligns the cross-modal representations at a higher semantic level, but is usually parameter-inefficient and may ignore fundamental feature-level association.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Learning to Attend</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib15" title="" class="ltx_ref">2021a</a>)</cite> propose SemVLP to learn the joint representation of vision and language, which aligns cross-modal semantics at multiple levels. It builds upon a shared Transformer encoder with specific self-attention masks for cross-modal interaction. However, the interaction between the two modalities are controlled by fixed self-attention mask with only two modes: interactive or non-interactive. Therefore, our VQA architecture uses two learnable self-attention weights for each layer to dynamically control the inter-modal and intra-modal interaction.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p2.1" class="ltx_p">In single-stream models, the input to a Transformer layer is the concatenation of both modalities, <math id="S2.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="X=[X_{L}|X_{V}]" display="inline"><semantics id="S2.SS3.SSS2.p2.1.m1.1a"><mrow id="S2.SS3.SSS2.p2.1.m1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p2.1.m1.1.1.3" xref="S2.SS3.SSS2.p2.1.m1.1.1.3.cmml">X</mi><mo id="S2.SS3.SSS2.p2.1.m1.1.1.2" xref="S2.SS3.SSS2.p2.1.m1.1.1.2.cmml">=</mo><mrow id="S2.SS3.SSS2.p2.1.m1.1.1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.2" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.2.1.cmml">[</mo><mrow id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.cmml"><msub id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.cmml"><mi id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.2" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml">X</mi><mi id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.3" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml">L</mi></msub><mo fence="false" id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.1.cmml">|</mo><msub id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.cmml"><mi id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.2" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.2.cmml">X</mi><mi id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.3" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.3.cmml">V</mi></msub></mrow><mo stretchy="false" id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.3" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.1.m1.1b"><apply id="S2.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1"><eq id="S2.SS3.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.2"></eq><ci id="S2.SS3.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.3">𝑋</ci><apply id="S2.SS3.SSS2.p2.1.m1.1.1.1.2.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.SSS2.p2.1.m1.1.1.1.2.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.2">𝑋</ci><ci id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.2.3">𝐿</ci></apply><apply id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.2.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.2">𝑋</ci><ci id="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.3.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1.1.1.1.3.3">𝑉</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.1.m1.1c">X=[X_{L}|X_{V}]</annotation></semantics></math>. As a result, in each single-stream attention head, the query representation is given by:</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para ltx_noindent">
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E5.m1.2" class="ltx_Math" alttext="\displaystyle Q=XW^{Q}=\left(\begin{array}[]{c}X_{L}\\
X_{V}\\
\end{array}\right)*W^{Q}=\left(\begin{array}[]{c}Q_{L}\\
Q_{V}\\
\end{array}\right)" display="inline"><semantics id="S2.E5.m1.2a"><mrow id="S2.E5.m1.2.3" xref="S2.E5.m1.2.3.cmml"><mi id="S2.E5.m1.2.3.2" xref="S2.E5.m1.2.3.2.cmml">Q</mi><mo id="S2.E5.m1.2.3.3" xref="S2.E5.m1.2.3.3.cmml">=</mo><mrow id="S2.E5.m1.2.3.4" xref="S2.E5.m1.2.3.4.cmml"><mi id="S2.E5.m1.2.3.4.2" xref="S2.E5.m1.2.3.4.2.cmml">X</mi><mo lspace="0em" rspace="0em" id="S2.E5.m1.2.3.4.1" xref="S2.E5.m1.2.3.4.1.cmml">​</mo><msup id="S2.E5.m1.2.3.4.3" xref="S2.E5.m1.2.3.4.3.cmml"><mi id="S2.E5.m1.2.3.4.3.2" xref="S2.E5.m1.2.3.4.3.2.cmml">W</mi><mi id="S2.E5.m1.2.3.4.3.3" xref="S2.E5.m1.2.3.4.3.3.cmml">Q</mi></msup></mrow><mo id="S2.E5.m1.2.3.5" xref="S2.E5.m1.2.3.5.cmml">=</mo><mrow id="S2.E5.m1.2.3.6" xref="S2.E5.m1.2.3.6.cmml"><mrow id="S2.E5.m1.2.3.6.2.2" xref="S2.E5.m1.1.1.cmml"><mo id="S2.E5.m1.2.3.6.2.2.1" xref="S2.E5.m1.1.1.cmml">(</mo><mtable rowspacing="0pt" id="S2.E5.m1.1.1" xref="S2.E5.m1.1.1.cmml"><mtr id="S2.E5.m1.1.1a" xref="S2.E5.m1.1.1.cmml"><mtd id="S2.E5.m1.1.1b" xref="S2.E5.m1.1.1.cmml"><msub id="S2.E1.1.1" xref="S2.E1.1.1.cmml"><mi id="S2.E1.1.1.2" xref="S2.E1.1.1.2.cmml">X</mi><mi id="S2.E1.1.1.3" xref="S2.E1.1.1.3.cmml">L</mi></msub></mtd></mtr><mtr id="S2.E5.m1.1.1c" xref="S2.E5.m1.1.1.cmml"><mtd id="S2.E5.m1.1.1d" xref="S2.E5.m1.1.1.cmml"><msub id="S2.E2.1.1" xref="S2.E2.1.1.cmml"><mi id="S2.E2.1.1.2" xref="S2.E2.1.1.2.cmml">X</mi><mi id="S2.E2.1.1.3" xref="S2.E2.1.1.3.cmml">V</mi></msub></mtd></mtr></mtable><mo rspace="0.055em" id="S2.E5.m1.2.3.6.2.2.2" xref="S2.E5.m1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S2.E5.m1.2.3.6.1" xref="S2.E5.m1.2.3.6.1.cmml">∗</mo><msup id="S2.E5.m1.2.3.6.3" xref="S2.E5.m1.2.3.6.3.cmml"><mi id="S2.E5.m1.2.3.6.3.2" xref="S2.E5.m1.2.3.6.3.2.cmml">W</mi><mi id="S2.E5.m1.2.3.6.3.3" xref="S2.E5.m1.2.3.6.3.3.cmml">Q</mi></msup></mrow><mo id="S2.E5.m1.2.3.7" xref="S2.E5.m1.2.3.7.cmml">=</mo><mrow id="S2.E5.m1.2.3.8.2" xref="S2.E5.m1.2.2.cmml"><mo id="S2.E5.m1.2.3.8.2.1" xref="S2.E5.m1.2.2.cmml">(</mo><mtable rowspacing="0pt" id="S2.E5.m1.2.2" xref="S2.E5.m1.2.2.cmml"><mtr id="S2.E5.m1.2.2a" xref="S2.E5.m1.2.2.cmml"><mtd id="S2.E5.m1.2.2b" xref="S2.E5.m1.2.2.cmml"><msub id="S2.E3.1.1" xref="S2.E3.1.1.cmml"><mi id="S2.E3.1.1.2" xref="S2.E3.1.1.2.cmml">Q</mi><mi id="S2.E3.1.1.3" xref="S2.E3.1.1.3.cmml">L</mi></msub></mtd></mtr><mtr id="S2.E5.m1.2.2c" xref="S2.E5.m1.2.2.cmml"><mtd id="S2.E5.m1.2.2d" xref="S2.E5.m1.2.2.cmml"><msub id="S2.E4.1.1" xref="S2.E4.1.1.cmml"><mi id="S2.E4.1.1.2" xref="S2.E4.1.1.2.cmml">Q</mi><mi id="S2.E4.1.1.3" xref="S2.E4.1.1.3.cmml">V</mi></msub></mtd></mtr></mtable><mo id="S2.E5.m1.2.3.8.2.2" xref="S2.E5.m1.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E5.m1.2b"><apply id="S2.E5.m1.2.3.cmml" xref="S2.E5.m1.2.3"><and id="S2.E5.m1.2.3a.cmml" xref="S2.E5.m1.2.3"></and><apply id="S2.E5.m1.2.3b.cmml" xref="S2.E5.m1.2.3"><eq id="S2.E5.m1.2.3.3.cmml" xref="S2.E5.m1.2.3.3"></eq><ci id="S2.E5.m1.2.3.2.cmml" xref="S2.E5.m1.2.3.2">𝑄</ci><apply id="S2.E5.m1.2.3.4.cmml" xref="S2.E5.m1.2.3.4"><times id="S2.E5.m1.2.3.4.1.cmml" xref="S2.E5.m1.2.3.4.1"></times><ci id="S2.E5.m1.2.3.4.2.cmml" xref="S2.E5.m1.2.3.4.2">𝑋</ci><apply id="S2.E5.m1.2.3.4.3.cmml" xref="S2.E5.m1.2.3.4.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.3.4.3.1.cmml" xref="S2.E5.m1.2.3.4.3">superscript</csymbol><ci id="S2.E5.m1.2.3.4.3.2.cmml" xref="S2.E5.m1.2.3.4.3.2">𝑊</ci><ci id="S2.E5.m1.2.3.4.3.3.cmml" xref="S2.E5.m1.2.3.4.3.3">𝑄</ci></apply></apply></apply><apply id="S2.E5.m1.2.3c.cmml" xref="S2.E5.m1.2.3"><eq id="S2.E5.m1.2.3.5.cmml" xref="S2.E5.m1.2.3.5"></eq><share href="#S2.E5.m1.2.3.4.cmml" id="S2.E5.m1.2.3d.cmml" xref="S2.E5.m1.2.3"></share><apply id="S2.E5.m1.2.3.6.cmml" xref="S2.E5.m1.2.3.6"><times id="S2.E5.m1.2.3.6.1.cmml" xref="S2.E5.m1.2.3.6.1"></times><matrix id="S2.E5.m1.1.1.cmml" xref="S2.E5.m1.2.3.6.2.2"><matrixrow id="S2.E5.m1.1.1a.cmml" xref="S2.E5.m1.2.3.6.2.2"><apply id="S2.E1.1.1.cmml" xref="S2.E1.1.1"><csymbol cd="ambiguous" id="S2.E1.1.1.1.cmml" xref="S2.E1.1.1">subscript</csymbol><ci id="S2.E1.1.1.2.cmml" xref="S2.E1.1.1.2">𝑋</ci><ci id="S2.E1.1.1.3.cmml" xref="S2.E1.1.1.3">𝐿</ci></apply></matrixrow><matrixrow id="S2.E5.m1.1.1b.cmml" xref="S2.E5.m1.2.3.6.2.2"><apply id="S2.E2.1.1.cmml" xref="S2.E2.1.1"><csymbol cd="ambiguous" id="S2.E2.1.1.1.cmml" xref="S2.E2.1.1">subscript</csymbol><ci id="S2.E2.1.1.2.cmml" xref="S2.E2.1.1.2">𝑋</ci><ci id="S2.E2.1.1.3.cmml" xref="S2.E2.1.1.3">𝑉</ci></apply></matrixrow></matrix><apply id="S2.E5.m1.2.3.6.3.cmml" xref="S2.E5.m1.2.3.6.3"><csymbol cd="ambiguous" id="S2.E5.m1.2.3.6.3.1.cmml" xref="S2.E5.m1.2.3.6.3">superscript</csymbol><ci id="S2.E5.m1.2.3.6.3.2.cmml" xref="S2.E5.m1.2.3.6.3.2">𝑊</ci><ci id="S2.E5.m1.2.3.6.3.3.cmml" xref="S2.E5.m1.2.3.6.3.3">𝑄</ci></apply></apply></apply><apply id="S2.E5.m1.2.3e.cmml" xref="S2.E5.m1.2.3"><eq id="S2.E5.m1.2.3.7.cmml" xref="S2.E5.m1.2.3.7"></eq><share href="#S2.E5.m1.2.3.6.cmml" id="S2.E5.m1.2.3f.cmml" xref="S2.E5.m1.2.3"></share><matrix id="S2.E5.m1.2.2.cmml" xref="S2.E5.m1.2.3.8.2"><matrixrow id="S2.E5.m1.2.2a.cmml" xref="S2.E5.m1.2.3.8.2"><apply id="S2.E3.1.1.cmml" xref="S2.E3.1.1"><csymbol cd="ambiguous" id="S2.E3.1.1.1.cmml" xref="S2.E3.1.1">subscript</csymbol><ci id="S2.E3.1.1.2.cmml" xref="S2.E3.1.1.2">𝑄</ci><ci id="S2.E3.1.1.3.cmml" xref="S2.E3.1.1.3">𝐿</ci></apply></matrixrow><matrixrow id="S2.E5.m1.2.2b.cmml" xref="S2.E5.m1.2.3.8.2"><apply id="S2.E4.1.1.cmml" xref="S2.E4.1.1"><csymbol cd="ambiguous" id="S2.E4.1.1.1.cmml" xref="S2.E4.1.1">subscript</csymbol><ci id="S2.E4.1.1.2.cmml" xref="S2.E4.1.1.2">𝑄</ci><ci id="S2.E4.1.1.3.cmml" xref="S2.E4.1.1.3">𝑉</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E5.m1.2c">\displaystyle Q=XW^{Q}=\left(\begin{array}[]{c}X_{L}\\
X_{V}\\
\end{array}\right)*W^{Q}=\left(\begin{array}[]{c}Q_{L}\\
Q_{V}\\
\end{array}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p4.2" class="ltx_p">where <math id="S2.SS3.SSS2.p4.1.m1.1" class="ltx_Math" alttext="\left(\begin{array}[]{c}\cdot L\\
\cdot V\\
\end{array}\right)" display="inline"><semantics id="S2.SS3.SSS2.p4.1.m1.1a"><mrow id="S2.SS3.SSS2.p4.1.m1.1.2.2" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml"><mo id="S2.SS3.SSS2.p4.1.m1.1.2.2.1" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml">(</mo><mtable rowspacing="0pt" id="S2.SS3.SSS2.p4.1.m1.1.1" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml"><mtr id="S2.SS3.SSS2.p4.1.m1.1.1a" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml"><mtd id="S2.SS3.SSS2.p4.1.m1.1.1b" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml"><mrow id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.2" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.1" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.1.cmml">⋅</mo><mi id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.3" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.3.cmml">L</mi></mrow></mtd></mtr><mtr id="S2.SS3.SSS2.p4.1.m1.1.1c" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml"><mtd id="S2.SS3.SSS2.p4.1.m1.1.1d" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml"><mrow id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.cmml"><mi id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.2" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.1" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.1.cmml">⋅</mo><mi id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.3" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.3.cmml">V</mi></mrow></mtd></mtr></mtable><mo id="S2.SS3.SSS2.p4.1.m1.1.2.2.2" xref="S2.SS3.SSS2.p4.1.m1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p4.1.m1.1b"><matrix id="S2.SS3.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.2.2"><matrixrow id="S2.SS3.SSS2.p4.1.m1.1.1a.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.2.2"><apply id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1"><ci id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.1">⋅</ci><csymbol cd="latexml" id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.2">absent</csymbol><ci id="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.1.1.1.3">𝐿</ci></apply></matrixrow><matrixrow id="S2.SS3.SSS2.p4.1.m1.1.1b.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.2.2"><apply id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1"><ci id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.1.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.1">⋅</ci><csymbol cd="latexml" id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.2.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.2">absent</csymbol><ci id="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.3.cmml" xref="S2.SS3.SSS2.p4.1.m1.1.1.2.1.1.3">𝑉</ci></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p4.1.m1.1c">\left(\begin{array}[]{c}\cdot L\\
\cdot V\\
\end{array}\right)</annotation></semantics></math> are the language and visual sub-matrices
of the input and the resulting output. As shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Reaching Human Parity ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the score matrix <math id="S2.SS3.SSS2.p4.2.m2.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS3.SSS2.p4.2.m2.1a"><mi id="S2.SS3.SSS2.p4.2.m2.1.1" xref="S2.SS3.SSS2.p4.2.m2.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p4.2.m2.1b"><ci id="S2.SS3.SSS2.p4.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p4.2.m2.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p4.2.m2.1c">S</annotation></semantics></math> can be defined in terms of four sub-matrices:</p>
</div>
<div id="S2.SS3.SSS2.p5" class="ltx_para ltx_noindent">
<table id="S5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E11.m1.3" class="ltx_Math" alttext="\displaystyle S=QK^{\mathrm{T}}=\left(\begin{array}[]{c}Q_{L}\\
Q_{V}\\
\end{array}\right)\left(\begin{array}[]{cc}K_{L}^{\mathrm{T}}&amp;K_{V}^{\mathrm{T}}\end{array}\right)=\left(\begin{array}[]{cc}S_{LL}&amp;S_{LV}\\
S_{VL}&amp;S_{VV}\\
\end{array}\right)" display="inline"><semantics id="S2.E11.m1.3a"><mrow id="S2.E11.m1.3.4" xref="S2.E11.m1.3.4.cmml"><mi id="S2.E11.m1.3.4.2" xref="S2.E11.m1.3.4.2.cmml">S</mi><mo id="S2.E11.m1.3.4.3" xref="S2.E11.m1.3.4.3.cmml">=</mo><mrow id="S2.E11.m1.3.4.4" xref="S2.E11.m1.3.4.4.cmml"><mi id="S2.E11.m1.3.4.4.2" xref="S2.E11.m1.3.4.4.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S2.E11.m1.3.4.4.1" xref="S2.E11.m1.3.4.4.1.cmml">​</mo><msup id="S2.E11.m1.3.4.4.3" xref="S2.E11.m1.3.4.4.3.cmml"><mi id="S2.E11.m1.3.4.4.3.2" xref="S2.E11.m1.3.4.4.3.2.cmml">K</mi><mi mathvariant="normal" id="S2.E11.m1.3.4.4.3.3" xref="S2.E11.m1.3.4.4.3.3.cmml">T</mi></msup></mrow><mo id="S2.E11.m1.3.4.5" xref="S2.E11.m1.3.4.5.cmml">=</mo><mrow id="S2.E11.m1.3.4.6" xref="S2.E11.m1.3.4.6.cmml"><mrow id="S2.E11.m1.3.4.6.2.2" xref="S2.E11.m1.1.1.cmml"><mo id="S2.E11.m1.3.4.6.2.2.1" xref="S2.E11.m1.1.1.cmml">(</mo><mtable rowspacing="0pt" id="S2.E11.m1.1.1" xref="S2.E11.m1.1.1.cmml"><mtr id="S2.E11.m1.1.1a" xref="S2.E11.m1.1.1.cmml"><mtd id="S2.E11.m1.1.1b" xref="S2.E11.m1.1.1.cmml"><msub id="S2.E6.1.1" xref="S2.E6.1.1.cmml"><mi id="S2.E6.1.1.2" xref="S2.E6.1.1.2.cmml">Q</mi><mi id="S2.E6.1.1.3" xref="S2.E6.1.1.3.cmml">L</mi></msub></mtd></mtr><mtr id="S2.E11.m1.1.1c" xref="S2.E11.m1.1.1.cmml"><mtd id="S2.E11.m1.1.1d" xref="S2.E11.m1.1.1.cmml"><msub id="S2.E7.1.1" xref="S2.E7.1.1.cmml"><mi id="S2.E7.1.1.2" xref="S2.E7.1.1.2.cmml">Q</mi><mi id="S2.E7.1.1.3" xref="S2.E7.1.1.3.cmml">V</mi></msub></mtd></mtr></mtable><mo id="S2.E11.m1.3.4.6.2.2.2" xref="S2.E11.m1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E11.m1.3.4.6.1" xref="S2.E11.m1.3.4.6.1.cmml">​</mo><mrow id="S2.E11.m1.3.4.6.3.2" xref="S2.E11.m1.2.2.cmml"><mo id="S2.E11.m1.3.4.6.3.2.1" xref="S2.E11.m1.2.2.cmml">(</mo><mtable columnspacing="5pt" id="S2.E11.m1.2.2" xref="S2.E11.m1.2.2.cmml"><mtr id="S2.E11.m1.2.2a" xref="S2.E11.m1.2.2.cmml"><mtd id="S2.E11.m1.2.2b" xref="S2.E11.m1.2.2.cmml"><msubsup id="S2.E8.1.1" xref="S2.E8.1.1.cmml"><mi id="S2.E8.1.1.2.2" xref="S2.E8.1.1.2.2.cmml">K</mi><mi id="S2.E8.1.1.2.3" xref="S2.E8.1.1.2.3.cmml">L</mi><mi mathvariant="normal" id="S2.E8.1.1.3" xref="S2.E8.1.1.3.cmml">T</mi></msubsup></mtd><mtd id="S2.E11.m1.2.2c" xref="S2.E11.m1.2.2.cmml"><msubsup id="S2.E8.2.1" xref="S2.E8.2.1.cmml"><mi id="S2.E8.2.1.2.2" xref="S2.E8.2.1.2.2.cmml">K</mi><mi id="S2.E8.2.1.2.3" xref="S2.E8.2.1.2.3.cmml">V</mi><mi mathvariant="normal" id="S2.E8.2.1.3" xref="S2.E8.2.1.3.cmml">T</mi></msubsup></mtd></mtr></mtable><mo id="S2.E11.m1.3.4.6.3.2.2" xref="S2.E11.m1.2.2.cmml">)</mo></mrow></mrow><mo id="S2.E11.m1.3.4.7" xref="S2.E11.m1.3.4.7.cmml">=</mo><mrow id="S2.E11.m1.3.4.8.2" xref="S2.E11.m1.3.3.cmml"><mo id="S2.E11.m1.3.4.8.2.1" xref="S2.E11.m1.3.3.cmml">(</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S2.E11.m1.3.3" xref="S2.E11.m1.3.3.cmml"><mtr id="S2.E11.m1.3.3a" xref="S2.E11.m1.3.3.cmml"><mtd id="S2.E11.m1.3.3b" xref="S2.E11.m1.3.3.cmml"><msub id="S2.E9.1.1" xref="S2.E9.1.1.cmml"><mi id="S2.E9.1.1.2" xref="S2.E9.1.1.2.cmml">S</mi><mrow id="S2.E9.1.1.3" xref="S2.E9.1.1.3.cmml"><mi id="S2.E9.1.1.3.2" xref="S2.E9.1.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E9.1.1.3.1" xref="S2.E9.1.1.3.1.cmml">​</mo><mi id="S2.E9.1.1.3.3" xref="S2.E9.1.1.3.3.cmml">L</mi></mrow></msub></mtd><mtd id="S2.E11.m1.3.3c" xref="S2.E11.m1.3.3.cmml"><msub id="S2.E9.2.1" xref="S2.E9.2.1.cmml"><mi id="S2.E9.2.1.2" xref="S2.E9.2.1.2.cmml">S</mi><mrow id="S2.E9.2.1.3" xref="S2.E9.2.1.3.cmml"><mi id="S2.E9.2.1.3.2" xref="S2.E9.2.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E9.2.1.3.1" xref="S2.E9.2.1.3.1.cmml">​</mo><mi id="S2.E9.2.1.3.3" xref="S2.E9.2.1.3.3.cmml">V</mi></mrow></msub></mtd></mtr><mtr id="S2.E11.m1.3.3d" xref="S2.E11.m1.3.3.cmml"><mtd id="S2.E11.m1.3.3e" xref="S2.E11.m1.3.3.cmml"><msub id="S2.E10.1.1" xref="S2.E10.1.1.cmml"><mi id="S2.E10.1.1.2" xref="S2.E10.1.1.2.cmml">S</mi><mrow id="S2.E10.1.1.3" xref="S2.E10.1.1.3.cmml"><mi id="S2.E10.1.1.3.2" xref="S2.E10.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S2.E10.1.1.3.1" xref="S2.E10.1.1.3.1.cmml">​</mo><mi id="S2.E10.1.1.3.3" xref="S2.E10.1.1.3.3.cmml">L</mi></mrow></msub></mtd><mtd id="S2.E11.m1.3.3f" xref="S2.E11.m1.3.3.cmml"><msub id="S2.E10.2.1" xref="S2.E10.2.1.cmml"><mi id="S2.E10.2.1.2" xref="S2.E10.2.1.2.cmml">S</mi><mrow id="S2.E10.2.1.3" xref="S2.E10.2.1.3.cmml"><mi id="S2.E10.2.1.3.2" xref="S2.E10.2.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S2.E10.2.1.3.1" xref="S2.E10.2.1.3.1.cmml">​</mo><mi id="S2.E10.2.1.3.3" xref="S2.E10.2.1.3.3.cmml">V</mi></mrow></msub></mtd></mtr></mtable><mo id="S2.E11.m1.3.4.8.2.2" xref="S2.E11.m1.3.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E11.m1.3b"><apply id="S2.E11.m1.3.4.cmml" xref="S2.E11.m1.3.4"><and id="S2.E11.m1.3.4a.cmml" xref="S2.E11.m1.3.4"></and><apply id="S2.E11.m1.3.4b.cmml" xref="S2.E11.m1.3.4"><eq id="S2.E11.m1.3.4.3.cmml" xref="S2.E11.m1.3.4.3"></eq><ci id="S2.E11.m1.3.4.2.cmml" xref="S2.E11.m1.3.4.2">𝑆</ci><apply id="S2.E11.m1.3.4.4.cmml" xref="S2.E11.m1.3.4.4"><times id="S2.E11.m1.3.4.4.1.cmml" xref="S2.E11.m1.3.4.4.1"></times><ci id="S2.E11.m1.3.4.4.2.cmml" xref="S2.E11.m1.3.4.4.2">𝑄</ci><apply id="S2.E11.m1.3.4.4.3.cmml" xref="S2.E11.m1.3.4.4.3"><csymbol cd="ambiguous" id="S2.E11.m1.3.4.4.3.1.cmml" xref="S2.E11.m1.3.4.4.3">superscript</csymbol><ci id="S2.E11.m1.3.4.4.3.2.cmml" xref="S2.E11.m1.3.4.4.3.2">𝐾</ci><ci id="S2.E11.m1.3.4.4.3.3.cmml" xref="S2.E11.m1.3.4.4.3.3">T</ci></apply></apply></apply><apply id="S2.E11.m1.3.4c.cmml" xref="S2.E11.m1.3.4"><eq id="S2.E11.m1.3.4.5.cmml" xref="S2.E11.m1.3.4.5"></eq><share href="#S2.E11.m1.3.4.4.cmml" id="S2.E11.m1.3.4d.cmml" xref="S2.E11.m1.3.4"></share><apply id="S2.E11.m1.3.4.6.cmml" xref="S2.E11.m1.3.4.6"><times id="S2.E11.m1.3.4.6.1.cmml" xref="S2.E11.m1.3.4.6.1"></times><matrix id="S2.E11.m1.1.1.cmml" xref="S2.E11.m1.3.4.6.2.2"><matrixrow id="S2.E11.m1.1.1a.cmml" xref="S2.E11.m1.3.4.6.2.2"><apply id="S2.E6.1.1.cmml" xref="S2.E6.1.1"><csymbol cd="ambiguous" id="S2.E6.1.1.1.cmml" xref="S2.E6.1.1">subscript</csymbol><ci id="S2.E6.1.1.2.cmml" xref="S2.E6.1.1.2">𝑄</ci><ci id="S2.E6.1.1.3.cmml" xref="S2.E6.1.1.3">𝐿</ci></apply></matrixrow><matrixrow id="S2.E11.m1.1.1b.cmml" xref="S2.E11.m1.3.4.6.2.2"><apply id="S2.E7.1.1.cmml" xref="S2.E7.1.1"><csymbol cd="ambiguous" id="S2.E7.1.1.1.cmml" xref="S2.E7.1.1">subscript</csymbol><ci id="S2.E7.1.1.2.cmml" xref="S2.E7.1.1.2">𝑄</ci><ci id="S2.E7.1.1.3.cmml" xref="S2.E7.1.1.3">𝑉</ci></apply></matrixrow></matrix><matrix id="S2.E11.m1.2.2.cmml" xref="S2.E11.m1.3.4.6.3.2"><matrixrow id="S2.E11.m1.2.2a.cmml" xref="S2.E11.m1.3.4.6.3.2"><apply id="S2.E8.1.1.cmml" xref="S2.E8.1.1"><csymbol cd="ambiguous" id="S2.E8.1.1.1.cmml" xref="S2.E8.1.1">superscript</csymbol><apply id="S2.E8.1.1.2.cmml" xref="S2.E8.1.1"><csymbol cd="ambiguous" id="S2.E8.1.1.2.1.cmml" xref="S2.E8.1.1">subscript</csymbol><ci id="S2.E8.1.1.2.2.cmml" xref="S2.E8.1.1.2.2">𝐾</ci><ci id="S2.E8.1.1.2.3.cmml" xref="S2.E8.1.1.2.3">𝐿</ci></apply><ci id="S2.E8.1.1.3.cmml" xref="S2.E8.1.1.3">T</ci></apply><apply id="S2.E8.2.1.cmml" xref="S2.E8.2.1"><csymbol cd="ambiguous" id="S2.E8.2.1.1.cmml" xref="S2.E8.2.1">superscript</csymbol><apply id="S2.E8.2.1.2.cmml" xref="S2.E8.2.1"><csymbol cd="ambiguous" id="S2.E8.2.1.2.1.cmml" xref="S2.E8.2.1">subscript</csymbol><ci id="S2.E8.2.1.2.2.cmml" xref="S2.E8.2.1.2.2">𝐾</ci><ci id="S2.E8.2.1.2.3.cmml" xref="S2.E8.2.1.2.3">𝑉</ci></apply><ci id="S2.E8.2.1.3.cmml" xref="S2.E8.2.1.3">T</ci></apply></matrixrow></matrix></apply></apply><apply id="S2.E11.m1.3.4e.cmml" xref="S2.E11.m1.3.4"><eq id="S2.E11.m1.3.4.7.cmml" xref="S2.E11.m1.3.4.7"></eq><share href="#S2.E11.m1.3.4.6.cmml" id="S2.E11.m1.3.4f.cmml" xref="S2.E11.m1.3.4"></share><matrix id="S2.E11.m1.3.3.cmml" xref="S2.E11.m1.3.4.8.2"><matrixrow id="S2.E11.m1.3.3a.cmml" xref="S2.E11.m1.3.4.8.2"><apply id="S2.E9.1.1.cmml" xref="S2.E9.1.1"><csymbol cd="ambiguous" id="S2.E9.1.1.1.cmml" xref="S2.E9.1.1">subscript</csymbol><ci id="S2.E9.1.1.2.cmml" xref="S2.E9.1.1.2">𝑆</ci><apply id="S2.E9.1.1.3.cmml" xref="S2.E9.1.1.3"><times id="S2.E9.1.1.3.1.cmml" xref="S2.E9.1.1.3.1"></times><ci id="S2.E9.1.1.3.2.cmml" xref="S2.E9.1.1.3.2">𝐿</ci><ci id="S2.E9.1.1.3.3.cmml" xref="S2.E9.1.1.3.3">𝐿</ci></apply></apply><apply id="S2.E9.2.1.cmml" xref="S2.E9.2.1"><csymbol cd="ambiguous" id="S2.E9.2.1.1.cmml" xref="S2.E9.2.1">subscript</csymbol><ci id="S2.E9.2.1.2.cmml" xref="S2.E9.2.1.2">𝑆</ci><apply id="S2.E9.2.1.3.cmml" xref="S2.E9.2.1.3"><times id="S2.E9.2.1.3.1.cmml" xref="S2.E9.2.1.3.1"></times><ci id="S2.E9.2.1.3.2.cmml" xref="S2.E9.2.1.3.2">𝐿</ci><ci id="S2.E9.2.1.3.3.cmml" xref="S2.E9.2.1.3.3">𝑉</ci></apply></apply></matrixrow><matrixrow id="S2.E11.m1.3.3b.cmml" xref="S2.E11.m1.3.4.8.2"><apply id="S2.E10.1.1.cmml" xref="S2.E10.1.1"><csymbol cd="ambiguous" id="S2.E10.1.1.1.cmml" xref="S2.E10.1.1">subscript</csymbol><ci id="S2.E10.1.1.2.cmml" xref="S2.E10.1.1.2">𝑆</ci><apply id="S2.E10.1.1.3.cmml" xref="S2.E10.1.1.3"><times id="S2.E10.1.1.3.1.cmml" xref="S2.E10.1.1.3.1"></times><ci id="S2.E10.1.1.3.2.cmml" xref="S2.E10.1.1.3.2">𝑉</ci><ci id="S2.E10.1.1.3.3.cmml" xref="S2.E10.1.1.3.3">𝐿</ci></apply></apply><apply id="S2.E10.2.1.cmml" xref="S2.E10.2.1"><csymbol cd="ambiguous" id="S2.E10.2.1.1.cmml" xref="S2.E10.2.1">subscript</csymbol><ci id="S2.E10.2.1.2.cmml" xref="S2.E10.2.1.2">𝑆</ci><apply id="S2.E10.2.1.3.cmml" xref="S2.E10.2.1.3"><times id="S2.E10.2.1.3.1.cmml" xref="S2.E10.2.1.3.1"></times><ci id="S2.E10.2.1.3.2.cmml" xref="S2.E10.2.1.3.2">𝑉</ci><ci id="S2.E10.2.1.3.3.cmml" xref="S2.E10.2.1.3.3">𝑉</ci></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E11.m1.3c">\displaystyle S=QK^{\mathrm{T}}=\left(\begin{array}[]{c}Q_{L}\\
Q_{V}\\
\end{array}\right)\left(\begin{array}[]{cc}K_{L}^{\mathrm{T}}&amp;K_{V}^{\mathrm{T}}\end{array}\right)=\left(\begin{array}[]{cc}S_{LL}&amp;S_{LV}\\
S_{VL}&amp;S_{VV}\\
\end{array}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.SSS2.p6" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p6.4" class="ltx_p">Then, two learnable self-attention weights <math id="S2.SS3.SSS2.p6.1.m1.1" class="ltx_Math" alttext="\varepsilon_{1}" display="inline"><semantics id="S2.SS3.SSS2.p6.1.m1.1a"><msub id="S2.SS3.SSS2.p6.1.m1.1.1" xref="S2.SS3.SSS2.p6.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p6.1.m1.1.1.2" xref="S2.SS3.SSS2.p6.1.m1.1.1.2.cmml">ε</mi><mn id="S2.SS3.SSS2.p6.1.m1.1.1.3" xref="S2.SS3.SSS2.p6.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p6.1.m1.1b"><apply id="S2.SS3.SSS2.p6.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p6.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p6.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p6.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p6.1.m1.1.1.2">𝜀</ci><cn type="integer" id="S2.SS3.SSS2.p6.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p6.1.m1.1c">\varepsilon_{1}</annotation></semantics></math> and <math id="S2.SS3.SSS2.p6.2.m2.1" class="ltx_Math" alttext="\varepsilon_{2}" display="inline"><semantics id="S2.SS3.SSS2.p6.2.m2.1a"><msub id="S2.SS3.SSS2.p6.2.m2.1.1" xref="S2.SS3.SSS2.p6.2.m2.1.1.cmml"><mi id="S2.SS3.SSS2.p6.2.m2.1.1.2" xref="S2.SS3.SSS2.p6.2.m2.1.1.2.cmml">ε</mi><mn id="S2.SS3.SSS2.p6.2.m2.1.1.3" xref="S2.SS3.SSS2.p6.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p6.2.m2.1b"><apply id="S2.SS3.SSS2.p6.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p6.2.m2.1.1.1.cmml" xref="S2.SS3.SSS2.p6.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p6.2.m2.1.1.2.cmml" xref="S2.SS3.SSS2.p6.2.m2.1.1.2">𝜀</ci><cn type="integer" id="S2.SS3.SSS2.p6.2.m2.1.1.3.cmml" xref="S2.SS3.SSS2.p6.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p6.2.m2.1c">\varepsilon_{2}</annotation></semantics></math> are introduced for intra-modal attention score sub-matrices (diagonal of <math id="S2.SS3.SSS2.p6.3.m3.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS3.SSS2.p6.3.m3.1a"><mi id="S2.SS3.SSS2.p6.3.m3.1.1" xref="S2.SS3.SSS2.p6.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p6.3.m3.1b"><ci id="S2.SS3.SSS2.p6.3.m3.1.1.cmml" xref="S2.SS3.SSS2.p6.3.m3.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p6.3.m3.1c">S</annotation></semantics></math>) and inter-modal attention score sub-matrices (anti-diagonal of <math id="S2.SS3.SSS2.p6.4.m4.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS3.SSS2.p6.4.m4.1a"><mi id="S2.SS3.SSS2.p6.4.m4.1.1" xref="S2.SS3.SSS2.p6.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p6.4.m4.1b"><ci id="S2.SS3.SSS2.p6.4.m4.1.1.cmml" xref="S2.SS3.SSS2.p6.4.m4.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p6.4.m4.1c">S</annotation></semantics></math>), respectively. In each Transformer layer, the learnable weights are multiplied by the attention score matrix to obtain the new attention score matrix:</p>
</div>
<div id="S2.SS3.SSS2.p7" class="ltx_para ltx_noindent">
<table id="S5.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E14.m1.1" class="ltx_Math" alttext="\displaystyle S_{\gamma}=\left(\begin{array}[]{cc}\varepsilon_{1}S_{LL}&amp;\varepsilon_{2}S_{LV}\\
\varepsilon_{2}S_{VL}&amp;\varepsilon_{1}S_{VV}\\
\end{array}\right)" display="inline"><semantics id="S2.E14.m1.1a"><mrow id="S2.E14.m1.1.2" xref="S2.E14.m1.1.2.cmml"><msub id="S2.E14.m1.1.2.2" xref="S2.E14.m1.1.2.2.cmml"><mi id="S2.E14.m1.1.2.2.2" xref="S2.E14.m1.1.2.2.2.cmml">S</mi><mi id="S2.E14.m1.1.2.2.3" xref="S2.E14.m1.1.2.2.3.cmml">γ</mi></msub><mo id="S2.E14.m1.1.2.1" xref="S2.E14.m1.1.2.1.cmml">=</mo><mrow id="S2.E14.m1.1.2.3.2" xref="S2.E14.m1.1.1.cmml"><mo id="S2.E14.m1.1.2.3.2.1" xref="S2.E14.m1.1.1.cmml">(</mo><mtable columnspacing="5pt" rowspacing="0pt" id="S2.E14.m1.1.1" xref="S2.E14.m1.1.1.cmml"><mtr id="S2.E14.m1.1.1a" xref="S2.E14.m1.1.1.cmml"><mtd id="S2.E14.m1.1.1b" xref="S2.E14.m1.1.1.cmml"><mrow id="S2.E12.1.1" xref="S2.E12.1.1.cmml"><msub id="S2.E12.1.1.2" xref="S2.E12.1.1.2.cmml"><mi id="S2.E12.1.1.2.2" xref="S2.E12.1.1.2.2.cmml">ε</mi><mn id="S2.E12.1.1.2.3" xref="S2.E12.1.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E12.1.1.1" xref="S2.E12.1.1.1.cmml">​</mo><msub id="S2.E12.1.1.3" xref="S2.E12.1.1.3.cmml"><mi id="S2.E12.1.1.3.2" xref="S2.E12.1.1.3.2.cmml">S</mi><mrow id="S2.E12.1.1.3.3" xref="S2.E12.1.1.3.3.cmml"><mi id="S2.E12.1.1.3.3.2" xref="S2.E12.1.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E12.1.1.3.3.1" xref="S2.E12.1.1.3.3.1.cmml">​</mo><mi id="S2.E12.1.1.3.3.3" xref="S2.E12.1.1.3.3.3.cmml">L</mi></mrow></msub></mrow></mtd><mtd id="S2.E14.m1.1.1c" xref="S2.E14.m1.1.1.cmml"><mrow id="S2.E12.2.1" xref="S2.E12.2.1.cmml"><msub id="S2.E12.2.1.2" xref="S2.E12.2.1.2.cmml"><mi id="S2.E12.2.1.2.2" xref="S2.E12.2.1.2.2.cmml">ε</mi><mn id="S2.E12.2.1.2.3" xref="S2.E12.2.1.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S2.E12.2.1.1" xref="S2.E12.2.1.1.cmml">​</mo><msub id="S2.E12.2.1.3" xref="S2.E12.2.1.3.cmml"><mi id="S2.E12.2.1.3.2" xref="S2.E12.2.1.3.2.cmml">S</mi><mrow id="S2.E12.2.1.3.3" xref="S2.E12.2.1.3.3.cmml"><mi id="S2.E12.2.1.3.3.2" xref="S2.E12.2.1.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E12.2.1.3.3.1" xref="S2.E12.2.1.3.3.1.cmml">​</mo><mi id="S2.E12.2.1.3.3.3" xref="S2.E12.2.1.3.3.3.cmml">V</mi></mrow></msub></mrow></mtd></mtr><mtr id="S2.E14.m1.1.1d" xref="S2.E14.m1.1.1.cmml"><mtd id="S2.E14.m1.1.1e" xref="S2.E14.m1.1.1.cmml"><mrow id="S2.E13.1.1" xref="S2.E13.1.1.cmml"><msub id="S2.E13.1.1.2" xref="S2.E13.1.1.2.cmml"><mi id="S2.E13.1.1.2.2" xref="S2.E13.1.1.2.2.cmml">ε</mi><mn id="S2.E13.1.1.2.3" xref="S2.E13.1.1.2.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S2.E13.1.1.1" xref="S2.E13.1.1.1.cmml">​</mo><msub id="S2.E13.1.1.3" xref="S2.E13.1.1.3.cmml"><mi id="S2.E13.1.1.3.2" xref="S2.E13.1.1.3.2.cmml">S</mi><mrow id="S2.E13.1.1.3.3" xref="S2.E13.1.1.3.3.cmml"><mi id="S2.E13.1.1.3.3.2" xref="S2.E13.1.1.3.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S2.E13.1.1.3.3.1" xref="S2.E13.1.1.3.3.1.cmml">​</mo><mi id="S2.E13.1.1.3.3.3" xref="S2.E13.1.1.3.3.3.cmml">L</mi></mrow></msub></mrow></mtd><mtd id="S2.E14.m1.1.1f" xref="S2.E14.m1.1.1.cmml"><mrow id="S2.E13.2.1" xref="S2.E13.2.1.cmml"><msub id="S2.E13.2.1.2" xref="S2.E13.2.1.2.cmml"><mi id="S2.E13.2.1.2.2" xref="S2.E13.2.1.2.2.cmml">ε</mi><mn id="S2.E13.2.1.2.3" xref="S2.E13.2.1.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E13.2.1.1" xref="S2.E13.2.1.1.cmml">​</mo><msub id="S2.E13.2.1.3" xref="S2.E13.2.1.3.cmml"><mi id="S2.E13.2.1.3.2" xref="S2.E13.2.1.3.2.cmml">S</mi><mrow id="S2.E13.2.1.3.3" xref="S2.E13.2.1.3.3.cmml"><mi id="S2.E13.2.1.3.3.2" xref="S2.E13.2.1.3.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S2.E13.2.1.3.3.1" xref="S2.E13.2.1.3.3.1.cmml">​</mo><mi id="S2.E13.2.1.3.3.3" xref="S2.E13.2.1.3.3.3.cmml">V</mi></mrow></msub></mrow></mtd></mtr></mtable><mo id="S2.E14.m1.1.2.3.2.2" xref="S2.E14.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E14.m1.1b"><apply id="S2.E14.m1.1.2.cmml" xref="S2.E14.m1.1.2"><eq id="S2.E14.m1.1.2.1.cmml" xref="S2.E14.m1.1.2.1"></eq><apply id="S2.E14.m1.1.2.2.cmml" xref="S2.E14.m1.1.2.2"><csymbol cd="ambiguous" id="S2.E14.m1.1.2.2.1.cmml" xref="S2.E14.m1.1.2.2">subscript</csymbol><ci id="S2.E14.m1.1.2.2.2.cmml" xref="S2.E14.m1.1.2.2.2">𝑆</ci><ci id="S2.E14.m1.1.2.2.3.cmml" xref="S2.E14.m1.1.2.2.3">𝛾</ci></apply><matrix id="S2.E14.m1.1.1.cmml" xref="S2.E14.m1.1.2.3.2"><matrixrow id="S2.E14.m1.1.1a.cmml" xref="S2.E14.m1.1.2.3.2"><apply id="S2.E12.1.1.cmml" xref="S2.E12.1.1"><times id="S2.E12.1.1.1.cmml" xref="S2.E12.1.1.1"></times><apply id="S2.E12.1.1.2.cmml" xref="S2.E12.1.1.2"><csymbol cd="ambiguous" id="S2.E12.1.1.2.1.cmml" xref="S2.E12.1.1.2">subscript</csymbol><ci id="S2.E12.1.1.2.2.cmml" xref="S2.E12.1.1.2.2">𝜀</ci><cn type="integer" id="S2.E12.1.1.2.3.cmml" xref="S2.E12.1.1.2.3">1</cn></apply><apply id="S2.E12.1.1.3.cmml" xref="S2.E12.1.1.3"><csymbol cd="ambiguous" id="S2.E12.1.1.3.1.cmml" xref="S2.E12.1.1.3">subscript</csymbol><ci id="S2.E12.1.1.3.2.cmml" xref="S2.E12.1.1.3.2">𝑆</ci><apply id="S2.E12.1.1.3.3.cmml" xref="S2.E12.1.1.3.3"><times id="S2.E12.1.1.3.3.1.cmml" xref="S2.E12.1.1.3.3.1"></times><ci id="S2.E12.1.1.3.3.2.cmml" xref="S2.E12.1.1.3.3.2">𝐿</ci><ci id="S2.E12.1.1.3.3.3.cmml" xref="S2.E12.1.1.3.3.3">𝐿</ci></apply></apply></apply><apply id="S2.E12.2.1.cmml" xref="S2.E12.2.1"><times id="S2.E12.2.1.1.cmml" xref="S2.E12.2.1.1"></times><apply id="S2.E12.2.1.2.cmml" xref="S2.E12.2.1.2"><csymbol cd="ambiguous" id="S2.E12.2.1.2.1.cmml" xref="S2.E12.2.1.2">subscript</csymbol><ci id="S2.E12.2.1.2.2.cmml" xref="S2.E12.2.1.2.2">𝜀</ci><cn type="integer" id="S2.E12.2.1.2.3.cmml" xref="S2.E12.2.1.2.3">2</cn></apply><apply id="S2.E12.2.1.3.cmml" xref="S2.E12.2.1.3"><csymbol cd="ambiguous" id="S2.E12.2.1.3.1.cmml" xref="S2.E12.2.1.3">subscript</csymbol><ci id="S2.E12.2.1.3.2.cmml" xref="S2.E12.2.1.3.2">𝑆</ci><apply id="S2.E12.2.1.3.3.cmml" xref="S2.E12.2.1.3.3"><times id="S2.E12.2.1.3.3.1.cmml" xref="S2.E12.2.1.3.3.1"></times><ci id="S2.E12.2.1.3.3.2.cmml" xref="S2.E12.2.1.3.3.2">𝐿</ci><ci id="S2.E12.2.1.3.3.3.cmml" xref="S2.E12.2.1.3.3.3">𝑉</ci></apply></apply></apply></matrixrow><matrixrow id="S2.E14.m1.1.1b.cmml" xref="S2.E14.m1.1.2.3.2"><apply id="S2.E13.1.1.cmml" xref="S2.E13.1.1"><times id="S2.E13.1.1.1.cmml" xref="S2.E13.1.1.1"></times><apply id="S2.E13.1.1.2.cmml" xref="S2.E13.1.1.2"><csymbol cd="ambiguous" id="S2.E13.1.1.2.1.cmml" xref="S2.E13.1.1.2">subscript</csymbol><ci id="S2.E13.1.1.2.2.cmml" xref="S2.E13.1.1.2.2">𝜀</ci><cn type="integer" id="S2.E13.1.1.2.3.cmml" xref="S2.E13.1.1.2.3">2</cn></apply><apply id="S2.E13.1.1.3.cmml" xref="S2.E13.1.1.3"><csymbol cd="ambiguous" id="S2.E13.1.1.3.1.cmml" xref="S2.E13.1.1.3">subscript</csymbol><ci id="S2.E13.1.1.3.2.cmml" xref="S2.E13.1.1.3.2">𝑆</ci><apply id="S2.E13.1.1.3.3.cmml" xref="S2.E13.1.1.3.3"><times id="S2.E13.1.1.3.3.1.cmml" xref="S2.E13.1.1.3.3.1"></times><ci id="S2.E13.1.1.3.3.2.cmml" xref="S2.E13.1.1.3.3.2">𝑉</ci><ci id="S2.E13.1.1.3.3.3.cmml" xref="S2.E13.1.1.3.3.3">𝐿</ci></apply></apply></apply><apply id="S2.E13.2.1.cmml" xref="S2.E13.2.1"><times id="S2.E13.2.1.1.cmml" xref="S2.E13.2.1.1"></times><apply id="S2.E13.2.1.2.cmml" xref="S2.E13.2.1.2"><csymbol cd="ambiguous" id="S2.E13.2.1.2.1.cmml" xref="S2.E13.2.1.2">subscript</csymbol><ci id="S2.E13.2.1.2.2.cmml" xref="S2.E13.2.1.2.2">𝜀</ci><cn type="integer" id="S2.E13.2.1.2.3.cmml" xref="S2.E13.2.1.2.3">1</cn></apply><apply id="S2.E13.2.1.3.cmml" xref="S2.E13.2.1.3"><csymbol cd="ambiguous" id="S2.E13.2.1.3.1.cmml" xref="S2.E13.2.1.3">subscript</csymbol><ci id="S2.E13.2.1.3.2.cmml" xref="S2.E13.2.1.3.2">𝑆</ci><apply id="S2.E13.2.1.3.3.cmml" xref="S2.E13.2.1.3.3"><times id="S2.E13.2.1.3.3.1.cmml" xref="S2.E13.2.1.3.3.1"></times><ci id="S2.E13.2.1.3.3.2.cmml" xref="S2.E13.2.1.3.3.2">𝑉</ci><ci id="S2.E13.2.1.3.3.3.cmml" xref="S2.E13.2.1.3.3.3">𝑉</ci></apply></apply></apply></matrixrow></matrix></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E14.m1.1c">\displaystyle S_{\gamma}=\left(\begin{array}[]{cc}\varepsilon_{1}S_{LL}&amp;\varepsilon_{2}S_{LV}\\
\varepsilon_{2}S_{VL}&amp;\varepsilon_{1}S_{VV}\\
\end{array}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.SSS2.p8" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS2.p8.2" class="ltx_p">The following two methods are investigated to learn the self-attention weights <math id="S2.SS3.SSS2.p8.1.m1.1" class="ltx_Math" alttext="\varepsilon_{1}" display="inline"><semantics id="S2.SS3.SSS2.p8.1.m1.1a"><msub id="S2.SS3.SSS2.p8.1.m1.1.1" xref="S2.SS3.SSS2.p8.1.m1.1.1.cmml"><mi id="S2.SS3.SSS2.p8.1.m1.1.1.2" xref="S2.SS3.SSS2.p8.1.m1.1.1.2.cmml">ε</mi><mn id="S2.SS3.SSS2.p8.1.m1.1.1.3" xref="S2.SS3.SSS2.p8.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p8.1.m1.1b"><apply id="S2.SS3.SSS2.p8.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p8.1.m1.1.1.1.cmml" xref="S2.SS3.SSS2.p8.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p8.1.m1.1.1.2.cmml" xref="S2.SS3.SSS2.p8.1.m1.1.1.2">𝜀</ci><cn type="integer" id="S2.SS3.SSS2.p8.1.m1.1.1.3.cmml" xref="S2.SS3.SSS2.p8.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p8.1.m1.1c">\varepsilon_{1}</annotation></semantics></math> and <math id="S2.SS3.SSS2.p8.2.m2.1" class="ltx_Math" alttext="\varepsilon_{2}" display="inline"><semantics id="S2.SS3.SSS2.p8.2.m2.1a"><msub id="S2.SS3.SSS2.p8.2.m2.1.1" xref="S2.SS3.SSS2.p8.2.m2.1.1.cmml"><mi id="S2.SS3.SSS2.p8.2.m2.1.1.2" xref="S2.SS3.SSS2.p8.2.m2.1.1.2.cmml">ε</mi><mn id="S2.SS3.SSS2.p8.2.m2.1.1.3" xref="S2.SS3.SSS2.p8.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p8.2.m2.1b"><apply id="S2.SS3.SSS2.p8.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS2.p8.2.m2.1.1.1.cmml" xref="S2.SS3.SSS2.p8.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.SSS2.p8.2.m2.1.1.2.cmml" xref="S2.SS3.SSS2.p8.2.m2.1.1.2">𝜀</ci><cn type="integer" id="S2.SS3.SSS2.p8.2.m2.1.1.3.cmml" xref="S2.SS3.SSS2.p8.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p8.2.m2.1c">\varepsilon_{2}</annotation></semantics></math>:</p>
<ol id="S2.I2" class="ltx_enumerate">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i1.p1.2" class="ltx_p">The weights are derived from a single-layer feed-forward network with the sigmoid activation function. <math id="S2.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="V_{CLS}" display="inline"><semantics id="S2.I2.i1.p1.1.m1.1a"><msub id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml"><mi id="S2.I2.i1.p1.1.m1.1.1.2" xref="S2.I2.i1.p1.1.m1.1.1.2.cmml">V</mi><mrow id="S2.I2.i1.p1.1.m1.1.1.3" xref="S2.I2.i1.p1.1.m1.1.1.3.cmml"><mi id="S2.I2.i1.p1.1.m1.1.1.3.2" xref="S2.I2.i1.p1.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.3.1" xref="S2.I2.i1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.I2.i1.p1.1.m1.1.1.3.3" xref="S2.I2.i1.p1.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.1.m1.1.1.3.1a" xref="S2.I2.i1.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.I2.i1.p1.1.m1.1.1.3.4" xref="S2.I2.i1.p1.1.m1.1.1.3.4.cmml">S</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.1b"><apply id="S2.I2.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i1.p1.1.m1.1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i1.p1.1.m1.1.1.2.cmml" xref="S2.I2.i1.p1.1.m1.1.1.2">𝑉</ci><apply id="S2.I2.i1.p1.1.m1.1.1.3.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3"><times id="S2.I2.i1.p1.1.m1.1.1.3.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3.1"></times><ci id="S2.I2.i1.p1.1.m1.1.1.3.2.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3.2">𝐶</ci><ci id="S2.I2.i1.p1.1.m1.1.1.3.3.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3.3">𝐿</ci><ci id="S2.I2.i1.p1.1.m1.1.1.3.4.cmml" xref="S2.I2.i1.p1.1.m1.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.1c">V_{CLS}</annotation></semantics></math> (the representation of <math id="S2.I2.i1.p1.2.m2.1" class="ltx_Math" alttext="[CLS]" display="inline"><semantics id="S2.I2.i1.p1.2.m2.1a"><mrow id="S2.I2.i1.p1.2.m2.1.1.1" xref="S2.I2.i1.p1.2.m2.1.1.2.cmml"><mo stretchy="false" id="S2.I2.i1.p1.2.m2.1.1.1.2" xref="S2.I2.i1.p1.2.m2.1.1.2.1.cmml">[</mo><mrow id="S2.I2.i1.p1.2.m2.1.1.1.1" xref="S2.I2.i1.p1.2.m2.1.1.1.1.cmml"><mi id="S2.I2.i1.p1.2.m2.1.1.1.1.2" xref="S2.I2.i1.p1.2.m2.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.2.m2.1.1.1.1.1" xref="S2.I2.i1.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S2.I2.i1.p1.2.m2.1.1.1.1.3" xref="S2.I2.i1.p1.2.m2.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.I2.i1.p1.2.m2.1.1.1.1.1a" xref="S2.I2.i1.p1.2.m2.1.1.1.1.1.cmml">​</mo><mi id="S2.I2.i1.p1.2.m2.1.1.1.1.4" xref="S2.I2.i1.p1.2.m2.1.1.1.1.4.cmml">S</mi></mrow><mo stretchy="false" id="S2.I2.i1.p1.2.m2.1.1.1.3" xref="S2.I2.i1.p1.2.m2.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.2.m2.1b"><apply id="S2.I2.i1.p1.2.m2.1.1.2.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S2.I2.i1.p1.2.m2.1.1.2.1.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1.2">delimited-[]</csymbol><apply id="S2.I2.i1.p1.2.m2.1.1.1.1.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1.1"><times id="S2.I2.i1.p1.2.m2.1.1.1.1.1.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1.1.1"></times><ci id="S2.I2.i1.p1.2.m2.1.1.1.1.2.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1.1.2">𝐶</ci><ci id="S2.I2.i1.p1.2.m2.1.1.1.1.3.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1.1.3">𝐿</ci><ci id="S2.I2.i1.p1.2.m2.1.1.1.1.4.cmml" xref="S2.I2.i1.p1.2.m2.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.2.m2.1c">[CLS]</annotation></semantics></math>) is used as the input feature to reflect how well an image matches with text. It gives a useful signal to measure intra-modal and inter-modal interaction.</p>
<table id="S5.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E15.m1.3" class="ltx_Math" alttext="\displaystyle(\varepsilon^{1},\varepsilon^{2})=FFN(V_{CLS})" display="inline"><semantics id="S2.E15.m1.3a"><mrow id="S2.E15.m1.3.3" xref="S2.E15.m1.3.3.cmml"><mrow id="S2.E15.m1.2.2.2.2" xref="S2.E15.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.E15.m1.2.2.2.2.3" xref="S2.E15.m1.2.2.2.3.cmml">(</mo><msup id="S2.E15.m1.1.1.1.1.1" xref="S2.E15.m1.1.1.1.1.1.cmml"><mi id="S2.E15.m1.1.1.1.1.1.2" xref="S2.E15.m1.1.1.1.1.1.2.cmml">ε</mi><mn id="S2.E15.m1.1.1.1.1.1.3" xref="S2.E15.m1.1.1.1.1.1.3.cmml">1</mn></msup><mo id="S2.E15.m1.2.2.2.2.4" xref="S2.E15.m1.2.2.2.3.cmml">,</mo><msup id="S2.E15.m1.2.2.2.2.2" xref="S2.E15.m1.2.2.2.2.2.cmml"><mi id="S2.E15.m1.2.2.2.2.2.2" xref="S2.E15.m1.2.2.2.2.2.2.cmml">ε</mi><mn id="S2.E15.m1.2.2.2.2.2.3" xref="S2.E15.m1.2.2.2.2.2.3.cmml">2</mn></msup><mo stretchy="false" id="S2.E15.m1.2.2.2.2.5" xref="S2.E15.m1.2.2.2.3.cmml">)</mo></mrow><mo id="S2.E15.m1.3.3.4" xref="S2.E15.m1.3.3.4.cmml">=</mo><mrow id="S2.E15.m1.3.3.3" xref="S2.E15.m1.3.3.3.cmml"><mi id="S2.E15.m1.3.3.3.3" xref="S2.E15.m1.3.3.3.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E15.m1.3.3.3.2" xref="S2.E15.m1.3.3.3.2.cmml">​</mo><mi id="S2.E15.m1.3.3.3.4" xref="S2.E15.m1.3.3.3.4.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E15.m1.3.3.3.2a" xref="S2.E15.m1.3.3.3.2.cmml">​</mo><mi id="S2.E15.m1.3.3.3.5" xref="S2.E15.m1.3.3.3.5.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.E15.m1.3.3.3.2b" xref="S2.E15.m1.3.3.3.2.cmml">​</mo><mrow id="S2.E15.m1.3.3.3.1.1" xref="S2.E15.m1.3.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.E15.m1.3.3.3.1.1.2" xref="S2.E15.m1.3.3.3.1.1.1.cmml">(</mo><msub id="S2.E15.m1.3.3.3.1.1.1" xref="S2.E15.m1.3.3.3.1.1.1.cmml"><mi id="S2.E15.m1.3.3.3.1.1.1.2" xref="S2.E15.m1.3.3.3.1.1.1.2.cmml">V</mi><mrow id="S2.E15.m1.3.3.3.1.1.1.3" xref="S2.E15.m1.3.3.3.1.1.1.3.cmml"><mi id="S2.E15.m1.3.3.3.1.1.1.3.2" xref="S2.E15.m1.3.3.3.1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.E15.m1.3.3.3.1.1.1.3.1" xref="S2.E15.m1.3.3.3.1.1.1.3.1.cmml">​</mo><mi id="S2.E15.m1.3.3.3.1.1.1.3.3" xref="S2.E15.m1.3.3.3.1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E15.m1.3.3.3.1.1.1.3.1a" xref="S2.E15.m1.3.3.3.1.1.1.3.1.cmml">​</mo><mi id="S2.E15.m1.3.3.3.1.1.1.3.4" xref="S2.E15.m1.3.3.3.1.1.1.3.4.cmml">S</mi></mrow></msub><mo stretchy="false" id="S2.E15.m1.3.3.3.1.1.3" xref="S2.E15.m1.3.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E15.m1.3b"><apply id="S2.E15.m1.3.3.cmml" xref="S2.E15.m1.3.3"><eq id="S2.E15.m1.3.3.4.cmml" xref="S2.E15.m1.3.3.4"></eq><interval closure="open" id="S2.E15.m1.2.2.2.3.cmml" xref="S2.E15.m1.2.2.2.2"><apply id="S2.E15.m1.1.1.1.1.1.cmml" xref="S2.E15.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E15.m1.1.1.1.1.1.1.cmml" xref="S2.E15.m1.1.1.1.1.1">superscript</csymbol><ci id="S2.E15.m1.1.1.1.1.1.2.cmml" xref="S2.E15.m1.1.1.1.1.1.2">𝜀</ci><cn type="integer" id="S2.E15.m1.1.1.1.1.1.3.cmml" xref="S2.E15.m1.1.1.1.1.1.3">1</cn></apply><apply id="S2.E15.m1.2.2.2.2.2.cmml" xref="S2.E15.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E15.m1.2.2.2.2.2.1.cmml" xref="S2.E15.m1.2.2.2.2.2">superscript</csymbol><ci id="S2.E15.m1.2.2.2.2.2.2.cmml" xref="S2.E15.m1.2.2.2.2.2.2">𝜀</ci><cn type="integer" id="S2.E15.m1.2.2.2.2.2.3.cmml" xref="S2.E15.m1.2.2.2.2.2.3">2</cn></apply></interval><apply id="S2.E15.m1.3.3.3.cmml" xref="S2.E15.m1.3.3.3"><times id="S2.E15.m1.3.3.3.2.cmml" xref="S2.E15.m1.3.3.3.2"></times><ci id="S2.E15.m1.3.3.3.3.cmml" xref="S2.E15.m1.3.3.3.3">𝐹</ci><ci id="S2.E15.m1.3.3.3.4.cmml" xref="S2.E15.m1.3.3.3.4">𝐹</ci><ci id="S2.E15.m1.3.3.3.5.cmml" xref="S2.E15.m1.3.3.3.5">𝑁</ci><apply id="S2.E15.m1.3.3.3.1.1.1.cmml" xref="S2.E15.m1.3.3.3.1.1"><csymbol cd="ambiguous" id="S2.E15.m1.3.3.3.1.1.1.1.cmml" xref="S2.E15.m1.3.3.3.1.1">subscript</csymbol><ci id="S2.E15.m1.3.3.3.1.1.1.2.cmml" xref="S2.E15.m1.3.3.3.1.1.1.2">𝑉</ci><apply id="S2.E15.m1.3.3.3.1.1.1.3.cmml" xref="S2.E15.m1.3.3.3.1.1.1.3"><times id="S2.E15.m1.3.3.3.1.1.1.3.1.cmml" xref="S2.E15.m1.3.3.3.1.1.1.3.1"></times><ci id="S2.E15.m1.3.3.3.1.1.1.3.2.cmml" xref="S2.E15.m1.3.3.3.1.1.1.3.2">𝐶</ci><ci id="S2.E15.m1.3.3.3.1.1.1.3.3.cmml" xref="S2.E15.m1.3.3.3.1.1.1.3.3">𝐿</ci><ci id="S2.E15.m1.3.3.3.1.1.1.3.4.cmml" xref="S2.E15.m1.3.3.3.1.1.1.3.4">𝑆</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E15.m1.3c">\displaystyle(\varepsilon^{1},\varepsilon^{2})=FFN(V_{CLS})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I2.i2.p1.1" class="ltx_p">The self-attention weights are learned directly as the two parameters with specified initial values:</p>
<table id="S5.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.E16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.E16.m1.2" class="ltx_Math" alttext="\displaystyle(\varepsilon^{1},\varepsilon^{2})=nn.Parameter(init\_value_{1},init\_value_{2})" display="inline"><semantics id="S2.E16.m1.2a"><mrow id="S2.E16.m1.2.2.2" xref="S2.E16.m1.2.2.3.cmml"><mrow id="S2.E16.m1.1.1.1.1" xref="S2.E16.m1.1.1.1.1.cmml"><mrow id="S2.E16.m1.1.1.1.1.2.2" xref="S2.E16.m1.1.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.E16.m1.1.1.1.1.2.2.3" xref="S2.E16.m1.1.1.1.1.2.3.cmml">(</mo><msup id="S2.E16.m1.1.1.1.1.1.1.1" xref="S2.E16.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.E16.m1.1.1.1.1.1.1.1.2" xref="S2.E16.m1.1.1.1.1.1.1.1.2.cmml">ε</mi><mn id="S2.E16.m1.1.1.1.1.1.1.1.3" xref="S2.E16.m1.1.1.1.1.1.1.1.3.cmml">1</mn></msup><mo id="S2.E16.m1.1.1.1.1.2.2.4" xref="S2.E16.m1.1.1.1.1.2.3.cmml">,</mo><msup id="S2.E16.m1.1.1.1.1.2.2.2" xref="S2.E16.m1.1.1.1.1.2.2.2.cmml"><mi id="S2.E16.m1.1.1.1.1.2.2.2.2" xref="S2.E16.m1.1.1.1.1.2.2.2.2.cmml">ε</mi><mn id="S2.E16.m1.1.1.1.1.2.2.2.3" xref="S2.E16.m1.1.1.1.1.2.2.2.3.cmml">2</mn></msup><mo stretchy="false" id="S2.E16.m1.1.1.1.1.2.2.5" xref="S2.E16.m1.1.1.1.1.2.3.cmml">)</mo></mrow><mo id="S2.E16.m1.1.1.1.1.3" xref="S2.E16.m1.1.1.1.1.3.cmml">=</mo><mrow id="S2.E16.m1.1.1.1.1.4" xref="S2.E16.m1.1.1.1.1.4.cmml"><mi id="S2.E16.m1.1.1.1.1.4.2" xref="S2.E16.m1.1.1.1.1.4.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.1.1.1.1.4.1" xref="S2.E16.m1.1.1.1.1.4.1.cmml">​</mo><mi id="S2.E16.m1.1.1.1.1.4.3" xref="S2.E16.m1.1.1.1.1.4.3.cmml">n</mi></mrow></mrow><mo lspace="0em" rspace="0.167em" id="S2.E16.m1.2.2.2.3" xref="S2.E16.m1.2.2.3a.cmml">.</mo><mrow id="S2.E16.m1.2.2.2.2" xref="S2.E16.m1.2.2.2.2.cmml"><mi id="S2.E16.m1.2.2.2.2.4" xref="S2.E16.m1.2.2.2.2.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.5" xref="S2.E16.m1.2.2.2.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3a" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.6" xref="S2.E16.m1.2.2.2.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3b" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.7" xref="S2.E16.m1.2.2.2.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3c" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.8" xref="S2.E16.m1.2.2.2.2.8.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3d" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.9" xref="S2.E16.m1.2.2.2.2.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3e" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.10" xref="S2.E16.m1.2.2.2.2.10.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3f" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.11" xref="S2.E16.m1.2.2.2.2.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3g" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.12" xref="S2.E16.m1.2.2.2.2.12.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.3h" xref="S2.E16.m1.2.2.2.2.3.cmml">​</mo><mrow id="S2.E16.m1.2.2.2.2.2.2" xref="S2.E16.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E16.m1.2.2.2.2.2.2.3" xref="S2.E16.m1.2.2.2.2.2.3.cmml">(</mo><mrow id="S2.E16.m1.2.2.2.2.1.1.1" xref="S2.E16.m1.2.2.2.2.1.1.1.cmml"><mi id="S2.E16.m1.2.2.2.2.1.1.1.2" xref="S2.E16.m1.2.2.2.2.1.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.3" xref="S2.E16.m1.2.2.2.2.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1a" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.4" xref="S2.E16.m1.2.2.2.2.1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1b" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.5" xref="S2.E16.m1.2.2.2.2.1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1c" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.E16.m1.2.2.2.2.1.1.1.6" xref="S2.E16.m1.2.2.2.2.1.1.1.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1d" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.7" xref="S2.E16.m1.2.2.2.2.1.1.1.7.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1e" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.8" xref="S2.E16.m1.2.2.2.2.1.1.1.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1f" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.9" xref="S2.E16.m1.2.2.2.2.1.1.1.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1g" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.1.1.1.10" xref="S2.E16.m1.2.2.2.2.1.1.1.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.1.1.1.1h" xref="S2.E16.m1.2.2.2.2.1.1.1.1.cmml">​</mo><msub id="S2.E16.m1.2.2.2.2.1.1.1.11" xref="S2.E16.m1.2.2.2.2.1.1.1.11.cmml"><mi id="S2.E16.m1.2.2.2.2.1.1.1.11.2" xref="S2.E16.m1.2.2.2.2.1.1.1.11.2.cmml">e</mi><mn id="S2.E16.m1.2.2.2.2.1.1.1.11.3" xref="S2.E16.m1.2.2.2.2.1.1.1.11.3.cmml">1</mn></msub></mrow><mo id="S2.E16.m1.2.2.2.2.2.2.4" xref="S2.E16.m1.2.2.2.2.2.3.cmml">,</mo><mrow id="S2.E16.m1.2.2.2.2.2.2.2" xref="S2.E16.m1.2.2.2.2.2.2.2.cmml"><mi id="S2.E16.m1.2.2.2.2.2.2.2.2" xref="S2.E16.m1.2.2.2.2.2.2.2.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.3" xref="S2.E16.m1.2.2.2.2.2.2.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1a" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.4" xref="S2.E16.m1.2.2.2.2.2.2.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1b" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.5" xref="S2.E16.m1.2.2.2.2.2.2.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1c" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S2.E16.m1.2.2.2.2.2.2.2.6" xref="S2.E16.m1.2.2.2.2.2.2.2.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1d" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.7" xref="S2.E16.m1.2.2.2.2.2.2.2.7.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1e" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.8" xref="S2.E16.m1.2.2.2.2.2.2.2.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1f" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.9" xref="S2.E16.m1.2.2.2.2.2.2.2.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1g" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><mi id="S2.E16.m1.2.2.2.2.2.2.2.10" xref="S2.E16.m1.2.2.2.2.2.2.2.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.E16.m1.2.2.2.2.2.2.2.1h" xref="S2.E16.m1.2.2.2.2.2.2.2.1.cmml">​</mo><msub id="S2.E16.m1.2.2.2.2.2.2.2.11" xref="S2.E16.m1.2.2.2.2.2.2.2.11.cmml"><mi id="S2.E16.m1.2.2.2.2.2.2.2.11.2" xref="S2.E16.m1.2.2.2.2.2.2.2.11.2.cmml">e</mi><mn id="S2.E16.m1.2.2.2.2.2.2.2.11.3" xref="S2.E16.m1.2.2.2.2.2.2.2.11.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S2.E16.m1.2.2.2.2.2.2.5" xref="S2.E16.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E16.m1.2b"><apply id="S2.E16.m1.2.2.3.cmml" xref="S2.E16.m1.2.2.2"><csymbol cd="ambiguous" id="S2.E16.m1.2.2.3a.cmml" xref="S2.E16.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.E16.m1.1.1.1.1.cmml" xref="S2.E16.m1.1.1.1.1"><eq id="S2.E16.m1.1.1.1.1.3.cmml" xref="S2.E16.m1.1.1.1.1.3"></eq><interval closure="open" id="S2.E16.m1.1.1.1.1.2.3.cmml" xref="S2.E16.m1.1.1.1.1.2.2"><apply id="S2.E16.m1.1.1.1.1.1.1.1.cmml" xref="S2.E16.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E16.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E16.m1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E16.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E16.m1.1.1.1.1.1.1.1.2">𝜀</ci><cn type="integer" id="S2.E16.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E16.m1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S2.E16.m1.1.1.1.1.2.2.2.cmml" xref="S2.E16.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E16.m1.1.1.1.1.2.2.2.1.cmml" xref="S2.E16.m1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S2.E16.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.E16.m1.1.1.1.1.2.2.2.2">𝜀</ci><cn type="integer" id="S2.E16.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.E16.m1.1.1.1.1.2.2.2.3">2</cn></apply></interval><apply id="S2.E16.m1.1.1.1.1.4.cmml" xref="S2.E16.m1.1.1.1.1.4"><times id="S2.E16.m1.1.1.1.1.4.1.cmml" xref="S2.E16.m1.1.1.1.1.4.1"></times><ci id="S2.E16.m1.1.1.1.1.4.2.cmml" xref="S2.E16.m1.1.1.1.1.4.2">𝑛</ci><ci id="S2.E16.m1.1.1.1.1.4.3.cmml" xref="S2.E16.m1.1.1.1.1.4.3">𝑛</ci></apply></apply><apply id="S2.E16.m1.2.2.2.2.cmml" xref="S2.E16.m1.2.2.2.2"><times id="S2.E16.m1.2.2.2.2.3.cmml" xref="S2.E16.m1.2.2.2.2.3"></times><ci id="S2.E16.m1.2.2.2.2.4.cmml" xref="S2.E16.m1.2.2.2.2.4">𝑃</ci><ci id="S2.E16.m1.2.2.2.2.5.cmml" xref="S2.E16.m1.2.2.2.2.5">𝑎</ci><ci id="S2.E16.m1.2.2.2.2.6.cmml" xref="S2.E16.m1.2.2.2.2.6">𝑟</ci><ci id="S2.E16.m1.2.2.2.2.7.cmml" xref="S2.E16.m1.2.2.2.2.7">𝑎</ci><ci id="S2.E16.m1.2.2.2.2.8.cmml" xref="S2.E16.m1.2.2.2.2.8">𝑚</ci><ci id="S2.E16.m1.2.2.2.2.9.cmml" xref="S2.E16.m1.2.2.2.2.9">𝑒</ci><ci id="S2.E16.m1.2.2.2.2.10.cmml" xref="S2.E16.m1.2.2.2.2.10">𝑡</ci><ci id="S2.E16.m1.2.2.2.2.11.cmml" xref="S2.E16.m1.2.2.2.2.11">𝑒</ci><ci id="S2.E16.m1.2.2.2.2.12.cmml" xref="S2.E16.m1.2.2.2.2.12">𝑟</ci><interval closure="open" id="S2.E16.m1.2.2.2.2.2.3.cmml" xref="S2.E16.m1.2.2.2.2.2.2"><apply id="S2.E16.m1.2.2.2.2.1.1.1.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1"><times id="S2.E16.m1.2.2.2.2.1.1.1.1.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.1"></times><ci id="S2.E16.m1.2.2.2.2.1.1.1.2.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.2">𝑖</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.3.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.3">𝑛</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.4.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.4">𝑖</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.5.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.5">𝑡</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.6.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.6">_</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.7.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.7">𝑣</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.8.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.8">𝑎</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.9.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.9">𝑙</ci><ci id="S2.E16.m1.2.2.2.2.1.1.1.10.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.10">𝑢</ci><apply id="S2.E16.m1.2.2.2.2.1.1.1.11.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.11"><csymbol cd="ambiguous" id="S2.E16.m1.2.2.2.2.1.1.1.11.1.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.11">subscript</csymbol><ci id="S2.E16.m1.2.2.2.2.1.1.1.11.2.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.11.2">𝑒</ci><cn type="integer" id="S2.E16.m1.2.2.2.2.1.1.1.11.3.cmml" xref="S2.E16.m1.2.2.2.2.1.1.1.11.3">1</cn></apply></apply><apply id="S2.E16.m1.2.2.2.2.2.2.2.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2"><times id="S2.E16.m1.2.2.2.2.2.2.2.1.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.1"></times><ci id="S2.E16.m1.2.2.2.2.2.2.2.2.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.2">𝑖</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.3.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.3">𝑛</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.4.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.4">𝑖</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.5.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.5">𝑡</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.6.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.6">_</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.7.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.7">𝑣</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.8.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.8">𝑎</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.9.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.9">𝑙</ci><ci id="S2.E16.m1.2.2.2.2.2.2.2.10.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.10">𝑢</ci><apply id="S2.E16.m1.2.2.2.2.2.2.2.11.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.11"><csymbol cd="ambiguous" id="S2.E16.m1.2.2.2.2.2.2.2.11.1.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.11">subscript</csymbol><ci id="S2.E16.m1.2.2.2.2.2.2.2.11.2.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.11.2">𝑒</ci><cn type="integer" id="S2.E16.m1.2.2.2.2.2.2.2.11.3.cmml" xref="S2.E16.m1.2.2.2.2.2.2.2.11.3">2</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E16.m1.2c">\displaystyle(\varepsilon^{1},\varepsilon^{2})=nn.Parameter(init\_value_{1},init\_value_{2})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
</div>
</li>
</ol>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>VLP with Learning to Attend</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">Section <a href="#S2.SS2.SSS1" title="2.2.1 Visual Features ‣ 2.2 Comprehensive Feature Representation ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a> discusses the use of three classes of visual features (Region, Grid and Patch). Each class of the visual features and text feature are fused by the novel <span id="S2.SS3.SSS3.p1.1.1" class="ltx_text ltx_font_italic">learning to attend</span> mechanism for cross-modal interaction, respectively.</p>
</div>
<section id="S2.SS3.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Input Embeddings</h5>

<div id="S2.SS3.SSS3.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.Px1.p1.2" class="ltx_p">The input to Transformer is the image feature and its associated sentence (e.g. caption text). Each image is represented as a sequence of image features <math id="S2.SS3.SSS3.Px1.p1.1.m1.3" class="ltx_Math" alttext="\{o_{1},...,o_{n}\}" display="inline"><semantics id="S2.SS3.SSS3.Px1.p1.1.m1.3a"><mrow id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.3.cmml"><mo stretchy="false" id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.3" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.3.cmml">{</mo><msub id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.cmml"><mi id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.2" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.2.cmml">o</mi><mn id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.3" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.4" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS3.SSS3.Px1.p1.1.m1.1.1" xref="S2.SS3.SSS3.Px1.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.5" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.3.cmml">,</mo><msub id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.cmml"><mi id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.2" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.2.cmml">o</mi><mi id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.3" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.6" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.Px1.p1.1.m1.3b"><set id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.3.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2"><apply id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.2">𝑜</ci><cn type="integer" id="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.2.2.1.1.3">1</cn></apply><ci id="S2.SS3.SSS3.Px1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.1.1">…</ci><apply id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.2">𝑜</ci><ci id="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.3.cmml" xref="S2.SS3.SSS3.Px1.p1.1.m1.3.3.2.2.3">𝑛</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.Px1.p1.1.m1.3c">\{o_{1},...,o_{n}\}</annotation></semantics></math>, and each sentence is represented as a sequence of words <math id="S2.SS3.SSS3.Px1.p1.2.m2.3" class="ltx_Math" alttext="\{w_{1},...,w_{m}\}" display="inline"><semantics id="S2.SS3.SSS3.Px1.p1.2.m2.3a"><mrow id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.3.cmml"><mo stretchy="false" id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.3" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.3.cmml">{</mo><msub id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.cmml"><mi id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.2" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.2.cmml">w</mi><mn id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.3" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.4" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS3.SSS3.Px1.p1.2.m2.1.1" xref="S2.SS3.SSS3.Px1.p1.2.m2.1.1.cmml">…</mi><mo id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.5" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.3.cmml">,</mo><msub id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.cmml"><mi id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.2" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.2.cmml">w</mi><mi id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.3" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.3.cmml">m</mi></msub><mo stretchy="false" id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.6" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.Px1.p1.2.m2.3b"><set id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.3.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2"><apply id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.2.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.2">𝑤</ci><cn type="integer" id="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.3.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.2.2.1.1.3">1</cn></apply><ci id="S2.SS3.SSS3.Px1.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.1.1">…</ci><apply id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.1.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.2">𝑤</ci><ci id="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.3.cmml" xref="S2.SS3.SSS3.Px1.p1.2.m2.3.3.2.2.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.Px1.p1.2.m2.3c">\{w_{1},...,w_{m}\}</annotation></semantics></math>. The image and text embedding features are concatenated as input to the Transformer with learning to attend. The image representation for each kind of feature is described in Section <a href="#S2.SS2.SSS1" title="2.2.1 Visual Features ‣ 2.2 Comprehensive Feature Representation ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>. To capture both local and global semantics of image and obtain diverse visual feature representation, different kinds of image features are fused by concatenating them together. It is then combined with the text embedding features as input to the Transformer with <em id="S2.SS3.SSS3.Px1.p1.2.1" class="ltx_emph ltx_font_italic">learning to attend</em>. This method is referred to as Fusion-VLP.</p>
</div>
</section>
<section id="S2.SS3.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training Tasks</h5>

<div id="S2.SS3.SSS3.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.Px2.p1.1" class="ltx_p">The pre-training tasks of the three types (language, vision and cross-modality) are introduced in the pre-training stage, following LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>.</p>
<ol id="S2.I3" class="ltx_enumerate">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I3.i1.p1" class="ltx_para ltx_noindent">
<p id="S2.I3.i1.p1.1" class="ltx_p"><span id="S2.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Masked LM Prediction.</span> The task setup is basically the same as in BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>. The masked words are predicted by exploiting visual modality which helps to resolve ambiguity.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I3.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I3.i2.p1.1" class="ltx_p"><span id="S2.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Masked Object Prediction.</span> Similarly, the vision side is pre-trained by randomly masking objects. In particular, 15% of image objects are randomly masked, and the model is then asked to predict properties of these masked objects with the output object representations <math id="S2.I3.i2.p1.1.m1.1" class="ltx_Math" alttext="O^{L}" display="inline"><semantics id="S2.I3.i2.p1.1.m1.1a"><msup id="S2.I3.i2.p1.1.m1.1.1" xref="S2.I3.i2.p1.1.m1.1.1.cmml"><mi id="S2.I3.i2.p1.1.m1.1.1.2" xref="S2.I3.i2.p1.1.m1.1.1.2.cmml">O</mi><mi id="S2.I3.i2.p1.1.m1.1.1.3" xref="S2.I3.i2.p1.1.m1.1.1.3.cmml">L</mi></msup><annotation-xml encoding="MathML-Content" id="S2.I3.i2.p1.1.m1.1b"><apply id="S2.I3.i2.p1.1.m1.1.1.cmml" xref="S2.I3.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I3.i2.p1.1.m1.1.1.1.cmml" xref="S2.I3.i2.p1.1.m1.1.1">superscript</csymbol><ci id="S2.I3.i2.p1.1.m1.1.1.2.cmml" xref="S2.I3.i2.p1.1.m1.1.1.2">𝑂</ci><ci id="S2.I3.i2.p1.1.m1.1.1.3.cmml" xref="S2.I3.i2.p1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i2.p1.1.m1.1c">O^{L}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I3.i3.p1" class="ltx_para ltx_noindent">
<p id="S2.I3.i3.p1.1" class="ltx_p"><span id="S2.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Image-Text Matching (ITM).</span> This task randomly samples 50% of mismatched image-text pairs and 50% matched pairs, and trains a classifier to predict whether an image and a sentence match with each other on the representation.</p>
</div>
</li>
<li id="S2.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I3.i4.p1" class="ltx_para ltx_noindent">
<p id="S2.I3.i4.p1.1" class="ltx_p"><span id="S2.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Image Question Answering (QA).</span> The image question answering task is cast as a classification problem where the model is pre-trained with image QA data as in LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>. A classifier is then built on top of the representation <math id="S2.I3.i4.p1.1.m1.1" class="ltx_Math" alttext="\textbf{h}^{L}_{CLS}" display="inline"><semantics id="S2.I3.i4.p1.1.m1.1a"><msubsup id="S2.I3.i4.p1.1.m1.1.1" xref="S2.I3.i4.p1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.I3.i4.p1.1.m1.1.1.2.2" xref="S2.I3.i4.p1.1.m1.1.1.2.2a.cmml">h</mtext><mrow id="S2.I3.i4.p1.1.m1.1.1.3" xref="S2.I3.i4.p1.1.m1.1.1.3.cmml"><mi id="S2.I3.i4.p1.1.m1.1.1.3.2" xref="S2.I3.i4.p1.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.I3.i4.p1.1.m1.1.1.3.1" xref="S2.I3.i4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.I3.i4.p1.1.m1.1.1.3.3" xref="S2.I3.i4.p1.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.I3.i4.p1.1.m1.1.1.3.1a" xref="S2.I3.i4.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.I3.i4.p1.1.m1.1.1.3.4" xref="S2.I3.i4.p1.1.m1.1.1.3.4.cmml">S</mi></mrow><mi id="S2.I3.i4.p1.1.m1.1.1.2.3" xref="S2.I3.i4.p1.1.m1.1.1.2.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.I3.i4.p1.1.m1.1b"><apply id="S2.I3.i4.p1.1.m1.1.1.cmml" xref="S2.I3.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I3.i4.p1.1.m1.1.1.1.cmml" xref="S2.I3.i4.p1.1.m1.1.1">subscript</csymbol><apply id="S2.I3.i4.p1.1.m1.1.1.2.cmml" xref="S2.I3.i4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I3.i4.p1.1.m1.1.1.2.1.cmml" xref="S2.I3.i4.p1.1.m1.1.1">superscript</csymbol><ci id="S2.I3.i4.p1.1.m1.1.1.2.2a.cmml" xref="S2.I3.i4.p1.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.I3.i4.p1.1.m1.1.1.2.2.cmml" xref="S2.I3.i4.p1.1.m1.1.1.2.2">h</mtext></ci><ci id="S2.I3.i4.p1.1.m1.1.1.2.3.cmml" xref="S2.I3.i4.p1.1.m1.1.1.2.3">𝐿</ci></apply><apply id="S2.I3.i4.p1.1.m1.1.1.3.cmml" xref="S2.I3.i4.p1.1.m1.1.1.3"><times id="S2.I3.i4.p1.1.m1.1.1.3.1.cmml" xref="S2.I3.i4.p1.1.m1.1.1.3.1"></times><ci id="S2.I3.i4.p1.1.m1.1.1.3.2.cmml" xref="S2.I3.i4.p1.1.m1.1.1.3.2">𝐶</ci><ci id="S2.I3.i4.p1.1.m1.1.1.3.3.cmml" xref="S2.I3.i4.p1.1.m1.1.1.3.3">𝐿</ci><ci id="S2.I3.i4.p1.1.m1.1.1.3.4.cmml" xref="S2.I3.i4.p1.1.m1.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i4.p1.1.m1.1c">\textbf{h}^{L}_{CLS}</annotation></semantics></math> in the model.</p>
</div>
</li>
</ol>
<p id="S2.SS3.SSS3.Px2.p1.2" class="ltx_p">For region-based features, the Region-VLP model is pre-trained with all the four pre-training tasks as in LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, and the four losses are added up with equal weights. For grid feature, the Grid-VLP model is pre-trained with the pre-training tasks except <span id="S2.SS3.SSS3.Px2.p1.2.1" class="ltx_text ltx_font_italic">masked object prediction</span>, since the grid feature does not capture explicit semantics. Besides, to accelerate the pre-training process, a random sampling strategy is adopted to dynamically sample 100 image grids for each image following PixelBERT <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. The <span id="S2.SS3.SSS3.Px2.p1.2.2" class="ltx_text ltx_font_italic">masked object prediction</span> task is also removed for Patch-VLP and Fusion-VLP.</p>
</div>
<div id="S2.SS3.SSS3.Px2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.SSS3.Px2.p2.1" class="ltx_p">During fine-tuning, the complete region/grid/patch features are used to retain all the extracted visual information. The hidden state <math id="S2.SS3.SSS3.Px2.p2.1.m1.1" class="ltx_Math" alttext="\textbf{h}^{L}_{CLS}" display="inline"><semantics id="S2.SS3.SSS3.Px2.p2.1.m1.1a"><msubsup id="S2.SS3.SSS3.Px2.p2.1.m1.1.1" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.2" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.2a.cmml">h</mtext><mrow id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.cmml"><mi id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.2" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.1" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.3" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.1a" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.1.cmml">​</mo><mi id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.4" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.4.cmml">S</mi></mrow><mi id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.3" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS3.Px2.p2.1.m1.1b"><apply id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.1.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1">subscript</csymbol><apply id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.1.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1">superscript</csymbol><ci id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.2a.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.2"><mtext class="ltx_mathvariant_bold" id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.2.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.2">h</mtext></ci><ci id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.3.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.2.3">𝐿</ci></apply><apply id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3"><times id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.1.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.1"></times><ci id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.2.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.2">𝐶</ci><ci id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.3.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.3">𝐿</ci><ci id="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.4.cmml" xref="S2.SS3.SSS3.Px2.p2.1.m1.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS3.Px2.p2.1.m1.1c">\textbf{h}^{L}_{CLS}</annotation></semantics></math> of the last layer is used for cross-modality calculation.</p>
</div>
</section>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Knowledge-guided Mixture of Experts</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">Due to the complexity of the VQA task, there exist questions that are difficult to address by combining the diverse feature representation and V&amp;L pre-training. To address these questions and enable the model to evolve constantly, we further propose a knowledge-guided framework using the Mixture of Experts (MoE) model, as shown in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.1 Reaching Human Parity ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Starting from a pre-trained V&amp;L model (the base Vision Understanding Expert in our case), a knowledge mining module is introduced to automatically discover the types of the questions that are not well-addressed by the Vision Understanding Expert, such as text-reading questions and clock-reading questions. These questions are then addressed by two extra expert modules newly introduced: Text Reading Expert and Clock Reading Expert, respectively. Finally, the three expert modules are combined together and routed to the right questions by the MoE model.</p>
</div>
<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Knowledge Mining</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">On top of the comprehensive study of diverse feature representation and specific design of cross-modal interaction, we propose a continual learning framework to boost the power of the pre-trained V&amp;L model. It includes two stages: 1) identify new sub-tasks which require extra knowledge to learn; 2) learn expert models for these sub-tasks with the knowledge collected from domain experts or internet.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p2.8" class="ltx_p">To identify new sub-tasks, we adopt a clustering-based method, which considers the low-confidence examples from a base model and discovers groups of these examples by their similarity to form new sub-tasks. Given a base model <math id="S2.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS4.SSS1.p2.1.m1.1a"><mi id="S2.SS4.SSS1.p2.1.m1.1.1" xref="S2.SS4.SSS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.1.m1.1b"><ci id="S2.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.1.m1.1c">M</annotation></semantics></math> (i.e., Vision Understanding Expert in our case), we first collect examples which the model <math id="S2.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS4.SSS1.p2.2.m2.1a"><mi id="S2.SS4.SSS1.p2.2.m2.1.1" xref="S2.SS4.SSS1.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.2.m2.1b"><ci id="S2.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p2.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.2.m2.1c">M</annotation></semantics></math> is difficult to give correct answers with high confidence. The model is unable to address these examples well with existing knowledge, which calls for specialized expert models with extra knowledge to handle them. Specifically, given an example <math id="S2.SS4.SSS1.p2.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS4.SSS1.p2.3.m3.1a"><mi id="S2.SS4.SSS1.p2.3.m3.1.1" xref="S2.SS4.SSS1.p2.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.3.m3.1b"><ci id="S2.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p2.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.3.m3.1c">t</annotation></semantics></math>, the base model <math id="S2.SS4.SSS1.p2.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS4.SSS1.p2.4.m4.1a"><mi id="S2.SS4.SSS1.p2.4.m4.1.1" xref="S2.SS4.SSS1.p2.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.4.m4.1b"><ci id="S2.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S2.SS4.SSS1.p2.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.4.m4.1c">M</annotation></semantics></math> is designed to give a prediction with confidence score <math id="S2.SS4.SSS1.p2.5.m5.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS4.SSS1.p2.5.m5.1a"><mi id="S2.SS4.SSS1.p2.5.m5.1.1" xref="S2.SS4.SSS1.p2.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.5.m5.1b"><ci id="S2.SS4.SSS1.p2.5.m5.1.1.cmml" xref="S2.SS4.SSS1.p2.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.5.m5.1c">s</annotation></semantics></math>. The output score on the predicted label of the Visual Understanding Expert is used to calculate this score <math id="S2.SS4.SSS1.p2.6.m6.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.SS4.SSS1.p2.6.m6.1a"><mi id="S2.SS4.SSS1.p2.6.m6.1.1" xref="S2.SS4.SSS1.p2.6.m6.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.6.m6.1b"><ci id="S2.SS4.SSS1.p2.6.m6.1.1.cmml" xref="S2.SS4.SSS1.p2.6.m6.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.6.m6.1c">s</annotation></semantics></math>. The examples with low confidence scores (<math id="S2.SS4.SSS1.p2.7.m7.1" class="ltx_Math" alttext="s&lt;0.1" display="inline"><semantics id="S2.SS4.SSS1.p2.7.m7.1a"><mrow id="S2.SS4.SSS1.p2.7.m7.1.1" xref="S2.SS4.SSS1.p2.7.m7.1.1.cmml"><mi id="S2.SS4.SSS1.p2.7.m7.1.1.2" xref="S2.SS4.SSS1.p2.7.m7.1.1.2.cmml">s</mi><mo id="S2.SS4.SSS1.p2.7.m7.1.1.1" xref="S2.SS4.SSS1.p2.7.m7.1.1.1.cmml">&lt;</mo><mn id="S2.SS4.SSS1.p2.7.m7.1.1.3" xref="S2.SS4.SSS1.p2.7.m7.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.7.m7.1b"><apply id="S2.SS4.SSS1.p2.7.m7.1.1.cmml" xref="S2.SS4.SSS1.p2.7.m7.1.1"><lt id="S2.SS4.SSS1.p2.7.m7.1.1.1.cmml" xref="S2.SS4.SSS1.p2.7.m7.1.1.1"></lt><ci id="S2.SS4.SSS1.p2.7.m7.1.1.2.cmml" xref="S2.SS4.SSS1.p2.7.m7.1.1.2">𝑠</ci><cn type="float" id="S2.SS4.SSS1.p2.7.m7.1.1.3.cmml" xref="S2.SS4.SSS1.p2.7.m7.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.7.m7.1c">s&lt;0.1</annotation></semantics></math>) thus indicate the cases that the base model finds difficult to handle. Then, the typical clustering algorithm K-Means <cite class="ltx_cite ltx_citemacro_citep">(MacQueen et al., <a href="#bib.bib34" title="" class="ltx_ref">1967</a>)</cite> is used to partition the set of these low-confidence examples into sub-task clusters. Under our V&amp;L circumstances, clustering is conducted on both the textual and visual content of examples. Therefore, the cross-modal representation of <math id="S2.SS4.SSS1.p2.8.m8.1" class="ltx_Math" alttext="[CLS]" display="inline"><semantics id="S2.SS4.SSS1.p2.8.m8.1a"><mrow id="S2.SS4.SSS1.p2.8.m8.1.1.1" xref="S2.SS4.SSS1.p2.8.m8.1.1.2.cmml"><mo stretchy="false" id="S2.SS4.SSS1.p2.8.m8.1.1.1.2" xref="S2.SS4.SSS1.p2.8.m8.1.1.2.1.cmml">[</mo><mrow id="S2.SS4.SSS1.p2.8.m8.1.1.1.1" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.cmml"><mi id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.2" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.1" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.1.cmml">​</mo><mi id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.3" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.1a" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.1.cmml">​</mo><mi id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.4" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.4.cmml">S</mi></mrow><mo stretchy="false" id="S2.SS4.SSS1.p2.8.m8.1.1.1.3" xref="S2.SS4.SSS1.p2.8.m8.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.8.m8.1b"><apply id="S2.SS4.SSS1.p2.8.m8.1.1.2.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1"><csymbol cd="latexml" id="S2.SS4.SSS1.p2.8.m8.1.1.2.1.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1"><times id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.1.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.1"></times><ci id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.2.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.2">𝐶</ci><ci id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.3.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.3">𝐿</ci><ci id="S2.SS4.SSS1.p2.8.m8.1.1.1.1.4.cmml" xref="S2.SS4.SSS1.p2.8.m8.1.1.1.1.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.8.m8.1c">[CLS]</annotation></semantics></math> in the last layer of the Transformer are used as input to the clustering algorithm.</p>
</div>
<div id="S2.SS4.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS1.p3.1" class="ltx_p">Clustering the low-confidence examples allows us to identify new sub-tasks.
In the VQA task, the clustering discovers the two types of visual questions (text-reading questions and clock-reading questions) that need OCR ability and clock-reading ability to deal with, respectively. Since the existing Vision Understanding Expert is incapable of resolving the two sub-tasks well, two extra expert modules: Text Reading Expert and Clock Reading Expert, are trained to deal with the low-confidence examples. Finally, the three expert modules work together as a complete VQA solution via the Mixture of Expert model.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Text Reading Expert</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">Text-reading VQA is an important VQA sub-task, which focuses on the questions that require to read the textual content shown in an input image. The existing models on the VQA dataset perform classification with frequent answers as labels. This classification modeling does not work well on text-reading samples where the answers are often not frequent enough to be included in the label set. Therefore, a specially designed deep LM that aims to capture structure information from texts, the StructuralLM model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib35" title="" class="ltx_ref">2021b</a>)</cite> is utilized on these text-reading samples to extract answers from the text in images recognized by OCR. StructuralLM introduces cell-level 2D-positional embeddings and a new pre-training objective that classifies cells’ positions. The pre-trained StructuralLM model is adopted to the text-reading VQA samples in the following way.</p>
</div>
<div id="S2.SS4.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS2.p2.4" class="ltx_p">In order to adapt our StructureLM to image texts, we fine-tuned a pre-trained StructuralLM by text-reading samples. In particular, an OCR tool is first used to recognize text and serialize the cells (bounding boxes) from top-left to bottom-right in images. Each image is represented as a sequence of cells <math id="S2.SS4.SSS2.p2.1.m1.3" class="ltx_Math" alttext="\{c_{1},...,c_{n}\}" display="inline"><semantics id="S2.SS4.SSS2.p2.1.m1.3a"><mrow id="S2.SS4.SSS2.p2.1.m1.3.3.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p2.1.m1.3.3.2.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml">{</mo><msub id="S2.SS4.SSS2.p2.1.m1.2.2.1.1" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.2.2.1.1.2" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1.2.cmml">c</mi><mn id="S2.SS4.SSS2.p2.1.m1.2.2.1.1.3" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS4.SSS2.p2.1.m1.3.3.2.4" xref="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.SSS2.p2.1.m1.1.1" xref="S2.SS4.SSS2.p2.1.m1.1.1.cmml">…</mi><mo id="S2.SS4.SSS2.p2.1.m1.3.3.2.5" xref="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml">,</mo><msub id="S2.SS4.SSS2.p2.1.m1.3.3.2.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2.cmml"><mi id="S2.SS4.SSS2.p2.1.m1.3.3.2.2.2" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2.2.cmml">c</mi><mi id="S2.SS4.SSS2.p2.1.m1.3.3.2.2.3" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2.3.cmml">n</mi></msub><mo stretchy="false" id="S2.SS4.SSS2.p2.1.m1.3.3.2.6" xref="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.1.m1.3b"><set id="S2.SS4.SSS2.p2.1.m1.3.3.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.2"><apply id="S2.SS4.SSS2.p2.1.m1.2.2.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.1.m1.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p2.1.m1.2.2.1.1.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1.2">𝑐</ci><cn type="integer" id="S2.SS4.SSS2.p2.1.m1.2.2.1.1.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.2.2.1.1.3">1</cn></apply><ci id="S2.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.1.1">…</ci><apply id="S2.SS4.SSS2.p2.1.m1.3.3.2.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.1.m1.3.3.2.2.1.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.SS4.SSS2.p2.1.m1.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2.2">𝑐</ci><ci id="S2.SS4.SSS2.p2.1.m1.3.3.2.2.3.cmml" xref="S2.SS4.SSS2.p2.1.m1.3.3.2.2.3">𝑛</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.1.m1.3c">\{c_{1},...,c_{n}\}</annotation></semantics></math>, each of which contains a sequence of words <math id="S2.SS4.SSS2.p2.2.m2.3" class="ltx_Math" alttext="c_{i}=\{w_{i}^{1},...,w_{i}^{m}\}" display="inline"><semantics id="S2.SS4.SSS2.p2.2.m2.3a"><mrow id="S2.SS4.SSS2.p2.2.m2.3.3" xref="S2.SS4.SSS2.p2.2.m2.3.3.cmml"><msub id="S2.SS4.SSS2.p2.2.m2.3.3.4" xref="S2.SS4.SSS2.p2.2.m2.3.3.4.cmml"><mi id="S2.SS4.SSS2.p2.2.m2.3.3.4.2" xref="S2.SS4.SSS2.p2.2.m2.3.3.4.2.cmml">c</mi><mi id="S2.SS4.SSS2.p2.2.m2.3.3.4.3" xref="S2.SS4.SSS2.p2.2.m2.3.3.4.3.cmml">i</mi></msub><mo id="S2.SS4.SSS2.p2.2.m2.3.3.3" xref="S2.SS4.SSS2.p2.2.m2.3.3.3.cmml">=</mo><mrow id="S2.SS4.SSS2.p2.2.m2.3.3.2.2" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.3.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.3" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.3.cmml">{</mo><msubsup id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.2" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.2.cmml">w</mi><mi id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.3" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.3.cmml">i</mi><mn id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.3" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msubsup><mo id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.4" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS4.SSS2.p2.2.m2.1.1" xref="S2.SS4.SSS2.p2.2.m2.1.1.cmml">…</mi><mo id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.5" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.3.cmml">,</mo><msubsup id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.2" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.2.cmml">w</mi><mi id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.3" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.3.cmml">i</mi><mi id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.3" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.3.cmml">m</mi></msubsup><mo stretchy="false" id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.6" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.2.m2.3b"><apply id="S2.SS4.SSS2.p2.2.m2.3.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3"><eq id="S2.SS4.SSS2.p2.2.m2.3.3.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.3"></eq><apply id="S2.SS4.SSS2.p2.2.m2.3.3.4.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.4"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.2.m2.3.3.4.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.4">subscript</csymbol><ci id="S2.SS4.SSS2.p2.2.m2.3.3.4.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.4.2">𝑐</ci><ci id="S2.SS4.SSS2.p2.2.m2.3.3.4.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.4.3">𝑖</ci></apply><set id="S2.SS4.SSS2.p2.2.m2.3.3.2.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2"><apply id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1">superscript</csymbol><apply id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.2">𝑤</ci><ci id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.2.3">𝑖</ci></apply><cn type="integer" id="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.1.1">…</ci><apply id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2">superscript</csymbol><apply id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.1.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.2.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.2">𝑤</ci><ci id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.2.3">𝑖</ci></apply><ci id="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.3.cmml" xref="S2.SS4.SSS2.p2.2.m2.3.3.2.2.2.3">𝑚</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.2.m2.3c">c_{i}=\{w_{i}^{1},...,w_{i}^{m}\}</annotation></semantics></math>. A separator <math id="S2.SS4.SSS2.p2.3.m3.1" class="ltx_Math" alttext="[SEP]" display="inline"><semantics id="S2.SS4.SSS2.p2.3.m3.1a"><mrow id="S2.SS4.SSS2.p2.3.m3.1.1.1" xref="S2.SS4.SSS2.p2.3.m3.1.1.2.cmml"><mo stretchy="false" id="S2.SS4.SSS2.p2.3.m3.1.1.1.2" xref="S2.SS4.SSS2.p2.3.m3.1.1.2.1.cmml">[</mo><mrow id="S2.SS4.SSS2.p2.3.m3.1.1.1.1" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.cmml"><mi id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.2" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.1" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.3" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.1a" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.1.cmml">​</mo><mi id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.4" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.4.cmml">P</mi></mrow><mo stretchy="false" id="S2.SS4.SSS2.p2.3.m3.1.1.1.3" xref="S2.SS4.SSS2.p2.3.m3.1.1.2.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p2.3.m3.1b"><apply id="S2.SS4.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1"><csymbol cd="latexml" id="S2.SS4.SSS2.p2.3.m3.1.1.2.1.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.2">delimited-[]</csymbol><apply id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1"><times id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.1.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.1"></times><ci id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.2.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.2">𝑆</ci><ci id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.3.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.3">𝐸</ci><ci id="S2.SS4.SSS2.p2.3.m3.1.1.1.1.4.cmml" xref="S2.SS4.SSS2.p2.3.m3.1.1.1.1.4">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.3.m3.1c">[SEP]</annotation></semantics></math> is added between every two bounding boxes to separate them, which gives an input sequence <math id="S2.SS4.SSS2.p2.4.m4.1" class="ltx_math_unparsed" alttext="\{q_{1},...,q_{e},[SEP],c_{1},[SEP],c_{2}..,[SEP],c_{n}\}" display="inline"><semantics id="S2.SS4.SSS2.p2.4.m4.1a"><mrow id="S2.SS4.SSS2.p2.4.m4.1b"><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.2">{</mo><msub id="S2.SS4.SSS2.p2.4.m4.1.3"><mi id="S2.SS4.SSS2.p2.4.m4.1.3.2">q</mi><mn id="S2.SS4.SSS2.p2.4.m4.1.3.3">1</mn></msub><mo id="S2.SS4.SSS2.p2.4.m4.1.4">,</mo><mi mathvariant="normal" id="S2.SS4.SSS2.p2.4.m4.1.1">…</mi><mo id="S2.SS4.SSS2.p2.4.m4.1.5">,</mo><msub id="S2.SS4.SSS2.p2.4.m4.1.6"><mi id="S2.SS4.SSS2.p2.4.m4.1.6.2">q</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.6.3">e</mi></msub><mo id="S2.SS4.SSS2.p2.4.m4.1.7">,</mo><mrow id="S2.SS4.SSS2.p2.4.m4.1.8"><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.8.1">[</mo><mi id="S2.SS4.SSS2.p2.4.m4.1.8.2">S</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.8.3">E</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.8.4">P</mi><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.8.5">]</mo></mrow><mo id="S2.SS4.SSS2.p2.4.m4.1.9">,</mo><msub id="S2.SS4.SSS2.p2.4.m4.1.10"><mi id="S2.SS4.SSS2.p2.4.m4.1.10.2">c</mi><mn id="S2.SS4.SSS2.p2.4.m4.1.10.3">1</mn></msub><mo id="S2.SS4.SSS2.p2.4.m4.1.11">,</mo><mrow id="S2.SS4.SSS2.p2.4.m4.1.12"><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.12.1">[</mo><mi id="S2.SS4.SSS2.p2.4.m4.1.12.2">S</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.12.3">E</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.12.4">P</mi><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.12.5">]</mo></mrow><mo id="S2.SS4.SSS2.p2.4.m4.1.13">,</mo><msub id="S2.SS4.SSS2.p2.4.m4.1.14"><mi id="S2.SS4.SSS2.p2.4.m4.1.14.2">c</mi><mn id="S2.SS4.SSS2.p2.4.m4.1.14.3">2</mn></msub><mo lspace="0em" rspace="0.0835em" id="S2.SS4.SSS2.p2.4.m4.1.15">.</mo><mo lspace="0.0835em" rspace="0.167em" id="S2.SS4.SSS2.p2.4.m4.1.16">.</mo><mo id="S2.SS4.SSS2.p2.4.m4.1.17">,</mo><mrow id="S2.SS4.SSS2.p2.4.m4.1.18"><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.18.1">[</mo><mi id="S2.SS4.SSS2.p2.4.m4.1.18.2">S</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.18.3">E</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.18.4">P</mi><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.18.5">]</mo></mrow><mo id="S2.SS4.SSS2.p2.4.m4.1.19">,</mo><msub id="S2.SS4.SSS2.p2.4.m4.1.20"><mi id="S2.SS4.SSS2.p2.4.m4.1.20.2">c</mi><mi id="S2.SS4.SSS2.p2.4.m4.1.20.3">n</mi></msub><mo stretchy="false" id="S2.SS4.SSS2.p2.4.m4.1.21">}</mo></mrow><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p2.4.m4.1c">\{q_{1},...,q_{e},[SEP],c_{1},[SEP],c_{2}..,[SEP],c_{n}\}</annotation></semantics></math>. The StructuralLM is pre-trained subsequently in the same way as it is pre-trained on document images. A token-level span prediction classifier is then built upon the token representation to perform an extractive QA task, as often did for machine reading comprehension <cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a href="#bib.bib36" title="" class="ltx_ref">2016</a>; Chen et al., <a href="#bib.bib37" title="" class="ltx_ref">2017</a>)</cite>. Finally, the added separator is removed from the predicted answer span.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3 </span>Clock Reading Expert</h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS3.p1.1" class="ltx_p">With the powerful VQA features aforementioned, many questions can get satisfactory answers. However, it still suffers from reading precise time from clocks, as clock reading requires specific prior knowledge. Hence, a clock reading expert is introduced to address such kind of problems.
The clock reading expert consists of a clock detector and a clock reader. The clock detector is used to detect clocks in images, which is essentially an object detection task. The Cascade-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Cai and Vasconcelos, <a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite> is used as the backbone network for the clock detector. A binary classification loss and a bounding box regression loss are applied for training as the standard detection framework does <cite class="ltx_cite ltx_citemacro_citep">(Cai and Vasconcelos, <a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite>.
The detected bounding boxes from the clock detector are fed into the clock reader, which reads the precise time in the clocks. The clock reading is modeled as both a classification task as well as a regression task.
</p>
</div>
<div id="S2.SS4.SSS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS3.p2.7" class="ltx_p">In the clock reader, Resnet50-IBN <cite class="ltx_cite ltx_citemacro_citep">(Pan et al., <a href="#bib.bib39" title="" class="ltx_ref">2018</a>)</cite> is adopted as our backbone, and two specific branches are introduced for hour and minute prediction respectively. Furthermore, as the hour and minute hands in the clock are the keys to predict the time, attention modules were introduced to force the focus of the model on the hands.
A SE-layer <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib40" title="" class="ltx_ref">2018</a>)</cite> is employed after the backbone for channel-wise attention, and a spatial attention module which consists of convolution layers and ReLU activation is employed in the beginning of hour and minute branch respectively for spatial-wise attention. Such a corporation of channel and spatial wise attention is able to adapt to the individual bias of hour and minute prediction. The feature outputs from two branches are listed as following:</p>
<table id="S2.E17" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E17.m1.4" class="ltx_Math" alttext="f_{m}=E_{m}(Attn_{sp}(F)*F),\qquad f_{h}=E_{h}(Attn_{sp}(F)*F)" display="block"><semantics id="S2.E17.m1.4a"><mrow id="S2.E17.m1.4.4.2" xref="S2.E17.m1.4.4.3.cmml"><mrow id="S2.E17.m1.3.3.1.1" xref="S2.E17.m1.3.3.1.1.cmml"><msub id="S2.E17.m1.3.3.1.1.3" xref="S2.E17.m1.3.3.1.1.3.cmml"><mi id="S2.E17.m1.3.3.1.1.3.2" xref="S2.E17.m1.3.3.1.1.3.2.cmml">f</mi><mi id="S2.E17.m1.3.3.1.1.3.3" xref="S2.E17.m1.3.3.1.1.3.3.cmml">m</mi></msub><mo id="S2.E17.m1.3.3.1.1.2" xref="S2.E17.m1.3.3.1.1.2.cmml">=</mo><mrow id="S2.E17.m1.3.3.1.1.1" xref="S2.E17.m1.3.3.1.1.1.cmml"><msub id="S2.E17.m1.3.3.1.1.1.3" xref="S2.E17.m1.3.3.1.1.1.3.cmml"><mi id="S2.E17.m1.3.3.1.1.1.3.2" xref="S2.E17.m1.3.3.1.1.1.3.2.cmml">E</mi><mi id="S2.E17.m1.3.3.1.1.1.3.3" xref="S2.E17.m1.3.3.1.1.1.3.3.cmml">m</mi></msub><mo lspace="0em" rspace="0em" id="S2.E17.m1.3.3.1.1.1.2" xref="S2.E17.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S2.E17.m1.3.3.1.1.1.1.1" xref="S2.E17.m1.3.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E17.m1.3.3.1.1.1.1.1.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E17.m1.3.3.1.1.1.1.1.1" xref="S2.E17.m1.3.3.1.1.1.1.1.1.cmml"><mrow id="S2.E17.m1.3.3.1.1.1.1.1.1.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.2.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.1" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.2.3" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.1a" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.2.4" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.1b" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.cmml"><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.2.cmml">n</mi><mrow id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.cmml"><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.1" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.1.cmml">​</mo><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.3" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.3.cmml">p</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.1c" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S2.E17.m1.3.3.1.1.1.1.1.1.2.6.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.6.2.1" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.E17.m1.1.1" xref="S2.E17.m1.1.1.cmml">F</mi><mo rspace="0.055em" stretchy="false" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.6.2.2" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.E17.m1.3.3.1.1.1.1.1.1.1" xref="S2.E17.m1.3.3.1.1.1.1.1.1.1.cmml">∗</mo><mi id="S2.E17.m1.3.3.1.1.1.1.1.1.3" xref="S2.E17.m1.3.3.1.1.1.1.1.1.3.cmml">F</mi></mrow><mo stretchy="false" id="S2.E17.m1.3.3.1.1.1.1.1.3" xref="S2.E17.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo rspace="2.167em" id="S2.E17.m1.4.4.2.3" xref="S2.E17.m1.4.4.3a.cmml">,</mo><mrow id="S2.E17.m1.4.4.2.2" xref="S2.E17.m1.4.4.2.2.cmml"><msub id="S2.E17.m1.4.4.2.2.3" xref="S2.E17.m1.4.4.2.2.3.cmml"><mi id="S2.E17.m1.4.4.2.2.3.2" xref="S2.E17.m1.4.4.2.2.3.2.cmml">f</mi><mi id="S2.E17.m1.4.4.2.2.3.3" xref="S2.E17.m1.4.4.2.2.3.3.cmml">h</mi></msub><mo id="S2.E17.m1.4.4.2.2.2" xref="S2.E17.m1.4.4.2.2.2.cmml">=</mo><mrow id="S2.E17.m1.4.4.2.2.1" xref="S2.E17.m1.4.4.2.2.1.cmml"><msub id="S2.E17.m1.4.4.2.2.1.3" xref="S2.E17.m1.4.4.2.2.1.3.cmml"><mi id="S2.E17.m1.4.4.2.2.1.3.2" xref="S2.E17.m1.4.4.2.2.1.3.2.cmml">E</mi><mi id="S2.E17.m1.4.4.2.2.1.3.3" xref="S2.E17.m1.4.4.2.2.1.3.3.cmml">h</mi></msub><mo lspace="0em" rspace="0em" id="S2.E17.m1.4.4.2.2.1.2" xref="S2.E17.m1.4.4.2.2.1.2.cmml">​</mo><mrow id="S2.E17.m1.4.4.2.2.1.1.1" xref="S2.E17.m1.4.4.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.E17.m1.4.4.2.2.1.1.1.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.E17.m1.4.4.2.2.1.1.1.1" xref="S2.E17.m1.4.4.2.2.1.1.1.1.cmml"><mrow id="S2.E17.m1.4.4.2.2.1.1.1.1.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.cmml"><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.2.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.1" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.1.cmml">​</mo><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.2.3" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.1a" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.1.cmml">​</mo><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.2.4" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.1b" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.1.cmml">​</mo><msub id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.cmml"><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.2.cmml">n</mi><mrow id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.cmml"><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.1" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.1.cmml">​</mo><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.3" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.3.cmml">p</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.1c" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.1.cmml">​</mo><mrow id="S2.E17.m1.4.4.2.2.1.1.1.1.2.6.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.6.2.1" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.cmml">(</mo><mi id="S2.E17.m1.2.2" xref="S2.E17.m1.2.2.cmml">F</mi><mo rspace="0.055em" stretchy="false" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.6.2.2" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.E17.m1.4.4.2.2.1.1.1.1.1" xref="S2.E17.m1.4.4.2.2.1.1.1.1.1.cmml">∗</mo><mi id="S2.E17.m1.4.4.2.2.1.1.1.1.3" xref="S2.E17.m1.4.4.2.2.1.1.1.1.3.cmml">F</mi></mrow><mo stretchy="false" id="S2.E17.m1.4.4.2.2.1.1.1.3" xref="S2.E17.m1.4.4.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E17.m1.4b"><apply id="S2.E17.m1.4.4.3.cmml" xref="S2.E17.m1.4.4.2"><csymbol cd="ambiguous" id="S2.E17.m1.4.4.3a.cmml" xref="S2.E17.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S2.E17.m1.3.3.1.1.cmml" xref="S2.E17.m1.3.3.1.1"><eq id="S2.E17.m1.3.3.1.1.2.cmml" xref="S2.E17.m1.3.3.1.1.2"></eq><apply id="S2.E17.m1.3.3.1.1.3.cmml" xref="S2.E17.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S2.E17.m1.3.3.1.1.3.1.cmml" xref="S2.E17.m1.3.3.1.1.3">subscript</csymbol><ci id="S2.E17.m1.3.3.1.1.3.2.cmml" xref="S2.E17.m1.3.3.1.1.3.2">𝑓</ci><ci id="S2.E17.m1.3.3.1.1.3.3.cmml" xref="S2.E17.m1.3.3.1.1.3.3">𝑚</ci></apply><apply id="S2.E17.m1.3.3.1.1.1.cmml" xref="S2.E17.m1.3.3.1.1.1"><times id="S2.E17.m1.3.3.1.1.1.2.cmml" xref="S2.E17.m1.3.3.1.1.1.2"></times><apply id="S2.E17.m1.3.3.1.1.1.3.cmml" xref="S2.E17.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.E17.m1.3.3.1.1.1.3.1.cmml" xref="S2.E17.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S2.E17.m1.3.3.1.1.1.3.2.cmml" xref="S2.E17.m1.3.3.1.1.1.3.2">𝐸</ci><ci id="S2.E17.m1.3.3.1.1.1.3.3.cmml" xref="S2.E17.m1.3.3.1.1.1.3.3">𝑚</ci></apply><apply id="S2.E17.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1"><times id="S2.E17.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.1"></times><apply id="S2.E17.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2"><times id="S2.E17.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.1"></times><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.2">𝐴</ci><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.3">𝑡</ci><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.2.4.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.4">𝑡</ci><apply id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5"><csymbol cd="ambiguous" id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.1.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5">subscript</csymbol><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.2.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.2">𝑛</ci><apply id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3"><times id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.1.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.1"></times><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.2.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.2">𝑠</ci><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.3.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.2.5.3.3">𝑝</ci></apply></apply><ci id="S2.E17.m1.1.1.cmml" xref="S2.E17.m1.1.1">𝐹</ci></apply><ci id="S2.E17.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.E17.m1.3.3.1.1.1.1.1.1.3">𝐹</ci></apply></apply></apply><apply id="S2.E17.m1.4.4.2.2.cmml" xref="S2.E17.m1.4.4.2.2"><eq id="S2.E17.m1.4.4.2.2.2.cmml" xref="S2.E17.m1.4.4.2.2.2"></eq><apply id="S2.E17.m1.4.4.2.2.3.cmml" xref="S2.E17.m1.4.4.2.2.3"><csymbol cd="ambiguous" id="S2.E17.m1.4.4.2.2.3.1.cmml" xref="S2.E17.m1.4.4.2.2.3">subscript</csymbol><ci id="S2.E17.m1.4.4.2.2.3.2.cmml" xref="S2.E17.m1.4.4.2.2.3.2">𝑓</ci><ci id="S2.E17.m1.4.4.2.2.3.3.cmml" xref="S2.E17.m1.4.4.2.2.3.3">ℎ</ci></apply><apply id="S2.E17.m1.4.4.2.2.1.cmml" xref="S2.E17.m1.4.4.2.2.1"><times id="S2.E17.m1.4.4.2.2.1.2.cmml" xref="S2.E17.m1.4.4.2.2.1.2"></times><apply id="S2.E17.m1.4.4.2.2.1.3.cmml" xref="S2.E17.m1.4.4.2.2.1.3"><csymbol cd="ambiguous" id="S2.E17.m1.4.4.2.2.1.3.1.cmml" xref="S2.E17.m1.4.4.2.2.1.3">subscript</csymbol><ci id="S2.E17.m1.4.4.2.2.1.3.2.cmml" xref="S2.E17.m1.4.4.2.2.1.3.2">𝐸</ci><ci id="S2.E17.m1.4.4.2.2.1.3.3.cmml" xref="S2.E17.m1.4.4.2.2.1.3.3">ℎ</ci></apply><apply id="S2.E17.m1.4.4.2.2.1.1.1.1.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1"><times id="S2.E17.m1.4.4.2.2.1.1.1.1.1.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.1"></times><apply id="S2.E17.m1.4.4.2.2.1.1.1.1.2.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2"><times id="S2.E17.m1.4.4.2.2.1.1.1.1.2.1.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.1"></times><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.2.2.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.2">𝐴</ci><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.2.3.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.3">𝑡</ci><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.2.4.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.4">𝑡</ci><apply id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5"><csymbol cd="ambiguous" id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.1.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5">subscript</csymbol><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.2.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.2">𝑛</ci><apply id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3"><times id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.1.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.1"></times><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.2.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.2">𝑠</ci><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.3.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.2.5.3.3">𝑝</ci></apply></apply><ci id="S2.E17.m1.2.2.cmml" xref="S2.E17.m1.2.2">𝐹</ci></apply><ci id="S2.E17.m1.4.4.2.2.1.1.1.1.3.cmml" xref="S2.E17.m1.4.4.2.2.1.1.1.1.3">𝐹</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E17.m1.4c">f_{m}=E_{m}(Attn_{sp}(F)*F),\qquad f_{h}=E_{h}(Attn_{sp}(F)*F)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS3.p2.6" class="ltx_p">where <math id="S2.SS4.SSS3.p2.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.SS4.SSS3.p2.1.m1.1a"><mi id="S2.SS4.SSS3.p2.1.m1.1.1" xref="S2.SS4.SSS3.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p2.1.m1.1b"><ci id="S2.SS4.SSS3.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p2.1.m1.1c">I</annotation></semantics></math> is the image, and <math id="S2.SS4.SSS3.p2.2.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.SS4.SSS3.p2.2.m2.1a"><mi id="S2.SS4.SSS3.p2.2.m2.1.1" xref="S2.SS4.SSS3.p2.2.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p2.2.m2.1b"><ci id="S2.SS4.SSS3.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS3.p2.2.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p2.2.m2.1c">E</annotation></semantics></math>, <math id="S2.SS4.SSS3.p2.3.m3.1" class="ltx_Math" alttext="E_{h}" display="inline"><semantics id="S2.SS4.SSS3.p2.3.m3.1a"><msub id="S2.SS4.SSS3.p2.3.m3.1.1" xref="S2.SS4.SSS3.p2.3.m3.1.1.cmml"><mi id="S2.SS4.SSS3.p2.3.m3.1.1.2" xref="S2.SS4.SSS3.p2.3.m3.1.1.2.cmml">E</mi><mi id="S2.SS4.SSS3.p2.3.m3.1.1.3" xref="S2.SS4.SSS3.p2.3.m3.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p2.3.m3.1b"><apply id="S2.SS4.SSS3.p2.3.m3.1.1.cmml" xref="S2.SS4.SSS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p2.3.m3.1.1.1.cmml" xref="S2.SS4.SSS3.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p2.3.m3.1.1.2.cmml" xref="S2.SS4.SSS3.p2.3.m3.1.1.2">𝐸</ci><ci id="S2.SS4.SSS3.p2.3.m3.1.1.3.cmml" xref="S2.SS4.SSS3.p2.3.m3.1.1.3">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p2.3.m3.1c">E_{h}</annotation></semantics></math>, <math id="S2.SS4.SSS3.p2.4.m4.1" class="ltx_Math" alttext="E_{m}" display="inline"><semantics id="S2.SS4.SSS3.p2.4.m4.1a"><msub id="S2.SS4.SSS3.p2.4.m4.1.1" xref="S2.SS4.SSS3.p2.4.m4.1.1.cmml"><mi id="S2.SS4.SSS3.p2.4.m4.1.1.2" xref="S2.SS4.SSS3.p2.4.m4.1.1.2.cmml">E</mi><mi id="S2.SS4.SSS3.p2.4.m4.1.1.3" xref="S2.SS4.SSS3.p2.4.m4.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p2.4.m4.1b"><apply id="S2.SS4.SSS3.p2.4.m4.1.1.cmml" xref="S2.SS4.SSS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p2.4.m4.1.1.1.cmml" xref="S2.SS4.SSS3.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p2.4.m4.1.1.2.cmml" xref="S2.SS4.SSS3.p2.4.m4.1.1.2">𝐸</ci><ci id="S2.SS4.SSS3.p2.4.m4.1.1.3.cmml" xref="S2.SS4.SSS3.p2.4.m4.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p2.4.m4.1c">E_{m}</annotation></semantics></math> are the backbone, hour branch and minute branch respectively. <math id="S2.SS4.SSS3.p2.5.m5.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S2.SS4.SSS3.p2.5.m5.1a"><mi id="S2.SS4.SSS3.p2.5.m5.1.1" xref="S2.SS4.SSS3.p2.5.m5.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p2.5.m5.1b"><ci id="S2.SS4.SSS3.p2.5.m5.1.1.cmml" xref="S2.SS4.SSS3.p2.5.m5.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p2.5.m5.1c">F</annotation></semantics></math> is the feature map from the backbone after SE-layer, <math id="S2.SS4.SSS3.p2.6.m6.2" class="ltx_Math" alttext="F=Attn_{se}(E(I))" display="inline"><semantics id="S2.SS4.SSS3.p2.6.m6.2a"><mrow id="S2.SS4.SSS3.p2.6.m6.2.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.cmml"><mi id="S2.SS4.SSS3.p2.6.m6.2.2.3" xref="S2.SS4.SSS3.p2.6.m6.2.2.3.cmml">F</mi><mo id="S2.SS4.SSS3.p2.6.m6.2.2.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.2.cmml">=</mo><mrow id="S2.SS4.SSS3.p2.6.m6.2.2.1" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.cmml"><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.3" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS3.p2.6.m6.2.2.1.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.2.cmml">​</mo><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.4" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS3.p2.6.m6.2.2.1.2a" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.2.cmml">​</mo><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.5" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS3.p2.6.m6.2.2.1.2b" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.2.cmml">​</mo><msub id="S2.SS4.SSS3.p2.6.m6.2.2.1.6" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.cmml"><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.2.cmml">n</mi><mrow id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.cmml"><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.1" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.1.cmml">​</mo><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.3" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.3.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.SS4.SSS3.p2.6.m6.2.2.1.2c" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.2.cmml">​</mo><mrow id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml">(</mo><mrow id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml"><mi id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.1" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.1.cmml">​</mo><mrow id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.3.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.3.2.1" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml">(</mo><mi id="S2.SS4.SSS3.p2.6.m6.1.1" xref="S2.SS4.SSS3.p2.6.m6.1.1.cmml">I</mi><mo stretchy="false" id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.3.2.2" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.3" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p2.6.m6.2b"><apply id="S2.SS4.SSS3.p2.6.m6.2.2.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2"><eq id="S2.SS4.SSS3.p2.6.m6.2.2.2.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.2"></eq><ci id="S2.SS4.SSS3.p2.6.m6.2.2.3.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.3">𝐹</ci><apply id="S2.SS4.SSS3.p2.6.m6.2.2.1.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1"><times id="S2.SS4.SSS3.p2.6.m6.2.2.1.2.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.2"></times><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.3.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.3">𝐴</ci><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.4.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.4">𝑡</ci><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.5.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.5">𝑡</ci><apply id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.1.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6">subscript</csymbol><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.2.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.2">𝑛</ci><apply id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3"><times id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.1.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.1"></times><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.2.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.2">𝑠</ci><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.3.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.6.3.3">𝑒</ci></apply></apply><apply id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1"><times id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.1.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.1"></times><ci id="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.2.cmml" xref="S2.SS4.SSS3.p2.6.m6.2.2.1.1.1.1.2">𝐸</ci><ci id="S2.SS4.SSS3.p2.6.m6.1.1.cmml" xref="S2.SS4.SSS3.p2.6.m6.1.1">𝐼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p2.6.m6.2c">F=Attn_{se}(E(I))</annotation></semantics></math>.</p>
</div>
<div id="S2.SS4.SSS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS3.p3.2" class="ltx_p">As the clock reader is formulated as both a classification task and a regression task, it introduces loss from two perspectives. From the classification aspect, a 12-category classification loss is used for both hour and minute <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>the minute task is divided into 12 bins by 5 moves per bin.</span></span></span> prediction. The cross-entropy loss is adopted as follows:</p>
<table id="S2.E18" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E18.m1.1" class="ltx_Math" alttext="L_{cls}=-\sum^{N}_{i}g_{i}\times\log p_{i}" display="block"><semantics id="S2.E18.m1.1a"><mrow id="S2.E18.m1.1.1" xref="S2.E18.m1.1.1.cmml"><msub id="S2.E18.m1.1.1.2" xref="S2.E18.m1.1.1.2.cmml"><mi id="S2.E18.m1.1.1.2.2" xref="S2.E18.m1.1.1.2.2.cmml">L</mi><mrow id="S2.E18.m1.1.1.2.3" xref="S2.E18.m1.1.1.2.3.cmml"><mi id="S2.E18.m1.1.1.2.3.2" xref="S2.E18.m1.1.1.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E18.m1.1.1.2.3.1" xref="S2.E18.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E18.m1.1.1.2.3.3" xref="S2.E18.m1.1.1.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E18.m1.1.1.2.3.1a" xref="S2.E18.m1.1.1.2.3.1.cmml">​</mo><mi id="S2.E18.m1.1.1.2.3.4" xref="S2.E18.m1.1.1.2.3.4.cmml">s</mi></mrow></msub><mo id="S2.E18.m1.1.1.1" xref="S2.E18.m1.1.1.1.cmml">=</mo><mrow id="S2.E18.m1.1.1.3" xref="S2.E18.m1.1.1.3.cmml"><mo id="S2.E18.m1.1.1.3a" xref="S2.E18.m1.1.1.3.cmml">−</mo><mrow id="S2.E18.m1.1.1.3.2" xref="S2.E18.m1.1.1.3.2.cmml"><munderover id="S2.E18.m1.1.1.3.2.1" xref="S2.E18.m1.1.1.3.2.1.cmml"><mo movablelimits="false" id="S2.E18.m1.1.1.3.2.1.2.2" xref="S2.E18.m1.1.1.3.2.1.2.2.cmml">∑</mo><mi id="S2.E18.m1.1.1.3.2.1.3" xref="S2.E18.m1.1.1.3.2.1.3.cmml">i</mi><mi id="S2.E18.m1.1.1.3.2.1.2.3" xref="S2.E18.m1.1.1.3.2.1.2.3.cmml">N</mi></munderover><mrow id="S2.E18.m1.1.1.3.2.2" xref="S2.E18.m1.1.1.3.2.2.cmml"><msub id="S2.E18.m1.1.1.3.2.2.2" xref="S2.E18.m1.1.1.3.2.2.2.cmml"><mi id="S2.E18.m1.1.1.3.2.2.2.2" xref="S2.E18.m1.1.1.3.2.2.2.2.cmml">g</mi><mi id="S2.E18.m1.1.1.3.2.2.2.3" xref="S2.E18.m1.1.1.3.2.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E18.m1.1.1.3.2.2.1" xref="S2.E18.m1.1.1.3.2.2.1.cmml">×</mo><mrow id="S2.E18.m1.1.1.3.2.2.3" xref="S2.E18.m1.1.1.3.2.2.3.cmml"><mi id="S2.E18.m1.1.1.3.2.2.3.1" xref="S2.E18.m1.1.1.3.2.2.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E18.m1.1.1.3.2.2.3a" xref="S2.E18.m1.1.1.3.2.2.3.cmml">⁡</mo><msub id="S2.E18.m1.1.1.3.2.2.3.2" xref="S2.E18.m1.1.1.3.2.2.3.2.cmml"><mi id="S2.E18.m1.1.1.3.2.2.3.2.2" xref="S2.E18.m1.1.1.3.2.2.3.2.2.cmml">p</mi><mi id="S2.E18.m1.1.1.3.2.2.3.2.3" xref="S2.E18.m1.1.1.3.2.2.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E18.m1.1b"><apply id="S2.E18.m1.1.1.cmml" xref="S2.E18.m1.1.1"><eq id="S2.E18.m1.1.1.1.cmml" xref="S2.E18.m1.1.1.1"></eq><apply id="S2.E18.m1.1.1.2.cmml" xref="S2.E18.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E18.m1.1.1.2.1.cmml" xref="S2.E18.m1.1.1.2">subscript</csymbol><ci id="S2.E18.m1.1.1.2.2.cmml" xref="S2.E18.m1.1.1.2.2">𝐿</ci><apply id="S2.E18.m1.1.1.2.3.cmml" xref="S2.E18.m1.1.1.2.3"><times id="S2.E18.m1.1.1.2.3.1.cmml" xref="S2.E18.m1.1.1.2.3.1"></times><ci id="S2.E18.m1.1.1.2.3.2.cmml" xref="S2.E18.m1.1.1.2.3.2">𝑐</ci><ci id="S2.E18.m1.1.1.2.3.3.cmml" xref="S2.E18.m1.1.1.2.3.3">𝑙</ci><ci id="S2.E18.m1.1.1.2.3.4.cmml" xref="S2.E18.m1.1.1.2.3.4">𝑠</ci></apply></apply><apply id="S2.E18.m1.1.1.3.cmml" xref="S2.E18.m1.1.1.3"><minus id="S2.E18.m1.1.1.3.1.cmml" xref="S2.E18.m1.1.1.3"></minus><apply id="S2.E18.m1.1.1.3.2.cmml" xref="S2.E18.m1.1.1.3.2"><apply id="S2.E18.m1.1.1.3.2.1.cmml" xref="S2.E18.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S2.E18.m1.1.1.3.2.1.1.cmml" xref="S2.E18.m1.1.1.3.2.1">subscript</csymbol><apply id="S2.E18.m1.1.1.3.2.1.2.cmml" xref="S2.E18.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="S2.E18.m1.1.1.3.2.1.2.1.cmml" xref="S2.E18.m1.1.1.3.2.1">superscript</csymbol><sum id="S2.E18.m1.1.1.3.2.1.2.2.cmml" xref="S2.E18.m1.1.1.3.2.1.2.2"></sum><ci id="S2.E18.m1.1.1.3.2.1.2.3.cmml" xref="S2.E18.m1.1.1.3.2.1.2.3">𝑁</ci></apply><ci id="S2.E18.m1.1.1.3.2.1.3.cmml" xref="S2.E18.m1.1.1.3.2.1.3">𝑖</ci></apply><apply id="S2.E18.m1.1.1.3.2.2.cmml" xref="S2.E18.m1.1.1.3.2.2"><times id="S2.E18.m1.1.1.3.2.2.1.cmml" xref="S2.E18.m1.1.1.3.2.2.1"></times><apply id="S2.E18.m1.1.1.3.2.2.2.cmml" xref="S2.E18.m1.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.E18.m1.1.1.3.2.2.2.1.cmml" xref="S2.E18.m1.1.1.3.2.2.2">subscript</csymbol><ci id="S2.E18.m1.1.1.3.2.2.2.2.cmml" xref="S2.E18.m1.1.1.3.2.2.2.2">𝑔</ci><ci id="S2.E18.m1.1.1.3.2.2.2.3.cmml" xref="S2.E18.m1.1.1.3.2.2.2.3">𝑖</ci></apply><apply id="S2.E18.m1.1.1.3.2.2.3.cmml" xref="S2.E18.m1.1.1.3.2.2.3"><log id="S2.E18.m1.1.1.3.2.2.3.1.cmml" xref="S2.E18.m1.1.1.3.2.2.3.1"></log><apply id="S2.E18.m1.1.1.3.2.2.3.2.cmml" xref="S2.E18.m1.1.1.3.2.2.3.2"><csymbol cd="ambiguous" id="S2.E18.m1.1.1.3.2.2.3.2.1.cmml" xref="S2.E18.m1.1.1.3.2.2.3.2">subscript</csymbol><ci id="S2.E18.m1.1.1.3.2.2.3.2.2.cmml" xref="S2.E18.m1.1.1.3.2.2.3.2.2">𝑝</ci><ci id="S2.E18.m1.1.1.3.2.2.3.2.3.cmml" xref="S2.E18.m1.1.1.3.2.2.3.2.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E18.m1.1c">L_{cls}=-\sum^{N}_{i}g_{i}\times\log p_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(18)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS3.p3.1" class="ltx_p">where <math id="S2.SS4.SSS3.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS4.SSS3.p3.1.m1.1a"><mi id="S2.SS4.SSS3.p3.1.m1.1.1" xref="S2.SS4.SSS3.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p3.1.m1.1b"><ci id="S2.SS4.SSS3.p3.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p3.1.m1.1c">N</annotation></semantics></math> is the number of categories and set to 12.</p>
</div>
<div id="S2.SS4.SSS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS3.p4.5" class="ltx_p">As 2:00 is closer to 3:00 than to 9:00, it is also important to solve the problem from a regression perspective. The regression loss is listed as below:</p>
<table id="S2.E19" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E19.m1.1" class="ltx_Math" alttext="L_{reg}=\sum^{B}_{i}\left(cos\left(\frac{2\pi}{C}\times\left(p_{i}-g_{i}\right)-\pi\right)+1\right)" display="block"><semantics id="S2.E19.m1.1a"><mrow id="S2.E19.m1.1.1" xref="S2.E19.m1.1.1.cmml"><msub id="S2.E19.m1.1.1.3" xref="S2.E19.m1.1.1.3.cmml"><mi id="S2.E19.m1.1.1.3.2" xref="S2.E19.m1.1.1.3.2.cmml">L</mi><mrow id="S2.E19.m1.1.1.3.3" xref="S2.E19.m1.1.1.3.3.cmml"><mi id="S2.E19.m1.1.1.3.3.2" xref="S2.E19.m1.1.1.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E19.m1.1.1.3.3.1" xref="S2.E19.m1.1.1.3.3.1.cmml">​</mo><mi id="S2.E19.m1.1.1.3.3.3" xref="S2.E19.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E19.m1.1.1.3.3.1a" xref="S2.E19.m1.1.1.3.3.1.cmml">​</mo><mi id="S2.E19.m1.1.1.3.3.4" xref="S2.E19.m1.1.1.3.3.4.cmml">g</mi></mrow></msub><mo rspace="0.111em" id="S2.E19.m1.1.1.2" xref="S2.E19.m1.1.1.2.cmml">=</mo><mrow id="S2.E19.m1.1.1.1" xref="S2.E19.m1.1.1.1.cmml"><munderover id="S2.E19.m1.1.1.1.2" xref="S2.E19.m1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S2.E19.m1.1.1.1.2.2.2" xref="S2.E19.m1.1.1.1.2.2.2.cmml">∑</mo><mi id="S2.E19.m1.1.1.1.2.3" xref="S2.E19.m1.1.1.1.2.3.cmml">i</mi><mi id="S2.E19.m1.1.1.1.2.2.3" xref="S2.E19.m1.1.1.1.2.2.3.cmml">B</mi></munderover><mrow id="S2.E19.m1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.cmml"><mo id="S2.E19.m1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E19.m1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E19.m1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.E19.m1.1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E19.m1.1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E19.m1.1.1.1.1.1.1.1.4" xref="S2.E19.m1.1.1.1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E19.m1.1.1.1.1.1.1.1.2a" xref="S2.E19.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mi id="S2.E19.m1.1.1.1.1.1.1.1.5" xref="S2.E19.m1.1.1.1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E19.m1.1.1.1.1.1.1.1.2b" xref="S2.E19.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E19.m1.1.1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E19.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mfrac id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mn id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">π</mi></mrow><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">C</mi></mfrac><mo lspace="0.222em" rspace="0.222em" id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">×</mo><mrow id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">p</mi><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">g</mi><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><mi id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">π</mi></mrow><mo id="S2.E19.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E19.m1.1.1.1.1.1.1.2" xref="S2.E19.m1.1.1.1.1.1.1.2.cmml">+</mo><mn id="S2.E19.m1.1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S2.E19.m1.1.1.1.1.1.3" xref="S2.E19.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E19.m1.1b"><apply id="S2.E19.m1.1.1.cmml" xref="S2.E19.m1.1.1"><eq id="S2.E19.m1.1.1.2.cmml" xref="S2.E19.m1.1.1.2"></eq><apply id="S2.E19.m1.1.1.3.cmml" xref="S2.E19.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E19.m1.1.1.3.1.cmml" xref="S2.E19.m1.1.1.3">subscript</csymbol><ci id="S2.E19.m1.1.1.3.2.cmml" xref="S2.E19.m1.1.1.3.2">𝐿</ci><apply id="S2.E19.m1.1.1.3.3.cmml" xref="S2.E19.m1.1.1.3.3"><times id="S2.E19.m1.1.1.3.3.1.cmml" xref="S2.E19.m1.1.1.3.3.1"></times><ci id="S2.E19.m1.1.1.3.3.2.cmml" xref="S2.E19.m1.1.1.3.3.2">𝑟</ci><ci id="S2.E19.m1.1.1.3.3.3.cmml" xref="S2.E19.m1.1.1.3.3.3">𝑒</ci><ci id="S2.E19.m1.1.1.3.3.4.cmml" xref="S2.E19.m1.1.1.3.3.4">𝑔</ci></apply></apply><apply id="S2.E19.m1.1.1.1.cmml" xref="S2.E19.m1.1.1.1"><apply id="S2.E19.m1.1.1.1.2.cmml" xref="S2.E19.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E19.m1.1.1.1.2.1.cmml" xref="S2.E19.m1.1.1.1.2">subscript</csymbol><apply id="S2.E19.m1.1.1.1.2.2.cmml" xref="S2.E19.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E19.m1.1.1.1.2.2.1.cmml" xref="S2.E19.m1.1.1.1.2">superscript</csymbol><sum id="S2.E19.m1.1.1.1.2.2.2.cmml" xref="S2.E19.m1.1.1.1.2.2.2"></sum><ci id="S2.E19.m1.1.1.1.2.2.3.cmml" xref="S2.E19.m1.1.1.1.2.2.3">𝐵</ci></apply><ci id="S2.E19.m1.1.1.1.2.3.cmml" xref="S2.E19.m1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E19.m1.1.1.1.1.1.1.cmml" xref="S2.E19.m1.1.1.1.1.1"><plus id="S2.E19.m1.1.1.1.1.1.1.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.2"></plus><apply id="S2.E19.m1.1.1.1.1.1.1.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1"><times id="S2.E19.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.2"></times><ci id="S2.E19.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.3">𝑐</ci><ci id="S2.E19.m1.1.1.1.1.1.1.1.4.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.4">𝑜</ci><ci id="S2.E19.m1.1.1.1.1.1.1.1.5.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.5">𝑠</ci><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1"><minus id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1"><times id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3"><divide id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3"></divide><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2"><times id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.1"></times><cn type="integer" id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">2</cn><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝜋</ci></apply><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝐶</ci></apply><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑝</ci><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑔</ci><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><ci id="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.1.1.1.1.3">𝜋</ci></apply></apply><cn type="integer" id="S2.E19.m1.1.1.1.1.1.1.3.cmml" xref="S2.E19.m1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E19.m1.1c">L_{reg}=\sum^{B}_{i}\left(cos\left(\frac{2\pi}{C}\times\left(p_{i}-g_{i}\right)-\pi\right)+1\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(19)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS3.p4.4" class="ltx_p">where <math id="S2.SS4.SSS3.p4.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S2.SS4.SSS3.p4.1.m1.1a"><mi id="S2.SS4.SSS3.p4.1.m1.1.1" xref="S2.SS4.SSS3.p4.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p4.1.m1.1b"><ci id="S2.SS4.SSS3.p4.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p4.1.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p4.1.m1.1c">B</annotation></semantics></math> is the batch size. The cosine formulation is used for the periodicity constraint of the clock prediction. <math id="S2.SS4.SSS3.p4.2.m2.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S2.SS4.SSS3.p4.2.m2.1a"><mi id="S2.SS4.SSS3.p4.2.m2.1.1" xref="S2.SS4.SSS3.p4.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p4.2.m2.1b"><ci id="S2.SS4.SSS3.p4.2.m2.1.1.cmml" xref="S2.SS4.SSS3.p4.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p4.2.m2.1c">C</annotation></semantics></math> is the periodicity of hour or minute, which is set to 60 moves. <math id="S2.SS4.SSS3.p4.3.m3.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S2.SS4.SSS3.p4.3.m3.1a"><msub id="S2.SS4.SSS3.p4.3.m3.1.1" xref="S2.SS4.SSS3.p4.3.m3.1.1.cmml"><mi id="S2.SS4.SSS3.p4.3.m3.1.1.2" xref="S2.SS4.SSS3.p4.3.m3.1.1.2.cmml">p</mi><mi id="S2.SS4.SSS3.p4.3.m3.1.1.3" xref="S2.SS4.SSS3.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p4.3.m3.1b"><apply id="S2.SS4.SSS3.p4.3.m3.1.1.cmml" xref="S2.SS4.SSS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p4.3.m3.1.1.1.cmml" xref="S2.SS4.SSS3.p4.3.m3.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p4.3.m3.1.1.2.cmml" xref="S2.SS4.SSS3.p4.3.m3.1.1.2">𝑝</ci><ci id="S2.SS4.SSS3.p4.3.m3.1.1.3.cmml" xref="S2.SS4.SSS3.p4.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p4.3.m3.1c">p_{i}</annotation></semantics></math> and <math id="S2.SS4.SSS3.p4.4.m4.1" class="ltx_Math" alttext="g_{i}" display="inline"><semantics id="S2.SS4.SSS3.p4.4.m4.1a"><msub id="S2.SS4.SSS3.p4.4.m4.1.1" xref="S2.SS4.SSS3.p4.4.m4.1.1.cmml"><mi id="S2.SS4.SSS3.p4.4.m4.1.1.2" xref="S2.SS4.SSS3.p4.4.m4.1.1.2.cmml">g</mi><mi id="S2.SS4.SSS3.p4.4.m4.1.1.3" xref="S2.SS4.SSS3.p4.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p4.4.m4.1b"><apply id="S2.SS4.SSS3.p4.4.m4.1.1.cmml" xref="S2.SS4.SSS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS3.p4.4.m4.1.1.1.cmml" xref="S2.SS4.SSS3.p4.4.m4.1.1">subscript</csymbol><ci id="S2.SS4.SSS3.p4.4.m4.1.1.2.cmml" xref="S2.SS4.SSS3.p4.4.m4.1.1.2">𝑔</ci><ci id="S2.SS4.SSS3.p4.4.m4.1.1.3.cmml" xref="S2.SS4.SSS3.p4.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p4.4.m4.1c">g_{i}</annotation></semantics></math> is the prediction and ground truth.</p>
</div>
<div id="S2.SS4.SSS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS3.p5.1" class="ltx_p">Because one full turn of the minute hand corresponds to 5 moves of the hour hand, a self-supervised loss is introduced. The self-supervision of hour and minute is regarded as a regularization loss to improve the generalization of clock reader.</p>
<table id="S2.E20" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E20.m1.1" class="ltx_Math" alttext="L_{self}=\sum^{B}_{i}Smooth_{L1}\left(C\times\left(p_{h}-\left[p_{h}\right]\right)-p_{m}\right)" display="block"><semantics id="S2.E20.m1.1a"><mrow id="S2.E20.m1.1.1" xref="S2.E20.m1.1.1.cmml"><msub id="S2.E20.m1.1.1.3" xref="S2.E20.m1.1.1.3.cmml"><mi id="S2.E20.m1.1.1.3.2" xref="S2.E20.m1.1.1.3.2.cmml">L</mi><mrow id="S2.E20.m1.1.1.3.3" xref="S2.E20.m1.1.1.3.3.cmml"><mi id="S2.E20.m1.1.1.3.3.2" xref="S2.E20.m1.1.1.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.3.3.1" xref="S2.E20.m1.1.1.3.3.1.cmml">​</mo><mi id="S2.E20.m1.1.1.3.3.3" xref="S2.E20.m1.1.1.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.3.3.1a" xref="S2.E20.m1.1.1.3.3.1.cmml">​</mo><mi id="S2.E20.m1.1.1.3.3.4" xref="S2.E20.m1.1.1.3.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.3.3.1b" xref="S2.E20.m1.1.1.3.3.1.cmml">​</mo><mi id="S2.E20.m1.1.1.3.3.5" xref="S2.E20.m1.1.1.3.3.5.cmml">f</mi></mrow></msub><mo rspace="0.111em" id="S2.E20.m1.1.1.2" xref="S2.E20.m1.1.1.2.cmml">=</mo><mrow id="S2.E20.m1.1.1.1" xref="S2.E20.m1.1.1.1.cmml"><munderover id="S2.E20.m1.1.1.1.2" xref="S2.E20.m1.1.1.1.2.cmml"><mo movablelimits="false" id="S2.E20.m1.1.1.1.2.2.2" xref="S2.E20.m1.1.1.1.2.2.2.cmml">∑</mo><mi id="S2.E20.m1.1.1.1.2.3" xref="S2.E20.m1.1.1.1.2.3.cmml">i</mi><mi id="S2.E20.m1.1.1.1.2.2.3" xref="S2.E20.m1.1.1.1.2.2.3.cmml">B</mi></munderover><mrow id="S2.E20.m1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.cmml"><mi id="S2.E20.m1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.2.cmml">​</mo><mi id="S2.E20.m1.1.1.1.1.4" xref="S2.E20.m1.1.1.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.2a" xref="S2.E20.m1.1.1.1.1.2.cmml">​</mo><mi id="S2.E20.m1.1.1.1.1.5" xref="S2.E20.m1.1.1.1.1.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.2b" xref="S2.E20.m1.1.1.1.1.2.cmml">​</mo><mi id="S2.E20.m1.1.1.1.1.6" xref="S2.E20.m1.1.1.1.1.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.2c" xref="S2.E20.m1.1.1.1.1.2.cmml">​</mo><mi id="S2.E20.m1.1.1.1.1.7" xref="S2.E20.m1.1.1.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.2d" xref="S2.E20.m1.1.1.1.1.2.cmml">​</mo><msub id="S2.E20.m1.1.1.1.1.8" xref="S2.E20.m1.1.1.1.1.8.cmml"><mi id="S2.E20.m1.1.1.1.1.8.2" xref="S2.E20.m1.1.1.1.1.8.2.cmml">h</mi><mrow id="S2.E20.m1.1.1.1.1.8.3" xref="S2.E20.m1.1.1.1.1.8.3.cmml"><mi id="S2.E20.m1.1.1.1.1.8.3.2" xref="S2.E20.m1.1.1.1.1.8.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.8.3.1" xref="S2.E20.m1.1.1.1.1.8.3.1.cmml">​</mo><mn id="S2.E20.m1.1.1.1.1.8.3.3" xref="S2.E20.m1.1.1.1.1.8.3.3.cmml">1</mn></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E20.m1.1.1.1.1.2e" xref="S2.E20.m1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E20.m1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.cmml"><mo id="S2.E20.m1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E20.m1.1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E20.m1.1.1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E20.m1.1.1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.1.3.cmml">C</mi><mo lspace="0.222em" rspace="0.222em" id="S2.E20.m1.1.1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.1.2.cmml">×</mo><mrow id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">h</mi></msub><mo id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">[</mo><msub id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">h</mi></msub><mo id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E20.m1.1.1.1.1.1.1.1.2" xref="S2.E20.m1.1.1.1.1.1.1.1.2.cmml">−</mo><msub id="S2.E20.m1.1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E20.m1.1.1.1.1.1.1.1.3.2" xref="S2.E20.m1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S2.E20.m1.1.1.1.1.1.1.1.3.3" xref="S2.E20.m1.1.1.1.1.1.1.1.3.3.cmml">m</mi></msub></mrow><mo id="S2.E20.m1.1.1.1.1.1.1.3" xref="S2.E20.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E20.m1.1b"><apply id="S2.E20.m1.1.1.cmml" xref="S2.E20.m1.1.1"><eq id="S2.E20.m1.1.1.2.cmml" xref="S2.E20.m1.1.1.2"></eq><apply id="S2.E20.m1.1.1.3.cmml" xref="S2.E20.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.3.1.cmml" xref="S2.E20.m1.1.1.3">subscript</csymbol><ci id="S2.E20.m1.1.1.3.2.cmml" xref="S2.E20.m1.1.1.3.2">𝐿</ci><apply id="S2.E20.m1.1.1.3.3.cmml" xref="S2.E20.m1.1.1.3.3"><times id="S2.E20.m1.1.1.3.3.1.cmml" xref="S2.E20.m1.1.1.3.3.1"></times><ci id="S2.E20.m1.1.1.3.3.2.cmml" xref="S2.E20.m1.1.1.3.3.2">𝑠</ci><ci id="S2.E20.m1.1.1.3.3.3.cmml" xref="S2.E20.m1.1.1.3.3.3">𝑒</ci><ci id="S2.E20.m1.1.1.3.3.4.cmml" xref="S2.E20.m1.1.1.3.3.4">𝑙</ci><ci id="S2.E20.m1.1.1.3.3.5.cmml" xref="S2.E20.m1.1.1.3.3.5">𝑓</ci></apply></apply><apply id="S2.E20.m1.1.1.1.cmml" xref="S2.E20.m1.1.1.1"><apply id="S2.E20.m1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.1.2.1.cmml" xref="S2.E20.m1.1.1.1.2">subscript</csymbol><apply id="S2.E20.m1.1.1.1.2.2.cmml" xref="S2.E20.m1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.1.2.2.1.cmml" xref="S2.E20.m1.1.1.1.2">superscript</csymbol><sum id="S2.E20.m1.1.1.1.2.2.2.cmml" xref="S2.E20.m1.1.1.1.2.2.2"></sum><ci id="S2.E20.m1.1.1.1.2.2.3.cmml" xref="S2.E20.m1.1.1.1.2.2.3">𝐵</ci></apply><ci id="S2.E20.m1.1.1.1.2.3.cmml" xref="S2.E20.m1.1.1.1.2.3">𝑖</ci></apply><apply id="S2.E20.m1.1.1.1.1.cmml" xref="S2.E20.m1.1.1.1.1"><times id="S2.E20.m1.1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.1.2"></times><ci id="S2.E20.m1.1.1.1.1.3.cmml" xref="S2.E20.m1.1.1.1.1.3">𝑆</ci><ci id="S2.E20.m1.1.1.1.1.4.cmml" xref="S2.E20.m1.1.1.1.1.4">𝑚</ci><ci id="S2.E20.m1.1.1.1.1.5.cmml" xref="S2.E20.m1.1.1.1.1.5">𝑜</ci><ci id="S2.E20.m1.1.1.1.1.6.cmml" xref="S2.E20.m1.1.1.1.1.6">𝑜</ci><ci id="S2.E20.m1.1.1.1.1.7.cmml" xref="S2.E20.m1.1.1.1.1.7">𝑡</ci><apply id="S2.E20.m1.1.1.1.1.8.cmml" xref="S2.E20.m1.1.1.1.1.8"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.1.1.8.1.cmml" xref="S2.E20.m1.1.1.1.1.8">subscript</csymbol><ci id="S2.E20.m1.1.1.1.1.8.2.cmml" xref="S2.E20.m1.1.1.1.1.8.2">ℎ</ci><apply id="S2.E20.m1.1.1.1.1.8.3.cmml" xref="S2.E20.m1.1.1.1.1.8.3"><times id="S2.E20.m1.1.1.1.1.8.3.1.cmml" xref="S2.E20.m1.1.1.1.1.8.3.1"></times><ci id="S2.E20.m1.1.1.1.1.8.3.2.cmml" xref="S2.E20.m1.1.1.1.1.8.3.2">𝐿</ci><cn type="integer" id="S2.E20.m1.1.1.1.1.8.3.3.cmml" xref="S2.E20.m1.1.1.1.1.8.3.3">1</cn></apply></apply><apply id="S2.E20.m1.1.1.1.1.1.1.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1"><minus id="S2.E20.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.2"></minus><apply id="S2.E20.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1"><times id="S2.E20.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.2"></times><ci id="S2.E20.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.3">𝐶</ci><apply id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.2"></minus><apply id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">ℎ</ci></apply><apply id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑝</ci><ci id="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ℎ</ci></apply></apply></apply></apply><apply id="S2.E20.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E20.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E20.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S2.E20.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E20.m1.1.1.1.1.1.1.1.3.3">𝑚</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E20.m1.1c">L_{self}=\sum^{B}_{i}Smooth_{L1}\left(C\times\left(p_{h}-\left[p_{h}\right]\right)-p_{m}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(20)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS4.SSS3.p6" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS3.p6.2" class="ltx_p">Finally, the total loss is:</p>
<table id="S2.E21" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E21.m1.1" class="ltx_Math" alttext="L=L_{cls}+L_{self}+\lambda L_{reg}" display="block"><semantics id="S2.E21.m1.1a"><mrow id="S2.E21.m1.1.1" xref="S2.E21.m1.1.1.cmml"><mi id="S2.E21.m1.1.1.2" xref="S2.E21.m1.1.1.2.cmml">L</mi><mo id="S2.E21.m1.1.1.1" xref="S2.E21.m1.1.1.1.cmml">=</mo><mrow id="S2.E21.m1.1.1.3" xref="S2.E21.m1.1.1.3.cmml"><msub id="S2.E21.m1.1.1.3.2" xref="S2.E21.m1.1.1.3.2.cmml"><mi id="S2.E21.m1.1.1.3.2.2" xref="S2.E21.m1.1.1.3.2.2.cmml">L</mi><mrow id="S2.E21.m1.1.1.3.2.3" xref="S2.E21.m1.1.1.3.2.3.cmml"><mi id="S2.E21.m1.1.1.3.2.3.2" xref="S2.E21.m1.1.1.3.2.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.2.3.1" xref="S2.E21.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.2.3.3" xref="S2.E21.m1.1.1.3.2.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.2.3.1a" xref="S2.E21.m1.1.1.3.2.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.2.3.4" xref="S2.E21.m1.1.1.3.2.3.4.cmml">s</mi></mrow></msub><mo id="S2.E21.m1.1.1.3.1" xref="S2.E21.m1.1.1.3.1.cmml">+</mo><msub id="S2.E21.m1.1.1.3.3" xref="S2.E21.m1.1.1.3.3.cmml"><mi id="S2.E21.m1.1.1.3.3.2" xref="S2.E21.m1.1.1.3.3.2.cmml">L</mi><mrow id="S2.E21.m1.1.1.3.3.3" xref="S2.E21.m1.1.1.3.3.3.cmml"><mi id="S2.E21.m1.1.1.3.3.3.2" xref="S2.E21.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.3.3.1" xref="S2.E21.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.3.3.3" xref="S2.E21.m1.1.1.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.3.3.1a" xref="S2.E21.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.3.3.4" xref="S2.E21.m1.1.1.3.3.3.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.3.3.1b" xref="S2.E21.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.3.3.5" xref="S2.E21.m1.1.1.3.3.3.5.cmml">f</mi></mrow></msub><mo id="S2.E21.m1.1.1.3.1a" xref="S2.E21.m1.1.1.3.1.cmml">+</mo><mrow id="S2.E21.m1.1.1.3.4" xref="S2.E21.m1.1.1.3.4.cmml"><mi id="S2.E21.m1.1.1.3.4.2" xref="S2.E21.m1.1.1.3.4.2.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.4.1" xref="S2.E21.m1.1.1.3.4.1.cmml">​</mo><msub id="S2.E21.m1.1.1.3.4.3" xref="S2.E21.m1.1.1.3.4.3.cmml"><mi id="S2.E21.m1.1.1.3.4.3.2" xref="S2.E21.m1.1.1.3.4.3.2.cmml">L</mi><mrow id="S2.E21.m1.1.1.3.4.3.3" xref="S2.E21.m1.1.1.3.4.3.3.cmml"><mi id="S2.E21.m1.1.1.3.4.3.3.2" xref="S2.E21.m1.1.1.3.4.3.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.4.3.3.1" xref="S2.E21.m1.1.1.3.4.3.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.4.3.3.3" xref="S2.E21.m1.1.1.3.4.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E21.m1.1.1.3.4.3.3.1a" xref="S2.E21.m1.1.1.3.4.3.3.1.cmml">​</mo><mi id="S2.E21.m1.1.1.3.4.3.3.4" xref="S2.E21.m1.1.1.3.4.3.3.4.cmml">g</mi></mrow></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E21.m1.1b"><apply id="S2.E21.m1.1.1.cmml" xref="S2.E21.m1.1.1"><eq id="S2.E21.m1.1.1.1.cmml" xref="S2.E21.m1.1.1.1"></eq><ci id="S2.E21.m1.1.1.2.cmml" xref="S2.E21.m1.1.1.2">𝐿</ci><apply id="S2.E21.m1.1.1.3.cmml" xref="S2.E21.m1.1.1.3"><plus id="S2.E21.m1.1.1.3.1.cmml" xref="S2.E21.m1.1.1.3.1"></plus><apply id="S2.E21.m1.1.1.3.2.cmml" xref="S2.E21.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E21.m1.1.1.3.2.1.cmml" xref="S2.E21.m1.1.1.3.2">subscript</csymbol><ci id="S2.E21.m1.1.1.3.2.2.cmml" xref="S2.E21.m1.1.1.3.2.2">𝐿</ci><apply id="S2.E21.m1.1.1.3.2.3.cmml" xref="S2.E21.m1.1.1.3.2.3"><times id="S2.E21.m1.1.1.3.2.3.1.cmml" xref="S2.E21.m1.1.1.3.2.3.1"></times><ci id="S2.E21.m1.1.1.3.2.3.2.cmml" xref="S2.E21.m1.1.1.3.2.3.2">𝑐</ci><ci id="S2.E21.m1.1.1.3.2.3.3.cmml" xref="S2.E21.m1.1.1.3.2.3.3">𝑙</ci><ci id="S2.E21.m1.1.1.3.2.3.4.cmml" xref="S2.E21.m1.1.1.3.2.3.4">𝑠</ci></apply></apply><apply id="S2.E21.m1.1.1.3.3.cmml" xref="S2.E21.m1.1.1.3.3"><csymbol cd="ambiguous" id="S2.E21.m1.1.1.3.3.1.cmml" xref="S2.E21.m1.1.1.3.3">subscript</csymbol><ci id="S2.E21.m1.1.1.3.3.2.cmml" xref="S2.E21.m1.1.1.3.3.2">𝐿</ci><apply id="S2.E21.m1.1.1.3.3.3.cmml" xref="S2.E21.m1.1.1.3.3.3"><times id="S2.E21.m1.1.1.3.3.3.1.cmml" xref="S2.E21.m1.1.1.3.3.3.1"></times><ci id="S2.E21.m1.1.1.3.3.3.2.cmml" xref="S2.E21.m1.1.1.3.3.3.2">𝑠</ci><ci id="S2.E21.m1.1.1.3.3.3.3.cmml" xref="S2.E21.m1.1.1.3.3.3.3">𝑒</ci><ci id="S2.E21.m1.1.1.3.3.3.4.cmml" xref="S2.E21.m1.1.1.3.3.3.4">𝑙</ci><ci id="S2.E21.m1.1.1.3.3.3.5.cmml" xref="S2.E21.m1.1.1.3.3.3.5">𝑓</ci></apply></apply><apply id="S2.E21.m1.1.1.3.4.cmml" xref="S2.E21.m1.1.1.3.4"><times id="S2.E21.m1.1.1.3.4.1.cmml" xref="S2.E21.m1.1.1.3.4.1"></times><ci id="S2.E21.m1.1.1.3.4.2.cmml" xref="S2.E21.m1.1.1.3.4.2">𝜆</ci><apply id="S2.E21.m1.1.1.3.4.3.cmml" xref="S2.E21.m1.1.1.3.4.3"><csymbol cd="ambiguous" id="S2.E21.m1.1.1.3.4.3.1.cmml" xref="S2.E21.m1.1.1.3.4.3">subscript</csymbol><ci id="S2.E21.m1.1.1.3.4.3.2.cmml" xref="S2.E21.m1.1.1.3.4.3.2">𝐿</ci><apply id="S2.E21.m1.1.1.3.4.3.3.cmml" xref="S2.E21.m1.1.1.3.4.3.3"><times id="S2.E21.m1.1.1.3.4.3.3.1.cmml" xref="S2.E21.m1.1.1.3.4.3.3.1"></times><ci id="S2.E21.m1.1.1.3.4.3.3.2.cmml" xref="S2.E21.m1.1.1.3.4.3.3.2">𝑟</ci><ci id="S2.E21.m1.1.1.3.4.3.3.3.cmml" xref="S2.E21.m1.1.1.3.4.3.3.3">𝑒</ci><ci id="S2.E21.m1.1.1.3.4.3.3.4.cmml" xref="S2.E21.m1.1.1.3.4.3.3.4">𝑔</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E21.m1.1c">L=L_{cls}+L_{self}+\lambda L_{reg}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(21)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS3.p6.1" class="ltx_p">where <math id="S2.SS4.SSS3.p6.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS4.SSS3.p6.1.m1.1a"><mi id="S2.SS4.SSS3.p6.1.m1.1.1" xref="S2.SS4.SSS3.p6.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS3.p6.1.m1.1b"><ci id="S2.SS4.SSS3.p6.1.m1.1.1.cmml" xref="S2.SS4.SSS3.p6.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS3.p6.1.m1.1c">\lambda</annotation></semantics></math> is used to weight the self-supervised loss, and set to 0.01.</p>
</div>
</section>
<section id="S2.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.4 </span>Visual Understanding Expert</h4>

<div id="S2.SS4.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS4.p1.1" class="ltx_p">Vison-and-Language Pre-training (VLP) models with different visual features can help capture diverse visual semantics from images, which facilitates deep vision-and-language understanding. For example, region features are good at capturing objects in an image, and thus very useful for answering questions on object counting. Grid features retain global or background information of an image, which can help to answer descriptive questions. Therefore, a <span id="S2.SS4.SSS4.p1.1.1" class="ltx_text ltx_font_italic">diverse feature ensemble</span> method is used to construct our visual understanding expert, which ensembles multiple VLP models with different types of visual features: <span id="S2.SS4.SSS4.p1.1.2" class="ltx_text ltx_font_italic">region feature</span>, <span id="S2.SS4.SSS4.p1.1.3" class="ltx_text ltx_font_italic">grid feature</span> and <span id="S2.SS4.SSS4.p1.1.4" class="ltx_text ltx_font_italic">patch feature</span>. For each kind of features, different VLP models are trained separately. A simple maximum voting strategy is then utilized to ensemble the different VLP models based on the prediction score. Our experiments demonstrate the advantage of the diverse feature ensemble over a single class of visual features.</p>
</div>
</section>
<section id="S2.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.5 </span>Putting It All Together by MoE</h4>

<div id="S2.SS4.SSS5.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS5.p1.1" class="ltx_p">The methodology of Mixture of Experts (MoE)  <cite class="ltx_cite ltx_citemacro_citep">(Jacobs et al., <a href="#bib.bib41" title="" class="ltx_ref">1991</a>; Shazeer et al., <a href="#bib.bib42" title="" class="ltx_ref">2017</a>)</cite> essentially decomposes a task into sub-tasks, and develops an expert on each sub-task. A gating network is then used to coordinate multiple experts for task completion. We follow the recent work of Switch Transformers <cite class="ltx_cite ltx_citemacro_citep">(Fedus et al., <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>, and adopt the simple routing strategy that the the gating network routes to only a <em id="S2.SS4.SSS5.p1.1.1" class="ltx_emph ltx_font_italic">single</em> expert. We use the simplest form to preserves model quality and reduce routing computation.
In our framework, the VQA task can be decomposed into three sub-tasks according to the analysis of the visual questions via proposed knowledge mining. A multi-layer perception network is trained as the gating network, which performs three-class classification to determine which expert to choose for each instance.</p>
</div>
<div id="S2.SS4.SSS5.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS5.p2.5" class="ltx_p">Given an expert <math id="S2.SS4.SSS5.p2.1.m1.1" class="ltx_Math" alttext="M_{t}" display="inline"><semantics id="S2.SS4.SSS5.p2.1.m1.1a"><msub id="S2.SS4.SSS5.p2.1.m1.1.1" xref="S2.SS4.SSS5.p2.1.m1.1.1.cmml"><mi id="S2.SS4.SSS5.p2.1.m1.1.1.2" xref="S2.SS4.SSS5.p2.1.m1.1.1.2.cmml">M</mi><mi id="S2.SS4.SSS5.p2.1.m1.1.1.3" xref="S2.SS4.SSS5.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.1.m1.1b"><apply id="S2.SS4.SSS5.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.1.m1.1.1.1.cmml" xref="S2.SS4.SSS5.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS5.p2.1.m1.1.1.2.cmml" xref="S2.SS4.SSS5.p2.1.m1.1.1.2">𝑀</ci><ci id="S2.SS4.SSS5.p2.1.m1.1.1.3.cmml" xref="S2.SS4.SSS5.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.1.m1.1c">M_{t}</annotation></semantics></math> for sub-task <math id="S2.SS4.SSS5.p2.2.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS4.SSS5.p2.2.m2.1a"><mi id="S2.SS4.SSS5.p2.2.m2.1.1" xref="S2.SS4.SSS5.p2.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.2.m2.1b"><ci id="S2.SS4.SSS5.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS5.p2.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.2.m2.1c">t</annotation></semantics></math>, the expert will give an answer and we compute a reward score <math id="S2.SS4.SSS5.p2.3.m3.1" class="ltx_Math" alttext="s_{t}" display="inline"><semantics id="S2.SS4.SSS5.p2.3.m3.1a"><msub id="S2.SS4.SSS5.p2.3.m3.1.1" xref="S2.SS4.SSS5.p2.3.m3.1.1.cmml"><mi id="S2.SS4.SSS5.p2.3.m3.1.1.2" xref="S2.SS4.SSS5.p2.3.m3.1.1.2.cmml">s</mi><mi id="S2.SS4.SSS5.p2.3.m3.1.1.3" xref="S2.SS4.SSS5.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.3.m3.1b"><apply id="S2.SS4.SSS5.p2.3.m3.1.1.cmml" xref="S2.SS4.SSS5.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.3.m3.1.1.1.cmml" xref="S2.SS4.SSS5.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS4.SSS5.p2.3.m3.1.1.2.cmml" xref="S2.SS4.SSS5.p2.3.m3.1.1.2">𝑠</ci><ci id="S2.SS4.SSS5.p2.3.m3.1.1.3.cmml" xref="S2.SS4.SSS5.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.3.m3.1c">s_{t}</annotation></semantics></math> between the predicted answer and the human annotated labels using Equation <a href="#S3.E24" title="In Evaluation Metric ‣ 3.1 Data ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>. The reward score <math id="S2.SS4.SSS5.p2.4.m4.1" class="ltx_Math" alttext="s_{t}" display="inline"><semantics id="S2.SS4.SSS5.p2.4.m4.1a"><msub id="S2.SS4.SSS5.p2.4.m4.1.1" xref="S2.SS4.SSS5.p2.4.m4.1.1.cmml"><mi id="S2.SS4.SSS5.p2.4.m4.1.1.2" xref="S2.SS4.SSS5.p2.4.m4.1.1.2.cmml">s</mi><mi id="S2.SS4.SSS5.p2.4.m4.1.1.3" xref="S2.SS4.SSS5.p2.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.4.m4.1b"><apply id="S2.SS4.SSS5.p2.4.m4.1.1.cmml" xref="S2.SS4.SSS5.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.4.m4.1.1.1.cmml" xref="S2.SS4.SSS5.p2.4.m4.1.1">subscript</csymbol><ci id="S2.SS4.SSS5.p2.4.m4.1.1.2.cmml" xref="S2.SS4.SSS5.p2.4.m4.1.1.2">𝑠</ci><ci id="S2.SS4.SSS5.p2.4.m4.1.1.3.cmml" xref="S2.SS4.SSS5.p2.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.4.m4.1c">s_{t}</annotation></semantics></math> is used as supervision for training, where the network is trained to route each instance to its best-match expert. At training time, we propose to maximize the Binary Cross Entropy (BCE) loss <math id="S2.SS4.SSS5.p2.5.m5.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S2.SS4.SSS5.p2.5.m5.1a"><mi id="S2.SS4.SSS5.p2.5.m5.1.1" xref="S2.SS4.SSS5.p2.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.5.m5.1b"><ci id="S2.SS4.SSS5.p2.5.m5.1.1.cmml" xref="S2.SS4.SSS5.p2.5.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.5.m5.1c">L</annotation></semantics></math> as follows:</p>
<table id="S2.E22" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E22.m1.3" class="ltx_Math" alttext="L_{MoE}=\sum_{t}s_{t}\log{\hat{s_{t}}}+(1-s_{t})\log{(1-\hat{s_{t}})}" display="block"><semantics id="S2.E22.m1.3a"><mrow id="S2.E22.m1.3.3" xref="S2.E22.m1.3.3.cmml"><msub id="S2.E22.m1.3.3.4" xref="S2.E22.m1.3.3.4.cmml"><mi id="S2.E22.m1.3.3.4.2" xref="S2.E22.m1.3.3.4.2.cmml">L</mi><mrow id="S2.E22.m1.3.3.4.3" xref="S2.E22.m1.3.3.4.3.cmml"><mi id="S2.E22.m1.3.3.4.3.2" xref="S2.E22.m1.3.3.4.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.E22.m1.3.3.4.3.1" xref="S2.E22.m1.3.3.4.3.1.cmml">​</mo><mi id="S2.E22.m1.3.3.4.3.3" xref="S2.E22.m1.3.3.4.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E22.m1.3.3.4.3.1a" xref="S2.E22.m1.3.3.4.3.1.cmml">​</mo><mi id="S2.E22.m1.3.3.4.3.4" xref="S2.E22.m1.3.3.4.3.4.cmml">E</mi></mrow></msub><mo rspace="0.111em" id="S2.E22.m1.3.3.3" xref="S2.E22.m1.3.3.3.cmml">=</mo><mrow id="S2.E22.m1.3.3.2" xref="S2.E22.m1.3.3.2.cmml"><mrow id="S2.E22.m1.3.3.2.4" xref="S2.E22.m1.3.3.2.4.cmml"><munder id="S2.E22.m1.3.3.2.4.1" xref="S2.E22.m1.3.3.2.4.1.cmml"><mo movablelimits="false" id="S2.E22.m1.3.3.2.4.1.2" xref="S2.E22.m1.3.3.2.4.1.2.cmml">∑</mo><mi id="S2.E22.m1.3.3.2.4.1.3" xref="S2.E22.m1.3.3.2.4.1.3.cmml">t</mi></munder><mrow id="S2.E22.m1.3.3.2.4.2" xref="S2.E22.m1.3.3.2.4.2.cmml"><msub id="S2.E22.m1.3.3.2.4.2.2" xref="S2.E22.m1.3.3.2.4.2.2.cmml"><mi id="S2.E22.m1.3.3.2.4.2.2.2" xref="S2.E22.m1.3.3.2.4.2.2.2.cmml">s</mi><mi id="S2.E22.m1.3.3.2.4.2.2.3" xref="S2.E22.m1.3.3.2.4.2.2.3.cmml">t</mi></msub><mo lspace="0.167em" rspace="0em" id="S2.E22.m1.3.3.2.4.2.1" xref="S2.E22.m1.3.3.2.4.2.1.cmml">​</mo><mrow id="S2.E22.m1.3.3.2.4.2.3" xref="S2.E22.m1.3.3.2.4.2.3.cmml"><mi id="S2.E22.m1.3.3.2.4.2.3.1" xref="S2.E22.m1.3.3.2.4.2.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E22.m1.3.3.2.4.2.3a" xref="S2.E22.m1.3.3.2.4.2.3.cmml">⁡</mo><mover accent="true" id="S2.E22.m1.3.3.2.4.2.3.2" xref="S2.E22.m1.3.3.2.4.2.3.2.cmml"><msub id="S2.E22.m1.3.3.2.4.2.3.2.2" xref="S2.E22.m1.3.3.2.4.2.3.2.2.cmml"><mi id="S2.E22.m1.3.3.2.4.2.3.2.2.2" xref="S2.E22.m1.3.3.2.4.2.3.2.2.2.cmml">s</mi><mi id="S2.E22.m1.3.3.2.4.2.3.2.2.3" xref="S2.E22.m1.3.3.2.4.2.3.2.2.3.cmml">t</mi></msub><mo id="S2.E22.m1.3.3.2.4.2.3.2.1" xref="S2.E22.m1.3.3.2.4.2.3.2.1.cmml">^</mo></mover></mrow></mrow></mrow><mo id="S2.E22.m1.3.3.2.3" xref="S2.E22.m1.3.3.2.3.cmml">+</mo><mrow id="S2.E22.m1.3.3.2.2" xref="S2.E22.m1.3.3.2.2.cmml"><mrow id="S2.E22.m1.2.2.1.1.1.1" xref="S2.E22.m1.2.2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E22.m1.2.2.1.1.1.1.2" xref="S2.E22.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.E22.m1.2.2.1.1.1.1.1" xref="S2.E22.m1.2.2.1.1.1.1.1.cmml"><mn id="S2.E22.m1.2.2.1.1.1.1.1.2" xref="S2.E22.m1.2.2.1.1.1.1.1.2.cmml">1</mn><mo id="S2.E22.m1.2.2.1.1.1.1.1.1" xref="S2.E22.m1.2.2.1.1.1.1.1.1.cmml">−</mo><msub id="S2.E22.m1.2.2.1.1.1.1.1.3" xref="S2.E22.m1.2.2.1.1.1.1.1.3.cmml"><mi id="S2.E22.m1.2.2.1.1.1.1.1.3.2" xref="S2.E22.m1.2.2.1.1.1.1.1.3.2.cmml">s</mi><mi id="S2.E22.m1.2.2.1.1.1.1.1.3.3" xref="S2.E22.m1.2.2.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo stretchy="false" id="S2.E22.m1.2.2.1.1.1.1.3" xref="S2.E22.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0.167em" rspace="0em" id="S2.E22.m1.3.3.2.2.3" xref="S2.E22.m1.3.3.2.2.3.cmml">​</mo><mrow id="S2.E22.m1.3.3.2.2.2.1" xref="S2.E22.m1.3.3.2.2.2.2.cmml"><mi id="S2.E22.m1.1.1" xref="S2.E22.m1.1.1.cmml">log</mi><mo id="S2.E22.m1.3.3.2.2.2.1a" xref="S2.E22.m1.3.3.2.2.2.2.cmml">⁡</mo><mrow id="S2.E22.m1.3.3.2.2.2.1.1" xref="S2.E22.m1.3.3.2.2.2.2.cmml"><mo stretchy="false" id="S2.E22.m1.3.3.2.2.2.1.1.2" xref="S2.E22.m1.3.3.2.2.2.2.cmml">(</mo><mrow id="S2.E22.m1.3.3.2.2.2.1.1.1" xref="S2.E22.m1.3.3.2.2.2.1.1.1.cmml"><mn id="S2.E22.m1.3.3.2.2.2.1.1.1.2" xref="S2.E22.m1.3.3.2.2.2.1.1.1.2.cmml">1</mn><mo id="S2.E22.m1.3.3.2.2.2.1.1.1.1" xref="S2.E22.m1.3.3.2.2.2.1.1.1.1.cmml">−</mo><mover accent="true" id="S2.E22.m1.3.3.2.2.2.1.1.1.3" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.cmml"><msub id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.cmml"><mi id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.2" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.2.cmml">s</mi><mi id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.3" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.3.cmml">t</mi></msub><mo id="S2.E22.m1.3.3.2.2.2.1.1.1.3.1" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S2.E22.m1.3.3.2.2.2.1.1.3" xref="S2.E22.m1.3.3.2.2.2.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E22.m1.3b"><apply id="S2.E22.m1.3.3.cmml" xref="S2.E22.m1.3.3"><eq id="S2.E22.m1.3.3.3.cmml" xref="S2.E22.m1.3.3.3"></eq><apply id="S2.E22.m1.3.3.4.cmml" xref="S2.E22.m1.3.3.4"><csymbol cd="ambiguous" id="S2.E22.m1.3.3.4.1.cmml" xref="S2.E22.m1.3.3.4">subscript</csymbol><ci id="S2.E22.m1.3.3.4.2.cmml" xref="S2.E22.m1.3.3.4.2">𝐿</ci><apply id="S2.E22.m1.3.3.4.3.cmml" xref="S2.E22.m1.3.3.4.3"><times id="S2.E22.m1.3.3.4.3.1.cmml" xref="S2.E22.m1.3.3.4.3.1"></times><ci id="S2.E22.m1.3.3.4.3.2.cmml" xref="S2.E22.m1.3.3.4.3.2">𝑀</ci><ci id="S2.E22.m1.3.3.4.3.3.cmml" xref="S2.E22.m1.3.3.4.3.3">𝑜</ci><ci id="S2.E22.m1.3.3.4.3.4.cmml" xref="S2.E22.m1.3.3.4.3.4">𝐸</ci></apply></apply><apply id="S2.E22.m1.3.3.2.cmml" xref="S2.E22.m1.3.3.2"><plus id="S2.E22.m1.3.3.2.3.cmml" xref="S2.E22.m1.3.3.2.3"></plus><apply id="S2.E22.m1.3.3.2.4.cmml" xref="S2.E22.m1.3.3.2.4"><apply id="S2.E22.m1.3.3.2.4.1.cmml" xref="S2.E22.m1.3.3.2.4.1"><csymbol cd="ambiguous" id="S2.E22.m1.3.3.2.4.1.1.cmml" xref="S2.E22.m1.3.3.2.4.1">subscript</csymbol><sum id="S2.E22.m1.3.3.2.4.1.2.cmml" xref="S2.E22.m1.3.3.2.4.1.2"></sum><ci id="S2.E22.m1.3.3.2.4.1.3.cmml" xref="S2.E22.m1.3.3.2.4.1.3">𝑡</ci></apply><apply id="S2.E22.m1.3.3.2.4.2.cmml" xref="S2.E22.m1.3.3.2.4.2"><times id="S2.E22.m1.3.3.2.4.2.1.cmml" xref="S2.E22.m1.3.3.2.4.2.1"></times><apply id="S2.E22.m1.3.3.2.4.2.2.cmml" xref="S2.E22.m1.3.3.2.4.2.2"><csymbol cd="ambiguous" id="S2.E22.m1.3.3.2.4.2.2.1.cmml" xref="S2.E22.m1.3.3.2.4.2.2">subscript</csymbol><ci id="S2.E22.m1.3.3.2.4.2.2.2.cmml" xref="S2.E22.m1.3.3.2.4.2.2.2">𝑠</ci><ci id="S2.E22.m1.3.3.2.4.2.2.3.cmml" xref="S2.E22.m1.3.3.2.4.2.2.3">𝑡</ci></apply><apply id="S2.E22.m1.3.3.2.4.2.3.cmml" xref="S2.E22.m1.3.3.2.4.2.3"><log id="S2.E22.m1.3.3.2.4.2.3.1.cmml" xref="S2.E22.m1.3.3.2.4.2.3.1"></log><apply id="S2.E22.m1.3.3.2.4.2.3.2.cmml" xref="S2.E22.m1.3.3.2.4.2.3.2"><ci id="S2.E22.m1.3.3.2.4.2.3.2.1.cmml" xref="S2.E22.m1.3.3.2.4.2.3.2.1">^</ci><apply id="S2.E22.m1.3.3.2.4.2.3.2.2.cmml" xref="S2.E22.m1.3.3.2.4.2.3.2.2"><csymbol cd="ambiguous" id="S2.E22.m1.3.3.2.4.2.3.2.2.1.cmml" xref="S2.E22.m1.3.3.2.4.2.3.2.2">subscript</csymbol><ci id="S2.E22.m1.3.3.2.4.2.3.2.2.2.cmml" xref="S2.E22.m1.3.3.2.4.2.3.2.2.2">𝑠</ci><ci id="S2.E22.m1.3.3.2.4.2.3.2.2.3.cmml" xref="S2.E22.m1.3.3.2.4.2.3.2.2.3">𝑡</ci></apply></apply></apply></apply></apply><apply id="S2.E22.m1.3.3.2.2.cmml" xref="S2.E22.m1.3.3.2.2"><times id="S2.E22.m1.3.3.2.2.3.cmml" xref="S2.E22.m1.3.3.2.2.3"></times><apply id="S2.E22.m1.2.2.1.1.1.1.1.cmml" xref="S2.E22.m1.2.2.1.1.1.1"><minus id="S2.E22.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.E22.m1.2.2.1.1.1.1.1.1"></minus><cn type="integer" id="S2.E22.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.E22.m1.2.2.1.1.1.1.1.2">1</cn><apply id="S2.E22.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.E22.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E22.m1.2.2.1.1.1.1.1.3.1.cmml" xref="S2.E22.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E22.m1.2.2.1.1.1.1.1.3.2.cmml" xref="S2.E22.m1.2.2.1.1.1.1.1.3.2">𝑠</ci><ci id="S2.E22.m1.2.2.1.1.1.1.1.3.3.cmml" xref="S2.E22.m1.2.2.1.1.1.1.1.3.3">𝑡</ci></apply></apply><apply id="S2.E22.m1.3.3.2.2.2.2.cmml" xref="S2.E22.m1.3.3.2.2.2.1"><log id="S2.E22.m1.1.1.cmml" xref="S2.E22.m1.1.1"></log><apply id="S2.E22.m1.3.3.2.2.2.1.1.1.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1"><minus id="S2.E22.m1.3.3.2.2.2.1.1.1.1.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.1"></minus><cn type="integer" id="S2.E22.m1.3.3.2.2.2.1.1.1.2.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.2">1</cn><apply id="S2.E22.m1.3.3.2.2.2.1.1.1.3.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3"><ci id="S2.E22.m1.3.3.2.2.2.1.1.1.3.1.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.1">^</ci><apply id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.1.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2">subscript</csymbol><ci id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.2.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.2">𝑠</ci><ci id="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.3.cmml" xref="S2.E22.m1.3.3.2.2.2.1.1.1.3.2.3">𝑡</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E22.m1.3c">L_{MoE}=\sum_{t}s_{t}\log{\hat{s_{t}}}+(1-s_{t})\log{(1-\hat{s_{t}})}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(22)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS5.p2.10" class="ltx_p">where <math id="S2.SS4.SSS5.p2.6.m1.1" class="ltx_Math" alttext="s_{t}" display="inline"><semantics id="S2.SS4.SSS5.p2.6.m1.1a"><msub id="S2.SS4.SSS5.p2.6.m1.1.1" xref="S2.SS4.SSS5.p2.6.m1.1.1.cmml"><mi id="S2.SS4.SSS5.p2.6.m1.1.1.2" xref="S2.SS4.SSS5.p2.6.m1.1.1.2.cmml">s</mi><mi id="S2.SS4.SSS5.p2.6.m1.1.1.3" xref="S2.SS4.SSS5.p2.6.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.6.m1.1b"><apply id="S2.SS4.SSS5.p2.6.m1.1.1.cmml" xref="S2.SS4.SSS5.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.6.m1.1.1.1.cmml" xref="S2.SS4.SSS5.p2.6.m1.1.1">subscript</csymbol><ci id="S2.SS4.SSS5.p2.6.m1.1.1.2.cmml" xref="S2.SS4.SSS5.p2.6.m1.1.1.2">𝑠</ci><ci id="S2.SS4.SSS5.p2.6.m1.1.1.3.cmml" xref="S2.SS4.SSS5.p2.6.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.6.m1.1c">s_{t}</annotation></semantics></math> denotes the ground-truth reward score of sub-task <math id="S2.SS4.SSS5.p2.7.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS4.SSS5.p2.7.m2.1a"><mi id="S2.SS4.SSS5.p2.7.m2.1.1" xref="S2.SS4.SSS5.p2.7.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.7.m2.1b"><ci id="S2.SS4.SSS5.p2.7.m2.1.1.cmml" xref="S2.SS4.SSS5.p2.7.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.7.m2.1c">t</annotation></semantics></math>, <math id="S2.SS4.SSS5.p2.8.m3.1" class="ltx_Math" alttext="\hat{s_{t}}" display="inline"><semantics id="S2.SS4.SSS5.p2.8.m3.1a"><mover accent="true" id="S2.SS4.SSS5.p2.8.m3.1.1" xref="S2.SS4.SSS5.p2.8.m3.1.1.cmml"><msub id="S2.SS4.SSS5.p2.8.m3.1.1.2" xref="S2.SS4.SSS5.p2.8.m3.1.1.2.cmml"><mi id="S2.SS4.SSS5.p2.8.m3.1.1.2.2" xref="S2.SS4.SSS5.p2.8.m3.1.1.2.2.cmml">s</mi><mi id="S2.SS4.SSS5.p2.8.m3.1.1.2.3" xref="S2.SS4.SSS5.p2.8.m3.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS4.SSS5.p2.8.m3.1.1.1" xref="S2.SS4.SSS5.p2.8.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.8.m3.1b"><apply id="S2.SS4.SSS5.p2.8.m3.1.1.cmml" xref="S2.SS4.SSS5.p2.8.m3.1.1"><ci id="S2.SS4.SSS5.p2.8.m3.1.1.1.cmml" xref="S2.SS4.SSS5.p2.8.m3.1.1.1">^</ci><apply id="S2.SS4.SSS5.p2.8.m3.1.1.2.cmml" xref="S2.SS4.SSS5.p2.8.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.8.m3.1.1.2.1.cmml" xref="S2.SS4.SSS5.p2.8.m3.1.1.2">subscript</csymbol><ci id="S2.SS4.SSS5.p2.8.m3.1.1.2.2.cmml" xref="S2.SS4.SSS5.p2.8.m3.1.1.2.2">𝑠</ci><ci id="S2.SS4.SSS5.p2.8.m3.1.1.2.3.cmml" xref="S2.SS4.SSS5.p2.8.m3.1.1.2.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.8.m3.1c">\hat{s_{t}}</annotation></semantics></math> stands for the prediction score of the MoE network. At test time, we choose the single routed expert with the maximum prediction score <math id="S2.SS4.SSS5.p2.9.m4.1" class="ltx_Math" alttext="\hat{t}=arg\max\hat{s_{t}}" display="inline"><semantics id="S2.SS4.SSS5.p2.9.m4.1a"><mrow id="S2.SS4.SSS5.p2.9.m4.1.1" xref="S2.SS4.SSS5.p2.9.m4.1.1.cmml"><mover accent="true" id="S2.SS4.SSS5.p2.9.m4.1.1.2" xref="S2.SS4.SSS5.p2.9.m4.1.1.2.cmml"><mi id="S2.SS4.SSS5.p2.9.m4.1.1.2.2" xref="S2.SS4.SSS5.p2.9.m4.1.1.2.2.cmml">t</mi><mo id="S2.SS4.SSS5.p2.9.m4.1.1.2.1" xref="S2.SS4.SSS5.p2.9.m4.1.1.2.1.cmml">^</mo></mover><mo id="S2.SS4.SSS5.p2.9.m4.1.1.1" xref="S2.SS4.SSS5.p2.9.m4.1.1.1.cmml">=</mo><mrow id="S2.SS4.SSS5.p2.9.m4.1.1.3" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.cmml"><mi id="S2.SS4.SSS5.p2.9.m4.1.1.3.2" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS5.p2.9.m4.1.1.3.1" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.1.cmml">​</mo><mi id="S2.SS4.SSS5.p2.9.m4.1.1.3.3" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.SS4.SSS5.p2.9.m4.1.1.3.1a" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.1.cmml">​</mo><mi id="S2.SS4.SSS5.p2.9.m4.1.1.3.4" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.4.cmml">g</mi><mo lspace="0.167em" rspace="0em" id="S2.SS4.SSS5.p2.9.m4.1.1.3.1b" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.1.cmml">​</mo><mrow id="S2.SS4.SSS5.p2.9.m4.1.1.3.5" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.cmml"><mi id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.1" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.1.cmml">max</mi><mo lspace="0.167em" id="S2.SS4.SSS5.p2.9.m4.1.1.3.5a" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.cmml">⁡</mo><mover accent="true" id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.cmml"><msub id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.cmml"><mi id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.2" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.2.cmml">s</mi><mi id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.3" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.3.cmml">t</mi></msub><mo id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.1" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.1.cmml">^</mo></mover></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.9.m4.1b"><apply id="S2.SS4.SSS5.p2.9.m4.1.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1"><eq id="S2.SS4.SSS5.p2.9.m4.1.1.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.1"></eq><apply id="S2.SS4.SSS5.p2.9.m4.1.1.2.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.2"><ci id="S2.SS4.SSS5.p2.9.m4.1.1.2.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.2.1">^</ci><ci id="S2.SS4.SSS5.p2.9.m4.1.1.2.2.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.2.2">𝑡</ci></apply><apply id="S2.SS4.SSS5.p2.9.m4.1.1.3.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3"><times id="S2.SS4.SSS5.p2.9.m4.1.1.3.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.1"></times><ci id="S2.SS4.SSS5.p2.9.m4.1.1.3.2.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.2">𝑎</ci><ci id="S2.SS4.SSS5.p2.9.m4.1.1.3.3.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.3">𝑟</ci><ci id="S2.SS4.SSS5.p2.9.m4.1.1.3.4.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.4">𝑔</ci><apply id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5"><max id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.1"></max><apply id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2"><ci id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.1">^</ci><apply id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.1.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2">subscript</csymbol><ci id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.2.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.2">𝑠</ci><ci id="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.3.cmml" xref="S2.SS4.SSS5.p2.9.m4.1.1.3.5.2.2.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.9.m4.1c">\hat{t}=arg\max\hat{s_{t}}</annotation></semantics></math>. The prediction score <math id="S2.SS4.SSS5.p2.10.m5.1" class="ltx_Math" alttext="\hat{s}" display="inline"><semantics id="S2.SS4.SSS5.p2.10.m5.1a"><mover accent="true" id="S2.SS4.SSS5.p2.10.m5.1.1" xref="S2.SS4.SSS5.p2.10.m5.1.1.cmml"><mi id="S2.SS4.SSS5.p2.10.m5.1.1.2" xref="S2.SS4.SSS5.p2.10.m5.1.1.2.cmml">s</mi><mo id="S2.SS4.SSS5.p2.10.m5.1.1.1" xref="S2.SS4.SSS5.p2.10.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.10.m5.1b"><apply id="S2.SS4.SSS5.p2.10.m5.1.1.cmml" xref="S2.SS4.SSS5.p2.10.m5.1.1"><ci id="S2.SS4.SSS5.p2.10.m5.1.1.1.cmml" xref="S2.SS4.SSS5.p2.10.m5.1.1.1">^</ci><ci id="S2.SS4.SSS5.p2.10.m5.1.1.2.cmml" xref="S2.SS4.SSS5.p2.10.m5.1.1.2">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.10.m5.1c">\hat{s}</annotation></semantics></math> is calculated using a Multi-Layer Perception network (MLP) as follows:</p>
<table id="S2.E23" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E23.m1.3" class="ltx_Math" alttext="\hat{s}=W_{3}(\tanh(W_{2}\tanh(W_{1}x+b_{1})+b_{2}))+b_{3}" display="block"><semantics id="S2.E23.m1.3a"><mrow id="S2.E23.m1.3.3" xref="S2.E23.m1.3.3.cmml"><mover accent="true" id="S2.E23.m1.3.3.3" xref="S2.E23.m1.3.3.3.cmml"><mi id="S2.E23.m1.3.3.3.2" xref="S2.E23.m1.3.3.3.2.cmml">s</mi><mo id="S2.E23.m1.3.3.3.1" xref="S2.E23.m1.3.3.3.1.cmml">^</mo></mover><mo id="S2.E23.m1.3.3.2" xref="S2.E23.m1.3.3.2.cmml">=</mo><mrow id="S2.E23.m1.3.3.1" xref="S2.E23.m1.3.3.1.cmml"><mrow id="S2.E23.m1.3.3.1.1" xref="S2.E23.m1.3.3.1.1.cmml"><msub id="S2.E23.m1.3.3.1.1.3" xref="S2.E23.m1.3.3.1.1.3.cmml"><mi id="S2.E23.m1.3.3.1.1.3.2" xref="S2.E23.m1.3.3.1.1.3.2.cmml">W</mi><mn id="S2.E23.m1.3.3.1.1.3.3" xref="S2.E23.m1.3.3.1.1.3.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S2.E23.m1.3.3.1.1.2" xref="S2.E23.m1.3.3.1.1.2.cmml">​</mo><mrow id="S2.E23.m1.3.3.1.1.1.1" xref="S2.E23.m1.3.3.1.1.cmml"><mo stretchy="false" id="S2.E23.m1.3.3.1.1.1.1.2" xref="S2.E23.m1.3.3.1.1.cmml">(</mo><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S2.E23.m1.2.2" xref="S2.E23.m1.2.2.cmml">tanh</mi><mo id="S2.E23.m1.3.3.1.1.1.1.1.1a" xref="S2.E23.m1.3.3.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.E23.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml">2</mn></msub><mo lspace="0.167em" rspace="0em" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S2.E23.m1.1.1" xref="S2.E23.m1.1.1.cmml">tanh</mi><mo id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1a" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml">W</mi><mn id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><mi id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">x</mi></mrow><mo id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mn id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo stretchy="false" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">+</mo><msub id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.2" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml">b</mi><mn id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.3" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.E23.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E23.m1.3.3.1.1.1.1.3" xref="S2.E23.m1.3.3.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E23.m1.3.3.1.2" xref="S2.E23.m1.3.3.1.2.cmml">+</mo><msub id="S2.E23.m1.3.3.1.3" xref="S2.E23.m1.3.3.1.3.cmml"><mi id="S2.E23.m1.3.3.1.3.2" xref="S2.E23.m1.3.3.1.3.2.cmml">b</mi><mn id="S2.E23.m1.3.3.1.3.3" xref="S2.E23.m1.3.3.1.3.3.cmml">3</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E23.m1.3b"><apply id="S2.E23.m1.3.3.cmml" xref="S2.E23.m1.3.3"><eq id="S2.E23.m1.3.3.2.cmml" xref="S2.E23.m1.3.3.2"></eq><apply id="S2.E23.m1.3.3.3.cmml" xref="S2.E23.m1.3.3.3"><ci id="S2.E23.m1.3.3.3.1.cmml" xref="S2.E23.m1.3.3.3.1">^</ci><ci id="S2.E23.m1.3.3.3.2.cmml" xref="S2.E23.m1.3.3.3.2">𝑠</ci></apply><apply id="S2.E23.m1.3.3.1.cmml" xref="S2.E23.m1.3.3.1"><plus id="S2.E23.m1.3.3.1.2.cmml" xref="S2.E23.m1.3.3.1.2"></plus><apply id="S2.E23.m1.3.3.1.1.cmml" xref="S2.E23.m1.3.3.1.1"><times id="S2.E23.m1.3.3.1.1.2.cmml" xref="S2.E23.m1.3.3.1.1.2"></times><apply id="S2.E23.m1.3.3.1.1.3.cmml" xref="S2.E23.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S2.E23.m1.3.3.1.1.3.1.cmml" xref="S2.E23.m1.3.3.1.1.3">subscript</csymbol><ci id="S2.E23.m1.3.3.1.1.3.2.cmml" xref="S2.E23.m1.3.3.1.1.3.2">𝑊</ci><cn type="integer" id="S2.E23.m1.3.3.1.1.3.3.cmml" xref="S2.E23.m1.3.3.1.1.3.3">3</cn></apply><apply id="S2.E23.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1"><tanh id="S2.E23.m1.2.2.cmml" xref="S2.E23.m1.2.2"></tanh><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1"><plus id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.2"></plus><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1"><times id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.2"></times><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">𝑊</ci><cn type="integer" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.3.3">2</cn></apply><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><tanh id="S2.E23.m1.1.1.cmml" xref="S2.E23.m1.1.1"></tanh><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1"><plus id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1"></times><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">𝑊</ci><cn type="integer" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">1</cn></apply><ci id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑥</ci></apply><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑏</ci><cn type="integer" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply><apply id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.2">𝑏</ci><cn type="integer" id="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E23.m1.3.3.1.1.1.1.1.1.1.1.3.3">2</cn></apply></apply></apply></apply><apply id="S2.E23.m1.3.3.1.3.cmml" xref="S2.E23.m1.3.3.1.3"><csymbol cd="ambiguous" id="S2.E23.m1.3.3.1.3.1.cmml" xref="S2.E23.m1.3.3.1.3">subscript</csymbol><ci id="S2.E23.m1.3.3.1.3.2.cmml" xref="S2.E23.m1.3.3.1.3.2">𝑏</ci><cn type="integer" id="S2.E23.m1.3.3.1.3.3.cmml" xref="S2.E23.m1.3.3.1.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E23.m1.3c">\hat{s}=W_{3}(\tanh(W_{2}\tanh(W_{1}x+b_{1})+b_{2}))+b_{3}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(23)</span></td>
</tr></tbody>
</table>
<p id="S2.SS4.SSS5.p2.12" class="ltx_p">where <math id="S2.SS4.SSS5.p2.11.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.SS4.SSS5.p2.11.m1.1a"><mi id="S2.SS4.SSS5.p2.11.m1.1.1" xref="S2.SS4.SSS5.p2.11.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.11.m1.1b"><ci id="S2.SS4.SSS5.p2.11.m1.1.1.cmml" xref="S2.SS4.SSS5.p2.11.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.11.m1.1c">x</annotation></semantics></math> is the input feature and <math id="S2.SS4.SSS5.p2.12.m2.2" class="ltx_Math" alttext="W_{i},b_{i}" display="inline"><semantics id="S2.SS4.SSS5.p2.12.m2.2a"><mrow id="S2.SS4.SSS5.p2.12.m2.2.2.2" xref="S2.SS4.SSS5.p2.12.m2.2.2.3.cmml"><msub id="S2.SS4.SSS5.p2.12.m2.1.1.1.1" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1.cmml"><mi id="S2.SS4.SSS5.p2.12.m2.1.1.1.1.2" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1.2.cmml">W</mi><mi id="S2.SS4.SSS5.p2.12.m2.1.1.1.1.3" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS4.SSS5.p2.12.m2.2.2.2.3" xref="S2.SS4.SSS5.p2.12.m2.2.2.3.cmml">,</mo><msub id="S2.SS4.SSS5.p2.12.m2.2.2.2.2" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2.cmml"><mi id="S2.SS4.SSS5.p2.12.m2.2.2.2.2.2" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2.2.cmml">b</mi><mi id="S2.SS4.SSS5.p2.12.m2.2.2.2.2.3" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS5.p2.12.m2.2b"><list id="S2.SS4.SSS5.p2.12.m2.2.2.3.cmml" xref="S2.SS4.SSS5.p2.12.m2.2.2.2"><apply id="S2.SS4.SSS5.p2.12.m2.1.1.1.1.cmml" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.12.m2.1.1.1.1.1.cmml" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS5.p2.12.m2.1.1.1.1.2.cmml" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1.2">𝑊</ci><ci id="S2.SS4.SSS5.p2.12.m2.1.1.1.1.3.cmml" xref="S2.SS4.SSS5.p2.12.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S2.SS4.SSS5.p2.12.m2.2.2.2.2.cmml" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS4.SSS5.p2.12.m2.2.2.2.2.1.cmml" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2">subscript</csymbol><ci id="S2.SS4.SSS5.p2.12.m2.2.2.2.2.2.cmml" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2.2">𝑏</ci><ci id="S2.SS4.SSS5.p2.12.m2.2.2.2.2.3.cmml" xref="S2.SS4.SSS5.p2.12.m2.2.2.2.2.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS5.p2.12.m2.2c">W_{i},b_{i}</annotation></semantics></math> are the learnable parameters.</p>
</div>
<div id="S2.SS4.SSS5.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.SSS5.p3.1" class="ltx_p">The following features are derived for training the gating network:</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p"><span id="S2.I4.i1.p1.1.1" class="ltx_text ltx_font_italic">Each expert’s confidence</span>: for Visual Understanding Expert, the maximum prediction score is used for confidence score. For Text Reading Expert and Clock Reading Expert, the output score is used for confidence score, and if an image does not have text or any clock, the score is set to <math id="S2.I4.i1.p1.1.m1.1" class="ltx_Math" alttext="-1" display="inline"><semantics id="S2.I4.i1.p1.1.m1.1a"><mrow id="S2.I4.i1.p1.1.m1.1.1" xref="S2.I4.i1.p1.1.m1.1.1.cmml"><mo id="S2.I4.i1.p1.1.m1.1.1a" xref="S2.I4.i1.p1.1.m1.1.1.cmml">−</mo><mn id="S2.I4.i1.p1.1.m1.1.1.2" xref="S2.I4.i1.p1.1.m1.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.I4.i1.p1.1.m1.1b"><apply id="S2.I4.i1.p1.1.m1.1.1.cmml" xref="S2.I4.i1.p1.1.m1.1.1"><minus id="S2.I4.i1.p1.1.m1.1.1.1.cmml" xref="S2.I4.i1.p1.1.m1.1.1"></minus><cn type="integer" id="S2.I4.i1.p1.1.m1.1.1.2.cmml" xref="S2.I4.i1.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I4.i1.p1.1.m1.1c">-1</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para ltx_noindent">
<p id="S2.I4.i2.p1.1" class="ltx_p"><span id="S2.I4.i2.p1.1.1" class="ltx_text ltx_font_italic">Question type</span>: A three-class classifier is trained to predict whether a question is asked about text reading, clock reading or visual understanding. To train the classifier, OCR-labeled data is collected from TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite> &amp; STVQA <cite class="ltx_cite ltx_citemacro_citep">(Biten et al., <a href="#bib.bib45" title="" class="ltx_ref">2019</a>)</cite>, and clock-labeled data from the VQA dataset by retrieving the keywords <em id="S2.I4.i2.p1.1.2" class="ltx_emph ltx_font_italic">clock</em> and <em id="S2.I4.i2.p1.1.3" class="ltx_emph ltx_font_italic">what time</em>. Other cases from VQA data are sampled as negative samples by the ratio of 1:2. The prediction scores of these three classes are used as the input features.</p>
</div>
</li>
</ul>
<p id="S2.SS4.SSS5.p3.2" class="ltx_p">Even though the current process is manually designed, in the future, we will be exploiting techniques that allow us to automatically discover subset of challenging cases, and incrementally add more experts to address the discovered cases.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Pre-training Data</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">The same in-domain data is used as in LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> for pre-training. It consists of the image caption data from MS COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib46" title="" class="ltx_ref">2014</a>)</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, image question answering data from VQA 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>, GQA balanced version <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning, <a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite> and VG-QA <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib49" title="" class="ltx_ref">2016</a>)</cite>. The total amount of the dataset is 9.18M image-and-sentence pairs on 180K distinct images. Also, additional out-of-domain data from Conceptual Captions <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al., <a href="#bib.bib50" title="" class="ltx_ref">2018</a>)</cite> for model pre-training, which consists of about 3M image-text pairs on 3M images.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px1.p2.1" class="ltx_p">For Text Reading Expert, the StructuralLM <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib35" title="" class="ltx_ref">2021b</a>)</cite> is used as the base model, which is pre-trained on the IIT-CDIP Test Collection 1.0 <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib51" title="" class="ltx_ref">2006</a>)</cite>. It is a large-scale scanned document image dataset containing more than 6 million documents, with more than 11 million scanned document images.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning Data</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Visual Question Answering (VQA) is a dataset containing open-ended questions about images <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>. These questions require understanding of vision, language and commonsense knowledge to answer. It contains a large number of labeled question-image-answer triplets with 10 human annotators for each question. The detailed statistics for VQA training/validation/test data splits is shown in Table <a href="#S3.T1" title="Table 1 ‣ Fine-tuning Data ‣ 3.1 Data ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>VQA Data Statistics.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Images</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Questions</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Yes/No</th>
<th id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number</th>
<th id="S3.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Other</th>
<th id="S3.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Answers</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Training</th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">80K</th>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">443K</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">169K</td>
<td id="S3.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">58K</td>
<td id="S3.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">219K</td>
<td id="S3.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">4.4M</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Validation</th>
<th id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">40K</th>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center">214K</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center">81K</td>
<td id="S3.T1.1.3.2.5" class="ltx_td ltx_align_center">28K</td>
<td id="S3.T1.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">106K</td>
<td id="S3.T1.1.3.2.7" class="ltx_td ltx_align_center">2.1M</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Test</th>
<th id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">80K</th>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">447K</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S3.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S3.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S3.T1.1.4.3.7" class="ltx_td ltx_align_center ltx_border_bb">-</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p2.1" class="ltx_p">For training Text Reading Expert, three text-reading VQA datasets is used including a subset of VQA data <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite> and ST-VQA <cite class="ltx_cite ltx_citemacro_citep">(Biten et al., <a href="#bib.bib45" title="" class="ltx_ref">2019</a>)</cite>. A classification model is trained to extract text-reading samples from VQA data. The questions of TextVQA and ST-VQA are treated as positive samples, and the questions on images without text in VQA are treated as negative samples. The detailed statistics for the three text-reading VQA datasets is shown in Table <a href="#S3.T2" title="Table 2 ‣ Fine-tuning Data ‣ 3.1 Data ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span> Text-reading VQA Data Statistics.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Images</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Questions</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<th id="S3.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">VQA-Subset</th>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20k</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">21k</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<th id="S3.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">TextVQA</th>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">25k</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center">39k</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<th id="S3.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">ST-VQA</th>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">19k</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">26k</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS1.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px2.p3.1" class="ltx_p">For training Clock Reading Expert, the images are collected from two sources. One is from <span id="S3.SS1.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_italic">open-access datasets</span>. Specifically, a total of 4863 images are collected from COCO2017 <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib46" title="" class="ltx_ref">2014</a>)</cite><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The COCO images used in the VQA test set are left unlabeled and excluded from our training data.</span></span></span> and 2691 images from ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib21" title="" class="ltx_ref">2009b</a>)</cite> for clock labeling, both of which are widely used open-access datasets. Annotators are required to give the bounding boxes and the precision time of clocks in images. After labeling, 4236 and 3271 valid clock bounding boxes are obtained from COCO2017 <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib46" title="" class="ltx_ref">2014</a>)</cite> and ImageNet <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib21" title="" class="ltx_ref">2009b</a>)</cite> respectively. 785 clock bounding boxes are randomly sampled from COCO2017 images for validation.
The other source is <span id="S3.SS1.SSS0.Px2.p3.1.2" class="ltx_text ltx_font_italic">Internet images</span>. To further increase the generalization and capacity of our clock reader, 2878 images from internet with various clocks are collected. After careful annotation, 2314 valid clocks are obtained. Note that this data is only used for the training of the clock reader.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Metric</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite>, an evaluation metric robust to inter-human variability is used in phrasing the answers:</p>
<table id="S3.E24" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E24.m1.2" class="ltx_Math" alttext="\text{Acc({\it ans})}=\text{min}\Big{\{}\frac{\text{\# human that said {\it ans}}}{3},1\Big{\}}" display="block"><semantics id="S3.E24.m1.2a"><mrow id="S3.E24.m1.2.3" xref="S3.E24.m1.2.3.cmml"><mrow id="S3.E24.m1.2.3.2" xref="S3.E24.m1.2.3.2d.cmml"><mtext id="S3.E24.m1.2.3.2a" xref="S3.E24.m1.2.3.2d.cmml">Acc(</mtext><mtext class="ltx_mathvariant_italic" id="S3.E24.m1.2.3.2b" xref="S3.E24.m1.2.3.2d.cmml">ans</mtext><mtext id="S3.E24.m1.2.3.2c" xref="S3.E24.m1.2.3.2d.cmml">)</mtext></mrow><mo id="S3.E24.m1.2.3.1" xref="S3.E24.m1.2.3.1.cmml">=</mo><mrow id="S3.E24.m1.2.3.3" xref="S3.E24.m1.2.3.3.cmml"><mtext id="S3.E24.m1.2.3.3.2" xref="S3.E24.m1.2.3.3.2a.cmml">min</mtext><mo lspace="0em" rspace="0em" id="S3.E24.m1.2.3.3.1" xref="S3.E24.m1.2.3.3.1.cmml">​</mo><mrow id="S3.E24.m1.2.3.3.3.2" xref="S3.E24.m1.2.3.3.3.1.cmml"><mo maxsize="160%" minsize="160%" id="S3.E24.m1.2.3.3.3.2.1" xref="S3.E24.m1.2.3.3.3.1.cmml">{</mo><mfrac id="S3.E24.m1.1.1" xref="S3.E24.m1.1.1.cmml"><mrow id="S3.E24.m1.1.1.2" xref="S3.E24.m1.1.1.2c.cmml"><mtext id="S3.E24.m1.1.1.2a" xref="S3.E24.m1.1.1.2c.cmml"># human that said </mtext><mtext class="ltx_mathvariant_italic" id="S3.E24.m1.1.1.2b" xref="S3.E24.m1.1.1.2c.cmml">ans</mtext></mrow><mn id="S3.E24.m1.1.1.3" xref="S3.E24.m1.1.1.3.cmml">3</mn></mfrac><mo id="S3.E24.m1.2.3.3.3.2.2" xref="S3.E24.m1.2.3.3.3.1.cmml">,</mo><mn id="S3.E24.m1.2.2" xref="S3.E24.m1.2.2.cmml">1</mn><mo maxsize="160%" minsize="160%" id="S3.E24.m1.2.3.3.3.2.3" xref="S3.E24.m1.2.3.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E24.m1.2b"><apply id="S3.E24.m1.2.3.cmml" xref="S3.E24.m1.2.3"><eq id="S3.E24.m1.2.3.1.cmml" xref="S3.E24.m1.2.3.1"></eq><ci id="S3.E24.m1.2.3.2d.cmml" xref="S3.E24.m1.2.3.2"><mrow id="S3.E24.m1.2.3.2.cmml" xref="S3.E24.m1.2.3.2"><mtext id="S3.E24.m1.2.3.2a.cmml" xref="S3.E24.m1.2.3.2">Acc(</mtext><mtext class="ltx_mathvariant_italic" id="S3.E24.m1.2.3.2b.cmml" xref="S3.E24.m1.2.3.2">ans</mtext><mtext id="S3.E24.m1.2.3.2c.cmml" xref="S3.E24.m1.2.3.2">)</mtext></mrow></ci><apply id="S3.E24.m1.2.3.3.cmml" xref="S3.E24.m1.2.3.3"><times id="S3.E24.m1.2.3.3.1.cmml" xref="S3.E24.m1.2.3.3.1"></times><ci id="S3.E24.m1.2.3.3.2a.cmml" xref="S3.E24.m1.2.3.3.2"><mtext id="S3.E24.m1.2.3.3.2.cmml" xref="S3.E24.m1.2.3.3.2">min</mtext></ci><set id="S3.E24.m1.2.3.3.3.1.cmml" xref="S3.E24.m1.2.3.3.3.2"><apply id="S3.E24.m1.1.1.cmml" xref="S3.E24.m1.1.1"><divide id="S3.E24.m1.1.1.1.cmml" xref="S3.E24.m1.1.1"></divide><ci id="S3.E24.m1.1.1.2c.cmml" xref="S3.E24.m1.1.1.2"><mrow id="S3.E24.m1.1.1.2.cmml" xref="S3.E24.m1.1.1.2"><mtext id="S3.E24.m1.1.1.2a.cmml" xref="S3.E24.m1.1.1.2"># human that said </mtext><mtext class="ltx_mathvariant_italic" id="S3.E24.m1.1.1.2b.cmml" xref="S3.E24.m1.1.1.2">ans</mtext></mrow></ci><cn type="integer" id="S3.E24.m1.1.1.3.cmml" xref="S3.E24.m1.1.1.3">3</cn></apply><cn type="integer" id="S3.E24.m1.2.2.cmml" xref="S3.E24.m1.2.2">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E24.m1.2c">\text{Acc({\it ans})}=\text{min}\Big{\{}\frac{\text{\# human that said {\it ans}}}{3},1\Big{\}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(24)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS0.Px3.p1.2" class="ltx_p">In order to be consistent with “human accuracies”, machine accuracies are averaged over all 10-choose-9 sets of human annotators.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Setup</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">VLP</h5>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px1.p1.4" class="ltx_p">The maximum sequence length for the sentence is set as 20. For the VLP models, the pre-trained Transformer encoder with 12 layers is used as our base architecture, and the one with 24 layers as the large architecture. The basic settings of the Transformer are the same as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, and the Transformer encoder is initialized with StructBERT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> for its good performance. For the method of <em id="S3.SS2.SSS0.Px1.p1.4.1" class="ltx_emph ltx_font_italic">learning to attend</em>, the two learnable parameters are initialized with <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="init\_value_{1}=1.0" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1a" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.4" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1b" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.5" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1c" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.6" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1d" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.7" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.7.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1e" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.8" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1f" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.9" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1g" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.10" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1h" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">​</mo><msub id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.2.cmml">e</mi><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.3.cmml">1</mn></msub></mrow><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><eq id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1"></eq><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2"><times id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.1"></times><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.2">𝑖</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.3">𝑛</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.4.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.4">𝑖</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.5.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.5">𝑡</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.6.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.6">_</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.7.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.7">𝑣</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.8.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.8">𝑎</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.9.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.9">𝑙</ci><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.10.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.10">𝑢</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.2">𝑒</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.11.3">1</cn></apply></apply><cn type="float" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">init\_value_{1}=1.0</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="init\_value_{2}=L_{s}/L" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1a" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.4" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1b" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.5" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1c" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.6" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.6.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1d" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.7" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.7.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1e" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.8" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.8.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1f" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.9" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1g" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.10" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.10.cmml">u</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1h" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml">​</mo><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.2.cmml">e</mi><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.3.cmml">2</mn></msub></mrow><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml"><msub id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.2.cmml">L</mi><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.3.cmml">s</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">/</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">L</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><eq id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></eq><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2"><times id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1"></times><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">𝑖</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3">𝑛</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.4.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.4">𝑖</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.5.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.5">𝑡</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.6.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.6">_</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.7.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.7">𝑣</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.8.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.8">𝑎</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.9.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.9">𝑙</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.10.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.10">𝑢</ci><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.2">𝑒</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.11.3">2</cn></apply></apply><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3"><divide id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.1"></divide><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.2">𝐿</ci><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.2.3">𝑠</ci></apply><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">init\_value_{2}=L_{s}/L</annotation></semantics></math>, where <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">L</annotation></semantics></math> is the number of Transformer layers and <math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="L_{s}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">L</mi><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.2">𝐿</ci><ci id="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">L_{s}</annotation></semantics></math> is the corresponding layer number. The base model is pre-trained with a total batch size of 512 for 30 epochs on 8 A100 GPUs and the AdamW optimizer with the initial learning rate of 1e-4. The 24-layer large architecture is pre-trained with the total batch size of 512 on 8 A100 GPUs. To deal with over-fitting, two-stage pre-training strategy is employed as in LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>. Specifically, the model is first pre-trained without the question answering task with the initial learning rate of 5e-5 for 20 epochs, and then pre-trained with all the tasks together with the initial learning rate of 2e-5 for another 10 epochs. The detailed settings for the three VLP methods are listed as below:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Region-VLP</span>: The detection model is used in VinVL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> to detect objects and extract region features. It is a large-scale object-attribute detection model based on the ResNeXt-152 C4 architecture. 100 objects is retained for each image to maximize the pre-training compute utilization by avoiding padding.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Grid-VLP</span>: It follows the basic settings in Grid-VLP <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>
. ResNeXt is chosen to be the visual encoder with different sizes <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a href="#bib.bib52" title="" class="ltx_ref">2017</a>)</cite> as in <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>; Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>. The shorter side of every input image is resized to 600, and the longer side is limit to at most 1000. A fixed number of 100 grids are randomly selected each time during pre-training <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We also tested with 64 and 128 selected grids. It did not lead to significantly different results.</span></span></span>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i3.p1.3" class="ltx_p"><span id="S3.I1.i3.p1.3.1" class="ltx_text ltx_font_bold">Patch-VLP</span>: It uses the visual Transformer encoder of the Swin detector <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> and CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>. The ViT-B/32 pre-trained model is chosen, which has 12 Transformers layers with input patches of size <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="32\times 32" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mn id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I1.i3.p1.1.m1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><times id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">32</cn><cn type="integer" id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">32\times 32</annotation></semantics></math>. Every input images is resized to <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><mrow id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mn id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I1.i3.p1.2.m2.1.1.1" xref="S3.I1.i3.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><times id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2">224</cn><cn type="integer" id="S3.I1.i3.p1.2.m2.1.1.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">224\times 224</annotation></semantics></math> as CLIP does, resulting in <math id="S3.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="7\times 7=49" display="inline"><semantics id="S3.I1.i3.p1.3.m3.1a"><mrow id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml"><mrow id="S3.I1.i3.p1.3.m3.1.1.2" xref="S3.I1.i3.p1.3.m3.1.1.2.cmml"><mn id="S3.I1.i3.p1.3.m3.1.1.2.2" xref="S3.I1.i3.p1.3.m3.1.1.2.2.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I1.i3.p1.3.m3.1.1.2.1" xref="S3.I1.i3.p1.3.m3.1.1.2.1.cmml">×</mo><mn id="S3.I1.i3.p1.3.m3.1.1.2.3" xref="S3.I1.i3.p1.3.m3.1.1.2.3.cmml">7</mn></mrow><mo id="S3.I1.i3.p1.3.m3.1.1.1" xref="S3.I1.i3.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.I1.i3.p1.3.m3.1.1.3" xref="S3.I1.i3.p1.3.m3.1.1.3.cmml">49</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><apply id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1"><eq id="S3.I1.i3.p1.3.m3.1.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.1"></eq><apply id="S3.I1.i3.p1.3.m3.1.1.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2"><times id="S3.I1.i3.p1.3.m3.1.1.2.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2.1"></times><cn type="integer" id="S3.I1.i3.p1.3.m3.1.1.2.2.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2.2">7</cn><cn type="integer" id="S3.I1.i3.p1.3.m3.1.1.2.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.2.3">7</cn></apply><cn type="integer" id="S3.I1.i3.p1.3.m3.1.1.3.cmml" xref="S3.I1.i3.p1.3.m3.1.1.3">49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">7\times 7=49</annotation></semantics></math> patches.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Fusion-VLP</span>: It fuses the three classes of image features (Region, Grid and Patch) by concatenating them together as the visual input to the Transformer. A two-stage strategy is employed to pre-train Fusion-VLP, which first trains the region-grid model initialized with Region-VLP, and then continues to train the region-grid-patch model.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Fine-tuning on VQA</h5>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, our architecture treats VQA as a multi-class classification task by picking an answer from a shared set of 3,129 answers. The hidden state of <math id="S3.SS2.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="h^{L}_{CLS}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><msubsup id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2.cmml">h</mi><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1a" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.4" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.4.cmml">S</mi></mrow><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.2">ℎ</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.2.3">𝐿</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3"><times id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.1"></times><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.2">𝐶</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.3">𝐿</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.4.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.3.4">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">h^{L}_{CLS}</annotation></semantics></math> is used to map the representation into 3,129 possible answers with an additional MLP layer. The model is trained with a binary cross-entropy loss on the soft target scores. The pre-trained models are fine-tuned based on the three classes of features on the VQA training data for 3 epochs with the batch size of 32, and the BERT Adam optimizer is employed with the initial learning rate of 1e-4 for base models and 2e-5 for large models. At inference, a <em id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">softmax</em> function is used for prediction.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text Reading Expert</h5>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">The text reading expert follows the basic settings in StructuralLM  <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib35" title="" class="ltx_ref">2021b</a>)</cite> and uses the pre-trained StructuralLM-large as the backbone model. In particular, StructuralLM is pre-trained with a batch size of 16 for 50K steps. The question tokens and the OCR tokens of an image are concatenated as an input sequence, of which the maximum length is set as 128. For fine-tuning, the three kinds of text-reading VQA datasets are merged and split with 10-fold cross-validation. The StructuralLM is fine-tuned with the total batch size of 16 for 4 epochs, and the AdamW optimizer is employed with the initial learning rate of 3e-5. Accuracy and ANLS (Average Normalized Levenshtein Similarity) are used as the metrics to evaluate the text reading expert.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Clock Reading Expert</h5>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px4.p1.2" class="ltx_p">The clock detector of the clock reading expert is trained following the basic settings of Cascade-RCNN <cite class="ltx_cite ltx_citemacro_citep">(Cai and Vasconcelos, <a href="#bib.bib38" title="" class="ltx_ref">2018</a>)</cite>. The clock reader is trained with the batch size of 96 in 2 GPUs, and the initial learning rate is set as 0.02. It is trained for 150 epochs with the learning rate multiplied by 0.1 at 90-th and 120-th epochs. The data augmentation pipeline consists of <math id="S3.SS2.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2">256</cn><cn type="integer" id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.1.m1.1c">256\times 256</annotation></semantics></math> random resized cropping, random color jittering, random gray-scale conversion, Gaussian blurring and random rotation within <math id="S3.SS2.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="\pm 45^{\circ}" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml"><mo id="S3.SS2.SSS0.Px4.p1.2.m2.1.1a" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml">±</mo><msup id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.cmml"><mn id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.2.cmml">45</mn><mo id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.3.cmml">∘</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1">plus-or-minus</csymbol><apply id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2">superscript</csymbol><cn type="integer" id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.2">45</cn><compose id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.3"></compose></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.2.m2.1c">\pm 45^{\circ}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Understanding Expert</h5>

<div id="S3.SS2.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px5.p1.1" class="ltx_p">The visual understanding expert ensembles 46 models in total, including 14 Region-VLP models, 21 Grid-VLP models, 4 Patch-VLP models and 7 Fusion-VLP models. Simple maximum voting is adopted to ensemble all the models based on their prediction scores.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mixture of Experts</h5>

<div id="S3.SS2.SSS0.Px6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS0.Px6.p1.1" class="ltx_p">The MoE adopts Multi-layer Perceptron (MLP) as the gating network to determine experts for given questions. The MLP has two hidden layers of 100 neurons and 50 neurons, respectively. It uses <em id="S3.SS2.SSS0.Px6.p1.1.1" class="ltx_emph ltx_font_italic">tanh</em> as the activation function, and the Adam optimizer with the initial learning rate of 1e-3. The network is trained for 5 epochs with the batch size of 256.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>VQA Challenge Leaderboard.</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="5">VQA Challenge Leaderboard (Test-std)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<th id="S3.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Models</th>
<th id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Overall</th>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Yes/No</td>
<td id="S3.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">Number</td>
<td id="S3.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">Other</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<th id="S3.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Human</th>
<th id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">80.83</th>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">95.48</span></td>
<td id="S3.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.3.2.4.1" class="ltx_text ltx_font_bold">81.29</span></td>
<td id="S3.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">67.97</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<th id="S3.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">LXMERT (<cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>)</th>
<th id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">74.34</th>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">89.45</td>
<td id="S3.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">56.69</td>
<td id="S3.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">65.22</td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<th id="S3.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">MCAN (<cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib53" title="" class="ltx_ref">2019a</a>)</cite>)</th>
<th id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">75.23</th>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center">90.36</td>
<td id="S3.T3.1.5.4.4" class="ltx_td ltx_align_center">59.17</td>
<td id="S3.T3.1.5.4.5" class="ltx_td ltx_align_center">65.75</td>
</tr>
<tr id="S3.T3.1.6.5" class="ltx_tr">
<th id="S3.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VILLA (<cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite>)</th>
<th id="S3.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">75.85</th>
<td id="S3.T3.1.6.5.3" class="ltx_td ltx_align_center">91.30</td>
<td id="S3.T3.1.6.5.4" class="ltx_td ltx_align_center">59.23</td>
<td id="S3.T3.1.6.5.5" class="ltx_td ltx_align_center">66.20</td>
</tr>
<tr id="S3.T3.1.7.6" class="ltx_tr">
<th id="S3.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">BGN (<cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a href="#bib.bib55" title="" class="ltx_ref">2019</a>)</cite>)</th>
<th id="S3.T3.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">75.92</th>
<td id="S3.T3.1.7.6.3" class="ltx_td ltx_align_center">90.89</td>
<td id="S3.T3.1.7.6.4" class="ltx_td ltx_align_center">61.13</td>
<td id="S3.T3.1.7.6.5" class="ltx_td ltx_align_center">66.28</td>
</tr>
<tr id="S3.T3.1.8.7" class="ltx_tr">
<th id="S3.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">InterBERT (<cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib56" title="" class="ltx_ref">2020</a>)</cite>)</th>
<th id="S3.T3.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">76.10</th>
<td id="S3.T3.1.8.7.3" class="ltx_td ltx_align_center">91.67</td>
<td id="S3.T3.1.8.7.4" class="ltx_td ltx_align_center">59.24</td>
<td id="S3.T3.1.8.7.5" class="ltx_td ltx_align_center">66.40</td>
</tr>
<tr id="S3.T3.1.9.8" class="ltx_tr">
<th id="S3.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">GridFeat+MoVie (<cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>)</th>
<th id="S3.T3.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">76.29</th>
<td id="S3.T3.1.9.8.3" class="ltx_td ltx_align_center">90.81</td>
<td id="S3.T3.1.9.8.4" class="ltx_td ltx_align_center">61.53</td>
<td id="S3.T3.1.9.8.5" class="ltx_td ltx_align_center">67.04</td>
</tr>
<tr id="S3.T3.1.10.9" class="ltx_tr">
<th id="S3.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VinVL (<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>)</th>
<th id="S3.T3.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">77.45</th>
<td id="S3.T3.1.10.9.3" class="ltx_td ltx_align_center">92.38</td>
<td id="S3.T3.1.10.9.4" class="ltx_td ltx_align_center">62.55</td>
<td id="S3.T3.1.10.9.5" class="ltx_td ltx_align_center">67.87</td>
</tr>
<tr id="S3.T3.1.11.10" class="ltx_tr">
<th id="S3.T3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ROSITA (<cite class="ltx_cite ltx_citemacro_cite">Cui et al. (<a href="#bib.bib57" title="" class="ltx_ref">2021</a>)</cite>)</th>
<th id="S3.T3.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">78.34</th>
<td id="S3.T3.1.11.10.3" class="ltx_td ltx_align_center">92.66</td>
<td id="S3.T3.1.11.10.4" class="ltx_td ltx_align_center">63.24</td>
<td id="S3.T3.1.11.10.5" class="ltx_td ltx_align_center">69.33</td>
</tr>
<tr id="S3.T3.1.12.11" class="ltx_tr">
<th id="S3.T3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">UNIMO (<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib58" title="" class="ltx_ref">2020a</a>)</cite>)</th>
<th id="S3.T3.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">78.40</th>
<td id="S3.T3.1.12.11.3" class="ltx_td ltx_align_center">93.10</td>
<td id="S3.T3.1.12.11.4" class="ltx_td ltx_align_center">63.06</td>
<td id="S3.T3.1.12.11.5" class="ltx_td ltx_align_center">69.12</td>
</tr>
<tr id="S3.T3.1.13.12" class="ltx_tr">
<th id="S3.T3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">VQA Challenge 2021 winner</th>
<th id="S3.T3.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">79.34</th>
<td id="S3.T3.1.13.12.3" class="ltx_td ltx_align_center">93.28</td>
<td id="S3.T3.1.13.12.4" class="ltx_td ltx_align_center">65.36</td>
<td id="S3.T3.1.13.12.5" class="ltx_td ltx_align_center">70.40</td>
</tr>
<tr id="S3.T3.1.14.13" class="ltx_tr">
<th id="S3.T3.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">PASH-SFE</th>
<th id="S3.T3.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">79.47</th>
<td id="S3.T3.1.14.13.3" class="ltx_td ltx_align_center">92.45</td>
<td id="S3.T3.1.14.13.4" class="ltx_td ltx_align_center">76.57</td>
<td id="S3.T3.1.14.13.5" class="ltx_td ltx_align_center">68.82</td>
</tr>
<tr id="S3.T3.1.15.14" class="ltx_tr">
<th id="S3.T3.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">SimVLM (<cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib59" title="" class="ltx_ref">2021</a>)</cite>)</th>
<th id="S3.T3.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">80.34</th>
<td id="S3.T3.1.15.14.3" class="ltx_td ltx_align_center">93.29</td>
<td id="S3.T3.1.15.14.4" class="ltx_td ltx_align_center">66.54</td>
<td id="S3.T3.1.15.14.5" class="ltx_td ltx_align_center">72.23</td>
</tr>
<tr id="S3.T3.1.16.15" class="ltx_tr">
<th id="S3.T3.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T3.1.16.15.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU</span></th>
<th id="S3.T3.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S3.T3.1.16.15.2.1" class="ltx_text ltx_font_bold">81.26</span></th>
<td id="S3.T3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">93.55</td>
<td id="S3.T3.1.16.15.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">72.01</td>
<td id="S3.T3.1.16.15.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T3.1.16.15.5.1" class="ltx_text ltx_font_bold">72.67</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison with other single models.</figcaption>
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.1.1.1" class="ltx_tr">
<th id="S3.T4.1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;" colspan="8">Performance of Single Models</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.1.2.1" class="ltx_tr">
<th id="S3.T4.1.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;" rowspan="2"><span id="S3.T4.1.2.1.1.1" class="ltx_text">Models</span></th>
<th id="S3.T4.1.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;" rowspan="2"><span id="S3.T4.1.2.1.2.1" class="ltx_text">Feature Type</span></th>
<td id="S3.T4.1.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;" colspan="3">BASE</td>
<td id="S3.T4.1.2.1.4" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;" colspan="3">LARGE</td>
</tr>
<tr id="S3.T4.1.3.2" class="ltx_tr">
<td id="S3.T4.1.3.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Params</td>
<td id="S3.T4.1.3.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Test-dev</td>
<td id="S3.T4.1.3.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Test-std</td>
<td id="S3.T4.1.3.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Params</td>
<td id="S3.T4.1.3.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Test-dev</td>
<td id="S3.T4.1.3.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Test-std</td>
</tr>
<tr id="S3.T4.1.4.3" class="ltx_tr">
<th id="S3.T4.1.4.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">VLBERT (<cite class="ltx_cite ltx_citemacro_cite">Su et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>)</th>
<th id="S3.T4.1.4.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.4.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.4.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">71.16</td>
<td id="S3.T4.1.4.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.4.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.4.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">71.79</td>
<td id="S3.T4.1.4.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">72.22</td>
</tr>
<tr id="S3.T4.1.5.4" class="ltx_tr">
<th id="S3.T4.1.5.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">UNITER (<cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>)</th>
<th id="S3.T4.1.5.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.5.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.5.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">72.70</td>
<td id="S3.T4.1.5.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">72.91</td>
<td id="S3.T4.1.5.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.5.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.82</td>
<td id="S3.T4.1.5.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">74.02</td>
</tr>
<tr id="S3.T4.1.6.5" class="ltx_tr">
<th id="S3.T4.1.6.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">OSCAR (<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib60" title="" class="ltx_ref">2020b</a>)</cite>)</th>
<th id="S3.T4.1.6.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.6.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.6.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.16</td>
<td id="S3.T4.1.6.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">73.44</td>
<td id="S3.T4.1.6.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.6.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.61</td>
<td id="S3.T4.1.6.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.82</td>
</tr>
<tr id="S3.T4.1.7.6" class="ltx_tr">
<th id="S3.T4.1.7.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">UNIMO (<cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib58" title="" class="ltx_ref">2020a</a>)</cite>)</th>
<th id="S3.T4.1.7.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.7.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.7.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.79</td>
<td id="S3.T4.1.7.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">74.02</td>
<td id="S3.T4.1.7.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.7.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">75.06</td>
<td id="S3.T4.1.7.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">75.27</td>
</tr>
<tr id="S3.T4.1.8.7" class="ltx_tr">
<th id="S3.T4.1.8.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">VinVL (<cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> )</th>
<th id="S3.T4.1.8.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.8.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.8.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">75.95</td>
<td id="S3.T4.1.8.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">76.12</td>
<td id="S3.T4.1.8.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.8.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">76.52</td>
<td id="S3.T4.1.8.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">76.60</td>
</tr>
<tr id="S3.T4.1.9.8" class="ltx_tr">
<th id="S3.T4.1.9.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">ViLBERT (<cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib61" title="" class="ltx_ref">2019</a>)</cite>)</th>
<th id="S3.T4.1.9.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.9.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">221M</td>
<td id="S3.T4.1.9.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">70.55</td>
<td id="S3.T4.1.9.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">70.92</td>
<td id="S3.T4.1.9.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.9.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.9.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.10.9" class="ltx_tr">
<th id="S3.T4.1.10.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">12-in-1 (<cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib62" title="" class="ltx_ref">2020</a>)</cite>)</th>
<th id="S3.T4.1.10.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.10.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">221M</td>
<td id="S3.T4.1.10.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.15</td>
<td id="S3.T4.1.10.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.10.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.10.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.10.9.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.11.10" class="ltx_tr">
<th id="S3.T4.1.11.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">LXMERT (<cite class="ltx_cite ltx_citemacro_cite">Tan and Bansal (<a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> )</th>
<th id="S3.T4.1.11.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.11.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">183M</td>
<td id="S3.T4.1.11.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">72.42</td>
<td id="S3.T4.1.11.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">72.54</td>
<td id="S3.T4.1.11.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.11.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.11.10.8" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:1.7pt;padding-right:1.7pt;"></td>
</tr>
<tr id="S3.T4.1.12.11" class="ltx_tr">
<th id="S3.T4.1.12.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">ERNIE-ViL (<cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> )</th>
<th id="S3.T4.1.12.11.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.12.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">250M</td>
<td id="S3.T4.1.12.11.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">73.18</td>
<td id="S3.T4.1.12.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">73.36</td>
<td id="S3.T4.1.12.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">510M</td>
<td id="S3.T4.1.12.11.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">74.95</td>
<td id="S3.T4.1.12.11.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">75.10</td>
</tr>
<tr id="S3.T4.1.13.12" class="ltx_tr">
<th id="S3.T4.1.13.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">PixelBERT (<cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>)</th>
<th id="S3.T4.1.13.12.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Grid</th>
<td id="S3.T4.1.13.12.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">170M</td>
<td id="S3.T4.1.13.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">74.45</td>
<td id="S3.T4.1.13.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">74.55</td>
<td id="S3.T4.1.13.12.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.13.12.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.13.12.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.14.13" class="ltx_tr">
<th id="S3.T4.1.14.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">ViLT (<cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>)</th>
<th id="S3.T4.1.14.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Patch</th>
<td id="S3.T4.1.14.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.14.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">71.26</td>
<td id="S3.T4.1.14.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.14.13.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.14.13.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.14.13.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.15.14" class="ltx_tr">
<th id="S3.T4.1.15.14.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Region-VLP</th>
<th id="S3.T4.1.15.14.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Region</th>
<td id="S3.T4.1.15.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.15.14.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">76.25</td>
<td id="S3.T4.1.15.14.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.15.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.15.14.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">77.17</td>
<td id="S3.T4.1.15.14.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.16.15" class="ltx_tr">
<th id="S3.T4.1.16.15.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">Grid-VLP</th>
<th id="S3.T4.1.16.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Grid</th>
<td id="S3.T4.1.16.15.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.16.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">76.50</td>
<td id="S3.T4.1.16.15.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.16.15.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.16.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">77.13</td>
<td id="S3.T4.1.16.15.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.17.16" class="ltx_tr">
<th id="S3.T4.1.17.16.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:1.7pt;padding-right:1.7pt;">Patch-VLP</th>
<th id="S3.T4.1.17.16.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Patch</th>
<td id="S3.T4.1.17.16.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.17.16.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">71.61</td>
<td id="S3.T4.1.17.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.17.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.17.16.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
<td id="S3.T4.1.17.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">-</td>
</tr>
<tr id="S3.T4.1.18.17" class="ltx_tr">
<th id="S3.T4.1.18.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">Fusion-VLP</th>
<th id="S3.T4.1.18.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Region+Grid+Patch</th>
<td id="S3.T4.1.18.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">110M</td>
<td id="S3.T4.1.18.17.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T4.1.18.17.4.1" class="ltx_text ltx_font_bold">76.80</span></td>
<td id="S3.T4.1.18.17.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T4.1.18.17.5.1" class="ltx_text ltx_font_bold">76.78</span></td>
<td id="S3.T4.1.18.17.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">345M</td>
<td id="S3.T4.1.18.17.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T4.1.18.17.7.1" class="ltx_text ltx_font_bold">77.59</span></td>
<td id="S3.T4.1.18.17.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T4.1.18.17.8.1" class="ltx_text ltx_font_bold">77.61</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Main Results</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Table <a href="#S3.T3" title="Table 3 ‣ Mixture of Experts ‣ 3.2 Experimental Setup ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents our main results compared with all the previous public and unpublic best results on the VQA Challenge Leaderboard. From the results, it can be observed that: 1) Our VQA architecture <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>represents the first to achieve human parity on VQA Challenge Leaderboard outperforming all the previous state-of-the-art methods by a large margin, which demonstrates the effectiveness of our framework. 2) With regard to a breakdown of performance on different question types, <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>performs much better on the “Other” type than human do, and gives comparable results on “Yes/No” questions. <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>performs worse than human do on type “Number” for the two reasons: a) in the “Number” type, there are many questions about reading OCR text, which are easier for human to answer; and b) there are many object counting questions that are more difficult for <span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>to answer.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">Table <a href="#S3.T4" title="Table 4 ‣ Mixture of Experts ‣ 3.2 Experimental Setup ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the detailed results of our single VLP models compared with other state-of-the-art methods. From the results, it is observed that: 1) the proposed VLP model outperforms the others on every kind of visual feature (region / grid / patch), respectively. It demonstrates the effectiveness of the proposed cross-modal interaction with learning to attend mechanism. 2) The methods with self-attention on patch feature perform worse than the region-based and grid-based methods do. There are two weaknesses of patch-based methods: a) the visual semantic information is not well-captured in existing patch-based VLP methods. How to inject visual semantics into patch representation remains largely unexplored; b) the image-text pre-training data is not enough for large-scale patch-based pre-training; 3) Fusion-VLP gives the best performance by fusing all the three classes of visual features as input, which validates the effectiveness of comprehensive feature representation.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model Analysis by Modules</h3>

<section id="S3.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Feature Importance</h5>

<div id="S3.SS4.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px1.p1.1" class="ltx_p">Here presents the ablation study to assess the importance of different visual features for VLP on the VQA test-dev set. The results shown in Table <a href="#S3.T5" title="Table 5 ‣ Visual Feature Importance ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> indicate that: 1) The VLP methods based on region and grid features achieve much better performance than the ones based on patch feature do, as stated in Section <a href="#S3.SS3" title="3.3 Main Results ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. When examining by individual question types, Region-VLP performs better on the “Number” type, while Grid-VLP does better on the “Yes/No” and “Other” types. The difference can be attributed to the fact that region feature captures more local information of an image at the object level, and thus is more effective in address the visual counting problem by identifying local objects in an image. On the other hand, grid feature captures globally visual context in an image, which helps to answer the “Yes/No” and “Other” questions; 2) by combining the three classes of features in the way of early fusion, Fusion-VLP performs the best among all the single models. It shows that the different kinds of features can complement well with each other.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Ablation study of visual features on VQA Test-dev.</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Yes/No</th>
<th id="S3.T5.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number</th>
<th id="S3.T5.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Other</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.2.1" class="ltx_tr">
<th id="S3.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Fusion-VLP</th>
<td id="S3.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.1.2.1.2.1" class="ltx_text ltx_font_bold">77.59</span></td>
<td id="S3.T5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">91.91</td>
<td id="S3.T5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.1.2.1.4.1" class="ltx_text ltx_font_bold">64.29</span></td>
<td id="S3.T5.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T5.1.2.1.5.1" class="ltx_text ltx_font_bold">68.33</span></td>
</tr>
<tr id="S3.T5.1.3.2" class="ltx_tr">
<th id="S3.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Region-VLP</th>
<td id="S3.T5.1.3.2.2" class="ltx_td ltx_align_center">77.17</td>
<td id="S3.T5.1.3.2.3" class="ltx_td ltx_align_center">91.62</td>
<td id="S3.T5.1.3.2.4" class="ltx_td ltx_align_center">63.69</td>
<td id="S3.T5.1.3.2.5" class="ltx_td ltx_align_center">67.84</td>
</tr>
<tr id="S3.T5.1.4.3" class="ltx_tr">
<th id="S3.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Grid-VLP</th>
<td id="S3.T5.1.4.3.2" class="ltx_td ltx_align_center">77.13</td>
<td id="S3.T5.1.4.3.3" class="ltx_td ltx_align_center"><span id="S3.T5.1.4.3.3.1" class="ltx_text ltx_font_bold">92.20</span></td>
<td id="S3.T5.1.4.3.4" class="ltx_td ltx_align_center">59.99</td>
<td id="S3.T5.1.4.3.5" class="ltx_td ltx_align_center">68.15</td>
</tr>
<tr id="S3.T5.1.5.4" class="ltx_tr">
<th id="S3.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Patch-VLP</th>
<td id="S3.T5.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">71.61</td>
<td id="S3.T5.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">88.17</td>
<td id="S3.T5.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb">49.44</td>
<td id="S3.T5.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb">62.54</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Learning to Attend</h5>

<div id="S3.SS4.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px2.p1.1" class="ltx_p">Here presents the ablation study to assess the importance of the <em id="S3.SS4.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">learning to attend</em> mechanism on the VQA test-dev set. The 24-layer Region-VLP is used as the baseline model, which is pre-trained and fine-tuned based on the original Transformer. The ablation applies the same pre-training and fine-tuning settings, and only modifies the self-attention block with the two ways of <em id="S3.SS4.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">learning to attend</em> stated in Section <a href="#S2.SS3.SSS2" title="2.3.2 Learning to Attend ‣ 2.3 Cross-Modal Interaction with Learning to Attend ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.2</span></a>. From Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:table:l2a</span>, it can be seen that the model with either way of <em id="S3.SS4.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">learning to attend</em> outperforms the best Region-VLP baseline. Among the two different ways, the one with two learnable parameters performs slightly better than the other one. The reason may lie in: 1) learning two unrestricted parameters for each layer allowing for more parameter freedom, so as to better align cross-modal semantics, 2) the discrepancy between pre-training and fine-tuning, where the representation of [CLS] is learnt to model the semantic relation between a caption and an image during pre-training, while it is repurposed for question answering in fine-tuning.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Ablation study of <em id="S3.T6.2.1" class="ltx_emph ltx_font_italic">learning to attend</em> on VQA Test-dev.</figcaption>
<table id="S3.T6.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.3.1.1" class="ltx_tr">
<th id="S3.T6.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T6.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
<th id="S3.T6.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Yes/no</th>
<th id="S3.T6.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number</th>
<th id="S3.T6.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Other</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.3.2.1" class="ltx_tr">
<th id="S3.T6.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Region-VLP (Baseline)</th>
<td id="S3.T6.3.2.1.2" class="ltx_td ltx_align_center ltx_border_t">76.75</td>
<td id="S3.T6.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t">91.28</td>
<td id="S3.T6.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t">63.31</td>
<td id="S3.T6.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t">67.34</td>
</tr>
<tr id="S3.T6.3.3.2" class="ltx_tr">
<th id="S3.T6.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   + Learning to Attend (FFN)</th>
<td id="S3.T6.3.3.2.2" class="ltx_td ltx_align_center">77.09</td>
<td id="S3.T6.3.3.2.3" class="ltx_td ltx_align_center">91.58</td>
<td id="S3.T6.3.3.2.4" class="ltx_td ltx_align_center">63.54</td>
<td id="S3.T6.3.3.2.5" class="ltx_td ltx_align_center">67.74</td>
</tr>
<tr id="S3.T6.3.4.3" class="ltx_tr">
<th id="S3.T6.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">   + Learning to Attend (Param)</th>
<td id="S3.T6.3.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.3.4.3.2.1" class="ltx_text ltx_font_bold">77.17</span></td>
<td id="S3.T6.3.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.3.4.3.3.1" class="ltx_text ltx_font_bold">91.62</span></td>
<td id="S3.T6.3.4.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.3.4.3.4.1" class="ltx_text ltx_font_bold">63.69</span></td>
<td id="S3.T6.3.4.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.3.4.3.5.1" class="ltx_text ltx_font_bold">67.84</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Ablation study of text reading expert on the VQA Test-dev.</figcaption>
<table id="S3.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T7.1.1.1" class="ltx_tr">
<th id="S3.T7.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T7.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
<th id="S3.T7.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Yes/No</th>
<th id="S3.T7.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number</th>
<th id="S3.T7.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Other</th>
<th id="S3.T7.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">ANLS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T7.1.2.1" class="ltx_tr">
<th id="S3.T7.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Visual Understanding Expert</th>
<td id="S3.T7.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">79.44</td>
<td id="S3.T7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">93.31</td>
<td id="S3.T7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">65.70</td>
<td id="S3.T7.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">71.16</td>
<td id="S3.T7.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T7.1.3.2" class="ltx_tr">
<th id="S3.T7.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ Text-reading VQA data</th>
<td id="S3.T7.1.3.2.2" class="ltx_td ltx_align_center">80.35</td>
<td id="S3.T7.1.3.2.3" class="ltx_td ltx_align_center">93.31</td>
<td id="S3.T7.1.3.2.4" class="ltx_td ltx_align_center">69.81</td>
<td id="S3.T7.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">71.49</td>
<td id="S3.T7.1.3.2.6" class="ltx_td ltx_align_center">79.85</td>
</tr>
<tr id="S3.T7.1.4.3" class="ltx_tr">
<th id="S3.T7.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">      + add separator</th>
<td id="S3.T7.1.4.3.2" class="ltx_td ltx_align_center">80.41</td>
<td id="S3.T7.1.4.3.3" class="ltx_td ltx_align_center">93.31</td>
<td id="S3.T7.1.4.3.4" class="ltx_td ltx_align_center">69.82</td>
<td id="S3.T7.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">71.64</td>
<td id="S3.T7.1.4.3.6" class="ltx_td ltx_align_center">79.96</td>
</tr>
<tr id="S3.T7.1.5.4" class="ltx_tr">
<th id="S3.T7.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">        + continue pre-training</th>
<td id="S3.T7.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T7.1.5.4.2.1" class="ltx_text ltx_font_bold">80.63</span></td>
<td id="S3.T7.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">93.31</td>
<td id="S3.T7.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T7.1.5.4.4.1" class="ltx_text ltx_font_bold">69.97</span></td>
<td id="S3.T7.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T7.1.5.4.5.1" class="ltx_text ltx_font_bold">72.01</span></td>
<td id="S3.T7.1.5.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T7.1.5.4.6.1" class="ltx_text ltx_font_bold">80.33</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Knowledge Mining</h5>

<div id="S3.SS4.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px3.p1.1" class="ltx_p">Figure <a href="#S3.F4" title="Figure 4 ‣ Knowledge Mining ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the clustering result. We choose the number of the clusters as 5, which gives the best performance on our quantitative test. For each cluster, we showcase two examples in the cluster and the percentage of the examples in this cluster. From the results, we can see that the proposed knowledge mining can actually mine certain meaningful topic clusters, where similar examples are clustered together. For example, Cluster 1 is about asking questions about time and clocks. Cluster 2 is about counting problems. Cluster 3 is about reading texts from the wall or the clothes. Cluster 4 is about reading number texts on the vehicles. We found that there are two new tasks: clock-reading task (Cluster 1) and text-reading task (Cluster 3,4), both of which require specific prior knowledge. We also use a three-class classifier in Section <a href="#S2.SS4.SSS5" title="2.4.5 Putting It All Together by MoE ‣ 2.4 Knowledge-guided Mixture of Experts ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4.5</span></a> to classify the filtered candidate examples, so as to measure the consistency between the clustering result and classification result.
Figure <a href="#S3.F5" title="Figure 5 ‣ Knowledge Mining ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> gives t-SNE visualization of the clustering result and classification result. From the result, we can see high consistency between the clustering and classification result on the measured topics. The knowledge mining method can properly separate part of the clock-reading examples and OCR-reading examples from the other examples, although for the OCR-related exampled there still exist limited examples mixed up in the common vision category.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2111.08896/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="218" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration examples of clustering results. Percent is the percentage number of each cluster’s examples.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F5.fig1" class="ltx_figure ltx_figure_panel">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2111.08896/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="207" height="154" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F5.fig2" class="ltx_figure ltx_figure_panel">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2111.08896/assets/x6.png" id="S3.F5.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="207" height="154" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The t-SNE visualization of clustering results. Figure (a) shows the clustering results and the label 1/2/3/4/5 is cluster id. Figure (b) shows the classification results and the label ocr/clock/vision is classification label. The classifier is manually built and the accuracy of it is 95.0%.</figcaption>
</figure>
<div id="S3.SS4.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px3.p2.1" class="ltx_p">To measure the consistency of the clustering result to the classification labels, we also provide detailed quantitative analysis on different clustering methods. We manually build a three-label classifier (OCR, clock and vision) with 95% accuracy as in Section <a href="#S2.SS4.SSS5" title="2.4.5 Putting It All Together by MoE ‣ 2.4 Knowledge-guided Mixture of Experts ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4.5</span></a> and apply it to evaluate the consistence of each cluster. We project each cluster to the corresponding label heuristically. For example, in Figure <a href="#S3.F4" title="Figure 4 ‣ Knowledge Mining ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, Cluster 1 is assigned to clock label, Cluster 3 and Cluster 4 are to OCR label, and Cluster 5 is to vision label. We then compare the assigned label of each cluster to that of the classification label (95% accuracy). We use accuracy, macro-precision, macro-recall and macro-f1 to measure how consistent the compared label in each cluster is. As list in Table <a href="#S3.T8" title="Table 8 ‣ Knowledge Mining ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, K-Means (K=5) achieves the best performance with 0.8448 accuracy and 0.8739 macro-F1, which shows that the clustering result is highly consistent with the assumed classification labels on OCR/clock/vision.</p>
</div>
<figure id="S3.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Quantitative analysis on the clustering results of different clustering methods.</figcaption>
<table id="S3.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T8.1.1.1" class="ltx_tr">
<th id="S3.T8.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S3.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Acc</th>
<th id="S3.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">P</th>
<th id="S3.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">R</th>
<th id="S3.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Macro-F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T8.1.2.1" class="ltx_tr">
<th id="S3.T8.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DBSCAN (eps=0.5)</th>
<th id="S3.T8.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0.1544</th>
<td id="S3.T8.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.4605</td>
<td id="S3.T8.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.3861</td>
<td id="S3.T8.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.1466</td>
</tr>
<tr id="S3.T8.1.3.2" class="ltx_tr">
<th id="S3.T8.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">K-Means (K=3)</th>
<th id="S3.T8.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.4969</th>
<td id="S3.T8.1.3.2.3" class="ltx_td ltx_align_center">0.6487</td>
<td id="S3.T8.1.3.2.4" class="ltx_td ltx_align_center">0.662</td>
<td id="S3.T8.1.3.2.5" class="ltx_td ltx_align_center">0.6163</td>
</tr>
<tr id="S3.T8.1.4.3" class="ltx_tr">
<th id="S3.T8.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">K-Means (K=4)</th>
<th id="S3.T8.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0.7894</th>
<td id="S3.T8.1.4.3.3" class="ltx_td ltx_align_center">0.8239</td>
<td id="S3.T8.1.4.3.4" class="ltx_td ltx_align_center">0.8219</td>
<td id="S3.T8.1.4.3.5" class="ltx_td ltx_align_center">0.8195</td>
</tr>
<tr id="S3.T8.1.5.4" class="ltx_tr">
<th id="S3.T8.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">K-Means (K=5)</th>
<th id="S3.T8.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S3.T8.1.5.4.2.1" class="ltx_text ltx_font_bold">0.8448</span></th>
<td id="S3.T8.1.5.4.3" class="ltx_td ltx_align_center">0.8659</td>
<td id="S3.T8.1.5.4.4" class="ltx_td ltx_align_center">0.8898</td>
<td id="S3.T8.1.5.4.5" class="ltx_td ltx_align_center">0.8739</td>
</tr>
<tr id="S3.T8.1.6.5" class="ltx_tr">
<th id="S3.T8.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">K-Means (K=6)</th>
<th id="S3.T8.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">0.8443</th>
<td id="S3.T8.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T8.1.6.5.3.1" class="ltx_text ltx_font_bold">0.8668</span></td>
<td id="S3.T8.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T8.1.6.5.4.1" class="ltx_text ltx_font_bold">0.8918</span></td>
<td id="S3.T8.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T8.1.6.5.5.1" class="ltx_text ltx_font_bold">0.8740</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Text Reading Expert</h5>

<div id="S3.SS4.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px4.p1.1" class="ltx_p">As stated in Section <a href="#S2.SS4.SSS2" title="2.4.2 Text Reading Expert ‣ 2.4 Knowledge-guided Mixture of Experts ‣ 2 VQA Modeling ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4.2</span></a>, the pre-trained StructuralLM model is adapted on text-reading VQA samples in different ways. Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:structlm</span> shows the ablation results on the VQA test-dev set. The visual understanding expert <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>An early version of visual understanding expert for VQA Challenge 2021 is used as the baseline.</span></span></span> is used as the baseline method, upon which all the ablation experiments for text reading expert is conducted. First, it is observed that text reading expert greatly improves the performance on the “Number” type by over 6%, where many questions are asked about reading numbers from OCR text, such as a bus number and a football player number. On the “Other” type, the performance can be improved by over 1%. Answering many questions of this type requires the ability to reason with both visual and textual OCR information. Adding the separator between textual bounding boxes and continual pre-training on domain data can lead to further improvement, demonstrating the effectiveness of adapting the pre-trained StructuralLM for text-reading VQA.</p>
</div>
</section>
<section id="S3.SS4.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Clock Reading Expert</h5>

<div id="S3.SS4.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px5.p1.1" class="ltx_p">The ablation study of clock reading expert is shown in Table <a href="#S3.T9" title="Table 9 ‣ Clock Reading Expert ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, where only the results on the “Number” and “Overall” types are given, because questions on reading clocks are only present in the “Number” type. Adding clock reading expert results in more than 4.5% performance improvement on the “Number” type (from 59.93 to 62.65), which demonstrates the effectiveness of proposed ideas in the clock reading expert.
Specifically, the proposed regression loss is prone to provide a larger gradient when there is a bigger difference between the predicted time and the ground truth, which benefits prediction of the clock reader. Moreover, it can be observed that the self-supervised loss boosts the performance significantly, as the relationship prior constrains hour and minute branches both, which eliminates the confusion of hour and minute hands.</p>
</div>
<figure id="S3.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Ablation study of clock reading expert. </figcaption>
<table id="S3.T9.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T9.1.1.1" class="ltx_tr">
<th id="S3.T9.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Clock Detector</th>
<th id="S3.T9.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4">Clock Reader</th>
<th id="S3.T9.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">VQA Test-dev</th>
</tr>
<tr id="S3.T9.1.2.2" class="ltx_tr">
<th id="S3.T9.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Detection(mAP)</th>
<th id="S3.T9.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Cls</th>
<th id="S3.T9.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Regression Loss</th>
<th id="S3.T9.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Self-supervised Loss</th>
<th id="S3.T9.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Clock Accuracy</th>
<th id="S3.T9.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Number</th>
<th id="S3.T9.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T9.1.3.1" class="ltx_tr">
<td id="S3.T9.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Baseline</td>
<td id="S3.T9.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T9.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="S3.T9.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S3.T9.1.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S3.T9.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">59.93</td>
<td id="S3.T9.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">76.51</td>
</tr>
<tr id="S3.T9.1.4.2" class="ltx_tr">
<td id="S3.T9.1.4.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" rowspan="3"><span id="S3.T9.1.4.2.1.1" class="ltx_text">79.30</span></td>
<td id="S3.T9.1.4.2.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T9.1.4.2.3" class="ltx_td"></td>
<td id="S3.T9.1.4.2.4" class="ltx_td ltx_border_r"></td>
<td id="S3.T9.1.4.2.5" class="ltx_td ltx_align_center ltx_border_r">72.5</td>
<td id="S3.T9.1.4.2.6" class="ltx_td ltx_align_center">62.52</td>
<td id="S3.T9.1.4.2.7" class="ltx_td ltx_align_center">76.79</td>
</tr>
<tr id="S3.T9.1.5.3" class="ltx_tr">
<td id="S3.T9.1.5.3.1" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T9.1.5.3.2" class="ltx_td ltx_align_center">✓</td>
<td id="S3.T9.1.5.3.3" class="ltx_td ltx_border_r"></td>
<td id="S3.T9.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r">73.0</td>
<td id="S3.T9.1.5.3.5" class="ltx_td ltx_align_center">62.59</td>
<td id="S3.T9.1.5.3.6" class="ltx_td ltx_align_center">76.80</td>
</tr>
<tr id="S3.T9.1.6.4" class="ltx_tr">
<td id="S3.T9.1.6.4.1" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T9.1.6.4.2" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
<td id="S3.T9.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">✓</td>
<td id="S3.T9.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S3.T9.1.6.4.4.1" class="ltx_text ltx_font_bold">74.7</span></td>
<td id="S3.T9.1.6.4.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T9.1.6.4.5.1" class="ltx_text ltx_font_bold">62.65</span></td>
<td id="S3.T9.1.6.4.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T9.1.6.4.6.1" class="ltx_text ltx_font_bold">76.81</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Ablation study of MoE on the VQA Test-dev.</figcaption>
<table id="S3.T10.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T10.1.1.1" class="ltx_tr">
<td id="S3.T10.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T10.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
<th id="S3.T10.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Yes/No</th>
<th id="S3.T10.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number</th>
<th id="S3.T10.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Other</th>
</tr>
<tr id="S3.T10.1.2.2" class="ltx_tr">
<td id="S3.T10.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Visual Understanding Expert</td>
<td id="S3.T10.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">80.05</td>
<td id="S3.T10.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">93.67</td>
<td id="S3.T10.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">66.78</td>
<td id="S3.T10.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">71.40</td>
</tr>
<tr id="S3.T10.1.3.3" class="ltx_tr">
<td id="S3.T10.1.3.3.1" class="ltx_td ltx_align_left">   + Text Reading Expert (MoE)</td>
<td id="S3.T10.1.3.3.2" class="ltx_td ltx_align_center">81.00</td>
<td id="S3.T10.1.3.3.3" class="ltx_td ltx_align_center">93.67</td>
<td id="S3.T10.1.3.3.4" class="ltx_td ltx_align_center">69.75</td>
<td id="S3.T10.1.3.3.5" class="ltx_td ltx_align_center"><span id="S3.T10.1.3.3.5.1" class="ltx_text ltx_font_bold">72.69</span></td>
</tr>
<tr id="S3.T10.1.4.4" class="ltx_tr">
<td id="S3.T10.1.4.4.1" class="ltx_td ltx_align_left ltx_border_bb">    + Clock Reading Expert (MoE)</td>
<td id="S3.T10.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T10.1.4.4.2.1" class="ltx_text ltx_font_bold">81.27</span></td>
<td id="S3.T10.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">93.67</td>
<td id="S3.T10.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T10.1.4.4.4.1" class="ltx_text ltx_font_bold">72.55</span></td>
<td id="S3.T10.1.4.4.5" class="ltx_td ltx_align_center ltx_border_bb">72.60</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Mixture of Experts (MoE)</h5>

<div id="S3.SS4.SSS0.Px6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS4.SSS0.Px6.p1.1" class="ltx_p">The ablation study of MoE is shown in Table <a href="#S3.T10" title="Table 10 ‣ Clock Reading Expert ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. With only visual understanding expert, the model gives a strong performance of accuracy 80.05 on the VQA Test-dev set. Adding text reading expert increases the overall performance by more than 1%, which already achieves human parity of 80.83. Adding clock reading expert further boosts the performance to 81.27, where the performance on the “Number” type increases by more than 4%. The gating network of MoE mimics human who is able to identify domain experts based on the nature of tasks. This knowledge-guided MoE framework can also be easily extended to incorporate more specialized experts for continual self-evolution.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2111.08896/assets/x7.png" id="S3.F6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="112" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The distribution of abilities in each answer type.</figcaption>
</figure>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>VQA Dataset Analysis</h3>

<div id="S3.SS5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS5.p1.1" class="ltx_p">This subsection provides more detailed analysis of our VQA results. To gain an understanding of types of questions and answers, 1000 examples are randomly sampled from the validation set for analysis.
The 1000 examples are classified into the categories listed below by manual examination based on the abilities required. The categorization is multi-label in the sense that every example is classified into all applicable categories. Figure <a href="#S3.F6" title="Figure 6 ‣ Mixture of Experts (MoE) ‣ 3.4 Model Analysis by Modules ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> provides an estimate of the proportion for each category. Commonsense Knowledge, Relational Reasoning and Object Counting are the top three categories in the overall distribution. Commonsense Knowledge accounts for over 80% of the <em id="S3.SS5.p1.1.1" class="ltx_emph ltx_font_italic">Yes/No</em> type. In the <em id="S3.SS5.p1.1.2" class="ltx_emph ltx_font_italic">Number</em> type, Object Counting and Textual Recognition are the two most popular categories compared with the other two types. The type <em id="S3.SS5.p1.1.3" class="ltx_emph ltx_font_italic">Other</em> has a similar distribution as that of the overall distribution. Figure <a href="#S3.F7" title="Figure 7 ‣ 3.5 VQA Dataset Analysis ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents representative examples from each category.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Commonsense Knowledge</span> This category contains questions inquiring commonsense knowledge from our daily life, such as colors, weathers, food and furniture.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Visual Recognition</span> This category requires the ability to acquire specialized knowledge with visual recognition to answer questions in this category.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Relational Reasoning</span> This category requires understanding and reasoning over certain relationships of objects in an image, such as positional relationship, comparison relationship, etc.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Textual Recognition (OCR)</span> This category requires the ability to recognize and utilize text together with the positions or visual information in an image (e.g., road signs, ads on a bus).</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p"><span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Object Counting</span> This category contains the examples that test the ability of counting objects in an image.</p>
</div>
</li>
<li id="S3.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i6.p1" class="ltx_para">
<p id="S3.I2.i6.p1.1" class="ltx_p"><span id="S3.I2.i6.p1.1.1" class="ltx_text ltx_font_bold">Clock Reading</span> This category contains the examples that test the ability of reading a clock.</p>
</div>
</li>
<li id="S3.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i7.p1" class="ltx_para ltx_noindent">
<p id="S3.I2.i7.p1.1" class="ltx_p"><span id="S3.I2.i7.p1.1.1" class="ltx_text ltx_font_bold">Other</span> This category contains the questions that are ambiguous or cannot be answered based on given images.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2111.08896/assets/x8.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Representative examples from each category.</figcaption>
</figure>
<figure id="S3.T11" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>The overall performance of <span id="S3.T11.2.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>and human on val split.</figcaption>
<table id="S3.T11.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T11.3.1.1" class="ltx_tr">
<th id="S3.T11.3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;" rowspan="2"></th>
<th id="S3.T11.3.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">Test-std</th>
<th id="S3.T11.3.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;" colspan="4">Val</th>
</tr>
<tr id="S3.T11.3.2.2" class="ltx_tr">
<th id="S3.T11.3.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Overall</th>
<th id="S3.T11.3.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Overall</th>
<th id="S3.T11.3.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Yes/no</th>
<th id="S3.T11.3.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Number</th>
<th id="S3.T11.3.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">Other</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T11.3.3.1" class="ltx_tr">
<th id="S3.T11.3.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">VLP</th>
<th id="S3.T11.3.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T11.3.3.1.2.1" class="ltx_text ltx_font_bold">81.26</span></th>
<td id="S3.T11.3.3.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">79.54</td>
<td id="S3.T11.3.3.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">92.47</td>
<td id="S3.T11.3.3.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">70.63</td>
<td id="S3.T11.3.3.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T11.3.3.1.6.1" class="ltx_text ltx_font_bold">72.00</span></td>
</tr>
<tr id="S3.T11.3.4.2" class="ltx_tr">
<th id="S3.T11.3.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Human</th>
<th id="S3.T11.3.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">80.83</th>
<td id="S3.T11.3.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">78.69</td>
<td id="S3.T11.3.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T11.3.4.2.4.1" class="ltx_text ltx_font_bold">94.87</span></td>
<td id="S3.T11.3.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T11.3.4.2.5.1" class="ltx_text ltx_font_bold">78.79</span></td>
<td id="S3.T11.3.4.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">66.34</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T12" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>The performance of <span id="S3.T12.2.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>and human by category.</figcaption>
<table id="S3.T12.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T12.3.1.1" class="ltx_tr">
<th id="S3.T12.3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;"></th>
<th id="S3.T12.3.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">
<table id="S3.T12.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T12.3.1.1.2.1.1" class="ltx_tr">
<td id="S3.T12.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Commonsense</td>
</tr>
<tr id="S3.T12.3.1.1.2.1.2" class="ltx_tr">
<td id="S3.T12.3.1.1.2.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Knowledge</td>
</tr>
</table>
</th>
<th id="S3.T12.3.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">
<table id="S3.T12.3.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T12.3.1.1.3.1.1" class="ltx_tr">
<td id="S3.T12.3.1.1.3.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Relational</td>
</tr>
<tr id="S3.T12.3.1.1.3.1.2" class="ltx_tr">
<td id="S3.T12.3.1.1.3.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Reasoning</td>
</tr>
</table>
</th>
<th id="S3.T12.3.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">Object Counting</th>
<th id="S3.T12.3.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">Visual Recognition</th>
<th id="S3.T12.3.1.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">
<table id="S3.T12.3.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T12.3.1.1.6.1.1" class="ltx_tr">
<td id="S3.T12.3.1.1.6.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">Textual Recognition</td>
</tr>
<tr id="S3.T12.3.1.1.6.1.2" class="ltx_tr">
<td id="S3.T12.3.1.1.6.1.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:1.7pt;padding-right:1.7pt;">(OCR)</td>
</tr>
</table>
</th>
<th id="S3.T12.3.1.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">Clock Reading</th>
<th id="S3.T12.3.1.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:1.7pt;padding-right:1.7pt;">Other</th>
</tr>
<tr id="S3.T12.3.2.2" class="ltx_tr">
<th id="S3.T12.3.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_column ltx_th_row ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;"></th>
<th id="S3.T12.3.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">767</th>
<th id="S3.T12.3.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">159</th>
<th id="S3.T12.3.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">103</th>
<th id="S3.T12.3.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">70</th>
<th id="S3.T12.3.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">74</th>
<th id="S3.T12.3.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">7</th>
<th id="S3.T12.3.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T12.3.3.1" class="ltx_tr">
<th id="S3.T12.3.3.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">VLP</th>
<td id="S3.T12.3.3.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.3.1.2.1" class="ltx_text ltx_font_bold">83.60</span></td>
<td id="S3.T12.3.3.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.3.1.3.1" class="ltx_text ltx_font_bold">71.19</span></td>
<td id="S3.T12.3.3.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">77.76</td>
<td id="S3.T12.3.3.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.3.1.5.1" class="ltx_text ltx_font_bold">68.14</span></td>
<td id="S3.T12.3.3.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;">52.03</td>
<td id="S3.T12.3.3.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.3.1.7.1" class="ltx_text ltx_font_bold">86.00</span></td>
<td id="S3.T12.3.3.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.3.1.8.1" class="ltx_text ltx_font_bold">70.00</span></td>
</tr>
<tr id="S3.T12.3.4.2" class="ltx_tr">
<th id="S3.T12.3.4.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:1.7pt;padding-right:1.7pt;">Human</th>
<td id="S3.T12.3.4.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">80.04</td>
<td id="S3.T12.3.4.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">70.20</td>
<td id="S3.T12.3.4.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.4.2.4.1" class="ltx_text ltx_font_bold">81.29</span></td>
<td id="S3.T12.3.4.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">59.76</td>
<td id="S3.T12.3.4.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;"><span id="S3.T12.3.4.2.6.1" class="ltx_text ltx_font_bold">76.62</span></td>
<td id="S3.T12.3.4.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">60.66</td>
<td id="S3.T12.3.4.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-left:1.7pt;padding-right:1.7pt;">49.52</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span><span id="S3.SS6.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>vs. Human</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p">A comparative study of <span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>and human on visual question answering has been conducted. Table <a href="#S3.T11" title="Table 11 ‣ 3.5 VQA Dataset Analysis ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> and Table <a href="#S3.T12" title="Table 12 ‣ 3.5 VQA Dataset Analysis ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> show the overall and per-category performance of <span id="S3.SS6.p1.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>and human on the val split, respectively, from which there are the following observations:
(i) <span id="S3.SS6.p1.1.3" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>outperforms human annotators on the two largest categories, Commonsense Knowledge and Relational Reasoning. It shows <span id="S3.SS6.p1.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>’s superiority of identifying common scene objects in daily life and leveraging commonsense knowledge such as colors and weathers. This result also demonstrates the power of <span id="S3.SS6.p1.1.5" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>in reasoning over relative positions, such as the left sign on a wall, to answer a spatial reasoning question. Besides, it is surprising that <span id="S3.SS6.p1.1.6" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>can reason over simple comparison, such as which object is the tallest.
(ii) The questions in the Object Counting category seem rather difficult for <span id="S3.SS6.p1.1.7" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>to answer. <span id="S3.SS6.p1.1.8" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>is found to be good at counting a small number (&lt;10) of objects. It would give an incorrect count when encountering a large number of small objects and/or requiring reasoning over them.
(iii) <span id="S3.SS6.p1.1.9" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>significantly surpasses human performance on Visual Recognition which requires specialized knowledge. It is expected that <span id="S3.SS6.p1.1.10" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>, as a machine learner trained with large data, is skilled in memorizing specialized/professional knowledge with visual recognition, compared with non-professional human annotators.
(iv) <span id="S3.SS6.p1.1.11" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>is more capable of reading time shown in a clock than human, as demonstrated by the result of Clock Reading. On text reading, however, there is still a big gap between <span id="S3.SS6.p1.1.12" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>and human in recognizing and understanding text in an image, as shown by the result of Textual Recognition. Some research progress has been made on text-reading VQA tasks, such as TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.p2.1" class="ltx_p">Figure <a href="#S3.F8" title="Figure 8 ‣ Commonsense Knowledge ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, Figure <a href="#S3.F9" title="Figure 9 ‣ Object Counting ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and Figure <a href="#S3.F10" title="Figure 10 ‣ Clock Reading ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> present <span id="S3.SS6.p2.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>’s predictions together with ground truth on each category for case study. In particular, a couple of representative examples are listed for each category, each containing a question, an image and the answer predicted by <span id="S3.SS6.p2.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>. The scores of human annotators and <span id="S3.SS6.p2.1.3" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>, as well as the top three ground-truth annotations are also given for comparison. The scores of Human and <span id="S3.SS6.p2.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>are calculated based on Equation <a href="#S3.E24" title="In Evaluation Metric ‣ 3.1 Data ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>. These examples are studied by category as follows:</p>
</div>
<section id="S3.SS6.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Commonsense Knowledge</h5>

<div id="S3.SS6.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS0.Px1.p1.1" class="ltx_p"><span id="S3.SS6.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>is knowledgeable in many aspects of daily life. As shown in Figure <a href="#S3.F8" title="Figure 8 ‣ Commonsense Knowledge ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <span id="S3.SS6.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>is able to tell not only weather and the sentiments of people, but also classic sports and electronic products as an ordinary person does. Also, it is skilled in geography and understands the food around the world. For example, <span id="S3.SS6.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>recognizes the small English flag and Big Ben in the image, by which the country is identified as England. As another example, based on the rice and dishes from the image, Chinese food can be identified by <span id="S3.SS6.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>and people familiar with Chinese cuisine. The other people may not tell cuisine of the food .
Our experiments show that <span id="S3.SS6.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>trained on adequate data can capture the commonsense knowledge (the largest category in the VQA dataset) in our daily life.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2111.08896/assets/x9.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Case Study for Commonsense Knowledge and Visual Recognition. The scores of Human and <span id="S3.F8.2.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>are calculated with Equation <a href="#S3.E24" title="In Evaluation Metric ‣ 3.1 Data ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>. Ground Truth gives the top three annotations.</figcaption>
</figure>
</section>
<section id="S3.SS6.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Visual Recognition</h5>

<div id="S3.SS6.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS0.Px2.p1.1" class="ltx_p">Except for Clock Reading, <span id="S3.SS6.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>shows much better performance than an ordinary person does in this category. As shown in Figure <a href="#S3.F8" title="Figure 8 ‣ Commonsense Knowledge ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, it is relatively easy for an AI model trained adequately to memorize specialized knowledge, while rather difficult for people unfamiliar with the specific domain. For example, <span id="S3.SS6.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>can better identify the specific categories of the animals, such as dog and bird, and the historical style of the furniture, which requires specialized knowledge. By locating and recognizing the barely visible logo from the motor bike, <span id="S3.SS6.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>correctly recognizes its brand, while human may miss the details in the image and give an answer based on their best guesses. On the other hand, <span id="S3.SS6.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>has a slim chance of being fooled by the activity present in the image. It incorrectly identifies that the boy is playing baseball based on his motion, while it is actually a Wii game. As a result, by incorporating relevant specialized knowledge and visual recognition capability, <span id="S3.SS6.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>can outperform human by a large margin in this category.</p>
</div>
</section>
<section id="S3.SS6.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Object Counting</h5>

<div id="S3.SS6.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS0.Px3.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F9" title="Figure 9 ‣ Object Counting ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, <span id="S3.SS6.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>achieves human parity when counting a small number of objects, but fails in more complicated cases where there are occlusions or a great quantity of objects. It surprises us that <span id="S3.SS6.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>can give a count very close to the correct answer in the example of counting trees. However, the object counting ability is still quite limited compared with ordinary people. One reason may lie in that the visual detection is too weak to detect all the objects in an image when the number of objects is large. There are few cases with more than 10 objects in the training set, and thus the model is not fully trained with sufficient data. This is shown in the third example where <span id="S3.SS6.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>fails to identify all people from the image. Another possible reason is that the object detector is difficult to count in the presence of occlusion. The second example shows that <span id="S3.SS6.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>counts the racing people incorrectly due to the occluded person.</p>
</div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2111.08896/assets/x10.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="358" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Case Study for Object Counting and Relational Reasoning.</figcaption>
</figure>
</section>
<section id="S3.SS6.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Relational Reasoning</h5>

<div id="S3.SS6.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS0.Px4.p1.1" class="ltx_p">Figure <a href="#S3.F9" title="Figure 9 ‣ Object Counting ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows that <span id="S3.SS6.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>has the abilities to reason over the relationship of positions, comparison and exclusion. It is observed that <span id="S3.SS6.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>may be more capable than human in precise position identification and knowledge reasoning for relational reasoning questions. Specifically, 1) <em id="S3.SS6.SSS0.Px4.p1.1.3" class="ltx_emph ltx_font_italic">position</em>: the first two examples show the power of <span id="S3.SS6.SSS0.Px4.p1.1.4" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>in distinguishing the positions of the left-right and front-back, and conducting one-step reasoning over the positional relationship; 2) <em id="S3.SS6.SSS0.Px4.p1.1.5" class="ltx_emph ltx_font_italic">comparison</em>: the third example demonstrates that <span id="S3.SS6.SSS0.Px4.p1.1.6" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>can even compare the colors of two objects, which is a simple one-step reasoning over the comparison between attributes of two objects; 3) <em id="S3.SS6.SSS0.Px4.p1.1.7" class="ltx_emph ltx_font_italic">exclusion</em>: the last example shows that <span id="S3.SS6.SSS0.Px4.p1.1.8" class="ltx_text ltx_font_smallcaps">AliceMind-MMU </span>is able to identify the exclusion relationship, and reason over it with commonsense knowledge.</p>
</div>
</section>
<section id="S3.SS6.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Textual Recognition (OCR)</h5>

<div id="S3.SS6.SSS0.Px5.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS0.Px5.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F10" title="Figure 10 ‣ Clock Reading ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the text reading expert (StructuralLM) is able to identify text and layout in simple cases. In the first example, the model correctly answers with the words displayed on the man’s shirt. In addition, StructuralLM is capable of learning the interactions between text and layout, which makes StructuralLM aware of location of text present in an image. This is shown in the second example, where the model predicts the answer correctly when asked about the sign on the left. However, the model fails in the two cases: 1) OCR errors; 2) answering complex questions which requires visual feature and reasoning abilities. As shown in the third example, when asked the words on the man’s shirt, the model can predict only “3” because the OCR tool cannot recognize the word “cardinals”. In the fourth example, given the question about the number present on the white shirt, the model answers incorrectly due to the lack of visual feature of colors and the reasoning ability. Currently, the text reading expert utilizes only the layout and textual information to answer a text-reading question, without leveraging visual signals. There is an urgent need for deep interaction between visual information and OCR textual information in images, which is left for future work.</p>
</div>
</section>
<section id="S3.SS6.SSS0.Px6" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Clock Reading</h5>

<div id="S3.SS6.SSS0.Px6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.SSS0.Px6.p1.1" class="ltx_p">As shown in Figure <a href="#S3.F10" title="Figure 10 ‣ Clock Reading ‣ 3.6 AliceMind-MMU vs. Human ‣ 3 Experiments ‣ Achieving Human Parity on Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the clock reading expert is able to read the clock time accurately at a five-minute level. One important problem to be addressed is distinguishing the hour hand (generally shorter) from the minute hand (generally longer). The clock reading expert is trained well on this objective. Therefore, in the first example, the model predicts the correct time “8:05”, while some human annotators misread the hour hand and minute hand, and thus give the wrong time reading “1:40”. In the second example, the clock reading expert can tell the time accurately even when the hour hand and minute hand overlap. There also exist limitations for the current clock reading expert, that it can’t tell more accurate time at a minute level. In the fourth example, the model only recognizes the time is about 12:10, but cannot tell the exact time of 12:12. The reason comes from casting the problem as detection and then classification, of which the clock training data is not adequate to support training of 1-minute level clock reading.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2111.08896/assets/x11.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Case Study for Textural Recognition (OCR) and Clock Reading.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Visual Question Answering</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">First released in 2016, the VQA task and research on the dataset have been developing for more than five years. A large number of new methods have been proposed to push the limit of this task. Existing approaches on the VQA task mostly put efforts on four key aspects: 1) better visual feature representation, 2) effective cross-modal fusion, 3) large-scale vision-and-language pre-training, and 4) task-specific expert optimization. Region-based visual features have long been treated as the <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">de facto</em> standard for vision and language tasks such as VQA, where visual semantics and salient image objects are captured with bottom-up attention <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>. The proposed bottom-up attention method with region features won the VQA Challenge of 2017, which has been largely adopted since then. Recently, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> has further pushed the limit of the region-based features by pre-training on extremely large object detection datasets. On the other hand, <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> studied the key factors that contributed to the effectiveness of existing bottom-up attention features, and found that the grid-based features from convolutional neural networks can yield comparable or even better results on the VQA task. It won the VQA Challenge of 2020.
More recently, inspired by the popular vision Transformer <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>, some pioneer work such as ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> began to study the use of patch-based visual feature for its efficiency.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">Cross-modal fusion or interaction has always been treated as one of the most important challenges in cross-modality research. At first, the cross-modal fusion on VQA is simply element-wise product of textual and visual representation <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. Then, it gradually improves from a linear model <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite> to bilinear pooling ones <cite class="ltx_cite ltx_citemacro_citep">(Fukui et al., <a href="#bib.bib63" title="" class="ltx_ref">2016</a>; Yu et al., <a href="#bib.bib64" title="" class="ltx_ref">2017</a>)</cite>, and attention-based fusion ones <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib65" title="" class="ltx_ref">2016</a>; Yang et al., <a href="#bib.bib66" title="" class="ltx_ref">2016</a>; Yu et al., <a href="#bib.bib67" title="" class="ltx_ref">2019b</a>)</cite>, where questions and image features are fully interacted with each other. Recently, with the popularity of <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">pre-train and fine-tune</em> paradigm such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> in NLP, large-scale vision-and-language pre-training <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Su et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib68" title="" class="ltx_ref">2020c</a>; Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib60" title="" class="ltx_ref">2020b</a>; Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Yu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> has been used to better align the vision-language representations. It has established the new state-of-the-art performance on the VQA task since 2020, by pre-training on a large amount of unlabeled image-text pairs.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p">To further improve the VQA performance, some studies analyze the weakness of existing models on the VQA task, and address it with specific expert optimization. For example, MoVie <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al., <a href="#bib.bib69" title="" class="ltx_ref">2020</a>)</cite> revisited modulated convolutions for visual counting, which is more efficient and advances the state-of-the-art on counting-specific VQA tasks. <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib70" title="" class="ltx_ref">2016</a>)</cite> propose a knowledge-enhanced VQA method to combine an internal representation of image content with information extracted from a general knowledge base to answer a broad range of image-based questions.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Visual Features for Vision-and-Language tasks</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">Feature representations for vision have played a key role in the advancement of vision and language tasks. Different kinds of features can capture diverse data characteristics, which complements with each other. In most of the vision-and-language tasks, visual feature is the bottleneck of existing multi-modal models and will set the upper bound for the final performance. To capture both the local and global information in the image, this work takes a comprehensive visual feature understanding on three kinds of typical features.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Region Feature</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">Popularized as bottom-up attention <cite class="ltx_cite ltx_citemacro_citep">(Anderson et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, region-based visual features have been treated as the de facto standard for vision and language tasks and achieved dominant performance in tasks such as visual question answering (VQA) and image captioning <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib71" title="" class="ltx_ref">2015</a>)</cite>. It uses pre-trained object detectors <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite> to identify salient regions based on the vision input. A region proposal network (RPN) is first used to propose regions of interest (RoI) based on the grid features pooled from the CNN backbone. Then non-maximum suppression (NMS) is used to select a small collection of proper RoIs, and RoI head is used to extract region feature for each selected RoI. As a result, images are represented by a collection of region-based features. Most current VLP approaches such as LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, UNITER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, ERNIE-ViL <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> and OSCAR <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib60" title="" class="ltx_ref">2020b</a>)</cite> adopt the region feature-based VLP paradigm. Recently, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite> have proposed an extreme large version of the region-based object detector, which is pretrained on four object detection datasets, including MS COCO <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib46" title="" class="ltx_ref">2014</a>)</cite>, Visual Genome <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, OpenImages <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova et al., <a href="#bib.bib72" title="" class="ltx_ref">2020</a>)</cite>, and Object365 <cite class="ltx_cite ltx_citemacro_citep">(Shao et al., <a href="#bib.bib73" title="" class="ltx_ref">2019</a>)</cite>. It greatly improved the performance of VLP models, creating new state-of-the-art results on seven public benchmarks.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Grid Feature</h5>

<div id="S4.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px2.p1.1" class="ltx_p">Besides the local region-based features, the grid-like feature map from convolutional neural networks such as ResNet <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> can also be used as visual features for visual-and-language tasks. One of the largest advantages of grid feature is that it removes all the time-consuming region-related steps, which can support more flexible architecture design of end-to-end vision-and-language models. Successful use of grid feature was first proposed by GridFeat <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite> on VQA and image captioning task. It adopts the same pre-training settings as in the region-based method, discarding all the region-related steps from the detector and using only the grid convolutional features during inference. In this way, it greatly reduces the inference time of visual encoder and obtains comparable or even better accuracy on VQA task. Grid-VLP <cite class="ltx_cite ltx_citemacro_citep">(Yan et al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite> further uses grid features for vision-language pre-training, and by using only in-domain datasets, Grid-VLP outperforms most state-of-the-art region-based VLP methods. Pixel-BERT <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> is the first end-to-end grid-based VLP method, which uses a ResNet image encoder and jointly optimizes both ResNet image encoder and cross-modality Transformer in an end-to-end manner. E2E-VLP <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> builds upon the detection transformer (DETR) <cite class="ltx_cite ltx_citemacro_citep">(Carion et al., <a href="#bib.bib74" title="" class="ltx_ref">2020</a>)</cite>, and proposes an end-to-end grid-based VLP model for both V+L understanding and generation. It enhances learning of the pre-trained model by capturing more semantic-related visual representation with object detection and image caption pretext tasks.</p>
</div>
</section>
<section id="S4.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Patch Feature</h5>

<div id="S4.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.SSS0.Px3.p1.1" class="ltx_p">Patch feature gains its popularity with the rise of Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite>, which firstly splits an image into fixed-size patches, then uses a simple linear projection of a patch before feeding them into transformers. Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite> is the first successful use of self-attention architecture to replace the convolutional neural network architecture in computer vision, after which a series of related work <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Touvron et al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>; Chen et al., <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> has been proposed to promote the development of this new direction. Swin Transformer <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> further builds the model on multi-scale feature maps and uses a shifted windowing scheme on image patches, which makes it compatible with a broad range of vision tasks such as image classification and object detection. Most of the work in this ViT series adopts the image patch feature so as to reserve high efficiency in self attention-based Transformer modeling. CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite> proposes a simple contrastive pre-training method to learn the visual models based on a dataset of 400 million image-text pairs, which obtains superior zero-shot performance on various existing computer vision datasets. Recently, ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> has proposed the pioneering work of vision-language pre-training on image patch feature, and achieved up to tens of times faster than previous VLP models with a convolution-free scheme.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Vision-and-Language Pre-training</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">Inspired by the breakthrough of language pre-training in NLP field, the research community begins to pay more attention to vision-language pre-training on large-scale image-text pairs, which also shows great effectiveness and achieves state-of-the-art performance across a variety of vision-language (VL) tasks <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib47" title="" class="ltx_ref">2015</a>; Suhr et al., <a href="#bib.bib75" title="" class="ltx_ref">2018</a>; Hudson and Manning, <a href="#bib.bib48" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p">Existing approaches to VLP <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Su et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib68" title="" class="ltx_ref">2020c</a>; Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib60" title="" class="ltx_ref">2020b</a>; Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Yu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> mainly take a two-step training pipeline, which first extracts semantic visual features by specific object detector and then learns a cross-modal pre-training model to align text and visual features. Current research about this topic can be roughly divided into two lines. The first line adopts a single-stream transformer architecture to model both image and text representations in a unified semantic space such as VLBERT <cite class="ltx_cite ltx_citemacro_citep">(Su et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>, UNITER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> and OSCAR <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib60" title="" class="ltx_ref">2020b</a>)</cite>. In contrast, the other line uses a two-stream Transformer architecture that first encodes the image and text modalities separately, and then fuses the cross-modal representations with another Transformer network, such as LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> and ERNIE-ViL <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. Recently, VinVL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
pre-trained a large-scale object-attribute detection model with much larger amounts of data on four public object detection datasets for extracting better region feature, and creating new state-of-the-art results on seven public benchmarks. In addition to image-text pairs, UNIMO <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib58" title="" class="ltx_ref">2020a</a>)</cite> also employed large scale of free text corpus and image collections for enhancing the cross-modal learning. These methods rely heavily on a task-specific bounding box (or region) based object detector, which impose unnecessary constraints on model designs and limit potential applications of existing vision and language systems. Therefore, PixelBERT <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> and E2E-VLP <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> further proposed end-to-end VLP method, by jointly learning both the visual encoder and cross-modal Transformer simultaneously. Besides, the object detector is removed from the whole process, and the VLP model directly conducts on the grid-based feature map from convolutional visual encoder. To further improve the training and inference speed of VLP model, ViLT <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite> removed both the region supervision and convolutional visual encoder, and conducted vision-language pre-training directly on image patch feature with linear projection.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Limitation</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">This paper describes our new research work on improving the full pipeline of the VQA task, which has achieved human parity on this challenging task for the first time. The key to the breakthrough lies in three aspects: 1) more comprehensive textual and visual feature representation with pre-trained technologies, 2) more effective cross-modal interaction with learning to attend, and 3) more knowledge-guided optimization with mixture of expert modules. It demonstrates the power of an AI model to achieve human parity on the challenging cross-modal understanding task under a closed-set scenario, which requires AI to have the ability to understand both visual and textual information. This makes it possible to further conduct higher-order cognition and commonsense reasoning intelligence.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">Despite the success, the current AI technology on V&amp;L understanding still has notable limitations, and bridging the gap between machine intelligence and real human intelligence still has a long way to go. For the VQA task, there still exist certain weaknesses for our model: 1) object counting is still a very difficult problem for the current VLP model, especially the case when a large number of different objects with a small or tiny size exist. Besides, some objects can even overlap with each other, which introduces more complexity; 2) composite reading of both OCR text and visual content is still challenging for the current VLP model, while it is relatively easy for human to answer; 3) the current model can only work in a closed-set scenario, which is not applicable to the open-domain or unseen set. In the future, we hope to witness more breakthrough on open-set learning and more intelligent AI models, which can evolve itself by acquiring and reasoning about new knowledge.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silver et al. [2016]</span>
<span class="ltx_bibblock">
David Silver, Aja Huang, Christopher J. Maddison, Arthur Guez, Laurent Sifre,
George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
Kavukcuoglu, Thore Graepel, and Demis Hassabis.

</span>
<span class="ltx_bibblock">Mastering the game of go with deep neural networks and tree search.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 529:484–503, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009a]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</em>, pages 248–255. Ieee, 2009a.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2018]</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman.

</span>
<span class="ltx_bibblock">GLUE: A multi-task benchmark and analysis platform for natural
language understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP</em>, pages 353–355,
Brussels, Belgium, November 2018. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharif Razavian et al. [2014]</span>
<span class="ltx_bibblock">
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.

</span>
<span class="ltx_bibblock">Cnn features off-the-shelf: an astounding baseline for recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</em>, pages 806–813, 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. [2018]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. [2017]</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence
Zitnick, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vision</em>, 123(1):4–31, May
2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. [2019]</span>
<span class="ltx_bibblock">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Vl-bert: Pre-training of generic visual-linguistic representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.08530</em>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 104–120.
Springer, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal [2019]</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.07490</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2021]</span>
<span class="ltx_bibblock">
Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang.

</span>
<span class="ltx_bibblock">Ernie-vil: Knowledge enhanced vision-language representations through
scene graphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 35, pages 3208–3216, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. [2018]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077–6086, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2015]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
28:91–99, 2015.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. [2017]</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32–73, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2021]</span>
<span class="ltx_bibblock">
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang,
Yejin Choi, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Vinvl: Revisiting visual representations in vision-language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 5579–5588, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021a]</span>
<span class="ltx_bibblock">
Chenliang Li, Ming Yan, Haiyang Xu, Fuli Luo, Wei Wang, Bin Bi, and Songfang
Huang.

</span>
<span class="ltx_bibblock">Semvlp: Vision-language pre-training by aligning semantics at
multiple levels.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.07829</em>, 2021a.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2020]</span>
<span class="ltx_bibblock">
Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu.

</span>
<span class="ltx_bibblock">Pixel-bert: Aligning image pixels with text by deep multi-modal
transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.00849</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2021]</span>
<span class="ltx_bibblock">
Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and
Fei Huang.

</span>
<span class="ltx_bibblock">E2e-vlp: End-to-end vision-language pre-training enhanced by visual
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.01804</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. [2021]</span>
<span class="ltx_bibblock">
Ming Yan, Haiyang Xu, Chenliang Li, Bin Bi, Junfeng Tian, Min Gui, and Wei
Wang.

</span>
<span class="ltx_bibblock">Grid-vlp: Revisiting grid features for vision-language pre-training,
2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2020]</span>
<span class="ltx_bibblock">
Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei
Chen.

</span>
<span class="ltx_bibblock">In defense of grid features for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 10267–10276, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2016]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 770–778, 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009b]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</em>, pages 248–255. Ieee, 2009b.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.00020</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. [2021]</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V
Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.

</span>
<span class="ltx_bibblock">Scaling up visual and vision-language representation learning with
noisy text supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.05918</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai and Johnson [2021]</span>
<span class="ltx_bibblock">
Karan Desai and Justin Johnson.

</span>
<span class="ltx_bibblock">Virtex: Learning visual representations from textual annotations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 11162–11173, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2020]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021]</span>
<span class="ltx_bibblock">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.

</span>
<span class="ltx_bibblock">Swin transformer: Hierarchical vision transformer using shifted
windows.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.14030</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2021]</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and Hervé Jégou.

</span>
<span class="ltx_bibblock">Training data-efficient image transformers &amp; distillation through
attention.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
10347–10357. PMLR, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2021]</span>
<span class="ltx_bibblock">
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei
Ma, Chunjing Xu, Chao Xu, and Wen Gao.

</span>
<span class="ltx_bibblock">Pre-trained image processing transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 12299–12310, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2021]</span>
<span class="ltx_bibblock">
Wonjae Kim, Bokyung Son, and Ildoo Kim.

</span>
<span class="ltx_bibblock">Vilt: Vision-and-language transformer without convolution or region
supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.03334</em>, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. [2021]</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual 12m: Pushing web-scale image-text pre-training to
recognize long-tail visual concepts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 3558–3568, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2019]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and
Luo Si.

</span>
<span class="ltx_bibblock">Structbert: incorporating language structures into pre-training for
deep language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.04577</em>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pages
5998–6008, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MacQueen et al. [1967]</span>
<span class="ltx_bibblock">
James MacQueen et al.

</span>
<span class="ltx_bibblock">Some methods for classification and analysis of multivariate
observations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the fifth Berkeley symposium on mathematical
statistics and probability</em>, volume 1, pages 281–297. Oakland, CA, USA,
1967.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021b]</span>
<span class="ltx_bibblock">
Chenliang Li, Bin Bi, Ming Yan, Wei Wang, Songfang Huang, Fei Huang, and Luo
Si.

</span>
<span class="ltx_bibblock">Structurallm: Structural pre-training for form understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2105.11210, 2021b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2105.11210" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2105.11210</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. [2016]</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.

</span>
<span class="ltx_bibblock">Squad: 100,000+ questions for machine comprehension of text.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.05250</em>, 2016.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2017]</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.

</span>
<span class="ltx_bibblock">Reading wikipedia to answer open-domain questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.00051</em>, 2017.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai and Vasconcelos [2018]</span>
<span class="ltx_bibblock">
Zhaowei Cai and Nuno Vasconcelos.

</span>
<span class="ltx_bibblock">Cascade r-cnn: Delving into high quality object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6154–6162, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al. [2018]</span>
<span class="ltx_bibblock">
Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang.

</span>
<span class="ltx_bibblock">Two at once: Enhancing learning and generalization capacities via
ibn-net.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em>, pages 464–479, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2018]</span>
<span class="ltx_bibblock">
Jie Hu, Li Shen, and Gang Sun.

</span>
<span class="ltx_bibblock">Squeeze-and-excitation networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 7132–7141, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. [1991]</span>
<span class="ltx_bibblock">
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Adaptive mixtures of local experts.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Neural computation</em>, 3(1):79–87, 1991.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et al. [2017]</span>
<span class="ltx_bibblock">
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
Geoffrey Hinton, and Jeff Dean.

</span>
<span class="ltx_bibblock">Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1701.06538</em>, 2017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus et al. [2021]</span>
<span class="ltx_bibblock">
William Fedus, Barret Zoph, and Noam Shazeer.

</span>
<span class="ltx_bibblock">Switch transformers: Scaling to trillion parameter models with simple
and efficient sparsity.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.03961</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2019]</span>
<span class="ltx_bibblock">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Towards VQA models that can read.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1904.08920, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1904.08920" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1904.08920</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biten et al. [2019]</span>
<span class="ltx_bibblock">
Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluís
Gómez, Marçal Rusiñol, Ernest Valveny, C. V. Jawahar, and
Dimosthenis Karatzas.

</span>
<span class="ltx_bibblock">Scene text visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1905.13648, 2019.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://arxiv.org/abs/1905.13648" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1905.13648</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning [2019]</span>
<span class="ltx_bibblock">
Drew A Hudson and Christopher D Manning.

</span>
<span class="ltx_bibblock">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em>, pages 6700–6709, 2019.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2016]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4995–5004, 2016.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. [2018]</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.

</span>
<span class="ltx_bibblock">Conceptual captions: A cleaned, hypernymed, image alt-text dataset
for automatic image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565,
2018.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. [2006]</span>
<span class="ltx_bibblock">
D. Lewis, G. Agam, S. Argamon, O. Frieder, D. Grossman, and J. Heard.

</span>
<span class="ltx_bibblock">Building a test collection for complex document information
processing.

</span>
<span class="ltx_bibblock">SIGIR ’06, page 665–666, New York, NY, USA, 2006. Association for
Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 1595933697.

</span>
<span class="ltx_bibblock">doi:<a target="_blank" href="https://doi.org/10.1145/1148170.1148307" title="" class="ltx_ref ltx_href">10.1145/1148170.1148307</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/1148170.1148307" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/1148170.1148307</a>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2017]</span>
<span class="ltx_bibblock">
Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.

</span>
<span class="ltx_bibblock">Aggregated residual transformations for deep neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 1492–1500, 2017.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2019a]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>, pages 6281–6290, 2019a.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. [2020]</span>
<span class="ltx_bibblock">
Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu.

</span>
<span class="ltx_bibblock">Large-scale adversarial training for vision-and-language
representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2019]</span>
<span class="ltx_bibblock">
Dalu Guo, Chang Xu, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Bilinear graph networks for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.09815</em>, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2020]</span>
<span class="ltx_bibblock">
Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang.

</span>
<span class="ltx_bibblock">Interbert: Vision-and-language interaction for multi-modal
pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.13198</em>, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al. [2021]</span>
<span class="ltx_bibblock">
Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and Jun
Yu.

</span>
<span class="ltx_bibblock">Rosita: Enhancing vision-and-language semantic alignments via cross-
and intra-modal knowledge integration, 2021.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020a]</span>
<span class="ltx_bibblock">
Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and
Haifeng Wang.

</span>
<span class="ltx_bibblock">Unimo: Towards unified-modal understanding and generation via
cross-modal contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.15409</em>, 2020a.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2021]</span>
<span class="ltx_bibblock">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.

</span>
<span class="ltx_bibblock">Simvlm: Simple visual language model pretraining with weak
supervision.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2108.10904, 2021.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020b]</span>
<span class="ltx_bibblock">
Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
Wang, Houdong Hu, Li Dong, Furu Wei, et al.

</span>
<span class="ltx_bibblock">Oscar: Object-semantics aligned pre-training for vision-language
tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 121–137.
Springer, 2020b.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2019]</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pages
13–23, 2019.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2020]</span>
<span class="ltx_bibblock">
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.

</span>
<span class="ltx_bibblock">12-in-1: Multi-task vision and language representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">The IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, June 2020.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui et al. [2016]</span>
<span class="ltx_bibblock">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1606.01847</em>, 2016.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2017]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao.

</span>
<span class="ltx_bibblock">Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 1821–1830, 2017.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2016]</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
29:289–297, 2016.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2016]</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.

</span>
<span class="ltx_bibblock">Stacked attention networks for image question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 21–29, 2016.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2019b]</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 6281–6290, 2019b.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020c]</span>
<span class="ltx_bibblock">
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang.

</span>
<span class="ltx_bibblock">Unicoder-vl: A universal encoder for vision and language by
cross-modal pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volume 34, pages 11336–11344, 2020c.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2020]</span>
<span class="ltx_bibblock">
Duy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen.

</span>
<span class="ltx_bibblock">Movie: Revisiting modulated convolutions for visual counting and
beyond.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.11883</em>, 2020.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2016]</span>
<span class="ltx_bibblock">
Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel.

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4622–4630, 2016.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2015]</span>
<span class="ltx_bibblock">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco captions: Data collection and evaluation server.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>, 2015.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. [2020]</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, et al.

</span>
<span class="ltx_bibblock">The open images dataset v4.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">International Journal of Computer Vision</em>, 128(7):1956–1981, 2020.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2019]</span>
<span class="ltx_bibblock">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
Li, and Jian Sun.

</span>
<span class="ltx_bibblock">Objects365: A large-scale, high-quality dataset for object detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 8430–8439, 2019.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al. [2020]</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko.

</span>
<span class="ltx_bibblock">End-to-end object detection with transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 213–229.
Springer, 2020.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr et al. [2018]</span>
<span class="ltx_bibblock">
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.

</span>
<span class="ltx_bibblock">A corpus for reasoning about natural language grounded in
photographs.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.00491</em>, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2111.08895" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2111.08896" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2111.08896">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2111.08896" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2111.08897" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 16:34:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
