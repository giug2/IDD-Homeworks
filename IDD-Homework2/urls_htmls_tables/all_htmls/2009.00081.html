<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2009.00081] Federated Edge Learning : Design Issues and Challenges</title><meta property="og:description" content="Federated Learning (FL) is a distributed machine learning technique, where each device contributes to the learning model by independently computing the gradient based on its local training data. It has recently become …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Edge Learning : Design Issues and Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Edge Learning : Design Issues and Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2009.00081">

<!--Generated on Thu Mar 14 11:40:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Challenges; Data Diversity; Device Scheduling; Design; Federated Learning; Resources Allocation.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Edge Learning : Design Issues and Challenges
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Afaf Taïk  and Soumaya Cherkaoui
</span><span class="ltx_author_notes"> Afaf Taïk and Soumaya Cherkaoui are with the INTERLAB Research Laboratory, Faculty of Engineering, Department of Electrical and Computer Science Engineering, University of Sherbrooke, Sherbrooke, QC J1k 2R1, Canada, (e-mail: afaf.taik@usherbrooke.ca, soumaya.cherkaoui@usherbrooke.ca).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning (FL) is a distributed machine learning technique, where each device contributes to the learning model by independently computing the gradient based on its local training data. It has recently become a hot research topic, as it promises several benefits related to data privacy and scalability. However, implementing FL at the network edge is challenging due to system and data heterogeneity and resources constraints.
In this article, we examine the existing challenges and trade-offs in Federated Edge Learning (FEEL). The design of FEEL algorithms for resources-efficient learning raises several challenges. These challenges are essentially related to the multidisciplinary nature of the problem. As the data is the key component of the learning, this article advocates a new set of considerations for data characteristics in wireless scheduling algorithms in FEEL. Hence, we propose a general framework for the data-aware scheduling as a guideline for future research directions. We also discuss the main axes and requirements for data evaluation and some exploitable techniques and metrics.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>
Challenges; Data Diversity; Device Scheduling; Design; Federated Learning; Resources Allocation.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The growing interest in intelligent services motivates the integration of artificial intelligence (AI) in Internet of Things (IoT) applications. The collection of large volumes from the different devices and sensors is necessary for training AI models.
However, uploading massive data generated by connected devices to the cloud is usually impractical, mainly due to issues including privacy, network congestion, and latency.
Federated Edge Learning (FEEL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a Machine Learning (ML) setting that utilizes edge computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> to tackle these concerns. In contrast to centralized ML, Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> consists of training the model on the devices, with the orchestration of a central entity, where only the resultant model parameters are sent to the edge servers to be aggregated.
FEEL refers to the use of FL at the edge of the network, which makes it a promising solution for privacy preserving ML.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An important design decision for a FEEL algorithm is whether to choose either asynchronous or synchronous aggregation. Recent works tend to promote synchronous training, where, for instance, synchronization among participating devices is required for updates averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and privacy-preservation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. However, there are many challenges upon using synchronous FL in edge environments.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To begin with, the heterogeneity of resources across different devices sparks new system challenges. For instance, significant delays can be caused by stragglers. Moreover, communication loads across devices limit the scalability of FL for large models. Participating devices communicate full model updates during every training iteration, which are of the same size as the trained model. For large models, such as deep neural networks, the model size can be in the range of gigabytes. As a result, if communication bandwidth is limited or communication is costly, FEEL can be deemed impractical or unfeasible, as communication overhead becomes a bottleneck for FEEL.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Furthermore, end devices have limited battery lives and varying available energy levels. As training ML models is a computation-heavy task, only devices that have enough energy can be solicited to participate. Furthermore, energy and computational constraints limit both the size of the models that can be trained on-device, and the number of local training iterations.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Additionally, as the data collected by the clients depends on their local environment and usage pattern, both the size and the distribution of the local datasets will typically vary between different clients. This non-Independently and Identically Distributed (non-IID) and unbalanced nature of data across the network imposes significant challenges linked to models’ convergence.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Consequently, designing an efficient FEEL algorithm should take into account the limited and heterogeneous nature of the resources, alongside the non-IID and unbalanced aspect of the data distributions. In general, proposed FEEL algorithms target efficient selection of participant devices, optimization of the resource allocation and usage, or adequate updates’ aggregation. However, it is hard to capture both the resources problems and the learning goal, as there is no direct relation between the model’s loss function and the resource optimization. A manageable approach found in current works is to focus on resource optimization with certain learning guarantees, such as maximizing the number of collected updates and maintaining the level of local accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Nonetheless, these guarantees are not sufficient, as a significant drop in accuracy is observed when data is non-IID and unbalanced.
Therefore, we propose to lighten the effects of design trade-offs through the direct integration of the data properties in the device selection and resource optimization algorithms. In fact, data properties were at the heart of FL since its inception, but they have been largely overlooked in the design of FEEL algorithms. Moreover, data diversity has long been premised on in active learning, where models can be trained using few labelled data samples if the highly diverse data is selectively added to the training set. Thus, data diversity should be considered in the design of FEEL algorithms, as we advocate in this article.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The main contributions of this article can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We discuss the FEEL challenges imposed by the nature of the edge environment, from an algorithms design perspective. We review the challenges related to computational and communication capacities, as well as data properties, as they are at the core of the trade-offs in learning and resource optimization algorithms.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a general framework for incorporating data properties in FEEL, by providing a guideline for a thorough algorithm design, and criteria for the choice of diversity measures in both datasets and models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We present several possible measures and techniques to evaluate data and model diversity, which can be applied in different scenarios (e.g., classification, time series forecasting), in an effort to assist fellow researchers to further address FEEL challenges.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of this article is as follows. In Section II, we review the challenges found in designing FEEL algorithms, and we derive the main trade-offs. Then, we shed the light on a new data-aware design direction for FEEL algorithms in section III. Some possible techniques and methods to evaluate diversity are detailed in this section. At last, a conclusion and final remarks are presented in Section IV.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Design challenges : Overview</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">FEEL has several constraints related to the nature of the edge environment. In fact, FEEL involves the participation of heterogeneous devices that have different computation and communication capabilities, energy states, and dataset characteristics. Under device and data heterogeneity, in addition to resources constraints, participants selection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and resource allocation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> have to be optimized for an efficient FEEL solution.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Design Challenges</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The core challenges associated with solving the distributed optimization problem are twofold: Resources and Data. These challenges increase the FEEL setting complexity compared to similar problems, such as distributed learning in data centers.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Resources:</span> The challenges related to the resources, namely computation, storage and communication, are mainly in terms of their heterogeneity and scarcity.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">Heterogeneity of the resources:</span> The computation, storage and communication capabilities vary from a device to another. Devices may be equipped with different hardware (CPU and memory), network connectivity (e.g., 4G/5G, Wi-Fi), and may differ in available power (battery level).
The gap in computational resources creates challenges such as delays caused by stragglers. FEEL algorithms must therefore be adaptive to the heterogeneous hardware and be tolerant toward device drop-out and low or partial participation. A potential solution to the straggler problem is asynchronous learning. However, the reliability of asynchronous FL and the model convergence in this setting are not always guaranteed. Thus, synchronous FL remains the preferred approach.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><span id="S2.SS1.p4.1.1" class="ltx_text ltx_font_italic">Limited Resources:</span> In a contrast to the cloud, the computing and storage resources of the devices are very limited. Therefore the models that can be trained on device are relatively simpler and smaller than the models trained on the cloud. Furthermore, devices are frequently offline or unavailable either due to low battery levels, or because their resources are fully or partially used by other applications.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">As for the communication resources, the available bandwidth is limited. It is therefore important to develop communication-efficient methods that allow to send compressed or partial model updates. To further reduce communication cost in FEEL settings, two potential directions are generally considered 1) reducing the total number of communication rounds until convergence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and 2) reducing the size of the transmitted updates through compression and partial updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p"><span id="S2.SS1.p6.1.1" class="ltx_text ltx_font_bold">Data:</span>
In most cases, data distributions depend on the users’ behaviour <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. As a result, the local datasets are massively distributed, statistically heterogeneous (i.e., non-IID and unbalanced), and highly redundant. Additionally, the raw generated data is often privacy-sensitive as it can reveal personal and confidential information.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_italic">Small and widely distributed datasets:</span> In FEEL scenarios, a large number of devices participate in the FL training with a small average number of data samples per client. Learning from small datasets makes local models prone to overfitting.</p>
</div>
<div id="S2.SS1.p8" class="ltx_para">
<p id="S2.SS1.p8.1" class="ltx_p"><span id="S2.SS1.p8.1.1" class="ltx_text ltx_font_italic">Non-IID:</span> The training data on a given device is typically based on the usage of the device by a particular user, and hence any particular user’s local dataset will not be representative of the population distribution. This data-generation paradigm fails to comply with the independent and identically distributed (IID) assumptions in distributed optimization, and thus adds complexity to the problem formulation and convergence analysis. The empirical evaluation of FEEL algorithms on non-IID data is usually performed on artificial partitions of MNIST or CIFAR-10, which do not provide a realistic model of a federated scenario.</p>
</div>
<div id="S2.SS1.p9" class="ltx_para">
<p id="S2.SS1.p9.1" class="ltx_p"><span id="S2.SS1.p9.1.1" class="ltx_text ltx_font_italic">Unbalance:</span> Similarly to the nature of the distributions, the size of the generated data depends on the user. Depending on users’ use of the device, these may have varying amounts of local training data.</p>
</div>
<div id="S2.SS1.p10" class="ltx_para">
<p id="S2.SS1.p10.1" class="ltx_p"><span id="S2.SS1.p10.1.1" class="ltx_text ltx_font_italic">Redundancy:</span> The unbalance of the data is also observed within the local datasets at a single device. In fact, IoT data is highly redundant. In sequential data (e.g., video surveillance, sensors data) for instance, only a subset of the data is informative or useful for the training.</p>
</div>
<div id="S2.SS1.p11" class="ltx_para">
<p id="S2.SS1.p11.1" class="ltx_p"><span id="S2.SS1.p11.1.1" class="ltx_text ltx_font_italic">Privacy:</span>
The privacy-preserving aspect is an essential requirement in FL applications. The raw data generated on each device is protected by sharing only model updates instead of the raw data. However, communicating model updates throughout the training process can still be reverse-engineered to reveal sensitive information, either by a third-party or a malicious central server.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Design Trade-offs</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Several efforts were made to tackle the aforementioned challenges. However, FEEL is a multi-dimensional problem that brings about several trade-offs. As a result, algorithms designed to address one issue at a time are deemed unpractical.
Perhaps a tractable solution may be to combine several techniques when developing and deploying FEEL algorithms.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In general, an end-to-end FEEL solution should cover devices selection, resource allocation, and updates aggregation. In the following, we discuss major trade-offs that should be considered when designing solutions in the FEEL setting.
<br class="ltx_break">
<br class="ltx_break"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">1) General FEEL solution</span>

<br class="ltx_break">Given the wide range of applications that can benefit from FEEL, there is no one-size-fits-all solution. However, in general, a FEEL solution needs to act on the following aspects: 
<br class="ltx_break"></p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Device selection:</span> Participant selection refers to the selection of devices to receive and train the model in each training round. Ideally, a set of participants is randomly selected by the server to participate. Then, the server has to aggregate parameter updates from all participants in the round before taking a weighted average of the models. However, due to the communication bottlenecks and the desire to tame the training latency, the device selection should be optimized in terms of resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and data criteria.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Resource allocation:</span> Device selection should not be considered independently from resource allocation, especially computation and bandwidth. We refer to the joint selection and resource allocation as a scheduling algorithm. Indeed, the number of scheduled devices is limited by the available bandwidth that can be allocated. Additionally, for an optimal learning round duration and energy consumption, both bandwidth and computation resources should be adapted based on the number of local iterations at each device, and the number of global iterations (i.e., learning rounds) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Due to the fast-changing aspect of the FEEL environment, the computational complexity of scheduling algorithms should be especially low. Therefore, the use of meta-heuristics and heuristics should be encouraged.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Updates aggregation:</span> This aspect of the solution design refers to how the updates are aggregated and how frequently they are aggregated.
For instance, the frequency of the communication and aggregation can be reduced with more local computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, or reduced through selective communication of gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
For instance, FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is one of the most used methods in aggregation which uses weighted average Stochastic Gradient Descent updates, where the corresponding weights are decided by the volume of the training dataset.
While FedAvg uses synchronous aggregation, in FedAsync <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> algorithm, newly received local updates are weighted according to their staleness, where stale updates received from stragglers are weighted less based on how many rounds elapsed.
It should also be noted that proposing new aggregation methods requires theoretical and empirical convergence analysis to guarantee that the learning loss function will converge to a global optimum. Updates aggregation should also be communication-efficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and secure by the means of techniques such as differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. 
<br class="ltx_break">
<br class="ltx_break"><span id="S2.SS2.p5.1.2" class="ltx_text ltx_font_italic">2) Optimization axes</span>

<br class="ltx_break"></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2009.00081/assets/figures/fig1_v7.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="720" height="463" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>FEEL algorithms, challenges and optimization axes</figcaption>
</figure>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">In addressing FEEL challenges, three optimization axes are often considered: Time, Energy and Learning. In many cases, the FEEL algorithm can be viewed as a Pareto optimal problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The relation between the three axes and the challenges is illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ II-B Design Trade-offs ‣ II Design challenges : Overview ‣ Federated Edge Learning : Design Issues and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p"><span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold">Time optimization:</span> Accelerating the learning time can be evaluated with different lenses: learning round duration and time until learning convergence.
Due to the synchronous model aggregation of FEEL, the total duration of a round is determined by the slowest device among all the scheduled devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. For this reason, more bandwidth should be allocated for transmission by stragglers and less for faster devices. This to some extent can equalize their total update time (computing plus communication time). Furthermore, to avoid squandering bandwidth on extremely slow devices, scheduling (i.e., joint selection and resource allocation) should exclude slowest devices by applying thresholds on their expected completion time, which can be inferred using their computing capacities and channel states. From a learning perspective, the learning latency is determined by the number of rounds until convergence. The optimization techniques centered on this aspect mainly focus either on the selective upload of updates, or on maximizing the participating devices in each round.</p>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p"><span id="S2.SS2.p8.1.1" class="ltx_text ltx_font_bold">Energy optimization:</span> Optimizing the energy consumption across the network is necessary to reduce the rate of drop-out devices because of battery drainage. In fact, training and transmission of large-scale models are energy consuming, while most edge and end devices have limited battery lives. Additionally, using the maximum capacity of the devices would make the users less-likely willing to participate in the training.
A design goal of a scheduling algorithm (i.e., joint selection and resource allocation) would be to allocate bandwidth based on the devices’ channel states and battery levels. As a result, more bandwidth should be allocated to devices with weaker channels or poorer power states, to maximize the collected updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p"><span id="S2.SS2.p9.1.1" class="ltx_text ltx_font_bold">Learning optimization:</span>
In contrast to centralized learning, optimizing the learning in the FEEL setting cannot be seen independently from time and energy optimization.
However, capturing the optimization of time, energy and the learning goal in the same optimization problem is hard, because there is no direct relation between the objective function of the learning (i.e., the loss function) and the time and energy minimization goal. A manageable approach used is to minimize time and energy under a certain convergence speed guarantee.
For instance, some works argue that the number of collected updates in each round is inversely proportional to the convergence speed, and therefore is used as a guarantee <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Indeed, multi-user diversity (i.e., collecting a maximum of updates) can yield a high convergence speed, especially in IID environments, however there is a significant chance of choosing the same sets of devices repeatedly. To avoid this issue, a goal of the FEEL algorithm can be to maximize the fairness in terms of the number of collected updates among devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The fairness measure maximizes the chance of more diverse data sources, thus achieving gradient diversity.
Nonetheless, the number of collected updates in this setting might be low. The fairness is also considered in the aggregation by q-Fair FL (q-FFL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, which reweighs the objective function in FedAvg to assign higher weights in the loss function to devices with higher loss.
Another approach is to use data size priority, which maximizes the size of data used in the training, by using a probability of selection inversely proportional to the available dataset’s size.
In the background, these scheduling algorithms all share the same idea : if the size of the training data is large then the training would converge faster. However, IoT data is highly redundant and inherently unbalanced. Thus, many of the proposed algorithms witness a drop in performance in non-IID and unbalanced experiments. Therefore, the data properties should be considered throughout the FEEL algorithm.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Data-aware FEEL design: Future Direction</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Even if FL was first proposed with data as a central aspect, it has been overlooked in the design of proposed FEEL scheduling algorithms. With the significant drop of accuracy of models trained with resource-aware FEEL algorithms in non-IID and unbalanced settings, it becomes clear that the data aspect should be considered.
Henceforth, we propose a new possible data-aware end-to-end FEEL solution based on the diversity properties of the different datasets. In general, diversity consists of two aspects, namely, richness and uncertainty. Richness quantifies the size of the data, while the uncertainty quantifies the information contained in the data. In fact, it has been long proven in Active Learning that by choosing highly uncertain data samples, a model can be trained using fewer labelled data samples. This fact suggests that data uncertainty should be incorporated into the design of FL scheduling algorithms. Nonetheless, the uncertainty measures used in Active Learning targets individual samples from unlabeled data in a centralized setting, thus, these measures cannot be directly integrated in FEEL.
In the FEEL setting, the updates’ scheduling can be either before the training or after it, therefore the diversity measures should be selected depending on the time of scheduling. If the scheduling before the training is preferred, then the datasets’ diversity is to be considered. Otherwise, if the scheduling is set after the training is over, the diversity to be considered is model diversity, as the diversity of the dataset can be reflected by the resulting model. In both cases, in addition to maximizing the diversity through careful selection of participating devices, the scheduling algorithm can focus on minimizing the consumed resources in terms of completion time of FL and transmission energy of participating devices. For the pre-training scheduling, local computation energy can also be optimized. Furthermore, the scheduling problems’ constraints are to be derived from the environment’s properties concerning resources and data. 
<br class="ltx_break">In this section, and to better illustrate the data-aware solutions, we consider the architecture illustrated in Figure <a href="#S3.F2" title="Figure 2 ‣ III Data-aware FEEL design: Future Direction ‣ Federated Edge Learning : Design Issues and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The architecture is a cellular network composed of one base station (BS) equipped with a parameter server, and <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\it{N}" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\it{N}</annotation></semantics></math> devices that collaboratively train a shared model.
In the following, we discuss different constraints related to the scheduling algorithms in this setting. Then, we present pre-training and post-training algorithms guidelines, where we detail the key criteria for the design of data-aware FEEL solutions, and we present some potential measures and methods to enable a variety of data-aware FEEL applications, which are summarized in Figure <a href="#S3.F3" title="Figure 3 ‣ III-B Pre-training scheduling: Dataset Diversity ‣ III Data-aware FEEL design: Future Direction ‣ Federated Edge Learning : Design Issues and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2009.00081/assets/figures/sysmodel_colorized.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="763" height="456" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The proposed FEEL system model</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Scheduling Constraints</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The scheduling algorithms’ must consider the following constraints that arise from the FEEL environment’s properties:
<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">Energy consumption:</span>
Due to the limited energy level and the high computational requirements of training algorithms, it is necessary to evaluate a device’s battery level before scheduling it for a training round.
When first FL was proposed, the selected devices were limited to the ones plugged for charging. However, this criterion limits the number of devices that can be selected, leading to a slow convergence of the learning.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">Radio Channel State:</span>
It is important to consider the radio channel state changes in the scheduling. The quality of the communication is critical for both the device selection and resource allocation.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">Expected completion time:</span>
The available computation resources, alongside data size, can be used to estimate the completion time of the device. Potential stragglers can be discarded even before the training process.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_italic">Number of participants:</span> A communication round cannot be considered valid unless a minimum number of updates is obtained. Therefore, a training round can be dropped if there are not enough devices to schedule.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_italic">Data size:</span> The available data in the device is smaller in size than a required minimum, it can be immediately discarded from the selection process. For instance, if the number of samples is less than the selected mini-batch size, the device should be excluded.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Pre-training scheduling: Dataset Diversity</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The pre-training scheduling that we propose uses dataset diversity to choose devices that will conduct the training and send the updates. Scheduling the devices before the training allows to eliminate potential stragglers, and adapt the number of epochs based on the battery levels available at the participating devices.
<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">1) Scheduling algorithm:</span>

<br class="ltx_break">In this algorithm, the global model is initialized by the BS. Afterwards, the following steps are repeated until the model converges or a maximum of rounds is attained:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Step 1:</span> At the beginning of each training round, the devices send their diversity indicators and battery levels to the server.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Step 2:</span> Based on the received information, alongside with the evaluated channel state indicator, the server schedules a subset of devices and sends them the current global model.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Step 3:</span> Each device in the subset uses its local data to train the model.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Step 4:</span> The updated models are sent to the server to be aggregated.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Step 5:</span> The PS aggregates the updates and created the new model.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">2) Datasets Diversity Measures:</span>

<br class="ltx_break">In the pre-training scheduling, dataset diversity will serve essentially as a lead for device selection, where it should prioritize devices that have potentially informative datasets with less redundancy, to speed up the learning process.
While the richness of datasets can be easily quantified through the total number of samples, the uncertainty of the dataset depends strongly on the application.
For supervised learning, the uncertainty can be evaluated through the evenness of the dataset (i.e., the degree of balance between the classes in classification problems), which can be calculated through entropy measures. For sequence data, the uncertainty is reflected by the regularity of the series. Moreover, for unsupervised learning, local dissimilarity between pseudo-classes or randomly sampled data points can be considered.
Furthermore, it is essential to consider the privacy as a component of the used index. Sending the number of samples from each class for instance is a violation of the privacy principle of FEEL. In the following, we introduce some potential methods to evaluate datasets diversity.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p"><span id="S3.SS2.p3.2.1" class="ltx_text ltx_font_bold">Diversity measures for classification:</span>
The measures of diversity have long been used in Active learning. In fact, uncertainty is used to choose the samples that should be labeled as this task is costly. However, in FL, the client selection does not concern independent samples, instead the diversity should be evaluated at the level of the entire dataset.
Moreover, in the premise of supervised FL, the labels are already known, which gives the possibility to use more informed measures. For instance, Shannon Entropy or Gini-Simpson index are suitable measures for datasets’ uncertainty in classification problems.
Shannon Entropy and Gini-Simpson index both favor IID partitions, where the maximum for both indexes is obtained for balanced distributions and the datasets with a single class has the minimum possible value.
The Shannon entropy quantifies the uncertainty (entropy or degree of surprise) of a prediction. It was first proposed to quantify the information content in strings of text. The underlying idea is that when a text contains more different letters, with almost equal proportional abundances, it will be more difficult to correctly predict which letter will be the next one in the string. However, Shannon Entropy is not defined for the case of classes with no representative samples. Therefore, it may not practical in scenarios with high unbalance.
Another possible measure is the Gini-Simpson index. The Simpson index <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\lambda</annotation></semantics></math> measures the probability that two samples taken at random from the dataset of interest are from the same class. The Gini–Simpson index is its transformation <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="1-\lambda" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mn id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">1</mn><mo id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">−</mo><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">λ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><minus id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"></minus><cn type="integer" id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">1</cn><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">𝜆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">1-\lambda</annotation></semantics></math>, which represents the probability that the two samples belong to different classes. Nonetheless, if the number of classes is large, the distinction using this index will be hard.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Diversity measures for time-series forecasting:</span>
In time series problems, other methods can be used, such as Approximate Entropy (ApEn) and Sample Entropy (SampEn). In sequential data, statistical measures such as the mean and the variance are not enough to illustrate the regularity, as they are influenced by system noise. ApEn is proposed to quantify the amount of regularity and the unpredictability of time-series data. It is based on the comparison between values of data in successive vectors, by quantifying how many data points vary more than a defined threshold. SampEn was proposed as a modification of ApEn. It is used for assessing the complexity of time-series data, with the advantage of being independent from the length of the vectors.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Diversity measures for clustering tasks:</span>
For clustering tasks, a similarity measure between data points from a randomly sampled subset should be considered. The measure can be distance based (e.g., Euclidean distance, Heat Kernel) or angular based (e.g., cosine similarity). A higher value is obtained if most of the data points in the sample are dissimilar, and thus the dataset should be considered as more diverse. It should be noted that angular based measures are invariant to scale, translation, rotation, and orientation, which makes them suitable for a wide range of applications, particularly multivariate datasets. 
<br class="ltx_break"></p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2009.00081/assets/x1.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="451" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Diversity measures that can be used in pre-training and post-training scheduling</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Post-training scheduling: Model Diversity</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The post-training setting uses model diversity to choose devices that will send the updates. The model diversity is evaluated on two different aspects: 1) by comparing the dissimilarity between the local model’s parameters and the previous global model’s parameters. 2) by comparing the diversity within the model’s parameters.
In fact, choosing the local models that are divergent from the previous global model will possibly improve the representational ability of the global model directly, by aggregating updates that have potentially new information.
Furthermore, if a dataset is highly unbalanced and limited in size, the model’s parameters would be very similar. The redundancy within parameters negatively affects the model’s representational ability. It is therefore necessary to prioritize updates with high diversity.
In the following, we detail the post-training scheduling algorithm, then we present some possible measures for model diversity. 
<br class="ltx_break">
<br class="ltx_break"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">1) Scheduling algorithm:</span>

<br class="ltx_break">Similarly to pre-training scheduling, the global model is initialized by the BS. Afterwards, the following steps are repeated until the model converges or a maximum of rounds is attained:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Step 1:</span> At the beginning of each training round, devices receive the current model.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Step 2:</span> Each device in the subset uses its local data to train the model.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Step 3:</span> The server sends an update request to the devices, to which each device responds by sending its model diversity index.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Step 4:</span> Based on the received information, alongside with the evaluated channel state indicator, the server schedules a subset of devices to upload their models. Then, the updated models are sent to the server to be aggregated.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p"><span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Step 5:</span> The PS aggregates the updates and created the new global model.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_italic">2) Model Diversity Measures:</span>

<br class="ltx_break">While the richness aspect of the diversity is irrelevant in models diversity due to fixed model size among devices, the information contained in the models can be quantified through how the local model’s vary compared to the global model, and how the parameters within the same model repulse from each other. Some possible measures are as follows:</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Local and global models’ dissimilarity:</span>
Choosing the local models that are divergent from the previous global model will possible improve the representational ability of the global model directly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Pairwise similarity measures such as cosine similarity and Euclidean distance can be used to evaluate the similarity of the new local parameters and the global parameters. Moreover, Divergence, a Bayesian method used to measure the difference between different data distributions, can be used to evaluate diversity of the learned model compared to the global model.
Nonetheless, relying on model’s dissimilarity might lead to collecting updates from outliers. It is thereby necessary to regulate these diversity measures through the use of thresholds in particular.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.3" class="ltx_p"><span id="S3.SS3.p4.3.1" class="ltx_text ltx_font_bold">Parameters Dissimilarity:</span>
To evaluate the redundancy within the model’s parameters, the same similarity measures used for clustering can also be applied to the parameters.
Additionally, the <math id="S3.SS3.p4.1.m1.2" class="ltx_Math" alttext="L_{2,1}" display="inline"><semantics id="S3.SS3.p4.1.m1.2a"><msub id="S3.SS3.p4.1.m1.2.3" xref="S3.SS3.p4.1.m1.2.3.cmml"><mi id="S3.SS3.p4.1.m1.2.3.2" xref="S3.SS3.p4.1.m1.2.3.2.cmml">L</mi><mrow id="S3.SS3.p4.1.m1.2.2.2.4" xref="S3.SS3.p4.1.m1.2.2.2.3.cmml"><mn id="S3.SS3.p4.1.m1.1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.1.cmml">2</mn><mo id="S3.SS3.p4.1.m1.2.2.2.4.1" xref="S3.SS3.p4.1.m1.2.2.2.3.cmml">,</mo><mn id="S3.SS3.p4.1.m1.2.2.2.2" xref="S3.SS3.p4.1.m1.2.2.2.2.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.2b"><apply id="S3.SS3.p4.1.m1.2.3.cmml" xref="S3.SS3.p4.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.2.3.1.cmml" xref="S3.SS3.p4.1.m1.2.3">subscript</csymbol><ci id="S3.SS3.p4.1.m1.2.3.2.cmml" xref="S3.SS3.p4.1.m1.2.3.2">𝐿</ci><list id="S3.SS3.p4.1.m1.2.2.2.3.cmml" xref="S3.SS3.p4.1.m1.2.2.2.4"><cn type="integer" id="S3.SS3.p4.1.m1.1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1.1">2</cn><cn type="integer" id="S3.SS3.p4.1.m1.2.2.2.2.cmml" xref="S3.SS3.p4.1.m1.2.2.2.2">1</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.2c">L_{2,1}</annotation></semantics></math> norm can be used to obtain a group-wise sparse representation of the dissimilarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The internal <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="L_{1}" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><msub id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">L</mi><mn id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">𝐿</ci><cn type="integer" id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">L_{1}</annotation></semantics></math> norm encourages different parameters to be sparse, while the external <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><msub id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">L</mi><mn id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">𝐿</ci><cn type="integer" id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">L_{2}</annotation></semantics></math> norm is used to control the complexity of entire model.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Federated Learning is a promising machine learning technique by virtue of its privacy-preserving aspect and ability to handle unbalanced and non-IID data. However, deploying federated learning based solutions at the edge of the network is subject to several challenges. In fact,
FEEL is a multi-disciplinary problem that requires optimization over both the resources and the data. Nonetheless, the data properties are overlooked in many parts of the proposed algorithms, despite being the essence of federated learning.
Several FEEL design challenges and issues are introduced and discussed in terms of trade-offs. Furthermore, a new research direction is presented in an effort to incorporate the datasets’ diversity properties into the design of FEEL algorithms. Our proposed method supposes that the data quality and veracity are guaranteed, which requires leveraging other techniques such as the blockchain as a trusted third-party for data verification.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">ACKNOWLEDGEMENT</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank the Natural Sciences and Engineering Research Council of Canada, for the financial support of this research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2019]</span>
<span class="ltx_bibblock">
G. Zhu, Y. Wang, and K. Huang, “Broadband Analog Aggregation for
Low-Latency Federated Edge Learning (Extended Version),”
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv:1812.11494 [cs, math]</em>, Jan. 2019. [Online]. Available:
http://arxiv.org/abs/1812.11494

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Filali et al. [2020]</span>
<span class="ltx_bibblock">
A. Filali <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Multi-Access Edge Computing: A Survey,”
<em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 197 017–197 046, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abouaomar et al. [2021]</span>
<span class="ltx_bibblock">
A. Abouaomar <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Resource Provisioning in Edge Computing for
Latency-Sensitive Applications,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">IEEE Internet of Things
Journal</em>, vol. 8, no. 14, pp. 11 088–11 099, Jul. 2021, conference Name:
IEEE Internet of Things Journal.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al. [2017]</span>
<span class="ltx_bibblock">
J. Konečný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh, and
D. Bacon, “Federated Learning: Strategies for Improving
Communication Efficiency,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv:1610.05492 [cs]</em>, Oct. 2017.
[Online]. Available: http://arxiv.org/abs/1610.05492

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2019]</span>
<span class="ltx_bibblock">
K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farhad, S. Jin, T. Q. S. Quek,
and H. V. Poor, “Federated Learning with Differential Privacy:
Algorithms and Performance Analysis,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv:1911.00222 [cs]</em>,
Nov. 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2019]</span>
<span class="ltx_bibblock">
Q. Zeng, Y. Du, K. K. Leung, and K. Huang, “Energy-Efficient Radio
Resource Allocation for Federated Edge Learning,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv:1907.06040 [cs, math]</em>, Jul. 2019. [Online]. Available:
http://arxiv.org/abs/1907.06040

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio and Yonetani [2019]</span>
<span class="ltx_bibblock">
T. Nishio and R. Yonetani, “Client Selection for Federated Learning with
Heterogeneous Resources in Mobile Edge,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ICC 2019</em>, pp. 1–7,
May 2019. [Online]. Available: http://arxiv.org/abs/1804.08333

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abouaomar et al. [2019]</span>
<span class="ltx_bibblock">
A. Abouaomar <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A resources representation for resource
allocation in fog computing networks,” in <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">2019 IEEE Global
Communications Conference (GLOBECOM)</em>, 2019, pp. 1–6.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019a]</span>
<span class="ltx_bibblock">
L. Wang, W. Wang, and B. Li, “CMFL: Mitigating
Communication Overhead for Federated Learning,” in
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE ICDCS 2019</em>, Dallas, TX, USA, Jul.
2019, pp. 954–964. [Online]. Available:
https://ieeexplore.ieee.org/document/8885054/

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taïk and Cherkaoui [2020]</span>
<span class="ltx_bibblock">
A. Taïk and S. Cherkaoui, “Electrical Load Forecasting Using Edge
Computing and Federated Learning,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICC 2020 - 2020 IEEE
International Conference on Communications (ICC)</em>, Jun. 2020, pp.
1–6.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019b]</span>
<span class="ltx_bibblock">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan,
“Adaptive Federated Learning in Resource Constrained Edge
Computing Systems,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in
Communications</em>, vol. 37, no. 6, pp. 1205–1221, Jun. 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2018]</span>
<span class="ltx_bibblock">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated
Learning with Non-IID Data,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv:1806.00582 [cs, stat]</em>,
Jun. 2018. [Online]. Available: http://arxiv.org/abs/1806.00582

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2019]</span>
<span class="ltx_bibblock">
C. Xie, S. Koyejo, and I. Gupta, “Asynchronous Federated Optimization,”
<em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv:1903.03934 [cs]</em>, Sep. 2019. [Online]. Available:
http://arxiv.org/abs/1903.03934

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020]</span>
<span class="ltx_bibblock">
J. Wang, C. Jiang, H. Zhang, Y. Ren, K.-C. Chen, and L. Hanzo, “Thirty Years
of Machine Learning: The Road to Pareto-Optimal Wireless
Networks,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys Tutorials</em>, pp. 1–1, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2019]</span>
<span class="ltx_bibblock">
W. Shi, S. Zhou, and Z. Niu, “Device Scheduling with Fast Convergence
for Wireless Federated Learning,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv:1911.00856 [cs, math]</em>,
Nov. 2019. [Online]. Available: http://arxiv.org/abs/1911.00856

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2020]</span>
<span class="ltx_bibblock">
H. H. Yang, A. Arafa, T. Q. S. Quek, and H. Vincent Poor, “Age-Based
Scheduling Policy for Federated Learning in Mobile Edge
Networks,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020</em>, May 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020]</span>
<span class="ltx_bibblock">
T. Li, M. Sanjabi, A. Beirami, and V. Smith, “Fair Resource Allocation in
Federated Learning,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv:1905.10497 [cs, stat]</em>, Feb. 2020.
[Online]. Available: http://arxiv.org/abs/1905.10497

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al. [2019]</span>
<span class="ltx_bibblock">
Z. Gong, P. Zhong, and W. Hu, “Diversity in Machine Learning,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">IEEE
Access</em>, vol. 7, pp. 64 323–64 350, 2019, conference Name: IEEE Access.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2009.00080" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2009.00081" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2009.00081">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2009.00081" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2009.00082" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 11:40:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
