<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.02160] Bias Propagation in Federated Learning</title><meta property="og:description" content="We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Bias Propagation in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Bias Propagation in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.02160">

<!--Generated on Wed Feb 28 07:41:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Bias Propagation in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hongyan Chang &amp; Reza Shokri 
<br class="ltx_break">School of Computing
<br class="ltx_break">National University of Singapore
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{hongyan,reza}@comp.nus.edu.sg</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p"><span id="id2.id1.1" class="ltx_text">We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Machine learning models can exhibit bias against demographic groups. Previous research has extensively studied how machine learning algorithms can reflect and amplify bias in training data, especially in centralized settings where data is held by a single party <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>; Dwork et al., <a href="#bib.bib13" title="" class="ltx_ref">2012</a>; Calders et al., <a href="#bib.bib4" title="" class="ltx_ref">2009</a>; Hashimoto et al., <a href="#bib.bib19" title="" class="ltx_ref">2018</a>; Zhang et al., <a href="#bib.bib53" title="" class="ltx_ref">2020</a>; Blum and Stangl, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Lakkaraju et al., <a href="#bib.bib26" title="" class="ltx_ref">2017</a>)</cite>. However, in practice, data is commonly owned by multiple parties and cannot be shared due to privacy concerns. Federated learning (FL) provides a promising solution by enabling parties to collaboratively learn a global model without sharing their data. In each round of FL, parties share their local model updates computed on their private datasets with a global server that aggregates them to update the global model. Despite the widespread adoption of FL in various applications such as healthcare, recruitment, and loan evaluation <cite class="ltx_cite ltx_citemacro_citep">(Rieke et al., <a href="#bib.bib41" title="" class="ltx_ref">2020</a>; Yang et al., <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite>, it is not yet fully understood how FL algorithms could magnify bias in training datasets.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">Recent studies have investigated the problem of measuring and mitigating bias in federated learning with respect to a <span id="S1.p2.1.1" class="ltx_text">single</span> global distribution <cite class="ltx_cite ltx_citemacro_citep">(Chu et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Zeng et al., <a href="#bib.bib51" title="" class="ltx_ref">2021a</a>; Hu et al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Du et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Abay et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>; Papadaki et al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>; <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Hu et al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. However, in practice, parties often have <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">heterogeneous</span> data distributions. Evaluating the model’s bias with respect to the global distribution does not accurately reflect the fairness of the FL model with respect to parties’ local data distributions, which are relevant to end-users. This is the critical problem that we address in this paper. Specifically, we investigate the following questions: How does participating in FL affect the bias and fairness of the resulting models compared to models which are trained in a standalone setting? Does FL provide parties with the potential fairness benefits of centralized training on the union of their data? Can parties with biased datasets negatively impact the experienced fairness of other parties on their local distributions? How and why does the bias of a small number of parties affect the entire network? To the best of our knowledge, we provide the first comprehensive analysis of how FL algorithms impact local fairness.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">We provide an empirical analysis based on real-world datasets. We show that FL might not sustain the benefits of collaboration in terms of fairness, as compared to its accuracy benefit. Specifically, compared with the standalone models, we find that the model trained in a centralized setting can be, on average, fairer on local data distributions. However, in those cases, the FL models, trained on the same dataset, can be more biased. This suggests that the FL algorithm itself can introduce new bias in the final model. Furthermore, we demonstrate that FL impacts different parties in different ways. Specifically, we find a strong correlation between parties’ fairness gap in the standalone setting and the fairness benefit they obtain from joining FL: parties with a greater bias in the standalone setting (caused by their local data) would receive a fairer model from FL. In contrast, FL has negative impacts on parties with a less significant bias in the standalone setting, resulting in a more biased model in FL. We further demonstrate that this is due to the fact that <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">FL propagates bias</span>: bias from a few parties can influence all parties in the network, hence aggravating the fairness problem globally.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.5" class="ltx_p">Finally, we offer potential explanations for how bias is propagated in FL. Specifically, we show that <span id="S1.p4.5.1" class="ltx_text ltx_font_italic">local updates</span> from biased parties increase the dependency between the model’s predictions and the sensitive attributes. Such an increase is achieved by the norm increase in a small number of parameters (around <math id="S1.p4.1.m1.1" class="ltx_Math" alttext="6\%" display="inline"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">6</mn><mo id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><csymbol cd="latexml" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">6\%</annotation></semantics></math> of the model parameters in some experiments). This increase then propagates to the global model through aggregation, subsequently impacting all other parties. In addition, we show that the fairness gap of the final model can be governed by adjusting the value of those parameters. Surprisingly, we find that scaling these parameters can either reduce the model’s bias to a small value of <math id="S1.p4.2.m2.1" class="ltx_Math" alttext="0.05" display="inline"><semantics id="S1.p4.2.m2.1a"><mn id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><cn type="float" id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">0.05</annotation></semantics></math> (on a measurement scale of 0 to 1) with only a <math id="S1.p4.3.m3.1" class="ltx_Math" alttext="0.6\%" display="inline"><semantics id="S1.p4.3.m3.1a"><mrow id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml"><mn id="S1.p4.3.m3.1.1.2" xref="S1.p4.3.m3.1.1.2.cmml">0.6</mn><mo id="S1.p4.3.m3.1.1.1" xref="S1.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><apply id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1"><csymbol cd="latexml" id="S1.p4.3.m3.1.1.1.cmml" xref="S1.p4.3.m3.1.1.1">percent</csymbol><cn type="float" id="S1.p4.3.m3.1.1.2.cmml" xref="S1.p4.3.m3.1.1.2">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">0.6\%</annotation></semantics></math> drop in accuracy or increase the bias to a large value of <math id="S1.p4.4.m4.1" class="ltx_Math" alttext="0.96" display="inline"><semantics id="S1.p4.4.m4.1a"><mn id="S1.p4.4.m4.1.1" xref="S1.p4.4.m4.1.1.cmml">0.96</mn><annotation-xml encoding="MathML-Content" id="S1.p4.4.m4.1b"><cn type="float" id="S1.p4.4.m4.1.1.cmml" xref="S1.p4.4.m4.1.1">0.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.4.m4.1c">0.96</annotation></semantics></math> with an <math id="S1.p4.5.m5.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S1.p4.5.m5.1a"><mrow id="S1.p4.5.m5.1.1" xref="S1.p4.5.m5.1.1.cmml"><mn id="S1.p4.5.m5.1.1.2" xref="S1.p4.5.m5.1.1.2.cmml">11</mn><mo id="S1.p4.5.m5.1.1.1" xref="S1.p4.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.5.m5.1b"><apply id="S1.p4.5.m5.1.1.cmml" xref="S1.p4.5.m5.1.1"><csymbol cd="latexml" id="S1.p4.5.m5.1.1.1.cmml" xref="S1.p4.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S1.p4.5.m5.1.1.2.cmml" xref="S1.p4.5.m5.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.5.m5.1c">11\%</annotation></semantics></math> drop in accuracy.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.9" class="ltx_p"><span id="S2.p1.9.1" class="ltx_text ltx_font_bold">Federated Learning.</span>
In this paper, we consider the conventional federated learning (FL) setting. <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib33" title="" class="ltx_ref">2017</a>)</cite>. The FL framework consists of a network of <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">K</annotation></semantics></math> parties, where each party <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="k\in[K]" display="inline"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.2" xref="S2.p1.2.m2.1.2.cmml"><mi id="S2.p1.2.m2.1.2.2" xref="S2.p1.2.m2.1.2.2.cmml">k</mi><mo id="S2.p1.2.m2.1.2.1" xref="S2.p1.2.m2.1.2.1.cmml">∈</mo><mrow id="S2.p1.2.m2.1.2.3.2" xref="S2.p1.2.m2.1.2.3.1.cmml"><mo stretchy="false" id="S2.p1.2.m2.1.2.3.2.1" xref="S2.p1.2.m2.1.2.3.1.1.cmml">[</mo><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">K</mi><mo stretchy="false" id="S2.p1.2.m2.1.2.3.2.2" xref="S2.p1.2.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.2.cmml" xref="S2.p1.2.m2.1.2"><in id="S2.p1.2.m2.1.2.1.cmml" xref="S2.p1.2.m2.1.2.1"></in><ci id="S2.p1.2.m2.1.2.2.cmml" xref="S2.p1.2.m2.1.2.2">𝑘</ci><apply id="S2.p1.2.m2.1.2.3.1.cmml" xref="S2.p1.2.m2.1.2.3.2"><csymbol cd="latexml" id="S2.p1.2.m2.1.2.3.1.1.cmml" xref="S2.p1.2.m2.1.2.3.2.1">delimited-[]</csymbol><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">k\in[K]</annotation></semantics></math> holds a local dataset <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\tilde{D}_{k}" display="inline"><semantics id="S2.p1.3.m3.1a"><msub id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mover accent="true" id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml"><mi id="S2.p1.3.m3.1.1.2.2" xref="S2.p1.3.m3.1.1.2.2.cmml">D</mi><mo id="S2.p1.3.m3.1.1.2.1" xref="S2.p1.3.m3.1.1.2.1.cmml">~</mo></mover><mi id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1">subscript</csymbol><apply id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2"><ci id="S2.p1.3.m3.1.1.2.1.cmml" xref="S2.p1.3.m3.1.1.2.1">~</ci><ci id="S2.p1.3.m3.1.1.2.2.cmml" xref="S2.p1.3.m3.1.1.2.2">𝐷</ci></apply><ci id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\tilde{D}_{k}</annotation></semantics></math> of size <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S2.p1.4.m4.1a"><msub id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">n</mi><mi id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">𝑛</ci><ci id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">n_{k}</annotation></semantics></math>, sampled from a local data distribution <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S2.p1.5.m5.1a"><msub id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml">𝒟</mi><mi id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">𝒟</ci><ci id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\mathcal{D}_{k}</annotation></semantics></math>. The objective of each party is to train a model that minimizes the loss on their local data distribution <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S2.p1.6.m6.1a"><msub id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">𝒟</mi><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p1.6.m6.1.1.1.cmml" xref="S2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2">𝒟</ci><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">\mathcal{D}_{k}</annotation></semantics></math>. To achieve this goal, FL trains a global model to minimize the average loss across parties, which is expressed as <math id="S2.p1.7.m7.2" class="ltx_Math" alttext="\min_{\theta}\frac{1}{N}\sum_{k=1}^{K}n_{k}L(\theta,\tilde{D}_{k})" display="inline"><semantics id="S2.p1.7.m7.2a"><mrow id="S2.p1.7.m7.2.2" xref="S2.p1.7.m7.2.2.cmml"><mrow id="S2.p1.7.m7.2.2.3" xref="S2.p1.7.m7.2.2.3.cmml"><msub id="S2.p1.7.m7.2.2.3.1" xref="S2.p1.7.m7.2.2.3.1.cmml"><mi id="S2.p1.7.m7.2.2.3.1.2" xref="S2.p1.7.m7.2.2.3.1.2.cmml">min</mi><mi id="S2.p1.7.m7.2.2.3.1.3" xref="S2.p1.7.m7.2.2.3.1.3.cmml">θ</mi></msub><mo lspace="0.167em" id="S2.p1.7.m7.2.2.3a" xref="S2.p1.7.m7.2.2.3.cmml">⁡</mo><mfrac id="S2.p1.7.m7.2.2.3.2" xref="S2.p1.7.m7.2.2.3.2.cmml"><mn id="S2.p1.7.m7.2.2.3.2.2" xref="S2.p1.7.m7.2.2.3.2.2.cmml">1</mn><mi id="S2.p1.7.m7.2.2.3.2.3" xref="S2.p1.7.m7.2.2.3.2.3.cmml">N</mi></mfrac></mrow><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.2.2.2" xref="S2.p1.7.m7.2.2.2.cmml">​</mo><mrow id="S2.p1.7.m7.2.2.1" xref="S2.p1.7.m7.2.2.1.cmml"><msubsup id="S2.p1.7.m7.2.2.1.2" xref="S2.p1.7.m7.2.2.1.2.cmml"><mo id="S2.p1.7.m7.2.2.1.2.2.2" xref="S2.p1.7.m7.2.2.1.2.2.2.cmml">∑</mo><mrow id="S2.p1.7.m7.2.2.1.2.2.3" xref="S2.p1.7.m7.2.2.1.2.2.3.cmml"><mi id="S2.p1.7.m7.2.2.1.2.2.3.2" xref="S2.p1.7.m7.2.2.1.2.2.3.2.cmml">k</mi><mo id="S2.p1.7.m7.2.2.1.2.2.3.1" xref="S2.p1.7.m7.2.2.1.2.2.3.1.cmml">=</mo><mn id="S2.p1.7.m7.2.2.1.2.2.3.3" xref="S2.p1.7.m7.2.2.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.p1.7.m7.2.2.1.2.3" xref="S2.p1.7.m7.2.2.1.2.3.cmml">K</mi></msubsup><mrow id="S2.p1.7.m7.2.2.1.1" xref="S2.p1.7.m7.2.2.1.1.cmml"><msub id="S2.p1.7.m7.2.2.1.1.3" xref="S2.p1.7.m7.2.2.1.1.3.cmml"><mi id="S2.p1.7.m7.2.2.1.1.3.2" xref="S2.p1.7.m7.2.2.1.1.3.2.cmml">n</mi><mi id="S2.p1.7.m7.2.2.1.1.3.3" xref="S2.p1.7.m7.2.2.1.1.3.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.2.2.1.1.2" xref="S2.p1.7.m7.2.2.1.1.2.cmml">​</mo><mi id="S2.p1.7.m7.2.2.1.1.4" xref="S2.p1.7.m7.2.2.1.1.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.2.2.1.1.2a" xref="S2.p1.7.m7.2.2.1.1.2.cmml">​</mo><mrow id="S2.p1.7.m7.2.2.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S2.p1.7.m7.2.2.1.1.1.1.2" xref="S2.p1.7.m7.2.2.1.1.1.2.cmml">(</mo><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">θ</mi><mo id="S2.p1.7.m7.2.2.1.1.1.1.3" xref="S2.p1.7.m7.2.2.1.1.1.2.cmml">,</mo><msub id="S2.p1.7.m7.2.2.1.1.1.1.1" xref="S2.p1.7.m7.2.2.1.1.1.1.1.cmml"><mover accent="true" id="S2.p1.7.m7.2.2.1.1.1.1.1.2" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2.cmml"><mi id="S2.p1.7.m7.2.2.1.1.1.1.1.2.2" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2.2.cmml">D</mi><mo id="S2.p1.7.m7.2.2.1.1.1.1.1.2.1" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2.1.cmml">~</mo></mover><mi id="S2.p1.7.m7.2.2.1.1.1.1.1.3" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3.cmml">k</mi></msub><mo stretchy="false" id="S2.p1.7.m7.2.2.1.1.1.1.4" xref="S2.p1.7.m7.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.2b"><apply id="S2.p1.7.m7.2.2.cmml" xref="S2.p1.7.m7.2.2"><times id="S2.p1.7.m7.2.2.2.cmml" xref="S2.p1.7.m7.2.2.2"></times><apply id="S2.p1.7.m7.2.2.3.cmml" xref="S2.p1.7.m7.2.2.3"><apply id="S2.p1.7.m7.2.2.3.1.cmml" xref="S2.p1.7.m7.2.2.3.1"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.3.1.1.cmml" xref="S2.p1.7.m7.2.2.3.1">subscript</csymbol><min id="S2.p1.7.m7.2.2.3.1.2.cmml" xref="S2.p1.7.m7.2.2.3.1.2"></min><ci id="S2.p1.7.m7.2.2.3.1.3.cmml" xref="S2.p1.7.m7.2.2.3.1.3">𝜃</ci></apply><apply id="S2.p1.7.m7.2.2.3.2.cmml" xref="S2.p1.7.m7.2.2.3.2"><divide id="S2.p1.7.m7.2.2.3.2.1.cmml" xref="S2.p1.7.m7.2.2.3.2"></divide><cn type="integer" id="S2.p1.7.m7.2.2.3.2.2.cmml" xref="S2.p1.7.m7.2.2.3.2.2">1</cn><ci id="S2.p1.7.m7.2.2.3.2.3.cmml" xref="S2.p1.7.m7.2.2.3.2.3">𝑁</ci></apply></apply><apply id="S2.p1.7.m7.2.2.1.cmml" xref="S2.p1.7.m7.2.2.1"><apply id="S2.p1.7.m7.2.2.1.2.cmml" xref="S2.p1.7.m7.2.2.1.2"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.2.1.cmml" xref="S2.p1.7.m7.2.2.1.2">superscript</csymbol><apply id="S2.p1.7.m7.2.2.1.2.2.cmml" xref="S2.p1.7.m7.2.2.1.2"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.2.2.1.cmml" xref="S2.p1.7.m7.2.2.1.2">subscript</csymbol><sum id="S2.p1.7.m7.2.2.1.2.2.2.cmml" xref="S2.p1.7.m7.2.2.1.2.2.2"></sum><apply id="S2.p1.7.m7.2.2.1.2.2.3.cmml" xref="S2.p1.7.m7.2.2.1.2.2.3"><eq id="S2.p1.7.m7.2.2.1.2.2.3.1.cmml" xref="S2.p1.7.m7.2.2.1.2.2.3.1"></eq><ci id="S2.p1.7.m7.2.2.1.2.2.3.2.cmml" xref="S2.p1.7.m7.2.2.1.2.2.3.2">𝑘</ci><cn type="integer" id="S2.p1.7.m7.2.2.1.2.2.3.3.cmml" xref="S2.p1.7.m7.2.2.1.2.2.3.3">1</cn></apply></apply><ci id="S2.p1.7.m7.2.2.1.2.3.cmml" xref="S2.p1.7.m7.2.2.1.2.3">𝐾</ci></apply><apply id="S2.p1.7.m7.2.2.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1"><times id="S2.p1.7.m7.2.2.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.2"></times><apply id="S2.p1.7.m7.2.2.1.1.3.cmml" xref="S2.p1.7.m7.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.1.3.1.cmml" xref="S2.p1.7.m7.2.2.1.1.3">subscript</csymbol><ci id="S2.p1.7.m7.2.2.1.1.3.2.cmml" xref="S2.p1.7.m7.2.2.1.1.3.2">𝑛</ci><ci id="S2.p1.7.m7.2.2.1.1.3.3.cmml" xref="S2.p1.7.m7.2.2.1.1.3.3">𝑘</ci></apply><ci id="S2.p1.7.m7.2.2.1.1.4.cmml" xref="S2.p1.7.m7.2.2.1.1.4">𝐿</ci><interval closure="open" id="S2.p1.7.m7.2.2.1.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">𝜃</ci><apply id="S2.p1.7.m7.2.2.1.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.7.m7.2.2.1.1.1.1.1.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1">subscript</csymbol><apply id="S2.p1.7.m7.2.2.1.1.1.1.1.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2"><ci id="S2.p1.7.m7.2.2.1.1.1.1.1.2.1.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2.1">~</ci><ci id="S2.p1.7.m7.2.2.1.1.1.1.1.2.2.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.2.2">𝐷</ci></apply><ci id="S2.p1.7.m7.2.2.1.1.1.1.1.3.cmml" xref="S2.p1.7.m7.2.2.1.1.1.1.1.3">𝑘</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.2c">\min_{\theta}\frac{1}{N}\sum_{k=1}^{K}n_{k}L(\theta,\tilde{D}_{k})</annotation></semantics></math>, where <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p1.8.m8.1a"><mi id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">N</annotation></semantics></math> is the sum of all local training dataset size. In each communication round <math id="S2.p1.9.m9.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.p1.9.m9.1a"><mi id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><ci id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">t</annotation></semantics></math>, a global server sends the current global model to all parties. All parties train the global model locally on their local dataset and send the updated local model to the server. The server then aggregates those local models to obtain the new global model. We consider the case where all parties participate in the training in each round, which helps to mitigate potential biases that could arise from the non-uniform sampling of participating parties.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.3" class="ltx_p"><span id="S2.p2.3.1" class="ltx_text ltx_font_bold">Group Fairness.</span>
Fairness has a wide variety of meanings in literature. Group fairness entails, in particular, that the model should perform comparably across groups defined by sensitive attributes (e.g., sex). It is now common practice to evaluate discrimination in a model (or system) based on quantitative measurements of group fairness. In light of this, we focus on two widely-used group fairness notions, equalized odds  <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> and demographic parity <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a href="#bib.bib13" title="" class="ltx_ref">2012</a>)</cite>. To formally define those fairness notions, we assume each data point is associated with a sensitive attribute <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="{a\in\mathcal{A}}" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">a</mi><mo id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">𝒜</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><in id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></in><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">𝑎</ci><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝒜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">{a\in\mathcal{A}}</annotation></semantics></math>, and we use <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.p2.2.m2.1a"><mi id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">X</annotation></semantics></math> and <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">Y</annotation></semantics></math> to denote the input features and the true label. To measure fairness, we use the fairness gap with respect to Equalized Odds Difference, defined as</p>
<table id="A5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex1.m1.6" class="ltx_math_unparsed" alttext="\displaystyle\Delta^{EO}(\theta,\mathcal{D}):=\max_{a,a^{\prime}\in\mathcal{A},y\in\mathcal{Y}}|\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a,Y=y)-\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a^{\prime},Y=y)|" display="inline"><semantics id="S2.Ex1.m1.6a"><mrow id="S2.Ex1.m1.6b"><msup id="S2.Ex1.m1.6.7"><mi mathvariant="normal" id="S2.Ex1.m1.6.7.2">Δ</mi><mrow id="S2.Ex1.m1.6.7.3"><mi id="S2.Ex1.m1.6.7.3.2">E</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.6.7.3.1">​</mo><mi id="S2.Ex1.m1.6.7.3.3">O</mi></mrow></msup><mrow id="S2.Ex1.m1.6.8"><mo stretchy="false" id="S2.Ex1.m1.6.8.1">(</mo><mi id="S2.Ex1.m1.4.4">θ</mi><mo id="S2.Ex1.m1.6.8.2">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.5.5">𝒟</mi><mo rspace="0.278em" stretchy="false" id="S2.Ex1.m1.6.8.3">)</mo></mrow><mo rspace="0.278em" id="S2.Ex1.m1.6.9">:=</mo><munder id="S2.Ex1.m1.6.10"><mi id="S2.Ex1.m1.6.10.2">max</mi><mrow id="S2.Ex1.m1.3.3.3.3"><mrow id="S2.Ex1.m1.2.2.2.2.1"><mrow id="S2.Ex1.m1.2.2.2.2.1.1.1"><mi id="S2.Ex1.m1.1.1.1.1">a</mi><mo id="S2.Ex1.m1.2.2.2.2.1.1.1.2">,</mo><msup id="S2.Ex1.m1.2.2.2.2.1.1.1.1"><mi id="S2.Ex1.m1.2.2.2.2.1.1.1.1.2">a</mi><mo id="S2.Ex1.m1.2.2.2.2.1.1.1.1.3">′</mo></msup></mrow><mo id="S2.Ex1.m1.2.2.2.2.1.2">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.2.2.2.2.1.3">𝒜</mi></mrow><mo id="S2.Ex1.m1.3.3.3.3.3">,</mo><mrow id="S2.Ex1.m1.3.3.3.3.2"><mi id="S2.Ex1.m1.3.3.3.3.2.2">y</mi><mo id="S2.Ex1.m1.3.3.3.3.2.1">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.3.3.3.3.2.3">𝒴</mi></mrow></mrow></munder><mo fence="false" rspace="0.167em" stretchy="false" id="S2.Ex1.m1.6.11">|</mo><munder id="S2.Ex1.m1.6.12"><mi id="S2.Ex1.m1.6.12.2">Pr</mi><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.6.12.3">𝒟</mi></munder><mrow id="S2.Ex1.m1.6.13"><mo stretchy="false" id="S2.Ex1.m1.6.13.1">(</mo><msub id="S2.Ex1.m1.6.13.2"><mi id="S2.Ex1.m1.6.13.2.2">f</mi><mi id="S2.Ex1.m1.6.13.2.3">θ</mi></msub><mrow id="S2.Ex1.m1.6.13.3"><mo stretchy="false" id="S2.Ex1.m1.6.13.3.1">(</mo><mi id="S2.Ex1.m1.6.6">X</mi><mo stretchy="false" id="S2.Ex1.m1.6.13.3.2">)</mo></mrow><mo rspace="0em" id="S2.Ex1.m1.6.13.4">=</mo><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.6.13.5">+</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.Ex1.m1.6.13.6">|</mo><mi id="S2.Ex1.m1.6.13.7">A</mi><mo id="S2.Ex1.m1.6.13.8">=</mo><mi id="S2.Ex1.m1.6.13.9">a</mi><mo id="S2.Ex1.m1.6.13.10">,</mo><mi id="S2.Ex1.m1.6.13.11">Y</mi><mo id="S2.Ex1.m1.6.13.12">=</mo><mi id="S2.Ex1.m1.6.13.13">y</mi><mo stretchy="false" id="S2.Ex1.m1.6.13.14">)</mo></mrow><mo id="S2.Ex1.m1.6.14">−</mo><munder id="S2.Ex1.m1.6.15"><mi id="S2.Ex1.m1.6.15.2">Pr</mi><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.6.15.3">𝒟</mi></munder><mrow id="S2.Ex1.m1.6.16"><mo stretchy="false" id="S2.Ex1.m1.6.16.1">(</mo><msub id="S2.Ex1.m1.6.16.2"><mi id="S2.Ex1.m1.6.16.2.2">f</mi><mi id="S2.Ex1.m1.6.16.2.3">θ</mi></msub><mrow id="S2.Ex1.m1.6.16.3"><mo stretchy="false" id="S2.Ex1.m1.6.16.3.1">(</mo><mi id="S2.Ex1.m1.6.16.3.2">X</mi><mo stretchy="false" id="S2.Ex1.m1.6.16.3.3">)</mo></mrow><mo rspace="0em" id="S2.Ex1.m1.6.16.4">=</mo><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.6.16.5">+</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.Ex1.m1.6.16.6">|</mo><mi id="S2.Ex1.m1.6.16.7">A</mi><mo id="S2.Ex1.m1.6.16.8">=</mo><msup id="S2.Ex1.m1.6.16.9"><mi id="S2.Ex1.m1.6.16.9.2">a</mi><mo id="S2.Ex1.m1.6.16.9.3">′</mo></msup><mo id="S2.Ex1.m1.6.16.10">,</mo><mi id="S2.Ex1.m1.6.16.11">Y</mi><mo id="S2.Ex1.m1.6.16.12">=</mo><mi id="S2.Ex1.m1.6.16.13">y</mi><mo stretchy="false" id="S2.Ex1.m1.6.16.14">)</mo></mrow><mo fence="false" stretchy="false" id="S2.Ex1.m1.6.17">|</mo></mrow><annotation encoding="application/x-tex" id="S2.Ex1.m1.6c">\displaystyle\Delta^{EO}(\theta,\mathcal{D}):=\max_{a,a^{\prime}\in\mathcal{A},y\in\mathcal{Y}}|\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a,Y=y)-\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a^{\prime},Y=y)|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.p2.4" class="ltx_p">with an ideal value equal to zero (perfectly fair). Furthermore, in many applications, there exists a favorable prediction from the model (e.g., grant the loan). We assume the positive prediction (<math id="S2.p2.4.m1.1" class="ltx_Math" alttext="+" display="inline"><semantics id="S2.p2.4.m1.1a"><mo id="S2.p2.4.m1.1.1" xref="S2.p2.4.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S2.p2.4.m1.1b"><plus id="S2.p2.4.m1.1.1.cmml" xref="S2.p2.4.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m1.1c">+</annotation></semantics></math>) as the favorable outcome. <span id="S2.p2.4.1" class="ltx_text ltx_font_italic">Demographic parity</span> <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a href="#bib.bib13" title="" class="ltx_ref">2012</a>)</cite> group fairness notion asks the model to give a favorable label to groups with equal rates. Similarly, the fairness gap with respect to Demographic Parity is defined as follows:</p>
<table id="A5.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S2.Ex2.m1.5" class="ltx_math_unparsed" alttext="\displaystyle\Delta^{DP}(\theta,\mathcal{D}):=\max_{a,a^{\prime}\in\mathcal{A}}|\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a)-\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a^{\prime})|" display="inline"><semantics id="S2.Ex2.m1.5a"><mrow id="S2.Ex2.m1.5b"><msup id="S2.Ex2.m1.5.6"><mi mathvariant="normal" id="S2.Ex2.m1.5.6.2">Δ</mi><mrow id="S2.Ex2.m1.5.6.3"><mi id="S2.Ex2.m1.5.6.3.2">D</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.5.6.3.1">​</mo><mi id="S2.Ex2.m1.5.6.3.3">P</mi></mrow></msup><mrow id="S2.Ex2.m1.5.7"><mo stretchy="false" id="S2.Ex2.m1.5.7.1">(</mo><mi id="S2.Ex2.m1.3.3">θ</mi><mo id="S2.Ex2.m1.5.7.2">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.Ex2.m1.4.4">𝒟</mi><mo rspace="0.278em" stretchy="false" id="S2.Ex2.m1.5.7.3">)</mo></mrow><mo rspace="0.278em" id="S2.Ex2.m1.5.8">:=</mo><munder id="S2.Ex2.m1.5.9"><mi id="S2.Ex2.m1.5.9.2">max</mi><mrow id="S2.Ex2.m1.2.2.2"><mrow id="S2.Ex2.m1.2.2.2.2.1"><mi id="S2.Ex2.m1.1.1.1.1">a</mi><mo id="S2.Ex2.m1.2.2.2.2.1.2">,</mo><msup id="S2.Ex2.m1.2.2.2.2.1.1"><mi id="S2.Ex2.m1.2.2.2.2.1.1.2">a</mi><mo id="S2.Ex2.m1.2.2.2.2.1.1.3">′</mo></msup></mrow><mo id="S2.Ex2.m1.2.2.2.3">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.Ex2.m1.2.2.2.4">𝒜</mi></mrow></munder><mo fence="false" rspace="0.167em" stretchy="false" id="S2.Ex2.m1.5.10">|</mo><munder id="S2.Ex2.m1.5.11"><mi id="S2.Ex2.m1.5.11.2">Pr</mi><mi class="ltx_font_mathcaligraphic" id="S2.Ex2.m1.5.11.3">𝒟</mi></munder><mrow id="S2.Ex2.m1.5.12"><mo stretchy="false" id="S2.Ex2.m1.5.12.1">(</mo><msub id="S2.Ex2.m1.5.12.2"><mi id="S2.Ex2.m1.5.12.2.2">f</mi><mi id="S2.Ex2.m1.5.12.2.3">θ</mi></msub><mrow id="S2.Ex2.m1.5.12.3"><mo stretchy="false" id="S2.Ex2.m1.5.12.3.1">(</mo><mi id="S2.Ex2.m1.5.5">X</mi><mo stretchy="false" id="S2.Ex2.m1.5.12.3.2">)</mo></mrow><mo rspace="0em" id="S2.Ex2.m1.5.12.4">=</mo><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.5.12.5">+</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.Ex2.m1.5.12.6">|</mo><mi id="S2.Ex2.m1.5.12.7">A</mi><mo id="S2.Ex2.m1.5.12.8">=</mo><mi id="S2.Ex2.m1.5.12.9">a</mi><mo stretchy="false" id="S2.Ex2.m1.5.12.10">)</mo></mrow><mo id="S2.Ex2.m1.5.13">−</mo><munder id="S2.Ex2.m1.5.14"><mi id="S2.Ex2.m1.5.14.2">Pr</mi><mi class="ltx_font_mathcaligraphic" id="S2.Ex2.m1.5.14.3">𝒟</mi></munder><mrow id="S2.Ex2.m1.5.15"><mo stretchy="false" id="S2.Ex2.m1.5.15.1">(</mo><msub id="S2.Ex2.m1.5.15.2"><mi id="S2.Ex2.m1.5.15.2.2">f</mi><mi id="S2.Ex2.m1.5.15.2.3">θ</mi></msub><mrow id="S2.Ex2.m1.5.15.3"><mo stretchy="false" id="S2.Ex2.m1.5.15.3.1">(</mo><mi id="S2.Ex2.m1.5.15.3.2">X</mi><mo stretchy="false" id="S2.Ex2.m1.5.15.3.3">)</mo></mrow><mo rspace="0em" id="S2.Ex2.m1.5.15.4">=</mo><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.5.15.5">+</mo><mo fence="false" rspace="0.167em" stretchy="false" id="S2.Ex2.m1.5.15.6">|</mo><mi id="S2.Ex2.m1.5.15.7">A</mi><mo id="S2.Ex2.m1.5.15.8">=</mo><msup id="S2.Ex2.m1.5.15.9"><mi id="S2.Ex2.m1.5.15.9.2">a</mi><mo id="S2.Ex2.m1.5.15.9.3">′</mo></msup><mo stretchy="false" id="S2.Ex2.m1.5.15.10">)</mo></mrow><mo fence="false" stretchy="false" id="S2.Ex2.m1.5.16">|</mo></mrow><annotation encoding="application/x-tex" id="S2.Ex2.m1.5c">\displaystyle\Delta^{DP}(\theta,\mathcal{D}):=\max_{a,a^{\prime}\in\mathcal{A}}|\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a)-\Pr_{\mathcal{D}}(f_{\theta}(X)=+|A=a^{\prime})|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.p2.7" class="ltx_p">with an ideal value equal to zero. In the rest of the paper, we use the fairness gap <math id="S2.p2.5.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S2.p2.5.m1.1a"><mi mathvariant="normal" id="S2.p2.5.m1.1.1" xref="S2.p2.5.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m1.1b"><ci id="S2.p2.5.m1.1.1.cmml" xref="S2.p2.5.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m1.1c">\Delta</annotation></semantics></math> (including <math id="S2.p2.6.m2.1" class="ltx_Math" alttext="\Delta^{EO}" display="inline"><semantics id="S2.p2.6.m2.1a"><msup id="S2.p2.6.m2.1.1" xref="S2.p2.6.m2.1.1.cmml"><mi mathvariant="normal" id="S2.p2.6.m2.1.1.2" xref="S2.p2.6.m2.1.1.2.cmml">Δ</mi><mrow id="S2.p2.6.m2.1.1.3" xref="S2.p2.6.m2.1.1.3.cmml"><mi id="S2.p2.6.m2.1.1.3.2" xref="S2.p2.6.m2.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.p2.6.m2.1.1.3.1" xref="S2.p2.6.m2.1.1.3.1.cmml">​</mo><mi id="S2.p2.6.m2.1.1.3.3" xref="S2.p2.6.m2.1.1.3.3.cmml">O</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.p2.6.m2.1b"><apply id="S2.p2.6.m2.1.1.cmml" xref="S2.p2.6.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.6.m2.1.1.1.cmml" xref="S2.p2.6.m2.1.1">superscript</csymbol><ci id="S2.p2.6.m2.1.1.2.cmml" xref="S2.p2.6.m2.1.1.2">Δ</ci><apply id="S2.p2.6.m2.1.1.3.cmml" xref="S2.p2.6.m2.1.1.3"><times id="S2.p2.6.m2.1.1.3.1.cmml" xref="S2.p2.6.m2.1.1.3.1"></times><ci id="S2.p2.6.m2.1.1.3.2.cmml" xref="S2.p2.6.m2.1.1.3.2">𝐸</ci><ci id="S2.p2.6.m2.1.1.3.3.cmml" xref="S2.p2.6.m2.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m2.1c">\Delta^{EO}</annotation></semantics></math> and <math id="S2.p2.7.m3.1" class="ltx_Math" alttext="\Delta^{DP}" display="inline"><semantics id="S2.p2.7.m3.1a"><msup id="S2.p2.7.m3.1.1" xref="S2.p2.7.m3.1.1.cmml"><mi mathvariant="normal" id="S2.p2.7.m3.1.1.2" xref="S2.p2.7.m3.1.1.2.cmml">Δ</mi><mrow id="S2.p2.7.m3.1.1.3" xref="S2.p2.7.m3.1.1.3.cmml"><mi id="S2.p2.7.m3.1.1.3.2" xref="S2.p2.7.m3.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.p2.7.m3.1.1.3.1" xref="S2.p2.7.m3.1.1.3.1.cmml">​</mo><mi id="S2.p2.7.m3.1.1.3.3" xref="S2.p2.7.m3.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.p2.7.m3.1b"><apply id="S2.p2.7.m3.1.1.cmml" xref="S2.p2.7.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.7.m3.1.1.1.cmml" xref="S2.p2.7.m3.1.1">superscript</csymbol><ci id="S2.p2.7.m3.1.1.2.cmml" xref="S2.p2.7.m3.1.1.2">Δ</ci><apply id="S2.p2.7.m3.1.1.3.cmml" xref="S2.p2.7.m3.1.1.3"><times id="S2.p2.7.m3.1.1.3.1.cmml" xref="S2.p2.7.m3.1.1.3.1"></times><ci id="S2.p2.7.m3.1.1.3.2.cmml" xref="S2.p2.7.m3.1.1.3.2">𝐷</ci><ci id="S2.p2.7.m3.1.1.3.3.cmml" xref="S2.p2.7.m3.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m3.1c">\Delta^{DP}</annotation></semantics></math> ) to measure the bias (fairness) of a model. The more significant fairness gap means the model is more biased (less fair). We will use bias or unfairness interchangeably.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.9" class="ltx_p"><span id="S2.p3.9.1" class="ltx_text ltx_font_bold">Measuring the impact of FL.</span>
Federated learning aims to enhance model performance compared to standalone training and achieve comparable performance to centralized training.
Thus, in order to evaluate the impact of FL on local fairness, we use centralized training and standalone training as baselines. In standalone training, each party trains a model <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="\theta_{k}" display="inline"><semantics id="S2.p3.1.m1.1a"><msub id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mi id="S2.p3.1.m1.1.1.2" xref="S2.p3.1.m1.1.1.2.cmml">θ</mi><mi id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.2">𝜃</ci><ci id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\theta_{k}</annotation></semantics></math> independently to minimize the loss on its training data <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="\tilde{D}_{k}" display="inline"><semantics id="S2.p3.2.m2.1a"><msub id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><mover accent="true" id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml"><mi id="S2.p3.2.m2.1.1.2.2" xref="S2.p3.2.m2.1.1.2.2.cmml">D</mi><mo id="S2.p3.2.m2.1.1.2.1" xref="S2.p3.2.m2.1.1.2.1.cmml">~</mo></mover><mi id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1">subscript</csymbol><apply id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2"><ci id="S2.p3.2.m2.1.1.2.1.cmml" xref="S2.p3.2.m2.1.1.2.1">~</ci><ci id="S2.p3.2.m2.1.1.2.2.cmml" xref="S2.p3.2.m2.1.1.2.2">𝐷</ci></apply><ci id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">\tilde{D}_{k}</annotation></semantics></math>. Note that the standalone model’s fairness gap for a party is contributed by herself. Therefore, we use the fairness gap of a party’s standalone model to represent the party’s bias. In contrast, in centralized training, all local training datasets from all parties are combined, and a centralized model <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="\theta_{c}" display="inline"><semantics id="S2.p3.3.m3.1a"><msub id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml"><mi id="S2.p3.3.m3.1.1.2" xref="S2.p3.3.m3.1.1.2.cmml">θ</mi><mi id="S2.p3.3.m3.1.1.3" xref="S2.p3.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><apply id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p3.3.m3.1.1.1.cmml" xref="S2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.p3.3.m3.1.1.2.cmml" xref="S2.p3.3.m3.1.1.2">𝜃</ci><ci id="S2.p3.3.m3.1.1.3.cmml" xref="S2.p3.3.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">\theta_{c}</annotation></semantics></math> is trained using this global training dataset.
We define the benefit of FL and collaboration (i.e., centralized training) in terms of fairness and accuracy using these baselines. The benefit of FL for each party <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">k</annotation></semantics></math> is defined as the difference between the standalone model <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="\theta_{k}" display="inline"><semantics id="S2.p3.5.m5.1a"><msub id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml"><mi id="S2.p3.5.m5.1.1.2" xref="S2.p3.5.m5.1.1.2.cmml">θ</mi><mi id="S2.p3.5.m5.1.1.3" xref="S2.p3.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><apply id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p3.5.m5.1.1.1.cmml" xref="S2.p3.5.m5.1.1">subscript</csymbol><ci id="S2.p3.5.m5.1.1.2.cmml" xref="S2.p3.5.m5.1.1.2">𝜃</ci><ci id="S2.p3.5.m5.1.1.3.cmml" xref="S2.p3.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">\theta_{k}</annotation></semantics></math> and the FL model <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="\theta_{g}" display="inline"><semantics id="S2.p3.6.m6.1a"><msub id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml"><mi id="S2.p3.6.m6.1.1.2" xref="S2.p3.6.m6.1.1.2.cmml">θ</mi><mi id="S2.p3.6.m6.1.1.3" xref="S2.p3.6.m6.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><apply id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p3.6.m6.1.1.1.cmml" xref="S2.p3.6.m6.1.1">subscript</csymbol><ci id="S2.p3.6.m6.1.1.2.cmml" xref="S2.p3.6.m6.1.1.2">𝜃</ci><ci id="S2.p3.6.m6.1.1.3.cmml" xref="S2.p3.6.m6.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">\theta_{g}</annotation></semantics></math>. This difference reveals the extent to which a party benefits from participating in FL. Specifically, for a party <math id="S2.p3.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p3.7.m7.1a"><mi id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><ci id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">k</annotation></semantics></math>, we define the <span id="S2.p3.9.2" class="ltx_text ltx_font_bold">accuracy benefit of FL</span> as <math id="S2.p3.8.m8.4" class="ltx_Math" alttext="{Acc(\theta_{g},\mathcal{D}_{k})-Acc(\theta_{k},\mathcal{D}_{k})}" display="inline"><semantics id="S2.p3.8.m8.4a"><mrow id="S2.p3.8.m8.4.4" xref="S2.p3.8.m8.4.4.cmml"><mrow id="S2.p3.8.m8.2.2.2" xref="S2.p3.8.m8.2.2.2.cmml"><mi id="S2.p3.8.m8.2.2.2.4" xref="S2.p3.8.m8.2.2.2.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.p3.8.m8.2.2.2.3" xref="S2.p3.8.m8.2.2.2.3.cmml">​</mo><mi id="S2.p3.8.m8.2.2.2.5" xref="S2.p3.8.m8.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.p3.8.m8.2.2.2.3a" xref="S2.p3.8.m8.2.2.2.3.cmml">​</mo><mi id="S2.p3.8.m8.2.2.2.6" xref="S2.p3.8.m8.2.2.2.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.p3.8.m8.2.2.2.3b" xref="S2.p3.8.m8.2.2.2.3.cmml">​</mo><mrow id="S2.p3.8.m8.2.2.2.2.2" xref="S2.p3.8.m8.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.p3.8.m8.2.2.2.2.2.3" xref="S2.p3.8.m8.2.2.2.2.3.cmml">(</mo><msub id="S2.p3.8.m8.1.1.1.1.1.1" xref="S2.p3.8.m8.1.1.1.1.1.1.cmml"><mi id="S2.p3.8.m8.1.1.1.1.1.1.2" xref="S2.p3.8.m8.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S2.p3.8.m8.1.1.1.1.1.1.3" xref="S2.p3.8.m8.1.1.1.1.1.1.3.cmml">g</mi></msub><mo id="S2.p3.8.m8.2.2.2.2.2.4" xref="S2.p3.8.m8.2.2.2.2.3.cmml">,</mo><msub id="S2.p3.8.m8.2.2.2.2.2.2" xref="S2.p3.8.m8.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.8.m8.2.2.2.2.2.2.2" xref="S2.p3.8.m8.2.2.2.2.2.2.2.cmml">𝒟</mi><mi id="S2.p3.8.m8.2.2.2.2.2.2.3" xref="S2.p3.8.m8.2.2.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S2.p3.8.m8.2.2.2.2.2.5" xref="S2.p3.8.m8.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.p3.8.m8.4.4.5" xref="S2.p3.8.m8.4.4.5.cmml">−</mo><mrow id="S2.p3.8.m8.4.4.4" xref="S2.p3.8.m8.4.4.4.cmml"><mi id="S2.p3.8.m8.4.4.4.4" xref="S2.p3.8.m8.4.4.4.4.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.p3.8.m8.4.4.4.3" xref="S2.p3.8.m8.4.4.4.3.cmml">​</mo><mi id="S2.p3.8.m8.4.4.4.5" xref="S2.p3.8.m8.4.4.4.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.p3.8.m8.4.4.4.3a" xref="S2.p3.8.m8.4.4.4.3.cmml">​</mo><mi id="S2.p3.8.m8.4.4.4.6" xref="S2.p3.8.m8.4.4.4.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S2.p3.8.m8.4.4.4.3b" xref="S2.p3.8.m8.4.4.4.3.cmml">​</mo><mrow id="S2.p3.8.m8.4.4.4.2.2" xref="S2.p3.8.m8.4.4.4.2.3.cmml"><mo stretchy="false" id="S2.p3.8.m8.4.4.4.2.2.3" xref="S2.p3.8.m8.4.4.4.2.3.cmml">(</mo><msub id="S2.p3.8.m8.3.3.3.1.1.1" xref="S2.p3.8.m8.3.3.3.1.1.1.cmml"><mi id="S2.p3.8.m8.3.3.3.1.1.1.2" xref="S2.p3.8.m8.3.3.3.1.1.1.2.cmml">θ</mi><mi id="S2.p3.8.m8.3.3.3.1.1.1.3" xref="S2.p3.8.m8.3.3.3.1.1.1.3.cmml">k</mi></msub><mo id="S2.p3.8.m8.4.4.4.2.2.4" xref="S2.p3.8.m8.4.4.4.2.3.cmml">,</mo><msub id="S2.p3.8.m8.4.4.4.2.2.2" xref="S2.p3.8.m8.4.4.4.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.8.m8.4.4.4.2.2.2.2" xref="S2.p3.8.m8.4.4.4.2.2.2.2.cmml">𝒟</mi><mi id="S2.p3.8.m8.4.4.4.2.2.2.3" xref="S2.p3.8.m8.4.4.4.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S2.p3.8.m8.4.4.4.2.2.5" xref="S2.p3.8.m8.4.4.4.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.8.m8.4b"><apply id="S2.p3.8.m8.4.4.cmml" xref="S2.p3.8.m8.4.4"><minus id="S2.p3.8.m8.4.4.5.cmml" xref="S2.p3.8.m8.4.4.5"></minus><apply id="S2.p3.8.m8.2.2.2.cmml" xref="S2.p3.8.m8.2.2.2"><times id="S2.p3.8.m8.2.2.2.3.cmml" xref="S2.p3.8.m8.2.2.2.3"></times><ci id="S2.p3.8.m8.2.2.2.4.cmml" xref="S2.p3.8.m8.2.2.2.4">𝐴</ci><ci id="S2.p3.8.m8.2.2.2.5.cmml" xref="S2.p3.8.m8.2.2.2.5">𝑐</ci><ci id="S2.p3.8.m8.2.2.2.6.cmml" xref="S2.p3.8.m8.2.2.2.6">𝑐</ci><interval closure="open" id="S2.p3.8.m8.2.2.2.2.3.cmml" xref="S2.p3.8.m8.2.2.2.2.2"><apply id="S2.p3.8.m8.1.1.1.1.1.1.cmml" xref="S2.p3.8.m8.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p3.8.m8.1.1.1.1.1.1.1.cmml" xref="S2.p3.8.m8.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p3.8.m8.1.1.1.1.1.1.2.cmml" xref="S2.p3.8.m8.1.1.1.1.1.1.2">𝜃</ci><ci id="S2.p3.8.m8.1.1.1.1.1.1.3.cmml" xref="S2.p3.8.m8.1.1.1.1.1.1.3">𝑔</ci></apply><apply id="S2.p3.8.m8.2.2.2.2.2.2.cmml" xref="S2.p3.8.m8.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.2.2.2.2.2.2.1.cmml" xref="S2.p3.8.m8.2.2.2.2.2.2">subscript</csymbol><ci id="S2.p3.8.m8.2.2.2.2.2.2.2.cmml" xref="S2.p3.8.m8.2.2.2.2.2.2.2">𝒟</ci><ci id="S2.p3.8.m8.2.2.2.2.2.2.3.cmml" xref="S2.p3.8.m8.2.2.2.2.2.2.3">𝑘</ci></apply></interval></apply><apply id="S2.p3.8.m8.4.4.4.cmml" xref="S2.p3.8.m8.4.4.4"><times id="S2.p3.8.m8.4.4.4.3.cmml" xref="S2.p3.8.m8.4.4.4.3"></times><ci id="S2.p3.8.m8.4.4.4.4.cmml" xref="S2.p3.8.m8.4.4.4.4">𝐴</ci><ci id="S2.p3.8.m8.4.4.4.5.cmml" xref="S2.p3.8.m8.4.4.4.5">𝑐</ci><ci id="S2.p3.8.m8.4.4.4.6.cmml" xref="S2.p3.8.m8.4.4.4.6">𝑐</ci><interval closure="open" id="S2.p3.8.m8.4.4.4.2.3.cmml" xref="S2.p3.8.m8.4.4.4.2.2"><apply id="S2.p3.8.m8.3.3.3.1.1.1.cmml" xref="S2.p3.8.m8.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.p3.8.m8.3.3.3.1.1.1.1.cmml" xref="S2.p3.8.m8.3.3.3.1.1.1">subscript</csymbol><ci id="S2.p3.8.m8.3.3.3.1.1.1.2.cmml" xref="S2.p3.8.m8.3.3.3.1.1.1.2">𝜃</ci><ci id="S2.p3.8.m8.3.3.3.1.1.1.3.cmml" xref="S2.p3.8.m8.3.3.3.1.1.1.3">𝑘</ci></apply><apply id="S2.p3.8.m8.4.4.4.2.2.2.cmml" xref="S2.p3.8.m8.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S2.p3.8.m8.4.4.4.2.2.2.1.cmml" xref="S2.p3.8.m8.4.4.4.2.2.2">subscript</csymbol><ci id="S2.p3.8.m8.4.4.4.2.2.2.2.cmml" xref="S2.p3.8.m8.4.4.4.2.2.2.2">𝒟</ci><ci id="S2.p3.8.m8.4.4.4.2.2.2.3.cmml" xref="S2.p3.8.m8.4.4.4.2.2.2.3">𝑘</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.8.m8.4c">{Acc(\theta_{g},\mathcal{D}_{k})-Acc(\theta_{k},\mathcal{D}_{k})}</annotation></semantics></math> and <span id="S2.p3.9.3" class="ltx_text ltx_font_bold">fairness benefit of FL</span> as <math id="S2.p3.9.m9.4" class="ltx_Math" alttext="{\Delta(\theta_{k},\mathcal{D}_{k})-\Delta(\theta_{g},\mathcal{D}_{k})}" display="inline"><semantics id="S2.p3.9.m9.4a"><mrow id="S2.p3.9.m9.4.4" xref="S2.p3.9.m9.4.4.cmml"><mrow id="S2.p3.9.m9.2.2.2" xref="S2.p3.9.m9.2.2.2.cmml"><mi mathvariant="normal" id="S2.p3.9.m9.2.2.2.4" xref="S2.p3.9.m9.2.2.2.4.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.p3.9.m9.2.2.2.3" xref="S2.p3.9.m9.2.2.2.3.cmml">​</mo><mrow id="S2.p3.9.m9.2.2.2.2.2" xref="S2.p3.9.m9.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.p3.9.m9.2.2.2.2.2.3" xref="S2.p3.9.m9.2.2.2.2.3.cmml">(</mo><msub id="S2.p3.9.m9.1.1.1.1.1.1" xref="S2.p3.9.m9.1.1.1.1.1.1.cmml"><mi id="S2.p3.9.m9.1.1.1.1.1.1.2" xref="S2.p3.9.m9.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S2.p3.9.m9.1.1.1.1.1.1.3" xref="S2.p3.9.m9.1.1.1.1.1.1.3.cmml">k</mi></msub><mo id="S2.p3.9.m9.2.2.2.2.2.4" xref="S2.p3.9.m9.2.2.2.2.3.cmml">,</mo><msub id="S2.p3.9.m9.2.2.2.2.2.2" xref="S2.p3.9.m9.2.2.2.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.9.m9.2.2.2.2.2.2.2" xref="S2.p3.9.m9.2.2.2.2.2.2.2.cmml">𝒟</mi><mi id="S2.p3.9.m9.2.2.2.2.2.2.3" xref="S2.p3.9.m9.2.2.2.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S2.p3.9.m9.2.2.2.2.2.5" xref="S2.p3.9.m9.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.p3.9.m9.4.4.5" xref="S2.p3.9.m9.4.4.5.cmml">−</mo><mrow id="S2.p3.9.m9.4.4.4" xref="S2.p3.9.m9.4.4.4.cmml"><mi mathvariant="normal" id="S2.p3.9.m9.4.4.4.4" xref="S2.p3.9.m9.4.4.4.4.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S2.p3.9.m9.4.4.4.3" xref="S2.p3.9.m9.4.4.4.3.cmml">​</mo><mrow id="S2.p3.9.m9.4.4.4.2.2" xref="S2.p3.9.m9.4.4.4.2.3.cmml"><mo stretchy="false" id="S2.p3.9.m9.4.4.4.2.2.3" xref="S2.p3.9.m9.4.4.4.2.3.cmml">(</mo><msub id="S2.p3.9.m9.3.3.3.1.1.1" xref="S2.p3.9.m9.3.3.3.1.1.1.cmml"><mi id="S2.p3.9.m9.3.3.3.1.1.1.2" xref="S2.p3.9.m9.3.3.3.1.1.1.2.cmml">θ</mi><mi id="S2.p3.9.m9.3.3.3.1.1.1.3" xref="S2.p3.9.m9.3.3.3.1.1.1.3.cmml">g</mi></msub><mo id="S2.p3.9.m9.4.4.4.2.2.4" xref="S2.p3.9.m9.4.4.4.2.3.cmml">,</mo><msub id="S2.p3.9.m9.4.4.4.2.2.2" xref="S2.p3.9.m9.4.4.4.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p3.9.m9.4.4.4.2.2.2.2" xref="S2.p3.9.m9.4.4.4.2.2.2.2.cmml">𝒟</mi><mi id="S2.p3.9.m9.4.4.4.2.2.2.3" xref="S2.p3.9.m9.4.4.4.2.2.2.3.cmml">k</mi></msub><mo stretchy="false" id="S2.p3.9.m9.4.4.4.2.2.5" xref="S2.p3.9.m9.4.4.4.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.9.m9.4b"><apply id="S2.p3.9.m9.4.4.cmml" xref="S2.p3.9.m9.4.4"><minus id="S2.p3.9.m9.4.4.5.cmml" xref="S2.p3.9.m9.4.4.5"></minus><apply id="S2.p3.9.m9.2.2.2.cmml" xref="S2.p3.9.m9.2.2.2"><times id="S2.p3.9.m9.2.2.2.3.cmml" xref="S2.p3.9.m9.2.2.2.3"></times><ci id="S2.p3.9.m9.2.2.2.4.cmml" xref="S2.p3.9.m9.2.2.2.4">Δ</ci><interval closure="open" id="S2.p3.9.m9.2.2.2.2.3.cmml" xref="S2.p3.9.m9.2.2.2.2.2"><apply id="S2.p3.9.m9.1.1.1.1.1.1.cmml" xref="S2.p3.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p3.9.m9.1.1.1.1.1.1.1.cmml" xref="S2.p3.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p3.9.m9.1.1.1.1.1.1.2.cmml" xref="S2.p3.9.m9.1.1.1.1.1.1.2">𝜃</ci><ci id="S2.p3.9.m9.1.1.1.1.1.1.3.cmml" xref="S2.p3.9.m9.1.1.1.1.1.1.3">𝑘</ci></apply><apply id="S2.p3.9.m9.2.2.2.2.2.2.cmml" xref="S2.p3.9.m9.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p3.9.m9.2.2.2.2.2.2.1.cmml" xref="S2.p3.9.m9.2.2.2.2.2.2">subscript</csymbol><ci id="S2.p3.9.m9.2.2.2.2.2.2.2.cmml" xref="S2.p3.9.m9.2.2.2.2.2.2.2">𝒟</ci><ci id="S2.p3.9.m9.2.2.2.2.2.2.3.cmml" xref="S2.p3.9.m9.2.2.2.2.2.2.3">𝑘</ci></apply></interval></apply><apply id="S2.p3.9.m9.4.4.4.cmml" xref="S2.p3.9.m9.4.4.4"><times id="S2.p3.9.m9.4.4.4.3.cmml" xref="S2.p3.9.m9.4.4.4.3"></times><ci id="S2.p3.9.m9.4.4.4.4.cmml" xref="S2.p3.9.m9.4.4.4.4">Δ</ci><interval closure="open" id="S2.p3.9.m9.4.4.4.2.3.cmml" xref="S2.p3.9.m9.4.4.4.2.2"><apply id="S2.p3.9.m9.3.3.3.1.1.1.cmml" xref="S2.p3.9.m9.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.p3.9.m9.3.3.3.1.1.1.1.cmml" xref="S2.p3.9.m9.3.3.3.1.1.1">subscript</csymbol><ci id="S2.p3.9.m9.3.3.3.1.1.1.2.cmml" xref="S2.p3.9.m9.3.3.3.1.1.1.2">𝜃</ci><ci id="S2.p3.9.m9.3.3.3.1.1.1.3.cmml" xref="S2.p3.9.m9.3.3.3.1.1.1.3">𝑔</ci></apply><apply id="S2.p3.9.m9.4.4.4.2.2.2.cmml" xref="S2.p3.9.m9.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S2.p3.9.m9.4.4.4.2.2.2.1.cmml" xref="S2.p3.9.m9.4.4.4.2.2.2">subscript</csymbol><ci id="S2.p3.9.m9.4.4.4.2.2.2.2.cmml" xref="S2.p3.9.m9.4.4.4.2.2.2.2">𝒟</ci><ci id="S2.p3.9.m9.4.4.4.2.2.2.3.cmml" xref="S2.p3.9.m9.4.4.4.2.2.2.3">𝑘</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.9.m9.4c">{\Delta(\theta_{k},\mathcal{D}_{k})-\Delta(\theta_{g},\mathcal{D}_{k})}</annotation></semantics></math>. Similarly, the benefit of collaboration is based on the difference between the standalone model and the centralized model. This difference indicates how much an individual party could gain from collaborating with others by sharing the entire training dataset. We compute the average benefit of FL or collaboration across all parties in the network. A positive benefit indicates that FL (or collaboration) improves accuracy or fairness, while a negative benefit implies that FL or collaboration has a detrimental effect.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.8" class="ltx_p"><span id="S2.p4.8.1" class="ltx_text ltx_font_bold">Measuring bias aside fairness gap.</span>
Measuring the bias of a model based on the fairness gap reveals the model’s performance disparity across groups but does not explain why the model is biased. Specifically, it does not reveal whether the prediction of the model is attributed to the sensitive attribute or any other insensitive attributes whose distributions vary between groups. The former is typically referred to as disparate treatment (direct discrimination) as the sensitive attribute directly influences the model prediction <cite class="ltx_cite ltx_citemacro_citep">(Grabowicz et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>; Zafar et al., <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>. Towards measuring bias in this aspect, we employ the feature attribution method to explain the model’s prediction on each data point. More precisely, given a model <math id="S2.p4.1.m1.1" class="ltx_Math" alttext="f_{\theta}" display="inline"><semantics id="S2.p4.1.m1.1a"><msub id="S2.p4.1.m1.1.1" xref="S2.p4.1.m1.1.1.cmml"><mi id="S2.p4.1.m1.1.1.2" xref="S2.p4.1.m1.1.1.2.cmml">f</mi><mi id="S2.p4.1.m1.1.1.3" xref="S2.p4.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.1.m1.1b"><apply id="S2.p4.1.m1.1.1.cmml" xref="S2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p4.1.m1.1.1.1.cmml" xref="S2.p4.1.m1.1.1">subscript</csymbol><ci id="S2.p4.1.m1.1.1.2.cmml" xref="S2.p4.1.m1.1.1.2">𝑓</ci><ci id="S2.p4.1.m1.1.1.3.cmml" xref="S2.p4.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.1.m1.1c">f_{\theta}</annotation></semantics></math>, and the input features <math id="S2.p4.2.m2.4" class="ltx_Math" alttext="x=(x_{1},x_{2},...,x_{d})" display="inline"><semantics id="S2.p4.2.m2.4a"><mrow id="S2.p4.2.m2.4.4" xref="S2.p4.2.m2.4.4.cmml"><mi id="S2.p4.2.m2.4.4.5" xref="S2.p4.2.m2.4.4.5.cmml">x</mi><mo id="S2.p4.2.m2.4.4.4" xref="S2.p4.2.m2.4.4.4.cmml">=</mo><mrow id="S2.p4.2.m2.4.4.3.3" xref="S2.p4.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S2.p4.2.m2.4.4.3.3.4" xref="S2.p4.2.m2.4.4.3.4.cmml">(</mo><msub id="S2.p4.2.m2.2.2.1.1.1" xref="S2.p4.2.m2.2.2.1.1.1.cmml"><mi id="S2.p4.2.m2.2.2.1.1.1.2" xref="S2.p4.2.m2.2.2.1.1.1.2.cmml">x</mi><mn id="S2.p4.2.m2.2.2.1.1.1.3" xref="S2.p4.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p4.2.m2.4.4.3.3.5" xref="S2.p4.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.p4.2.m2.3.3.2.2.2" xref="S2.p4.2.m2.3.3.2.2.2.cmml"><mi id="S2.p4.2.m2.3.3.2.2.2.2" xref="S2.p4.2.m2.3.3.2.2.2.2.cmml">x</mi><mn id="S2.p4.2.m2.3.3.2.2.2.3" xref="S2.p4.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p4.2.m2.4.4.3.3.6" xref="S2.p4.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.p4.2.m2.1.1" xref="S2.p4.2.m2.1.1.cmml">…</mi><mo id="S2.p4.2.m2.4.4.3.3.7" xref="S2.p4.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.p4.2.m2.4.4.3.3.3" xref="S2.p4.2.m2.4.4.3.3.3.cmml"><mi id="S2.p4.2.m2.4.4.3.3.3.2" xref="S2.p4.2.m2.4.4.3.3.3.2.cmml">x</mi><mi id="S2.p4.2.m2.4.4.3.3.3.3" xref="S2.p4.2.m2.4.4.3.3.3.3.cmml">d</mi></msub><mo stretchy="false" id="S2.p4.2.m2.4.4.3.3.8" xref="S2.p4.2.m2.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.2.m2.4b"><apply id="S2.p4.2.m2.4.4.cmml" xref="S2.p4.2.m2.4.4"><eq id="S2.p4.2.m2.4.4.4.cmml" xref="S2.p4.2.m2.4.4.4"></eq><ci id="S2.p4.2.m2.4.4.5.cmml" xref="S2.p4.2.m2.4.4.5">𝑥</ci><vector id="S2.p4.2.m2.4.4.3.4.cmml" xref="S2.p4.2.m2.4.4.3.3"><apply id="S2.p4.2.m2.2.2.1.1.1.cmml" xref="S2.p4.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p4.2.m2.2.2.1.1.1.1.cmml" xref="S2.p4.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.p4.2.m2.2.2.1.1.1.2.cmml" xref="S2.p4.2.m2.2.2.1.1.1.2">𝑥</ci><cn type="integer" id="S2.p4.2.m2.2.2.1.1.1.3.cmml" xref="S2.p4.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.p4.2.m2.3.3.2.2.2.cmml" xref="S2.p4.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p4.2.m2.3.3.2.2.2.1.cmml" xref="S2.p4.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.p4.2.m2.3.3.2.2.2.2.cmml" xref="S2.p4.2.m2.3.3.2.2.2.2">𝑥</ci><cn type="integer" id="S2.p4.2.m2.3.3.2.2.2.3.cmml" xref="S2.p4.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.p4.2.m2.1.1.cmml" xref="S2.p4.2.m2.1.1">…</ci><apply id="S2.p4.2.m2.4.4.3.3.3.cmml" xref="S2.p4.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p4.2.m2.4.4.3.3.3.1.cmml" xref="S2.p4.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.p4.2.m2.4.4.3.3.3.2.cmml" xref="S2.p4.2.m2.4.4.3.3.3.2">𝑥</ci><ci id="S2.p4.2.m2.4.4.3.3.3.3.cmml" xref="S2.p4.2.m2.4.4.3.3.3.3">𝑑</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.2.m2.4c">x=(x_{1},x_{2},...,x_{d})</annotation></semantics></math>, the attribution of the prediction at input <math id="S2.p4.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.p4.3.m3.1a"><mi id="S2.p4.3.m3.1.1" xref="S2.p4.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.p4.3.m3.1b"><ci id="S2.p4.3.m3.1.1.cmml" xref="S2.p4.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.3.m3.1c">x</annotation></semantics></math> relative to a baseline input <math id="S2.p4.4.m4.1" class="ltx_Math" alttext="x^{\prime}" display="inline"><semantics id="S2.p4.4.m4.1a"><msup id="S2.p4.4.m4.1.1" xref="S2.p4.4.m4.1.1.cmml"><mi id="S2.p4.4.m4.1.1.2" xref="S2.p4.4.m4.1.1.2.cmml">x</mi><mo id="S2.p4.4.m4.1.1.3" xref="S2.p4.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.p4.4.m4.1b"><apply id="S2.p4.4.m4.1.1.cmml" xref="S2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p4.4.m4.1.1.1.cmml" xref="S2.p4.4.m4.1.1">superscript</csymbol><ci id="S2.p4.4.m4.1.1.2.cmml" xref="S2.p4.4.m4.1.1.2">𝑥</ci><ci id="S2.p4.4.m4.1.1.3.cmml" xref="S2.p4.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.4.m4.1c">x^{\prime}</annotation></semantics></math> is a vector <math id="S2.p4.5.m5.3" class="ltx_Math" alttext="(a_{1},...,a_{d})" display="inline"><semantics id="S2.p4.5.m5.3a"><mrow id="S2.p4.5.m5.3.3.2" xref="S2.p4.5.m5.3.3.3.cmml"><mo stretchy="false" id="S2.p4.5.m5.3.3.2.3" xref="S2.p4.5.m5.3.3.3.cmml">(</mo><msub id="S2.p4.5.m5.2.2.1.1" xref="S2.p4.5.m5.2.2.1.1.cmml"><mi id="S2.p4.5.m5.2.2.1.1.2" xref="S2.p4.5.m5.2.2.1.1.2.cmml">a</mi><mn id="S2.p4.5.m5.2.2.1.1.3" xref="S2.p4.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.p4.5.m5.3.3.2.4" xref="S2.p4.5.m5.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.p4.5.m5.1.1" xref="S2.p4.5.m5.1.1.cmml">…</mi><mo id="S2.p4.5.m5.3.3.2.5" xref="S2.p4.5.m5.3.3.3.cmml">,</mo><msub id="S2.p4.5.m5.3.3.2.2" xref="S2.p4.5.m5.3.3.2.2.cmml"><mi id="S2.p4.5.m5.3.3.2.2.2" xref="S2.p4.5.m5.3.3.2.2.2.cmml">a</mi><mi id="S2.p4.5.m5.3.3.2.2.3" xref="S2.p4.5.m5.3.3.2.2.3.cmml">d</mi></msub><mo stretchy="false" id="S2.p4.5.m5.3.3.2.6" xref="S2.p4.5.m5.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.5.m5.3b"><vector id="S2.p4.5.m5.3.3.3.cmml" xref="S2.p4.5.m5.3.3.2"><apply id="S2.p4.5.m5.2.2.1.1.cmml" xref="S2.p4.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S2.p4.5.m5.2.2.1.1.1.cmml" xref="S2.p4.5.m5.2.2.1.1">subscript</csymbol><ci id="S2.p4.5.m5.2.2.1.1.2.cmml" xref="S2.p4.5.m5.2.2.1.1.2">𝑎</ci><cn type="integer" id="S2.p4.5.m5.2.2.1.1.3.cmml" xref="S2.p4.5.m5.2.2.1.1.3">1</cn></apply><ci id="S2.p4.5.m5.1.1.cmml" xref="S2.p4.5.m5.1.1">…</ci><apply id="S2.p4.5.m5.3.3.2.2.cmml" xref="S2.p4.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S2.p4.5.m5.3.3.2.2.1.cmml" xref="S2.p4.5.m5.3.3.2.2">subscript</csymbol><ci id="S2.p4.5.m5.3.3.2.2.2.cmml" xref="S2.p4.5.m5.3.3.2.2.2">𝑎</ci><ci id="S2.p4.5.m5.3.3.2.2.3.cmml" xref="S2.p4.5.m5.3.3.2.2.3">𝑑</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.5.m5.3c">(a_{1},...,a_{d})</annotation></semantics></math> where <math id="S2.p4.6.m6.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.p4.6.m6.1a"><msub id="S2.p4.6.m6.1.1" xref="S2.p4.6.m6.1.1.cmml"><mi id="S2.p4.6.m6.1.1.2" xref="S2.p4.6.m6.1.1.2.cmml">a</mi><mi id="S2.p4.6.m6.1.1.3" xref="S2.p4.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.6.m6.1b"><apply id="S2.p4.6.m6.1.1.cmml" xref="S2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S2.p4.6.m6.1.1.1.cmml" xref="S2.p4.6.m6.1.1">subscript</csymbol><ci id="S2.p4.6.m6.1.1.2.cmml" xref="S2.p4.6.m6.1.1.2">𝑎</ci><ci id="S2.p4.6.m6.1.1.3.cmml" xref="S2.p4.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.6.m6.1c">a_{i}</annotation></semantics></math> is the contribution of <math id="S2.p4.7.m7.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S2.p4.7.m7.1a"><msub id="S2.p4.7.m7.1.1" xref="S2.p4.7.m7.1.1.cmml"><mi id="S2.p4.7.m7.1.1.2" xref="S2.p4.7.m7.1.1.2.cmml">x</mi><mi id="S2.p4.7.m7.1.1.3" xref="S2.p4.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p4.7.m7.1b"><apply id="S2.p4.7.m7.1.1.cmml" xref="S2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p4.7.m7.1.1.1.cmml" xref="S2.p4.7.m7.1.1">subscript</csymbol><ci id="S2.p4.7.m7.1.1.2.cmml" xref="S2.p4.7.m7.1.1.2">𝑥</ci><ci id="S2.p4.7.m7.1.1.3.cmml" xref="S2.p4.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.7.m7.1c">x_{i}</annotation></semantics></math> to the prediction of <math id="S2.p4.8.m8.1" class="ltx_Math" alttext="f_{\theta}(x)" display="inline"><semantics id="S2.p4.8.m8.1a"><mrow id="S2.p4.8.m8.1.2" xref="S2.p4.8.m8.1.2.cmml"><msub id="S2.p4.8.m8.1.2.2" xref="S2.p4.8.m8.1.2.2.cmml"><mi id="S2.p4.8.m8.1.2.2.2" xref="S2.p4.8.m8.1.2.2.2.cmml">f</mi><mi id="S2.p4.8.m8.1.2.2.3" xref="S2.p4.8.m8.1.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S2.p4.8.m8.1.2.1" xref="S2.p4.8.m8.1.2.1.cmml">​</mo><mrow id="S2.p4.8.m8.1.2.3.2" xref="S2.p4.8.m8.1.2.cmml"><mo stretchy="false" id="S2.p4.8.m8.1.2.3.2.1" xref="S2.p4.8.m8.1.2.cmml">(</mo><mi id="S2.p4.8.m8.1.1" xref="S2.p4.8.m8.1.1.cmml">x</mi><mo stretchy="false" id="S2.p4.8.m8.1.2.3.2.2" xref="S2.p4.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p4.8.m8.1b"><apply id="S2.p4.8.m8.1.2.cmml" xref="S2.p4.8.m8.1.2"><times id="S2.p4.8.m8.1.2.1.cmml" xref="S2.p4.8.m8.1.2.1"></times><apply id="S2.p4.8.m8.1.2.2.cmml" xref="S2.p4.8.m8.1.2.2"><csymbol cd="ambiguous" id="S2.p4.8.m8.1.2.2.1.cmml" xref="S2.p4.8.m8.1.2.2">subscript</csymbol><ci id="S2.p4.8.m8.1.2.2.2.cmml" xref="S2.p4.8.m8.1.2.2.2">𝑓</ci><ci id="S2.p4.8.m8.1.2.2.3.cmml" xref="S2.p4.8.m8.1.2.2.3">𝜃</ci></apply><ci id="S2.p4.8.m8.1.1.cmml" xref="S2.p4.8.m8.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p4.8.m8.1c">f_{\theta}(x)</annotation></semantics></math>. We use Integrated Gradient <cite class="ltx_cite ltx_citemacro_citep">(Sundararajan et al., <a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite> to compute the feature attribution, which only requires a few calls to the gradient operation. Following the suggestions by <cite class="ltx_cite ltx_citemacro_cite">Sundararajan et al. (<a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite>, we use the average feature values computed on the whole test dataset as the baseline for non-sensitive and non-binary sensitive features. For the binary sensitive attribute, we use the opposite feature value as the baseline. For instance, if the test input is (Sex: female, Age:28), the baseline input would be (Sex: male, Age: average age over the data points). By employing this feature attribution, we are able to discover the bias associated with disparate treatment. For more details about the Integrated Gradient, please refer to the original paper <cite class="ltx_cite ltx_citemacro_citep">(Sundararajan et al., <a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Empirical Analysis</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">Our objective is to understand how FL impacts fairness for parties. We start by comparing the average performance between FL and baselines (i.e., standalone training and centralized training) to answer the following question: <span id="S3.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">does FL improve fairness for parties compared to standalone training, and does FL provide the same benefit as centralized training in terms of fairness?</span> Our findings in Section <a href="#S3.SS2" title="3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> demonstrate that FL can exacerbate fairness issues for parties and does not retain the fairness benefit of collaboration, thus not achieving comparable performance to centralized training.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">We further investigate how FL impacts performance for each party to answer the following questions: <span id="S3.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">does FL provide the same benefit in terms of fairness for parties, and what causes the disparate impact of FL on parties’ fairness?</span> Our analysis in Section <a href="#S3.SS3" title="3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> shows that FL can propagate bias among parties. As a result, FL improves fairness for parties with greater bias while worsening fairness issues for parties with less bias.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">Finally, we investigate <span id="S3.p3.1.1" class="ltx_text ltx_font_bold ltx_font_italic">how the bias is propagated in FL</span>. The analysis in Section <a href="#S3.SS4" title="3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a> shows that biased parties encode the bias in a few parameters through local updates, which are propagated to the aggregated model and ultimately to other parties via parameter aggregation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Setup</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.3" class="ltx_p">We use the two datasets with different tasks for our empirical analysis:
<span id="S3.SS1.p1.3.1" class="ltx_text ltx_font_bold ltx_font_italic">US Census Data</span> and <span id="S3.SS1.p1.3.2" class="ltx_text ltx_font_bold ltx_font_italic">CelebA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib31" title="" class="ltx_ref">2015</a>)</cite></span>. The US Census dataset consists of census data from different places in the US. We naturally partition the dataset based on the source of the data. Thus, we have 51 parties in total, representing 50 states in the US and Puerto Rico. We consider three tasks defined in the folktables <cite class="ltx_cite ltx_citemacro_citep">(Ding et al., <a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>: Income (ACSIncome) <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We use the Adult Reconstruction dataset <cite class="ltx_cite ltx_citemacro_citep">(Sarah et al., <a href="#bib.bib42" title="" class="ltx_ref">2020</a>)</cite>.</span></span></span>, Health (ACSPublicCoverage), and Employment (ACSEmployment). We train two-layer neural network models for all the tasks. We consider the binary notion of sex (i.e., male and female) and multi-value race as the sensitive attribute for every task. Each party has <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="1,000" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.1.cmml"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">1</mn><mo id="S3.SS1.p1.1.m1.2.3.2.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><list id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">1</cn><cn type="integer" id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">1,000</annotation></semantics></math> training points and <math id="S3.SS1.p1.2.m2.2" class="ltx_Math" alttext="2,000" display="inline"><semantics id="S3.SS1.p1.2.m2.2a"><mrow id="S3.SS1.p1.2.m2.2.3.2" xref="S3.SS1.p1.2.m2.2.3.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">2</mn><mo id="S3.SS1.p1.2.m2.2.3.2.1" xref="S3.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.2.m2.2.2" xref="S3.SS1.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.2b"><list id="S3.SS1.p1.2.m2.2.3.1.cmml" xref="S3.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">2</cn><cn type="integer" id="S3.SS1.p1.2.m2.2.2.cmml" xref="S3.SS1.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.2c">2,000</annotation></semantics></math> test points. Note that we use a large test dataset to ensure an accurate estimation of the fairness gap with respect to parities’ test distribution. For CelebA, an image dataset consisting of <math id="S3.SS1.p1.3.m3.2" class="ltx_Math" alttext="200,000" display="inline"><semantics id="S3.SS1.p1.3.m3.2a"><mrow id="S3.SS1.p1.3.m3.2.3.2" xref="S3.SS1.p1.3.m3.2.3.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">200</mn><mo id="S3.SS1.p1.3.m3.2.3.2.1" xref="S3.SS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS1.p1.3.m3.2.2" xref="S3.SS1.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.2b"><list id="S3.SS1.p1.3.m3.2.3.1.cmml" xref="S3.SS1.p1.3.m3.2.3.2"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">200</cn><cn type="integer" id="S3.SS1.p1.3.m3.2.2.cmml" xref="S3.SS1.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.2c">200,000</annotation></semantics></math> celebrity images, we focus on the age prediction task and use the binary attribute "Male" (male and non-male) as the sensitive attribute, which we refer to as the Sex attribute. We partition the data uniformly at random among all parties and train CNN models. We also consider non-IID partitioning on the CelebA dataset, and the results are presented in Appendix <a href="#A2" title="Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. In all experiments, we report the average results over five runs with different random seeds. We use FedML <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>, a PyTorch <cite class="ltx_cite ltx_citemacro_citep">(Paszke et al., <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>-based library for FL, to train models in FL. We also use Captum <cite class="ltx_cite ltx_citemacro_citep">(Kokhlikyan et al., <a href="#bib.bib25" title="" class="ltx_ref">2020</a>)</cite> to compute feature attribution. For further details on the datasets and models, please refer to Appendix <a href="#A1" title="Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Collaboration via FL can worsen fairness issue</h3>

<figure id="S3.SS2.7" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.SS2.7.9.1" class="ltx_text ltx_font_bold">Average benefit of collaboration and FL</span>. The lowest accuracy and highest fairness gap are bold. The standard deviation across five runs is indicated between parentheses. The green arrows (red arrows) represent the positive (negative) impacts of FL or centralized learning compared to standalone training (i.e., FL or centralized training increases the accuracy or decreases the fairness gap compared to standalone training). </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S3.SS2.2.2" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:50pt;vertical-align:-40.5pt;"><span class="ltx_transformed_inner" style="transform:translate(17.5pt,-0.4pt) scale(1.08790103394876,1.08790103394876) ;"><span id="S3.SS2.2.2.3" class="ltx_ERROR undefined">\csvreader</span>
<p id="S3.SS2.2.2.2" class="ltx_p">[tabular=c*3a*3e*3d,table head=   <span id="S3.SS2.2.2.2.2" class="ltx_text" style="background-color:#FFF6F6;"> Accuracy    <span id="S3.SS2.2.2.2.2.2" class="ltx_text" style="background-color:#F6FCF2;"> <math id="S3.SS2.1.1.1.1.1.m1.1" class="ltx_Math" style="background-color:#F6FCF2;" alttext="\Delta^{EO}" display="inline"><semantics id="S3.SS2.1.1.1.1.1.m1.1a"><msup id="S3.SS2.1.1.1.1.1.m1.1.1" xref="S3.SS2.1.1.1.1.1.m1.1.1.cmml"><mi mathbackground="#F6FCF2" mathvariant="normal" id="S3.SS2.1.1.1.1.1.m1.1.1.2" xref="S3.SS2.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mrow id="S3.SS2.1.1.1.1.1.m1.1.1.3" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.cmml"><mi mathbackground="#F6FCF2" id="S3.SS2.1.1.1.1.1.m1.1.1.3.2" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS2.1.1.1.1.1.m1.1.1.3.1" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#F6FCF2" id="S3.SS2.1.1.1.1.1.m1.1.1.3.3" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.3.cmml">O</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.1.1.1.1.1.m1.1b"><apply id="S3.SS2.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1.2">Δ</ci><apply id="S3.SS2.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1.3"><times id="S3.SS2.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.SS2.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.2">𝐸</ci><ci id="S3.SS2.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.1.1.1.1.1.m1.1c">\Delta^{EO}</annotation></semantics></math>    <math id="S3.SS2.2.2.2.2.2.m2.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="S3.SS2.2.2.2.2.2.m2.1a"><msup id="S3.SS2.2.2.2.2.2.m2.1.1" xref="S3.SS2.2.2.2.2.2.m2.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="S3.SS2.2.2.2.2.2.m2.1.1.2" xref="S3.SS2.2.2.2.2.2.m2.1.1.2.cmml">Δ</mi><mrow id="S3.SS2.2.2.2.2.2.m2.1.1.3" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS2.2.2.2.2.2.m2.1.1.3.2" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS2.2.2.2.2.2.m2.1.1.3.1" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="S3.SS2.2.2.2.2.2.m2.1.1.3.3" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.2.2.2.2.2.m2.1b"><apply id="S3.SS2.2.2.2.2.2.m2.1.1.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.2.2.2.2.2.m2.1.1.1.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.2.2.2.2.2.m2.1.1.2.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1.2">Δ</ci><apply id="S3.SS2.2.2.2.2.2.m2.1.1.3.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1.3"><times id="S3.SS2.2.2.2.2.2.m2.1.1.3.1.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.1"></times><ci id="S3.SS2.2.2.2.2.2.m2.1.1.3.2.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.2">𝐷</ci><ci id="S3.SS2.2.2.2.2.2.m2.1.1.3.3.cmml" xref="S3.SS2.2.2.2.2.2.m2.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.2.2.2.2.2.m2.1c">\Delta^{DP}</annotation></semantics></math><span id="S3.SS2.2.2.2.2.2.1" class="ltx_text" style="background-color:#F5FAFC;">  
<br class="ltx_break">Dataset  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg  Standalone  Centralized  FedAvg 
<br class="ltx_break">, table foot=]data/data_benefit.csv
<span id="S3.SS2.2.2.2.2.2.1.1" class="ltx_ERROR undefined">\csvcoli</span>- <span id="S3.SS2.2.2.2.2.2.1.2" class="ltx_ERROR undefined">\csvcolii</span></span></span></span></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.SS2.7.7" class="ltx_p ltx_figure_panel"><span id="S3.SS2.7.7.5" class="ltx_text" style="background-color:#F5FAFC;">Table <a href="#S3.SS2" title="3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> presents the average accuracy and fairness gaps of the centralized model, FL model, and standalone models on local datasets. Centralized training is observed to improve accuracy and fairness at the same time for parties. For instance, on the Income dataset, centralized training improves the accuracy by <math id="S3.SS2.3.3.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="6\%" display="inline"><semantics id="S3.SS2.3.3.1.m1.1a"><mrow id="S3.SS2.3.3.1.m1.1.1" xref="S3.SS2.3.3.1.m1.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS2.3.3.1.m1.1.1.2" xref="S3.SS2.3.3.1.m1.1.1.2.cmml">6</mn><mo mathbackground="#F5FAFC" id="S3.SS2.3.3.1.m1.1.1.1" xref="S3.SS2.3.3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.3.3.1.m1.1b"><apply id="S3.SS2.3.3.1.m1.1.1.cmml" xref="S3.SS2.3.3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.3.3.1.m1.1.1.1.cmml" xref="S3.SS2.3.3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.3.3.1.m1.1.1.2.cmml" xref="S3.SS2.3.3.1.m1.1.1.2">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.3.3.1.m1.1c">6\%</annotation></semantics></math> and reduces the fairness gap across racial groups by <math id="S3.SS2.4.4.2.m2.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="5.5\%" display="inline"><semantics id="S3.SS2.4.4.2.m2.1a"><mrow id="S3.SS2.4.4.2.m2.1.1" xref="S3.SS2.4.4.2.m2.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS2.4.4.2.m2.1.1.2" xref="S3.SS2.4.4.2.m2.1.1.2.cmml">5.5</mn><mo mathbackground="#F5FAFC" id="S3.SS2.4.4.2.m2.1.1.1" xref="S3.SS2.4.4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.4.4.2.m2.1b"><apply id="S3.SS2.4.4.2.m2.1.1.cmml" xref="S3.SS2.4.4.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.4.4.2.m2.1.1.1.cmml" xref="S3.SS2.4.4.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.4.4.2.m2.1.1.2.cmml" xref="S3.SS2.4.4.2.m2.1.1.2">5.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.4.4.2.m2.1c">5.5\%</annotation></semantics></math> with respect to equalized odds and by <math id="S3.SS2.5.5.3.m3.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="5.5\%" display="inline"><semantics id="S3.SS2.5.5.3.m3.1a"><mrow id="S3.SS2.5.5.3.m3.1.1" xref="S3.SS2.5.5.3.m3.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS2.5.5.3.m3.1.1.2" xref="S3.SS2.5.5.3.m3.1.1.2.cmml">5.5</mn><mo mathbackground="#F5FAFC" id="S3.SS2.5.5.3.m3.1.1.1" xref="S3.SS2.5.5.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.5.5.3.m3.1b"><apply id="S3.SS2.5.5.3.m3.1.1.cmml" xref="S3.SS2.5.5.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.5.5.3.m3.1.1.1.cmml" xref="S3.SS2.5.5.3.m3.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.5.5.3.m3.1.1.2.cmml" xref="S3.SS2.5.5.3.m3.1.1.2">5.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.5.5.3.m3.1c">5.5\%</annotation></semantics></math> with respect to demographic parity. However, FL may not achieve the same benefit as centralized training in terms of fairness and can even exacerbate the fairness issue for parties. For example, on the Income dataset, the EO fairness gap of the FL model for the sex groups increases by <math id="S3.SS2.6.6.4.m4.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="35\%" display="inline"><semantics id="S3.SS2.6.6.4.m4.1a"><mrow id="S3.SS2.6.6.4.m4.1.1" xref="S3.SS2.6.6.4.m4.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS2.6.6.4.m4.1.1.2" xref="S3.SS2.6.6.4.m4.1.1.2.cmml">35</mn><mo mathbackground="#F5FAFC" id="S3.SS2.6.6.4.m4.1.1.1" xref="S3.SS2.6.6.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.6.6.4.m4.1b"><apply id="S3.SS2.6.6.4.m4.1.1.cmml" xref="S3.SS2.6.6.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.6.6.4.m4.1.1.1.cmml" xref="S3.SS2.6.6.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.6.6.4.m4.1.1.2.cmml" xref="S3.SS2.6.6.4.m4.1.1.2">35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.6.6.4.m4.1c">35\%</annotation></semantics></math>, and the DP fairness gap increases by <math id="S3.SS2.7.7.5.m5.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="29.9\%" display="inline"><semantics id="S3.SS2.7.7.5.m5.1a"><mrow id="S3.SS2.7.7.5.m5.1.1" xref="S3.SS2.7.7.5.m5.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS2.7.7.5.m5.1.1.2" xref="S3.SS2.7.7.5.m5.1.1.2.cmml">29.9</mn><mo mathbackground="#F5FAFC" id="S3.SS2.7.7.5.m5.1.1.1" xref="S3.SS2.7.7.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.7.7.5.m5.1b"><apply id="S3.SS2.7.7.5.m5.1.1.cmml" xref="S3.SS2.7.7.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.7.7.5.m5.1.1.1.cmml" xref="S3.SS2.7.7.5.m5.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.7.7.5.m5.1.1.2.cmml" xref="S3.SS2.7.7.5.m5.1.1.2">29.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.7.7.5.m5.1c">29.9\%</annotation></semantics></math>, compared to standalone models. We include results for other popular FL algorithms, including FedNova <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>, Scaffold <cite class="ltx_cite ltx_citemacro_cite">Karimireddy et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020b</a>)</cite>, FedOpt <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite>, and Mime <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al., <a href="#bib.bib23" title="" class="ltx_ref">2020a</a>)</cite>, in Table <a href="#A2.T2" title="Table 2 ‣ B.5 Effect of FL algorithm ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> in Appendix <a href="#A2" title="Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>, which show a similar pattern to that of the FedAvg algorithm. Appendix <a href="#A2" title="Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> provides more detailed results on other FL algorithms.
The centralized and FL models are trained on the same dataset. The difference between the FL model and the centralized model in terms of fairness suggests FL algorithm can introduce more bias compared to standard training. Therefore, the explanation of how bias is introduced during standard training in the centralized setting may not fully explain how FL introduces bias. In the following, we further explore how FL impacts fairness from the party level.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S3.SS3" class="ltx_subsection ltx_figure_panel" style="background-color:#F5FAFC;">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>FL propagates bias among parties</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Disparate impact of FL on group fairness across parties. </span>
Figure <a href="#S3.F1" title="Figure 1 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the benefit of FL on fairness and accuracy for parties. We observe that FL improves accuracy for almost all parties, and the variance in accuracy improvement across parties is small. On the other hand, the fairness benefit of FL is negative for most parties. It implies that most parties obtain a more biased model in FL compared to standalone training. Furthermore, we notice that the variance in the fairness benefits across parties is large, suggesting that although all parties have the same global model in FL, they do not benefit from the FL model equally (each party evaluates the model performance on their local test dataset).</p>
</div>
<figure id="S3.F1" class="ltx_figure ltx_align_floatright"><img src="/html/2309.02160/assets/images/main_png/income_benefit_box_all.png" id="S3.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="209" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S3.F1.3.1" class="ltx_text ltx_font_bold">Accuracy and Fairness Benefit of FL - Income (Sex).</span> The benefit of FL is the increase in accuracy or reduction in the fairness gap of the FL model compared to standalone training.
</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">To explore the impact of FL on fairness at the party level, Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows strong correlations between the fairness benefit a party obtains in FL and the fairness gap of the standalone model for the party, i.e., the bias level of the party. This finding highlights the disparate impact of FL on fairness: FL can improve fairness for more biased parties but at the cost of worsening the issue for less biased parties.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2309.02160/assets/images/main_png/income_bias_propagation.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.4.1" class="ltx_text ltx_font_bold">Correlation between the fairness gap of the standalone model and the benefit of FL - Income.</span> The x-axis shows the fairness gap of the standalone model, and the y-axis shows the fairness benefit of FL, which is the fairness gap of the standalone model subtracted by that of the FL model (defined in Section <a href="#S2" title="2 Background ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The Pearson correlation coefficients between the fairness gap of the parties’ standalone models and the fairness benefit they obtain from FL are presented. The p-value for all settings is smaller than <math id="S3.F2.2.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.0001" display="inline"><semantics id="S3.F2.2.m1.1b"><mn mathbackground="#F5FAFC" id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><cn type="float" id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">0.0001</annotation></semantics></math>.
</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2309.02160/assets/images/main_png/income_effect_local_agg.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S3.F3.2.1" class="ltx_text ltx_font_bold">Dynamic of fairness gap during the training - Income (Sex, DP). </span> Figure shows the fairness gap of the global model and locally updated model for the most biased party and least biased party in the first 30 rounds (Figure <a href="#A2.F16" title="Figure 16 ‣ B.2 Aggregation contradicts with local update ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> in Appendix <a href="#A2" title="Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> shows the results for all training rounds.). The most (least) biased party has the highest (lowest) fairness gap in the standalone setting. The fairness gap in the standalone setting for those parties is shown in the title.
</figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Contradiction between aggregation and local update.</span>
During the training process of FL, we observe that aggregation and local update contradict each other, shown in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Local update from the least biased party, whose standalone model has the lowest fairness gap, reduces the fairness gap of the model. However, this reduction is eliminated by the aggregation step. Conversely, the local update from the most biased party increases the fairness gap of the model, which is then reduced by aggregation. This finding implies that the aggregation contributes to the disparate impact of FL on fairness, improving fairness for more biased parties but worsening the fairness for less biased parties compared to the standalone setting.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="120" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp.png" id="S3.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="122" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S3.F4.2.1" class="ltx_text ltx_font_bold">Influence graphs - Income (DP). </span> The figure shows the most influential pairs of parties (with respect to demographic parity), with nodes on the top representing parties that influence other parties and the nodes on the bottom representing parties that are influenced by others. The number inside the node represents the party index, and the color of the node represents the fairness gap in the standalone setting. Green edges (resp. red edges) connect pairs of parties where the top party positively (negatively) influences the bottom party. We show the top 5 pairs with maximal positive influence and the top 5 pairs with maximal negative influence. </figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="/html/2309.02160/assets/images/main_png/income_influence_standalone_dp.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="247" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S3.F6.1.2.1" class="ltx_text ltx_font_bold">Correlation between the influence and standalone bias - Income (DP).</span> The y-axis represents the average influence of each party. The Pearson correlation coefficient between the fairness gap a party obtains in the standalone setting and the impact she has on local fairness for parties in FL are shown in the figure.
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:208.1pt;"><img src="/html/2309.02160/assets/images/main_png/income_cum_influence.png" id="S3.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="241" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S3.F6.2.2.1" class="ltx_text ltx_font_bold">Cumulative influence - Income (DP). </span> The y-axis shows the cumulative influence of each party on all parties up until the current round. The results for the top five most biased parties (parties with the highest fairness gap) and the top five least biased parties (parties with the lowest fairness gap) are presented. </figcaption>
</figure>
</div>
</div>
</figure>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.12" class="ltx_p"><span id="S3.SS3.p4.12.1" class="ltx_text ltx_font_bold">Biased parties negatively influence other parties via aggregation throughout the training. </span>
Why does aggregation have disparate impacts on local fairness for parties? Specifically, we ask which party’s local update causes the increase (or decrease) of the fairness gap for other parties via aggregation. To answer this question, we look into the influence of a party’s local update on other parties’ fairness via aggregation. More precisely, we compute the influence of party <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="i" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">i</annotation></semantics></math> on party <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="j" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">j</annotation></semantics></math> as the fairness gap increase when party <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="i" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><ci id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">i</annotation></semantics></math>’s local update is removed from the aggregation in each round <math id="S3.SS3.p4.4.m4.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="t" display="inline"><semantics id="S3.SS3.p4.4.m4.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">t</annotation></semantics></math> and sum over all the training rounds. Formally, we define the influence of party <math id="S3.SS3.p4.5.m5.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="i" display="inline"><semantics id="S3.SS3.p4.5.m5.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.5.m5.1.1" xref="S3.SS3.p4.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.5.m5.1b"><ci id="S3.SS3.p4.5.m5.1.1.cmml" xref="S3.SS3.p4.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.5.m5.1c">i</annotation></semantics></math> on party <math id="S3.SS3.p4.6.m6.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="j" display="inline"><semantics id="S3.SS3.p4.6.m6.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.6.m6.1.1" xref="S3.SS3.p4.6.m6.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.6.m6.1b"><ci id="S3.SS3.p4.6.m6.1.1.cmml" xref="S3.SS3.p4.6.m6.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.6.m6.1c">j</annotation></semantics></math> as <math id="S3.SS3.p4.7.m7.8" class="ltx_Math" style="background-color:#F5FAFC;" alttext="I_{i,j}=\sum_{t=1}^{T}\Delta(\theta_{t,-i},D_{j})-\Delta(\theta_{t},D_{j})" display="inline"><semantics id="S3.SS3.p4.7.m7.8a"><mrow id="S3.SS3.p4.7.m7.8.8" xref="S3.SS3.p4.7.m7.8.8.cmml"><msub id="S3.SS3.p4.7.m7.8.8.6" xref="S3.SS3.p4.7.m7.8.8.6.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.8.8.6.2" xref="S3.SS3.p4.7.m7.8.8.6.2.cmml">I</mi><mrow id="S3.SS3.p4.7.m7.2.2.2.4" xref="S3.SS3.p4.7.m7.2.2.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.1.1.1.1" xref="S3.SS3.p4.7.m7.1.1.1.1.cmml">i</mi><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.2.2.2.4.1" xref="S3.SS3.p4.7.m7.2.2.2.3.cmml">,</mo><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.2.2.2.2" xref="S3.SS3.p4.7.m7.2.2.2.2.cmml">j</mi></mrow></msub><mo mathbackground="#F5FAFC" rspace="0.111em" id="S3.SS3.p4.7.m7.8.8.5" xref="S3.SS3.p4.7.m7.8.8.5.cmml">=</mo><mrow id="S3.SS3.p4.7.m7.8.8.4" xref="S3.SS3.p4.7.m7.8.8.4.cmml"><mrow id="S3.SS3.p4.7.m7.6.6.2.2" xref="S3.SS3.p4.7.m7.6.6.2.2.cmml"><msubsup id="S3.SS3.p4.7.m7.6.6.2.2.3" xref="S3.SS3.p4.7.m7.6.6.2.2.3.cmml"><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.3.2.2" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.2.cmml">∑</mo><mrow id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2.cmml">t</mi><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1.cmml">=</mo><mn mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3.cmml">1</mn></mrow><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.3.3" xref="S3.SS3.p4.7.m7.6.6.2.2.3.3.cmml">T</mi></msubsup><mrow id="S3.SS3.p4.7.m7.6.6.2.2.2" xref="S3.SS3.p4.7.m7.6.6.2.2.2.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="S3.SS3.p4.7.m7.6.6.2.2.2.4" xref="S3.SS3.p4.7.m7.6.6.2.2.2.4.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.6.6.2.2.2.3" xref="S3.SS3.p4.7.m7.6.6.2.2.2.3.cmml">​</mo><mrow id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml"><mo mathbackground="#F5FAFC" stretchy="false" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.3" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml">(</mo><msub id="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1" xref="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2" xref="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2.cmml">θ</mi><mrow id="S3.SS3.p4.7.m7.4.4.2.2" xref="S3.SS3.p4.7.m7.4.4.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.3.3.1.1" xref="S3.SS3.p4.7.m7.3.3.1.1.cmml">t</mi><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.4.4.2.2.2" xref="S3.SS3.p4.7.m7.4.4.2.3.cmml">,</mo><mrow id="S3.SS3.p4.7.m7.4.4.2.2.1" xref="S3.SS3.p4.7.m7.4.4.2.2.1.cmml"><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.4.4.2.2.1a" xref="S3.SS3.p4.7.m7.4.4.2.2.1.cmml">−</mo><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.4.4.2.2.1.2" xref="S3.SS3.p4.7.m7.4.4.2.2.1.2.cmml">i</mi></mrow></mrow></msub><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.4" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml">,</mo><msub id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2.cmml">D</mi><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3.cmml">j</mi></msub><mo mathbackground="#F5FAFC" stretchy="false" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.5" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.8.8.4.5" xref="S3.SS3.p4.7.m7.8.8.4.5.cmml">−</mo><mrow id="S3.SS3.p4.7.m7.8.8.4.4" xref="S3.SS3.p4.7.m7.8.8.4.4.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="S3.SS3.p4.7.m7.8.8.4.4.4" xref="S3.SS3.p4.7.m7.8.8.4.4.4.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.7.m7.8.8.4.4.3" xref="S3.SS3.p4.7.m7.8.8.4.4.3.cmml">​</mo><mrow id="S3.SS3.p4.7.m7.8.8.4.4.2.2" xref="S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml"><mo mathbackground="#F5FAFC" stretchy="false" id="S3.SS3.p4.7.m7.8.8.4.4.2.2.3" xref="S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml">(</mo><msub id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2.cmml">θ</mi><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3.cmml">t</mi></msub><mo mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.8.8.4.4.2.2.4" xref="S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml">,</mo><msub id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2.cmml">D</mi><mi mathbackground="#F5FAFC" id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3.cmml">j</mi></msub><mo mathbackground="#F5FAFC" stretchy="false" id="S3.SS3.p4.7.m7.8.8.4.4.2.2.5" xref="S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.7.m7.8b"><apply id="S3.SS3.p4.7.m7.8.8.cmml" xref="S3.SS3.p4.7.m7.8.8"><eq id="S3.SS3.p4.7.m7.8.8.5.cmml" xref="S3.SS3.p4.7.m7.8.8.5"></eq><apply id="S3.SS3.p4.7.m7.8.8.6.cmml" xref="S3.SS3.p4.7.m7.8.8.6"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.8.8.6.1.cmml" xref="S3.SS3.p4.7.m7.8.8.6">subscript</csymbol><ci id="S3.SS3.p4.7.m7.8.8.6.2.cmml" xref="S3.SS3.p4.7.m7.8.8.6.2">𝐼</ci><list id="S3.SS3.p4.7.m7.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.2.2.2.4"><ci id="S3.SS3.p4.7.m7.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.1.1.1.1">𝑖</ci><ci id="S3.SS3.p4.7.m7.2.2.2.2.cmml" xref="S3.SS3.p4.7.m7.2.2.2.2">𝑗</ci></list></apply><apply id="S3.SS3.p4.7.m7.8.8.4.cmml" xref="S3.SS3.p4.7.m7.8.8.4"><minus id="S3.SS3.p4.7.m7.8.8.4.5.cmml" xref="S3.SS3.p4.7.m7.8.8.4.5"></minus><apply id="S3.SS3.p4.7.m7.6.6.2.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2"><apply id="S3.SS3.p4.7.m7.6.6.2.2.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.6.6.2.2.3.1.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3">superscript</csymbol><apply id="S3.SS3.p4.7.m7.6.6.2.2.3.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.6.6.2.2.3.2.1.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3">subscript</csymbol><sum id="S3.SS3.p4.7.m7.6.6.2.2.3.2.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.2"></sum><apply id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3"><eq id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.1"></eq><ci id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.2">𝑡</ci><cn type="integer" id="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.SS3.p4.7.m7.6.6.2.2.3.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.3.3">𝑇</ci></apply><apply id="S3.SS3.p4.7.m7.6.6.2.2.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2"><times id="S3.SS3.p4.7.m7.6.6.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.3"></times><ci id="S3.SS3.p4.7.m7.6.6.2.2.2.4.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.4">Δ</ci><interval closure="open" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2"><apply id="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.5.5.1.1.1.1.1.1.2">𝜃</ci><list id="S3.SS3.p4.7.m7.4.4.2.3.cmml" xref="S3.SS3.p4.7.m7.4.4.2.2"><ci id="S3.SS3.p4.7.m7.3.3.1.1.cmml" xref="S3.SS3.p4.7.m7.3.3.1.1">𝑡</ci><apply id="S3.SS3.p4.7.m7.4.4.2.2.1.cmml" xref="S3.SS3.p4.7.m7.4.4.2.2.1"><minus id="S3.SS3.p4.7.m7.4.4.2.2.1.1.cmml" xref="S3.SS3.p4.7.m7.4.4.2.2.1"></minus><ci id="S3.SS3.p4.7.m7.4.4.2.2.1.2.cmml" xref="S3.SS3.p4.7.m7.4.4.2.2.1.2">𝑖</ci></apply></list></apply><apply id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.1.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.2">𝐷</ci><ci id="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.6.6.2.2.2.2.2.2.3">𝑗</ci></apply></interval></apply></apply><apply id="S3.SS3.p4.7.m7.8.8.4.4.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4"><times id="S3.SS3.p4.7.m7.8.8.4.4.3.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.3"></times><ci id="S3.SS3.p4.7.m7.8.8.4.4.4.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.4">Δ</ci><interval closure="open" id="S3.SS3.p4.7.m7.8.8.4.4.2.3.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2"><apply id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.cmml" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.1.cmml" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1">subscript</csymbol><ci id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2.cmml" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.2">𝜃</ci><ci id="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3.cmml" xref="S3.SS3.p4.7.m7.7.7.3.3.1.1.1.3">𝑡</ci></apply><apply id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.1.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2">subscript</csymbol><ci id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.2">𝐷</ci><ci id="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3.cmml" xref="S3.SS3.p4.7.m7.8.8.4.4.2.2.2.3">𝑗</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.7.m7.8c">I_{i,j}=\sum_{t=1}^{T}\Delta(\theta_{t,-i},D_{j})-\Delta(\theta_{t},D_{j})</annotation></semantics></math>, where <math id="S3.SS3.p4.8.m8.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\theta_{t}" display="inline"><semantics id="S3.SS3.p4.8.m8.1a"><msub id="S3.SS3.p4.8.m8.1.1" xref="S3.SS3.p4.8.m8.1.1.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.8.m8.1.1.2" xref="S3.SS3.p4.8.m8.1.1.2.cmml">θ</mi><mi mathbackground="#F5FAFC" id="S3.SS3.p4.8.m8.1.1.3" xref="S3.SS3.p4.8.m8.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.8.m8.1b"><apply id="S3.SS3.p4.8.m8.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.8.m8.1.1.1.cmml" xref="S3.SS3.p4.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.p4.8.m8.1.1.2.cmml" xref="S3.SS3.p4.8.m8.1.1.2">𝜃</ci><ci id="S3.SS3.p4.8.m8.1.1.3.cmml" xref="S3.SS3.p4.8.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.8.m8.1c">\theta_{t}</annotation></semantics></math> is the global model (i.e., the aggregated model overall local updated models) and <math id="S3.SS3.p4.9.m9.2" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\theta_{t,-i}" display="inline"><semantics id="S3.SS3.p4.9.m9.2a"><msub id="S3.SS3.p4.9.m9.2.3" xref="S3.SS3.p4.9.m9.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.9.m9.2.3.2" xref="S3.SS3.p4.9.m9.2.3.2.cmml">θ</mi><mrow id="S3.SS3.p4.9.m9.2.2.2.2" xref="S3.SS3.p4.9.m9.2.2.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.9.m9.1.1.1.1" xref="S3.SS3.p4.9.m9.1.1.1.1.cmml">t</mi><mo mathbackground="#F5FAFC" id="S3.SS3.p4.9.m9.2.2.2.2.2" xref="S3.SS3.p4.9.m9.2.2.2.3.cmml">,</mo><mrow id="S3.SS3.p4.9.m9.2.2.2.2.1" xref="S3.SS3.p4.9.m9.2.2.2.2.1.cmml"><mo mathbackground="#F5FAFC" id="S3.SS3.p4.9.m9.2.2.2.2.1a" xref="S3.SS3.p4.9.m9.2.2.2.2.1.cmml">−</mo><mi mathbackground="#F5FAFC" id="S3.SS3.p4.9.m9.2.2.2.2.1.2" xref="S3.SS3.p4.9.m9.2.2.2.2.1.2.cmml">i</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.9.m9.2b"><apply id="S3.SS3.p4.9.m9.2.3.cmml" xref="S3.SS3.p4.9.m9.2.3"><csymbol cd="ambiguous" id="S3.SS3.p4.9.m9.2.3.1.cmml" xref="S3.SS3.p4.9.m9.2.3">subscript</csymbol><ci id="S3.SS3.p4.9.m9.2.3.2.cmml" xref="S3.SS3.p4.9.m9.2.3.2">𝜃</ci><list id="S3.SS3.p4.9.m9.2.2.2.3.cmml" xref="S3.SS3.p4.9.m9.2.2.2.2"><ci id="S3.SS3.p4.9.m9.1.1.1.1.cmml" xref="S3.SS3.p4.9.m9.1.1.1.1">𝑡</ci><apply id="S3.SS3.p4.9.m9.2.2.2.2.1.cmml" xref="S3.SS3.p4.9.m9.2.2.2.2.1"><minus id="S3.SS3.p4.9.m9.2.2.2.2.1.1.cmml" xref="S3.SS3.p4.9.m9.2.2.2.2.1"></minus><ci id="S3.SS3.p4.9.m9.2.2.2.2.1.2.cmml" xref="S3.SS3.p4.9.m9.2.2.2.2.1.2">𝑖</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.9.m9.2c">\theta_{t,-i}</annotation></semantics></math> is the aggregated model over local updated models from parties excluding party <math id="S3.SS3.p4.10.m10.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="i" display="inline"><semantics id="S3.SS3.p4.10.m10.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.10.m10.1.1" xref="S3.SS3.p4.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.10.m10.1b"><ci id="S3.SS3.p4.10.m10.1.1.cmml" xref="S3.SS3.p4.10.m10.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.10.m10.1c">i</annotation></semantics></math>. If party <math id="S3.SS3.p4.11.m11.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="i" display="inline"><semantics id="S3.SS3.p4.11.m11.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.11.m11.1.1" xref="S3.SS3.p4.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.11.m11.1b"><ci id="S3.SS3.p4.11.m11.1.1.cmml" xref="S3.SS3.p4.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.11.m11.1c">i</annotation></semantics></math> improves fairness for party <math id="S3.SS3.p4.12.m12.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="j" display="inline"><semantics id="S3.SS3.p4.12.m12.1a"><mi mathbackground="#F5FAFC" id="S3.SS3.p4.12.m12.1.1" xref="S3.SS3.p4.12.m12.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.12.m12.1b"><ci id="S3.SS3.p4.12.m12.1.1.cmml" xref="S3.SS3.p4.12.m12.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.12.m12.1c">j</annotation></semantics></math>, the influence is positive and vice versa. We compute the influence for all pairs of parties, and the most influential pairs are shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We observe that a less biased party has a positive influence on fairness for other parties, while a more biased party has a negative influence on other parties. This result shows that a biased party can negatively influence other parties’ fairness via aggregation throughout the training.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p">Furthermore, we investigate the relationship between a party’s bias (i.e., the bias of the party’s standalone model) and its average influence on all parties’ fairness (i.e., <math id="S3.SS3.p5.1.m1.2" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\sum_{k=1}^{K}I_{i,k}/K" display="inline"><semantics id="S3.SS3.p5.1.m1.2a"><mrow id="S3.SS3.p5.1.m1.2.3" xref="S3.SS3.p5.1.m1.2.3.cmml"><msubsup id="S3.SS3.p5.1.m1.2.3.1" xref="S3.SS3.p5.1.m1.2.3.1.cmml"><mo mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.1.2.2" xref="S3.SS3.p5.1.m1.2.3.1.2.2.cmml">∑</mo><mrow id="S3.SS3.p5.1.m1.2.3.1.2.3" xref="S3.SS3.p5.1.m1.2.3.1.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.1.2.3.2" xref="S3.SS3.p5.1.m1.2.3.1.2.3.2.cmml">k</mi><mo mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.1.2.3.1" xref="S3.SS3.p5.1.m1.2.3.1.2.3.1.cmml">=</mo><mn mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.1.2.3.3" xref="S3.SS3.p5.1.m1.2.3.1.2.3.3.cmml">1</mn></mrow><mi mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.1.3" xref="S3.SS3.p5.1.m1.2.3.1.3.cmml">K</mi></msubsup><mrow id="S3.SS3.p5.1.m1.2.3.2" xref="S3.SS3.p5.1.m1.2.3.2.cmml"><msub id="S3.SS3.p5.1.m1.2.3.2.2" xref="S3.SS3.p5.1.m1.2.3.2.2.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.2.2.2" xref="S3.SS3.p5.1.m1.2.3.2.2.2.cmml">I</mi><mrow id="S3.SS3.p5.1.m1.2.2.2.4" xref="S3.SS3.p5.1.m1.2.2.2.3.cmml"><mi mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.cmml">i</mi><mo mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.2.2.4.1" xref="S3.SS3.p5.1.m1.2.2.2.3.cmml">,</mo><mi mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.cmml">k</mi></mrow></msub><mo mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.2.1" xref="S3.SS3.p5.1.m1.2.3.2.1.cmml">/</mo><mi mathbackground="#F5FAFC" id="S3.SS3.p5.1.m1.2.3.2.3" xref="S3.SS3.p5.1.m1.2.3.2.3.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.2b"><apply id="S3.SS3.p5.1.m1.2.3.cmml" xref="S3.SS3.p5.1.m1.2.3"><apply id="S3.SS3.p5.1.m1.2.3.1.cmml" xref="S3.SS3.p5.1.m1.2.3.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.3.1.1.cmml" xref="S3.SS3.p5.1.m1.2.3.1">superscript</csymbol><apply id="S3.SS3.p5.1.m1.2.3.1.2.cmml" xref="S3.SS3.p5.1.m1.2.3.1"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.3.1.2.1.cmml" xref="S3.SS3.p5.1.m1.2.3.1">subscript</csymbol><sum id="S3.SS3.p5.1.m1.2.3.1.2.2.cmml" xref="S3.SS3.p5.1.m1.2.3.1.2.2"></sum><apply id="S3.SS3.p5.1.m1.2.3.1.2.3.cmml" xref="S3.SS3.p5.1.m1.2.3.1.2.3"><eq id="S3.SS3.p5.1.m1.2.3.1.2.3.1.cmml" xref="S3.SS3.p5.1.m1.2.3.1.2.3.1"></eq><ci id="S3.SS3.p5.1.m1.2.3.1.2.3.2.cmml" xref="S3.SS3.p5.1.m1.2.3.1.2.3.2">𝑘</ci><cn type="integer" id="S3.SS3.p5.1.m1.2.3.1.2.3.3.cmml" xref="S3.SS3.p5.1.m1.2.3.1.2.3.3">1</cn></apply></apply><ci id="S3.SS3.p5.1.m1.2.3.1.3.cmml" xref="S3.SS3.p5.1.m1.2.3.1.3">𝐾</ci></apply><apply id="S3.SS3.p5.1.m1.2.3.2.cmml" xref="S3.SS3.p5.1.m1.2.3.2"><divide id="S3.SS3.p5.1.m1.2.3.2.1.cmml" xref="S3.SS3.p5.1.m1.2.3.2.1"></divide><apply id="S3.SS3.p5.1.m1.2.3.2.2.cmml" xref="S3.SS3.p5.1.m1.2.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.2.3.2.2.1.cmml" xref="S3.SS3.p5.1.m1.2.3.2.2">subscript</csymbol><ci id="S3.SS3.p5.1.m1.2.3.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.3.2.2.2">𝐼</ci><list id="S3.SS3.p5.1.m1.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.4"><ci id="S3.SS3.p5.1.m1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1">𝑖</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2">𝑘</ci></list></apply><ci id="S3.SS3.p5.1.m1.2.3.2.3.cmml" xref="S3.SS3.p5.1.m1.2.3.2.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.2c">\sum_{k=1}^{K}I_{i,k}/K</annotation></semantics></math>). The results are presented in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, which shows a strong correlation between these two factors. This finding further supports our conclusion that the more biased parties have a stronger negative influence on other parties’ fairness, while the less biased parties have a stronger positive influence.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para ltx_noindent">
<p id="S3.SS3.p6.1" class="ltx_p">Finally, we analyze the dynamics of the influence of the top 5 most biased parties (i.e., parties with the largest fairness gap in the standalone setting) and the top 5 least biased parties, as shown in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We observe that the influence of the most biased parties is monotonically increasing throughout the training, while the influence of the least biased parties is decreasing. In other words, biased parties consistently have a negative influence on others’ fairness gaps, while less biased parties have a positive influence. These results suggest that FL can propagate bias among parties: the bias from biased parties negatively influence the fairness of other parties via aggregation throughout the training. Next, we will explore how the bias is propagated in FL.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2309.02160/assets/images/main_png/income_attribution_distribution.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S3.F7.2.1" class="ltx_text ltx_font_bold">Histogram of feature attribution value for the sensitive attribute - Income (Sex).</span> The average attribution value for the female group and male group is shown in the figure. The results are computed on all test data points over five different runs. We show the results for the most biased party with the highest fairness gap in the standalone setting.
</figcaption>
</figure>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>How is bias propagated in FL?</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p"><span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_bold">Disparate treatment causes large fairness gaps.</span>
Our first investigation explores what the bias represents, specifically whether the bias increase in FL is directly caused by the disparate treatment of the model among sensitive groups. To answer this question, we utilize Integrated Gradients <cite class="ltx_cite ltx_citemacro_citep">(Sundararajan et al., <a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite> to measure the attribution of each input feature to the models’ predictions with respect to the positive class.
Figure <a href="#S3.F7" title="Figure 7 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the attribution value distribution for the sensitive attribute "Sex" over individual test points from the female and male groups. We notice that the sex attribute has a large attribution value for the standalone model’s predictions and the FL model’s predictions. This finding implies that the predictions of those models are heavily dependent on the sensitive attribute of the test data. Moreover, the sensitive attribute affects the model predictions differently for the male and female groups, with the average attribution value being positive for the male group and negative for the female group. Furthermore, we find that there is minimal difference in the attribution value for other attributes with respect to the female and male groups (see Figure <a href="#A2.F19" title="Figure 19 ‣ B.4 Attribution value for sensitive attribute ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> in Appendix <a href="#A2" title="Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>). This indicates that the large fairness gap in the models is not caused by the distinct distribution of other insensitive attributes over protected groups; rather, it is mainly caused by the models’ disparate treatment of protected groups.</p>
</div>
<figure id="S3.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:212.5pt;"><img src="/html/2309.02160/assets/images/main_png/income_attribution_dynamic.png" id="S3.F9.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="S3.F9.1.2.1" class="ltx_text ltx_font_bold">Dynamic of absolute attribution value for the sensitive attribute - Income (Sex).</span> The results of different models are shown in different lines. We present the results for the most biased party and the least biased party.
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:212.5pt;"><img src="/html/2309.02160/assets/images/main_png/income_attribution_local_agg.png" id="S3.F9.2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="245" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S3.F9.2.2.1" class="ltx_text ltx_font_bold">Effect of local update and aggregation on the attribution value - Income (Sex).</span> We show the attribution value for sex attribute before and after aggregation for the most biased party and the least biased party during the training. </figcaption>
</figure>
</div>
</div>
</figure>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">FL model learns more biased patterns. </span>
In Figure <a href="#S3.F7" title="Figure 7 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we compare the attribution value of “Sex” for different models and find that, while the predictions of the FL model depend less heavily on the sensitive attribute compared to those of the standalone model, the dependence is still stronger than that of the centralized model. This suggests that the FL model learns a more biased pattern compared to what could be learned in the centralized setting.
Figure <a href="#S3.F9" title="Figure 9 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the dynamic of the average absolute feature attribution for “Sex” during the training. We find that standalone training increases the model’s dependence on the sensitive attribute throughout training for the most biased party, but centralized training decreases it. This suggests that collaboration through centralized training improves local fairness by guiding the model to learn less biased features. In FL, however, the absolute attribution value barely changes after 100 rounds and remains significantly greater than that in the centralized setting. This implies that the FL algorithm may introduce bias to the final model by inhibiting the model from learning less biased features.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Biased parties increase the model dependence on the sensitive attribute. </span>
Figure <a href="#S3.F9" title="Figure 9 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the attribution value of the aggregated model and locally updated model from the most biased party and least biased party. We observe that the biased party increases the global model dependence on sensitive attributes during the local update, and this increase persists throughout the training. In contrast, the least biased party reduces the dependence on the sensitive attribute, which is aligned with the trend of the fairness gap in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This suggests that the biased parties have a negative impact on the fairness of other parties by increasing the model’s dependence on the sensitive attribute.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p"><span id="S3.SS4.p4.1.1" class="ltx_text ltx_font_bold">Bias is encoded in a few parameters. </span></p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2309.02160/assets/images/main_png/income_effect_weights.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="S3.F10.5.1" class="ltx_text ltx_font_bold">Effect of a few parameters on fairness - Income (Sex).</span> <span id="S3.F10.6.2" class="ltx_text ltx_font_italic">(a) Effect of local update and aggregation on the parameter values:</span> The y-axis shows the norm of parameters that are directly computed on the sensitive attribute, normalized by the parameter norm of the first layer. We show the dynamic of this parameter norm during FL for the aggregated model and locally updated model for the parties who benefit most from FL and suffer the most from FL (represented by the line with “Benefit” and “Suffer” respectively). <span id="S3.F10.7.3" class="ltx_text ltx_font_italic">(b) Correlation between standalone bias and parameter values:</span> The x-axis shows the bias a party receives in the standalone setting, and the y-axis shows the normalized norm of parameters (associated with sex attribute) for the locally updated model in the last round. <span id="S3.F10.8.4" class="ltx_text ltx_font_italic">(c) Effect of scaling the parameter values:</span> The figure shows the model performance in terms of accuracy and fairness gap when the parameter values (associated with sex attribute) are multiplied by a scaling factor.</figcaption>
</figure>
<div id="S3.SS4.p5" class="ltx_para ltx_noindent">
<p id="S3.SS4.p5.1" class="ltx_p">Biased parties increase the model dependence on the sensitive attribute. But how does this increase propagate to other parties in the network? Since parties share the model parameters of the local model with the server, the bias is likely encoded in the model parameters. Therefore, we investigate which parameters are related to the model’s bias. Intuitively, the parameters used to extract sensitive attribute information impact the attribution value of the sensitive attribute to the model prediction. If the absolute value (signal) of those parameters is substantial, the value of the sensitive attribute will significantly affect the model’s prediction. In our evaluation, the sensitive attribute is part of the input feature, so the parameters directly applied to the “Sex” attribute in the first layer of the neural network should contribute to the model’s bias.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para ltx_noindent">
<p id="S3.SS4.p6.1" class="ltx_p">Figure <a href="#S3.F10" title="Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(a) shows the normalized norm of the parameters associated with the sensitive attribute for the most and least biased parties in the aggregated and locally updated models. The normalized norm is defined as the norm of parameters associated with the sensitive attribute divided by the parameter norm of the first layer. We observe that the least biased party reduces the parameter norm, hence decreasing the model’s sensitivity to the sensitive attribute. On the other hand, the biased party increases the norm for those parameters, amplifying the impact of the sensitive attribute on the model’s prediction. Through aggregation, this amplification will be propagated to the global model.
Figure <a href="#S3.F10" title="Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(b) reveals a moderate correlation between the fairness gap party experiences in the standalone setting and the normalized parameter norm for the parameters used to extract sensitive attribute information. This implies that biased parties increase the parameter value associated with sensitive attributes during the local update, thereby boosting the model’s susceptibility to the sensitive attribute.</p>
</div>
<div id="S3.SS4.p7" class="ltx_para ltx_noindent">
<p id="S3.SS4.p7.18" class="ltx_p"><span id="S3.SS4.p7.18.1" class="ltx_text ltx_font_bold">Controlling fairness gap by scaling a few parameters. </span>
To further investigate the impact of the parameters associated with the sensitive attribute (i.e., <math id="S3.SS4.p7.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="104" display="inline"><semantics id="S3.SS4.p7.1.m1.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.1.m1.1.1" xref="S3.SS4.p7.1.m1.1.1.cmml">104</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.1.m1.1b"><cn type="integer" id="S3.SS4.p7.1.m1.1.1.cmml" xref="S3.SS4.p7.1.m1.1.1">104</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.1.m1.1c">104</annotation></semantics></math> parameters out of <math id="S3.SS4.p7.2.m2.2" class="ltx_Math" style="background-color:#F5FAFC;" alttext="1,792" display="inline"><semantics id="S3.SS4.p7.2.m2.2a"><mrow id="S3.SS4.p7.2.m2.2.3.2" xref="S3.SS4.p7.2.m2.2.3.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.2.m2.1.1" xref="S3.SS4.p7.2.m2.1.1.cmml">1</mn><mo mathbackground="#F5FAFC" id="S3.SS4.p7.2.m2.2.3.2.1" xref="S3.SS4.p7.2.m2.2.3.1.cmml">,</mo><mn mathbackground="#F5FAFC" id="S3.SS4.p7.2.m2.2.2" xref="S3.SS4.p7.2.m2.2.2.cmml">792</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.2.m2.2b"><list id="S3.SS4.p7.2.m2.2.3.1.cmml" xref="S3.SS4.p7.2.m2.2.3.2"><cn type="integer" id="S3.SS4.p7.2.m2.1.1.cmml" xref="S3.SS4.p7.2.m2.1.1">1</cn><cn type="integer" id="S3.SS4.p7.2.m2.2.2.cmml" xref="S3.SS4.p7.2.m2.2.2">792</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.2.m2.2c">1,792</annotation></semantics></math>) on model fairness, we examine the effect of scaling these parameters on the fairness gap in Figure <a href="#S3.F10" title="Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>(c). We find that the fairness gap can be greatly widened or narrowed at the expense of a moderate degree of accuracy. Specifically, by scaling the parameter value by <math id="S3.SS4.p7.3.m3.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.1" display="inline"><semantics id="S3.SS4.p7.3.m3.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.3.m3.1.1" xref="S3.SS4.p7.3.m3.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.3.m3.1b"><cn type="float" id="S3.SS4.p7.3.m3.1.1.cmml" xref="S3.SS4.p7.3.m3.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.3.m3.1c">0.1</annotation></semantics></math> for the trained FL model, we significantly reduce the EO gap by <math id="S3.SS4.p7.4.m4.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="74.6\%" display="inline"><semantics id="S3.SS4.p7.4.m4.1a"><mrow id="S3.SS4.p7.4.m4.1.1" xref="S3.SS4.p7.4.m4.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.4.m4.1.1.2" xref="S3.SS4.p7.4.m4.1.1.2.cmml">74.6</mn><mo mathbackground="#F5FAFC" id="S3.SS4.p7.4.m4.1.1.1" xref="S3.SS4.p7.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.4.m4.1b"><apply id="S3.SS4.p7.4.m4.1.1.cmml" xref="S3.SS4.p7.4.m4.1.1"><csymbol cd="latexml" id="S3.SS4.p7.4.m4.1.1.1.cmml" xref="S3.SS4.p7.4.m4.1.1.1">percent</csymbol><cn type="float" id="S3.SS4.p7.4.m4.1.1.2.cmml" xref="S3.SS4.p7.4.m4.1.1.2">74.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.4.m4.1c">74.6\%</annotation></semantics></math> (from <math id="S3.SS4.p7.5.m5.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.198" display="inline"><semantics id="S3.SS4.p7.5.m5.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.5.m5.1.1" xref="S3.SS4.p7.5.m5.1.1.cmml">0.198</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.5.m5.1b"><cn type="float" id="S3.SS4.p7.5.m5.1.1.cmml" xref="S3.SS4.p7.5.m5.1.1">0.198</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.5.m5.1c">0.198</annotation></semantics></math> to <math id="S3.SS4.p7.6.m6.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.05" display="inline"><semantics id="S3.SS4.p7.6.m6.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.6.m6.1.1" xref="S3.SS4.p7.6.m6.1.1.cmml">0.05</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.6.m6.1b"><cn type="float" id="S3.SS4.p7.6.m6.1.1.cmml" xref="S3.SS4.p7.6.m6.1.1">0.05</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.6.m6.1c">0.05</annotation></semantics></math>) and the DP gap by <math id="S3.SS4.p7.7.m7.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="69.1\%" display="inline"><semantics id="S3.SS4.p7.7.m7.1a"><mrow id="S3.SS4.p7.7.m7.1.1" xref="S3.SS4.p7.7.m7.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.7.m7.1.1.2" xref="S3.SS4.p7.7.m7.1.1.2.cmml">69.1</mn><mo mathbackground="#F5FAFC" id="S3.SS4.p7.7.m7.1.1.1" xref="S3.SS4.p7.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.7.m7.1b"><apply id="S3.SS4.p7.7.m7.1.1.cmml" xref="S3.SS4.p7.7.m7.1.1"><csymbol cd="latexml" id="S3.SS4.p7.7.m7.1.1.1.cmml" xref="S3.SS4.p7.7.m7.1.1.1">percent</csymbol><cn type="float" id="S3.SS4.p7.7.m7.1.1.2.cmml" xref="S3.SS4.p7.7.m7.1.1.2">69.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.7.m7.1c">69.1\%</annotation></semantics></math> (from <math id="S3.SS4.p7.8.m8.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.217" display="inline"><semantics id="S3.SS4.p7.8.m8.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.8.m8.1.1" xref="S3.SS4.p7.8.m8.1.1.cmml">0.217</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.8.m8.1b"><cn type="float" id="S3.SS4.p7.8.m8.1.1.cmml" xref="S3.SS4.p7.8.m8.1.1">0.217</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.8.m8.1c">0.217</annotation></semantics></math> to <math id="S3.SS4.p7.9.m9.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.067" display="inline"><semantics id="S3.SS4.p7.9.m9.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.9.m9.1.1" xref="S3.SS4.p7.9.m9.1.1.cmml">0.067</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.9.m9.1b"><cn type="float" id="S3.SS4.p7.9.m9.1.1.cmml" xref="S3.SS4.p7.9.m9.1.1">0.067</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.9.m9.1c">0.067</annotation></semantics></math>) with just a moderate accuracy loss of <math id="S3.SS4.p7.10.m10.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.8\%" display="inline"><semantics id="S3.SS4.p7.10.m10.1a"><mrow id="S3.SS4.p7.10.m10.1.1" xref="S3.SS4.p7.10.m10.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.10.m10.1.1.2" xref="S3.SS4.p7.10.m10.1.1.2.cmml">0.8</mn><mo mathbackground="#F5FAFC" id="S3.SS4.p7.10.m10.1.1.1" xref="S3.SS4.p7.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.10.m10.1b"><apply id="S3.SS4.p7.10.m10.1.1.cmml" xref="S3.SS4.p7.10.m10.1.1"><csymbol cd="latexml" id="S3.SS4.p7.10.m10.1.1.1.cmml" xref="S3.SS4.p7.10.m10.1.1.1">percent</csymbol><cn type="float" id="S3.SS4.p7.10.m10.1.1.2.cmml" xref="S3.SS4.p7.10.m10.1.1.2">0.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.10.m10.1c">0.8\%</annotation></semantics></math> (from <math id="S3.SS4.p7.11.m11.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.785" display="inline"><semantics id="S3.SS4.p7.11.m11.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.11.m11.1.1" xref="S3.SS4.p7.11.m11.1.1.cmml">0.785</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.11.m11.1b"><cn type="float" id="S3.SS4.p7.11.m11.1.1.cmml" xref="S3.SS4.p7.11.m11.1.1">0.785</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.11.m11.1c">0.785</annotation></semantics></math> to <math id="S3.SS4.p7.12.m12.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.779" display="inline"><semantics id="S3.SS4.p7.12.m12.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.12.m12.1.1" xref="S3.SS4.p7.12.m12.1.1.cmml">0.779</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.12.m12.1b"><cn type="float" id="S3.SS4.p7.12.m12.1.1.cmml" xref="S3.SS4.p7.12.m12.1.1">0.779</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.12.m12.1c">0.779</annotation></semantics></math>). In contrast, scaling the same set of parameters by a factor of 10 increases the EO gap to <math id="S3.SS4.p7.13.m13.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.96" display="inline"><semantics id="S3.SS4.p7.13.m13.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.13.m13.1.1" xref="S3.SS4.p7.13.m13.1.1.cmml">0.96</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.13.m13.1b"><cn type="float" id="S3.SS4.p7.13.m13.1.1.cmml" xref="S3.SS4.p7.13.m13.1.1">0.96</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.13.m13.1c">0.96</annotation></semantics></math> (the maximal fairness gap is <math id="S3.SS4.p7.14.m14.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="1" display="inline"><semantics id="S3.SS4.p7.14.m14.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.14.m14.1.1" xref="S3.SS4.p7.14.m14.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.14.m14.1b"><cn type="integer" id="S3.SS4.p7.14.m14.1.1.cmml" xref="S3.SS4.p7.14.m14.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.14.m14.1c">1</annotation></semantics></math>), which is almost five times larger, and increases the DP gap by <math id="S3.SS4.p7.15.m15.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="259\%" display="inline"><semantics id="S3.SS4.p7.15.m15.1a"><mrow id="S3.SS4.p7.15.m15.1.1" xref="S3.SS4.p7.15.m15.1.1.cmml"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.15.m15.1.1.2" xref="S3.SS4.p7.15.m15.1.1.2.cmml">259</mn><mo mathbackground="#F5FAFC" id="S3.SS4.p7.15.m15.1.1.1" xref="S3.SS4.p7.15.m15.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.15.m15.1b"><apply id="S3.SS4.p7.15.m15.1.1.cmml" xref="S3.SS4.p7.15.m15.1.1"><csymbol cd="latexml" id="S3.SS4.p7.15.m15.1.1.1.cmml" xref="S3.SS4.p7.15.m15.1.1.1">percent</csymbol><cn type="integer" id="S3.SS4.p7.15.m15.1.1.2.cmml" xref="S3.SS4.p7.15.m15.1.1.2">259</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.15.m15.1c">259\%</annotation></semantics></math> to <math id="S3.SS4.p7.16.m16.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.78" display="inline"><semantics id="S3.SS4.p7.16.m16.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.16.m16.1.1" xref="S3.SS4.p7.16.m16.1.1.cmml">0.78</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.16.m16.1b"><cn type="float" id="S3.SS4.p7.16.m16.1.1.cmml" xref="S3.SS4.p7.16.m16.1.1">0.78</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.16.m16.1c">0.78</annotation></semantics></math>, while reducing the accuracy from <math id="S3.SS4.p7.17.m17.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.785" display="inline"><semantics id="S3.SS4.p7.17.m17.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.17.m17.1.1" xref="S3.SS4.p7.17.m17.1.1.cmml">0.785</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.17.m17.1b"><cn type="float" id="S3.SS4.p7.17.m17.1.1.cmml" xref="S3.SS4.p7.17.m17.1.1">0.785</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.17.m17.1c">0.785</annotation></semantics></math> to <math id="S3.SS4.p7.18.m18.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="0.67" display="inline"><semantics id="S3.SS4.p7.18.m18.1a"><mn mathbackground="#F5FAFC" id="S3.SS4.p7.18.m18.1.1" xref="S3.SS4.p7.18.m18.1.1.cmml">0.67</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p7.18.m18.1b"><cn type="float" id="S3.SS4.p7.18.m18.1.1.cmml" xref="S3.SS4.p7.18.m18.1.1">0.67</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p7.18.m18.1c">0.67</annotation></semantics></math>. These findings explain how bias is propagated in FL: biased parties magnify the impact of sensitive attributes on model predictions by increasing the model parameter used to extract sensitive attributes. This rise in parameters is subsequently propagated to the global model through aggregation, further aggravating the issue of fairness for other parties. Our results explain how bias is propagated in FL: <span id="S3.SS4.p7.18.2" class="ltx_text ltx_font_bold">Biased parties encode bias in a few parameters through a local update, and this bias is consequently propagated to the entire network through parameter aggregation.</span></p>
</div>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related work</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">Fairness has received considerable attention due to the growing deployment of machine learning in decision-making processes. Various definitions of fairness have been presented <cite class="ltx_cite ltx_citemacro_citep">(Hardt et al., <a href="#bib.bib18" title="" class="ltx_ref">2016</a>; Dwork et al., <a href="#bib.bib13" title="" class="ltx_ref">2012</a>; Calders et al., <a href="#bib.bib4" title="" class="ltx_ref">2009</a>)</cite>. Specifically, group fairness requires that the model behave similarly for groups defined by a sensitive attribute (e.g., race). While how machine learning algorithms propagate data bias to the final model has been extensively investigated in a centralized setting  <cite class="ltx_cite ltx_citemacro_citep">(Blum and Stangl, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>; Lakkaraju et al., <a href="#bib.bib26" title="" class="ltx_ref">2017</a>; Rambachan and Roth, <a href="#bib.bib39" title="" class="ltx_ref">2020</a>; Friedler et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Dullerud et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, the effect of FL on model fairness is not yet fully understood.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">The existing literature on fairness in FL mainly focuses on the performance disparity of FL models across parties, rather than demographic groups <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>; Zhao and Joshi, <a href="#bib.bib55" title="" class="ltx_ref">2022</a>; Li et al., <a href="#bib.bib27" title="" class="ltx_ref">2019</a>; Mohri et al., <a href="#bib.bib34" title="" class="ltx_ref">2019</a>; Deng et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Donahue and Kleinberg, <a href="#bib.bib10" title="" class="ltx_ref">2021</a>; Hao et al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>; Zhou et al., <a href="#bib.bib56" title="" class="ltx_ref">2021</a>; Yu et al., <a href="#bib.bib48" title="" class="ltx_ref">2020</a>; Lyu et al., <a href="#bib.bib32" title="" class="ltx_ref">2020</a>)</cite>. However, we focus on group fairness, which concerns performance disparity among groups. In terms of group fairness, <cite class="ltx_cite ltx_citemacro_cite">Abay et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> listed a few potential sources of bias in FL. Recently, considerable progress has been made in training group fair models in FL <cite class="ltx_cite ltx_citemacro_citep">(Abay et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>; Chu et al., <a href="#bib.bib5" title="" class="ltx_ref">2021</a>; Zeng et al., <a href="#bib.bib51" title="" class="ltx_ref">2021a</a>; Hu et al., <a href="#bib.bib21" title="" class="ltx_ref">2022</a>; Du et al., <a href="#bib.bib11" title="" class="ltx_ref">2021</a>; Ezzeldin et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. Nonetheless, the majority of these works suggest techniques for achieving fairness on a single test distribution. Instead, we focus on fairness issues for parties. Some studies <cite class="ltx_cite ltx_citemacro_citep">(Cui et al., <a href="#bib.bib7" title="" class="ltx_ref">2021</a>; Papadaki et al., <a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite> proposed algorithms to improve local fairness for parties. Our purpose, instead, is to gain a comprehensive understanding of how FL influences local fairness on its own, which we believe is equally crucial as designing fair algorithms.</p>
</div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Work &amp; Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Future Work. </span>
In this work, we have investigated how bias is propagated in FL when the sensitive attribute is included as an input feature. In practice, however, sensitive attributes may be prohibited from being included in input features. In such situations, the model may still be heavily biased due to variables that are correlated with the (unobserved) sensitive attribute. For instance, a person’s zip code may be highly correlated with their race, a phenomenon known as "redlining". A promising direction for future work is to identify which model parameters contribute to the bias and to audit the bias propagation in this setting. Another important direction is to design FL algorithms that are robust to bias propagation. In Appendix <a href="#A5" title="Appendix E Potential Mitigation ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, we briefly discuss a few potential ways to achieve this goal.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Conclusion. </span> Federated learning has become increasingly popular in various applications with significant individual-level consequences, making it essential to anticipate the possible bias introduced by FL. Our paper takes the first step in this direction by providing a comprehensive analysis of the impact of FL on local fairness for parties. We demonstrated that the FL algorithm could introduce bias on its own which may exacerbate the issue of fairness for the involved parties. Moreover, we showed that this exacerbation is not evenly distributed among parties, as FL can propagate bias among them. Finally, we explained how bias is propagated in FL: biased parties encode their bias into the local updates by increasing the signal of a few parameters steadily throughout the training process, which is then propagated to the global model via aggregation and, ultimately, to other parties.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgement</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">The authors would like to thank Ergute Bao, Ta Duy Nguyen, and Martin Strobel for their valuable feedback on earlier versions of this paper, as well as the anonymous reviewers for their insightful comments. This research is supported by Google PDPO faculty research award, Intel within the www.private-ai.org center, Meta faculty research award, the NUS Early Career Research Award (NUS ECRA award num- ber NUS ECRA FY19 P16), and the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not reflect the views of the National Research Foundation, Singapore.</p>
</div>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Ethics Statement</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">Our analysis focuses on group fairness, which is typically used to audit the model or system for any bias or discrimination. Auditing the bias with respect to other definitions of fairness, such as individual fairness, may result in different conclusions. Moreover, our study focuses primarily on the binary notion of sex attributes and the multi-valued race attribute. We recognize that there are numerous protected groups outside those considered in the analysis, such as those defined by multiple sensitive attributes. The propagation of bias against fine-grained subgroups may be even more substantial than we found in the paper.</p>
</div>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Reproducibility Statement</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We provide details about the model, datasets, and implementations in Appendix <a href="#A1" title="Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>, and the code for the paper is available at <a target="_blank" href="https://github.com/privacytrustlab/bias_in_FL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/privacytrustlab/bias_in_FL</a>.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abay et al. [2020]</span>
<span class="ltx_bibblock">
Annie Abay, Yi Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and
Heiko Ludwig.

</span>
<span class="ltx_bibblock">Mitigating bias in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2012.02447</em>, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asad et al. [2020]</span>
<span class="ltx_bibblock">
Muhammad Asad, Ahmed Moustafa, and Takayuki Ito.

</span>
<span class="ltx_bibblock">Fedopt: Towards communication efficiency and privacy preservation in
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 10(8):2864, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blum and Stangl [2020]</span>
<span class="ltx_bibblock">
Avrim Blum and Kevin Stangl.

</span>
<span class="ltx_bibblock">Recovering from biased data: Can fairness constraints improve
accuracy?

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">1st Symposium on Foundations of Responsible Computing</em>,
2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calders et al. [2009]</span>
<span class="ltx_bibblock">
Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy.

</span>
<span class="ltx_bibblock">Building classifiers with independency constraints.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2009 IEEE International Conference on Data Mining
Workshops</em>, pages 13–18. IEEE, 2009.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. [2021]</span>
<span class="ltx_bibblock">
Lingyang Chu, Lanjun Wang, Yanjie Dong, Jian Pei, Zirui Zhou, and Yong Zhang.

</span>
<span class="ltx_bibblock">Fedfair: Training fair models in cross-silo federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.05662</em>, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Combalia et al. [2019]</span>
<span class="ltx_bibblock">
Marc Combalia, Noel CF Codella, Veronica Rotemberg, Brian Helba, Veronica
Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan C Halpern,
Susana Puig, et al.

</span>
<span class="ltx_bibblock">Bcn20000: Dermoscopic lesions in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1908.02288</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al. [2021]</span>
<span class="ltx_bibblock">
Sen Cui, Weishen Pan, Jian Liang, Changshui Zhang, and Fei Wang.

</span>
<span class="ltx_bibblock">Fair and consistent federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em>, pages arXiv–2108, 2021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2020]</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">Distributionally robust federated averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:15111–15122, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2021]</span>
<span class="ltx_bibblock">
Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Retiring adult: New datasets for fair machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.04884</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Donahue and Kleinberg [2021]</span>
<span class="ltx_bibblock">
Kate Donahue and Jon M. Kleinberg.

</span>
<span class="ltx_bibblock">Models of fairness in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2112.00818, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2112.00818" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.00818</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. [2021]</span>
<span class="ltx_bibblock">
Wei Du, Depeng Xu, Xintao Wu, and Hanghang Tong.

</span>
<span class="ltx_bibblock">Fairness-aware agnostic federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 SIAM International Conference on
Data Mining (SDM)</em>, pages 181–189. SIAM, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dullerud et al. [2022]</span>
<span class="ltx_bibblock">
Natalie Dullerud, Karsten Roth, Kimia Hamidieh, Nicolas Papernot, and Marzyeh
Ghassemi.

</span>
<span class="ltx_bibblock">Is fairness only metric deep? evaluating and addressing subgroup gaps
in deep metric learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.12748</em>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. [2012]</span>
<span class="ltx_bibblock">
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.

</span>
<span class="ltx_bibblock">Fairness through awareness.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Innovations in Theoretical Computer
Science Conference</em>, ITCS ’12, page 214–226, New York, NY, USA, 2012.
Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9781450311151.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/2090236.2090255</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/2090236.2090255" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/2090236.2090255</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ezzeldin et al. [2021]</span>
<span class="ltx_bibblock">
Yahya H Ezzeldin, Shen Yan, Chaoyang He, Emilio Ferrara, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fairfed: Enabling group fairness in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">NeurIPS Workshop on New Frontiers in Federated Learning (NFFL
2021)</em>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedler et al. [2019]</span>
<span class="ltx_bibblock">
Sorelle A Friedler, Carlos Scheidegger, Suresh Venkatasubramanian, Sonam
Choudhary, Evan P Hamilton, and Derek Roth.

</span>
<span class="ltx_bibblock">A comparative study of fairness-enhancing interventions in machine
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the conference on fairness, accountability,
and transparency</em>, pages 329–338, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grabowicz et al. [2022]</span>
<span class="ltx_bibblock">
Przemyslaw A Grabowicz, Nicholas Perello, and Aarshee Mishra.

</span>
<span class="ltx_bibblock">Marrying fairness and explainability in supervised learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2022 ACM Conference on Fairness, Accountability, and
Transparency</em>, pages 1905–1916, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. [2021]</span>
<span class="ltx_bibblock">
Weituo Hao, Mostafa El-Khamy, Jungwon Lee, Jianyi Zhang, Kevin J Liang,
Changyou Chen, and Lawrence Carin Duke.

</span>
<span class="ltx_bibblock">Towards fair federated learning with zero-shot data augmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 3310–3319, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardt et al. [2016]</span>
<span class="ltx_bibblock">
Moritz Hardt, Eric Price, Eric Price, and Nati Srebro.

</span>
<span class="ltx_bibblock">Equality of opportunity in supervised learning.

</span>
<span class="ltx_bibblock">In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett,
editors, <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 29.
Curran Associates, Inc., 2016.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hashimoto et al. [2018]</span>
<span class="ltx_bibblock">
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.

</span>
<span class="ltx_bibblock">Fairness without demographics in repeated loss minimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
1929–1938. PMLR, 2018.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang
Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">NeurIPS 2020 FL Workshop Best Paper Award</em>, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2022]</span>
<span class="ltx_bibblock">
Shengyuan Hu, Zhiwei Steven Wu, and Virginia Smith.

</span>
<span class="ltx_bibblock">Provably fair federated learning via bounded group loss.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.10190</em>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamiran and Calders [2012]</span>
<span class="ltx_bibblock">
Faisal Kamiran and Toon Calders.

</span>
<span class="ltx_bibblock">Data preprocessing techniques for classification without
discrimination.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Knowledge and information systems</em>, 33(1):1–33, 2012.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. [2020a]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J
Reddi, Sebastian U Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Mime: Mimicking centralized stochastic algorithms in federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.03606</em>, 2020a.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al. [2020b]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
5132–5143. PMLR, 2020b.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kokhlikyan et al. [2020]</span>
<span class="ltx_bibblock">
Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh,
Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi
Yan, and Orion Reblitz-Richardson.

</span>
<span class="ltx_bibblock">Captum: A unified and generic model interpretability library for
pytorch, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lakkaraju et al. [2017]</span>
<span class="ltx_bibblock">
Himabindu Lakkaraju, Jon Kleinberg, Jure Leskovec, Jens Ludwig, and Sendhil
Mullainathan.

</span>
<span class="ltx_bibblock">The selective labels problem: Evaluating algorithmic predictions in
the presence of unobservables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</em>, pages 275–284, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2019]</span>
<span class="ltx_bibblock">
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.

</span>
<span class="ltx_bibblock">Fair resource allocation in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10497</em>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2020]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>, 2:429–450, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2021]</span>
<span class="ltx_bibblock">
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.

</span>
<span class="ltx_bibblock">Ditto: Fair and robust federated learning through personalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
6357–6368. PMLR, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2022]</span>
<span class="ltx_bibblock">
Ji Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao Xu, and Hanghang Tong.

</span>
<span class="ltx_bibblock">Fair representation learning: An alternative to mutual information.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining</em>, pages 1088–1097, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2015]</span>
<span class="ltx_bibblock">
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.

</span>
<span class="ltx_bibblock">Deep learning face attributes in the wild.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Proceedings of International Conference on Computer Vision
(ICCV)</em>, December 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. [2020]</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Xinyi Xu, Qian Wang, and Han Yu.

</span>
<span class="ltx_bibblock">Collaborative fairness in federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Federated Learning</em>, pages 189–204. Springer, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. [2017]</span>
<span class="ltx_bibblock">
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mohri et al. [2019]</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
4615–4625. PMLR, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogier du Terrail et al. [2022]</span>
<span class="ltx_bibblock">
Jean Ogier du Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,
Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum
Mushtaq, Boris Muzellec, Constantin Philippenko, Santiago Silva, Maria
Teleńczuk, Shadi Albarqouni, Salman Avestimehr, Aurélien Bellet, Aymeric
Dieuleveut, Martin Jaggi, Sai Praneeth Karimireddy, Marco Lorenzi, Giovanni
Neglia, Marc Tommasi, and Mathieu Andreux.

</span>
<span class="ltx_bibblock">Flamby: Datasets and benchmarks for cross-silo federated learning in
realistic healthcare settings, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadaki et al. [2021]</span>
<span class="ltx_bibblock">
Afroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and
Miguel Rodrigues.

</span>
<span class="ltx_bibblock">Federating for learning group fair models.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.01999</em>, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadaki et al. [2022]</span>
<span class="ltx_bibblock">
Afroditi Papadaki, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and
Miguel Rodrigues.

</span>
<span class="ltx_bibblock">Minimax demographic group fairness in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.08304</em>, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. [2019]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems 32</em>, pages 8024–8035. Curran Associates,
Inc., 2019.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rambachan and Roth [2020]</span>
<span class="ltx_bibblock">
Ashesh Rambachan and Jonathan Roth.

</span>
<span class="ltx_bibblock">Bias in, bias out? evaluating the folk wisdom.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">1st Symposium on Foundations of Responsible Computing</em>,
2020.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. [2020]</span>
<span class="ltx_bibblock">
Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and Hugh Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rieke et al. [2020]</span>
<span class="ltx_bibblock">
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi
Albarqouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus
Maier-Hein, et al.

</span>
<span class="ltx_bibblock">The future of digital health with federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">NPJ digital medicine</em>, 3(1):1–7, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarah et al. [2020]</span>
<span class="ltx_bibblock">
Flood Sarah, King Miriam, Rodger Renae, Ruggles Steven, and Warren J. Robert.

</span>
<span class="ltx_bibblock">Integrated public use microdata series, current population survey:
Version 8.0 [dataset], 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.ipums.org/projects/ipums-cps/d030.v8.0" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.ipums.org/projects/ipums-cps/d030.v8.0</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundararajan et al. [2017]</span>
<span class="ltx_bibblock">
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.

</span>
<span class="ltx_bibblock">Axiomatic attribution for deep networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
3319–3328. PMLR, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le [2019]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le.

</span>
<span class="ltx_bibblock">Efficientnet: Rethinking model scaling for convolutional neural
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
6105–6114. PMLR, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tschandl et al. [2018]</span>
<span class="ltx_bibblock">
Philipp Tschandl, Cliff Rosendahl, and Harald Kittler.

</span>
<span class="ltx_bibblock">The ham10000 dataset, a large collection of multi-source
dermatoscopic images of common pigmented skin lesions.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Scientific data</em>, 5(1):1–9, 2018.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2020]</span>
<span class="ltx_bibblock">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Tackling the objective inconsistency problem in heterogeneous
federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:7611–7623, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2019]</span>
<span class="ltx_bibblock">
Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu.

</span>
<span class="ltx_bibblock">Ffd: A federated learning based method for credit card fraud
detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">International conference on big data</em>, pages 18–32.
Springer, 2019.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2020]</span>
<span class="ltx_bibblock">
Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit
Niyato, and Qiang Yang.

</span>
<span class="ltx_bibblock">A fairness-aware incentive scheme for federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society</em>, pages 393–399, 2020.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zafar et al. [2017]</span>
<span class="ltx_bibblock">
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P
Gummadi.

</span>
<span class="ltx_bibblock">Fairness beyond disparate treatment &amp; disparate impact: Learning
classification without disparate mistreatment.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 26th international conference on world
wide web</em>, pages 1171–1180, 2017.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zemel et al. [2013]</span>
<span class="ltx_bibblock">
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork.

</span>
<span class="ltx_bibblock">Learning fair representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
325–333. PMLR, 2013.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2021a]</span>
<span class="ltx_bibblock">
Yuchen Zeng, Hongxu Chen, and Kangwook Lee.

</span>
<span class="ltx_bibblock">Improving fairness via federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.15545</em>, 2021a.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2021b]</span>
<span class="ltx_bibblock">
Ziqian Zeng, Rashidul Islam, Kamrun Naher Keya, James Foulds, Yangqiu Song, and
Shimei Pan.

</span>
<span class="ltx_bibblock">Fair representation learning for heterogeneous information networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International AAAI Conference on Weblogs
and Social Media</em>, volume 15, 2021b.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2020]</span>
<span class="ltx_bibblock">
Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and
Cheng Zhang.

</span>
<span class="ltx_bibblock">How do fair decisions fare in long-term qualification?

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:18457–18469, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2019]</span>
<span class="ltx_bibblock">
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon.

</span>
<span class="ltx_bibblock">Conditional learning of fair representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao and Joshi [2022]</span>
<span class="ltx_bibblock">
Zhiyuan Zhao and Gauri Joshi.

</span>
<span class="ltx_bibblock">A dynamic reweighting strategy for fair federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">ICASSP 2022-2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 8772–8776. IEEE, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2021]</span>
<span class="ltx_bibblock">
Zirui Zhou, Lingyang Chu, Changxin Liu, Lanjun Wang, Jian Pei, and Yong Zhang.

</span>
<span class="ltx_bibblock">Towards fair federated learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery &amp; Data Mining</em>, pages 4100–4101, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

<div id="Ax1.p1" class="ltx_para ltx_noindent">
<p id="Ax1.p1.1" class="ltx_p">This appendix is divided into five sections. Appendix <a href="#A1" title="Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> provides additional details about the experimental setups, including information about the models, datasets, and hyperparameters used. Appendix <a href="#A2" title="Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> presents further experimental results that support the claims made in the paper. In Appendix <a href="#A3" title="Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, we discuss the results of an existing fair FL algorithm. Appendix <a href="#A4" title="Appendix D Extending to a real-world medical dataset ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> extends our analysis to real-world medical datasets. Finally, in Appendix <a href="#A5" title="Appendix E Potential Mitigation ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>, we discuss potential methods for mitigating bias in FL.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details about Experiment Setup</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Datasets and Models</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p">We explain the datasets and models used in the paper.</p>
</div>
<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Census Dataset</h4>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px1.p1.1" class="ltx_p">We use the datasets provided by folktables. In particular, we consider the ACSIncome, ACSPublicCoverage, and ACSEmployment tasks defined in the forlktables. In the ACSIncome, the goal is to predict whether an individual’s Income is above $50,000. In the ACSEmployment task, the goal is to predict whether an individual is employed. Similarly, in the Health (i.e., ACSPublicCoverage) task, the objective is to predict whether an individual is covered by public health insurance. We use the same pre-processing as in the folktables and train a fully connected neural network model with one hidden layer of 32 neurons for Income and 64 neurons for Employment and Health tasks. For all the tasks, we use the RELU activation function. We use an SGD optimizer with a learning rate of 0.001 for centralized training on Health and Employment datasets and 0.1 for other settings, and the batch size is 32. We train the NN models for 200 epochs. In FL, each client updates the global mdoel for 1 epoch and shares it with the server. We encode the categorical features based on the encoding template provided in folktables. After the encoding, the input feature size for Income is 54, 154 for Health, and 109 for Employment. We consider sex and race as sensitive attributes. Accordingly, there are two gender groups (male and female) and nine racial groups ("White alone," "Black or African American alone," "American Indian alone," "Alaska Native alone," and "American Indian and Alaska Native tribes specified; or American Indian or Alaska Native, not specified and no other," "Asian alone," "Native Hawaiian and Other Pacific Islander alone," "Some Other Race alone," "Two or More Races").</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CelebA Dataset</h4>

<div id="A1.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.SSS0.Px2.p1.1" class="ltx_p">We train CNN models on the dataset with one CNN layer whose output channel is 32, kernel size is 3, and stride is 1. We use ’same’ padding for the CNN layer. Following this CNN layer, we have the Batch normalization layer and Max Pooling layer. After which, we have the connected layer. We train the model for 500 communication rounds or epochs with SGD optimizer. The learning rate is 0.1, and the batch size is 128. The train, test, and validation datasets ratio for each party is 6:2:2.</p>
</div>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Hyper-parameter for FL algorithms and Implementation</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p">In our paper, we also evaluate various popular FL algorithms, including FedNova <cite class="ltx_cite ltx_citemacro_citep">[Wang et al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>]</cite>, Scaffold <cite class="ltx_cite ltx_citemacro_cite">Karimireddy et al. [<a href="#bib.bib24" title="" class="ltx_ref">2020b</a>]</cite>, FedOpt <cite class="ltx_cite ltx_citemacro_citep">[Reddi et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>]</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib28" title="" class="ltx_ref">2020</a>]</cite>, and Mime <cite class="ltx_cite ltx_citemacro_citep">[Karimireddy et al., <a href="#bib.bib23" title="" class="ltx_ref">2020a</a>]</cite>. We use the same local learning rate and the number of the local epoch as in FedAvg. We provided the detailed hyper-parameters for each of the algorithms in our code (See supplementary). We run all experiments on Ubuntu with two NVIDIA TITAN RTX GPUs.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Data Heterogeneity</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">Figure <a href="#A1.F11" title="Figure 11 ‣ A.3 Data Heterogeneity ‣ Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the fraction of samples for each subgroup across all parties, while Figure <a href="#A1.F12" title="Figure 12 ‣ A.3 Data Heterogeneity ‣ Appendix A Details about Experiment Setup ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> shows the histogram of subgroup proportions for each party. We observe that the parties have different fractions of samples from each group in the Income dataset, indicating that their local data distributions are dissimilar. In contrast, for the Health and Employment datasets, parties have similar fractions of samples from each subgroup, suggesting that their data distributions are more alike than those of the Income dataset.</p>
</div>
<figure id="A1.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2309.02160/assets/images/income/non-iid/subgroup_distribution_attr_1.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="168" height="955" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2309.02160/assets/images/employment/non-iid/subgroup_distribution_attr_1.png" id="A1.F11.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="174" height="950" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/2309.02160/assets/images/health/non-iid/subgroup_distribution_attr_1.png" id="A1.F11.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_portrait" width="168" height="955" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Fraction of samples for each subgroup - Sex</figcaption>
</figure>
<figure id="A1.F12" class="ltx_figure"><img src="/html/2309.02160/assets/images/hist_base_rate_attr_combined.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Histogram of the fraction of samples for each group with positive label - Sex</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Experimental Results</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Bias propagation effect</h3>

<section id="A2.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Census dataset</h4>

<div id="A2.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px1.p1.1" class="ltx_p">We show the bias propagation effect of FL for the Health and Employment task in Figure <a href="#A2.F13" title="Figure 13 ‣ Census dataset ‣ B.1 Bias propagation effect ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> and <a href="#A2.F14" title="Figure 14 ‣ Census dataset ‣ B.1 Bias propagation effect ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> respectively.</p>
</div>
<figure id="A2.F13" class="ltx_figure"><img src="/html/2309.02160/assets/images/health/bias_propagation/combined_attrs.png" id="A2.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span id="A2.F13.2.1" class="ltx_text ltx_font_bold">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - Health</span></figcaption>
</figure>
<figure id="A2.F14" class="ltx_figure"><img src="/html/2309.02160/assets/images/employment/bias_propagation/combined_attrs.png" id="A2.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="111" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span id="A2.F14.2.1" class="ltx_text ltx_font_bold">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - Employment</span></figcaption>
</figure>
</section>
<section id="A2.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CelebA</h4>

<div id="A2.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS1.SSS0.Px2.p1.1" class="ltx_p">We show the bias propagation effect of FL on CelebA dataset. Besides partitioning the data in an IID manner (we refer to as setting (i)), we also evaluate two different settings, where we change the number of samples from the minority subgroup (the female group with the “Not Young” age label). In this way, we aim to change the data bias in the local training datasets for clients. In particular, in setting (ii), half of the parties have more samples from the minority subgroup than another half of the parties. The ratio is 8:2. In setting (iii), a single party has half of the data from the minority subgroup, and the data is then iid partitioned among the other parties. In the figure, we find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties. Figure <a href="#A2.F15" title="Figure 15 ‣ CelebA ‣ B.1 Bias propagation effect ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows the correlation between the fairness gap of the standalone model and the benefit a party gets in the FL. We find that the in the non-iid setting, the more biased parties have a larger and more positive benefit, while FL hurts the less biased parties.</p>
</div>
<figure id="A2.F15" class="ltx_figure"><img src="/html/2309.02160/assets/images/celeba/bias_propagation_celeba_age.png" id="A2.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span><span id="A2.F15.2.1" class="ltx_text ltx_font_bold">Correlation between the fairness gap of the standalone model and the benefit obtained from FL - CelebA (Age)</span></figcaption>
</figure>
</section>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Aggregation contradicts with local update</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">Figure <a href="#A2.F16" title="Figure 16 ‣ B.2 Aggregation contradicts with local update ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> shows the dynamic of the fairness gap for the aggregated model and locally updated model from the most biased party and least biased party during the training.</p>
</div>
<figure id="A2.F16" class="ltx_figure"><img src="/html/2309.02160/assets/images/income/dynamic/attr_1_metric_dp_most_biased_200.png" id="A2.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="321" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span><span id="A2.F16.2.1" class="ltx_text ltx_font_bold">Dynamic of <math id="A2.F16.2.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A2.F16.2.1.m1.1b"><msup id="A2.F16.2.1.m1.1.1" xref="A2.F16.2.1.m1.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.F16.2.1.m1.1.1.2" xref="A2.F16.2.1.m1.1.1.2.cmml">Δ</mi><mrow id="A2.F16.2.1.m1.1.1.3" xref="A2.F16.2.1.m1.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.F16.2.1.m1.1.1.3.2" xref="A2.F16.2.1.m1.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A2.F16.2.1.m1.1.1.3.1" xref="A2.F16.2.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.F16.2.1.m1.1.1.3.3" xref="A2.F16.2.1.m1.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.F16.2.1.m1.1c"><apply id="A2.F16.2.1.m1.1.1.cmml" xref="A2.F16.2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.F16.2.1.m1.1.1.1.cmml" xref="A2.F16.2.1.m1.1.1">superscript</csymbol><ci id="A2.F16.2.1.m1.1.1.2.cmml" xref="A2.F16.2.1.m1.1.1.2">Δ</ci><apply id="A2.F16.2.1.m1.1.1.3.cmml" xref="A2.F16.2.1.m1.1.1.3"><times id="A2.F16.2.1.m1.1.1.3.1.cmml" xref="A2.F16.2.1.m1.1.1.3.1"></times><ci id="A2.F16.2.1.m1.1.1.3.2.cmml" xref="A2.F16.2.1.m1.1.1.3.2">𝐷</ci><ci id="A2.F16.2.1.m1.1.1.3.3.cmml" xref="A2.F16.2.1.m1.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F16.2.1.m1.1d">\Delta^{DP}</annotation></semantics></math> during the training - Income (sex). </span> Figure shows the fairness gap for the aggregated model and local updated model from the most biased party and least biased party during the training. The most (least) biased party is the party with the highest (lowest) fairness gap in the standalone setting. The fairness gap in the standalone setting for those parties is shown in the title.
</figcaption>
</figure>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Influence Sub-graphs</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">We show the top 10 maximal positive and top 10 maximal native influence client pairs in Figure <a href="#A2.F17" title="Figure 17 ‣ B.3 Influence Sub-graphs ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> and the top 15 in Figure <a href="#A2.F18" title="Figure 18 ‣ B.3 Influence Sub-graphs ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>. We can see that the less biased client has a strong positive influence on other clients while the more biased client has a strong negative influence on other clients.</p>
</div>
<figure id="A2.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp_appendix_top_10.png" id="A2.F17.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="129" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp_appendix_top_10.png" id="A2.F17.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="130" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span><span id="A2.F17.2.1" class="ltx_text ltx_font_bold">Influence subgraph - Income (<math id="A2.F17.2.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A2.F17.2.1.m1.1b"><msup id="A2.F17.2.1.m1.1.1" xref="A2.F17.2.1.m1.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.F17.2.1.m1.1.1.2" xref="A2.F17.2.1.m1.1.1.2.cmml">Δ</mi><mrow id="A2.F17.2.1.m1.1.1.3" xref="A2.F17.2.1.m1.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.F17.2.1.m1.1.1.3.2" xref="A2.F17.2.1.m1.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A2.F17.2.1.m1.1.1.3.1" xref="A2.F17.2.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.F17.2.1.m1.1.1.3.3" xref="A2.F17.2.1.m1.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.F17.2.1.m1.1c"><apply id="A2.F17.2.1.m1.1.1.cmml" xref="A2.F17.2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.F17.2.1.m1.1.1.1.cmml" xref="A2.F17.2.1.m1.1.1">superscript</csymbol><ci id="A2.F17.2.1.m1.1.1.2.cmml" xref="A2.F17.2.1.m1.1.1.2">Δ</ci><apply id="A2.F17.2.1.m1.1.1.3.cmml" xref="A2.F17.2.1.m1.1.1.3"><times id="A2.F17.2.1.m1.1.1.3.1.cmml" xref="A2.F17.2.1.m1.1.1.3.1"></times><ci id="A2.F17.2.1.m1.1.1.3.2.cmml" xref="A2.F17.2.1.m1.1.1.3.2">𝐷</ci><ci id="A2.F17.2.1.m1.1.1.3.3.cmml" xref="A2.F17.2.1.m1.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F17.2.1.m1.1d">\Delta^{DP}</annotation></semantics></math>)</span> Top 10 maximal positive influential pair and top 10 maximal negative influential pair. The <span id="A2.F17.9.2" class="ltx_text" style="color:#7FB77E;">green</span> edge and <span id="A2.F17.10.3" class="ltx_text" style="color:#EC7272;">red</span> edge represent the <span id="A2.F17.11.4" class="ltx_text" style="color:#7FB77E;">positive</span> and <span id="A2.F17.12.5" class="ltx_text" style="color:#EC7272;">negative</span> influence, respectively. The <span id="A2.F17.13.6" class="ltx_text" style="color:#5C8AA8;">color</span> of the node represents the <span id="A2.F17.14.7" class="ltx_text" style="color:#5C8AA8;">fairness gap in the standalone setting</span>.</figcaption>
</figure>
<figure id="A2.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/propagation_path/overall_attr_1_metric_d_dp_appendix_top_15.png" id="A2.F18.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="129" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/propagation_path/overall_attr_0_metric_d_dp_appendix_top_15.png" id="A2.F18.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="130" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span><span id="A2.F18.2.1" class="ltx_text ltx_font_bold">Influence subgraph - Income (<math id="A2.F18.2.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A2.F18.2.1.m1.1b"><msup id="A2.F18.2.1.m1.1.1" xref="A2.F18.2.1.m1.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.F18.2.1.m1.1.1.2" xref="A2.F18.2.1.m1.1.1.2.cmml">Δ</mi><mrow id="A2.F18.2.1.m1.1.1.3" xref="A2.F18.2.1.m1.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.F18.2.1.m1.1.1.3.2" xref="A2.F18.2.1.m1.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A2.F18.2.1.m1.1.1.3.1" xref="A2.F18.2.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.F18.2.1.m1.1.1.3.3" xref="A2.F18.2.1.m1.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.F18.2.1.m1.1c"><apply id="A2.F18.2.1.m1.1.1.cmml" xref="A2.F18.2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.F18.2.1.m1.1.1.1.cmml" xref="A2.F18.2.1.m1.1.1">superscript</csymbol><ci id="A2.F18.2.1.m1.1.1.2.cmml" xref="A2.F18.2.1.m1.1.1.2">Δ</ci><apply id="A2.F18.2.1.m1.1.1.3.cmml" xref="A2.F18.2.1.m1.1.1.3"><times id="A2.F18.2.1.m1.1.1.3.1.cmml" xref="A2.F18.2.1.m1.1.1.3.1"></times><ci id="A2.F18.2.1.m1.1.1.3.2.cmml" xref="A2.F18.2.1.m1.1.1.3.2">𝐷</ci><ci id="A2.F18.2.1.m1.1.1.3.3.cmml" xref="A2.F18.2.1.m1.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F18.2.1.m1.1d">\Delta^{DP}</annotation></semantics></math>)</span> Top 15 maximal positive influential pair and top 15 maximal negative influential pair. The <span id="A2.F18.9.2" class="ltx_text" style="color:#7FB77E;">green</span> edge and <span id="A2.F18.10.3" class="ltx_text" style="color:#EC7272;">red</span> edge represent the <span id="A2.F18.11.4" class="ltx_text" style="color:#7FB77E;">positive</span> and <span id="A2.F18.12.5" class="ltx_text" style="color:#EC7272;">negative</span> influence, respectively. The <span id="A2.F18.13.6" class="ltx_text" style="color:#5C8AA8;">color</span> of the node represents the <span id="A2.F18.14.7" class="ltx_text" style="color:#5C8AA8;">fairness gap in the standalone setting</span>.</figcaption>
</figure>
</section>
<section id="A2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Attribution value for sensitive attribute</h3>

<div id="A2.SS4.p1" class="ltx_para">
<p id="A2.SS4.p1.1" class="ltx_p">We show the attribution value for all input features for the most biased party and least biased party in Figure <a href="#A2.F19" title="Figure 19 ‣ B.4 Attribution value for sensitive attribute ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> and Figure <a href="#A2.F20" title="Figure 20 ‣ B.4 Attribution value for sensitive attribute ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>, respectively. We observe that the attribution value for other features is similar for female and male groups. However, the sex attribute has the largest attribution value and affects the groups differently. It implies that the models are highly dependent on the sensitive attribute (i.e., "Sex") for making predictions.</p>
</div>
<figure id="A2.F19" class="ltx_figure"><img src="/html/2309.02160/assets/images/income/attribution_value/client_43_gender_199.png" id="A2.F19.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span><span id="A2.F19.2.1" class="ltx_text ltx_font_bold">Feature attribution value - Income (Most biased party)</span> We shows the feature attribution value for all the input features. The model trained in FL and standalone setting has a large dependency on the sensitive attribute "Sex". </figcaption>
</figure>
<figure id="A2.F20" class="ltx_figure"><img src="/html/2309.02160/assets/images/income/attribution_value/client_50_gender_199.png" id="A2.F20.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span><span id="A2.F20.2.1" class="ltx_text ltx_font_bold">Feature attribution value - Income (Least biased party)</span> We shows the feature attribution value for all the input features. The model trained in FL and standalone setting has a large dependency on the sensitive attribute "Sex". </figcaption>
</figure>
</section>
<section id="A2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>Effect of FL algorithm</h3>

<div id="A2.SS5.p1" class="ltx_para ltx_noindent">
<p id="A2.SS5.p1.1" class="ltx_p">Table <a href="#A2.T2" title="Table 2 ‣ B.5 Effect of FL algorithm ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows results on other FL algorithms, including FedNova <cite class="ltx_cite ltx_citemacro_citep">[Wang et al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>]</cite>, Scaffold <cite class="ltx_cite ltx_citemacro_cite">Karimireddy et al. [<a href="#bib.bib24" title="" class="ltx_ref">2020b</a>]</cite>, FedOpt <cite class="ltx_cite ltx_citemacro_citep">[Asad et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>]</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. [<a href="#bib.bib28" title="" class="ltx_ref">2020</a>]</cite>, and Mime <cite class="ltx_cite ltx_citemacro_citep">[Karimireddy et al., <a href="#bib.bib23" title="" class="ltx_ref">2020a</a>]</cite>.
We find that when the FL model achieves higher accuracy than standalone models, the fairness gap of the FL model can be higher than that of standalone models. This observation is consistent across multiple FL algorithms. This strong evidence implies that the improvement of accuracy in FL can come at the cost of fairness. In addition, this is not unique to only the FedAvg algorithm.</p>
</div>
<figure id="A2.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="A2.T2.6.1" class="ltx_text ltx_font_bold">Benefit of collaboration on local datasets - Various FL algorithms (Income).</span> The standard deviation across five runs is indicated between parentheses.</figcaption>
<div id="A2.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:42.2pt;vertical-align:-32.9pt;"><span class="ltx_transformed_inner" style="transform:translate(42.2pt,-0.9pt) scale(1.2415726193884,1.2415726193884) ;"><span id="A2.T2.4.5" class="ltx_ERROR undefined">\csvreader</span>
<p id="A2.T2.4.4" class="ltx_p">[tabular=ca*2e*2d,table head=    <span id="A2.T2.4.4.4" class="ltx_text" style="background-color:#F6FCF2;"> Race     <span id="A2.T2.4.4.4.4" class="ltx_text" style="background-color:#F5FAFC;">Sex  
<br class="ltx_break">Algorithm  Accuracy  <math id="A2.T2.1.1.1.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{EO}" display="inline"><semantics id="A2.T2.1.1.1.1.m1.1a"><msup id="A2.T2.1.1.1.1.m1.1.1" xref="A2.T2.1.1.1.1.m1.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.T2.1.1.1.1.m1.1.1.2" xref="A2.T2.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mrow id="A2.T2.1.1.1.1.m1.1.1.3" xref="A2.T2.1.1.1.1.m1.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.T2.1.1.1.1.m1.1.1.3.2" xref="A2.T2.1.1.1.1.m1.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="A2.T2.1.1.1.1.m1.1.1.3.1" xref="A2.T2.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.T2.1.1.1.1.m1.1.1.3.3" xref="A2.T2.1.1.1.1.m1.1.1.3.3.cmml">O</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.T2.1.1.1.1.m1.1b"><apply id="A2.T2.1.1.1.1.m1.1.1.cmml" xref="A2.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T2.1.1.1.1.m1.1.1.1.cmml" xref="A2.T2.1.1.1.1.m1.1.1">superscript</csymbol><ci id="A2.T2.1.1.1.1.m1.1.1.2.cmml" xref="A2.T2.1.1.1.1.m1.1.1.2">Δ</ci><apply id="A2.T2.1.1.1.1.m1.1.1.3.cmml" xref="A2.T2.1.1.1.1.m1.1.1.3"><times id="A2.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="A2.T2.1.1.1.1.m1.1.1.3.1"></times><ci id="A2.T2.1.1.1.1.m1.1.1.3.2.cmml" xref="A2.T2.1.1.1.1.m1.1.1.3.2">𝐸</ci><ci id="A2.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="A2.T2.1.1.1.1.m1.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.1.1.1.1.m1.1c">\Delta^{EO}</annotation></semantics></math>  <math id="A2.T2.2.2.2.2.m2.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A2.T2.2.2.2.2.m2.1a"><msup id="A2.T2.2.2.2.2.m2.1.1" xref="A2.T2.2.2.2.2.m2.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.T2.2.2.2.2.m2.1.1.2" xref="A2.T2.2.2.2.2.m2.1.1.2.cmml">Δ</mi><mrow id="A2.T2.2.2.2.2.m2.1.1.3" xref="A2.T2.2.2.2.2.m2.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.T2.2.2.2.2.m2.1.1.3.2" xref="A2.T2.2.2.2.2.m2.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A2.T2.2.2.2.2.m2.1.1.3.1" xref="A2.T2.2.2.2.2.m2.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.T2.2.2.2.2.m2.1.1.3.3" xref="A2.T2.2.2.2.2.m2.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.T2.2.2.2.2.m2.1b"><apply id="A2.T2.2.2.2.2.m2.1.1.cmml" xref="A2.T2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="A2.T2.2.2.2.2.m2.1.1.1.cmml" xref="A2.T2.2.2.2.2.m2.1.1">superscript</csymbol><ci id="A2.T2.2.2.2.2.m2.1.1.2.cmml" xref="A2.T2.2.2.2.2.m2.1.1.2">Δ</ci><apply id="A2.T2.2.2.2.2.m2.1.1.3.cmml" xref="A2.T2.2.2.2.2.m2.1.1.3"><times id="A2.T2.2.2.2.2.m2.1.1.3.1.cmml" xref="A2.T2.2.2.2.2.m2.1.1.3.1"></times><ci id="A2.T2.2.2.2.2.m2.1.1.3.2.cmml" xref="A2.T2.2.2.2.2.m2.1.1.3.2">𝐷</ci><ci id="A2.T2.2.2.2.2.m2.1.1.3.3.cmml" xref="A2.T2.2.2.2.2.m2.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.2.2.2.2.m2.1c">\Delta^{DP}</annotation></semantics></math> <math id="A2.T2.3.3.3.3.m3.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{EO}" display="inline"><semantics id="A2.T2.3.3.3.3.m3.1a"><msup id="A2.T2.3.3.3.3.m3.1.1" xref="A2.T2.3.3.3.3.m3.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.T2.3.3.3.3.m3.1.1.2" xref="A2.T2.3.3.3.3.m3.1.1.2.cmml">Δ</mi><mrow id="A2.T2.3.3.3.3.m3.1.1.3" xref="A2.T2.3.3.3.3.m3.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.T2.3.3.3.3.m3.1.1.3.2" xref="A2.T2.3.3.3.3.m3.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="A2.T2.3.3.3.3.m3.1.1.3.1" xref="A2.T2.3.3.3.3.m3.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.T2.3.3.3.3.m3.1.1.3.3" xref="A2.T2.3.3.3.3.m3.1.1.3.3.cmml">O</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.T2.3.3.3.3.m3.1b"><apply id="A2.T2.3.3.3.3.m3.1.1.cmml" xref="A2.T2.3.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="A2.T2.3.3.3.3.m3.1.1.1.cmml" xref="A2.T2.3.3.3.3.m3.1.1">superscript</csymbol><ci id="A2.T2.3.3.3.3.m3.1.1.2.cmml" xref="A2.T2.3.3.3.3.m3.1.1.2">Δ</ci><apply id="A2.T2.3.3.3.3.m3.1.1.3.cmml" xref="A2.T2.3.3.3.3.m3.1.1.3"><times id="A2.T2.3.3.3.3.m3.1.1.3.1.cmml" xref="A2.T2.3.3.3.3.m3.1.1.3.1"></times><ci id="A2.T2.3.3.3.3.m3.1.1.3.2.cmml" xref="A2.T2.3.3.3.3.m3.1.1.3.2">𝐸</ci><ci id="A2.T2.3.3.3.3.m3.1.1.3.3.cmml" xref="A2.T2.3.3.3.3.m3.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.3.3.3.3.m3.1c">\Delta^{EO}</annotation></semantics></math>  <math id="A2.T2.4.4.4.4.m4.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A2.T2.4.4.4.4.m4.1a"><msup id="A2.T2.4.4.4.4.m4.1.1" xref="A2.T2.4.4.4.4.m4.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A2.T2.4.4.4.4.m4.1.1.2" xref="A2.T2.4.4.4.4.m4.1.1.2.cmml">Δ</mi><mrow id="A2.T2.4.4.4.4.m4.1.1.3" xref="A2.T2.4.4.4.4.m4.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A2.T2.4.4.4.4.m4.1.1.3.2" xref="A2.T2.4.4.4.4.m4.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A2.T2.4.4.4.4.m4.1.1.3.1" xref="A2.T2.4.4.4.4.m4.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A2.T2.4.4.4.4.m4.1.1.3.3" xref="A2.T2.4.4.4.4.m4.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A2.T2.4.4.4.4.m4.1b"><apply id="A2.T2.4.4.4.4.m4.1.1.cmml" xref="A2.T2.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="A2.T2.4.4.4.4.m4.1.1.1.cmml" xref="A2.T2.4.4.4.4.m4.1.1">superscript</csymbol><ci id="A2.T2.4.4.4.4.m4.1.1.2.cmml" xref="A2.T2.4.4.4.4.m4.1.1.2">Δ</ci><apply id="A2.T2.4.4.4.4.m4.1.1.3.cmml" xref="A2.T2.4.4.4.4.m4.1.1.3"><times id="A2.T2.4.4.4.4.m4.1.1.3.1.cmml" xref="A2.T2.4.4.4.4.m4.1.1.3.1"></times><ci id="A2.T2.4.4.4.4.m4.1.1.3.2.cmml" xref="A2.T2.4.4.4.4.m4.1.1.3.2">𝐷</ci><ci id="A2.T2.4.4.4.4.m4.1.1.3.3.cmml" xref="A2.T2.4.4.4.4.m4.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.4.4.4.4.m4.1c">\Delta^{DP}</annotation></semantics></math> 
<br class="ltx_break">, table foot=]data/data_other_fl.csv
 <span id="A2.T2.4.4.4.4.1" class="ltx_ERROR undefined">\csvcoli</span> <span id="A2.T2.4.4.4.4.2" class="ltx_ERROR undefined">\csvcolii</span> (<span id="A2.T2.4.4.4.4.3" class="ltx_ERROR undefined">\csvcoliii</span>)  <span id="A2.T2.4.4.4.4.4" class="ltx_ERROR undefined">\csvcoliv</span> (<span id="A2.T2.4.4.4.4.5" class="ltx_ERROR undefined">\csvcolv</span>)  <span id="A2.T2.4.4.4.4.6" class="ltx_ERROR undefined">\csvcolvi</span> (<span id="A2.T2.4.4.4.4.7" class="ltx_ERROR undefined">\csvcolvii</span>)  <span id="A2.T2.4.4.4.4.8" class="ltx_ERROR undefined">\csvcolviii</span> (<span id="A2.T2.4.4.4.4.9" class="ltx_ERROR undefined">\csvcolix</span>)  <span id="A2.T2.4.4.4.4.10" class="ltx_ERROR undefined">\csvcolx</span> (<span id="A2.T2.4.4.4.4.11" class="ltx_ERROR undefined">\csvcolxi</span>)</span></span></p>
</span></div>
</figure>
</section>
<section id="A2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.6 </span>Group Performance</h3>

<div id="A2.SS6.p1" class="ltx_para">
<p id="A2.SS6.p1.1" class="ltx_p">In Figure <a href="#A2.F21" title="Figure 21 ‣ B.6 Group Performance ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>, we present the model ROC on groups to illustrate the performance disparity across groups. Surprisingly, there is no huge difference between the ROC between groups. In contrast, Figure <a href="#A2.F22" title="Figure 22 ‣ B.6 Group Performance ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a> shows the noticeable difference between groups with respect to the precision-recall curve, especially for the most biased party. The main reason is that the dataset is imbalanced. Thus, ROC is usually misleading. On the precision-recall curve, we find that the model achieves a higher precision on the majority (i.e., Male group) compared to the minority group (i.e., Female group).</p>
</div>
<figure id="A2.F21" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/group_roc/roc_attr_1_43.png" id="A2.F21.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="190" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/group_roc/roc_attr_1_50.png" id="A2.F21.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="190" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Receiver operating characteristic (ROC) of different models on groups - (Income, Sex)</figcaption>
</figure>
<figure id="A2.F22" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/group_roc/ppc_attr_1_43.png" id="A2.F22.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="189" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2309.02160/assets/images/income/group_roc/ppc_attr_1_50.png" id="A2.F22.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="189" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Precision-Recall curve (PRC) of different models on groups - (Income, Sex)</figcaption>
</figure>
</section>
<section id="A2.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.7 </span>Effect of scaling other parameters</h3>

<div id="A2.SS7.p1" class="ltx_para">
<p id="A2.SS7.p1.1" class="ltx_p">Figure <a href="#A2.F23" title="Figure 23 ‣ B.7 Effect of scaling other parameters ‣ Appendix B Additional Experimental Results ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a> shows a comparison of the model’s performance when scaling different model parameters. Specifically, we examine the impact of scaling the parameters in the first layer of the neural network that does not have any computation on the Sex attribute (referred to as "Other parameters"), as well as the parameter related to the Sex attribute (referred to as "Related parameters"). The results indicate that up-scaling or down-scaling other parameters do not significantly affect the fairness gap while scaling the related parameters has a considerable impact on model fairness. This finding supports our hypothesis that the model’s bias is primarily encoded in a few model parameters. In FL, biased parties introduce bias during local training by increasing the weights for those related parameters.</p>
</div>
<figure id="A2.F23" class="ltx_figure"><img src="/html/2309.02160/assets/images/income/effect_parameter/effect_scaling_comparision_199.png" id="A2.F23.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Effect of scaling model parameters - (Income, Sex)</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Evaluation on existing Fair FL</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p">Table <a href="#A3.T3" title="Table 3 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the results for a fair FL algorithm proposed by <cite class="ltx_cite ltx_citemacro_cite">Abay et al. [<a href="#bib.bib1" title="" class="ltx_ref">2020</a>]</cite>, which uses a reweighting algorithm proposed in the centralized setting <cite class="ltx_cite ltx_citemacro_citep">[Kamiran and Calders, <a href="#bib.bib22" title="" class="ltx_ref">2012</a>]</cite>. The algorithm involves assigning weights to each subgroup (defined by the label and a sensitive attribute) before FL, based on the local or global training dataset. This is called "local reweighting" or "global reweighting," respectively. During FL, parties update the FL model to minimize the weighted loss. The table presents the accuracy and fairness gap of the FL algorithm using local and global reweighting for the Income, Health, and Employment datasets.</p>
</div>
<figure id="A3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="A3.T3.6.1" class="ltx_text ltx_font_bold">Effect of Fair FL <cite class="ltx_cite ltx_citemacro_citep">[Abay et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>]</cite> - (Income).</span> The standard deviation across five runs is indicated between parentheses. The "Sensitive Attribute" column indicates the sensitive attribute used in the reweighting algorithm.</figcaption>
<div id="A3.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:40.8pt;vertical-align:-31.8pt;"><span class="ltx_transformed_inner" style="transform:translate(36.1pt,-0.7pt) scale(1.19959219035754,1.19959219035754) ;"><span id="A3.T3.4.5" class="ltx_ERROR undefined">\csvreader</span>
<p id="A3.T3.4.4" class="ltx_p">[tabular=cca*2e*2d,table head=     <span id="A3.T3.4.4.4" class="ltx_text" style="background-color:#F6FCF2;"> Race     <span id="A3.T3.4.4.4.4" class="ltx_text" style="background-color:#F5FAFC;">Sex  
<br class="ltx_break">Algorithm  Sensitive Attribute  Accuracy  <math id="A3.T3.1.1.1.1.m1.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{EO}" display="inline"><semantics id="A3.T3.1.1.1.1.m1.1a"><msup id="A3.T3.1.1.1.1.m1.1.1" xref="A3.T3.1.1.1.1.m1.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A3.T3.1.1.1.1.m1.1.1.2" xref="A3.T3.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mrow id="A3.T3.1.1.1.1.m1.1.1.3" xref="A3.T3.1.1.1.1.m1.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A3.T3.1.1.1.1.m1.1.1.3.2" xref="A3.T3.1.1.1.1.m1.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="A3.T3.1.1.1.1.m1.1.1.3.1" xref="A3.T3.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A3.T3.1.1.1.1.m1.1.1.3.3" xref="A3.T3.1.1.1.1.m1.1.1.3.3.cmml">O</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A3.T3.1.1.1.1.m1.1b"><apply id="A3.T3.1.1.1.1.m1.1.1.cmml" xref="A3.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A3.T3.1.1.1.1.m1.1.1.1.cmml" xref="A3.T3.1.1.1.1.m1.1.1">superscript</csymbol><ci id="A3.T3.1.1.1.1.m1.1.1.2.cmml" xref="A3.T3.1.1.1.1.m1.1.1.2">Δ</ci><apply id="A3.T3.1.1.1.1.m1.1.1.3.cmml" xref="A3.T3.1.1.1.1.m1.1.1.3"><times id="A3.T3.1.1.1.1.m1.1.1.3.1.cmml" xref="A3.T3.1.1.1.1.m1.1.1.3.1"></times><ci id="A3.T3.1.1.1.1.m1.1.1.3.2.cmml" xref="A3.T3.1.1.1.1.m1.1.1.3.2">𝐸</ci><ci id="A3.T3.1.1.1.1.m1.1.1.3.3.cmml" xref="A3.T3.1.1.1.1.m1.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.1.1.1.1.m1.1c">\Delta^{EO}</annotation></semantics></math>  <math id="A3.T3.2.2.2.2.m2.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A3.T3.2.2.2.2.m2.1a"><msup id="A3.T3.2.2.2.2.m2.1.1" xref="A3.T3.2.2.2.2.m2.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A3.T3.2.2.2.2.m2.1.1.2" xref="A3.T3.2.2.2.2.m2.1.1.2.cmml">Δ</mi><mrow id="A3.T3.2.2.2.2.m2.1.1.3" xref="A3.T3.2.2.2.2.m2.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A3.T3.2.2.2.2.m2.1.1.3.2" xref="A3.T3.2.2.2.2.m2.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A3.T3.2.2.2.2.m2.1.1.3.1" xref="A3.T3.2.2.2.2.m2.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A3.T3.2.2.2.2.m2.1.1.3.3" xref="A3.T3.2.2.2.2.m2.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A3.T3.2.2.2.2.m2.1b"><apply id="A3.T3.2.2.2.2.m2.1.1.cmml" xref="A3.T3.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="A3.T3.2.2.2.2.m2.1.1.1.cmml" xref="A3.T3.2.2.2.2.m2.1.1">superscript</csymbol><ci id="A3.T3.2.2.2.2.m2.1.1.2.cmml" xref="A3.T3.2.2.2.2.m2.1.1.2">Δ</ci><apply id="A3.T3.2.2.2.2.m2.1.1.3.cmml" xref="A3.T3.2.2.2.2.m2.1.1.3"><times id="A3.T3.2.2.2.2.m2.1.1.3.1.cmml" xref="A3.T3.2.2.2.2.m2.1.1.3.1"></times><ci id="A3.T3.2.2.2.2.m2.1.1.3.2.cmml" xref="A3.T3.2.2.2.2.m2.1.1.3.2">𝐷</ci><ci id="A3.T3.2.2.2.2.m2.1.1.3.3.cmml" xref="A3.T3.2.2.2.2.m2.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.2.2.2.2.m2.1c">\Delta^{DP}</annotation></semantics></math> <math id="A3.T3.3.3.3.3.m3.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{EO}" display="inline"><semantics id="A3.T3.3.3.3.3.m3.1a"><msup id="A3.T3.3.3.3.3.m3.1.1" xref="A3.T3.3.3.3.3.m3.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A3.T3.3.3.3.3.m3.1.1.2" xref="A3.T3.3.3.3.3.m3.1.1.2.cmml">Δ</mi><mrow id="A3.T3.3.3.3.3.m3.1.1.3" xref="A3.T3.3.3.3.3.m3.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A3.T3.3.3.3.3.m3.1.1.3.2" xref="A3.T3.3.3.3.3.m3.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="A3.T3.3.3.3.3.m3.1.1.3.1" xref="A3.T3.3.3.3.3.m3.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A3.T3.3.3.3.3.m3.1.1.3.3" xref="A3.T3.3.3.3.3.m3.1.1.3.3.cmml">O</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A3.T3.3.3.3.3.m3.1b"><apply id="A3.T3.3.3.3.3.m3.1.1.cmml" xref="A3.T3.3.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="A3.T3.3.3.3.3.m3.1.1.1.cmml" xref="A3.T3.3.3.3.3.m3.1.1">superscript</csymbol><ci id="A3.T3.3.3.3.3.m3.1.1.2.cmml" xref="A3.T3.3.3.3.3.m3.1.1.2">Δ</ci><apply id="A3.T3.3.3.3.3.m3.1.1.3.cmml" xref="A3.T3.3.3.3.3.m3.1.1.3"><times id="A3.T3.3.3.3.3.m3.1.1.3.1.cmml" xref="A3.T3.3.3.3.3.m3.1.1.3.1"></times><ci id="A3.T3.3.3.3.3.m3.1.1.3.2.cmml" xref="A3.T3.3.3.3.3.m3.1.1.3.2">𝐸</ci><ci id="A3.T3.3.3.3.3.m3.1.1.3.3.cmml" xref="A3.T3.3.3.3.3.m3.1.1.3.3">𝑂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.3.3.3.3.m3.1c">\Delta^{EO}</annotation></semantics></math>  <math id="A3.T3.4.4.4.4.m4.1" class="ltx_Math" style="background-color:#F5FAFC;" alttext="\Delta^{DP}" display="inline"><semantics id="A3.T3.4.4.4.4.m4.1a"><msup id="A3.T3.4.4.4.4.m4.1.1" xref="A3.T3.4.4.4.4.m4.1.1.cmml"><mi mathbackground="#F5FAFC" mathvariant="normal" id="A3.T3.4.4.4.4.m4.1.1.2" xref="A3.T3.4.4.4.4.m4.1.1.2.cmml">Δ</mi><mrow id="A3.T3.4.4.4.4.m4.1.1.3" xref="A3.T3.4.4.4.4.m4.1.1.3.cmml"><mi mathbackground="#F5FAFC" id="A3.T3.4.4.4.4.m4.1.1.3.2" xref="A3.T3.4.4.4.4.m4.1.1.3.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="A3.T3.4.4.4.4.m4.1.1.3.1" xref="A3.T3.4.4.4.4.m4.1.1.3.1.cmml">​</mo><mi mathbackground="#F5FAFC" id="A3.T3.4.4.4.4.m4.1.1.3.3" xref="A3.T3.4.4.4.4.m4.1.1.3.3.cmml">P</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A3.T3.4.4.4.4.m4.1b"><apply id="A3.T3.4.4.4.4.m4.1.1.cmml" xref="A3.T3.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="A3.T3.4.4.4.4.m4.1.1.1.cmml" xref="A3.T3.4.4.4.4.m4.1.1">superscript</csymbol><ci id="A3.T3.4.4.4.4.m4.1.1.2.cmml" xref="A3.T3.4.4.4.4.m4.1.1.2">Δ</ci><apply id="A3.T3.4.4.4.4.m4.1.1.3.cmml" xref="A3.T3.4.4.4.4.m4.1.1.3"><times id="A3.T3.4.4.4.4.m4.1.1.3.1.cmml" xref="A3.T3.4.4.4.4.m4.1.1.3.1"></times><ci id="A3.T3.4.4.4.4.m4.1.1.3.2.cmml" xref="A3.T3.4.4.4.4.m4.1.1.3.2">𝐷</ci><ci id="A3.T3.4.4.4.4.m4.1.1.3.3.cmml" xref="A3.T3.4.4.4.4.m4.1.1.3.3">𝑃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.T3.4.4.4.4.m4.1c">\Delta^{DP}</annotation></semantics></math> 
<br class="ltx_break">, table foot=]data/data_reweiging.csv
 <span id="A3.T3.4.4.4.4.1" class="ltx_ERROR undefined">\csvcoli</span> <span id="A3.T3.4.4.4.4.2" class="ltx_ERROR undefined">\csvcolxii</span><span id="A3.T3.4.4.4.4.3" class="ltx_ERROR undefined">\csvcolii</span> (<span id="A3.T3.4.4.4.4.4" class="ltx_ERROR undefined">\csvcoliii</span>)  <span id="A3.T3.4.4.4.4.5" class="ltx_ERROR undefined">\csvcoliv</span> (<span id="A3.T3.4.4.4.4.6" class="ltx_ERROR undefined">\csvcolv</span>)  <span id="A3.T3.4.4.4.4.7" class="ltx_ERROR undefined">\csvcolvi</span> (<span id="A3.T3.4.4.4.4.8" class="ltx_ERROR undefined">\csvcolvii</span>)  <span id="A3.T3.4.4.4.4.9" class="ltx_ERROR undefined">\csvcolviii</span> (<span id="A3.T3.4.4.4.4.10" class="ltx_ERROR undefined">\csvcolix</span>)  <span id="A3.T3.4.4.4.4.11" class="ltx_ERROR undefined">\csvcolx</span> (<span id="A3.T3.4.4.4.4.12" class="ltx_ERROR undefined">\csvcolxi</span>)</span></span></p>
</span></div>
</figure>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p"><span id="A3.p2.1.1" class="ltx_text">We found that applying reweighing algorithms in FL reduces the average fairness gap across parties. This is due to the fact that, after reweighing, the parties that were initially biased do not introduce significant bias to the model during local training, resulting in a reduction of the fairness gap in the FL model.</span></p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">However, we also noted that the fairness gap in the model remains high for the Race sensitive attribute, indicating that the global and local reweighing algorithms do not entirely eliminate bias in FL. Furthermore, we would like to highlight some concerns regarding the fair FL algorithm:</p>
<ol id="A3.I1" class="ltx_enumerate">
<li id="A3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A3.I1.i1.p1" class="ltx_para">
<p id="A3.I1.i1.p1.1" class="ltx_p">The fair FL algorithm enhances the performance of the minority group at the expense of the majority group. Figure <a href="#A3.F24" title="Figure 24 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a> illustrates that the minority group (i.e., the Female group) has a better performance after the reweighing, but at the cost of reduced performance for the majority.</p>
</div>
</li>
<li id="A3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A3.I1.i2.p1" class="ltx_para">
<p id="A3.I1.i2.p1.1" class="ltx_p">When improving fairness with respect to one sensitive attribute, the fairness issue with respect to another sensitive attribute may worsen, as shown in Table <a href="#A3.T3" title="Table 3 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For instance, when applying local reweighing to reduce bias with respect to the "Sex" attribute, the fairness gap with respect to "Race" increased from 0.514 (results on FedAvg) to 0.521, indicating that reweighing to improve fairness with respect to Sex may amplify the bias with respect to Race. This implies that the bias with respect to Race can still propagate in FL, and our analysis remains valid.</p>
</div>
</li>
<li id="A3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A3.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="A3.I1.i3.p1.1" class="ltx_p">The reweighing algorithms assume that parties are interested in mitigating the bias with respect to the same sensitive attribute, which may not be the case in practice. Figure <a href="#A3.F25" title="Figure 25 ‣ Appendix C Evaluation on existing Fair FL ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a> shows the fairness gap of standalone models with respect to different sensitive attributes. We observe that many parties have a low fairness gap with respect to one sensitive attribute and a high fairness gap with respect to another sensitive attribute. Thus, parties may aim to enhance fairness with respect to different sensitive attributes, which renders this reweighing algorithm unsuitable. This problem is more prevalent in FL, given the different data distributions of parties.</p>
</div>
</li>
</ol>
</div>
<figure id="A3.F24" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2309.02160/assets/images/income/reweiging/income_group_PPV_global_reweighing_fedavg.png" id="A3.F24.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="287" height="216" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2309.02160/assets/images/income/reweiging/income_group_TPR_global_reweighing_fedavg.png" id="A3.F24.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="287" height="216" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Effect of Fair FL algorithm <cite class="ltx_cite ltx_citemacro_citep">[Abay et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>]</cite> on group performance - (Income, Sex)</figcaption>
</figure>
<figure id="A3.F25" class="ltx_figure"><img src="/html/2309.02160/assets/images/income/reweiging/income_standalone_race_sex_gap.png" id="A3.F25.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="180" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>Fairness gap of standalone models with respect to different sensitive attributes - (Income). Each point represents the result for a single party in one run. We show the fairness gap with respect to different sensitive attributes differs a lot for each party.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Extending to a real-world medical dataset</h2>

<figure id="A4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="A4.T4.6.1" class="ltx_text ltx_font_bold">Effect of FL on the real-world medical dataset - (ISIC2019).</span> The standard deviation across four runs is indicated between parentheses. </figcaption>
<div id="A4.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:62.9pt;vertical-align:-54.8pt;"><span class="ltx_transformed_inner" style="transform:translate(16.9pt,-0.3pt) scale(1.0845000976968,1.0845000976968) ;"><span id="A4.T4.4.5" class="ltx_ERROR undefined">\csvreader</span>
<p id="A4.T4.4.4" class="ltx_p">[tabular=c*2a*2e*2d*2f,table head= Setting   <span id="A4.T4.4.4.4" class="ltx_text" style="background-color:#FFF3F3;"> Party 1    <span id="A4.T4.4.4.4.4" class="ltx_text" style="background-color:#F6FCF2;"> Party 2     <span id="A4.T4.4.4.4.4.4" class="ltx_text" style="background-color:#F5FAFC;"> Party 3     <span id="A4.T4.4.4.4.4.4.4" class="ltx_text" style="background-color:#FFF9F8;"> Party 4 
<br class="ltx_break">Algorithm  Accuracy  <math id="A4.T4.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta^{Acc}" display="inline"><semantics id="A4.T4.1.1.1.1.1.1.m1.1a"><msup id="A4.T4.1.1.1.1.1.1.m1.1.1" xref="A4.T4.1.1.1.1.1.1.m1.1.1.cmml"><mi mathbackground="#FFF9F8" mathvariant="normal" id="A4.T4.1.1.1.1.1.1.m1.1.1.2" xref="A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mrow id="A4.T4.1.1.1.1.1.1.m1.1.1.3" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathbackground="#FFF9F8" id="A4.T4.1.1.1.1.1.1.m1.1.1.3.2" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.T4.1.1.1.1.1.1.m1.1.1.3.1" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.1.1.1.1.1.1.m1.1.1.3.3" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.T4.1.1.1.1.1.1.m1.1.1.3.1a" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.1.1.1.1.1.1.m1.1.1.3.4" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml">c</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.T4.1.1.1.1.1.1.m1.1b"><apply id="A4.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.T4.1.1.1.1.1.1.m1.1.1.1.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1">superscript</csymbol><ci id="A4.T4.1.1.1.1.1.1.m1.1.1.2.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1.2">Δ</ci><apply id="A4.T4.1.1.1.1.1.1.m1.1.1.3.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3"><times id="A4.T4.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.1"></times><ci id="A4.T4.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.2">𝐴</ci><ci id="A4.T4.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.3">𝑐</ci><ci id="A4.T4.1.1.1.1.1.1.m1.1.1.3.4.cmml" xref="A4.T4.1.1.1.1.1.1.m1.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.1.1.1.1.1.1.m1.1c">\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id="A4.T4.2.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\Delta^{Acc}" display="inline"><semantics id="A4.T4.2.2.2.2.2.2.m2.1a"><msup id="A4.T4.2.2.2.2.2.2.m2.1.1" xref="A4.T4.2.2.2.2.2.2.m2.1.1.cmml"><mi mathbackground="#FFF9F8" mathvariant="normal" id="A4.T4.2.2.2.2.2.2.m2.1.1.2" xref="A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml">Δ</mi><mrow id="A4.T4.2.2.2.2.2.2.m2.1.1.3" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml"><mi mathbackground="#FFF9F8" id="A4.T4.2.2.2.2.2.2.m2.1.1.3.2" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.T4.2.2.2.2.2.2.m2.1.1.3.1" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.2.2.2.2.2.2.m2.1.1.3.3" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.T4.2.2.2.2.2.2.m2.1.1.3.1a" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.2.2.2.2.2.2.m2.1.1.3.4" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml">c</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.T4.2.2.2.2.2.2.m2.1b"><apply id="A4.T4.2.2.2.2.2.2.m2.1.1.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="A4.T4.2.2.2.2.2.2.m2.1.1.1.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1">superscript</csymbol><ci id="A4.T4.2.2.2.2.2.2.m2.1.1.2.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1.2">Δ</ci><apply id="A4.T4.2.2.2.2.2.2.m2.1.1.3.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3"><times id="A4.T4.2.2.2.2.2.2.m2.1.1.3.1.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.1"></times><ci id="A4.T4.2.2.2.2.2.2.m2.1.1.3.2.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.2">𝐴</ci><ci id="A4.T4.2.2.2.2.2.2.m2.1.1.3.3.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.3">𝑐</ci><ci id="A4.T4.2.2.2.2.2.2.m2.1.1.3.4.cmml" xref="A4.T4.2.2.2.2.2.2.m2.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.2.2.2.2.2.2.m2.1c">\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id="A4.T4.3.3.3.3.3.3.m3.1" class="ltx_Math" alttext="\Delta^{Acc}" display="inline"><semantics id="A4.T4.3.3.3.3.3.3.m3.1a"><msup id="A4.T4.3.3.3.3.3.3.m3.1.1" xref="A4.T4.3.3.3.3.3.3.m3.1.1.cmml"><mi mathbackground="#FFF9F8" mathvariant="normal" id="A4.T4.3.3.3.3.3.3.m3.1.1.2" xref="A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml">Δ</mi><mrow id="A4.T4.3.3.3.3.3.3.m3.1.1.3" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml"><mi mathbackground="#FFF9F8" id="A4.T4.3.3.3.3.3.3.m3.1.1.3.2" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.T4.3.3.3.3.3.3.m3.1.1.3.1" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.3.3.3.3.3.3.m3.1.1.3.3" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.T4.3.3.3.3.3.3.m3.1.1.3.1a" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.3.3.3.3.3.3.m3.1.1.3.4" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml">c</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.T4.3.3.3.3.3.3.m3.1b"><apply id="A4.T4.3.3.3.3.3.3.m3.1.1.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1"><csymbol cd="ambiguous" id="A4.T4.3.3.3.3.3.3.m3.1.1.1.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1">superscript</csymbol><ci id="A4.T4.3.3.3.3.3.3.m3.1.1.2.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1.2">Δ</ci><apply id="A4.T4.3.3.3.3.3.3.m3.1.1.3.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3"><times id="A4.T4.3.3.3.3.3.3.m3.1.1.3.1.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.1"></times><ci id="A4.T4.3.3.3.3.3.3.m3.1.1.3.2.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.2">𝐴</ci><ci id="A4.T4.3.3.3.3.3.3.m3.1.1.3.3.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.3">𝑐</ci><ci id="A4.T4.3.3.3.3.3.3.m3.1.1.3.4.cmml" xref="A4.T4.3.3.3.3.3.3.m3.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.3.3.3.3.3.3.m3.1c">\Delta^{Acc}</annotation></semantics></math>  Accuracy  <math id="A4.T4.4.4.4.4.4.4.m4.1" class="ltx_Math" alttext="\Delta^{Acc}" display="inline"><semantics id="A4.T4.4.4.4.4.4.4.m4.1a"><msup id="A4.T4.4.4.4.4.4.4.m4.1.1" xref="A4.T4.4.4.4.4.4.4.m4.1.1.cmml"><mi mathbackground="#FFF9F8" mathvariant="normal" id="A4.T4.4.4.4.4.4.4.m4.1.1.2" xref="A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml">Δ</mi><mrow id="A4.T4.4.4.4.4.4.4.m4.1.1.3" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml"><mi mathbackground="#FFF9F8" id="A4.T4.4.4.4.4.4.4.m4.1.1.3.2" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A4.T4.4.4.4.4.4.4.m4.1.1.3.1" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.4.4.4.4.4.4.m4.1.1.3.3" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="A4.T4.4.4.4.4.4.4.m4.1.1.3.1a" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml">​</mo><mi mathbackground="#FFF9F8" id="A4.T4.4.4.4.4.4.4.m4.1.1.3.4" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml">c</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="A4.T4.4.4.4.4.4.4.m4.1b"><apply id="A4.T4.4.4.4.4.4.4.m4.1.1.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1"><csymbol cd="ambiguous" id="A4.T4.4.4.4.4.4.4.m4.1.1.1.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1">superscript</csymbol><ci id="A4.T4.4.4.4.4.4.4.m4.1.1.2.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1.2">Δ</ci><apply id="A4.T4.4.4.4.4.4.4.m4.1.1.3.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3"><times id="A4.T4.4.4.4.4.4.4.m4.1.1.3.1.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.1"></times><ci id="A4.T4.4.4.4.4.4.4.m4.1.1.3.2.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.2">𝐴</ci><ci id="A4.T4.4.4.4.4.4.4.m4.1.1.3.3.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.3">𝑐</ci><ci id="A4.T4.4.4.4.4.4.4.m4.1.1.3.4.cmml" xref="A4.T4.4.4.4.4.4.4.m4.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T4.4.4.4.4.4.4.m4.1c">\Delta^{Acc}</annotation></semantics></math> 
<br class="ltx_break">, table foot=]data/isic2019.csv
 <span id="A4.T4.4.4.4.4.4.4.1" class="ltx_ERROR undefined">\csvcolii</span> <span id="A4.T4.4.4.4.4.4.4.2" class="ltx_ERROR undefined">\csvcoliii</span> (<span id="A4.T4.4.4.4.4.4.4.3" class="ltx_ERROR undefined">\csvcoliv</span>)  <span id="A4.T4.4.4.4.4.4.4.4" class="ltx_ERROR undefined">\csvcolv</span> (<span id="A4.T4.4.4.4.4.4.4.5" class="ltx_ERROR undefined">\csvcolvi</span>)  <span id="A4.T4.4.4.4.4.4.4.6" class="ltx_ERROR undefined">\csvcolvii</span> (<span id="A4.T4.4.4.4.4.4.4.7" class="ltx_ERROR undefined">\csvcolviii</span>)  <span id="A4.T4.4.4.4.4.4.4.8" class="ltx_ERROR undefined">\csvcolix</span> (<span id="A4.T4.4.4.4.4.4.4.9" class="ltx_ERROR undefined">\csvcolx</span>)  <span id="A4.T4.4.4.4.4.4.4.10" class="ltx_ERROR undefined">\csvcolxi</span> (<span id="A4.T4.4.4.4.4.4.4.11" class="ltx_ERROR undefined">\csvcolxii</span>)  <span id="A4.T4.4.4.4.4.4.4.12" class="ltx_ERROR undefined">\csvcolxiii</span> (<span id="A4.T4.4.4.4.4.4.4.13" class="ltx_ERROR undefined">\csvcolxiv</span>)  <span id="A4.T4.4.4.4.4.4.4.14" class="ltx_ERROR undefined">\csvcolxv</span> (<span id="A4.T4.4.4.4.4.4.4.15" class="ltx_ERROR undefined">\csvcolxvi</span>)  <span id="A4.T4.4.4.4.4.4.4.16" class="ltx_ERROR undefined">\csvcolxvii</span> (<span id="A4.T4.4.4.4.4.4.4.17" class="ltx_ERROR undefined">\csvcolxviii</span>)</span></span></span></span></p>
</span></div>
</figure>
<figure id="A4.F26" class="ltx_figure"><img src="/html/2309.02160/assets/images/isic/hist.png" id="A4.F26.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="357" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>Number of samples from each class for each party - ISIC2019</figcaption>
</figure>
<div id="A4.p1" class="ltx_para ltx_noindent">
<p id="A4.p1.1" class="ltx_p">We evaluate the effect of FL on local fairness on a real-world medical dataset, ISIC2019 <cite class="ltx_cite ltx_citemacro_citep">[Abay et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>, Tschandl et al., <a href="#bib.bib45" title="" class="ltx_ref">2018</a>, Combalia et al., <a href="#bib.bib6" title="" class="ltx_ref">2019</a>]</cite>, which contains dermoscopic images of skin lesions collected from six medical centers. The task is to classify dermoscopic images among nine different diagnostic categories. We filter out medical centers with less than 1900 images and regard each medical center with over 1900 images as a party (4 centers remain after filtering). For every party, we randomly select 1500 and 400 images for training and validation, respectively. We regard the binary notion of Sex as our sensitive attribute. In this multi-class and medical dataset setting, we measure the bias (fairness gap) based on the accuracy gap across groups. Following  <cite class="ltx_cite ltx_citemacro_citep">[Ogier du Terrail et al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>]</cite> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/owkin/FLamby</span></span></span>, we train EfficientNets <cite class="ltx_cite ltx_citemacro_citep">[Tan and Le, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>]</cite>, the state-of-the-art model structure for medical data, under the centralized, FL, and standalone setting. The accuracy and fairness gap for each client in different settings are shown in Table <a href="#A4.T4" title="Table 4 ‣ Appendix D Extending to a real-world medical dataset ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.1" class="ltx_p">FedAvg does not improve accuracy, possibly due to data heterogeneity among parties (as shown in Figure <a href="#A4.F26" title="Figure 26 ‣ Appendix D Extending to a real-world medical dataset ‣ 8 Reproducibility Statement ‣ 7 Ethics Statement ‣ 6 Acknowledgement ‣ 5 Future Work &amp; Conclusion ‣ 4 Related work ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">26</span></a>), resulting in the model’s instability and slow convergence, which is known as ‘client-drift’ issue <cite class="ltx_cite ltx_citemacro_citep">[Karimireddy et al., <a href="#bib.bib24" title="" class="ltx_ref">2020b</a>]</cite>.</p>
</div>
<div id="A4.p3" class="ltx_para ltx_noindent">
<p id="A4.p3.1" class="ltx_p">Moreover, the FedAvg model results in a higher accuracy gap between groups, worsening the fairness issue compared to the standalone setting or the centralized setting (see the fairness gaps for party 1, party 3, and party 4 as examples). Our results suggest that, in real-world settings, FedAvg may not improve accuracy compared to the standalone setting for the local data distribution and may even exacerbate the fairness issue, leading to a more biased model.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Potential Mitigation</h2>

<div id="A5.p1" class="ltx_para ltx_noindent">
<p id="A5.p1.1" class="ltx_p">In this section, we suggest some potential solutions to reduce the effect of bias propagation in FL. Our hope is that this will inspire further research on developing fair FL in the future.</p>
</div>
<section id="A5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Personalized FL</h4>

<div id="A5.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="A5.SS0.SSS0.Px1.p1.1" class="ltx_p">Our study has revealed that aggregate updates, which involve the average local update from all parties, may result in a higher bias than local updates for less biased parties (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). This can cause bias from other parties to propagate to the local model, thus increasing the local fairness gap. To mitigate the bias propagation effect, one possible approach is to avoid completely overwriting the local model with the global model. Personalized Federated Learning (PFL) techniques <cite class="ltx_cite ltx_citemacro_citep">[Li et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>]</cite> have recently been proposed to address this issue by focusing on improving the model’s accuracy. PFL can be a promising direction for developing fairness-aware personalized federated learning approaches that can help reduce the bias propagation effect in FL.</p>
</div>
</section>
<section id="A5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Fair Representation Learning</h4>

<div id="A5.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="A5.SS0.SSS0.Px2.p1.1" class="ltx_p">Our analysis demonstrated that bias is encoded in a small number of parameters in the first layer of the neural network, which is typically considered the feature extractor (as illustrated in Figure <a href="#S3.F10" title="Figure 10 ‣ 3.4 How is bias propagated in FL? ‣ 3.3 FL propagates bias among parties ‣ 3.2 Collaboration via FL can worsen fairness issue ‣ 3 Empirical Analysis ‣ Bias Propagation in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). This suggests that the FL model is biased because the learned representation is biased, heavily influenced by the sensitive attribute. To address this issue, one possible solution is to implement fair representation learning techniques <cite class="ltx_cite ltx_citemacro_citep">[Zemel et al., <a href="#bib.bib50" title="" class="ltx_ref">2013</a>, Zhao et al., <a href="#bib.bib54" title="" class="ltx_ref">2019</a>, Liu et al., <a href="#bib.bib30" title="" class="ltx_ref">2022</a>, Zeng et al., <a href="#bib.bib52" title="" class="ltx_ref">2021b</a>]</cite> that aim to learn a feature extractor which is fair, meaning that it minimizes the dependence on the sensitive attribute while still retaining enough information about the task (or input features). Therefore, using fair representation learning in FL can potentially mitigate the propagation of bias effects.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.02159" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.02160" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.02160">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.02160" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.02161" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:41:36 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
