<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining</title>
<!--Generated on Tue Sep  3 22:34:02 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.02326v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S1" title="In Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2" title="In Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text">Arctic-SnowCoder</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS1" title="In 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Raw data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS2" title="In 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>General pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS3" title="In 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Continued pretraining with high-quality data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS4" title="In 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Enhanced pretraining with synthetic data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3" title="In 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS1" title="In 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="In 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Baseline comparison and effectiveness of three-stage pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS3" title="In 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Repo-level data in general pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4" title="In 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Design choices in continued pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px1" title="In 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title">Model-based quality annotator</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px2" title="In Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title">Learning rate schedule</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px3" title="In Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title">Repetitions of high-quality data</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S4" title="In Repetitions of high-quality data ‣ Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S4.SS0.SSS0.Px1" title="In 4 Related Work ‣ Repetitions of high-quality data ‣ Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title">Code pretraining corpus for language models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S4.SS0.SSS0.Px2" title="In 4 Related Work ‣ Repetitions of high-quality data ‣ Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title">Model-based quality filtering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph">
<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S4.SS0.SSS0.Px3" title="In 4 Related Work ‣ Repetitions of high-quality data ‣ Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title">High-quality code data for pretraining</span></a>
<ol class="ltx_toclist ltx_toclist_paragraph">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S5" title="In High-quality code data for pretraining ‣ 4 Related Work ‣ Repetitions of high-quality data ‣ Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\UseTblrLibrary</span>
<p class="ltx_p" id="p1.2">booktabs
<span class="ltx_ERROR undefined" id="p1.2.1">\SetTblrInner</span>[booktabs]abovesep=0pt, belowsep=0pt, rowsep=0.15pt, colsep=3.5pt
<span class="ltx_ERROR undefined" id="p1.2.2">\SetTblrInner</span>[booktabs]cells = cmd=
<span class="ltx_ERROR undefined" id="p1.2.3">\NewTableCommand</span><span class="ltx_ERROR undefined" id="p1.2.4">\seprule</span><span class="ltx_ERROR undefined" id="p1.2.5">\specialrule</span><span class="ltx_ERROR undefined" id="p1.2.6">\lightrulewidth</span>,gray82pt2pt
<span class="ltx_ERROR undefined" id="p1.2.7">\NewTableCommand</span><span class="ltx_ERROR undefined" id="p1.2.8">\uniquerule</span><span class="ltx_ERROR undefined" id="p1.2.9">\specialrule</span><span class="ltx_ERROR undefined" id="p1.2.10">\lightrulewidth</span>,gray7,dashed2pt2pt





































































</p>
</div>
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text" id="id1.id1">Arctic-SnowCoder</span>: Demystifying High-Quality Data in Code Pretraining</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yuxiang Wei<sup class="ltx_sup" id="id2.1.id1">2</sup> Hojae Han<sup class="ltx_sup" id="id3.2.id2">1,3</sup> Rajhans Samdani<sup class="ltx_sup" id="id4.3.id3">1</sup> 
<br class="ltx_break"/>
<sup class="ltx_sup" id="id5.4.id4">1</sup>Snowflake AI Research

<br class="ltx_break"/>
<sup class="ltx_sup" id="id6.5.id5">2</sup>University of Illinois at Urbana-Champaign
 <sup class="ltx_sup" id="id7.6.id6">3</sup>Seoul National University

<br class="ltx_break"/>
<span class="ltx_text ltx_font_typewriter" id="id8.7.id7">ywei40@illinois.edu  {hojae.han,rajhans.samdani}@snowflake.com</span>
</span><span class="ltx_author_notes">Work done during an internship at Snowflake AI Research.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of “high-quality” remains underexplored. Focusing on the code domain, we introduce <span class="ltx_text" id="id9.id1.1">Arctic-SnowCoder</span>-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data:
(1) <em class="ltx_emph ltx_font_italic" id="id9.id1.2">general pretraining</em> with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination,
(2) <em class="ltx_emph ltx_font_italic" id="id9.id1.3">continued pretraining</em> with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct,
and (3) <em class="ltx_emph ltx_font_italic" id="id9.id1.4">enhanced pretraining</em> with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining.
Despite being trained on a limited dataset, <span class="ltx_text" id="id9.id1.5">Arctic-SnowCoder</span> achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%.
Across all evaluated benchmarks, <span class="ltx_text" id="id9.id1.6">Arctic-SnowCoder</span>-1.3B beats StarCoderBase-3B pretrained on 1T tokens.
Additionally, it matches the performance of leading small base code models trained on trillions of tokens.
For example, <span class="ltx_text" id="id9.id1.7">Arctic-SnowCoder</span>-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. Our evaluation presents a comprehensive analysis justifying various design choices for <span class="ltx_text" id="id9.id1.8">Arctic-SnowCoder</span>. Most importantly, we find that the key to high-quality data is its alignment with the distribution of downstream applications.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Three-phase pretraining of <span class="ltx_text" id="S0.F1.2.1">Arctic-SnowCoder</span>-1.3B with progressively higher-quality data.</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pretraining large language models (LLMs) has generally relied on vast quantities of data. This emphasis on data volume is especially true in specialized domains like code, where researchers obtain massive code pretraining datasets by crawling platforms like GitHub <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>.
Recent studies, however, have increasingly showed that high-quality data is crucial for effective pretraining <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib1" title="">1</a>]</cite>, including the code domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the general domain, researchers have explored various techniques to curate high-quality pretraining data for language models. FineWeb-Edu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib30" title="">30</a>]</cite> uses a linear regressor built on <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.1">Snowflake-arctic-embed-m</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib26" title="">26</a>]</cite> embeddings to assess the educational value of web pages and select high-quality content, while the DCLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib18" title="">18</a>]</cite> approach employs a <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.2">fastText</span>-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib5" title="">5</a>]</cite> filter trained on positive examples from high-quality online sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib39" title="">39</a>]</cite> and instruction data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite>, and random negative web pages to identify high-quality text. These model-based quality filters have been shown to significantly enhance language model performance on downstream tasks, compared to using unfiltered, large-scale datasets. Similarly, researchers have recognized the importance of high-quality code data for pretraining, with Phi-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib13" title="">13</a>]</cite> using a random forest classifier on CodeGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib28" title="">28</a>]</cite> embeddings to select educational code samples, and DeepSeek-Coder-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib9" title="">9</a>]</cite> employing a multi-stage <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.3">fastText</span>-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib5" title="">5</a>]</cite> pipeline to recall web-related code data and high-quality code from GitHub, achieving state-of-the-art coding performance.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we introduce <span class="ltx_text" id="S1.p3.1.1">Arctic-SnowCoder</span>-1.3B, a high-performing small code model created by a novel three-step training methodology focused on progressive improvements in data quality. As a result of this methodology, <span class="ltx_text" id="S1.p3.1.2">Arctic-SnowCoder</span>-1.3B outperforms StarCoderBase-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>]</cite> across all evaluated benchmarks and exceeds Phi-1.5-1.3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib20" title="">20</a>]</cite> by 36% on the complex and practical BigCodeBench benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib46" title="">46</a>]</cite>, a benchmark that truly matters for real-world programming.
As shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S0.F1" title="In Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, <span class="ltx_text" id="S1.p3.1.3">Arctic-SnowCoder</span> is developed through a three-stage, data-efficient pretraining process that progressively refines the quality of the data used. The first stage involves general pretraining for a 500B token horizon using 400B unique raw code data, which have been preprocessed through basic filtering, deduplication, and decontamination. The 400B raw corpus is primarily derived from the coding data used to train Snowflake Arctic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib32" title="">32</a>]</cite>,
combining cleaned The Stack v1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>]</cite> and GitHub crawls.
This is followed by continued pretraining on 50B tokens, utilizing a smaller, high-quality subset of 12.5B code files, repeated four times. The high-quality tokens are selected from phase one by a BERT-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib11" title="">11</a>]</cite> quality annotator trained to distinguish good code from random data, using positive examples drawn from publicly available high-quality code files <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib39" title="">39</a>]</cite>, along with instruction data from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> and StarCoder2-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib40" title="">40</a>]</cite>.
Finally, the model undergoes an enhanced pretraining phase for 5B tokens, leveraging roughly 2B synthetic data generated by Llama-3.1-70B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib12" title="">12</a>]</cite>. This process uses the phase two data as seeds and adapts the OSS-Instruct methodology from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> by transforming lower-quality seed code into high-quality code documents.
Notably, all training phases of <span class="ltx_text" id="S1.p3.1.4">Arctic-SnowCoder</span> derive data from the same raw pretraining corpus, ensuring that minimal new knowledge is introduced.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text" id="S1.p4.1.1">Arctic-SnowCoder</span>-1.3B achieves state-of-the-art results on BigCodeBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib46" title="">46</a>]</cite>, a coding benchmark focusing on practical and challenging programming tasks, among models of similar size trained with <math alttext="\leq" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mo id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><leq id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">≤</annotation></semantics></math> 1T tokens. Particularly, it outperforming Phi-1.5-1.3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib20" title="">20</a>]</cite> by 36%. Despite being trained on 555B tokens, compared to other state-of-the-art small code models trained on trillions of tokens, <span class="ltx_text" id="S1.p4.1.2">Arctic-SnowCoder</span> matches or surpasses the performance of these models on several benchmarks. For instance, <span class="ltx_text" id="S1.p4.1.3">Arctic-SnowCoder</span>-1.3B beats StarCoderBase-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>]</cite>, trained on over 1T tokens, across all evaluated benchmarks. <span class="ltx_text" id="S1.p4.1.4">Arctic-SnowCoder</span>-1.3B outperforms StarCoder2-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite>, trained on over 3T tokens, on HumanEval+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib21" title="">21</a>]</cite> (28.0 vs. 27.4), a benchmark evaluating function-level code generation, while remaining competitive on BigCodeBench (19.4 vs. 21.4).
We conduct comprehensive ablation studies to validate the design decisions behind training <span class="ltx_text" id="S1.p4.1.5">Arctic-SnowCoder</span>:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">First, our findings indicate that, in general pretraining, organizing file-level data into repositories after partitioning by programming language significantly outperforms the approach of grouping data solely by repository names.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Additionally, we determine the optimal learning rate schedule, which involves a re-warmup phase followed by linear decay, as well as the ideal repetition of high-quality data during continued pretraining, which we find to be four times.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">More importantly, our comparisons of model-based quality annotators, trained on various data combinations, highlight that the alignment of pretraining data with downstream tasks is crucial for achieving superior performance.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In summary, we make the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We introduce <span class="ltx_text" id="S1.I2.i1.p1.1.1">Arctic-SnowCoder</span>-1.3B, a high-performing small code model trained on 555B tokens that benefits from progressive improvements in data quality.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We demonstrate that high-quality data and synthetic data can significantly improve the model performance despite being seeded from the same raw corpus.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">For the first time, we demystify the notion of data quality in code pretraining by systematically comparing model-based quality annotators trained on different data combinations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i4.p1">
<p class="ltx_p" id="S1.I2.i4.p1.1">We provide practical insights into optimal design choices for repo-level grouping in general pretraining, and optimal learning rate schedules and repetitions of high-quality data during continued pretraining, providing practical guidelines for future model development.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text" id="S2.1.1">Arctic-SnowCoder</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we provide a detailed explanation of the training methodology used for <span class="ltx_text" id="S2.p1.1.1">Arctic-SnowCoder</span>-1.3B, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S0.F1" title="In Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>. We begin by discussing the composition of the raw training data in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS1" title="2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>, followed by an overview of the general pretraining phase in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS2" title="2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>. Next, we describe the continued pretraining process using high-quality data in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS3" title="2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>, and finally, we elaborate on the enhanced pretraining with synthetic data in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2.SS4" title="2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.4</span></a>. The model architecture is based on Llama-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib38" title="">38</a>]</cite>, with specific details provided in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S2" title="2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.tab1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model architecture details of <span class="ltx_text" id="S2.tab1.2.1">Arctic-SnowCoder</span>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S2.tab1.3">{booktabs}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S2.tab1.4">colspec=@ll@,

Parameter  <span class="ltx_text" id="S2.tab1.4.1">Arctic-SnowCoder</span>-1.3B 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.2">hidden_dim</span>  2048 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.3">ffn_hidden_dim</span>  5632 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.4">num_heads</span>  16 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.5">num_kv_heads</span>  16 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.6">num_layers</span>  24 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.7">vocab_size</span>  64000 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.8">seq_len</span>  8192 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.9">positional_encodings</span>  RoPE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib35" title="">35</a>]</cite>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="S2.tab1.4.10">tie_embeddings_and_output_weights</span>  True 
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsection ltx_figure_panel" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Raw data</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The raw pretraining data used to train <span class="ltx_text" id="S2.SS1.p1.1.1">Arctic-SnowCoder</span>-1.3B consists exclusively of code, primarily derived from the coding data used to train Snowflake Arctic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib32" title="">32</a>]</cite>. This data combines cleaned versions of The Stack v1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>]</cite> and GitHub crawls. From this data, we select 18 popular programming languages for training, similar to StarCoder2-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite>. These languages include Python, Java, C++, C, JavaScript, PHP, C#, Go, TypeScript, SQL, Ruby, Rust, Jupyter Notebook, Scala, Kotlin, Shell, Dart, Swift, amounting to a total of 400B unique tokens.</p>
</div>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>General pretraining</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.2">In general pretraining, the model is trained for 500B tokens with a sequence length of 8,192 and a batch size of 512 using Adam <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib16" title="">16</a>]</cite>. The learning rate follows a cosine decay after a linear warmup of 600 iterations. We set the maximum learning rate to <math alttext="5.3\times 10^{-4}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">5.3</mn><mo id="S2.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.p1.1.m1.1.1.1.cmml">×</mo><msup id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml"><mn id="S2.SS2.p1.1.m1.1.1.3.2" xref="S2.SS2.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS2.p1.1.m1.1.1.3.3" xref="S2.SS2.p1.1.m1.1.1.3.3.cmml"><mo id="S2.SS2.p1.1.m1.1.1.3.3a" xref="S2.SS2.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS2.p1.1.m1.1.1.3.3.2" xref="S2.SS2.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><times id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1"></times><cn id="S2.SS2.p1.1.m1.1.1.2.cmml" type="float" xref="S2.SS2.p1.1.m1.1.1.2">5.3</cn><apply id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3">superscript</csymbol><cn id="S2.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.3.2">10</cn><apply id="S2.SS2.p1.1.m1.1.1.3.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3"><minus id="S2.SS2.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS2.p1.1.m1.1.1.3.3"></minus><cn id="S2.SS2.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.SS2.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">5.3\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">5.3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and the minimum to <math alttext="5.3\times 10^{-5}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mrow id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mn id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">5.3</mn><mo id="S2.SS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.p1.2.m2.1.1.1.cmml">×</mo><msup id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml"><mn id="S2.SS2.p1.2.m2.1.1.3.2" xref="S2.SS2.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S2.SS2.p1.2.m2.1.1.3.3" xref="S2.SS2.p1.2.m2.1.1.3.3.cmml"><mo id="S2.SS2.p1.2.m2.1.1.3.3a" xref="S2.SS2.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S2.SS2.p1.2.m2.1.1.3.3.2" xref="S2.SS2.p1.2.m2.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><times id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1.1"></times><cn id="S2.SS2.p1.2.m2.1.1.2.cmml" type="float" xref="S2.SS2.p1.2.m2.1.1.2">5.3</cn><apply id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.3.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3">superscript</csymbol><cn id="S2.SS2.p1.2.m2.1.1.3.2.cmml" type="integer" xref="S2.SS2.p1.2.m2.1.1.3.2">10</cn><apply id="S2.SS2.p1.2.m2.1.1.3.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3.3"><minus id="S2.SS2.p1.2.m2.1.1.3.3.1.cmml" xref="S2.SS2.p1.2.m2.1.1.3.3"></minus><cn id="S2.SS2.p1.2.m2.1.1.3.3.2.cmml" type="integer" xref="S2.SS2.p1.2.m2.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">5.3\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">5.3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, following DeepSeek-Coder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib14" title="">14</a>]</cite>.
In this phase, we use the entire 400B raw data without applying additional quality filtering. We start by partitioning code files by programming language, grouping them by repository, and then concatenating them in random order, similar to the StarCoder2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite> approach.
In <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS3" title="3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, we show the advantage of first partitioning code files by programming language.
We name the model produced by this phase as <span class="ltx_text" id="S2.SS2.p1.2.1">Arctic-SnowCoder-alpha</span>.</p>
</div>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Continued pretraining with high-quality data</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">After general pretraining, we continue pretraining <span class="ltx_text" id="S2.SS3.p1.1.1">Arctic-SnowCoder-alpha</span> with 50B high-quality tokens sourced from the same raw pretraining corpus. The 50B high-quality tokens are formed by repeating 12.5B top-percentile code file tokens for 4 times scored by our code quality annotator.
Inspired by FineWeb-Edu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib30" title="">30</a>]</cite> and DCLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib18" title="">18</a>]</cite>,
we train a linear classification head on top of <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.2">Snowflake-arctic-embed-m</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib26" title="">26</a>]</cite>, a state-of-the-art embedding model based on BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib11" title="">11</a>]</cite>.
The training data comprises 300k positive examples, sampled from a blend of 220k high-quality open-source code files <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib39" title="">39</a>]</cite>, 80k high-quality instruction data from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> and StarCoder2-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib40" title="">40</a>]</cite>, and 300 randomly selected code documents from the pretraining corpus.
Prior research on code quality, such as Phi-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib13" title="">13</a>]</cite>, often overemphasizes the “educational value” of code, skewing models towards simpler benchmarks like HumanEval+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib21" title="">21</a>]</cite>. In <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, we show that our annotation leads to a more balanced enhancement of model capabilities.
Furthermore, given that these code documents typically exceed 1000 tokens, surpassing the BERT context window size of 512, we improve over FineWeb-Edu’s pipeline to calculate the score for each file by averaging the scores from the top, middle, and bottom sections as produced by the quality annotator.
In this phase, we rewarmup the learning rate for 1000 iterations from 0 to <math alttext="5.3\times 10^{-4}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mn id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">5.3</mn><mo id="S2.SS3.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS3.p1.1.m1.1.1.1.cmml">×</mo><msup id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml"><mn id="S2.SS3.p1.1.m1.1.1.3.2" xref="S2.SS3.p1.1.m1.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p1.1.m1.1.1.3.3" xref="S2.SS3.p1.1.m1.1.1.3.3.cmml"><mo id="S2.SS3.p1.1.m1.1.1.3.3a" xref="S2.SS3.p1.1.m1.1.1.3.3.cmml">−</mo><mn id="S2.SS3.p1.1.m1.1.1.3.3.2" xref="S2.SS3.p1.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><cn id="S2.SS3.p1.1.m1.1.1.2.cmml" type="float" xref="S2.SS3.p1.1.m1.1.1.2">5.3</cn><apply id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3">superscript</csymbol><cn id="S2.SS3.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S2.SS3.p1.1.m1.1.1.3.2">10</cn><apply id="S2.SS3.p1.1.m1.1.1.3.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3"><minus id="S2.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S2.SS3.p1.1.m1.1.1.3.3"></minus><cn id="S2.SS3.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="S2.SS3.p1.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">5.3\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">5.3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, the maximum pretraining learning rate, followed by a linear decay to 0. The model produced in this phase is referred to as <span class="ltx_text" id="S2.SS3.p1.1.3">Arctic-SnowCoder-beta</span>. In <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4" title="3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, we validate all of our design choices.</p>
</div>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Enhanced pretraining with synthetic data</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">In the enhanced pretraining stage, we generate even higher-quality data than in continued pretraining leveraging Llama-3.1-70B-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib12" title="">12</a>]</cite> and increase the Python mix ratio to approximately 50% while keeping the proportions of the other languages unchanged.
Phi-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib13" title="">13</a>]</cite> demonstrates that synthetic, textbook-like pretraining data can significantly enhance model performance. However, overemphasis on such data risks skewing the model’s distribution, potentially impairing its effectiveness in real-world coding tasks.
For example, we show in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> that Phi-1.5 excels in HumanEval+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib21" title="">21</a>]</cite> and MBPP+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib21" title="">21</a>]</cite>, which resemble textbook exercises, but performs less effectively on the more complex and practical coding tasks in BigCodeBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib46" title="">46</a>]</cite>.
To address this, we adapt the OSS-Instruct method from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> for pretraining purposes. Originally, OSS-Instruct was originally designed to generate realistic instruction-tuning data by prompting a model to create question-answer pairs inspired by open-source code snippets. In contrast, we produce high-quality synthetic pretraining data by using Llama-3.1-70B-Instruct to generate high-quality and problem-solving oriented code files, seeded with code documents scored in the top percentile during the continued pretraining phase.
In <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, we demonstrate that each pretraining phase significantly outperforms the previous one, highlighting the effectiveness of progressively enhancing data quality.</p>
</div>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we compare <span class="ltx_text" id="S3.p1.1.1">Arctic-SnowCoder</span> with state-of-the-art small language models and show performance boost over each pretraining stage (<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>), evaluate two strategies of forming repo-level data in general pretraining (<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS3" title="3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>), and perform detailed ablation to justify our design choices in continued pretraining (<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4" title="3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We consider the following four diverse programming benchmarks to comprehensively evaluate the code generation capability of different code models:</p>
<dl class="ltx_description" id="S3.I1">
<dt class="ltx_item" id="S3.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix1.1.1.1">HumanEval+ and MBPP+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib21" title="">21</a>]</cite>.</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">HumanEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib7" title="">7</a>]</cite> and MBPP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib4" title="">4</a>]</cite> are the two most widely-used benchmarks for function-level code generation. We adopt their augmented version powered by EvalPlus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib21" title="">21</a>]</cite>, with 80×/35× more test cases for rigorous evaluation. HumanEval+ and MBPP+ include 164 and 378 coding problems, respectively.</p>
</div>
</dd>
<dt class="ltx_item" id="S3.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix2.1.1.1">EvoEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib43" title="">43</a>]</cite></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">is a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains. We employ its five default transformation categories, namely <span class="ltx_text ltx_font_typewriter" id="S3.I1.ix2.p1.1.1">difficult</span>, <span class="ltx_text ltx_font_typewriter" id="S3.I1.ix2.p1.1.2">creative</span>, <span class="ltx_text ltx_font_typewriter" id="S3.I1.ix2.p1.1.3">subtle</span>, <span class="ltx_text ltx_font_typewriter" id="S3.I1.ix2.p1.1.4">combine</span> and <span class="ltx_text ltx_font_typewriter" id="S3.I1.ix2.p1.1.5">tool_use</span>, totaling 500 tasks.</p>
</div>
</dd>
<dt class="ltx_item" id="S3.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix3.1.1.1">BigCodeBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib46" title="">46</a>]</cite></span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">evaluates LLMs with practical and challenging programming tasks. It has 1140 programming tasks, where each task in BigCodeBench is created through human-LLM collaboration, where the task quality is ensured by human experts.</p>
</div>
</dd>
</dl>
<p class="ltx_p" id="S3.SS1.p1.2">We incorporate HumanEval+, MBPP+, EvoEval, and BigCodeBench for baseline comparison in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>. For the subsequent ablation studies in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS3" title="3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">3.3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4" title="3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">3.4</span></a>,
we include the base versions of HumanEval and MBPP while omitting BigCodeBench for faster evaluation.
Throughout the experiments, we report the pass@1 metric <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib7" title="">7</a>]</cite> using greedy decoding.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Baseline comparison and effectiveness of three-stage pretraining</h3>
<figure class="ltx_table" id="S3.SS2.2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparing <span class="ltx_text" id="S3.SS2.2.7.1">Arctic-SnowCoder</span> with state-of-the-art small language models (&lt; 3B), divided by whether training compute &gt; 1T tokens. <span class="ltx_text" id="S3.SS2.2.8.2">Arctic-SnowCoder-alpha</span> and <span class="ltx_text" id="S3.SS2.2.9.3">Arctic-SnowCoder-beta</span> are checkpoints after general pretraining and continued pretraining with high-quality data, respectively. <span class="ltx_text" id="S3.SS2.2.10.4">Arctic-SnowCoder</span> is the final checkpoint after enhanced pretraining with synthetic data.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.SS2.2.11">{booktabs}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS2.2.12">colspec=@lrrrrr@,

Model  Training compute  HumanEval+  MBPP+  EvoEval  BigCodeBench 
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS2.2.2">StableCode-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib31" title="">31</a>]</cite>  1.3T  26.2  43.9  18.6  25.9 
<br class="ltx_break"/>StarCoder2-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite>  3.3T to 4.3T  27.4  <span class="ltx_text ltx_font_bold" id="S3.SS2.2.2.1">49.2</span>  19.0  21.4 
<br class="ltx_break"/>Granite-Code-Base-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib27" title="">27</a>]</cite>  4.5T  29.3  45.8  <span class="ltx_text ltx_font_bold" id="S3.SS2.2.2.2">19.8</span>  20.0 
<br class="ltx_break"/>CodeGemma-2B-v1.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib36" title="">36</a>]</cite>  3T + 1T  18.3  46.3  15.4  23.9 
<br class="ltx_break"/>CodeGemma-2B-v1.1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib36" title="">36</a>]</cite>  3T + 500B  <span class="ltx_text ltx_font_bold" id="S3.SS2.2.2.3">32.3</span>  48.9  <span class="ltx_text ltx_font_bold" id="S3.SS2.2.2.4">19.8</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.2.5">28.0</span>
<br class="ltx_break"/>Qwen1.5-1.8B<sup class="ltx_sup" id="S3.SS2.2.2.6">1</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib44" title="">44</a>]</cite>  3T  19.5  28.3  5.0  6.3 
<br class="ltx_break"/>Qwen2-1.5B<sup class="ltx_sup" id="S3.SS2.2.2.7">1</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib44" title="">44</a>]</cite>  7T  31.1  38.4  17.2  16.5 
<br class="ltx_break"/>DeepSeek-Coder-1.3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib14" title="">14</a>]</cite>  2T  28.7  48.1  19.2  22.2 
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS2.2.13">StarCoderBase-3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>]</cite>  1T  17.7  36.8  11.6  5.9 
<br class="ltx_break"/>SmolLM-1.7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib2" title="">2</a>]</cite>  1T  15.9  34.7  10.0  2.5 
<br class="ltx_break"/>Phi-1.5-1.3B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib20" title="">20</a>]</cite>  150B  <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.1">31.7</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.2">43.7</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.3">20.6</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.4">14.3</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="S3.SS2.2.13.5">\seprule</span><span class="ltx_text" id="S3.SS2.2.13.6">Arctic-SnowCoder-alpha</span>-1.3B  500B  14.0  27.8  7.4  10.3 
<br class="ltx_break"/><span class="ltx_text" id="S3.SS2.2.13.7">Arctic-SnowCoder-beta</span>-1.3B  500B + 50B  21.3  34.7  12.8  12.3 
<br class="ltx_break"/><span class="ltx_text" id="S3.SS2.2.13.8">Arctic-SnowCoder</span>-1.3B  550B + 5B  <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.9">28.0</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.10">42.9</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.11">18.0</span> <span class="ltx_text ltx_font_bold" id="S3.SS2.2.13.12">19.4</span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_figure_panel" id="S3.I2">
<li class="ltx_item" id="S3.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span>
<div class="ltx_para" id="S3.I2.ix1.p1">
<p class="ltx_p" id="S3.I2.ix1.p1.1"><sup class="ltx_sup" id="S3.I2.ix1.p1.1.1"><span class="ltx_text ltx_font_italic" id="S3.I2.ix1.p1.1.1.1" style="font-size:70%;">1</span></sup><span class="ltx_text" id="S3.I2.ix1.p1.1.2" style="font-size:70%;"> We remove trailing newlines from prompts in most HumanEval (+) and EvoEval evaluations. However, for Qwen1.5-1.8B and Qwen2-1.5B, we keep them due to their high sensitivity (&gt;15 points drop) to newlines.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS2.2.14"><a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS2" title="3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> presents a comprehensive comparison of various small language models (less than 3B parameters) across multiple coding benchmarks, categorized by whether their training compute exceeds 1T tokens. Notably, <span class="ltx_text" id="S3.SS2.2.14.1">Arctic-SnowCoder</span> demonstrates exceptional performance, particularly given its limited training data.
<span class="ltx_text" id="S3.SS2.2.14.2">Arctic-SnowCoder</span>-1.3B achieves state-of-the-art performance on BigCodeBench
compared to similarly sized models trained on no more than 1T token, significantly outperforming StarCoderBase-3B, SmolLM-1.7B, and Phi-1.5-1.3B.
Particularly, although Phi-1.5-1.3B has an advantage in “textbook-like” benchmarks such as HumanEval+, MBPP+, and EvoEval,
<span class="ltx_text" id="S3.SS2.2.14.3">Arctic-SnowCoder</span>-1.3B outperforms Phi-1.5-1.3B by 36% on the more complex and practical BigCodeBench.
Also, <span class="ltx_text" id="S3.SS2.2.14.4">Arctic-SnowCoder</span>-1.3B beats StarCoderBase-3B, the predecessor of StarCoder2-3B trained on 1T tokens, across all evaluated benchmarks.
Despite being trained on only 555B tokens, on HumanEval+, <span class="ltx_text" id="S3.SS2.2.14.5">Arctic-SnowCoder</span>-1.3B rivals and even surpasses models that have undergone significantly more extensive training, such as StarCoder2-3B, StableCode-3B, CodeGemma-2B-v1.0, and Qwen1.5-1.8B. On EvoEval and BigCodeBench, <span class="ltx_text" id="S3.SS2.2.14.6">Arctic-SnowCoder</span> remains competitive.
Additionally, the table highlights the consistent improvement of <span class="ltx_text" id="S3.SS2.2.14.7">Arctic-SnowCoder</span> across its training phases: <span class="ltx_text" id="S3.SS2.2.14.8">Arctic-SnowCoder-alpha</span>, <span class="ltx_text" id="S3.SS2.2.14.9">Arctic-SnowCoder-beta</span>, and the final <span class="ltx_text" id="S3.SS2.2.14.10">Arctic-SnowCoder</span>. Each phase builds on the previous one, with <span class="ltx_text" id="S3.SS2.2.14.11">Arctic-SnowCoder</span> achieving the highest scores in all benchmarks. This steady enhancement emphasizes the crucial role of high-quality and synthetic data in the final phase. Despite starting with the same data, each iteration of <span class="ltx_text" id="S3.SS2.2.14.12">Arctic-SnowCoder</span> narrows the gap with state-of-the-art models, demonstrating the efficacy of the overall training approach.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsection ltx_figure_panel" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Repo-level data in general pretraining</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In the general pretraining phase, we adopt StarCoder2’s approach to group file-level data randomly into repositories through a random concatenation of file contents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite>.
In <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS3" title="3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>, we study two methods: (1) grouping files just by repository names, meaning that each training document can be a mix of multi-lingual code files if the repository is written in different languages, and (2) partitioning files into different programming languages before grouping them into repositories, meaning that each training document only focuses on one single language.</p>
</div>
<figure class="ltx_table" id="S3.SS3.tab1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of two methods for grouping repo-level data for pretraining. (1) “Group by repo” treats each repository as a single training unit with possibly mixed languages, and (2) “Group by language and repo” partitions data by programming language before grouping by repository.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.SS3.tab1.1">{booktabs}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS3.tab1.2">colspec=@lrrr@,

Setting  HumanEval (+)  MBPP (+)  EvoEval 
<br class="ltx_break"/>Group by repo  12.8 (10.4)  30.7 (25.9)  7.0 
<br class="ltx_break"/>Group by language and repo  <span class="ltx_text ltx_font_bold" id="S3.SS3.tab1.2.1">17.1 (15.9)</span> <span class="ltx_text ltx_font_bold" id="S3.SS3.tab1.2.2">33.9 (27.8)</span> <span class="ltx_text ltx_font_bold" id="S3.SS3.tab1.2.3">7.4</span>
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS3.tab1.3">We can observe that the second approach, which we finally adopt in general pretraining, performs significantly better than the first one.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsection ltx_figure_panel" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Design choices in continued pretraining</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">In continued pretraining, we source high-quality tokens from our pretraining corpus and train an improved base model. To obtain high-quality tokens, a model-based quality annotator is employed. In this section, we experiment with various design choices, including the training data for the annotator, the learning rate used in continued pretraining, and the optimal repetitions of high-quality tokens.</p>
</div>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model-based quality annotator</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">Similar to FineWeb-Edu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib30" title="">30</a>]</cite>, we train a linear head on top of the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS0.Px1.p1.1.1">Snowflake-arctic-embed-m</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib26" title="">26</a>]</cite> embedding model to score each code file.
In <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px1" title="Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, we experiment with 4 variants:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I3.i1.p1.1.1">Ann-Edu</span>: We prompt Mixtral-8x7B-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib15" title="">15</a>]</cite> to annotate the educational value of each code file (1 to 5). 400k annotations are used to train a linear regression head.
For the following variants, similar to DCLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib18" title="">18</a>]</cite>, we sample negative documents randomly and change the positive parts only. A linear classification head is used instead.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I3.i2.p1.1.1">Ann-Ins</span>: Positives are a mix of 100k educational data (3.5+) bootstrapped from <span class="ltx_text ltx_font_smallcaps" id="S3.I3.i2.p1.1.2">Ann-Edu</span> and 100k high-quality instruction data from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> and StarCoder2-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I3.i3.p1.1.1">Ann-HQ</span>: Positives are 220k open-source, synthetic, high-quality code files <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib39" title="">39</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.I3.i4.p1.1.1">Ann-HQIns</span>: Positives are a mix of 220k <span class="ltx_text ltx_font_smallcaps" id="S3.I3.i4.p1.1.2">Ann-HQ</span> training data and 80k instruction data from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> and StarCoder2-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S3.SS4.SSS0.Px1.tab1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of downstream performance by applying model-based quality annotators trained with different recipes to 10B continued pretraining.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" id="S3.SS4.SSS0.Px1.tab1.1" style="width:433.6pt;height:119pt;vertical-align:-111.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.8pt,0.0pt) scale(0.991956029295419,0.991956029295419) ;"><span class="ltx_ERROR undefined" id="S3.SS4.SSS0.Px1.tab1.1.1">{booktabs}</span>
<p class="ltx_p" id="S3.SS4.SSS0.Px1.tab1.1.2">colspec=@lQ[l,20em]rrr@,
cell2,31=c=2l

Annotator  Training data  HumanEval (+)  MBPP (+)  EvoEval 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="S3.SS4.SSS0.Px1.tab1.1.2.1">\seprule</span>Pretrained model (no continued pretraining)   17.1 (15.9)  33.9 (27.8)  7.4
<br class="ltx_break"/>Continued pretraining on random 10B tokens   15.9 (12.8)  30.7 (23.3)  8.0
<br class="ltx_break"/><span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.1.2.2">Ann-Edu</span>  400k Mixtral annotations for educational scores (0–5)  19.5 (16.5)  27.8 (22.2)  10.4
<br class="ltx_break"/><span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.1.2.3">Ann-Ins</span>  100k high <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.1.2.4">Ann-Edu</span> + 100k instruction data from Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> and StarCoder2-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib40" title="">40</a>]</cite>  21.3 (18.3)  37.3 (29.9)  10.4 
<br class="ltx_break"/><span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.1.2.5">Ann-HQ</span>  220k open-source, synthetic high-quality code files <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib39" title="">39</a>]</cite>  19.5 (16.5)  33.9 (26.7)  9.2 
<br class="ltx_break"/><span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.1.2.6">Ann-HQIns</span>  220k <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.1.2.7">Ann-HQ</span> data mixed with 80k instruction data  <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px1.tab1.1.2.8">22.0 (18.3)</span> <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px1.tab1.1.2.9">40.2 (33.1)</span> <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px1.tab1.1.2.10">11.6</span>
<br class="ltx_break"/></p>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS4.SSS0.Px1.tab1.2">After training the annotators, we first apply each annotator to the entire pretraining corpus to obtain a score for each file. Unlike FineWeb-Edu, which only scans the top 2k characters, we scan the top, middle, and bottom parts of a code file and average the scores. We then rank the code files per language based on these scores and select the top percentile of documents until we reach approximately 10 billion tokens. We maintain the same mix ratio as used in pretraining. The table shows that <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px1.tab1.2.1">Ann-HQIns</span>, combining high-quality files and instruction data, achieves the best downstream performance.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS4.SSS0.Px1.tab1.3">We conduct an additional analysis in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.F2" title="In Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>. For each annotator, we create a validation dataset with positives from code solution benchmarks and negatives from random pretraining data not seen during training. We use the ROC-AUC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib6" title="">6</a>]</cite> (Area Under the Receiver Operating Characteristic Curve) score to evaluate how well the annotator ranks benchmark data. The figure illustrates the correlation between per-benchmark ROC-AUC scores and benchmark pass rates. There is an almost consistent trend: higher ROC-AUC scores lead to better benchmark performance. A good ROC-AUC score indicates that the annotator effectively shapes the distribution of downstream tasks. Thus, the key to high-quality data is essentially the alignment with downstream application distributions.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="356" id="S3.F2.g1" src="x2.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Correlation between annotator ROC-AUC score and benchmark pass@1.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_paragraph ltx_figure_panel" id="S3.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Learning rate schedule</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px2.p1.1">We also study different learning rate schedules for continued pretraining in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px2" title="Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, including (1) a linear annealing starting from the minimum pretraining learning rate to zero, (2) a constant schedule using the minimum pretraining learning rate, and (3) a re-warmup to the maximum pretraining learning rate followed by a linear decay to zero.</p>
</div>
<figure class="ltx_table" id="S3.SS4.SSS0.Px2.8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of different learning rate schedules in 10B continued pretraining using <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px2.8.10.1">Ann-HQIns</span>. Here <math alttext="\texttt{MIN\_LR}=5.3\times 10^{-5}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.3.3.m1.1"><semantics id="S3.SS4.SSS0.Px2.3.3.m1.1b"><mrow id="S3.SS4.SSS0.Px2.3.3.m1.1.1" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.3.3.m1.1.1.2" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.2a.cmml">MIN_LR</mtext><mo id="S3.SS4.SSS0.Px2.3.3.m1.1.1.1" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.1.cmml">=</mo><mrow id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.cmml"><mn id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.2" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.2.cmml">5.3</mn><mo id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.1.cmml">×</mo><msup id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.cmml"><mn id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.2" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.2.cmml">10</mn><mrow id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.cmml"><mo id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3b" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.cmml">−</mo><mn id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.2" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.2.cmml">5</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.3.3.m1.1c"><apply id="S3.SS4.SSS0.Px2.3.3.m1.1.1.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1"><eq id="S3.SS4.SSS0.Px2.3.3.m1.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.1"></eq><ci id="S3.SS4.SSS0.Px2.3.3.m1.1.1.2a.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.3.3.m1.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.2">MIN_LR</mtext></ci><apply id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3"><times id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.1"></times><cn id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.2.cmml" type="float" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.2">5.3</cn><apply id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.1.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3">superscript</csymbol><cn id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.2">10</cn><apply id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3"><minus id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.1.cmml" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3"></minus><cn id="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.2.cmml" type="integer" xref="S3.SS4.SSS0.Px2.3.3.m1.1.1.3.3.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.3.3.m1.1d">\texttt{MIN\_LR}=5.3\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.3.3.m1.1e">MIN_LR = 5.3 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\texttt{MAX\_LR}=5.3\times 10^{-4}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.4.4.m2.1"><semantics id="S3.SS4.SSS0.Px2.4.4.m2.1b"><mrow id="S3.SS4.SSS0.Px2.4.4.m2.1.1" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.4.4.m2.1.1.2" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.2a.cmml">MAX_LR</mtext><mo id="S3.SS4.SSS0.Px2.4.4.m2.1.1.1" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.1.cmml">=</mo><mrow id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.cmml"><mn id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.2" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.2.cmml">5.3</mn><mo id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.1.cmml">×</mo><msup id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.cmml"><mn id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.2" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.2.cmml">10</mn><mrow id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.cmml"><mo id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3b" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.cmml">−</mo><mn id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.2" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.2.cmml">4</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.4.4.m2.1c"><apply id="S3.SS4.SSS0.Px2.4.4.m2.1.1.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1"><eq id="S3.SS4.SSS0.Px2.4.4.m2.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.1"></eq><ci id="S3.SS4.SSS0.Px2.4.4.m2.1.1.2a.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.4.4.m2.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.2">MAX_LR</mtext></ci><apply id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3"><times id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.1.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.1"></times><cn id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.2.cmml" type="float" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.2">5.3</cn><apply id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.1.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3">superscript</csymbol><cn id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.2.cmml" type="integer" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.2">10</cn><apply id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3"><minus id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.1.cmml" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3"></minus><cn id="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.2.cmml" type="integer" xref="S3.SS4.SSS0.Px2.4.4.m2.1.1.3.3.3.2">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.4.4.m2.1d">\texttt{MAX\_LR}=5.3\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.4.4.m2.1e">MAX_LR = 5.3 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.SS4.SSS0.Px2.8.11">{booktabs}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS4.SSS0.Px2.8.8">colspec=@llrrr@,

Setting  Schedule  HumanEval (+)  MBPP (+)  EvoEval 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="S3.SS4.SSS0.Px2.8.8.1">\seprule</span>Pretraining  <math alttext="0\to\texttt{MAX\_LR}\to\texttt{MIN\_LR}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.5.5.m1.1"><semantics id="S3.SS4.SSS0.Px2.5.5.m1.1a"><mrow id="S3.SS4.SSS0.Px2.5.5.m1.1.1" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.cmml"><mn id="S3.SS4.SSS0.Px2.5.5.m1.1.1.2" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.2.cmml">0</mn><mo id="S3.SS4.SSS0.Px2.5.5.m1.1.1.3" stretchy="false" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.3.cmml">→</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.5.5.m1.1.1.4" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.4a.cmml">MAX_LR</mtext><mo id="S3.SS4.SSS0.Px2.5.5.m1.1.1.5" stretchy="false" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.5.cmml">→</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.5.5.m1.1.1.6" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.6a.cmml">MIN_LR</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.5.5.m1.1b"><apply id="S3.SS4.SSS0.Px2.5.5.m1.1.1.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1"><and id="S3.SS4.SSS0.Px2.5.5.m1.1.1a.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1"></and><apply id="S3.SS4.SSS0.Px2.5.5.m1.1.1b.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1"><ci id="S3.SS4.SSS0.Px2.5.5.m1.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.3">→</ci><cn id="S3.SS4.SSS0.Px2.5.5.m1.1.1.2.cmml" type="integer" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.2">0</cn><ci id="S3.SS4.SSS0.Px2.5.5.m1.1.1.4a.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.4"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.5.5.m1.1.1.4.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.4">MAX_LR</mtext></ci></apply><apply id="S3.SS4.SSS0.Px2.5.5.m1.1.1c.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1"><ci id="S3.SS4.SSS0.Px2.5.5.m1.1.1.5.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px2.5.5.m1.1.1.4.cmml" id="S3.SS4.SSS0.Px2.5.5.m1.1.1d.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1"></share><ci id="S3.SS4.SSS0.Px2.5.5.m1.1.1.6a.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.6"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.5.5.m1.1.1.6.cmml" xref="S3.SS4.SSS0.Px2.5.5.m1.1.1.6">MIN_LR</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.5.5.m1.1c">0\to\texttt{MAX\_LR}\to\texttt{MIN\_LR}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.5.5.m1.1d">0 → MAX_LR → MIN_LR</annotation></semantics></math>  17.1 (15.9)  33.9 (27.8)  7.4
<br class="ltx_break"/>Linear  <math alttext="\texttt{MIN\_LR}\to 0" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.6.6.m2.1"><semantics id="S3.SS4.SSS0.Px2.6.6.m2.1a"><mrow id="S3.SS4.SSS0.Px2.6.6.m2.1.1" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.6.6.m2.1.1.2" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.2a.cmml">MIN_LR</mtext><mo id="S3.SS4.SSS0.Px2.6.6.m2.1.1.1" stretchy="false" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.1.cmml">→</mo><mn id="S3.SS4.SSS0.Px2.6.6.m2.1.1.3" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.6.6.m2.1b"><apply id="S3.SS4.SSS0.Px2.6.6.m2.1.1.cmml" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1"><ci id="S3.SS4.SSS0.Px2.6.6.m2.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.1">→</ci><ci id="S3.SS4.SSS0.Px2.6.6.m2.1.1.2a.cmml" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.6.6.m2.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.2">MIN_LR</mtext></ci><cn id="S3.SS4.SSS0.Px2.6.6.m2.1.1.3.cmml" type="integer" xref="S3.SS4.SSS0.Px2.6.6.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.6.6.m2.1c">\texttt{MIN\_LR}\to 0</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.6.6.m2.1d">MIN_LR → 0</annotation></semantics></math>  18.3 (16.5)  37.0 (30.4)  9.8 
<br class="ltx_break"/>Constant  <math alttext="\texttt{MIN\_LR}\to\texttt{MIN\_LR}" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.7.7.m3.1"><semantics id="S3.SS4.SSS0.Px2.7.7.m3.1a"><mrow id="S3.SS4.SSS0.Px2.7.7.m3.1.1" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.7.7.m3.1.1.2" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.2a.cmml">MIN_LR</mtext><mo id="S3.SS4.SSS0.Px2.7.7.m3.1.1.1" stretchy="false" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.1.cmml">→</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.7.7.m3.1.1.3" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.3a.cmml">MIN_LR</mtext></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.7.7.m3.1b"><apply id="S3.SS4.SSS0.Px2.7.7.m3.1.1.cmml" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1"><ci id="S3.SS4.SSS0.Px2.7.7.m3.1.1.1.cmml" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.1">→</ci><ci id="S3.SS4.SSS0.Px2.7.7.m3.1.1.2a.cmml" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.7.7.m3.1.1.2.cmml" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.2">MIN_LR</mtext></ci><ci id="S3.SS4.SSS0.Px2.7.7.m3.1.1.3a.cmml" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.7.7.m3.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.7.7.m3.1.1.3">MIN_LR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.7.7.m3.1c">\texttt{MIN\_LR}\to\texttt{MIN\_LR}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.7.7.m3.1d">MIN_LR → MIN_LR</annotation></semantics></math>  20.7 (18.3)  39.4 (31.7)  9.4 
<br class="ltx_break"/>Re-warmup  <math alttext="0\to\texttt{MAX\_LR}\to 0" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px2.8.8.m4.1"><semantics id="S3.SS4.SSS0.Px2.8.8.m4.1a"><mrow id="S3.SS4.SSS0.Px2.8.8.m4.1.1" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.cmml"><mn id="S3.SS4.SSS0.Px2.8.8.m4.1.1.2" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.2.cmml">0</mn><mo id="S3.SS4.SSS0.Px2.8.8.m4.1.1.3" stretchy="false" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.3.cmml">→</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.8.8.m4.1.1.4" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.4a.cmml">MAX_LR</mtext><mo id="S3.SS4.SSS0.Px2.8.8.m4.1.1.5" stretchy="false" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.5.cmml">→</mo><mn id="S3.SS4.SSS0.Px2.8.8.m4.1.1.6" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.6.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px2.8.8.m4.1b"><apply id="S3.SS4.SSS0.Px2.8.8.m4.1.1.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1"><and id="S3.SS4.SSS0.Px2.8.8.m4.1.1a.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1"></and><apply id="S3.SS4.SSS0.Px2.8.8.m4.1.1b.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1"><ci id="S3.SS4.SSS0.Px2.8.8.m4.1.1.3.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.3">→</ci><cn id="S3.SS4.SSS0.Px2.8.8.m4.1.1.2.cmml" type="integer" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.2">0</cn><ci id="S3.SS4.SSS0.Px2.8.8.m4.1.1.4a.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.4"><mtext class="ltx_mathvariant_monospace" id="S3.SS4.SSS0.Px2.8.8.m4.1.1.4.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.4">MAX_LR</mtext></ci></apply><apply id="S3.SS4.SSS0.Px2.8.8.m4.1.1c.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1"><ci id="S3.SS4.SSS0.Px2.8.8.m4.1.1.5.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.5">→</ci><share href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px2.8.8.m4.1.1.4.cmml" id="S3.SS4.SSS0.Px2.8.8.m4.1.1d.cmml" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1"></share><cn id="S3.SS4.SSS0.Px2.8.8.m4.1.1.6.cmml" type="integer" xref="S3.SS4.SSS0.Px2.8.8.m4.1.1.6">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px2.8.8.m4.1c">0\to\texttt{MAX\_LR}\to 0</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px2.8.8.m4.1d">0 → MAX_LR → 0</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px2.8.8.2">22.0 (18.3)</span> <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px2.8.8.3">40.2 (33.1)</span> <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px2.8.8.4">11.6</span>
<br class="ltx_break"/>

Empirically, we find that the re-warmup approach performs the best and use it consistently in all the other experiments with respect to continued pretraining.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_paragraph ltx_figure_panel" id="S3.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Repetitions of high-quality data</h4>
<div class="ltx_para" id="S3.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px3.p1.1">Finally, we scale up the token horizon from 10 billion to 50 billion in continued pretraining. One remaining question to address is determining the optimal repetitions for high-quality tokens. We experiment with repetitions ranging from 1 to 5, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#S3.SS4.SSS0.Px3" title="Repetitions of high-quality data ‣ Learning rate schedule ‣ Model-based quality annotator ‣ 3.4 Design choices in continued pretraining ‣ 3.3 Repo-level data in general pretraining ‣ 3.2 Baseline comparison and effectiveness of three-stage pretraining ‣ 3 Experiments ‣ 2.4 Enhanced pretraining with synthetic data ‣ 2.3 Continued pretraining with high-quality data ‣ 2.2 General pretraining ‣ 2.1 Raw data ‣ 2 Arctic-SnowCoder ‣ Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>, by selecting the top percentile tokens ranked by <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px3.p1.1.1">Ann-HQIns</span>.</p>
</div>
<figure class="ltx_table" id="S3.SS4.SSS0.Px3.13">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Downstream performance with varying repetitions of high-quality data in 50B continued pretraining using <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.SSS0.Px3.13.15.1">Ann-HQIns</span>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S3.SS4.SSS0.Px3.13.16">{booktabs}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S3.SS4.SSS0.Px3.13.13">colspec=@lrrr@,

Repetition pattern  HumanEval (+)  MBPP (+)  EvoEval 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="S3.SS4.SSS0.Px3.13.13.1">\seprule</span>Pretrained  17.1 (15.9)  33.9 (27.8)  7.4
<br class="ltx_break"/>1 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.1.1.m1.1"><semantics id="S3.SS4.SSS0.Px3.1.1.m1.1a"><mo id="S3.SS4.SSS0.Px3.1.1.m1.1.1" xref="S3.SS4.SSS0.Px3.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.1.1.m1.1b"><times id="S3.SS4.SSS0.Px3.1.1.m1.1.1.cmml" xref="S3.SS4.SSS0.Px3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.1.1.m1.1d">×</annotation></semantics></math> 10.0B  22.0 (18.3)  40.2 (33.1)  11.6 
<br class="ltx_break"/>1 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.2.2.m2.1"><semantics id="S3.SS4.SSS0.Px3.2.2.m2.1a"><mo id="S3.SS4.SSS0.Px3.2.2.m2.1.1" xref="S3.SS4.SSS0.Px3.2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.2.2.m2.1b"><times id="S3.SS4.SSS0.Px3.2.2.m2.1.1.cmml" xref="S3.SS4.SSS0.Px3.2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.2.2.m2.1d">×</annotation></semantics></math> 50.0B  17.4 (14.0)  41.5 (33.6)  9.6 
<br class="ltx_break"/>2 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.3.3.m3.1"><semantics id="S3.SS4.SSS0.Px3.3.3.m3.1a"><mo id="S3.SS4.SSS0.Px3.3.3.m3.1.1" xref="S3.SS4.SSS0.Px3.3.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.3.3.m3.1b"><times id="S3.SS4.SSS0.Px3.3.3.m3.1.1.cmml" xref="S3.SS4.SSS0.Px3.3.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.3.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.3.3.m3.1d">×</annotation></semantics></math> 25.0B  23.2 (19.5)  42.1 (34.7)  9.2 
<br class="ltx_break"/>3 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.4.4.m4.1"><semantics id="S3.SS4.SSS0.Px3.4.4.m4.1a"><mo id="S3.SS4.SSS0.Px3.4.4.m4.1.1" xref="S3.SS4.SSS0.Px3.4.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.4.4.m4.1b"><times id="S3.SS4.SSS0.Px3.4.4.m4.1.1.cmml" xref="S3.SS4.SSS0.Px3.4.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.4.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.4.4.m4.1d">×</annotation></semantics></math> 16.7B  23.8 (18.9)  42.3 (34.4)  11.2 
<br class="ltx_break"/>4 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.5.5.m5.1"><semantics id="S3.SS4.SSS0.Px3.5.5.m5.1a"><mo id="S3.SS4.SSS0.Px3.5.5.m5.1.1" xref="S3.SS4.SSS0.Px3.5.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.5.5.m5.1b"><times id="S3.SS4.SSS0.Px3.5.5.m5.1.1.cmml" xref="S3.SS4.SSS0.Px3.5.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.5.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.5.5.m5.1d">×</annotation></semantics></math> 12.5B  <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px3.13.13.2">26.2 (21.3)</span>  40.2 (32.5)  <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px3.13.13.3">12.8</span>
<br class="ltx_break"/>5 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.6.6.m6.1"><semantics id="S3.SS4.SSS0.Px3.6.6.m6.1a"><mo id="S3.SS4.SSS0.Px3.6.6.m6.1.1" xref="S3.SS4.SSS0.Px3.6.6.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.6.6.m6.1b"><times id="S3.SS4.SSS0.Px3.6.6.m6.1.1.cmml" xref="S3.SS4.SSS0.Px3.6.6.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.6.6.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.6.6.m6.1d">×</annotation></semantics></math> 10.0B  20.1 (17.7)  <span class="ltx_text ltx_font_bold" id="S3.SS4.SSS0.Px3.13.13.4">43.9 (36.0)</span>  10.4 
<br class="ltx_break"/>

In this context, the top percentile tokens are the highest quality tokens available. For example, 1 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.7.7.m7.1"><semantics id="S3.SS4.SSS0.Px3.7.7.m7.1a"><mo id="S3.SS4.SSS0.Px3.7.7.m7.1.1" xref="S3.SS4.SSS0.Px3.7.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.7.7.m7.1b"><times id="S3.SS4.SSS0.Px3.7.7.m7.1.1.cmml" xref="S3.SS4.SSS0.Px3.7.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.7.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.7.7.m7.1d">×</annotation></semantics></math> 50B indicates one repetition of the top 50B tokens, while 4 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.8.8.m8.1"><semantics id="S3.SS4.SSS0.Px3.8.8.m8.1a"><mo id="S3.SS4.SSS0.Px3.8.8.m8.1.1" xref="S3.SS4.SSS0.Px3.8.8.m8.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.8.8.m8.1b"><times id="S3.SS4.SSS0.Px3.8.8.m8.1.1.cmml" xref="S3.SS4.SSS0.Px3.8.8.m8.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.8.8.m8.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.8.8.m8.1d">×</annotation></semantics></math> 12.5B denotes four repetitions of the top 12.5B tokens, ensuring that the selected tokens are of the best quality.
Based on the results in the table, repeating the high-quality tokens four times (4 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.9.9.m9.1"><semantics id="S3.SS4.SSS0.Px3.9.9.m9.1a"><mo id="S3.SS4.SSS0.Px3.9.9.m9.1.1" xref="S3.SS4.SSS0.Px3.9.9.m9.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.9.9.m9.1b"><times id="S3.SS4.SSS0.Px3.9.9.m9.1.1.cmml" xref="S3.SS4.SSS0.Px3.9.9.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.9.9.m9.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.9.9.m9.1d">×</annotation></semantics></math> 12.5B) yields the best overall downstream performance across multiple evaluation metrics, showing the highest scores for HumanEval and EvoEval. Two repetitions (2 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.10.10.m10.1"><semantics id="S3.SS4.SSS0.Px3.10.10.m10.1a"><mo id="S3.SS4.SSS0.Px3.10.10.m10.1.1" xref="S3.SS4.SSS0.Px3.10.10.m10.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.10.10.m10.1b"><times id="S3.SS4.SSS0.Px3.10.10.m10.1.1.cmml" xref="S3.SS4.SSS0.Px3.10.10.m10.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.10.10.m10.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.10.10.m10.1d">×</annotation></semantics></math> 25.0B) and three repetitions (3 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.11.11.m11.1"><semantics id="S3.SS4.SSS0.Px3.11.11.m11.1a"><mo id="S3.SS4.SSS0.Px3.11.11.m11.1.1" xref="S3.SS4.SSS0.Px3.11.11.m11.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.11.11.m11.1b"><times id="S3.SS4.SSS0.Px3.11.11.m11.1.1.cmml" xref="S3.SS4.SSS0.Px3.11.11.m11.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.11.11.m11.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.11.11.m11.1d">×</annotation></semantics></math> 16.7B) also demonstrate strong performance, particularly in <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS0.Px3.13.13.5">mbpp</span>. Five repetitions (5 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.12.12.m12.1"><semantics id="S3.SS4.SSS0.Px3.12.12.m12.1a"><mo id="S3.SS4.SSS0.Px3.12.12.m12.1.1" xref="S3.SS4.SSS0.Px3.12.12.m12.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.12.12.m12.1b"><times id="S3.SS4.SSS0.Px3.12.12.m12.1.1.cmml" xref="S3.SS4.SSS0.Px3.12.12.m12.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.12.12.m12.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.12.12.m12.1d">×</annotation></semantics></math> 10.0B) achieve the highest MBPP score but do not surpass the four repetitions in overall metrics. A single repetition (1 <math alttext="\times" class="ltx_Math" display="inline" id="S3.SS4.SSS0.Px3.13.13.m13.1"><semantics id="S3.SS4.SSS0.Px3.13.13.m13.1a"><mo id="S3.SS4.SSS0.Px3.13.13.m13.1.1" xref="S3.SS4.SSS0.Px3.13.13.m13.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.SSS0.Px3.13.13.m13.1b"><times id="S3.SS4.SSS0.Px3.13.13.m13.1.1.cmml" xref="S3.SS4.SSS0.Px3.13.13.m13.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.SSS0.Px3.13.13.m13.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.SSS0.Px3.13.13.m13.1d">×</annotation></semantics></math> 50.0B) shows the least improvement compared to multiple repetitions.</p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_section ltx_figure_panel" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Code pretraining corpus for language models</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">Code data is essential to improving the reasoning capabilities of large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>. Typically, researchers obtain massive code pretraining data by crawling from public platforms hosting code repositories such as GitHub <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>.
For example The Stack v1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib17" title="">17</a>]</cite> is a 3.1 TB dataset consisting of permissively licensed source code mined from GitHub in 30 programming languages.
Its successor The Stack v2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite>, built on the Software Heritage archive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib8" title="">8</a>]</cite>, is an order of magnitude larger, with a raw dataset of 67.5 TB spanning 619 programming languages.
However, directly using these massive unfiltered code for pretraining is suboptimal, because the code documents may contain undesired contents or duplicates. Therefore, further preprocessing steps are needed to downscale the raw corpus, which can include deduplication <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib36" title="">36</a>]</cite>, PII (Personally Identifiable Information) redaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib27" title="">27</a>]</cite>, benchmark decontamination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>, and model-based filtering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>.
As an example, StarCoder2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib23" title="">23</a>]</cite> selects only 3 TB of data for pretraining from the 67.5 TB total data available in The Stack v2.
The code pretraining corpus of <span class="ltx_text" id="S4.SS0.SSS0.Px1.p1.1.1">Arctic-SnowCoder</span> follows a similar preprocessing pipeline, comprising approximately 400B unique tokens from a mix of filtered The Stack v1 and GitHub crawls.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Model-based quality filtering</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">In addition to common preprocessing steps like deduplication and heuristic filtering, a recent trend is using model-based quality filters to select high-quality pretraining data.
Phi-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib13" title="">13</a>]</cite> employs a random forest classifier trained on top of the CodeGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib28" title="">28</a>]</cite> embedding layer on GPT-4 annotations, to assess the educational value of files. This filter selects high-quality The Stack v1 and StackOverflow content, significantly enhancing coding performance.
FineWeb-Edu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib30" title="">30</a>]</cite> employs a linear regressor built on <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.1">Snowflake-arctic-embed-m</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib26" title="">26</a>]</cite>, an advanced embedding model based on BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib11" title="">11</a>]</cite>. This regressor, trained on 400k Llama-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib12" title="">12</a>]</cite> annotations rating the educational value (0-5) of FineWeb dataset documents, significantly enhances STEM performance.
DCLM-Baseline <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib18" title="">18</a>]</cite> uses a <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.2">fastText</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib5" title="">5</a>]</cite> filter trained on positives from OpenHermes 2.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib37" title="">37</a>]</cite>, high-scoring posts from <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.3">r/ExplainLikeImFive</span>, and random negatives. It outperforms FineWeb-Edu in top-10% selection.
DeepSeek-Coder-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite> follows DeepSeek-Math <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib34" title="">34</a>]</cite> by leveraging a multi-stage <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.4">fastText</span>-based pipeline to recall high-quality code and math contents.
Llama-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib12" title="">12</a>]</cite> uses <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px2.p1.1.5">fastText</span> for recognizing text referenced by Wikipedia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib42" title="">42</a>]</cite> and Roberta-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib22" title="">22</a>]</cite> classifiers trained on Llama-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib38" title="">38</a>]</cite> predictions.
While prior work focuses on initial pretraining, <span class="ltx_text" id="S4.SS0.SSS0.Px2.p1.1.6">Arctic-SnowCoder</span> demonstrates that high-quality data from the pretraining corpus can significantly enhance model performance during continued pretraining. We are also the first to uncover the secret of data quality, revealing the importance of matching data distribution with downstream tasks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">High-quality code data for pretraining</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Phi-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib13" title="">13</a>]</cite> is one of the first to study the impact of high-quality code data. It first uses a random forest classifier to filter out high-quality code data from The Stack v1 and StackOverflow, and then creates synthetic textbook-like data and exercises using GPT-3.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib29" title="">29</a>]</cite>, showing significant coding performance with only 50B+ training tokens.
DeepSeek-Coder-V2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib10" title="">10</a>]</cite>, pretrained for around 14T tokens in total, achieves state-of-the-art coding performance, with a multi-stage <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS0.Px3.p1.1.1">fastText</span>-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib5" title="">5</a>]</cite> pipeline to recall web-related code data as well as high-quality GitHub code.
<span class="ltx_text" id="S4.SS0.SSS0.Px3.p1.1.2">Arctic-SnowCoder</span> utilizes a high-quality code annotator to extract high-quality code from pretraining datasets and generates synthetic files seeded from this high-quality data, adapting Magicoder OSS-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.02326v1#bib.bib41" title="">41</a>]</cite> into pretraining.</p>
</div>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduce <span class="ltx_text" id="S5.p1.1.1">Arctic-SnowCoder</span>-1.3B, a high-performing code model that underscores the critical importance of data quality in the pretraining process. Trained on 555B tokens, <span class="ltx_text" id="S5.p1.1.2">Arctic-SnowCoder</span>-1.3B achieves competitive results with state-of-the-art small code models while using significantly fewer tokens. Our three-stage pretraining process begins with 500B tokens of general pretraining on a raw code corpus, followed by 50B high-quality tokens scored by a quality annotator, and concludes with 5B tokens of synthetic data for further enhancement. This work demystifies the notion of high-quality data in code pretraining by demonstrating the key to high-quality data is its alignment with the distribution of downstream applications. Additionally, the paper offers practical guidelines for repo-level data grouping, learning rate scheduling, and the repetition of high-quality data, paving the way for more efficient and effective code model development.</p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang
Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou.

</span>
<span class="ltx_bibblock">Phi-3 technical report: A highly capable language model locally on your phone, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch.

</span>
<span class="ltx_bibblock">Smollm - blazingly fast and remarkably powerful.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/blog/smollm" title="">https://huggingface.co/blog/smollm</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker.

</span>
<span class="ltx_bibblock">To code, or not to code? exploring impact of code in pre-training, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.

</span>
<span class="ltx_bibblock">Program synthesis with large language models, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.

</span>
<span class="ltx_bibblock">Enriching word vectors with subword information, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Andrew P. Bradley.

</span>
<span class="ltx_bibblock">The use of the area under the roc curve in the evaluation of machine learning algorithms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Pattern Recognition</span>, 30(7):1145–1159, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Roberto Di Cosmo and Stefano Zacchiroli.

</span>
<span class="ltx_bibblock">Software heritage: Why and how to preserve software source code.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">iPRES 2017: 14th International Conference on Digital Preservation</span>, Kyoto, Japan, 2017.

</span>
<span class="ltx_bibblock">https://hal.archives-ouvertes.fr/hal-01590958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen
Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie.

</span>
<span class="ltx_bibblock">Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
DeepSeek-AI, Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y. Wu, Yukun Li, Huazuo Gao, Shirong Ma, Wangding Zeng, Xiao Bi, Zihui Gu, Hanwei Xu, Damai Dai, Kai Dong, Liyue Zhang, Yishi Piao, Zhibin Gou, Zhenda Xie, Zhewen Hao, Bingxuan Wang, Junxiao Song, Deli Chen, Xin Xie, Kang Guan, Yuxiang You, Aixin Liu, Qiushi Du, Wenjun Gao, Xuan Lu, Qinyu Chen, Yaohui Wang, Chengqi Deng, Jiashi Li, Chenggang Zhao, Chong Ruan, Fuli Luo, and Wenfeng Liang.

</span>
<span class="ltx_bibblock">Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu,
Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl
Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzmán, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli,
Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha
Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny
Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao.

</span>
<span class="ltx_bibblock">The llama 3 herd of models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.

</span>
<span class="ltx_bibblock">Textbooks are all you need, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang.

</span>
<span class="ltx_bibblock">Deepseek-coder: When the large language model meets programming – the rise of code intelligence, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.

</span>
<span class="ltx_bibblock">Mixtral of experts, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries.

</span>
<span class="ltx_bibblock">The stack: 3 tb of permissively licensed source code, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.

</span>
<span class="ltx_bibblock">Datacomp-lm: In search of the next generation of training sets for language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von
Werra, and Harm de Vries.

</span>
<span class="ltx_bibblock">Starcoder: may the source be with you!, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.

</span>
<span class="ltx_bibblock">Textbooks are all you need ii: phi-1.5 technical report, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG.

</span>
<span class="ltx_bibblock">Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.

</span>
<span class="ltx_bibblock">In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing Systems</span>, volume 36, pages 21558–21572. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.

</span>
<span class="ltx_bibblock">Starcoder 2 and the stack v2: The next generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li.

</span>
<span class="ltx_bibblock">At which training stage does code data help LLMs reasoning?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">The Twelfth International Conference on Learning Representations</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig.

</span>
<span class="ltx_bibblock">Language models of code are few-shot commonsense learners.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</span>, pages 1384–1403, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Luke Merrick, Danmei Xu, Gaurav Nuti, and Daniel Campos.

</span>
<span class="ltx_bibblock">Arctic-embed: Scalable, efficient, and accurate text embedding models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan-Hong Dang, Pengyuan Li, Kun-Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos Fonseca, Amith Singhee, Nirmit Desai, David D. Cox, Ruchir Puri, and Rameswar Panda.

</span>
<span class="ltx_bibblock">Granite code models: A family of open foundation models for code intelligence, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.

</span>
<span class="ltx_bibblock">Codegen: An open large language model for code with multi-turn program synthesis.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt: Optimizing language models for dialogue.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt/" title="">https://openai.com/blog/chatgpt/</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf.

</span>
<span class="ltx_bibblock">The fineweb datasets: Decanting the web for the finest text data at scale, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente, Carlos Riquelme, and Nathan Cooper.

</span>
<span class="ltx_bibblock">Stable code technical report, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Snowflake AI Research.

</span>
<span class="ltx_bibblock">Snowflake arctic: The best llm for enterprise ai — efficiently intelligent, truly open, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea Hu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis, Mateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec, Kelly Schaefer, and Scott Huffman.

</span>
<span class="ltx_bibblock">Codegemma: Open code models based on gemma, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Teknium.

</span>
<span class="ltx_bibblock">Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/teknium/OpenHermes2.5" title="">https://huggingface.co/datasets/teknium/OpenHermes2.5</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Yuxiang Wei.

</span>
<span class="ltx_bibblock">hqcode.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/yuxiang630/hqcode" title="">https://huggingface.co/datasets/yuxiang630/hqcode</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Starcoder2-instruct: Fully transparent and permissive self-alignment for code generation.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/blog/sc2-instruct" title="">https://huggingface.co/blog/sc2-instruct</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Magicoder: Empowering code generation with OSS-instruct.

</span>
<span class="ltx_bibblock">In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 41st International Conference on Machine Learning</span>, volume 235 of <span class="ltx_text ltx_font_italic" id="bib.bib41.2.2">Proceedings of Machine Learning Research</span>, pages 52632–52657. PMLR, 21–27 Jul 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Wikipedia contributors.

</span>
<span class="ltx_bibblock">Plagiarism — Wikipedia, the free encyclopedia, 2004.

</span>
<span class="ltx_bibblock">[Online; accessed 22-July-2004].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Chunqiu Steven Xia, Yinlin Deng, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Top leaderboard ranking = top coding proficiency, always? evoeval: Evolving coding benchmarks via llm, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan.

</span>
<span class="ltx_bibblock">Qwen2 technical report, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Heng Ji, and ChengXiang Zhai.

</span>
<span class="ltx_bibblock">If LLM is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">ICLR 2024 Workshop on Large Language Model (LLM) Agents</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra.

</span>
<span class="ltx_bibblock">Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</div>
</div>
</figure>
</section>
</div>
</div>
</figure>
</section>
</section>
</div>
</div>
</figure>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep  3 22:34:02 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
