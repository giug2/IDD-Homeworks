<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation</title>
<!--Generated on Fri Aug 30 14:02:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.17308v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S1" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S1.SS0.SSS0.Px1" title="In 1 Introduction ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S2" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Literary MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Machine <span class="ltx_text ltx_font_italic">Translationese</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Reranking Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Why Recover Rather Than Increase Lexical Diversity?</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3.SS1" title="In 3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Theoretical Support</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3.SS2" title="In 3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Empirical Support</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S4" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Reranking Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental set-up</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS1" title="In 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Vanilla MT System</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS1.SSS0.Px1" title="In 5.1 Vanilla MT System ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS1.SSS0.Px2" title="In 5.1 Vanilla MT System ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS1.SSS0.Px3" title="In 5.1 Vanilla MT System ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Decoding Strategies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS2" title="In 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Original-Text Classification</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS2.SSS0.Px1" title="In 5.2 Original-Text Classification ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS2.SSS0.Px2" title="In 5.2 Original-Text Classification ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS3" title="In 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Baselines</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS3.SSS0.Px1" title="In 5.3 Baselines ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">APE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS3.SSS0.Px2" title="In 5.3 Baselines ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Tagging</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS1" title="In 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>General Text Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS1.SSS0.Px1" title="In 6.1 General Text Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">TTR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS1.SSS0.Px2" title="In 6.1 General Text Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Yule’s I</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS1.SSS0.Px3" title="In 6.1 General Text Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">MTLD</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS2" title="In 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Translation-specific Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS2.SSS0.Px1" title="In 6.2 Translation-specific Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">PTF</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS2.SSS0.Px2" title="In 6.2 Translation-specific Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">CDU</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS2.SSS0.Px3" title="In 6.2 Translation-specific Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">SynTTR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.SS2.SSS1" title="In 6.2 Translation-specific Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Translation Quality</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.SS1" title="In 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Quantitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.SS2" title="In 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Surface-level Inspection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.SS3" title="In 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Ranks and Lexical Diversity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.SS4" title="In 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Tailoring and Lexical Diversity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S8" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#A0.SS0.SSS0.Px1" title="In Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">Special cases</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Tailored Recovery of Lexical Diversity in Literary 
<br class="ltx_break"/>Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Esther Ploeger❄   
Huiyuan Lai<sup class="ltx_sup" id="id1.1.id1">✰</sup>   
Rik van Noord<sup class="ltx_sup" id="id2.2.id2">✰</sup>   
Antonio Toral<sup class="ltx_sup" id="id3.3.id3">✰</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id4.4.id4">❄</sup>Department of Computer Science, Aalborg University, Denmark 
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id5">✰</sup>CLCG, University of Groningen, The Netherlands 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id6.6.id6">espl@cs.aau.dk</span>  <span class="ltx_text ltx_font_typewriter" id="id7.7.id7">{h.lai,r.i.k.van.noord,a.toral.ruiz}@rug.nl</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.id1">Machine translations are found to be lexically poorer than human translations.
The loss of lexical diversity through MT poses an issue in the automatic translation of literature, where it matters not only <span class="ltx_text ltx_font_italic" id="id8.id1.1">what</span> is written, but also <span class="ltx_text ltx_font_italic" id="id8.id1.2">how</span> it is written.
Current methods for increasing lexical diversity in MT are rigid.
Yet, as we demonstrate, the degree of lexical diversity can vary considerably
across different novels.
Thus, rather than aiming for the rigid <span class="ltx_text ltx_font_italic" id="id8.id1.3">increase</span>
of lexical diversity, we reframe the task as <span class="ltx_text ltx_font_italic" id="id8.id1.4">recovering</span> what is lost in the machine translation process.
We propose a novel approach that consists of reranking translation candidates with a classifier that distinguishes between original and translated text.
We evaluate our approach on 31 English-to-Dutch book translations, and find that, for certain books, our approach retrieves lexical diversity scores that are close to human translation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the introduction of neural machine translation (NMT), the performance of high-resource automatic translation has improved substantially.
Especially since the introduction of the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx42" title="">Vaswani et al., 2017</a>]</cite>, state-of-the-art NMT systems have outperformed previous approaches considerably <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx19" title="">Lakew et al., 2018</a>]</cite>, with some works even claiming human parity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx26" title="">Popel et al., 2020</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S1.F1.g1" src="x1.png" width="418"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Reranking translation hypotheses based on the probability they are originally written in the target language, where the chosen rank is based on the lexical diversity score of the original book, and could be lower than the most lexically diverse option.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, these claims are based mostly on accuracy and fluency measures, while style is often overlooked. In fact, according to expert evaluation, machine translation (MT) did actually not reach human parity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx36" title="">Toral et al., 2018</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx10" title="">Fischer and Läubli, 2020</a>]</cite>.
For instance, MT models have been found to exacerbate linguistic patterns that occur frequently, while underrepresenting patterns that are found less commonly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx40" title="">Vanmassenhove et al., 2019</a>]</cite>. As a result, automatically translated texts are found to be lexically poorer than human translations (HT). This ‘artificially impoverished language’ has previously been referred to as <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">machine translationese</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx41" title="">Vanmassenhove et al., 2021</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we focus on the translation of novels. Contrary to technical domains, where meaning preservation is the main criterion for acceptable translations, literary translations have the additional criterion of style.
This is because apart from meaning preservation (<span class="ltx_text ltx_font_italic" id="S1.p3.1.1">what</span> is written), maintaining a certain reading experience (<span class="ltx_text ltx_font_italic" id="S1.p3.1.2">how</span> it is written) is vital for novels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx35" title="">Toral and Way, 2015</a>]</cite>.
Importantly however, writing style (and linguistic complexity) can vary considerably between books. Some books contain repetitive language use, while others are written in embellished language (see Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3" title="3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>).
Current approaches that aim to mitigate the loss of lexical diversity do not accommodate this. State-of-the-art previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx11" title="">Freitag et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx12" title="">Freitag et al., 2022</a>]</cite> increases lexical diversity in a rigid way, not allowing for flexibility at inference time.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Contributions</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.SS0.SSS0.Px1.p1.1.1">(i)</span> We show that lexical diversity varies
considerably across
books, and argue that this should be taken into account in MT;
<span class="ltx_text ltx_font_italic" id="S1.SS0.SSS0.Px1.p1.1.2">(ii)</span> We introduce a novel flexible method for recovering
lexical diversity in MT, informed by the diversity of the original.
<span class="ltx_text ltx_font_italic" id="S1.SS0.SSS0.Px1.p1.1.3">(ii)</span> We evaluate our method on 31 English novels which are translated to Dutch, and find that our approach is effective when it comes to book-tailored promotion of lexical diversity.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Literary MT</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">NMT has been argued to hold potential for literary texts, for instance in assisting professional translators or improving the immediate accessibility of untranslated foreign language books <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx22" title="">Matusov, 2019</a>]</cite>.
However, MT has been shown to decrease lexical diversity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx40" title="">Vanmassenhove et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx41" title="">Vanmassenhove et al., 2021</a>]</cite>.
This is an issue, because literary works can be viewed as a special domain in translation. Typically, literary translators are expected to preserve not only literal elements from the source, such as the plot, but also some sense of creative value <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx31" title="">Riera, 2022</a>]</cite>. In other words, a goal of literary translation could be to recreate the ‘aesthetic intentions or effects’ that are possibly present in the source book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx7" title="">Delabastita, 2011</a>]</cite>.
Such ‘aesthetic intentions’ can for instance be voice and metaphor, but also repetition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx45" title="">Wright, 2016</a>]</cite>.
Repetitive use of language is commonly a conscious choice by the writer, and has a function, such as drawing attention or establishing a pattern <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx4" title="">Boase-Beier, 2011</a>]</cite>.
Given that lexical diversity can be an intentional writing choice, it should be apparent that an approach that aims at recovering lexical diversity in MT should not be boundless.
Therefore, it is our aim to inform recovery with the degree of relative lexical diversity of the source text.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Machine <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.1.1">Translationese</span>
</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Following recommendations from Jiménez-Crespo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx15" title="">Jimenez-Crespo, 2023</a>]</cite>, we will largely refrain from using the term <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">translationese</span> in the rest of this paper.
However, it is important to note that previous work that aims to increase lexical diversity in MT has mostly been framed as part of ‘machine translationese’ reduction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx11" title="">Freitag et al., 2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx12" title="">Freitag et al., 2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx9" title="">Dutta Chowdhury et al., 2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx14" title="">Jalota et al., 2023</a>]</cite>. Translations have been found to differ from original texts in a number of ways.
For one, <span class="ltx_text ltx_font_bold" id="S2.SS0.SSS0.Px2.p1.1.2">?</span>) argues that human translations into a language tend to be lexically simpler than text originally written in that language.
Automatic classification approaches have been effective in detecting this difference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx3" title="">Baroni and Bernardini, 2005</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx17" title="">Koppel and Ordan, 2011</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx44" title="">Volansky et al., 2015</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx29" title="">Rabinovich and Wintner, 2015</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx28" title="">Pylypenko et al., 2021</a>]</cite>.
More recently, work has investigated linguistic differences between MT and HT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx39" title="">van der Werff et al., 2022</a>]</cite>.
Thus, it seems that modelling characteristics of original versus translated texts has a direct link to lexical diversity.
Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx12" title="">Freitag et al., 2022</a>]</cite> leveraged these detectable differences in their approach to increase the naturalness of output translations.
We take inspiration from their lexical diversity evaluation methods, and implement their method as a baseline.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Reranking Methods</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Reranking hypotheses in text generation originated before the age of neural paradigms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx32" title="">Shen et al., 2004</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx5" title="">Collins and Koo, 2005</a>]</cite>.
In essence, reranking entails re-ordering the set of candidate outputs according to some criterion, with the aim of providing a final output that adheres better to that criterion.
Such methods have been applied for various tasks, such as summarization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx21" title="">Liu and Liu, 2021</a>]</cite> and semantic parsing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx46" title="">Yin and Neubig, 2019</a>]</cite>. In machine translation, previous approaches include discriminative reranking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx20" title="">Lee et al., 2021</a>]</cite> and reranking with energy-based models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx1" title="">Arcadinho et al., 2022</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Why Recover Rather Than Increase Lexical Diversity?</h2>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="79" id="S3.F2.g1" src="x2.png" width="348"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="S3.F2.g2" src="x3.png" width="349"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="77" id="S3.F2.g3" src="x4.png" width="349"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="79" id="S3.F2.g4" src="x5.png" width="349"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="78" id="S3.F2.g5" src="x6.png" width="349"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="79" id="S3.F2.g6" src="x7.png" width="348"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Range and spread of lexical diversity metrics for HT (left, yellow) and original English (right, blue).</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this paper, we argue for tailored recovery of lexical diversity.
In this section, we first discuss support for this idea from the field of literary studies. Then, we provide empirical evidence by applying lexical diversity metrics to our test set.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Theoretical Support</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Previous work on writing style in novels acknowledges that some books exhibit more lexical diversity than others.
As an example, <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">?</span>)
finds that no word in the original (i.e. English) version of in <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">The Old man and the Sea</span> by Ernest Hemingway contains more than six syllables. Additionally, Hemingway tends to stick to particular words, even when there are more diverse options: in 184 situations of direct speech, he chooses to use the word ‘said’ 170 times instead of for example ‘asked’, ‘remarked’, ‘noticed’ or ‘yelled’.
An example from the other end of the spectrum is James Joyce’s <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.3">Ulysses</span>.
This work is known for its experimental techniques and unorthodox language use. <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.4">?</span>) illustrates this by highlighting Joyce’s use of neologisms, such as ‘He <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.5">smellsipped</span> the cordial juice’ and ‘Davy Byrne <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.6">smiledyawnednodded</span> all in one’. Moreover, Joyce repeatedly uses non-verbs as verbs, like in ‘I am <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.7">almosting</span> it.’ and even writes long sequences in unconventional spelling (<span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.8">Ahbeesee defeegee kelomen opeecue rustyouvee doubleyou</span>. Boys are they?’).
These examples make it clear that books can be written with vastly different ‘aesthetic intentions’. Thus, for preserving these intentions, MT approaches should not render them equally diverse in terms of lexicon.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Empirical Support</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We empirically verify whether these findings hold for our data specifically, by estimating the lexical diversity of the 31 books in our test set, which we introduce in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS1" title="5.1 Vanilla MT System ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">5.1</span></a>.
We calculate three measures of lexical variety (type-token ratio; TTR, Yule’s I <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx47" title="">Yule, 1944</a>]</cite>, and MTLD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx23" title="">McCarthy, 2005</a>]</cite>) for each book in our test set.
We further elaborate on these metrics in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6" title="6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>.
Next, we apply the same metrics to the human translations of those same books.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3.F2" title="Figure 2 ‣ 3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows that there is indeed a wide range of diversity across books, for both HT and original text.
For example, in both settings, we find that the highest MTLD value is almost two times as large as the lowest.
This emphasises why it is not our aim to generate the highest possible lexical diversity for every book.
While we observe similar ranges and distributions in HT vs. original, the HT metrics are slightly higher.
However, this does not necessarily mean that HT contains more embellished language.
We note that the languages in our study, Dutch and English, are relatively similar (both in terms of genealogy and linguistic typology), but they differ in ways that can influence diversity metrics. For instance, Dutch contains compound nouns while English does not, making a higher TTR for Dutch more likely.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">This discrepancy means that we cannot compare our Dutch MT to the original English book diversity directly. Instead, here we compare MT with HT.
To verify whether this is sensible, we assess the relationship between HT and the English originals, by computing Pearson’s correlation on the corresponding diversity metrics. The results are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3.T1" title="Table 1 ‣ 3.2 Empirical Support ‣ 3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>,
and the corresponding
regression plots are found in Appendix B. We observe strong correlations that are all statistically significant. This is important, because as the source diversity is a reliable indicator of HT diversity, it makes sense to use the source scores to approach HT (see Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S4" title="4 Reranking Method ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.3.4.1.1.1">Metric</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.3.4.1.2.1">Correlation coefficient</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.4.1.3"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.3.4.1.3.1">p<span class="ltx_text ltx_font_upright" id="S3.T1.3.4.1.3.1.1">-value</span></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T1.1.1.2">TTR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.3">0.971</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.1">
<math alttext="&lt;" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><lt id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">&lt;</annotation></semantics></math> 0.00001</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.2.2">Yule’s I</th>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.3">0.929</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.1">
<math alttext="&lt;" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1.1"><semantics id="S3.T1.2.2.1.m1.1a"><mo id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><lt id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.m1.1d">&lt;</annotation></semantics></math> 0.00001</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.3.3.2">MTLD</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.3.3">0.953</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.3.1">
<math alttext="&lt;" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1.1"><semantics id="S3.T1.3.3.1.m1.1a"><mo id="S3.T1.3.3.1.m1.1.1" xref="S3.T1.3.3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.m1.1b"><lt id="S3.T1.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.m1.1d">&lt;</annotation></semantics></math> 0.00001</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Pearson correlation coefficients for HT and OR lexdiv metrics, rounded to three decimals.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Reranking Method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, our approach consists of two parts: hypothesis generation and hypothesis reranking.
Firstly, we generate the <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">n</span> best translation candidates for each source sentence in the test set with a vanilla domain-specific MT system (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS1" title="5.1 Vanilla MT System ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">5.1</span></a>). Note that we decode all books separately, instead of concatenating all test set books.
Then, for each book, we apply a classifier (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS2" title="5.2 Original-Text Classification ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">5.2</span></a>) to the translation hypotheses and, through a softmax layer, obtain the probability for each candidate that it is an original Dutch sequence.
Based on these probabilities, we rerank the translation candidates.
In order to obtain the (expected) most lexically rich candidate, we would then choose the rank with the highest original-text probability.
However, note that this simple approach is flexible in the sense that, instead of choosing the most original-like option, we have the option to choose a lower original-text rank.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We leverage this flexibility for tailoring rank selection to the lexical diversity of the original English book.
First, for each original book, we calculate a <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">LexDiv</span> score, which consists of the average of the normalized TTR, Yule’s I and MTLD scores (see Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6" title="6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>).
Then, we bin the books according to their <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">LexDiv</span> score, relative to the total distribution.
That is, given a list that is sorted based on <span class="ltx_text ltx_font_italic" id="S4.p2.1.3">LexDiv</span>, we categorize these into groups, where the number of groups depends on the number of <span class="ltx_text ltx_font_italic" id="S4.p2.1.4">nbest</span> candidates in decoding.
For example, for <math alttext="n=5" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">n</mi><mo id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><eq id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></eq><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑛</ci><cn id="S4.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.p2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n=5</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_n = 5</annotation></semantics></math>, we bin the books into 5 different groups of 6 books (adding any remainders into the last bin). The bin per book corresponds to the original-text rank that is selected.
As such, the selected rank for each book depends on the lexical diversity of its source, relative to the other books.
Reranking translation candidates is a suitable solution to our task, because it accomodates flexibility, which is tunable at inference time. There is no need to train a separate model per diversity setting, saving computational expenses.
Additionally, our approach is model-agnostic: reranking can be applied to any MT model that can generate multiple translation candidates.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental set-up</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Vanilla MT System</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">We use the dataset by <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px1.p1.1.1">?</span>), which contains 531 books that were originally written in English and manually translated into Dutch.
We use 495 books for training, 5 for development and 31 as a test set.
The genres of the books vary: they include literary fiction, popular fiction, non-fiction and children’s books from over 100 authors.
We do not make a distinction between literary and ‘unliterary’ novels, as we believe this to be a subjective judgment.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>A full list of author names, titles, genres and publishing years of the test set books can be found in Appendix A, Table <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib" title="References ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_title">References</span></a>.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Training</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">Firstly, we align the sentences of the English and Dutch versions of each book using Vecalign <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx34" title="">Thompson and Koehn, 2019</a>]</cite>.
For the books in the test set, we manually discard sentences for which there existed no proper alignment, such as front matter sentences.
Additionally, we discard sentences with a cosine distance higher than 0.7 (2.3% of all sentences).
Then, we normalise all punctuation using the MOSES toolkit.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>http://www.statmt.org/moses/</span></span></span>
We then apply SentencePiece <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx18" title="">Kudo and Richardson, 2018</a>]</cite> subword segmentation to the data. For this, we train a SentencePiece unigram model with a joint vocabulary for both languages and a vocabulary size of 32,000.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">We train a Transformer-based translation model using the Fairseq toolkit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx24" title="">Ott et al., 2019</a>]</cite>.
More specifically, we use the <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p2.1.1">transformer_iwslt_de_en</span> architecture. This is a Transformer base model with 6 encoder and decoder layers and an embedding dimension of 512. During training, we use an Adam optimiser, a learning rate of 5e-4, the loss function cross entropy with label smoothing 0.1 and the batch size is 64. Each model is trained until convergence with a patience of 3 epochs, using the BLEU score as a maximisation metric for finding the best checkpoint.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Decoding Strategies</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">By default, we use beam search for decoding.
Reranking approaches rely heavily on the diversity of the translation hypotheses: if the hypotheses are all very similar, reranking them is not likely to have a large effect.
To ensure diverse hypotheses, we use a beam size of 20. Additionally, we experiment with decoding through diverse beam search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx43" title="">Vijayakumar et al., 2016</a>]</cite>. We follow <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px3.p1.1.1">?</span>) by using 3 groups, with a beam size of 21.
Beyond beam search, we investigate the effects of top-k and top-p sampling, with the default parameters and sampling size 10.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Original-Text Classification</h3>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Data</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">We use a monolingual dataset of more than 7,000 Dutch books from varying original languages, authors and genres <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx37" title="">Toral et al., 2024</a>]</cite>. For each book, we annotate whether it was originally written in Dutch.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The full annotation workflow can be found in Appendix C</span></span></span> We discard 2,182 books for which the original language is unclear or that were not prose.
We make sure to avoid overlap with the parallel data set by removing any books that are also part of the parallel data.
Finally, we randomly sample 1,794 of the remaining 2,190 books as to match the total number of translated books, ensuring an equal distribution.
In total, we are left with over 3,500 books and over 29M sentences. We further divide these into data for system development and data for original-text classification. We use this data for reproducing previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx12" title="">Freitag et al., 2022</a>]</cite> and for training our classifier.
Additionally, we translate the classifier section of the monolingual data set using a reverse-direction trained version of the vanilla MT system (NL <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mo id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" stretchy="false" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.1.m1.1d">→</annotation></semantics></math> EN), and then perform round-trip-translation (RTT) back to Dutch with the vanilla MT system, to obtain an MT version of the monolingual classifier data.
The full data size statistics and division in training, development and testing splits are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.T2" title="Table 2 ‣ Data ‣ 5.2 Original-Text Classification ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:433.6pt;height:452pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.3pt,-91.0pt) scale(1.67402582377279,1.67402582377279) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S5.T2.1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1">System development (90%)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.2.2">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.2.1.1">Split</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.2.2.1">Orig.</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.2.3.1"># Books</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.2.4.1"># Sentences</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.2.2.5.1"># Words</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">Train (80%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">Dutch</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">1,291</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">8,576,756</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">10,425,656</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.4">
<td class="ltx_td" id="S5.T2.1.1.4.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">Other</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">1,291</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">12,470,149</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">165,263,466</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">Dev (10%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">Dutch</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">162</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">1,005,832</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">12,533,406</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.6.6">
<td class="ltx_td" id="S5.T2.1.1.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">Other</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">162</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">1,546,057</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">19,723,706</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">Test (10%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.7.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">Dutch</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">162</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.7.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">1,189,690</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.7.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">14,721,914</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.8.8">
<td class="ltx_td" id="S5.T2.1.1.8.8.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.8.8.2" style="padding-left:3.0pt;padding-right:3.0pt;">Other</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.8.3" style="padding-left:3.0pt;padding-right:3.0pt;">162</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.8.4" style="padding-left:3.0pt;padding-right:3.0pt;">1,573,499</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.8.8.5" style="padding-left:3.0pt;padding-right:3.0pt;">20,968,346</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="5" id="S5.T2.1.1.9.9.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.9.9.1.1">Original-text Classification (10%)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.10.10">
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.10.10.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.10.10.1.1">Split</span></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.10.10.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.10.10.2.1">Orig.</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.10.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.10.10.3.1"># Books</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.10.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.10.10.4.1"># Sentences</span></td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.10.10.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.10.10.5.1"># Words</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.11.11.1" style="padding-left:3.0pt;padding-right:3.0pt;">Train (80%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.11.11.2" style="padding-left:3.0pt;padding-right:3.0pt;">Dutch</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.11.11.3" style="padding-left:3.0pt;padding-right:3.0pt;">143</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.11.11.4" style="padding-left:3.0pt;padding-right:3.0pt;">982,114</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.11.11.5" style="padding-left:3.0pt;padding-right:3.0pt;">11,528,789</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.12.12">
<td class="ltx_td" id="S5.T2.1.1.12.12.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.12.12.2" style="padding-left:3.0pt;padding-right:3.0pt;">Other</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.12.3" style="padding-left:3.0pt;padding-right:3.0pt;">143</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.12.4" style="padding-left:3.0pt;padding-right:3.0pt;">139,0351</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.12.12.5" style="padding-left:3.0pt;padding-right:3.0pt;">17,951,613</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.13.13.1" style="padding-left:3.0pt;padding-right:3.0pt;">Test (20%)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.1.1.13.13.2" style="padding-left:3.0pt;padding-right:3.0pt;">Dutch</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.13.13.3" style="padding-left:3.0pt;padding-right:3.0pt;">36</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.13.13.4" style="padding-left:3.0pt;padding-right:3.0pt;">261,151</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S5.T2.1.1.13.13.5" style="padding-left:3.0pt;padding-right:3.0pt;">2,974,873</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.14.14">
<td class="ltx_td" id="S5.T2.1.1.14.14.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_left" id="S5.T2.1.1.14.14.2" style="padding-left:3.0pt;padding-right:3.0pt;">Other</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.14.3" style="padding-left:3.0pt;padding-right:3.0pt;">36</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.14.4" style="padding-left:3.0pt;padding-right:3.0pt;">340,950</td>
<td class="ltx_td ltx_align_right" id="S5.T2.1.1.14.14.5" style="padding-left:3.0pt;padding-right:3.0pt;">4,283,604</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.15.15">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_tt" id="S5.T2.1.1.15.15.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.15.15.1.1">Total</span></td>
<td class="ltx_td ltx_border_bb ltx_border_tt" id="S5.T2.1.1.15.15.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt" id="S5.T2.1.1.15.15.3" style="padding-left:3.0pt;padding-right:3.0pt;">3,588</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt" id="S5.T2.1.1.15.15.4" style="padding-left:3.0pt;padding-right:3.0pt;">29,336,549</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_tt" id="S5.T2.1.1.15.15.5" style="padding-left:3.0pt;padding-right:3.0pt;">376,130,733</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Monolingual data set division and size.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Training</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">Currently, state-of-the-art performance for original-text detection is based on BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx8" title="">Devlin et al., 2019</a>]</cite>, as demonstrated by <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p1.1.1">?</span>).
We implement a similar system that distinguishes between original text and MT by training a binary classification model. We fine-tune Dutch language model BERTje <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx6" title="">de Vries et al., 2019</a>]</cite>. We train each model on the training split of the original-text classification data (see Table <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.T2" title="Table 2 ‣ Data ‣ 5.2 Original-Text Classification ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>).
We train models with batch size 128, accumulating gradients over 8 update steps, using the Adam optimiser <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx16" title="">Kingma and Ba, 2015</a>]</cite> with a learning rate of 3e-5. We use early stopping (patience 3) if validation performance does not improve.
On the held-out test set, the classifier achieves an accuracy of 85.9%. It obtains a precision of 90.6%, a recall of 80.2% and the F1 score is 85.0%.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Baselines</h3>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">APE</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.SSS0.Px1.p1.1.1">?</span>) introduced Automatic Post-Editing (APE) as a post-hoc method to increase the ‘naturalness’ of MT output.
Following their approach, we train a post-processor that ‘translates’ synthetic Dutch sequences into more natural Dutch sequences.
For training this system, we use the same data that was used to train the classifier (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S5.SS2" title="5.2 Original-Text Classification ‣ 5 Experimental set-up ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">5.2</span></a>), consisting of RTT Dutch (which we use as source) and original Dutch (which we use as target). We train a model with the same architecture as the vanilla MT system.
We apply the post-processor to the output of the vanilla MT system, in an attempt to obtain a translation with a lexical diversity that is closer to HT.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Tagging</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.3">Our second baseline is based on <span class="ltx_text ltx_font_bold" id="S5.SS3.SSS0.Px2.p1.3.1">?</span>). We train an MT system that learns to differentiate between original and translated text during training.
This method requires both translated and original Dutch target samples.
The translated target samples are found in our parallel dataset. We use the same original Dutch samples that are used in training the translationese classifier.
Following <span class="ltx_text ltx_font_bold" id="S5.SS3.SSS0.Px2.p1.3.2">?</span>), we then prepend <math alttext="{&lt;}orig{&gt;}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S5.SS3.SSS0.Px2.p1.1.m1.1a"><mrow id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml"><mo fence="true" id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.2" rspace="0em" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.cmml"><mi id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.2" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.2.cmml">o</mi><mo id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.3" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.3.cmml">r</mi><mo id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1a" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.4" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.4.cmml">i</mi><mo id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1b" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.5" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.5.cmml">g</mi></mrow><mo fence="true" id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.3" lspace="0em" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1"><csymbol cd="latexml" id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.2.1.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.2">expectation</csymbol><apply id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1"><times id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.1"></times><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.2">𝑜</ci><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.3.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.3">𝑟</ci><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.4.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.4">𝑖</ci><ci id="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.5.cmml" xref="S5.SS3.SSS0.Px2.p1.1.m1.1.1.1.1.5">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px2.p1.1.m1.1c">{&lt;}orig{&gt;}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px2.p1.1.m1.1d">&lt; italic_o italic_r italic_i italic_g &gt;</annotation></semantics></math> to the English source sentences that have original Dutch on the target side, and <math alttext="{&lt;}trans{&gt;}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.2.m2.1"><semantics id="S5.SS3.SSS0.Px2.p1.2.m2.1a"><mrow id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml"><mo fence="true" id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.2" rspace="0em" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.cmml"><mi id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.2" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml">t</mi><mo id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.3" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml">r</mi><mo id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1a" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.4" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.4.cmml">a</mi><mo id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1b" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.5" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.5.cmml">n</mi><mo id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1c" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.6" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.6.cmml">s</mi></mrow><mo fence="true" id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.3" lspace="0em" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1"><csymbol cd="latexml" id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.2.1.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.2">expectation</csymbol><apply id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1"><times id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.1"></times><ci id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.2">𝑡</ci><ci id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.3">𝑟</ci><ci id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.4.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.4">𝑎</ci><ci id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.5.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.5">𝑛</ci><ci id="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.6.cmml" xref="S5.SS3.SSS0.Px2.p1.2.m2.1.1.1.1.6">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px2.p1.2.m2.1c">{&lt;}trans{&gt;}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px2.p1.2.m2.1d">&lt; italic_t italic_r italic_a italic_n italic_s &gt;</annotation></semantics></math> for the source sentences that have translated Dutch. We train an MT system (same parameters as vanilla MT) on this data set. For inference, we prepend the source with <math alttext="{&lt;}orig{&gt;}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S5.SS3.SSS0.Px2.p1.3.m3.1a"><mrow id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml"><mo fence="true" id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.2" rspace="0em" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2.1.cmml">&lt;</mo><mrow id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.cmml"><mi id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.2" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.2.cmml">o</mi><mo id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.3" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.3.cmml">r</mi><mo id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1a" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.4" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.4.cmml">i</mi><mo id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1b" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.5" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.5.cmml">g</mi></mrow><mo fence="true" id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.3" lspace="0em" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.SSS0.Px2.p1.3.m3.1b"><apply id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1"><csymbol cd="latexml" id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.2">expectation</csymbol><apply id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1"><times id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.1"></times><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.2.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.2">𝑜</ci><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.3.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.3">𝑟</ci><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.4.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.4">𝑖</ci><ci id="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.5.cmml" xref="S5.SS3.SSS0.Px2.p1.3.m3.1.1.1.1.5">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.SSS0.Px2.p1.3.m3.1c">{&lt;}orig{&gt;}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.SSS0.Px2.p1.3.m3.1d">&lt; italic_o italic_r italic_i italic_g &gt;</annotation></semantics></math>, which prompts the model to produce a translation that exhibits characteristics that are often found in original Dutch. Note that, in contrast to APE, this method cannot be applied post-hoc.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Note that our implementation differs from <span class="ltx_text ltx_font_bold" id="footnote4.1">?</span>) in that they automatically differentiate natural and unnatural samples from a large parallel corpus using contrasting language models.</span></span></span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduce three classes of metrics. Firstly, we look at general text metrics, which are commonly used for evaluating lexical diversity. Secondly, we use translation-specific metrics.
Lastly, we evaluate the general translation quality.</p>
</div>
<section class="ltx_subsection ltx_pruned_first" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>General Text Metrics</h3>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">TTR</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1">The type-token ratio is the ratio of types (set of words) to tokens (actual words). A higher TTR indicates that more (different) words are used, which in turn indicates a higher lexical diversity. While this method is known to be influenced by the length of the text it is applied to, we report it because it is easy to interpret and widely used.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.8" style="width:435.9pt;height:173.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.6pt,12.9pt) scale(0.87,0.87) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.8.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T3.8.8.8.9"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.8.9.1">Approach</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1">TTR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.1.1.1.1.1.m1.1"><semantics id="S6.T3.1.1.1.1.1.m1.1a"><mo id="S6.T3.1.1.1.1.1.m1.1.1" stretchy="false" xref="S6.T3.1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T3.2.2.2.2.1">Yule’s I <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.2.2.2.2.1.m1.1"><semantics id="S6.T3.2.2.2.2.1.m1.1a"><mo id="S6.T3.2.2.2.2.1.m1.1.1" stretchy="false" xref="S6.T3.2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.2.2.1.m1.1b"><ci id="S6.T3.2.2.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.2.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S6.T3.3.3.3.3.1">MTLD <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.3.3.3.3.1.m1.1"><semantics id="S6.T3.3.3.3.3.1.m1.1a"><mo id="S6.T3.3.3.3.3.1.m1.1.1" stretchy="false" xref="S6.T3.3.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.3.3.3.3.1.m1.1b"><ci id="S6.T3.3.3.3.3.1.m1.1.1.cmml" xref="S6.T3.3.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.3.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.3.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T3.4.4.4.4.1">PTF <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T3.4.4.4.4.1.m1.1"><semantics id="S6.T3.4.4.4.4.1.m1.1a"><mo id="S6.T3.4.4.4.4.1.m1.1.1" stretchy="false" xref="S6.T3.4.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.4.4.4.4.1.m1.1b"><ci id="S6.T3.4.4.4.4.1.m1.1.1.cmml" xref="S6.T3.4.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.4.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.4.4.4.4.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S6.T3.5.5.5.5.1">CDU <math alttext="\downarrow" class="ltx_Math" display="inline" id="S6.T3.5.5.5.5.1.m1.1"><semantics id="S6.T3.5.5.5.5.1.m1.1a"><mo id="S6.T3.5.5.5.5.1.m1.1.1" stretchy="false" xref="S6.T3.5.5.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S6.T3.5.5.5.5.1.m1.1b"><ci id="S6.T3.5.5.5.5.1.m1.1.1.cmml" xref="S6.T3.5.5.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.5.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.5.5.5.5.1.m1.1d">↓</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.6.6.6.6"><span class="ltx_text ltx_font_bold" id="S6.T3.6.6.6.6.1">SynTTR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.6.6.6.6.1.m1.1"><semantics id="S6.T3.6.6.6.6.1.m1.1a"><mo id="S6.T3.6.6.6.6.1.m1.1.1" stretchy="false" xref="S6.T3.6.6.6.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.6.6.6.6.1.m1.1b"><ci id="S6.T3.6.6.6.6.1.m1.1.1.cmml" xref="S6.T3.6.6.6.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.6.6.6.6.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.6.6.6.6.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.7.7.7.7"><span class="ltx_text ltx_font_bold" id="S6.T3.7.7.7.7.1">BLEU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.7.7.7.7.1.m1.1"><semantics id="S6.T3.7.7.7.7.1.m1.1a"><mo id="S6.T3.7.7.7.7.1.m1.1.1" stretchy="false" xref="S6.T3.7.7.7.7.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.7.7.7.7.1.m1.1b"><ci id="S6.T3.7.7.7.7.1.m1.1.1.cmml" xref="S6.T3.7.7.7.7.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.7.7.7.7.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.7.7.7.7.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.8.8.8.8"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.8.8.1">COMET <math alttext="\uparrow" class="ltx_Math" display="inline" id="S6.T3.8.8.8.8.1.m1.1"><semantics id="S6.T3.8.8.8.8.1.m1.1a"><mo id="S6.T3.8.8.8.8.1.m1.1.1" stretchy="false" xref="S6.T3.8.8.8.8.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.8.8.8.8.1.m1.1b"><ci id="S6.T3.8.8.8.8.1.m1.1.1.cmml" xref="S6.T3.8.8.8.8.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.8.8.8.8.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S6.T3.8.8.8.8.1.m1.1d">↑</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.8.8.9.1.1">HT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.9.1.2">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.9.1.3">1.226</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.8.8.9.1.4">96.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.9.1.5">0.817</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.9.1.6">0.549</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.8.8.9.1.7">0.042</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.9.1.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.9.1.9">-</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.8.8.10.2.1">Vanilla MT</th>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.10.2.2">0.089</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.10.2.3">0.951</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.10.2.4">90.21</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.10.2.5">0.832</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.10.2.6"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.10.2.6.1">0.550</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.10.2.7">0.040</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.10.2.8">32.32</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.10.2.9">0.824</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.11.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.8.8.11.3.1">APE</th>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.11.3.2">0.092</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.11.3.3">0.985</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.11.3.4">90.59</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.11.3.5">0.827</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.11.3.6">0.554</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.11.3.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.11.3.7.1">0.041</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.11.3.8">30.39</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.11.3.9">0.808</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.8.8.12.4.1">Tagging</th>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.12.4.2"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.12.4.2.1">0.095</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.12.4.3">1.111</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.12.4.4"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.12.4.4.1">94.08</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.12.4.5">0.829</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.12.4.6"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.12.4.6.1">0.550</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.12.4.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.12.4.7.1">0.041</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.12.4.8">31.33</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.12.4.9">0.807</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.13.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.8.8.13.5.1">Tailored RR <span class="ltx_text ltx_font_italic" id="S6.T3.8.8.13.5.1.1">(n=5)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.13.5.2">0.091</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.13.5.3">1.002</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.8.8.13.5.4">92.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.13.5.5">0.829</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.13.5.6">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.8.8.13.5.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.13.5.7.1">0.041</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.13.5.8">30.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.13.5.9">0.815</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.8.8.14.6.1">Tailored RR <span class="ltx_text ltx_font_italic" id="S6.T3.8.8.14.6.1.1">(n=10)</span>
</th>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.14.6.2">0.091</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.14.6.3">1.013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.14.6.4">93.26</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.14.6.5">0.829</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.14.6.6">0.547</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.14.6.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.14.6.7.1">0.041</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.14.6.8">30.07</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.14.6.9">0.810</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.8.8.15.7.1">Tailored RR <span class="ltx_text ltx_font_italic" id="S6.T3.8.8.15.7.1.1">(n=20)</span>
</th>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.15.7.2">0.092</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.15.7.3">1.010</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.15.7.4">93.27</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.15.7.5">0.830</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.15.7.6">0.558</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.15.7.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.15.7.7.1">0.041</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.15.7.8">28.98</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.15.7.9">0.802</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.16.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T3.8.8.16.8.1">Tailored RR <span class="ltx_text ltx_font_italic" id="S6.T3.8.8.16.8.1.1">(Top-k)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.16.8.2"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.16.8.2.1">0.101</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.16.8.3"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.16.8.3.1">1.286</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.8.8.16.8.4">104.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.16.8.5"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.16.8.5.1">0.815</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.16.8.6">0.559</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T3.8.8.16.8.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.16.8.7.1">0.043</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.16.8.8">21.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T3.8.8.16.8.9">0.745</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.17.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T3.8.8.17.9.1">Tailored RR <span class="ltx_text ltx_font_italic" id="S6.T3.8.8.17.9.1.1">(Top-p)</span>
</th>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.17.9.2">0.092</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.17.9.3">1.017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.17.9.4">91.21</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.17.9.5">0.828</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.17.9.6">0.552</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.8.8.17.9.7"><span class="ltx_text ltx_font_bold" id="S6.T3.8.8.17.9.7.1">0.041</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.17.9.8">29.97</td>
<td class="ltx_td ltx_align_center" id="S6.T3.8.8.17.9.9">0.808</td>
</tr>
<tr class="ltx_tr" id="S6.T3.8.8.18.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T3.8.8.18.10.1">Tailored RR <span class="ltx_text ltx_font_italic" id="S6.T3.8.8.18.10.1.1">(DBS)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.8.8.18.10.2">0.092</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.8.8.18.10.3">1.010</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T3.8.8.18.10.4">92.70</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.8.8.18.10.5">0.828</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.8.8.18.10.6">0.553</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T3.8.8.18.10.7">0.040</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.8.8.18.10.8">29.36</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T3.8.8.18.10.9">0.805</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Scores averaged across books, where RR stands for reranking. We provide results for multiple decoding strategies. Beam size is 20. Scores closest to HT are in bold font.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Yule’s I</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px2.p1.1">As a metric that is less sensitive to variation in text length, we use Yule’s I <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx47" title="">Yule, 1944</a>]</cite>. We calculate this value as stated in Equation <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.E1" title="In Yule’s I ‣ 6.1 General Text Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, where V is the size of the vocabulary (number of types) and <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px2.p1.1.1">t(i,N)</span> denotes the frequency of types which occur i times in a sample of length N.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px2.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Yule's}\hskip 3.30002pt\text{I}=\frac{V^{2}}{\sum_{i=1}^{V}\times t(i,N)%
-V}" class="ltx_Math" display="block" id="S6.E1.m1.2"><semantics id="S6.E1.m1.2a"><mrow id="S6.E1.m1.2.3" xref="S6.E1.m1.2.3.cmml"><mrow id="S6.E1.m1.2.3.2" xref="S6.E1.m1.2.3.2.cmml"><mtext id="S6.E1.m1.2.3.2.2" xref="S6.E1.m1.2.3.2.2a.cmml">Yule’s</mtext><mo id="S6.E1.m1.2.3.2.1" lspace="0.330em" xref="S6.E1.m1.2.3.2.1.cmml">⁢</mo><mtext id="S6.E1.m1.2.3.2.3" xref="S6.E1.m1.2.3.2.3a.cmml">I</mtext></mrow><mo id="S6.E1.m1.2.3.1" xref="S6.E1.m1.2.3.1.cmml">=</mo><mfrac id="S6.E1.m1.2.2" xref="S6.E1.m1.2.2.cmml"><msup id="S6.E1.m1.2.2.4" xref="S6.E1.m1.2.2.4.cmml"><mi id="S6.E1.m1.2.2.4.2" xref="S6.E1.m1.2.2.4.2.cmml">V</mi><mn id="S6.E1.m1.2.2.4.3" xref="S6.E1.m1.2.2.4.3.cmml">2</mn></msup><mrow id="S6.E1.m1.2.2.2" xref="S6.E1.m1.2.2.2.cmml"><mrow id="S6.E1.m1.2.2.2.4" xref="S6.E1.m1.2.2.2.4.cmml"><mrow id="S6.E1.m1.2.2.2.4.2" xref="S6.E1.m1.2.2.2.4.2.cmml"><msubsup id="S6.E1.m1.2.2.2.4.2.2" xref="S6.E1.m1.2.2.2.4.2.2.cmml"><mo id="S6.E1.m1.2.2.2.4.2.2.2.2" xref="S6.E1.m1.2.2.2.4.2.2.2.2.cmml">∑</mo><mrow id="S6.E1.m1.2.2.2.4.2.2.2.3" xref="S6.E1.m1.2.2.2.4.2.2.2.3.cmml"><mi id="S6.E1.m1.2.2.2.4.2.2.2.3.2" xref="S6.E1.m1.2.2.2.4.2.2.2.3.2.cmml">i</mi><mo id="S6.E1.m1.2.2.2.4.2.2.2.3.1" xref="S6.E1.m1.2.2.2.4.2.2.2.3.1.cmml">=</mo><mn id="S6.E1.m1.2.2.2.4.2.2.2.3.3" xref="S6.E1.m1.2.2.2.4.2.2.2.3.3.cmml">1</mn></mrow><mi id="S6.E1.m1.2.2.2.4.2.2.3" xref="S6.E1.m1.2.2.2.4.2.2.3.cmml">V</mi></msubsup><mo id="S6.E1.m1.2.2.2.4.2.1" lspace="0em" rspace="0.222em" xref="S6.E1.m1.2.2.2.4.2.1.cmml">×</mo><mi id="S6.E1.m1.2.2.2.4.2.3" xref="S6.E1.m1.2.2.2.4.2.3.cmml">t</mi></mrow><mo id="S6.E1.m1.2.2.2.4.1" xref="S6.E1.m1.2.2.2.4.1.cmml">⁢</mo><mrow id="S6.E1.m1.2.2.2.4.3.2" xref="S6.E1.m1.2.2.2.4.3.1.cmml"><mo id="S6.E1.m1.2.2.2.4.3.2.1" stretchy="false" xref="S6.E1.m1.2.2.2.4.3.1.cmml">(</mo><mi id="S6.E1.m1.1.1.1.1" xref="S6.E1.m1.1.1.1.1.cmml">i</mi><mo id="S6.E1.m1.2.2.2.4.3.2.2" xref="S6.E1.m1.2.2.2.4.3.1.cmml">,</mo><mi id="S6.E1.m1.2.2.2.2" xref="S6.E1.m1.2.2.2.2.cmml">N</mi><mo id="S6.E1.m1.2.2.2.4.3.2.3" stretchy="false" xref="S6.E1.m1.2.2.2.4.3.1.cmml">)</mo></mrow></mrow><mo id="S6.E1.m1.2.2.2.3" xref="S6.E1.m1.2.2.2.3.cmml">−</mo><mi id="S6.E1.m1.2.2.2.5" xref="S6.E1.m1.2.2.2.5.cmml">V</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S6.E1.m1.2b"><apply id="S6.E1.m1.2.3.cmml" xref="S6.E1.m1.2.3"><eq id="S6.E1.m1.2.3.1.cmml" xref="S6.E1.m1.2.3.1"></eq><apply id="S6.E1.m1.2.3.2.cmml" xref="S6.E1.m1.2.3.2"><times id="S6.E1.m1.2.3.2.1.cmml" xref="S6.E1.m1.2.3.2.1"></times><ci id="S6.E1.m1.2.3.2.2a.cmml" xref="S6.E1.m1.2.3.2.2"><mtext id="S6.E1.m1.2.3.2.2.cmml" xref="S6.E1.m1.2.3.2.2">Yule’s</mtext></ci><ci id="S6.E1.m1.2.3.2.3a.cmml" xref="S6.E1.m1.2.3.2.3"><mtext id="S6.E1.m1.2.3.2.3.cmml" xref="S6.E1.m1.2.3.2.3">I</mtext></ci></apply><apply id="S6.E1.m1.2.2.cmml" xref="S6.E1.m1.2.2"><divide id="S6.E1.m1.2.2.3.cmml" xref="S6.E1.m1.2.2"></divide><apply id="S6.E1.m1.2.2.4.cmml" xref="S6.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S6.E1.m1.2.2.4.1.cmml" xref="S6.E1.m1.2.2.4">superscript</csymbol><ci id="S6.E1.m1.2.2.4.2.cmml" xref="S6.E1.m1.2.2.4.2">𝑉</ci><cn id="S6.E1.m1.2.2.4.3.cmml" type="integer" xref="S6.E1.m1.2.2.4.3">2</cn></apply><apply id="S6.E1.m1.2.2.2.cmml" xref="S6.E1.m1.2.2.2"><minus id="S6.E1.m1.2.2.2.3.cmml" xref="S6.E1.m1.2.2.2.3"></minus><apply id="S6.E1.m1.2.2.2.4.cmml" xref="S6.E1.m1.2.2.2.4"><times id="S6.E1.m1.2.2.2.4.1.cmml" xref="S6.E1.m1.2.2.2.4.1"></times><apply id="S6.E1.m1.2.2.2.4.2.cmml" xref="S6.E1.m1.2.2.2.4.2"><times id="S6.E1.m1.2.2.2.4.2.1.cmml" xref="S6.E1.m1.2.2.2.4.2.1"></times><apply id="S6.E1.m1.2.2.2.4.2.2.cmml" xref="S6.E1.m1.2.2.2.4.2.2"><csymbol cd="ambiguous" id="S6.E1.m1.2.2.2.4.2.2.1.cmml" xref="S6.E1.m1.2.2.2.4.2.2">superscript</csymbol><apply id="S6.E1.m1.2.2.2.4.2.2.2.cmml" xref="S6.E1.m1.2.2.2.4.2.2"><csymbol cd="ambiguous" id="S6.E1.m1.2.2.2.4.2.2.2.1.cmml" xref="S6.E1.m1.2.2.2.4.2.2">subscript</csymbol><sum id="S6.E1.m1.2.2.2.4.2.2.2.2.cmml" xref="S6.E1.m1.2.2.2.4.2.2.2.2"></sum><apply id="S6.E1.m1.2.2.2.4.2.2.2.3.cmml" xref="S6.E1.m1.2.2.2.4.2.2.2.3"><eq id="S6.E1.m1.2.2.2.4.2.2.2.3.1.cmml" xref="S6.E1.m1.2.2.2.4.2.2.2.3.1"></eq><ci id="S6.E1.m1.2.2.2.4.2.2.2.3.2.cmml" xref="S6.E1.m1.2.2.2.4.2.2.2.3.2">𝑖</ci><cn id="S6.E1.m1.2.2.2.4.2.2.2.3.3.cmml" type="integer" xref="S6.E1.m1.2.2.2.4.2.2.2.3.3">1</cn></apply></apply><ci id="S6.E1.m1.2.2.2.4.2.2.3.cmml" xref="S6.E1.m1.2.2.2.4.2.2.3">𝑉</ci></apply><ci id="S6.E1.m1.2.2.2.4.2.3.cmml" xref="S6.E1.m1.2.2.2.4.2.3">𝑡</ci></apply><interval closure="open" id="S6.E1.m1.2.2.2.4.3.1.cmml" xref="S6.E1.m1.2.2.2.4.3.2"><ci id="S6.E1.m1.1.1.1.1.cmml" xref="S6.E1.m1.1.1.1.1">𝑖</ci><ci id="S6.E1.m1.2.2.2.2.cmml" xref="S6.E1.m1.2.2.2.2">𝑁</ci></interval></apply><ci id="S6.E1.m1.2.2.2.5.cmml" xref="S6.E1.m1.2.2.2.5">𝑉</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E1.m1.2c">\text{Yule's}\hskip 3.30002pt\text{I}=\frac{V^{2}}{\sum_{i=1}^{V}\times t(i,N)%
-V}</annotation><annotation encoding="application/x-llamapun" id="S6.E1.m1.2d">Yule’s I = divide start_ARG italic_V start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT × italic_t ( italic_i , italic_N ) - italic_V end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">MTLD</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px3.p1.1">As an additional metric that has proven to be robust to document length variety, we use the measure of textual lexical diversity (MTLD), which is sequentially calculated as the ‘average length of sequential word strings in a text that maintain a given TTR value’ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx23" title="">McCarthy, 2005</a>]</cite>. We use the same TTR threshold (0.72) as <span class="ltx_text ltx_font_bold" id="S6.SS1.SSS0.Px3.p1.1.1">?</span>).</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S6.SS1.SSS0.Px3.p2.1">We calculate these values using the <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS0.Px3.p2.1.1">LexicalRichness</span> Python library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx33" title="">Shen, 2022</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Translation-specific Metrics</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">?</span>) introduce a novel automatic evaluation method for measuring lexical diversity in translations: Synonym Frequency Analysis (SFA).
It provides an insight into the diversity of lexical choices in translations.
For English words that have multiple translations in Dutch, it takes into account the frequency of these translation options.
We re-implement this method, as it was not implemented for our language pair before.
We first lemmatise each word in the source (English) side of our test set, using SpaCy (<span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.2">nl_core_news_lg</span>).<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://spacy.io/models/nl#nl_core_news_lg</span></span></span> Next, we extract all possible translation options for the English adjectives, nouns and verbs by using a English-to-Dutch bilingual dictionary.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We use the dictionary from <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://freedict.org/downloads</span>. As an example, for the English adjective <em class="ltx_emph ltx_font_italic" id="footnote6.1">touching</em>, we find as Dutch translations: <span class="ltx_text ltx_font_italic" id="footnote6.2">ontroerend</span>, <span class="ltx_text ltx_font_italic" id="footnote6.3">aangrijpend</span>, <span class="ltx_text ltx_font_italic" id="footnote6.4">emotioneel</span>, <span class="ltx_text ltx_font_italic" id="footnote6.5">treffend</span>, <span class="ltx_text ltx_font_italic" id="footnote6.6">roerend</span> and <span class="ltx_text ltx_font_italic" id="footnote6.7">aandoenlijk</span>.</span></span></span>
Next, for each translation option, we count the number of occurrences in the MT output for each system.
The result is a vector which contains the occurrence frequency of each translation synonym for an English word.</p>
</div>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="273" id="S6.F3.g1" src="x8.png" width="872"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Per-book comparison of MTLD between the (rigid) tagging baseline and (tailored) reranking method, where green dotted lines are HT scores, and red dotted lines represent vanilla MT.</figcaption>
</figure>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">PTF</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px1.p1.1">The primary translation frequency (PTF) is the average percentage (over all relevant source words) of times the most frequent translation option was chosen, from all translation options. The assumption is that if the output contains more secondary candidates, the text is more lexically diverse.
We report the average PTF of all source words.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">CDU</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px2.p1.1">The CDU is the cosine distance between the output vector for each source word and a vector of the same length with an equal distribution for each translation option (with the same total). We take the average CDU over all relevant source words to compute a final CDU.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">SynTTR</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px3.p1.1">Lastly, we compute the SynTTR by dividing the number of types (the length of the set of all translation options) by the number of tokens (the sum of all translation options vectors).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Translation Quality</h4>
<div class="ltx_para ltx_noindent" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">We
also
calculate a general measure of translation quality, because the ‘naturalness’ of a translation does not necessarily imply that a translation is a faithful representation of the source. A randomly generated string sequence might be very lexically diverse, but likely does not carry the source meaning.
Firstly, we calculate BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx25" title="">Papineni et al., 2002</a>]</cite>, as implemented in SacreBLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx27" title="">Post, 2018</a>]</cite>. We use the default settings, which are case-sensitive.
Secondly, to account for the fact that BLEU does not necessarily evaluate meaning preservation, we additionally evaluate with COMET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#bib.bibx30" title="">Rei et al., 2020</a>]</cite>. English and Dutch are relatively high-resource languages, so we can use multilingual language embeddings. We report <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.1">comet-score</span>, calculated with the default <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.2">wmt22-comet-da</span>.
Still, it should be noted that these automatic metrics do not necessarily correlate strongly with human judgements, especially for literary translation.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Results and Analysis</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Quantitative Results</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">We first discuss the results over all books.
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.T3" title="Table 3 ‣ TTR ‣ 6.1 General Text Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the average results of measuring lexical diversity and general translation quality across the various approaches.
We find that vanilla MT indeed produces lexically poorer translations than HT, according to all our metrics.
While the scores of the APE baseline remain close to vanilla MT, our tailored reranking approach retrieves a lexical diversity that is closer to HT. This suggests that our method is a suitable alternative for post-hoc editing, given that one has access to the MT model for generating translation hypotheses.
The tagging baseline, which cannot be applied post-hoc, retrieves and MTLD and CDU that is on average closest to HT.
Importantly though, it should be noted that reranking and tagging are not mutually exclusive: one could apply reranking to the tagging baseline to increase or decrease lexical diversity further, where desired.
When we compare decoding strategies of the tailored reranking method, we first observe that using diverse beams search and choosing a larger <math alttext="n" class="ltx_Math" display="inline" id="S7.SS1.p1.1.m1.1"><semantics id="S7.SS1.p1.1.m1.1a"><mi id="S7.SS1.p1.1.m1.1.1" xref="S7.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S7.SS1.p1.1.m1.1b"><ci id="S7.SS1.p1.1.m1.1.1.cmml" xref="S7.SS1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p1.1.m1.1d">italic_n</annotation></semantics></math> retrieves at most slightly more diversity.
Especially top-k decoding retrieves a much higher lexical diversity.
However, tailored reranking comes with a compromise in terms of translation quality metrics.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">Next, we demonstrate that these averages omit a more fine-grained view.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S6.F3" title="Figure 3 ‣ 6.2 Translation-specific Metrics ‣ 6 Evaluation ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> shows the difference in MTLD per book between vanilla MT, HT, tagging and our most diverse reranking system, based on top-k sampling, which is tailored to the LexDiv score of the original English book.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>A similar figure with the posthoc baseline APE instead of tagging is shown in Appendix D.</span></span></span>
Our method renders almost every single book more lexically diverse than the tagging baseline.
In some cases, this makes the results closer to HT in terms of lexical diversity (e.g. 7, 13, 14, 16).
However, especially in cases where vanilla MT and HT are close already, this is not always true (e.g. 1, 3, 5).</p>
</div>
<figure class="ltx_table" id="S7.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S7.T4.1" style="width:447.8pt;height:274.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.8pt,7.2pt) scale(0.95,0.95) ;">
<table class="ltx_tabular ltx_align_middle" id="S7.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T4.1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T4.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.1.1.1">Ex. #</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S7.T4.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.1.2.1">Approach</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S7.T4.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S7.T4.1.1.1.1.3.1">Text</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T4.1.1.2.2.1">1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T4.1.1.2.2.2">Source</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T4.1.1.2.2.3">The <span class="ltx_text" id="S7.T4.1.1.2.2.3.1" style="background-color:#FAED5C;">kid</span> had no mother.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.3.3">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.3.3.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.3.3.2">HT</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.3.3.3">Dat <span class="ltx_text" id="S7.T4.1.1.3.3.3.1" style="background-color:#FAED5C;">joch</span> heeft geen moeder gehad.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.4.4">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.4.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.4.4.2">Vanilla MT</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.4.4.3">Het <span class="ltx_text" id="S7.T4.1.1.4.4.3.1" style="background-color:#FAED5C;">kind</span> had geen moeder.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.5.5">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.5.5.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.5.5.2">Tagging</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.5.5.3">De <span class="ltx_text" id="S7.T4.1.1.5.5.3.1" style="background-color:#FAED5C;">jongen</span> had geen moeder.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.6.6">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.6.6.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.6.6.2">Tailored RR</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.6.6.3">Het <span class="ltx_text" id="S7.T4.1.1.6.6.3.1" style="background-color:#FAED5C;">joch</span> had geen moeder.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T4.1.1.7.7.1">2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T4.1.1.7.7.2">Source</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T4.1.1.7.7.3">He <span class="ltx_text" id="S7.T4.1.1.7.7.3.1" style="background-color:#FAED5C;">shipped</span> his oars and brought a small <span class="ltx_text" id="S7.T4.1.1.7.7.3.2" style="background-color:#A1C9F2;">line</span> from under the bow.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.8.8">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.8.8.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.8.8.2">HT</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.8.8.3">Hij <span class="ltx_text" id="S7.T4.1.1.8.8.3.1" style="background-color:#FAED5C;">haalde</span> de riemen <span class="ltx_text" id="S7.T4.1.1.8.8.3.2" style="background-color:#FAED5C;">in</span> en pakte een kleine <span class="ltx_text" id="S7.T4.1.1.8.8.3.3" style="background-color:#A1C9F2;">lijn</span> die voor in de boot lag.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.9.9">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.9.9.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.9.9.2">Vanilla MT</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.9.9.3">Hij <span class="ltx_text" id="S7.T4.1.1.9.9.3.1" style="background-color:#FAED5C;">trok</span> zijn riemen <span class="ltx_text" id="S7.T4.1.1.9.9.3.2" style="background-color:#FAED5C;">aan</span> en haalde een klein <span class="ltx_text" id="S7.T4.1.1.9.9.3.3" style="background-color:#A1C9F2;">lijntje</span> onder de boeg vandaan.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.10.10">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.10.10.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.10.10.2">Tagging</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.10.10.3">Hij <span class="ltx_text" id="S7.T4.1.1.10.10.3.1" style="background-color:#FAED5C;">verscheurde</span> zijn riemen en haalde een klein <span class="ltx_text" id="S7.T4.1.1.10.10.3.2" style="background-color:#A1C9F2;">streepje</span> onder de boeg vandaan.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.11.11">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.11.11.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.11.11.2">Tailored RR</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.11.11.3">Hij <span class="ltx_text" id="S7.T4.1.1.11.11.3.1" style="background-color:#FAED5C;">haalde</span> zijn riemen en trok er een kleine <span class="ltx_text" id="S7.T4.1.1.11.11.3.2" style="background-color:#A1C9F2;">lijn</span> voor onder de boot vandaan.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T4.1.1.12.12.1">3</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S7.T4.1.1.12.12.2">Source</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T4.1.1.12.12.3">In long <span class="ltx_text" id="S7.T4.1.1.12.12.3.1" style="background-color:#FAED5C;">shaky</span> strokes Sargent <span class="ltx_text" id="S7.T4.1.1.12.12.3.2" style="background-color:#A1C9F2;">copied</span> the data.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.13.13">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.13.13.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.13.13.2">HT</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.13.13.3">In lange <span class="ltx_text" id="S7.T4.1.1.13.13.3.1" style="background-color:#FAED5C;">beverige</span> halen <span class="ltx_text" id="S7.T4.1.1.13.13.3.2" style="background-color:#A1C9F2;">kopieerde</span> Sargent de gegevenheden.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.14.14">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.14.14.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.14.14.2">Vanilla MT</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.14.14.3">Met lange, <span class="ltx_text" id="S7.T4.1.1.14.14.3.1" style="background-color:#FAED5C;">bevende</span> slagen <span class="ltx_text" id="S7.T4.1.1.14.14.3.2" style="background-color:#A1C9F2;">kopieerde</span> Sargent de gegevens.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.15.15">
<td class="ltx_td ltx_border_r" id="S7.T4.1.1.15.15.1"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T4.1.1.15.15.2">Tagging</td>
<td class="ltx_td ltx_align_left" id="S7.T4.1.1.15.15.3">Met lange <span class="ltx_text" id="S7.T4.1.1.15.15.3.1" style="background-color:#FAED5C;">bevende</span> halen <span class="ltx_text" id="S7.T4.1.1.15.15.3.2" style="background-color:#A1C9F2;">kopieerde</span> Sargent de gegevens.</td>
</tr>
<tr class="ltx_tr" id="S7.T4.1.1.16.16">
<td class="ltx_td ltx_border_bb ltx_border_r" id="S7.T4.1.1.16.16.1"></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S7.T4.1.1.16.16.2">Tailored RR</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S7.T4.1.1.16.16.3">Met lange <span class="ltx_text" id="S7.T4.1.1.16.16.3.1" style="background-color:#FAED5C;">beverige</span> halen <span class="ltx_text" id="S7.T4.1.1.16.16.3.2" style="background-color:#A1C9F2;">schreef</span> Sargent de data <span class="ltx_text" id="S7.T4.1.1.16.16.3.3" style="background-color:#A1C9F2;">over</span>.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Examples to highlight surface-level differences between the systems’ output translations, where Tailored RR uses top-k sampling.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Surface-level Inspection</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">The output translations were inspected by a native speaker. Table 4 shows three examples of how translations differ between vanilla MT, tagging and tailored reranking (with top-k sampling).
In Example 1 (from book 1, <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.1">Sunset Park</span>), we see that the English noun ‘kid’ is translated as <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.2">joch</span> (‘boy’) in the human translation, which is less common than the vanilla MT’s <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.3">kind</span> (‘child’) and tagging’s <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.4">jongen</span> (‘boy’). This is recovered by our tailored reranking system, which uses <span class="ltx_text ltx_font_italic" id="S7.SS2.p1.1.5">joch</span> too.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Example 2 is taken from book 10, <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.1">The Old Man and the Sea</span>, which has low lexical diversity by default (see Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3" title="3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>). This is not taken into account by the tagging baseline: the English ‘shipped’ is translated as a less common (and wrong) <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.2">verscheurde</span> (‘shredded’).
The tailored reranking system (<span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.3">haalde</span>, ‘brought’) is closest to HT (<span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.4">haalde in</span>, ‘brought in’).
Additionally, the tagging baseline wrongly translates the English ‘line’ as <span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.5">streepje</span> (‘small stripe’), while tailored reranking (<span class="ltx_text ltx_font_italic" id="S7.SS2.p2.1.6">lijn</span>, ‘line’) is again identical to HT. This case illustrates that choosing a more common translation synonym, which may for instance results in a lower PTF, may for some books be closer to HT.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">By contrast, in Example 3 from the more lexically diverse <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.1">Ulysses</span> (book 15), the tagging baseline stays closer to vanilla MT: both translate ‘shaky’ as <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.2">bevend</span> (‘trembling’).
Tailored reranking outputs <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.3">beverig</span> (‘shaky’), which is again recovering the HT.
Furthermore, tailored reranking deviates from all other systems (and HT) by translating ‘copied’ into the translation synonym <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.4">schreef over</span> (copying something by writing). This case may illustrate why the tailored reranking based on top-k sampling surpasses the other systems in the overall metrics.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Ranks and Lexical Diversity</h3>
<figure class="ltx_figure" id="S7.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S7.F4.g1" src="x9.png" width="392"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Change in MTLD for choosing different ranks, where beam size is 20 and <math alttext="n=20" class="ltx_Math" display="inline" id="S7.F4.2.m1.1"><semantics id="S7.F4.2.m1.1b"><mrow id="S7.F4.2.m1.1.1" xref="S7.F4.2.m1.1.1.cmml"><mi id="S7.F4.2.m1.1.1.2" xref="S7.F4.2.m1.1.1.2.cmml">n</mi><mo id="S7.F4.2.m1.1.1.1" xref="S7.F4.2.m1.1.1.1.cmml">=</mo><mn id="S7.F4.2.m1.1.1.3" xref="S7.F4.2.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.F4.2.m1.1c"><apply id="S7.F4.2.m1.1.1.cmml" xref="S7.F4.2.m1.1.1"><eq id="S7.F4.2.m1.1.1.1.cmml" xref="S7.F4.2.m1.1.1.1"></eq><ci id="S7.F4.2.m1.1.1.2.cmml" xref="S7.F4.2.m1.1.1.2">𝑛</ci><cn id="S7.F4.2.m1.1.1.3.cmml" type="integer" xref="S7.F4.2.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F4.2.m1.1d">n=20</annotation><annotation encoding="application/x-llamapun" id="S7.F4.2.m1.1e">italic_n = 20</annotation></semantics></math>.</figcaption>
</figure>
<figure class="ltx_figure" id="S7.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="208" id="S7.F5.g1" src="x10.png" width="871"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>MTLD for highest (green), lowest (red) and tailored (yellow) original-text rank.</figcaption>
</figure>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.2">So far, we have assumed that reranking based on the probability of a candidate being original text leads to more lexically diverse output translations. Here, we verify whether choosing a lower probability of a candidate being original, actually implies lexically poorer output translations (Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.F4" title="Figure 4 ‣ 7.3 Ranks and Lexical Diversity ‣ 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>).
For the vanilla MT system with beam size 20 and <math alttext="n=20" class="ltx_Math" display="inline" id="S7.SS3.p1.1.m1.1"><semantics id="S7.SS3.p1.1.m1.1a"><mrow id="S7.SS3.p1.1.m1.1.1" xref="S7.SS3.p1.1.m1.1.1.cmml"><mi id="S7.SS3.p1.1.m1.1.1.2" xref="S7.SS3.p1.1.m1.1.1.2.cmml">n</mi><mo id="S7.SS3.p1.1.m1.1.1.1" xref="S7.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS3.p1.1.m1.1.1.3" xref="S7.SS3.p1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.1.m1.1b"><apply id="S7.SS3.p1.1.m1.1.1.cmml" xref="S7.SS3.p1.1.m1.1.1"><eq id="S7.SS3.p1.1.m1.1.1.1.cmml" xref="S7.SS3.p1.1.m1.1.1.1"></eq><ci id="S7.SS3.p1.1.m1.1.1.2.cmml" xref="S7.SS3.p1.1.m1.1.1.2">𝑛</ci><cn id="S7.SS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S7.SS3.p1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.1.m1.1c">n=20</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p1.1.m1.1d">italic_n = 20</annotation></semantics></math>, we first calculate the original-text probability for each translation hypothesis.
Similar to reranking, we sort the hypotheses according to this probability. Then, instead of binning, we choose the <math alttext="n^{th}" class="ltx_Math" display="inline" id="S7.SS3.p1.2.m2.1"><semantics id="S7.SS3.p1.2.m2.1a"><msup id="S7.SS3.p1.2.m2.1.1" xref="S7.SS3.p1.2.m2.1.1.cmml"><mi id="S7.SS3.p1.2.m2.1.1.2" xref="S7.SS3.p1.2.m2.1.1.2.cmml">n</mi><mrow id="S7.SS3.p1.2.m2.1.1.3" xref="S7.SS3.p1.2.m2.1.1.3.cmml"><mi id="S7.SS3.p1.2.m2.1.1.3.2" xref="S7.SS3.p1.2.m2.1.1.3.2.cmml">t</mi><mo id="S7.SS3.p1.2.m2.1.1.3.1" xref="S7.SS3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S7.SS3.p1.2.m2.1.1.3.3" xref="S7.SS3.p1.2.m2.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S7.SS3.p1.2.m2.1b"><apply id="S7.SS3.p1.2.m2.1.1.cmml" xref="S7.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S7.SS3.p1.2.m2.1.1.1.cmml" xref="S7.SS3.p1.2.m2.1.1">superscript</csymbol><ci id="S7.SS3.p1.2.m2.1.1.2.cmml" xref="S7.SS3.p1.2.m2.1.1.2">𝑛</ci><apply id="S7.SS3.p1.2.m2.1.1.3.cmml" xref="S7.SS3.p1.2.m2.1.1.3"><times id="S7.SS3.p1.2.m2.1.1.3.1.cmml" xref="S7.SS3.p1.2.m2.1.1.3.1"></times><ci id="S7.SS3.p1.2.m2.1.1.3.2.cmml" xref="S7.SS3.p1.2.m2.1.1.3.2">𝑡</ci><ci id="S7.SS3.p1.2.m2.1.1.3.3.cmml" xref="S7.SS3.p1.2.m2.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p1.2.m2.1c">n^{th}</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p1.2.m2.1d">italic_n start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT</annotation></semantics></math> rank, and calculate lexical diversity of the output.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.F4" title="Figure 4 ‣ 7.3 Ranks and Lexical Diversity ‣ 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> shows the change in MTLD scores for choosing a lower diversity rank. We observe that indeed, choosing a lower rank retrieves lower diversity (note that there, a higher rank represents a smaller original-text probability). This trend holds for TTR and Yule’s I as well (see Appendix E).</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Tailoring and Lexical Diversity</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">To further demonstrate the effect of a <span class="ltx_text ltx_font_italic" id="S7.SS4.p1.1.1">tailored</span> approach in lexical diversity, we compare MTLD scores of a top-k reranking system that always outputs the highest original-text probability, with the same system that always outputs the lowest, and a tailored version. Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S7.F5" title="Figure 5 ‣ 7.3 Ranks and Lexical Diversity ‣ 7 Results and Analysis ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> shows the results.
Firstly, we observe that, in every case, choosing a rank that represents lower original-text probability retrieves a lower MTLD score than choosing the opposite. This corroborates the findings from the previous section.
Next, we look into how the tailored reranking affects the output lexical diversity.
In Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3" title="3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, we used <span class="ltx_text ltx_font_italic" id="S7.SS4.p1.1.2">The Old Man and the Sea</span> (book 10) as an example of a book with a low default lexical diversity. We observe that our tailored reranking system outputs the lowest original-text probability rank for this book, resulting in a lower MTLD score.
For the example from Section <a class="ltx_ref" href="https://arxiv.org/html/2408.17308v1#S3" title="3 Why Recover Rather Than Increase Lexical Diversity? ‣ Towards Tailored Recovery of Lexical Diversity in Literary Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a> of a lexically rich book, <span class="ltx_text ltx_font_italic" id="S7.SS4.p1.1.3">Ulysses</span> (book 15), our tailored system outputs a rank with a original-text probability higher than the minimum, thus retrieving an MTLD score that is higher.
This shows that tailoring is at least somewhat intuitive.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We have argued for flexible recovery of lexical diversity in literary MT.
We showed that default diversity varies per book in our dataset, and that this lexical diversity is partially lost through MT.
We presented the first approach towards tailored rescoring of translation candidates, which matches HT more closely than previous baselines for some books.
Future work could explore how our method can be combined with previous work, as it is in principle model-agnostic.
Investigations with document-level translation, instead of sentence-level translation only, could provide additional insights.
Furthermore, it may be useful to address this task at an even finer-grained level, by exploring diversity reranking on a sequence-level, instead of a book-level.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">In this paper, we addressed the increase of lexical diversity in literary MT. However, it should be noted that this is does not encompass writing style as a whole.
We evaluated our approach on one high-resource language pair that consist of relatively similar languages, in one translation direction. For the domain of literary translation, we find this to be difficult to avoid. Still, experiments with more languages and resource-scenarios may retrieve interesting results.
Moreover, while our data is transparent in the sense that we know and can explain exactly what it contains, we cannot distribute the data ourselves because of copyright.
Lastly, we acknowledge that large-scale human evaluation could give useful insights into the differences between the systems.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work was supported by a <span class="ltx_text ltx_font_italic" id="Sx2.p1.1.1">Semper Ardens: Accelerate research grant (CF21-0454)</span> from the Carlsberg Foundation.
We thank the Center for Information Technology of the University of Groningen for their support and for providing access to the Peregrine and Hábrók high performance computing cluster.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bibx1">
<span class="ltx_tag ltx_tag_bibitem">[Arcadinho et al., 2022] </span>
<span class="ltx_bibblock">
Arcadinho, Samuel David, David Aparicio, Hugo Veiga, and Antonio Alegria.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">T5QL: Taming language models for SQL generation.

</span>
<span class="ltx_bibblock">In Bosselut, Antoine, Khyathi Chandu, Kaustubh Dhole, Varun Gangal, Sebastian Gehrmann, Yacine Jernite, Jekaterina Novikova, and Laura Perez-Beltrachini, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx1.1.1">Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)</span>, pages 276–286, Abu Dhabi, United Arab Emirates (Hybrid), December. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx2">
<span class="ltx_tag ltx_tag_bibitem">[Baker, 1993] </span>
<span class="ltx_bibblock">
Baker, Mona.

</span>
<span class="ltx_bibblock">1993.

</span>
<span class="ltx_bibblock">Corpus linguistics and translation studies—implications and applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx2.1.1">Text and Technology</span>, page 233. John Benjamins.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx3">
<span class="ltx_tag ltx_tag_bibitem">[Baroni and Bernardini, 2005] </span>
<span class="ltx_bibblock">
Baroni, Marco and Silvia Bernardini.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">A New Approach to the Study of Translationese: Machine-learning the Difference between Original and Translated Text.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx3.1.1">Literary and Linguistic Computing</span>, 21(3):259–274, 08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx4">
<span class="ltx_tag ltx_tag_bibitem">[Boase-Beier, 2011] </span>
<span class="ltx_bibblock">
Boase-Beier, Jean.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx4.1.1">A critical introduction to translation studies</span>.

</span>
<span class="ltx_bibblock">Bloomsbury Publishing.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx5">
<span class="ltx_tag ltx_tag_bibitem">[Collins and Koo, 2005] </span>
<span class="ltx_bibblock">
Collins, Michael and Terry Koo.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock">Discriminative reranking for natural language parsing.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx5.1.1">Computational Linguistics</span>, 31(1):25–70.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx6">
<span class="ltx_tag ltx_tag_bibitem">[de Vries et al., 2019] </span>
<span class="ltx_bibblock">
de Vries, Wietse, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">BERTje: A Dutch BERT Model.

</span>
<span class="ltx_bibblock">arXiv:1912.09582, December.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx7">
<span class="ltx_tag ltx_tag_bibitem">[Delabastita, 2011] </span>
<span class="ltx_bibblock">
Delabastita, Dirk.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Literary translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx7.1.1">Handbook of translation studies</span>, 2:69–78.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx8">
<span class="ltx_tag ltx_tag_bibitem">[Devlin et al., 2019] </span>
<span class="ltx_bibblock">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx8.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx9">
<span class="ltx_tag ltx_tag_bibitem">[Dutta Chowdhury et al., 2022] </span>
<span class="ltx_bibblock">
Dutta Chowdhury, Koel, Rricha Jalota, Cristina España-Bonet, and Josef Genabith.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Towards debiasing translation artifacts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx9.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, pages 3983–3991, Seattle, United States, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx10">
<span class="ltx_tag ltx_tag_bibitem">[Fischer and Läubli, 2020] </span>
<span class="ltx_bibblock">
Fischer, Lukas and Samuel Läubli.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">What’s the difference between professional human and machine translation? a blind multi-language study on domain-specific MT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx10.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</span>, pages 215–224, Lisboa, Portugal, November. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx11">
<span class="ltx_tag ltx_tag_bibitem">[Freitag et al., 2019] </span>
<span class="ltx_bibblock">
Freitag, Markus, Isaac Caswell, and Scott Roy.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">APE at scale and its implications on MT evaluation biases.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx11.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</span>, pages 34–44, Florence, Italy, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx12">
<span class="ltx_tag ltx_tag_bibitem">[Freitag et al., 2022] </span>
<span class="ltx_bibblock">
Freitag, Markus, David Vilar, David Grangier, Colin Cherry, and George Foster.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">A natural diet: Towards improving naturalness of machine translation output.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx12.1.1">Findings of the Association for Computational Linguistics: ACL 2022</span>, pages 3340–3353, Dublin, Ireland, May. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx13">
<span class="ltx_tag ltx_tag_bibitem">[Heaton, 1970] </span>
<span class="ltx_bibblock">
Heaton, CP.

</span>
<span class="ltx_bibblock">1970.

</span>
<span class="ltx_bibblock">Style in the old man and the sea.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx13.1.1">Style</span>, pages 11–27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx14">
<span class="ltx_tag ltx_tag_bibitem">[Jalota et al., 2023] </span>
<span class="ltx_bibblock">
Jalota, Rricha, Koel Chowdhury, Cristina España-Bonet, and Josef van Genabith.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">Translating away translationese without parallel data.

</span>
<span class="ltx_bibblock">In Bouamor, Houda, Juan Pino, and Kalika Bali, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx14.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</span>, pages 7086–7100, Singapore, December. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx15">
<span class="ltx_tag ltx_tag_bibitem">[Jimenez-Crespo, 2023] </span>
<span class="ltx_bibblock">
Jimenez-Crespo, Miguel A.

</span>
<span class="ltx_bibblock">2023.

</span>
<span class="ltx_bibblock">“translationese” (and “post-editese”?) no more: on importing fuzzy conceptual tools from translation studies in MT research.

</span>
<span class="ltx_bibblock">In Nurminen, Mary, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx15.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</span>, pages 261–268, Tampere, Finland, June. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx16">
<span class="ltx_tag ltx_tag_bibitem">[Kingma and Ba, 2015] </span>
<span class="ltx_bibblock">
Kingma, Diederik P. and Jimmy Ba.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In Bengio, Yoshua and Yann LeCun, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx16.1.1">3rd International Conference on Learning Representations (ICLR 2015)</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx17">
<span class="ltx_tag ltx_tag_bibitem">[Koppel and Ordan, 2011] </span>
<span class="ltx_bibblock">
Koppel, Moshe and Noam Ordan.

</span>
<span class="ltx_bibblock">2011.

</span>
<span class="ltx_bibblock">Translationese and its dialects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx17.1.1">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</span>, pages 1318–1326, Portland, Oregon, USA, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx18">
<span class="ltx_tag ltx_tag_bibitem">[Kudo and Richardson, 2018] </span>
<span class="ltx_bibblock">
Kudo, Taku and John Richardson.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx18.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</span>, pages 66–71, Brussels, Belgium, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx19">
<span class="ltx_tag ltx_tag_bibitem">[Lakew et al., 2018] </span>
<span class="ltx_bibblock">
Lakew, Surafel Melaku, Mauro Cettolo, and Marcello Federico.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">A comparison of transformer and recurrent neural networks on multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx19.1.1">Proceedings of the 27th International Conference on Computational Linguistics</span>, pages 641–652, Santa Fe, New Mexico, USA, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx20">
<span class="ltx_tag ltx_tag_bibitem">[Lee et al., 2021] </span>
<span class="ltx_bibblock">
Lee, Ann, Michael Auli, and Marc’Aurelio Ranzato.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Discriminative reranking for neural machine translation.

</span>
<span class="ltx_bibblock">In Zong, Chengqing, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx20.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 7250–7264, Online, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx21">
<span class="ltx_tag ltx_tag_bibitem">[Liu and Liu, 2021] </span>
<span class="ltx_bibblock">
Liu, Yixin and Pengfei Liu.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">SimCLS: A simple framework for contrastive learning of abstractive summarization.

</span>
<span class="ltx_bibblock">In Zong, Chengqing, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx21.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</span>, pages 1065–1072, Online, August. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx22">
<span class="ltx_tag ltx_tag_bibitem">[Matusov, 2019] </span>
<span class="ltx_bibblock">
Matusov, Evgeny.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">The challenges of using neural machine translation for literature.

</span>
<span class="ltx_bibblock">In Hadley, James, Maja Popović, Haithem Afli, and Andy Way, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx22.1.1">Proceedings of the Qualities of Literary Machine Translation</span>, pages 10–19, Dublin, Ireland, August. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx23">
<span class="ltx_tag ltx_tag_bibitem">[McCarthy, 2005] </span>
<span class="ltx_bibblock">
McCarthy, Philip M.

</span>
<span class="ltx_bibblock">2005.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx23.1.1">An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity (MTLD)</span>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, The University of Memphis.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx24">
<span class="ltx_tag ltx_tag_bibitem">[Ott et al., 2019] </span>
<span class="ltx_bibblock">
Ott, Myle, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx24.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</span>, pages 48–53, Minneapolis, Minnesota, June. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx25">
<span class="ltx_tag ltx_tag_bibitem">[Papineni et al., 2002] </span>
<span class="ltx_bibblock">
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx25.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</span>, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx26">
<span class="ltx_tag ltx_tag_bibitem">[Popel et al., 2020] </span>
<span class="ltx_bibblock">
Popel, Martin, Marketa Tomkova, Jakub Tomek, Łukasz Kaiser, Jakob Uszkoreit, Ondřej Bojar, and Zdeněk Žabokrtskỳ.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx26.1.1">Nature communications</span>, 11(1):1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx27">
<span class="ltx_tag ltx_tag_bibitem">[Post, 2018] </span>
<span class="ltx_bibblock">
Post, Matt.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting BLEU scores.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx27.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 186–191, Brussels, Belgium, October. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx28">
<span class="ltx_tag ltx_tag_bibitem">[Pylypenko et al., 2021] </span>
<span class="ltx_bibblock">
Pylypenko, Daria, Kwabena Amponsah-Kaakyire, Koel Dutta Chowdhury, Josef van Genabith, and Cristina España-Bonet.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Comparing feature-engineering and feature-learning approaches for multilingual translationese classification.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx28.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</span>, pages 8596–8611, Online and Punta Cana, Dominican Republic, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx29">
<span class="ltx_tag ltx_tag_bibitem">[Rabinovich and Wintner, 2015] </span>
<span class="ltx_bibblock">
Rabinovich, Ella and Shuly Wintner.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Unsupervised identification of translationese.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx29.1.1">Transactions of the Association for Computational Linguistics</span>, 3:419–432.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx30">
<span class="ltx_tag ltx_tag_bibitem">[Rei et al., 2020] </span>
<span class="ltx_bibblock">
Rei, Ricardo, Craig Stewart, Ana C Farinha, and Alon Lavie.

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">COMET: A neural framework for MT evaluation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx30.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 2685–2702, Online, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx31">
<span class="ltx_tag ltx_tag_bibitem">[Riera, 2022] </span>
<span class="ltx_bibblock">
Riera, Jorge Braga.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Literatura-traducción.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx31.1.1">Enciclopedia de Traducción e Interpretación</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx32">
<span class="ltx_tag ltx_tag_bibitem">[Shen et al., 2004] </span>
<span class="ltx_bibblock">
Shen, Libin, Anoop Sarkar, and Franz Josef Och.

</span>
<span class="ltx_bibblock">2004.

</span>
<span class="ltx_bibblock">Discriminative reranking for machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx32.1.1">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</span>, pages 177–184.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx33">
<span class="ltx_tag ltx_tag_bibitem">[Shen, 2022] </span>
<span class="ltx_bibblock">
Shen, Lucas.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">LexicalRichness: A small module to compute textual lexical richness.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx34">
<span class="ltx_tag ltx_tag_bibitem">[Thompson and Koehn, 2019] </span>
<span class="ltx_bibblock">
Thompson, Brian and Philipp Koehn.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Vecalign: Improved sentence alignment in linear time and space.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx34.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span>, pages 1342–1348, Hong Kong, China, November. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx35">
<span class="ltx_tag ltx_tag_bibitem">[Toral and Way, 2015] </span>
<span class="ltx_bibblock">
Toral, Antonio and Andy Way.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">Machine-assisted translation of literary text: A case study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx35.1.1">Translation Spaces</span>, 4(2):240–267.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx36">
<span class="ltx_tag ltx_tag_bibitem">[Toral et al., 2018] </span>
<span class="ltx_bibblock">
Toral, Antonio, Sheila Castilho, Ke Hu, and Andy Way.

</span>
<span class="ltx_bibblock">2018.

</span>
<span class="ltx_bibblock">Attaining the unattainable? Reassessing claims of human parity in neural machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx36.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</span>, pages 113–123, Brussels, Belgium, October. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx37">
<span class="ltx_tag ltx_tag_bibitem">[Toral et al., 2024] </span>
<span class="ltx_bibblock">
Toral, Antonio, Andreas van Cranenburgh, and Tia Nutters, 2024.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx37.1.1">Literary-adapted machine translation in a well-resourced language pair: Explorations with More Data and Wider Contexts</span>, pages 27–52.

</span>
<span class="ltx_bibblock">Routledge.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx38">
<span class="ltx_tag ltx_tag_bibitem">[Trotta, 2014] </span>
<span class="ltx_bibblock">
Trotta, Joe.

</span>
<span class="ltx_bibblock">2014.

</span>
<span class="ltx_bibblock">Creativity, playfulness and linguistic carnivalization in james joyce’s ulysses.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx39">
<span class="ltx_tag ltx_tag_bibitem">[van der Werff et al., 2022] </span>
<span class="ltx_bibblock">
van der Werff, Tobias, Rik van Noord, and Antonio Toral.

</span>
<span class="ltx_bibblock">2022.

</span>
<span class="ltx_bibblock">Automatic discrimination of human and neural machine translation: A study with multiple pre-trained models and longer context.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx39.1.1">Proceedings of the 23rd Annual Conference of the European Association for Machine Translation</span>, pages 161–170, Ghent, Belgium, June. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx40">
<span class="ltx_tag ltx_tag_bibitem">[Vanmassenhove et al., 2019] </span>
<span class="ltx_bibblock">
Vanmassenhove, Eva, Dimitar Shterionov, and Andy Way.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Lost in translation: Loss and decay of linguistic richness in machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx40.1.1">Proceedings of Machine Translation Summit XVII: Research Track</span>, pages 222–232.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx41">
<span class="ltx_tag ltx_tag_bibitem">[Vanmassenhove et al., 2021] </span>
<span class="ltx_bibblock">
Vanmassenhove, Eva, Dimitar Shterionov, and Matthew Gwilliam.

</span>
<span class="ltx_bibblock">2021.

</span>
<span class="ltx_bibblock">Machine translationese: Effects of algorithmic bias on linguistic complexity in machine translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bibx41.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</span>, pages 2203–2213, Online, April. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx42">
<span class="ltx_tag ltx_tag_bibitem">[Vaswani et al., 2017] </span>
<span class="ltx_bibblock">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx42.1.1">Advances in neural information processing systems</span>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx43">
<span class="ltx_tag ltx_tag_bibitem">[Vijayakumar et al., 2016] </span>
<span class="ltx_bibblock">
Vijayakumar, Ashwin K, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock">Diverse beam search: Decoding diverse solutions from neural sequence models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx43.1.1">arXiv preprint arXiv:1610.02424</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx44">
<span class="ltx_tag ltx_tag_bibitem">[Volansky et al., 2015] </span>
<span class="ltx_bibblock">
Volansky, Vered, Noam Ordan, and Shuly Wintner.

</span>
<span class="ltx_bibblock">2015.

</span>
<span class="ltx_bibblock">On the features of translationese.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx44.1.1">Digital Scholarship in the Humanities</span>, 30(1):98–118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx45">
<span class="ltx_tag ltx_tag_bibitem">[Wright, 2016] </span>
<span class="ltx_bibblock">
Wright, Chantal.

</span>
<span class="ltx_bibblock">2016.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx45.1.1">Literary translation</span>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx46">
<span class="ltx_tag ltx_tag_bibitem">[Yin and Neubig, 2019] </span>
<span class="ltx_bibblock">
Yin, Pengcheng and Graham Neubig.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">Reranking for neural semantic parsing.

</span>
<span class="ltx_bibblock">In Korhonen, Anna, David Traum, and Lluís Màrquez, editors, <span class="ltx_text ltx_font_italic" id="bib.bibx46.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 4553–4559, Florence, Italy, July. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bibx47">
<span class="ltx_tag ltx_tag_bibitem">[Yule, 1944] </span>
<span class="ltx_bibblock">
Yule, C Udny.

</span>
<span class="ltx_bibblock">1944.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bibx47.1.1">The statistical study of literary vocabulary</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">A     Test set novels</span></p>
</div>
<figure class="ltx_table" id="A0.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A0.T5.1" style="width:384.2pt;height:450.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.2pt,63.4pt) scale(0.78,0.78) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A0.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A0.T5.1.1.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A0.T5.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A0.T5.1.1.1.1.1.1">ID</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A0.T5.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A0.T5.1.1.1.1.2.1">Author</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A0.T5.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A0.T5.1.1.1.1.3.1">Title</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A0.T5.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A0.T5.1.1.1.1.4.1">Year Published</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A0.T5.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A0.T5.1.1.1.1.5.1">Genre</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A0.T5.1.1.2.1">
<td class="ltx_td ltx_align_right ltx_border_t" id="A0.T5.1.1.2.1.1">1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A0.T5.1.1.2.1.2">Paul Auster</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A0.T5.1.1.2.1.3">Sunset Park</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A0.T5.1.1.2.1.4">2010</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A0.T5.1.1.2.1.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.3.2">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.3.2.1">2</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.3.2.2">David Baldacci</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.3.2.3">Divine Justice</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.3.2.4">2008</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.3.2.5">Thriller, suspense</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.4.3">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.4.3.1">3</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.4.3.2">Julian Barnes</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.4.3.3">The Sense of an Ending</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.4.3.4">2011</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.4.3.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.5.4">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.5.4.1">4</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.5.4.2">John Boyne</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.5.4.3">The Boy in the Striped Pyjamas</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.5.4.4">2006</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.5.4.5">Historical fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.6.5">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.6.5.1">5</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.6.5.2">John le Carré</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.6.5.3">Our Kind of Traitor</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.6.5.4">2010</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.6.5.5">Thriller, spy fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.7.6">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.7.6.1">6</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.7.6.2">Jonathan Franzen</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.7.6.3">The Corrections</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.7.6.4">2001</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.7.6.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.8.7">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.8.7.1">7</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.8.7.2">Nicci French</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.8.7.3">Blue Monday: A Frieda Klein Mystery</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.8.7.4">2011</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.8.7.5">Thriller, suspense</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.9.8">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.9.8.1">8</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.9.8.2">William Golding</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.9.8.3">Lord of the Flies</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.9.8.4">1954</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.9.8.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.10.9">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.10.9.1">9</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.10.9.2">John Grisham</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.10.9.3">The Confession</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.10.9.4">2010</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.10.9.5">Thriller, suspense</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.11.10">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.11.10.1">10</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.11.10.2">Ernest Hemingway</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.11.10.3">The Old Man and the Sea</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.11.10.4">1952</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.11.10.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.12.11">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.12.11.1">11</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.12.11.2">Patricia Highsmith</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.12.11.3">Ripley Under Water</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.12.11.4">1991</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.12.11.5">Thriller, suspense</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.13.12">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.13.12.1">12</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.13.12.2">Khaled Hosseini</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.13.12.3">A Thousand Splendid Suns</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.13.12.4">2007</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.13.12.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.14.13">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.14.13.1">13</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.14.13.2">John Irving</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.14.13.3">Last Night in Twisted River</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.14.13.4">2009</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.14.13.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.15.14">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.15.14.1">14</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.15.14.2">E.L. James</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.15.14.3">Fifty Shades of Grey</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.15.14.4">2011</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.15.14.5">Erotic thriller</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.16.15">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.16.15.1">15</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.16.15.2">James Joyce</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.16.15.3">Ulysses</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.16.15.4">1922</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.16.15.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.17.16">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.17.16.1">16</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.17.16.2">Jack Kerouac</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.17.16.3">On the Road</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.17.16.4">1957</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.17.16.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.18.17">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.18.17.1">17</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.18.17.2">Stephen King</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.18.17.3">11/22/63</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.18.17.4">2011</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.18.17.5">Science-fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.19.18">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.19.18.1">18</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.19.18.2">Sophie Kinsella</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.19.18.3">Shopaholic and Baby</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.19.18.4">2007</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.19.18.5">Popular literature</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.20.19">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.20.19.1">19</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.20.19.2">David Mitchell</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.20.19.3">The Thousand Autumns of Jacob de Zoet</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.20.19.4">2010</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.20.19.5">Historical fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.21.20">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.21.20.1">20</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.21.20.2">George Orwell</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.21.20.3">1984</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.21.20.4">1949</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.21.20.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.22.21">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.22.21.1">21</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.22.21.2">James Patterson</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.22.21.3">The Quickie</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.22.21.4">2007</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.22.21.5">Thriller, suspense</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.23.22">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.23.22.1">22</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.23.22.2">Thomas Pynchon</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.23.22.3">Gravity’s Rainbow</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.23.22.4">1973</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.23.22.5">Historical fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.24.23">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.24.23.1">23</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.24.23.2">Philip Roth</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.24.23.3">The Plot Against America</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.24.23.4">2004</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.24.23.5">Political fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.25.24">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.25.24.1">24</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.25.24.2">J.K. Rowling</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.25.24.3">Harry Potter and the Deathly Hallows</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.25.24.4">2007</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.25.24.5">Fantasy</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.26.25">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.26.25.1">25</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.26.25.2">J.D. Salinger</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.26.25.3">The Catcher in the Rye</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.26.25.4">1951</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.26.25.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.27.26">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.27.26.1">26</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.27.26.2">Karin Slaughter</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.27.26.3">Fractured</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.27.26.4">2008</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.27.26.5">Thriller, suspense</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.28.27">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.28.27.1">27</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.28.27.2">John Steinbeck</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.28.27.3">The Grapes of Wrath</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.28.27.4">1939</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.28.27.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.29.28">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.29.28.1">28</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.29.28.2">J.R.R Tolkien</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.29.28.3">The Return of the King</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.29.28.4">1955</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.29.28.5">Fantasy</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.30.29">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.30.29.1">29</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.30.29.2">Mark Twain</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.30.29.3">Adventures of Huckleberry Finn</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.30.29.4">1884</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.30.29.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.31.30">
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.31.30.1">30</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.31.30.2">Oscar Wilde</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.31.30.3">The Picture of Dorian Gray</td>
<td class="ltx_td ltx_align_right" id="A0.T5.1.1.31.30.4">1890</td>
<td class="ltx_td ltx_align_left" id="A0.T5.1.1.31.30.5">Literary fiction</td>
</tr>
<tr class="ltx_tr" id="A0.T5.1.1.32.31">
<td class="ltx_td ltx_align_right ltx_border_bb" id="A0.T5.1.1.32.31.1">31</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A0.T5.1.1.32.31.2">Irvin D. Yalom</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A0.T5.1.1.32.31.3">The Spinoza Problem</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A0.T5.1.1.32.31.4">2012</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A0.T5.1.1.32.31.5">Historical fiction</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Information on test set books.</figcaption>
</figure>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1">B     Regression plots for human translation vs. original text lexical diversity</span></p>
</div>
<figure class="ltx_figure" id="A0.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="216" id="A0.F6.g1" src="x11.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="216" id="A0.F6.g2" src="x12.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="216" id="A0.F6.g3" src="x13.png" width="287"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Regression plots for TTR, Yule’s I and MTLD, with on the y-axis the scores for the original (English) versions, and on the x-axis those for human translations.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text ltx_font_bold" id="p3.1.1">C     Annotation workflow for monolingual Dutch books</span></p>
</div>
<div class="ltx_para" id="p4">
<ol class="ltx_enumerate" id="A0.I1">
<li class="ltx_item" id="A0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A0.I1.i1.p1">
<p class="ltx_p" id="A0.I1.i1.p1.1">Check whether the book is prose: we generally discard other forms of literature such as poetry and plays and annotate this in category 3 (no label).</p>
</div>
</li>
<li class="ltx_item" id="A0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A0.I1.i2.p1">
<p class="ltx_p" id="A0.I1.i2.p1.1">Check whether the original language of the book is listed on the website of the National Dutch Library.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://www.bibliotheek.nl/</span></span></span> If this is not the case:</p>
<ol class="ltx_enumerate" id="A0.I1.i2.I1">
<li class="ltx_item" id="A0.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="A0.I1.i2.I1.i1.p1">
<p class="ltx_p" id="A0.I1.i2.I1.i1.p1.1">Check whether the language of the book is listed on the website of a Dutch reading community website.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>https://www.hebban.nl/</span></span></span></p>
</div>
</li>
<li class="ltx_item" id="A0.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span>
<div class="ltx_para" id="A0.I1.i2.I1.i2.p1">
<p class="ltx_p" id="A0.I1.i2.I1.i2.p1.1">If step (a) is also not conclusive: check whether more information on the author is available, for instance on a personal website where we can find the original titles.</p>
</div>
</li>
<li class="ltx_item" id="A0.I1.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span>
<div class="ltx_para" id="A0.I1.i2.I1.i3.p1">
<p class="ltx_p" id="A0.I1.i2.I1.i3.p1.1">In case there is no reliable information available on the original language of a book, we discard the book (category 3: no label)</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A0.I1.i3.p1">
<p class="ltx_p" id="A0.I1.i3.p1.1">Book titles with Dutch as their original language are annotated with the label ‘1’ (category 1). Books that were written in a language other than Dutch were annotated with the label ‘0’ (category 2).</p>
</div>
</li>
</ol>
</div>
<section class="ltx_paragraph" id="A0.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Special cases</h5>
<div class="ltx_para" id="A0.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A0.SS0.SSS0.Px1.p1.1">An interesting annotation case regards books from bilingual authors who learned Dutch at a later age, such as Kader Abdolah. In our current guidelines, we do not take this into account specifically; if originally written in Dutch, these books are annotated with category 1.
We note that books that were translated to Dutch were not all originally written in English: other source languages in the data set include German, French and Spanish.</p>
</div>
<div class="ltx_para" id="A0.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="A0.SS0.SSS0.Px1.p2.1"><span class="ltx_text ltx_font_bold" id="A0.SS0.SSS0.Px1.p2.1.1">D     Book-level MTLD comparison of APE and tailored reranking (top-k sampling)</span></p>
</div>
<figure class="ltx_figure" id="A0.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="A0.F7.g1" src="x14.png" width="784"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>MTLD scores for APE and tailored reranking with top-k sampling, with on the y-axis the MTLD score for each book in our test set (x-axis).</figcaption>
</figure>
<div class="ltx_para" id="A0.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="A0.SS0.SSS0.Px1.p3.1"><span class="ltx_text ltx_font_bold" id="A0.SS0.SSS0.Px1.p3.1.1">E     Lexical diversity according to ranks</span></p>
</div>
<figure class="ltx_figure" id="A0.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="211" id="A0.F8.g1" src="x15.png" width="287"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="217" id="A0.F8.g2" src="x16.png" width="288"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="217" id="A0.F8.g3" src="x17.png" width="287"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>TTR, MTLD and Yule’s I according to original-text rank, where a higher rank represents smaller original-text probability.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 30 14:02:59 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
