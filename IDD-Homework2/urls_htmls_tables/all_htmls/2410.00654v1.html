<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Explainable Multi-Stakeholder Job Recommender Systems</title>
<!--Generated on Tue Oct  1 13:08:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Job Recommender Systems,  Explainable AI,  Multi-Stakeholder Recommendation,  Graph Neural Networks,  Knowledge Graphs" lang="en" name="keywords"/>
<base href="/html/2410.00654v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S1" title="In Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Background and Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S2" title="In Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Completed research</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S2.SS1" title="In 2. Completed research ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Stakeholder preferences (SQ1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S2.SS2" title="In 2. Completed research ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Mock-up System Experiment (SQ1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S2.SS3" title="In 2. Completed research ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Explainable Graph Neural Network (SQ2)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3" title="In Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Future work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS1" title="In 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>SQ1: Designing desirable explanations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS1.SSS1" title="In 3.1. SQ1: Designing desirable explanations ‣ 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Improving explanation coherence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS1.SSS2" title="In 3.1. SQ1: Designing desirable explanations ‣ 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Clarifying textual explanations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS2" title="In 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>SQ2: Improving model performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS2.SSS1" title="In 3.2. SQ2: Improving model performance ‣ 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Exploiting linked data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS3" title="In 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>SQ3: Evaluating the system as a whole</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS3.SSS1" title="In 3.3. SQ3: Evaluating the system as a whole ‣ 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Evaluating in a real-world context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S3.SS3.SSS2" title="In 3.3. SQ3: Evaluating the system as a whole ‣ 3. Future work ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>List-wise explanations</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Explainable Multi-Stakeholder Job Recommender Systems</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roan Schellingerhout
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-7388-309X" title="ORCID identifier">0000-0002-7388-309X</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Maastricht University</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">Paul-henri Spaaklaan 1</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Maastricht</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">Limburg</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">The Netherlands</span><span class="ltx_text ltx_affiliation_postcode" id="id6.6.id6">6229 EN</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:roan.schellingerhout@maastrichtuniversity.nl">roan.schellingerhout@maastrichtuniversity.nl</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id7.id1">Public opinion on recommender systems has become increasingly wary in recent years. In line with this trend, lawmakers have also started to become more critical of such systems, resulting in the introduction of new laws focusing on aspects such as privacy, fairness, and explainability for recommender systems and AI at large. These concepts are especially crucial in high-risk domains such as recruitment. In recruitment specifically, decisions carry substantial weight, as the outcomes can significantly impact individuals’ careers and companies’ success. Additionally, there is a need for a multi-stakeholder approach, as these systems are used by job seekers, recruiters, and companies simultaneously, each with its own requirements and expectations. In this paper, I summarize my current research on the topic of explainable, multi-stakeholder job recommender systems and set out a number of future research directions.</p>
</div>
<div class="ltx_keywords">Job Recommender Systems, Explainable AI, Multi-Stakeholder Recommendation, Graph Neural Networks, Knowledge Graphs
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3640457.3688014</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>October 14–18,
2024; Bari, Italy; </span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Background and Context</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With recommender systems being one of the most widespread forms of machine learning used, they tend to be under heavy scrutiny by the public <cite class="ltx_cite ltx_citemacro_citep">(Ricci et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib15" title="">2011</a>)</cite>. These systems are extensively utilized across various domains such as e-commerce, social media, and content streaming, making their impact on daily life significant. Consequently, concerns about privacy, bias, and transparency have become more pronounced <cite class="ltx_cite ltx_citemacro_citep">(Pu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib14" title="">2012</a>)</cite>. Oftentimes, recommender systems are even distrusted, with users and representatives being wary of potential manipulation being performed by the system to nudge them into certain beliefs or behaviors <cite class="ltx_cite ltx_citemacro_citep">(Stray et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib17" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">One way to address such suspicions is through the use of explainable artificial intelligence (XAI). By allowing users (and lawmakers alike) to gain insights into how specific recommendations came to be, we can enable them to understand the system better, leading to more trust in its efficacy and less suspicion of foul play <cite class="ltx_cite ltx_citemacro_citep">(Arrieta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib5" title="">2020</a>)</cite>. Explainable AI can be critical for gaining user trust, as well as compliance with regulations such as the GDPR and the EU AI Act <cite class="ltx_cite ltx_citemacro_citep">(Commission, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib9" title="">2021</a>; European Parliament and Council of the European Union, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib11" title="">[n. d.]</a>; Guidotti et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib12" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">While considerable research has been done on using explainable AI to aid system developers and users, further prominent stakeholders (e.g., advertising companies, item providers, lawmakers) should not be ignored. This need for a multi-stakeholder approach requires a nuanced approach, as it makes recommending and explaining items more complex <cite class="ltx_cite ltx_citemacro_citep">(Abdollahpouri and Burke, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib3" title="">2019</a>; Abdollahpouri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib2" title="">2020</a>; Abdollahpouri and Burke, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib4" title="">2021</a>; Bauer and Zangerle, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib6" title="">2019</a>)</cite>. All stakeholders want recommendations and explanations optimized for their needs, but they often have conflicting interests. This balancing act becomes even more complex in high-risk domains, such as recruitment. Job recommender systems (JRSs), which match job seekers with potential employment opportunities, can have a considerable impact on individuals’ lives <cite class="ltx_cite ltx_citemacro_citep">(De Ruijt and Bhulai, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib10" title="">2021</a>)</cite>. Job recommender systems generally have three main stakeholders, all of which fall under the three main recommender system stakeholder types identified by <cite class="ltx_cite ltx_citemacro_citet">Abdollahpouri et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib2" title="">2020</a>)</cite>: <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">candidates</span> - the people looking for a job (i.e., consumers); <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">companies</span> - businesses offering positions of employment (i.e., providers); and <span class="ltx_text ltx_font_italic" id="S1.p3.1.3">recruiters</span> - people whose job it is to match candidates and vacancies (i.e., system). Each of these stakeholders has different needs and priorities, making a multi-stakeholder approach to generating explanations crucial <cite class="ltx_cite ltx_citemacro_citep">(Abdollahpouri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib2" title="">2020</a>; De Ruijt and Bhulai, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib10" title="">2021</a>)</cite>. For example, candidates need to trust the system and understand why a job is suitable for them before making such an impactful decision <cite class="ltx_cite ltx_citemacro_citep">(Schellingerhout et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib16" title="">2023</a>)</cite>. Proper explanations can also help mitigate biases and ensure fair treatment of all candidates, as it enables the system to be scrutinized. Furthermore, recruiters can use explanations to understand why certain candidates are recommended, allowing them to focus more efficiently on promising matches. Companies, on the other hand, can be enabled to quickly find the most relevant candidate from a large pool of options, increasing their productivity.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">This leads us to formulate the following research question for my PhD project: <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">How can we create an explainable, multi-stakeholder job recommender system that supports targeted explanations for different stakeholders?</span></p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To assist in answering this research question, we consider the following sub-questions:</p>
<dl class="ltx_description" id="S1.I1">
<dt class="ltx_item" id="S1.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix1.1.1.1">SQ1:: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">What are the stakeholder-specific explanations requirements and preferences of candidates, recruiters, and companies respectively?</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix2.1.1.1">SQ2:: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">How can we design an explainable multi-stakeholder job recommender system that outperforms state-of-the-art systems in user- and provider-side performance metrics?</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix3.1.1.1">SQ3:: </span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">To what extent does the inclusion of explainability into a real-world job recommender system improve its perceived usefulness, transparency, and trust?</p>
</div>
</dd>
</dl>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In the rest of this paper, we first summarize the research conducted so far. Then, we set out multiple directions for future work.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Completed research</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The research conducted during the PhD project up until this point has focused on SQ1 (finding stakeholder requirements and preferences) and SQ2 (building an explainable multi-stakeholder job recommender system). In this section, we describe the specific experiments we have performed so far.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Stakeholder preferences (SQ1)</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">To get an initial indication of the explanation preferences of the three main stakeholder types, we conducted a small-scale user study wherein we interviewed 6 participants while we exposed them to different examples of possible explanation types <cite class="ltx_cite ltx_citemacro_citep">(Schellingerhout et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib16" title="">2023</a>)</cite>. We used this user study as a starting point to get an indication of what explanation types were most promising to explore in more depth in future work. While we used a relatively small sample size, this allowed us to spend a considerable amount of time with each participant (around 1 hour per person), which enabled us to have the participants co-design the explanation types.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">We found considerable preference differences, both <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">between</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">within</em> stakeholder types. Candidates and recruiters strongly preferred textual explanations over visual explanations, mentioning that those were easier to grasp and had a more ‘personal’ feel to them. Company representatives initially gravitated towards the textual explanations too, as those were easiest to understand at first. However, they indicated a preference towards visual, graph-based explanations once they had spent some time trying to grasp those. Once they understood how the graph-based explanations should be interpreted, they mentioned how such visualizations allowed them to get a comprehensive overview of the complex relations in the data at a glance. This difference between stakeholders could largely be attributed to the fact that company representatives tended to have more experience with working with charts and graphs as part of their day-to-day job.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">However, there was significant disagreement between members of each stakeholder type as well. For example, recruiters disagreed on how comprehensive the explanations should be - either preferring long texts allowing them to provide sufficient detail to their clients when trying to convince them of potential matches, or preferring more limited explanations to offer them an initial indication of suitability, after which they could use their expertise to come to a more honed-in decision. To allow users to cater the explanation environment to their personal preferences and needs, we determined that interactive interfaces are crucial for job recommendation, as those allow individuals to access the data they find important, while not getting overwhelmed by information they do not consider useful.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Mock-up System Experiment (SQ1)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">After having co-designed the different explanation types, we created a prototypical explanation environment wherein users could browse multiple recommended items and their accompanying explanations (<a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#S2.F1" title="In 2.2. Mock-up System Experiment (SQ1) ‣ 2. Completed research ‣ Explainable Multi-Stakeholder Job Recommender Systems"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>). We tested this explanation environment with 30 participants in total; 10 of each stakeholder type. When interacting with the environment, both subjective (perceived usefulness, trust, and transparency) and objective (correctness and efficiency) metrics were collected. Participants were tasked to select what they considered to be the best option from the list of items twice - once after having seen real explanations generated by a graph neural network, and once after having seen random explanations (they were shown the different explanations in random order). Due to the nature of the data available to us at the time, this system did not allow users to get recommendations for their personal CV or vacancy, but rather had them read a pre-selected CV or vacancy before seeing the recommended items, after which they were instructed to decide as if they were the person/company whose CV/vacancy they just read.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="604" id="S2.F1.g1" src="x1.png" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The interface of the online environment with which the participants interacted. In this screenshot, all explanations are enabled. These can individually be toggled based on the user’s preference. The web environment uses exclusively Dutch text, as the interviewees were all native Dutch speakers. The environment consists of the following components: (1) the list of recommended items, which were presented in a randomized order (i.e., the top item was not necessarily the best match); (2) the textual explanation; (3) the bar chart explanation; (4) the graph-based explanation. This example shows a <span class="ltx_text ltx_font_italic" id="S2.F1.2.1">real</span> explanation.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">We found that preferences largely stayed the same, with candidates and recruiters strongly gravitating towards the textual explanations, and company representatives having a more diverse range of preferences. However, regardless of which explanation type the participants preferred, the difference in metrics between the random and real explanations was very limited. I.e., whether a user was shown a ‘nonsensical’ random explanation or a genuine explanation, their opinion of the system barely changed. While the subjective metrics trended upward with the real explanations, this trend was not statistically significant. Correctness, on the other hand, even went <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1">down</em> when participants were using real explanations to come to a decision. One contributing factor to this lack of difference is that the most commonly preferred explanation type, text, lends itself quite poorly to indicating minor differences between examples (i.e., it is hard for users to spot discrepancies in phrasing between explanations, e.g., ‘somewhat’ instead of ‘strongly’). Furthermore, the decrease in correctness indicates that users do not actively engage with the explanations and instead apply their own reasoning to the situation. Even if the system gives a specific argument for why an item is a good match, users often come up with widely different reasons for their decision, even when agreeing with the model. This lack of engagement leads to a slight benefit for the random explanations, as those are less likely to create ‘friction’, allowing participants to always apply their own reasoning to their decision without feeling like they disagree with the model.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Based on these findings, we determine that we should instead provide <em class="ltx_emph ltx_font_italic" id="S2.SS2.p3.1.1">decision-support</em> to the users of a job recommender system. While most XAI research focuses on persuasive explanations, we find that trying to persuade the stakeholders of the model’s correctness is futile, as they will apply their domain expertise to the decision-making process regardless. As such, it is better to support them in this process, rather than trying to steer them in a certain direction. This also addresses another concern in job recommendation: ground truth values are often generated manually by human recruiters - as a result, they are not <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">objective</span> truths. Attempting to force users to agree with the system can therefore be counterproductive, as their decision could, in theory, be preferable over the one determined to be ‘correct’. Furthermore, during the experiment, multiple participants indicated wanting a clear, direct relation between the explanation and the ‘source material’ (i.e., the CV or vacancy). If the explanation contained information that was not (directly) present in the CV/vacancy (e.g., work experience stored in the data, but not written in the CV), participants tended to get confused, wondering where that new information came from. Therefore, we conclude that the arguments used by the decision-support system should be clearly grounded in the source material.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Explainable Graph Neural Network (SQ2)</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The aforementioned papers made use of a rather low-quality, but publicly available dataset. As a result, the performance of the recommender system, and as a consequence its explanations, had significant room for improvement. To address this shortcoming, we collaborated with a large, international recruitment agency in order to gain access to a high-quality, manually labeled, proprietary job recommendation dataset. To determine the efficacy of graph neural networks (GNN) on this dataset, we implemented a novel explainable GNN, the <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">Occupational Knowledge-based Recommender using Attention</span> (OKRA). We then compared OKRA to multiple state-of-the-art job recommendation models; both text-only and other graph-based models.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Our experiments showed that graph-based models strongly outperformed text-only models. Considering the majority of JRS research focuses primarily on text-based solutions, this finding could have considerable consequences on the field. While most research focuses on utilizing transformer-based models to compare CV and vacancy texts to find matches, our findings indicate that this leaves a significant amount of predictive power unused. Compared to state-of-the-art graph-based models, OKRA performed significantly better due to its ability to make stakeholder-specific decisions (as candidates and companies are not necessarily always in agreement), but at the cost of increased training time. Additionally, we found that most state-of-the-art models are slightly biased against both rural candidates and companies, indicating a need for the consideration of regional fairness in the field of job recommendation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">While OKRA is inherently explainable, the focus of this paper was on its recommendation performance rather than its explainability. Due to the architecture we used, OKRA is able to generate multiple explanations for a single prediction, meaning it can separate ‘positive’ and ‘negative’ contributions to a decision. While this theoretically lends OKRA’s explanations to a decision support system, we did not evaluate the model’s explainability component, leaving it for future work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Future work</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">During the rest of the project, we will primarily focus on improving the model and its explanations so that they conform to the stakeholders’ demands as much as possible. In the rest of this section, we set out multiple avenues for future research related to each sub-question.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>SQ1: Designing desirable explanations</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Based on the research we have conducted for sub-question 1 so far, we already have a general understanding of the preferences and needs of the different stakeholders. However, we have also identified multiple shortcomings with our previous approach that need to be addressed. To conclusively answer SQ1, we will focus on alleviating these shortcomings in future work, so that we can present a concrete implementation that adheres to all stakeholder requirements.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Improving explanation coherence</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">One of the main difficulties faced by the stakeholders in our mock-up experiment was that they struggled to connect the explanations to the source material. Considering the explanations were generated using both the source material <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">and</em> structured data, parts of the explanation based on the structured data did not necessarily align with what was shown in the CV or vacancy (e.g., it highlighted a skill that someone did not list on their CV). This led to confusion and made it more difficult for participants to understand the explanations. To address this shortcoming, we will attempt to integrate the CVs/vacancies and the structured data so that their contents are more aligned.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">One possible approach is to make use of automated knowledge graph construction from text <cite class="ltx_cite ltx_citemacro_citep">(Bosselut et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib8" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib18" title="">2021</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib19" title="">2019</a>)</cite>. For this, we would create a machine learning pipeline capable of automatically finding entities and their underlying relations in a given text. The graphs generated from this text could then be linked to the rest of the structured data, so that for every piece of structured data, there exists a link to a word or phrase in the source material. This would not only improve the coherence of the textual explanations but also allow the model to more directly integrate the information stored in the CV/vacancy into the recommendation process. However, this approach would require some type of training data, as zero-shot learning is likely to be insufficiently integrated into the existing ontology. Alternatively, we could apply untrained clustering algorithms to cluster the embeddings of different tokens, so that different tokens referring to the same concept can be coalesced. While this approach does not require training data, it is presumed to be less reliable, as mismatches between the structured and unstructured data could still occur. When only using textual data to generate the knowledge graph, it is certain that all the information in the graph is also stored in the text, however, when combining structured and unstructured data, even with clustering, some tokens/concepts come up in one data type, but not the other (e.g., work experience that is stored in the structured data, but not mentioned in the CV).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Clarifying textual explanations</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Furthermore, we found that substantive differences in attention weights can lead to rather minute differences in the textual explanations. As mentioned above, this made it difficult for users to differentiate between textual explanations with different contents correctly. While an attention weight of 0.7 instead of 0.2 stands out immediately, properly communicating this difference without referring to the exact values (since referring to numeric values directly was indicated as complicated and overwhelming by stakeholders) can be difficult for LLMs. For example, while describing these values as ‘moderately high’ and ‘fairly low’ is correct, such formulations do not stand out immediately, which causes users to easily gloss over them. To solve this, we intend to fine-tune an LLM, such as GPT-4, on a collection of explanations that have been manually verified as ‘clear’ or ‘understandable’. To determine what constitutes a clear explanation, we will conduct an experiment wherein participants will be asked to pick a preferred option between two versions of the same explanation, but with the value of one textual feature (such as word count, word complexity, level of formality, etc.) altered. By repeating this match-up multiple times, each time with different features being changed, we can finally determine a user’s preferred explanation type (e.g., high word count, low word complexity, low formality, etc.) Given a sufficient sample, we can then determine what type of explanation is preferred by the end users.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>SQ2: Improving model performance</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Exploiting linked data</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">One major benefit of using knowledge graphs is that they are capable of easily combining data from multiple sources <cite class="ltx_cite ltx_citemacro_citep">(Bizer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib7" title="">2011</a>)</cite>. While regular databases can be difficult to combine, primarily due to differences in (naming) conventions and higher-order relations being hard to implement using relational algebra, knowledge graphs easily allow data from multiple sources to be combined. One major aspect of job recommendation where this can make a large difference, is in the initial creation of node embeddings. Currently, all node embeddings used by OKRA were initialized randomly, except for those based on CVs or vacancies. The CV and vacancy nodes had a starting embedding based on the text embedding value created by a transformer-based model. By incorporating linked data, node types that currently do not have any text related to them, such as function titles and codes (e.g., <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">international standard classification of occupation</span>,<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://isco.ilo.org/en/" title="">https://isco.ilo.org/en/</a></span></span></span> or ISCO, codes), can be linked to their respective nodes in existing graphs like that of WikiData<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.wikidata.org/" title="">https://www.wikidata.org/</a></span></span></span> and DBpedia.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.dbpedia.org/" title="">https://www.dbpedia.org/</a></span></span></span> These linked data sources often have extensive descriptions of the functions/codes, allowing the model to use those descriptions to create starting embeddings. Furthermore, these data sources contain significantly more data than most domain-specific datasets, using which the knowledge graph can be made more exhaustive, enabling more high-level relations in the data to be identified.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>SQ3: Evaluating the system as a whole</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Evaluating in a real-world context</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">The explanation evaluations we have done so far have all been conducted using non-personal data. The CVs and vacancies used in our mock-up system were examples using which the participants had to role-play. While this was sufficient for the scope of this study, using such data can lead to some bias, as it is more difficult to make a decision on behalf of someone else than for oneself. Logically, we aim to address this in future work by creating a live version of the environment, wherein users will be able to submit their own vacancy/CV and personal data, so that the model can generate a personalized list of recommended items for them. This will enable the users to go through a more natural decision-making process, as they do not have to bear the additional cognitive load of having to remember a CV/vacancy that is not theirs. As a result, we will be able to evaluate the system in a more holistic manner using this live version of the environment; users can interact with the system as they would in a real-world scenario as well, making it possible for us to determine to what extent the users interact with the explanations (rather than simply reading the recommended vacancies/CVs like they would with a non-explainable environment). Furthermore, by having a live, working system, we will be able to more easily experiment with a large sample size, as participants will be able to interact with the system independently on their own time (which was not possible with the mock-up version, as there they needed to be supervised).</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">We will again need to collaborate with a recruitment agency to conduct such a live evaluation, which may cause difficulties (or even be impossible within our time span). This leads to another avenue of future research: determining to what extent our findings generalize to other domains. Although our project focuses specifically on JRSs, many overlaps exist between the recruitment domain and domains such as dating. While the stakeholders’ exact requirements are likely to differ, we do expect general similarities to exist between the two groups. As a result, it would be interesting to additionally evaluate our findings in a different domain that is similar in nature, to assess the generalizability of our findings.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>List-wise explanations</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Lastly, a significant challenge we anticipate when shifting towards decision-support-focused explanations is having to present explanations related to a list of recommendations. When users are shown the pros and cons of different recommended items, understanding why one item is ranked higher than another can be complex. While providing pair-wise comparisons is relatively straightforward, offering clear and comprehensive explanations in a list-wise context is much more challenging <cite class="ltx_cite ltx_citemacro_citep">(Heuss et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00654v1#bib.bib13" title="">2024</a>)</cite>. This difficulty arises from the need to show the intricate relationships and trade-offs among multiple items simultaneously. To address this issue, we will explore ways to effectively communicate the advantages and disadvantages of multiple items in future research. This could involve developing new comparative visualization techniques, interactive interfaces, or summary metrics that can help users grasp the overall ranking rationale and make informed decisions based on the recommendations provided.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
I would like to thank my supervisors, Francesco Barile and Nava Tintarev, for their guidance and support during my PhD research.

</div>
</section>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdollahpouri et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Himan Abdollahpouri, Gediminas Adomavicius, Robin Burke, Ido Guy, Dietmar Jannach, Toshihiro Kamishima, Jan Krasnodebski, and Luiz Pizzato. 2020.

</span>
<span class="ltx_bibblock">Multistakeholder recommendation: Survey and research directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">User Modeling and User-Adapted Interaction</em> 30, 1 (2020), 127–158.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdollahpouri and Burke (2019)</span>
<span class="ltx_bibblock">
Himan Abdollahpouri and Robin Burke. 2019.

</span>
<span class="ltx_bibblock">Multi-stakeholder recommendation and its connection to multi-sided fairness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:1907.13158</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdollahpouri and Burke (2021)</span>
<span class="ltx_bibblock">
Himan Abdollahpouri and Robin Burke. 2021.

</span>
<span class="ltx_bibblock">Multistakeholder recommender systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Recommender systems handbook</em>. Springer, 647–677.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arrieta et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alejandro Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, et al<span class="ltx_text" id="bib.bib5.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">Information Fusion</em> 58 (2020), 82–115.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bauer and Zangerle (2019)</span>
<span class="ltx_bibblock">
Christine Bauer and Eva Zangerle. 2019.

</span>
<span class="ltx_bibblock">Leveraging multi-method evaluation for multi-stakeholder settings. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Impact of Recommender Systems 2019. Proceedings of the 1st Workshop on the Impact of Recommender Systems co-located with 13th ACM Conference on Recommender Systems (ACM RecSys 2019)/Shalom, Oren Sar; Jannach, Dietmar; Guy, Ido</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bizer et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Christian Bizer, Tom Heath, and Tim Berners-Lee. 2011.

</span>
<span class="ltx_bibblock">Linked data: The story so far.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Semantic services, interoperability and web applications: emerging concepts</em>. IGI global, 205–227.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bosselut et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">COMET: Commonsense transformers for automatic knowledge graph construction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">arXiv preprint arXiv:1906.05317</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Commission (2021)</span>
<span class="ltx_bibblock">
European Commission. 2021.

</span>
<span class="ltx_bibblock">Proposal for a Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A52021PC0206" title="">https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A52021PC0206</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Ruijt and Bhulai (2021)</span>
<span class="ltx_bibblock">
Corné De Ruijt and Sandjai Bhulai. 2021.

</span>
<span class="ltx_bibblock">Job recommender systems: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2111.13576</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Parliament and Council of the European Union ([n. d.])</span>
<span class="ltx_bibblock">
European Parliament and Council of the European Union. [n. d.].

</span>
<span class="ltx_bibblock">Regulation (EU) 2016/679 of the European Parliament and of the Council.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://data.europa.eu/eli/reg/2016/679/oj" title="">https://data.europa.eu/eli/reg/2016/679/oj</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guidotti et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi. 2018.

</span>
<span class="ltx_bibblock">A survey of methods for explaining black box models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">ACM computing surveys (CSUR)</em> 51, 5 (2018), 1–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heuss et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Maria Heuss, Maarten de Rijke, and Avishek Anand. 2024.

</span>
<span class="ltx_bibblock">RankingSHAP-Listwise Feature Attribution Explanations for Ranking Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">CoRR</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pu et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Pearl Pu, Li Chen, and Rong Hu. 2012.

</span>
<span class="ltx_bibblock">Evaluating recommender systems from the user’s perspective: Survey of the state of the art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">User Modeling and User-Adapted Interaction</em> 22, 4-5 (2012), 317–355.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ricci et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B Kantor. 2011.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Introduction to recommender systems handbook</em>. Vol. 1.

</span>
<span class="ltx_bibblock">Springer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schellingerhout et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Roan Schellingerhout, Francesco Barile, and Nava Tintarev. 2023.

</span>
<span class="ltx_bibblock">A Co-design Study for Multi-stakeholder Job Recommender System Explanations. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">World Conference on Explainable Artificial Intelligence</em>. Springer, 597–620.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stray et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jonathan Stray, Alon Halevy, Parisa Assar, Dylan Hadfield-Menell, Craig Boutilier, Amar Ashar, Chloe Bakalar, Lex Beattie, Michael Ekstrand, Claire Leibowicz, et al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Building human values into recommender systems: An interdisciplinary synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">ACM Transactions on Recommender Systems</em> 2, 3 (2024), 1–57.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wenguang Wang, Yonglin Xu, Chunhui Du, Yunwen Chen, Yijie Wang, and Hui Wen. 2021.

</span>
<span class="ltx_bibblock">Data set and evaluation of automated construction of financial knowledge graph.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Data Intelligence</em> 3, 3 (2021), 418–443.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xindong Wu, Jia Wu, Xiaoyi Fu, Jiachen Li, Peng Zhou, and Xu Jiang. 2019.

</span>
<span class="ltx_bibblock">Automatic knowledge graph construction: A report on the 2019 icdm/icbk contest. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">2019 IEEE International Conference on Data Mining (ICDM)</em>. IEEE, 1540–1545.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  1 13:08:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
