<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  Large Language Models as Test Case Generators: Performance Evaluation and Enhancement
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Kefan Li
    <br class="ltx_break"/>
    Beihang University
    <br class="ltx_break"/>
    <br class="ltx_break"/>
    <span class="ltx_ERROR undefined" id="id1.1.id1">
     \And
    </span>
    Yuan Yuan
    <br class="ltx_break"/>
    Beihang University
    <br class="ltx_break"/>
    <br class="ltx_break"/>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id2.id1">
   Code generation with Large Language Models (LLMs) has been extensively studied and achieved remarkable progress.
As a complementary aspect to code generation, test case generation is of crucial importance
in ensuring the quality and reliability of code. However, using LLMs as test case generators has been much less explored.
Current research along this line primarily focuses on enhancing code generation with assistance from test cases
generated by LLMs, while the performance of LLMs in test case generation alone has not been comprehensively examined.
To bridge this gap, we conduct extensive experiments to study how well LLMs can generate high-quality test cases.
We find that as the problem difficulty increases, state-of-the-art LLMs struggle to generate correct test cases, largely
due to their inherent limitations in computation and reasoning. To mitigate this issue, we further propose
a multi-agent framework called
   <em class="ltx_emph ltx_font_italic" id="id2.id1.1">
    TestChain
   </em>
   that decouples the generation of test inputs and test outputs.
Notably, TestChain uses a ReAct format conversation chain for LLMs to interact with a Python interpreter in order to
provide more accurate test outputs. Our results indicate that TestChain outperforms the baseline by a large margin.
Particularly, in terms of the accuracy of test cases,
TestChain using GPT-4 as the backbone achieves a 13.84% improvement over the baseline on the LeetCode-hard dataset.
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Large Language Models (LLMs) have shown significant capabilities in code generation, paving the way
for transformative changes in software development. Among these LLMs, there are those specifically designed for coding, such as Codex
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2021
     </a>
     )
    </cite>
    ,
CodeGen
    <cite class="ltx_cite ltx_citemacro_cite">
     Nijkamp et al. (
     <a class="ltx_ref" href="#bib.bib15" title="">
      2022
     </a>
     )
    </cite>
    and CodeLlama
    <cite class="ltx_cite ltx_citemacro_cite">
     Roziere et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    , as well as general-purpose models like GPT-3.5 and GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     Achiam et al. (
     <a class="ltx_ref" href="#bib.bib1" title="">
      2023
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Test case generation usually refers to the automatic process of creating test cases.
It is essential for ensuring the quality and reliability of code, serving as a complementary aspect to code generation.
Note that the concept of test cases spans various levels (unit testing, system testing, etc.) and scopes (functional testing, security testing, etc.) within the software testing process.
Following the recent related studies
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2022
     </a>
     ); Huang et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023a
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib9" title="">
      b
     </a>
     ); Shinn et al. (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2023
     </a>
     )
    </cite>
    ,
we consider function-level unit test cases in this paper, where
    <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">
     a test case
    </em>
    refers to a pair of input and expected output for the function defined in the given context.
Figure
    <a class="ltx_ref" href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    describes the formulation of test case generation.
   </p>
  </div>
  <figure class="ltx_figure" id="S1.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="251" id="S1.F1.1.g1" src="/html/2404.13340/assets/x1.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Formulation of test case generation in this paper, where an assertion encapsulates a test case.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Because of breakthroughs in code generation achieved by LLMs, it is intuitive to apply these models to test case generation as well.
This type of research has only recently emerged as a field of interest.
CodeT
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2022
     </a>
     )
    </cite>
    uses the test cases generated by LLMs
to select a single solution from multiple code samples.
In CodeCoT
    <cite class="ltx_cite ltx_citemacro_cite">
     Huang et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023a
     </a>
     )
    </cite>
    , Reflexion
    <cite class="ltx_cite ltx_citemacro_cite">
     Shinn et al. (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2023
     </a>
     )
    </cite>
    and AgentCoder
    <cite class="ltx_cite ltx_citemacro_cite">
     Huang et al. (
     <a class="ltx_ref" href="#bib.bib9" title="">
      2023b
     </a>
     )
    </cite>
    ,
the execution of programs on test cases generated by LLMs provides feedback information for self-correction.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    However, most of current research in this direction primarily focuses on utilizing test cases to benefit code generation, with little attention to the quality of test cases generated by LLMs.
If the test cases generated by LLMs contain many errors, the effectiveness of these approaches will be significantly diminished.
A great challenge in test case generation is how to accurately map each test input to its corresponding output, often requiring precise mathematical calculations and complex logical reasoning.
Nonetheless, existing studies
    <cite class="ltx_cite ltx_citemacro_cite">
     Frieder et al. (
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023
     </a>
     ); Liu et al. (
     <a class="ltx_ref" href="#bib.bib13" title="">
      2023
     </a>
     )
    </cite>
    have shown that even the most capable LLMs are not proficient in this regard.
Thus, it naturally leads us to pose the following question:
    <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">
     Can LLMs really generate high-quality test cases?
    </em>
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    In this paper, we first conduct extensive experiments to explore this question, where the quality of test cases is evaluated by comprehensive metrics including accuracy, line coverage and our proposed
new metric called
    <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">
     Code-with-Bugs
    </em>
    concerning the strength of tests.
We examine four notable LLMs comprising both open and closed-source frameworks. Specifically, we include two open-source models: StarChat
    <cite class="ltx_cite ltx_citemacro_cite">
     Li et al. (
     <a class="ltx_ref" href="#bib.bib11" title="">
      2023
     </a>
     )
    </cite>
    and CodeLlama
    <cite class="ltx_cite ltx_citemacro_cite">
     Roziere et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    ,
alongside two close-source models: GPT-3.5 and GPT-4.
We choose a relatively easy dataset HumanEval
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2021
     </a>
     )
    </cite>
    and a more challenging dataset LeetCode-hard
    <cite class="ltx_cite ltx_citemacro_cite">
     Shinn et al. (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2023
     </a>
     )
    </cite>
    for evaluation.
Experiments show that StarChat and CodeLlama struggle to generate correct test cases, while GPT-3.5 and GPT-4 perform well on HumanEval but tend to produce many incorrect test cases on LeetCode-hard.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    Based on the insights from the empirical evaluation, we further propose a multi-agent framework called
    <em class="ltx_emph ltx_font_italic" id="S1.p6.1.1">
     TestChain
    </em>
    ,
in order to enhance the performance of LLMs in test case generation.
In TestChain, one agent called
    <em class="ltx_emph ltx_font_italic" id="S1.p6.1.2">
     Designer agent
    </em>
    is responsible for generating diverse test inputs, while another agent called
    <em class="ltx_emph ltx_font_italic" id="S1.p6.1.3">
     Calculator agent
    </em>
    is responsible for correctly mapping the inputs to outputs.
Notably, Calculator agent uses a ReAct
    <cite class="ltx_cite ltx_citemacro_cite">
     Yao et al. (
     <a class="ltx_ref" href="#bib.bib27" title="">
      2022
     </a>
     )
    </cite>
    format conversation chain interaction with a Python interpreter for executing code,
which can significantly reduce the complexity and inaccuracy in the input-output mapping.
Experiments demonstrate that our approach significantly outperforms the baseline in terms of all metrics.
In summary, the main contributions of this paper are twofold:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We conduct a comprehensive evaluation of LLMs in test case generation. We find that the performance of LLMs
drops sharply in handling harder problems, where the generated test cases may contain many errors.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We propose
       <em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.1">
        TestChain
       </em>
       , which significantly enhances the performance of LLMs in generating test cases.
TestChain achieves an accuracy of
       <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.2">
        71.79%
       </span>
       with GPT-4 on the LeetCode-hard dataset, while the Test Agent (1-shot) baseline method only gets an accuracy of
       <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.3">
        57.95%
       </span>
       .
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     LLMs for Test Case Generation.
    </span>
    Recently, there has been a growing trend of utilizing LLMs for test case generation.
    <cite class="ltx_cite ltx_citemacro_citet">
     Tufano et al. (
     <a class="ltx_ref" href="#bib.bib21" title="">
      2020
     </a>
     )
    </cite>
    and
    <cite class="ltx_cite ltx_citemacro_citet">
     Li et al. (
     <a class="ltx_ref" href="#bib.bib12" title="">
      2022
     </a>
     )
    </cite>
    utilize pre-trained language models fine-tuned on labeled data to generate test cases. CodeT
    <cite class="ltx_cite ltx_citemacro_cite">
     Chen et al. (
     <a class="ltx_ref" href="#bib.bib4" title="">
      2022
     </a>
     )
    </cite>
    directly generates test cases with the powerful model Codex using zero-shot prompts. CodeCoT
    <cite class="ltx_cite ltx_citemacro_cite">
     Huang et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023a
     </a>
     )
    </cite>
    employs a Chain of Thought prompt to generate both solutions and test cases for programming tasks. Reflexion and AgentCoder employ LLMs to generate test cases and execute them on programs to produce feedback information for self-correction.
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    Automatically generating test cases with LLMs can also be used in real-world software development.
    <cite class="ltx_cite ltx_citemacro_citet">
     Zhang et al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023
     </a>
     )
    </cite>
    utilizes GPT-4 to generate security tests to find vulnerabilities.
    <cite class="ltx_cite ltx_citemacro_citet">
     Schäfer et al. (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    employs LLMs to assist in automatic software testing by generating unit tests.
    <cite class="ltx_cite ltx_citemacro_citet">
     Kang et al. (
     <a class="ltx_ref" href="#bib.bib10" title="">
      2023
     </a>
     )
    </cite>
    utilizes LLMs to generate test cases to reproduce general bugs in software.
   </p>
  </div>
  <div class="ltx_para" id="S2.p3">
   <p class="ltx_p" id="S2.p3.1">
    <span class="ltx_text ltx_font_bold" id="S2.p3.1.1">
     Multi-Agent Structure with LLMs.
    </span>
    Utilizing multiple agents driven by LLMs to collaborate in solving tasks has emerged as a popular approach. Self-Collaboration
    <cite class="ltx_cite ltx_citemacro_cite">
     Dong et al. (
     <a class="ltx_ref" href="#bib.bib6" title="">
      2023
     </a>
     )
    </cite>
    establishes a team comprising analyst, coder, and tester roles for software development. INTERVENOR
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib23" title="">
      2023b
     </a>
     )
    </cite>
    employs a learner agent for generating programs and a teacher agent for suggesting repairs.
AgentCoder
    <cite class="ltx_cite ltx_citemacro_cite">
     Huang et al. (
     <a class="ltx_ref" href="#bib.bib9" title="">
      2023b
     </a>
     )
    </cite>
    introduces a programmer agent, a test designer agent, and a test executor agent for code generation and refinement, with the programmer agent and the test designer agent powered by LLMs. AutoGen
    <cite class="ltx_cite ltx_citemacro_cite">
     Wu et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023a
     </a>
     )
    </cite>
    provides a straightforward method for creating multi-agent applications with LLMs.
   </p>
  </div>
  <div class="ltx_para" id="S2.p4">
   <p class="ltx_p" id="S2.p4.1">
    <span class="ltx_text ltx_font_bold" id="S2.p4.1.1">
     LLMs Interaction with Tools.
    </span>
    By defining specific conversation formats, the generations of LLMs can be parsed into tool invocations, enabling interaction between LLMs and external tools.
ReAct
    <cite class="ltx_cite ltx_citemacro_cite">
     Yao et al. (
     <a class="ltx_ref" href="#bib.bib27" title="">
      2022
     </a>
     )
    </cite>
    employs a chain in “Thought/Action/Observation” format, allowing LLMs to provide tool types and return the execution results of these tools to LLMs.
Recently, an open-source project LangChain
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       https://python.langchain.com
      </span>
     </span>
    </span>
    allows the creation of LLM applications with ReAct or custom-defined conversation chains.
FunSearch
    <cite class="ltx_cite ltx_citemacro_cite">
     Romera-Paredes et al. (
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023
     </a>
     )
    </cite>
    utilizes Google’s PaLM 2
    <cite class="ltx_cite ltx_citemacro_cite">
     Anil et al. (
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023
     </a>
     )
    </cite>
    and customized evaluators to discover new solutions to the cap set problem.
HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Shen et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    employs GPT-4 and numerous expert models to solve AI tasks in a collaborative way.
WebGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Nakano et al. (
     <a class="ltx_ref" href="#bib.bib14" title="">
      2021
     </a>
     )
    </cite>
    allows LLMs to use a browser to assist in answering questions.
AutoGen can create a Python interpreter environment locally or via docker and interact with LLM agents.
Research based on AutoGen can create conversations to conduct hyperparameter Optimization
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023a
     </a>
     )
    </cite>
    and solve math problems
    <cite class="ltx_cite ltx_citemacro_cite">
     Wu et al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2023b
     </a>
     )
    </cite>
    .
   </p>
  </div>
  <figure class="ltx_table" id="S2.T1">
   <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
    <thead class="ltx_thead">
     <tr class="ltx_tr" id="S2.T1.1.1.1">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.1" rowspan="2">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">
        Model
       </span>
      </th>
      <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.2" rowspan="2">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">
        Method
       </span>
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S2.T1.1.1.1.3">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">
        HumanEval-no-exp
       </span>
      </th>
      <th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4">
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S2.T1.1.1.1.5">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1">
        LeetCode-no-exp
       </span>
      </th>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.2.2">
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.2.2.1">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.1.1">
        Accuracy
       </span>
       (%)
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.2.2.2">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.2.1">
        Line Cov
       </span>
       (%)
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.2.2.3">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.3.1">
        CwB
       </span>
       (%)
      </th>
      <th class="ltx_td ltx_th ltx_th_column" id="S2.T1.1.2.2.4">
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.2.2.5">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.5.1">
        Accuracy
       </span>
       (%)
      </th>
      <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.2.2.6">
       <span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.6.1">
        Line Cov
       </span>
       (%)
      </th>
     </tr>
    </thead>
    <tbody class="ltx_tbody">
     <tr class="ltx_tr" id="S2.T1.1.3.1">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.3.1.1" rowspan="2">
       <span class="ltx_text" id="S2.T1.1.3.1.1.1">
        StarChat
       </span>
      </th>
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.3.1.2">
       Test Agent (0-shot)
      </th>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.1.3">
       37.44
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.1.4">
       51.86
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.1.5">
       45.61
      </td>
      <td class="ltx_td ltx_border_t" id="S2.T1.1.3.1.6">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.1.7">
       9.74
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.1.8">
       31.10
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.4.2">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.4.2.1">
       Test Agent (1-shot)
      </th>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.4.2.2">
       41.10
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.4.2.3">
       52.27
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.4.2.4">
       45.46
      </td>
      <td class="ltx_td" id="S2.T1.1.4.2.5">
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.4.2.6">
       12.31
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.4.2.7">
       23.61
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.5.3">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.5.3.1" rowspan="2">
       <span class="ltx_text" id="S2.T1.1.5.3.1.1">
        CodeLlama
       </span>
      </th>
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.5.3.2">
       Test Agent (0-shot)
      </th>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.3.3">
       51.83
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.3.4">
       63.88
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.3.5">
       54.85
      </td>
      <td class="ltx_td ltx_border_t" id="S2.T1.1.5.3.6">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.3.7">
       23.08
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.3.8">
       58.61
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.6.4">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.6.4.1">
       Test Agent (1-shot)
      </th>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.6.4.2">
       67.07
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.6.4.3">
       71.71
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.6.4.4">
       63.23
      </td>
      <td class="ltx_td" id="S2.T1.1.6.4.5">
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.6.4.6">
       28.72
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.6.4.7">
       55.95
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.7.5">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.7.5.1" rowspan="2">
       <span class="ltx_text" id="S2.T1.1.7.5.1.1">
        GPT-3.5
       </span>
      </th>
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.7.5.2">
       Test Agent (0-shot)
      </th>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.7.5.3">
       65.98
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.7.5.4">
       71.60
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.7.5.5">
       69.60
      </td>
      <td class="ltx_td ltx_border_t" id="S2.T1.1.7.5.6">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.7.5.7">
       26.67
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.7.5.8">
       63.68
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.8.6">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.1.8.6.1">
       Test Agent (1-shot)
      </th>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.8.6.2">
       74.02
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.8.6.3">
       74.69
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.8.6.4">
       74.15
      </td>
      <td class="ltx_td" id="S2.T1.1.8.6.5">
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.8.6.6">
       38.97
      </td>
      <td class="ltx_td ltx_align_center" id="S2.T1.1.8.6.7">
       73.66
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.9.7">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S2.T1.1.9.7.1" rowspan="2">
       <span class="ltx_text" id="S2.T1.1.9.7.1.1">
        GPT-4
       </span>
      </th>
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.9.7.2">
       Test Agent (0-shot)
      </th>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.9.7.3">
       84.02
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.9.7.4">
       77.54
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.9.7.5">
       82.53
      </td>
      <td class="ltx_td ltx_border_t" id="S2.T1.1.9.7.6">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.9.7.7">
       58.97
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.9.7.8">
       88.83
      </td>
     </tr>
     <tr class="ltx_tr" id="S2.T1.1.10.8">
      <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.1.10.8.1">
       Test Agent (1-shot)
      </th>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.10.8.2">
       84.63
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.10.8.3">
       77.04
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.10.8.4">
       83.11
      </td>
      <td class="ltx_td ltx_border_bb" id="S2.T1.1.10.8.5">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.10.8.6">
       57.95
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.10.8.7">
       88.47
      </td>
     </tr>
    </tbody>
   </table>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_table">
     Table 1:
    </span>
    Evaluation results of the Tester Agent with 0-shot and 1-shot prompt.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Can LLMs Really Generate High-Quality Test Cases?
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    To thoroughly investigate the capabilities of LLMs in generating test cases, we select four LLMs and two datasets for experimentation.
We employ three metrics to evaluate the quality of test cases from different aspects, including correctness and strength.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">
      Models.
     </span>
     The four typical LLMs include two open-source models:
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2">
      StarChat-beta
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.3">
      CodeLlama-13b-Instruct-hf
     </em>
     , and two closed-source models:
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.4">
      GPT-3.5-turbo-1106
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.5">
      GPT-4-turbo-preview
     </em>
     .
In the following, we refer to the four models by
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.6">
      StarChat
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.7">
      CodeLlama
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.8">
      GPT-3.5
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.9">
      GPT-4
     </em>
     in short.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Datasets.
     </span>
     We choose a popular Python programming dataset HumanEval that contains a canonical solution for each question.
Moreover, we choose a more challenging dataset LeetCode-hard that includes
     <math alttext="39" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1">
      <semantics id="S3.SS1.p2.1.m1.1a">
       <mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">
        39
       </mn>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b">
        <cn id="S3.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1">
         39
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">
        39
       </annotation>
      </semantics>
     </math>
     difficult Python programming tasks.
For some questions in the original LeetCode-hard, the canonical solution is either missing or faulty.
So we manually collect correct solutions for them and verify these solutions via the LeetCode website.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     We have also noticed that LLMs may plagiarize the test case examples included in the question prompt, so we remove all the examples from the prompts of the two datasets.
Examples of prompts with and without examples can be found in Appendix
     <a class="ltx_ref" href="#A1" title="Appendix A Dataset Examples ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       A
      </span>
     </a>
     .
In the following, we refer to the two datasets without examples by
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.1">
      HumanEval-no-exp
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.2">
      LeetCode-no-exp
     </em>
     .
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">
      Generation Setup.
     </span>
     For using LLMs as test case generators, we design a
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.2">
      system prompt
     </em>
     that sets the LLM as a Python tester and instructs it
to generate
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.3">
      basic
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.4">
      edge
     </em>
     , and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.5">
      large scale
     </em>
     test cases
     <cite class="ltx_cite ltx_citemacro_cite">
      Huang et al. (
      <a class="ltx_ref" href="#bib.bib9" title="">
       2023b
      </a>
      )
     </cite>
     for a given context.
We consider two paradigms for generation:
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.6">
      Test Agent
     </em>
     with
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.7">
      0-shot
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p4.1.8">
      1-shot
     </em>
     prompt.
For 1-shot generation, examples illustrating the test case generation are
prefixed to the user prompt.
Please refer to Figures
     <a class="ltx_ref" href="#A2.F9" title="Figure 9 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     –
     <a class="ltx_ref" href="#A2.F10" title="Figure 10 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     in Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     for the details of the prompts used in
our experiments.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.1">
     For each question, we remove duplicate and syntactically erroneous test cases. If more than five remain, we retain the first five for evaluation; otherwise, we keep all.
The HumanEval-no-exp dataset comprises 164 questions, with a maximum of 820 test cases. The LeetCode-no-exp dataset includes 39 questions, with a maximum of 195 test cases.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p6">
    <p class="ltx_p" id="S3.SS1.p6.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">
      Metrics.
     </span>
     A test case is considered to be correct if it can be passed by the canonical solution.
The accuracy measures the percentage of correct test cases among all those generated.
To comprehensively assess the quality of test cases for a question, line coverage (Line Cov) is used as another metric, measuring the proportion of lines accessed
when the test cases are executed on the canonical solution. For comparison, we calculate the average line coverage on the full dataset.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p7">
    <p class="ltx_p" id="S3.SS1.p7.1">
     Line coverage alone is recognized as insufficient for evaluating the strength of test cases
     <cite class="ltx_cite ltx_citemacro_cite">
      Andrews et al. (
      <a class="ltx_ref" href="#bib.bib2" title="">
       2006
      </a>
      )
     </cite>
     .
We also introduce a new metric called
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p7.1.1">
      Code-with-Bugs
     </em>
     pass rate (CwB in short).
To compute CwB for a set of test cases, we first randomly
generate a number of faulty programs (20 in our experiments) for the question.
Then, CwB is the proportion of programs that fail this test set.
We use the average CwB on the full dataset for comparison.
Note that to conduct a fair comparison, we utilize another model,
CodeGen-Mono 6B
     <cite class="ltx_cite ltx_citemacro_cite">
      Nijkamp et al. (
      <a class="ltx_ref" href="#bib.bib15" title="">
       2022
      </a>
      )
     </cite>
     , to generate faulty programs.
Additionally, CwB is only applied to HumanEval in our experiments, as the canonical test cases for verifying correctness are not available for LeetCode-hard.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p8">
    <p class="ltx_p" id="S3.SS1.p8.1">
     To further investigate why LLMs generate incorrect test cases, we analyze error types,
categorizing them into
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p8.1.1">
      Assertion Error
     </em>
     ,
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p8.1.2">
      Runtime Error
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S3.SS1.p8.1.3">
      Timeout Error
     </em>
     :
    </p>
    <ul class="ltx_itemize" id="S3.I1">
     <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i1.p1">
       <p class="ltx_p" id="S3.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">
         Assertion Error
        </span>
        : Indicates a mismatch between the execution output of the target function and the expected output.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i2.p1">
       <p class="ltx_p" id="S3.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">
         Runtime Error
        </span>
        : Indicates an internal error during function execution, often due to non-compliant input.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i3.p1">
       <p class="ltx_p" id="S3.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">
         Timeout Error
        </span>
        : Indicates that the execution of the target function exceeded the allotted time limit (1 second in our experiments).
       </p>
      </div>
     </li>
    </ul>
    <p class="ltx_p" id="S3.SS1.p8.2">
     We will provide statistics on the occurrence of each error type.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p9">
    <p class="ltx_p" id="S3.SS1.p9.6">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p9.6.1">
      Parameters.
     </span>
     For all the models, we use a temperature of
     <math alttext="T=0.2" class="ltx_Math" display="inline" id="S3.SS1.p9.1.m1.1">
      <semantics id="S3.SS1.p9.1.m1.1a">
       <mrow id="S3.SS1.p9.1.m1.1.1" xref="S3.SS1.p9.1.m1.1.1.cmml">
        <mi id="S3.SS1.p9.1.m1.1.1.2" xref="S3.SS1.p9.1.m1.1.1.2.cmml">
         T
        </mi>
        <mo id="S3.SS1.p9.1.m1.1.1.1" xref="S3.SS1.p9.1.m1.1.1.1.cmml">
         =
        </mo>
        <mn id="S3.SS1.p9.1.m1.1.1.3" xref="S3.SS1.p9.1.m1.1.1.3.cmml">
         0.2
        </mn>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p9.1.m1.1b">
        <apply id="S3.SS1.p9.1.m1.1.1.cmml" xref="S3.SS1.p9.1.m1.1.1">
         <eq id="S3.SS1.p9.1.m1.1.1.1.cmml" xref="S3.SS1.p9.1.m1.1.1.1">
         </eq>
         <ci id="S3.SS1.p9.1.m1.1.1.2.cmml" xref="S3.SS1.p9.1.m1.1.1.2">
          𝑇
         </ci>
         <cn id="S3.SS1.p9.1.m1.1.1.3.cmml" type="float" xref="S3.SS1.p9.1.m1.1.1.3">
          0.2
         </cn>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p9.1.m1.1c">
        T=0.2
       </annotation>
      </semantics>
     </math>
     and
     <math alttext="top\_p" class="ltx_Math" display="inline" id="S3.SS1.p9.2.m2.1">
      <semantics id="S3.SS1.p9.2.m2.1a">
       <mrow id="S3.SS1.p9.2.m2.1.1" xref="S3.SS1.p9.2.m2.1.1.cmml">
        <mi id="S3.SS1.p9.2.m2.1.1.2" xref="S3.SS1.p9.2.m2.1.1.2.cmml">
         t
        </mi>
        <mo id="S3.SS1.p9.2.m2.1.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p9.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.2.m2.1.1.3" xref="S3.SS1.p9.2.m2.1.1.3.cmml">
         o
        </mi>
        <mo id="S3.SS1.p9.2.m2.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS1.p9.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.2.m2.1.1.4" xref="S3.SS1.p9.2.m2.1.1.4.cmml">
         p
        </mi>
        <mo id="S3.SS1.p9.2.m2.1.1.1b" lspace="0em" rspace="0em" xref="S3.SS1.p9.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.2.m2.1.1.5" mathvariant="normal" xref="S3.SS1.p9.2.m2.1.1.5.cmml">
         _
        </mi>
        <mo id="S3.SS1.p9.2.m2.1.1.1c" lspace="0em" rspace="0em" xref="S3.SS1.p9.2.m2.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.2.m2.1.1.6" xref="S3.SS1.p9.2.m2.1.1.6.cmml">
         p
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p9.2.m2.1b">
        <apply id="S3.SS1.p9.2.m2.1.1.cmml" xref="S3.SS1.p9.2.m2.1.1">
         <times id="S3.SS1.p9.2.m2.1.1.1.cmml" xref="S3.SS1.p9.2.m2.1.1.1">
         </times>
         <ci id="S3.SS1.p9.2.m2.1.1.2.cmml" xref="S3.SS1.p9.2.m2.1.1.2">
          𝑡
         </ci>
         <ci id="S3.SS1.p9.2.m2.1.1.3.cmml" xref="S3.SS1.p9.2.m2.1.1.3">
          𝑜
         </ci>
         <ci id="S3.SS1.p9.2.m2.1.1.4.cmml" xref="S3.SS1.p9.2.m2.1.1.4">
          𝑝
         </ci>
         <ci id="S3.SS1.p9.2.m2.1.1.5.cmml" xref="S3.SS1.p9.2.m2.1.1.5">
          _
         </ci>
         <ci id="S3.SS1.p9.2.m2.1.1.6.cmml" xref="S3.SS1.p9.2.m2.1.1.6">
          𝑝
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p9.2.m2.1c">
        top\_p
       </annotation>
      </semantics>
     </math>
     is set to
     <math alttext="0.95" class="ltx_Math" display="inline" id="S3.SS1.p9.3.m3.1">
      <semantics id="S3.SS1.p9.3.m3.1a">
       <mn id="S3.SS1.p9.3.m3.1.1" xref="S3.SS1.p9.3.m3.1.1.cmml">
        0.95
       </mn>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p9.3.m3.1b">
        <cn id="S3.SS1.p9.3.m3.1.1.cmml" type="float" xref="S3.SS1.p9.3.m3.1.1">
         0.95
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p9.3.m3.1c">
        0.95
       </annotation>
      </semantics>
     </math>
     .
The
     <math alttext="max\_new\_tokens" class="ltx_Math" display="inline" id="S3.SS1.p9.4.m4.1">
      <semantics id="S3.SS1.p9.4.m4.1a">
       <mrow id="S3.SS1.p9.4.m4.1.1" xref="S3.SS1.p9.4.m4.1.1.cmml">
        <mi id="S3.SS1.p9.4.m4.1.1.2" xref="S3.SS1.p9.4.m4.1.1.2.cmml">
         m
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.3" xref="S3.SS1.p9.4.m4.1.1.3.cmml">
         a
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.4" xref="S3.SS1.p9.4.m4.1.1.4.cmml">
         x
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1b" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.5" mathvariant="normal" xref="S3.SS1.p9.4.m4.1.1.5.cmml">
         _
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1c" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.6" xref="S3.SS1.p9.4.m4.1.1.6.cmml">
         n
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1d" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.7" xref="S3.SS1.p9.4.m4.1.1.7.cmml">
         e
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1e" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.8" xref="S3.SS1.p9.4.m4.1.1.8.cmml">
         w
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1f" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.9" mathvariant="normal" xref="S3.SS1.p9.4.m4.1.1.9.cmml">
         _
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1g" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.10" xref="S3.SS1.p9.4.m4.1.1.10.cmml">
         t
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1h" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.11" xref="S3.SS1.p9.4.m4.1.1.11.cmml">
         o
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1i" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.12" xref="S3.SS1.p9.4.m4.1.1.12.cmml">
         k
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1j" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.13" xref="S3.SS1.p9.4.m4.1.1.13.cmml">
         e
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1k" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.14" xref="S3.SS1.p9.4.m4.1.1.14.cmml">
         n
        </mi>
        <mo id="S3.SS1.p9.4.m4.1.1.1l" lspace="0em" rspace="0em" xref="S3.SS1.p9.4.m4.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.4.m4.1.1.15" xref="S3.SS1.p9.4.m4.1.1.15.cmml">
         s
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p9.4.m4.1b">
        <apply id="S3.SS1.p9.4.m4.1.1.cmml" xref="S3.SS1.p9.4.m4.1.1">
         <times id="S3.SS1.p9.4.m4.1.1.1.cmml" xref="S3.SS1.p9.4.m4.1.1.1">
         </times>
         <ci id="S3.SS1.p9.4.m4.1.1.2.cmml" xref="S3.SS1.p9.4.m4.1.1.2">
          𝑚
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.3.cmml" xref="S3.SS1.p9.4.m4.1.1.3">
          𝑎
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.4.cmml" xref="S3.SS1.p9.4.m4.1.1.4">
          𝑥
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.5.cmml" xref="S3.SS1.p9.4.m4.1.1.5">
          _
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.6.cmml" xref="S3.SS1.p9.4.m4.1.1.6">
          𝑛
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.7.cmml" xref="S3.SS1.p9.4.m4.1.1.7">
          𝑒
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.8.cmml" xref="S3.SS1.p9.4.m4.1.1.8">
          𝑤
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.9.cmml" xref="S3.SS1.p9.4.m4.1.1.9">
          _
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.10.cmml" xref="S3.SS1.p9.4.m4.1.1.10">
          𝑡
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.11.cmml" xref="S3.SS1.p9.4.m4.1.1.11">
          𝑜
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.12.cmml" xref="S3.SS1.p9.4.m4.1.1.12">
          𝑘
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.13.cmml" xref="S3.SS1.p9.4.m4.1.1.13">
          𝑒
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.14.cmml" xref="S3.SS1.p9.4.m4.1.1.14">
          𝑛
         </ci>
         <ci id="S3.SS1.p9.4.m4.1.1.15.cmml" xref="S3.SS1.p9.4.m4.1.1.15">
          𝑠
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p9.4.m4.1c">
        max\_new\_tokens
       </annotation>
      </semantics>
     </math>
     for HuggingFace models and the
     <math alttext="max\_tokens" class="ltx_Math" display="inline" id="S3.SS1.p9.5.m5.1">
      <semantics id="S3.SS1.p9.5.m5.1a">
       <mrow id="S3.SS1.p9.5.m5.1.1" xref="S3.SS1.p9.5.m5.1.1.cmml">
        <mi id="S3.SS1.p9.5.m5.1.1.2" xref="S3.SS1.p9.5.m5.1.1.2.cmml">
         m
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.3" xref="S3.SS1.p9.5.m5.1.1.3.cmml">
         a
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1a" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.4" xref="S3.SS1.p9.5.m5.1.1.4.cmml">
         x
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1b" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.5" mathvariant="normal" xref="S3.SS1.p9.5.m5.1.1.5.cmml">
         _
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1c" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.6" xref="S3.SS1.p9.5.m5.1.1.6.cmml">
         t
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1d" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.7" xref="S3.SS1.p9.5.m5.1.1.7.cmml">
         o
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1e" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.8" xref="S3.SS1.p9.5.m5.1.1.8.cmml">
         k
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1f" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.9" xref="S3.SS1.p9.5.m5.1.1.9.cmml">
         e
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1g" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.10" xref="S3.SS1.p9.5.m5.1.1.10.cmml">
         n
        </mi>
        <mo id="S3.SS1.p9.5.m5.1.1.1h" lspace="0em" rspace="0em" xref="S3.SS1.p9.5.m5.1.1.1.cmml">
         ​
        </mo>
        <mi id="S3.SS1.p9.5.m5.1.1.11" xref="S3.SS1.p9.5.m5.1.1.11.cmml">
         s
        </mi>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p9.5.m5.1b">
        <apply id="S3.SS1.p9.5.m5.1.1.cmml" xref="S3.SS1.p9.5.m5.1.1">
         <times id="S3.SS1.p9.5.m5.1.1.1.cmml" xref="S3.SS1.p9.5.m5.1.1.1">
         </times>
         <ci id="S3.SS1.p9.5.m5.1.1.2.cmml" xref="S3.SS1.p9.5.m5.1.1.2">
          𝑚
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.3.cmml" xref="S3.SS1.p9.5.m5.1.1.3">
          𝑎
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.4.cmml" xref="S3.SS1.p9.5.m5.1.1.4">
          𝑥
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.5.cmml" xref="S3.SS1.p9.5.m5.1.1.5">
          _
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.6.cmml" xref="S3.SS1.p9.5.m5.1.1.6">
          𝑡
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.7.cmml" xref="S3.SS1.p9.5.m5.1.1.7">
          𝑜
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.8.cmml" xref="S3.SS1.p9.5.m5.1.1.8">
          𝑘
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.9.cmml" xref="S3.SS1.p9.5.m5.1.1.9">
          𝑒
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.10.cmml" xref="S3.SS1.p9.5.m5.1.1.10">
          𝑛
         </ci>
         <ci id="S3.SS1.p9.5.m5.1.1.11.cmml" xref="S3.SS1.p9.5.m5.1.1.11">
          𝑠
         </ci>
        </apply>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p9.5.m5.1c">
        max\_tokens
       </annotation>
      </semantics>
     </math>
     for OpenAI models are set to
     <math alttext="1024" class="ltx_Math" display="inline" id="S3.SS1.p9.6.m6.1">
      <semantics id="S3.SS1.p9.6.m6.1a">
       <mn id="S3.SS1.p9.6.m6.1.1" xref="S3.SS1.p9.6.m6.1.1.cmml">
        1024
       </mn>
       <annotation-xml encoding="MathML-Content" id="S3.SS1.p9.6.m6.1b">
        <cn id="S3.SS1.p9.6.m6.1.1.cmml" type="integer" xref="S3.SS1.p9.6.m6.1.1">
         1024
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS1.p9.6.m6.1c">
        1024
       </annotation>
      </semantics>
     </math>
     .
For GPT-3.5 and GPT-4, we call them via the official OpenAI APIs
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_tag ltx_tag_note">
         2
        </span>
        https://platform.openai.com
       </span>
      </span>
     </span>
     .
For StarChat and CodeLlama, we use HuggingFace transformers
     <cite class="ltx_cite ltx_citemacro_cite">
      Wolf et al. (
      <a class="ltx_ref" href="#bib.bib24" title="">
       2019
      </a>
      )
     </cite>
     for inference.
We conduct experiments on two V100 GPUs with 32GB of memory each.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Evaluation Results
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.2 Evaluation Results ‣ 3 Can LLMs Really Generate High-Quality Test Cases? ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     shows the number of test cases each model and method successfully generated. It reveals that, for the relatively easy HumanEval-no-exp dataset, models are capable of producing an ample number of test cases. Conversely, for the harder LeetCode-no-exp dataset, less advanced models like StarChat face difficulties in generating a sufficient quantity of test cases.
This also implies that the most powerful models like GPT-3.5 and GPT-4 can capture the syntax of the test cases very well.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="481" id="S3.F2.1.g1" src="/html/2404.13340/assets/x2.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     Statistics on the number of test cases successfully generated.
The gray line indicates the maximum number of test cases allowed by the dataset.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S3.F3">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf1">
       <br class="ltx_break"/>
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="S3.F3.sf1.g1" src="/html/2404.13340/assets/x3.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        Statistics on the HumanEval-no-exp dataset.
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F3.sf2">
       <br class="ltx_break"/>
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="78" id="S3.F3.sf2.g1" src="/html/2404.13340/assets/x4.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        Statistics on the LeetCode-no-exp dataset.
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Statistics on the number of incorect test cases for each type of error.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     Table
     <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2 Related Work ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     presents the evaluation results across all models and datasets.
The results reveal that the Test Agent (1-shot) method generally outperforms the zero-shot method in terms of accuracy.
This is particularly clear for CodeLlama on HumanEval-no-exp (a 15.24% improvement) and GPT-3.5 on LeetCode-no-exp (a 12.3% improvement).
While in terms of test strength, the situation varies significantly between different models.
For example, 1-shot is of great help to GPT-3.5 in achieving an improved test strength, while it
is unhelpful or even harmful to StarChat in this respect.
It is worth noting that, for GPT-4, there is little difference between 0-shot and 1-shot in all metrics.
This suggests that for the particularly powerful models like GPT-4, the illustrating examples may not be necessary.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     In Table
     <a class="ltx_ref" href="#S2.T1" title="Table 1 ‣ 2 Related Work ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , we can also observe a sharp decline in accuracy for all the generation methods from
HumanEval-no-exp to LeetCode-no-exp.
Even the accuracy of GPT-4 decreases by about 25%,
reaching only about 58% on LeetCode-hard.
This underscores the need for further advancements in test case generation, particularly for hard questions.
In terms of line coverage, there has been a notable decrease observed in StarChat and CodeLlama,
whereas GPT-3.5 and GPT-4 have not experienced a similar decline.
This indicates that the overall quality of test cases
generated by the most advanced models like GPT-4 is bottlenecked by accuracy rather than coverage.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p4">
    <p class="ltx_p" id="S3.SS2.p4.1">
     Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.2 Evaluation Results ‣ 3 Can LLMs Really Generate High-Quality Test Cases? ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     shows the number of incorrect test cases for each type of error.
It can be seen that Assertion Error is the most prevalent type of error, while the other types of errors constitute only a small fraction.
This suggests that the trickiest issue for generating correct test cases lies
in accurately computing the corresponding test outputs given the test inputs.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p5">
    <p class="ltx_p" id="S3.SS2.p5.1">
     In summary, LLMs are capable of generating a considerable number of correct test cases for relatively easy questions, such as those in HumanEval.
However, for harder questions such as those in LeetCode-hard, all of them experience a sharp decline in accuracy and
struggle to generate correct test cases.
It becomes evident that the pivotal factor in elevating the quality of generated test cases is the enhancement of their correctness,
as the overall quality is primarily constrained by accuracy for the advanced LLMs like GPT-4.
The correctness of test cases essentially hinges on the accurate mapping of test inputs to their outputs, a process that may involve complex calculations and reasoning, areas where LLMs do not excel.
This motivates us to decompose this mapping problem into a number of steps to reduce the complexity, and to utilize external tools to enhance the precision of computation and reasoning.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F4">
    <p class="ltx_p ltx_align_center ltx_align_center" id="S3.F4.1.1">
     <span class="ltx_text" id="S3.F4.1.1.1">
      <img alt="Refer to caption" class="ltx_graphics ltx_missing ltx_missing_image" id="S3.F4.1.1.1.g1" src=""/>
     </span>
    </p>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Illustration of the TestChain framework.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Methodology
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    TestChain Framework
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     Based on the above insights, we propose the
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1">
      TestChain
     </em>
     framework that allows LLMs to generate test cases with the assistance of a Python interpreter.
TestChain uses the divide-and-conquer idea to decompose the test case generation into two sequential sub-tasks: test input generation and test output generation.
The two subtasks are handled by two agents called
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.2">
      Designer agent
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.3">
      Calculator agent
     </em>
     , respectively.
Because the test input-output mapping may involve complex computation
and reasoning, TestChain divides the mapping process into a number of small steps using a conservation chain similar to ReAct
     <cite class="ltx_cite ltx_citemacro_cite">
      Yao et al. (
      <a class="ltx_ref" href="#bib.bib27" title="">
       2022
      </a>
      )
     </cite>
     .
In each step, the Python code snippet will be written by the LLM and then be executed by the Python interpreter, in order to
achieve the desired goal. Figure
     <a class="ltx_ref" href="#S3.F4" title="Figure 4 ‣ 3.2 Evaluation Results ‣ 3 Can LLMs Really Generate High-Quality Test Cases? ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     illustrates our TestChain framework.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.1">
     It should be noted that in the generation process, Designer agent will be called only once to generate diverse test inputs, but Calculator agent will be called for each test input.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F5">
    <br class="ltx_break"/>
    <p class="ltx_p ltx_align_center ltx_align_center" id="S4.F5.1.1">
     <span class="ltx_text" id="S4.F5.1.1.1">
      <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="912" id="S4.F5.1.1.1.g1" src="/html/2404.13340/assets/x6.png" width="922"/>
     </span>
    </p>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     Example of the conversation process produced by Calculator agent.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Designer Agent
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     Designer agent employs the LLM for test input generation.
In this agent, the system prompt describes that it will generate basic and edge test inputs.
The 1-shot prompt provides an example of the two types of test inputs
in addition to the function definition and docstring.
Please refer to Figures
     <a class="ltx_ref" href="#A2.F11" title="Figure 11 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     –
     <a class="ltx_ref" href="#A2.F12" title="Figure 12 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       12
      </span>
     </a>
     in Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     for the details of the prompts used by Designer agent.
After generation, the duplicated test inputs will be omitted and the retained ones will be the input to Calculator agent.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS2.p2">
    <p class="ltx_p" id="S4.SS2.p2.1">
     Note that Designer agent does not need to concern itself with test outputs;
it only needs to generate test inputs rather than complete test cases.
With this approach, the LLM can concentrate on producing diverse test inputs without the necessity of computing the corresponding test outputs, thereby minimizing the likelihood of errors.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Calculator Agent
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     Calculator agent is also powered by LLMs.
Based on each test input generated by Designer agent,
Calculator agent is instructed to determine the corresponding test output and write the entire assert statement.
In Calculator agent, the system prompt defines the conversation format in addition to role assignment,
and the 1-shot prompt contains a full example of a conversation trajectory.
Please refer to Figures
     <a class="ltx_ref" href="#A2.F13" title="Figure 13 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     –
     <a class="ltx_ref" href="#A2.F14" title="Figure 14 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     in
Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     for the details of the prompts used by Calculator agent.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">
      Interaction with a Python Interpreter.
     </span>
     We create a ReAct format conversation chain for LLMs to interact with the Python interpreter.
The conversation starts with a user message that contains the function definition, docstring and a test input.
Note that if the 1-shot setting is used, the 1-shot prompt should be prefixed to this user message.
In each round, the LLM will think about what to do and write a Python code snippet or write the test case directly.
The executed result of the Python code snippet will be returned to the LLM subsequently.
The conversation ends when the LLM writes the test case successfully.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p3">
    <p class="ltx_p" id="S4.SS3.p3.1">
     During the conversation process, if the LLM only generates thinking text without code and test case, a
     <em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.1">
      go_on_prompt
     </em>
     like “Observation: go on.” will be sent to the LLM.
The max round of iterations is set to 5, if the LLM fails to write the test case after 5 rounds,
a
     <em class="ltx_emph ltx_font_italic" id="S4.SS3.p3.1.2">
      final_prompt
     </em>
     like “Thought: I now know the final answer.
     <code class="ltx_verbatim ltx_font_typewriter" id="S4.SS3.p3.1.3">
      \
     </code>
     nTest Case:” will be sent to the LLM to force it to write the test case.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p4">
    <p class="ltx_p" id="S4.SS3.p4.1">
     Once started, Calculator agent will run a Python shell environment.
During the conversation, all the code snippets will be executed in the same context, which means the subsequent code snippets can access variables from preceding code snippets.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p5">
    <p class="ltx_p" id="S4.SS3.p5.1">
     Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.1 TestChain Framework ‣ 4 Methodology ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     shows an example of the conversation process produced by Calculator agent.
In this example, Calculator agent is tasked with parsing the input string into a list of nested parentheses while disregarding any spaces within the input string.
To handle this question, the LLM divides the task into two steps: (1) Remove any spaces from the input string;
(2) Iterate through the string and separate it into parentheses.
For each step, the LLM writes Python code snippet and obtains the executed results from the Python interpreter.
Finally, the LLM receives the separated parentheses and writes the complete test case.
Please refer to Figures
     <a class="ltx_ref" href="#A3.F17" title="Figure 17 ‣ Appendix C Examples of Trajectories by Calculator Agent ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       17
      </span>
     </a>
     –
     <a class="ltx_ref" href="#A3.F34" title="Figure 34 ‣ Appendix C Examples of Trajectories by Calculator Agent ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       34
      </span>
     </a>
     in Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     for more examples.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T2">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S4.T2.1.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1" rowspan="2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">
         Model
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.2" rowspan="2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">
         Method
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S4.T2.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">
         HumanEval-no-exp
        </span>
       </th>
       <th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T2.1.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1">
         LeetCode-no-exp
        </span>
       </th>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.2.2">
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.1.1">
         Accuracy
        </span>
        (%)
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.2.1">
         Line Cov
        </span>
        (%)
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.3.1">
         CwB
        </span>
        (%)
       </th>
       <th class="ltx_td ltx_th ltx_th_column" id="S4.T2.1.2.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.5.1">
         Accuracy
        </span>
        (%)
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.2.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.2.2.6.1">
         Line Cov
        </span>
        (%)
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T2.1.3.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.1.1" rowspan="2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text" id="S4.T2.1.3.1.1.1">
         GPT-3.5
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">
        Tester Agent (1-shot)
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">
        74.02
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">
        74.69
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">
        74.15
       </td>
       <td class="ltx_td ltx_border_t" id="S4.T2.1.3.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">
        38.97
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">
        73.66
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.4.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">
        TestChain
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.4.2.2.1">
         80.85
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.4.2.3.1">
         77.53
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.4.2.4.1">
         80.80
        </span>
       </td>
       <td class="ltx_td" id="S4.T2.1.4.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.4.2.6.1">
         48.72
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.1.4.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.4.2.7.1">
         80.23
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.5.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.5.3.1" rowspan="2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text" id="S4.T2.1.5.3.1.1">
         GPT-4
        </span>
       </th>
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.5.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">
        Tester Agent (1-shot)
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">
        84.63
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.3.4" style="padding-left:6.5pt;padding-right:6.5pt;">
        77.04
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.3.5" style="padding-left:6.5pt;padding-right:6.5pt;">
        83.11
       </td>
       <td class="ltx_td ltx_border_t" id="S4.T2.1.5.3.6" style="padding-left:6.5pt;padding-right:6.5pt;">
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.3.7" style="padding-left:6.5pt;padding-right:6.5pt;">
        57.95
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.3.8" style="padding-left:6.5pt;padding-right:6.5pt;">
        88.47
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.1.6.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.6.4.1" style="padding-left:6.5pt;padding-right:6.5pt;">
        TestChain
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.6.4.2.1">
         90.24
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.4.3" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.6.4.3.1">
         80.00
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.4.4" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.6.4.4.1">
         88.66
        </span>
       </td>
       <td class="ltx_td ltx_border_bb" id="S4.T2.1.6.4.5" style="padding-left:6.5pt;padding-right:6.5pt;">
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.4.6" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.6.4.6.1">
         71.79
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.4.7" style="padding-left:6.5pt;padding-right:6.5pt;">
        <span class="ltx_text ltx_font_bold" id="S4.T2.1.6.4.7.1">
         90.60
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Evaluation results of the Tester Agent with 1-shot prompt approach and the TestChain approach.
    </figcaption>
   </figure>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Evaluation of TestChain
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     We follow the same setups in Section
     <a class="ltx_ref" href="#S3.SS1" title="3.1 Experimental Setup ‣ 3 Can LLMs Really Generate High-Quality Test Cases? ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     for datasets and metrics.
We consider two advanced models GPT-3.5 and GPT-4 in our TestChain framework.
Because TestChain uses the 1-shot setting by default, we choose
the Test Agent (1-shot) method depicted in Section
     <a class="ltx_ref" href="#S3.SS1" title="3.1 Experimental Setup ‣ 3 Can LLMs Really Generate High-Quality Test Cases? ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       3.1
      </span>
     </a>
     as the baseline for comparison.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Evaluation Results
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     We analyze the statistics regarding the number of successfully generated test cases, as illustrated in Figure
     <a class="ltx_ref" href="#S5.F6" title="Figure 6 ‣ 5.2 Evaluation Results ‣ 5 Evaluation of TestChain ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     .
Our results indicate that the advanced models such as GPT-3.5 and GPT-4 exhibit the strong capability to produce a sufficient number of valid test cases using both the Test Agent (1-shot) method and the TestChain method.
Hence, the accuracy comparison among generation methods primarily relies on the number of correct test cases that a method can generate.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S5.F6.1.g1" src=""/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Statistics on the number of test cases successfully generated.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S5.F7">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf1">
       <br class="ltx_break"/>
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="141" id="S5.F7.sf1.g1" src="/html/2404.13340/assets/x8.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        On the HumanEval-no-exp dataset.
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_break">
     </div>
     <div class="ltx_flex_cell ltx_flex_size_1">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.sf2">
       <br class="ltx_break"/>
       <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="141" id="S5.F7.sf2.g1" src="/html/2404.13340/assets/x9.png" width="461"/>
       <figcaption class="ltx_caption ltx_centering">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        On the LeetCode-no-exp dataset.
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7:
     </span>
     Statistics on the number of incorrect test cases for each type of error.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.3 Calculator Agent ‣ 4 Methodology ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     presents a comparison of all metrics between the TestChain method and the Test Agent (1-shot) method.
It is evident that TestChain achieves significant improvements over the Test Agent (1-shot) baseline across all metrics and datasets,
regardless of whether GPT-3.5 or GPT-4 is utilized as the backbone model.
Notably, on the challenging LeetCode-no-exp dataset, TestChain with GPT-4
surpasses the corresponding baseline by 13.84% in terms of accuracy.
As for Line Cov and CwB metrics,
our TestChain method also always achieves better results than the Test Agent (1-shot) method.
These findings underscore the ability of our proposed TestChain to generate high-quality test cases that are both accurate and reliable.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p3">
    <p class="ltx_p" id="S5.SS2.p3.1">
     Figure
     <a class="ltx_ref" href="#S5.F7" title="Figure 7 ‣ 5.2 Evaluation Results ‣ 5 Evaluation of TestChain ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     displays the statistical distribution of incorrect test cases across different error categories.
It can be seen that Assertion Error is still the most common type of error for all the methods.
But the number of assertion errors by TestChain is greatly reduced compared to the Test Agent (1-shot) method,
contributing most to TestChain’s higher accuracy.
This demonstrates that our TestChain,
leveraging Python codes for step-by-step computation and reasoning,
effectively decreases the incidence of errors in test outputs, consequently improving the accuracy of the test cases generated.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Effectiveness of the Ingredients in TestChain
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     In this subsection, we want to examine the effectiveness of two ingredients in the TestChain framework:
the decoupling of test input and output generation, and the interaction with the Python interpreter.
To this end, we design a modified TestChain method, where
Calculator agent no longer uses the Python interpreter and instead uses a 1-shot prompt to generate the test case directly.
Please refer to Figures
     <a class="ltx_ref" href="#A2.F15" title="Figure 15 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       15
      </span>
     </a>
     –
     <a class="ltx_ref" href="#A2.F16" title="Figure 16 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       16
      </span>
     </a>
     in
Appendix
     <a class="ltx_ref" href="#A2" title="Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       B
      </span>
     </a>
     for the details of the prompts used by this modified TestChain method.
We use the same setups with Section
     <a class="ltx_ref" href="#S5.SS2" title="5.2 Evaluation Results ‣ 5 Evaluation of TestChain ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       5.2
      </span>
     </a>
     and compare it with the original TestChain method along with
the Test Agent (1-shot) baseline method.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T3">
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
     <thead class="ltx_thead">
      <tr class="ltx_tr" id="S5.T3.1.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.1.1.1.1">
        <span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">
         Method
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.2">
        <span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.2.1">
         Accuracy
        </span>
        (%)
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.1.1.3">
        <span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.3.1">
         Line Cov
        </span>
        (%)
       </th>
      </tr>
     </thead>
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S5.T3.1.2.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.2.1.1">
        Test Agent (1-shot)
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.2">
        57.95
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.2.1.3">
        88.47
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.1.3.2">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.3.2.1">
        TestChain (no-py)
       </th>
       <td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.2">
        61.54
       </td>
       <td class="ltx_td ltx_align_center" id="S5.T3.1.3.2.3">
        83.65
       </td>
      </tr>
      <tr class="ltx_tr" id="S5.T3.1.4.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.4.3.1">
        TestChain
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.3.2">
        <span class="ltx_text ltx_font_bold" id="S5.T3.1.4.3.2.1">
         71.79
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.4.3.3">
        <span class="ltx_text ltx_font_bold" id="S5.T3.1.4.3.3.1">
         90.60
        </span>
       </td>
      </tr>
     </tbody>
    </table>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Results on the LeetCode-no-exp dataset with GPT-4.
TestChain (no-py) refers to the TestChain method without the Python interpreter interaction.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     Table
     <a class="ltx_ref" href="#S5.T3" title="Table 3 ‣ 5.3 Effectiveness of the Ingredients in TestChain ‣ 5 Evaluation of TestChain ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     shows the comparison results.
The main difference between the modified TestChain method and the Test Agent (1-shot) method is that the modified TestChain
separates the generation of test inputs and test outputs, while the Test Agent method generates them together.
The modified TestChain outperforms the Test Agent (1-shot) method by 3.59% in accuracy, demonstrating the benefit of the decoupling through two agents.
The original TestChain achieves substantial improvements over the modified one in terms of
both accuracy and line coverage. This suggests that in TestChain, the interaction with the Python interpreter
is vital in enhancing the generation performance.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this paper, we have conducted a thorough performance evaluation of LLMs in test case generation.
We find that, for relatively hard questions such as those in LeetCode-hard,
even the state-of-the-art LLMs like GPT-4 have great difficulty in generating correct test cases.
To address this challenge, we propose TestChain, characterized by decoupling the generation of test inputs and test outputs,
and interacting with the Python interpreter through the ReAct chain.
Our results clearly demonstrate the superiority of TestChain over the baselines.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_title_section">
   Limitations
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    The proposed TestChain framework demands high model capabilities, thus it is tailored for more robust models such as GPT-3.5 and GPT-4.
Future work can explore how to enhance the performance of weaker models in test case generation based on the TestChain paradigm.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Achiam et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      arXiv preprint arXiv:2303.08774
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Andrews et al. (2006)
    </span>
    <span class="ltx_bibblock">
     James H Andrews, Lionel C Briand, Yvan Labiche, and Akbar Siami Namin. 2006.
    </span>
    <span class="ltx_bibblock">
     Using mutation analysis for assessing and comparing testing coverage criteria.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      IEEE Transactions on Software Engineering
     </em>
     , 32(8):608–624.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Anil et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Palm 2 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2305.10403
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022.
    </span>
    <span class="ltx_bibblock">
     Codet: Code generation with generated tests.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2207.10397
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chen et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Evaluating large language models trained on code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2107.03374
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Dong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. 2023.
    </span>
    <span class="ltx_bibblock">
     Self-collaboration code generation via chatgpt.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2304.07590
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Frieder et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, and Julius Berner. 2023.
    </span>
    <span class="ltx_bibblock">
     Mathematical capabilities of chatgpt.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2301.13867
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Dong Huang, Qingwen Bu, and Heming Cui. 2023a.
    </span>
    <span class="ltx_bibblock">
     Codecot and beyond: Learning to program and test like a developer.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2308.08784
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Huang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Dong Huang, Qingwen Bu, Jie M Zhang, Michael Luck, and Heming Cui. 2023b.
    </span>
    <span class="ltx_bibblock">
     Agentcoder: Multi-agent-based code generation with iterative testing and optimisation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      arXiv preprint arXiv:2312.13010
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sungmin Kang, Juyeon Yoon, and Shin Yoo. 2023.
    </span>
    <span class="ltx_bibblock">
     Large language models are few-shot testers: Exploring llm-based general bug reproduction.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
     </em>
     , pages 2312–2323. IEEE.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Starcoder: may the source be with you!
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      arXiv preprint arXiv:2305.06161
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Li et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Competition-level code generation with alphacode.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      Science
     </em>
     , 378(6624):1092–1097.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023.
    </span>
    <span class="ltx_bibblock">
     Evaluating the logical reasoning ability of chatgpt and gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      arXiv preprint arXiv:2304.03439
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nakano et al. (2021)
    </span>
    <span class="ltx_bibblock">
     Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021.
    </span>
    <span class="ltx_bibblock">
     Webgpt: Browser-assisted question-answering with human feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      arXiv preprint arXiv:2112.09332
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Nijkamp et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022.
    </span>
    <span class="ltx_bibblock">
     Codegen: An open large language model for code with multi-turn program synthesis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2203.13474
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Romera-Paredes et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. 2023.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41586-023-06924-6" target="_blank" title="">
      Mathematical discoveries from program search with large language models
     </a>
     .
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      Nature
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Roziere et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Code llama: Open foundation models for code.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2308.12950
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schäfer et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023.
    </span>
    <span class="ltx_bibblock">
     An empirical evaluation of using large language models for automated unit test generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      IEEE Transactions on Software Engineering
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2303.17580
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, and Shunyu Yao. 2023.
    </span>
    <span class="ltx_bibblock">
     Reflexion: Language agents with verbal reinforcement learning.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Thirty-seventh Conference on Neural Information Processing Systems
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Tufano et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. 2020.
    </span>
    <span class="ltx_bibblock">
     Unit test case generation with transformers and focal context.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2009.05617
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Chi Wang, Susan Xueqing Liu, and Ahmed H. Awadallah. 2023a.
    </span>
    <span class="ltx_bibblock">
     Cost-effective hyperparameter optimization for large language model generation inference.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">
      AutoML’23
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, and Ge Yu. 2023b.
    </span>
    <span class="ltx_bibblock">
     Intervenor: Prompt the coding ability of large language models with the interactive chain of repairing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2311.09868
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wolf et al. (2019)
    </span>
    <span class="ltx_bibblock">
     Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019.
    </span>
    <span class="ltx_bibblock">
     Huggingface’s transformers: State-of-the-art natural language processing.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      arXiv preprint arXiv:1910.03771
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. 2023a.
    </span>
    <span class="ltx_bibblock">
     Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2308.08155
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, and Chi Wang. 2023b.
    </span>
    <span class="ltx_bibblock">
     An empirical study on challenging math problem solving with gpt-4.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      ArXiv preprint arXiv:2306.01337
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
    </span>
    <span class="ltx_bibblock">
     React: Synergizing reasoning and acting in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      arXiv preprint arXiv:2210.03629
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Ying Zhang, Wenjia Song, Zhengjie Ji, Na Meng, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     How well does llm generate security tests?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2310.00710
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Dataset Examples
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    Figure
    <a class="ltx_ref" href="#A1.F8" title="Figure 8 ‣ Appendix A Dataset Examples ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    shows the prompt of HumanEval and LeetCode datasets with and without examples.
   </p>
  </div>
  <figure class="ltx_figure" id="A1.F8">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A1.F8.1.1">
    <span class="ltx_text" id="A1.F8.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="448" id="A1.F8.1.1.1.g1" src="/html/2404.13340/assets/x10.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 8:
    </span>
    Illustration of the modifications for the prompt of the HumanEval and LeetCode-hard datasets.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Prompts for Test Case Generation
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    Figures
    <a class="ltx_ref" href="#A2.F9" title="Figure 9 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
     <span class="ltx_text ltx_ref_tag">
      9
     </span>
    </a>
    –
    <a class="ltx_ref" href="#A2.F16" title="Figure 16 ‣ Appendix B Prompts for Test Case Generation ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
     <span class="ltx_text ltx_ref_tag">
      16
     </span>
    </a>
    provide prompts used by different generation methods considered in our paper.
   </p>
  </div>
  <figure class="ltx_figure" id="A2.F9">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F9.1.1">
    <span class="ltx_text" id="A2.F9.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="115" id="A2.F9.1.1.1.g1" src="/html/2404.13340/assets/x11.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 9:
    </span>
    System prompt for the Test Agent method.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F10">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F10.1.1">
    <span class="ltx_text" id="A2.F10.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="570" id="A2.F10.1.1.1.g1" src="/html/2404.13340/assets/x12.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 10:
    </span>
    1-shot Prompt for the Test Agent method.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F11">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F11.1.1">
    <span class="ltx_text" id="A2.F11.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="115" id="A2.F11.1.1.1.g1" src="/html/2404.13340/assets/x13.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 11:
    </span>
    System prompt for the Designer agent.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F12">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F12.1.1">
    <span class="ltx_text" id="A2.F12.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="559" id="A2.F12.1.1.1.g1" src="/html/2404.13340/assets/x14.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 12:
    </span>
    1-shot prompt for the Designer agent.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F13">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F13.1.1">
    <span class="ltx_text" id="A2.F13.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="426" id="A2.F13.1.1.1.g1" src="/html/2404.13340/assets/x15.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 13:
    </span>
    System prompt for the Calculator agent.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F14">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F14.1.1">
    <span class="ltx_text" id="A2.F14.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1058" id="A2.F14.1.1.1.g1" src="/html/2404.13340/assets/x16.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 14:
    </span>
    1-shot prompt for the Calculator agent.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F15">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F15.1.1">
    <span class="ltx_text" id="A2.F15.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="115" id="A2.F15.1.1.1.g1" src="/html/2404.13340/assets/x17.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 15:
    </span>
    System prompt for the Calculator agent without the Python interpreter.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A2.F16">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A2.F16.1.1">
    <span class="ltx_text" id="A2.F16.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="A2.F16.1.1.1.g1" src="/html/2404.13340/assets/x18.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 16:
    </span>
    1-shot prompt for the Calculator agent without the Python interpreter.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Examples of Trajectories by Calculator Agent
  </h2>
  <div class="ltx_para" id="A3.p1">
   <p class="ltx_p" id="A3.p1.1">
    Figures
    <a class="ltx_ref" href="#A3.F17" title="Figure 17 ‣ Appendix C Examples of Trajectories by Calculator Agent ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
     <span class="ltx_text ltx_ref_tag">
      17
     </span>
    </a>
    –
    <a class="ltx_ref" href="#A3.F34" title="Figure 34 ‣ Appendix C Examples of Trajectories by Calculator Agent ‣ Large Language Models as Test Case Generators: Performance Evaluation and Enhancement">
     <span class="ltx_text ltx_ref_tag">
      34
     </span>
    </a>
    provide four examples of
trajectories produced by Calculator agent, where the correct test outputs are
obtained given the test inputs.
   </p>
  </div>
  <figure class="ltx_figure" id="A3.F17">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_figure_panel" id="A3.F17.2">
      <span class="ltx_text ltx_font_bold" id="A3.F17.2.1">
       Example 1.
      </span>
      <br class="ltx_break"/>
     </p>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center" id="A3.F17.1.1">
      <span class="ltx_text" id="A3.F17.1.1.1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="575" id="A3.F17.1.1.1.g1" src="/html/2404.13340/assets/x19.png" width="922"/>
      </span>
     </p>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 17:
    </span>
    Message 1 of Example 1.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F18">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F18.1.1">
    <span class="ltx_text" id="A3.F18.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="212" id="A3.F18.1.1.1.g1" src="/html/2404.13340/assets/x20.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 18:
    </span>
    Message 2 of example 1.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F19">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_figure_panel" id="A3.F19.2">
      <span class="ltx_text ltx_font_bold" id="A3.F19.2.1">
       Example 2.
      </span>
      <br class="ltx_break"/>
     </p>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center" id="A3.F19.1.1">
      <span class="ltx_text" id="A3.F19.1.1.1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="575" id="A3.F19.1.1.1.g1" src="/html/2404.13340/assets/x21.png" width="922"/>
      </span>
     </p>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 19:
    </span>
    Message 1 of Example 2.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F20">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F20.1.1">
    <span class="ltx_text" id="A3.F20.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="212" id="A3.F20.1.1.1.g1" src="/html/2404.13340/assets/x22.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 20:
    </span>
    Message 2 of Example 2.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F21">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_figure_panel" id="A3.F21.2">
      <span class="ltx_text ltx_font_bold" id="A3.F21.2.1">
       Example 3.
      </span>
      <br class="ltx_break"/>
     </p>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center" id="A3.F21.1.1">
      <span class="ltx_text" id="A3.F21.1.1.1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="717" id="A3.F21.1.1.1.g1" src="/html/2404.13340/assets/x23.png" width="922"/>
      </span>
     </p>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 21:
    </span>
    Message 1 of Example 3.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F22">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F22.1.1">
    <span class="ltx_text" id="A3.F22.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="251" id="A3.F22.1.1.1.g1" src="/html/2404.13340/assets/x24.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 22:
    </span>
    Message 2 of Example 3.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F23">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F23.1.1">
    <span class="ltx_text" id="A3.F23.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_missing ltx_missing_image" id="A3.F23.1.1.1.g1" src=""/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 23:
    </span>
    Message 3 of Example 3.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F24">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F24.1.1">
    <span class="ltx_text" id="A3.F24.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="406" id="A3.F24.1.1.1.g1" src="/html/2404.13340/assets/x26.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 24:
    </span>
    Message 4 of Example 3.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F25">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F25.1.1">
    <span class="ltx_text" id="A3.F25.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="110" id="A3.F25.1.1.1.g1" src="/html/2404.13340/assets/x27.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 25:
    </span>
    Message 5 of Example 3.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F26">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F26.1.1">
    <span class="ltx_text" id="A3.F26.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="206" id="A3.F26.1.1.1.g1" src="/html/2404.13340/assets/x28.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 26:
    </span>
    Message 6 of Example 3.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F27">
   <div class="ltx_flex_figure">
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_figure_panel" id="A3.F27.2">
      <span class="ltx_text ltx_font_bold" id="A3.F27.2.1">
       Example 4.
      </span>
      <br class="ltx_break"/>
     </p>
    </div>
    <div class="ltx_flex_break">
    </div>
    <div class="ltx_flex_cell ltx_flex_size_1">
     <p class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center" id="A3.F27.1.1">
      <span class="ltx_text" id="A3.F27.1.1.1">
       <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="545" id="A3.F27.1.1.1.g1" src="/html/2404.13340/assets/x29.png" width="922"/>
      </span>
     </p>
    </div>
   </div>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 27:
    </span>
    Message 1 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F28">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F28.1.1">
    <span class="ltx_text" id="A3.F28.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="329" id="A3.F28.1.1.1.g1" src="/html/2404.13340/assets/x30.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 28:
    </span>
    Message 2 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F29">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F29.1.1">
    <span class="ltx_text" id="A3.F29.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="117" id="A3.F29.1.1.1.g1" src="/html/2404.13340/assets/x31.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 29:
    </span>
    Message 3 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F30">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F30.1.1">
    <span class="ltx_text" id="A3.F30.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="235" id="A3.F30.1.1.1.g1" src="/html/2404.13340/assets/x32.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 30:
    </span>
    Message 4 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F31">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F31.1.1">
    <span class="ltx_text" id="A3.F31.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="277" id="A3.F31.1.1.1.g1" src="/html/2404.13340/assets/x33.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 31:
    </span>
    Message 5 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F32">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F32.1.1">
    <span class="ltx_text" id="A3.F32.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="212" id="A3.F32.1.1.1.g1" src="/html/2404.13340/assets/x34.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 32:
    </span>
    Message 6 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F33">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F33.1.1">
    <span class="ltx_text" id="A3.F33.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="122" id="A3.F33.1.1.1.g1" src="/html/2404.13340/assets/x35.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 33:
    </span>
    Message 7 of Example 4.
   </figcaption>
  </figure>
  <figure class="ltx_figure" id="A3.F34">
   <br class="ltx_break"/>
   <p class="ltx_p ltx_align_center ltx_align_center" id="A3.F34.1.1">
    <span class="ltx_text" id="A3.F34.1.1.1">
     <img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="187" id="A3.F34.1.1.1.g1" src="/html/2404.13340/assets/x36.png" width="922"/>
    </span>
   </p>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 34:
    </span>
    Message 8 of Example 4.
   </figcaption>
  </figure>
 </section>
</article>
