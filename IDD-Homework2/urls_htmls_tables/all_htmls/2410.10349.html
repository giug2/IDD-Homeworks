<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LLM-based Code-Switched Text Generation for Grammatical Error Correction</title>
<!--Generated on Mon Oct 14 10:51:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.10349v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S1" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS1" title="In 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Genuine CSW GEC Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2" title="In 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Synthetic CSW GEC Data Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2.SSS1" title="In 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Step 1: CSW Text Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2.SSS1.Px1" title="In 2.2.1 Step 1: CSW Text Generation ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">Translation-based CSW Text Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2.SSS1.Px2" title="In 2.2.1 Step 1: CSW Text Generation ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">Parallel Corpus-based CSW Text Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2.SSS1.Px3" title="In 2.2.1 Step 1: CSW Text Generation ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">LLM Prompting-based CSW Text Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2.SSS1.Px4" title="In 2.2.1 Step 1: CSW Text Generation ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">Comparison of Synthetic CSW Text</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.SS2.SSS2" title="In 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Step 2: Synthetic Error Generation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S3" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>CSW GEC Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S4" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S4.SS1" title="In 4 Results and Analysis ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Baseline Comparisons</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S4.SS2" title="In 4 Results and Analysis ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Detailed Model Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S4.SS3" title="In 4 Results and Analysis ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Inference Tweaking and Error Thresholds</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S4.SS4" title="In 4 Results and Analysis ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Synthetic Data Impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S5" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S6" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A1" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Example LLM Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A2" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Error Type Analysis of SOTA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Training Data Schedule</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3.SS0.SSS0.Px1" title="In Appendix C Training Data Schedule ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">Stage 1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3.SS0.SSS0.Px2" title="In Appendix C Training Data Schedule ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">Stage 2</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3.SS0.SSS0.Px3" title="In Appendix C Training Data Schedule ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title">Stage 3</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A4" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Inference Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A5" title="In LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Error Type Analysis of Proposed Model</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">LLM-based Code-Switched Text Generation for 
<br class="ltx_break"/>Grammatical Error Correction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tom Potter 
<br class="ltx_break"/>University of Manchester
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">thomas.potter@postgrad.manchester.ac.uk
<br class="ltx_break"/></span>&amp;Zheng Yuan 
<br class="ltx_break"/>King’s College London 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">zheng.yuan@kcl.ac.uk</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">  Work completed whilst at King’s College London.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">With the rise of globalisation, code-switching (CSW) has become a ubiquitous part of multilingual conversation, posing new challenges for natural language processing (NLP), especially in Grammatical Error Correction (GEC). This work explores the complexities of applying GEC systems to CSW texts. Our objectives include evaluating the performance of state-of-the-art GEC systems on an authentic CSW dataset from English as a Second Language (ESL) learners, exploring synthetic data generation as a solution to data scarcity, and developing a model capable of correcting grammatical errors in monolingual and CSW texts. We generated synthetic CSW GEC data, resulting in one of the first substantial datasets for this task, and showed that a model trained on this data is capable of significant improvements over existing systems. This work targets ESL learners, aiming to provide educational technologies that aid in the development of their English grammatical correctness without constraining their natural multilingualism.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">LLM-based Code-Switched Text Generation for 
<br class="ltx_break"/>Grammatical Error Correction</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Tom Potter<span class="ltx_note ltx_role_thanks" id="p1.1.2.1.1.1.1.1.1.1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>  Work completed whilst at King’s College London.</span></span></span></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">University of Manchester</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.3.1.1">thomas.potter@postgrad.manchester.ac.uk</span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Zheng Yuan</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">King’s College London</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.3.3.1.1">zheng.yuan@kcl.ac.uk</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Code-switching (CSW), the practice of fluidly alternating between two or more languages in conversation, has become commonplace in recent years. This linguistic phenomenon, emerging as a natural consequence of multilingualism, is now widely accepted in social and professional settings <cite class="ltx_cite ltx_citemacro_cite">Yow et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib28" title="">2018</a>)</cite>. Many works have highlighted the utility and cultural importance of CSW in general conversation <cite class="ltx_cite ltx_citemacro_cite">Beatty-Martínez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib3" title="">2020</a>); Falbo and LaCroix (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib11" title="">2021</a>)</cite>. Further research indicates that these advantages extend to language learning, with CSW offering many pedagogical benefits. These include increasing students’ access to content and improving their confidence. <cite class="ltx_cite ltx_citemacro_citet">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib21" title="">2022</a>)</cite> discuss the mechanisms for this, where students use a familiar language to grasp foreign, complex concepts. CSW can also serve as a scaffolding tool, helping to bridge gaps in a student’s comprehension of a language and enabling them to build upon existing knowledge. These benefits reduce the barriers between a student and their target language and help promote a learning environment conducive with active exploration and deeper understanding. Therefore, it is essential that English as a Second Language (ESL) learners are not penalised for expressing their cultural identity through CSW. Grammatical error correction (GEC) is the task of automatically detecting and correcting errors in text. Research on GEC for CSW text remained largely unexplored. <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib7" title="">2024</a>)</cite> were the first to demonstrate that exposing a sequence-tagging GEC model to CSW text during the training process improves performance compared to a monolingual system. However, further work is essential to ensure language technology is inclusive and reflective of real-world linguistic practices.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">1</span></a> shows two examples of CSW from our target population with their grammatical corrections.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The definition of CSW is a subject of ongoing debate. Throughout this work, we use the term CSW to refer specifically to the type of language mixing exhibited by ESL learners.</span></span></span></p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="S1.F1.2" style="width:390.3pt;">
<p class="ltx_p" id="S1.F1.1.1"><em class="ltx_emph ltx_font_italic" id="S1.F1.1.1.1">Example 1:</em><span class="ltx_ERROR undefined" id="S1.F1.1.1.2">{CJK}</span>UTF8minAccording to the test, [<span class="ltx_text" id="S1.F1.1.1.3" style="color:#FF0000;">lacks in me</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.F1.1.1.m1.1"><semantics id="S1.F1.1.1.m1.1a"><mo id="S1.F1.1.1.m1.1.1" stretchy="false" xref="S1.F1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.1.1.m1.1b"><ci id="S1.F1.1.1.m1.1.1.cmml" xref="S1.F1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.F1.1.1.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S1.F1.1.1.4" style="color:#00FF00;">my shortcomings</span>] are 靴下 and ご主人様.</p>
<p class="ltx_p" id="S1.F1.2.2"><em class="ltx_emph ltx_font_italic" id="S1.F1.2.2.1">Example 2</em>: <span class="ltx_ERROR undefined" id="S1.F1.2.2.2">{CJK}</span>UTF8minWhen we [<span class="ltx_text" id="S1.F1.2.2.3" style="color:#FF0000;">call</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.F1.2.2.m1.1"><semantics id="S1.F1.2.2.m1.1a"><mo id="S1.F1.2.2.m1.1.1" stretchy="false" xref="S1.F1.2.2.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.2.2.m1.1b"><ci id="S1.F1.2.2.m1.1.1.cmml" xref="S1.F1.2.2.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.2.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.2.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S1.F1.2.2.4" style="color:#00FF00;">say</span>] ダッシュボード, do we actually mean a glove compartment in English?</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of GEC in ESL learner language.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite significant advancements in GEC in recent years, a gap persists in addressing CSW texts, with monolingual GEC datasets labelling CSW as a type of error <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib21" title="">2022</a>)</cite>. There are several reasons for this, the most prominent being the scarcity of high-quality training data, a problem that plagues monolingual GEC systems. The unique linguistic features of CSW, including its variable syntax, semantics and pragmatics, add additional complexity to this task. Monolingual seq2seq GEC models, e.g. T5 <cite class="ltx_cite ltx_citemacro_cite">Rothe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib24" title="">2021</a>)</cite>, struggle with CSW text as they fail to represent the non-English inputs, resulting in their inability to output the CSW text. On the other hand, multilingual seq2seq models and edit-based GEC models like GECToR <cite class="ltx_cite ltx_citemacro_cite">Omelianchuk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib22" title="">2020</a>)</cite> can handle CSW text but struggle with the ambiguity present at language switching points. This ambiguity challenges the models’ ability to accurately correct the text.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This paper aims to bridge this gap. Firstly, to address the data scarcity issue, we propose a method for generating high-quality synthetic CSW GEC data, using which we produce, to our knowledge, one of the first substantial datasets labelled for this task<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>This dataset is available on <a class="ltx_ref ltx_href" href="https://github.com/tpotterer/Synthetic-CSW-Text-for-GEC" title="">GitHub.</a></span></span></span>. Secondly, we train a token classification-style GEC system, tailored to correct errors in texts produced by ESL learners. This demographic is significant for our study as they not only present consistent CSW patterns but also stand to benefit greatly from a GEC system capable of handling CSW text.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Data</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Genuine CSW GEC Dataset</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">One of the only datasets labelled for GEC which does not remove CSW text, is the Lang-8 dataset <cite class="ltx_cite ltx_citemacro_cite">Mizumoto et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib20" title="">2013</a>)</cite>, sourced from the Lang-8 language learning platform. This dataset, when filtered to contain entries where CSW is present, offers a foundation of authentic data, comprises 5,875 pairs of ungrammatical and corrected sentences across 6 CSW language pairs: English-Japanese (81.9%), English-Korean (13.0%), English-Traditional Chinese (3.4%), English-Russian (1.2%), English-Thai (0.5%) and English-Arabic (0.1%).</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The crowd-sourced nature of Lang-8 required manual validation to ensure accuracy. We tasked an annotator with the responsibility of verifying the original corrections in the dataset, as well as combing for missed errors, incorrect annotations and over-annotations.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic CSW GEC Data Generation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Given the small size of the available CSW GEC dataset, we introduced a 2-step approach to synthetic CSW GEC data generation. First, we generated grammatically correct CSW sentences. This is followed by the introduction of errors.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S2.T1.1.1.1.1" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Metric</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S2.T1.1.1.1.2" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Genuine CSW</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S2.T1.1.1.1.3" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">LLM CSW</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S2.T1.1.1.1.4" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">Translation CSW</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S2.T1.1.1.1.5" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1">Corpus CSW</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1">CMI</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.1.2.1.2">15.52</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.1.2.1.3">16.14</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.1.2.1.4">27.81</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T1.1.2.1.5">11.42</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.3.2.1">M-Index</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.3.2.2">0.007</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.3.2.3">0.004</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.3.2.4">0.015</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.3.2.5">0.006</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.4.3.1">I-Index</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.4.3.2">0.21</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.4.3.3">0.21</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.4.3.4">0.30</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.4.3.5">0.20</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.5.4.1">Burstiness</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.5.4.2">-0.07</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.5.4.3">-0.04</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.5.4.4">0.03</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.5.4.5">-0.11</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.6.5.1">CF1</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.6.5.2">6.38</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.6.5.3">5.82</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.6.5.4">17.13</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.6.5.5">2.55</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.7.6.1">CF2</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.7.6.2">19.77</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.7.6.3">19.03</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.7.6.4">31.11</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.7.6.5">16.04</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T1.1.8.7.1">CF3</th>
<td class="ltx_td ltx_align_right" id="S2.T1.1.8.7.2">18.34</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.8.7.3">17.61</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.8.7.4">30.05</td>
<td class="ltx_td ltx_align_right" id="S2.T1.1.8.7.5">14.20</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Quantitative Description of the Genuine and Generated CSW Datasets Using Various CSW Metrics.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Step 1: CSW Text Generation</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Three different synthetic data generation techniques have been explored to generate CSW data.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Translation-based CSW Text Generation</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">required a monolingual corpus, a machine translation (MT) algorithm, and a sentence parser. To generate a CSW utterance, we used the Stanford Parser v4.5.4<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This can be downloaded from the <a class="ltx_ref ltx_href" href="https://stanfordnlp.github.io/CoreNLP/history.html" title="">CoreNLP website</a>.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Manning et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib19" title="">2014</a>)</cite> to build a syntactic parse tree. We then randomly selected and translated a subtree using the ArgosTranslate MT package<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We used ArgosTranslate v1.8.0, available on <a class="ltx_ref ltx_href" href="https://github.com/argosopentech/argos-translate" title="">GitHub</a>.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Finlay (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib12" title="">2023</a>); Klein et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib17" title="">2017</a>)</cite>. This method generates plausible CSW text. However, performance is dependent on the strength of the parsing and translation algorithms; and the style of language within the corpus. To approximate the style of our authentic CSW text, we used corrected monolingual sentences from the Lang-8 corpus.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Parallel Corpus-based CSW Text Generation</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">avoids the need for a translation algorithm. Instead, we used the same Stanford Parser, this time with Spanish, French and German configurations; and the AWESOME word-alignment model<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>The authors shared their model on <a class="ltx_ref ltx_href" href="https://huggingface.co/aneuraz/awesome-align-with-co" title="">HuggingFace</a>.</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Dou and Neubig (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib10" title="">2021</a>)</cite>, to identify parts of parallel corpora labelled for MT with similar syntactic structure. For this method, we use the Europarl corpus <cite class="ltx_cite ltx_citemacro_cite">Koehn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib18" title="">2012</a>)</cite> due to the grammatical quality of its English component. Under the Equivalence Constraint Theory <cite class="ltx_cite ltx_citemacro_cite">Rizvi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib23" title="">2021</a>)</cite>, these areas are where CSW is likely to take place. We, therefore, randomly chose overlapping subtrees as candidates for injection of non-English text. Although this method does not require MT, it is reliant on performant word-alignment and parsing systems; these are rare for many languages.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">LLM Prompting-based CSW Text Generation</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">The other methods of generating CSW text rely on injecting a second language into existing monolingual corpora. Hence, they are not able to recreate one of the main switching styles shown by ESL learners - CSW as a genuine pragmatic strategy. A common reason for this style of switching is when quoting another language. It is difficult to recreate this style using a monolingual foundation as sentences like <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.Px3.p1.1.1">The Japanese word for “dog” is “<span class="ltx_ERROR undefined" id="S2.SS2.SSS1.Px3.p1.1.1.1">{CJK}</span>UTF8min犬”</span> seldom appear in authentic monolingual corpora.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p2.1">To generate diverse CSW texts without relying on existing corpora or inaccurate alignment algorithms, we leveraged the strong general knowledge of Large Language Models (LLMs). We demonstrated that OpenAI’s GPT-3.5 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib4" title="">2020</a>)</cite> can create high-quality CSW sentences when shown examples of authentic utterances. Along with genuine CSW texts, we supplied a one-shot example of how to use the switching styles of an existing CSW text to generate a new sentence.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>The full prompt can be seen in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A1" title="Appendix A Example LLM Prompt ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px4">
<h5 class="ltx_title ltx_title_paragraph">Comparison of Synthetic CSW Text</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px4.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px4.p1.1">We used several CSW metrics to quantify the qualities of CSW texts: Code Mixing Index (CMI) <cite class="ltx_cite ltx_citemacro_cite">Gambäck and Das (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib13" title="">2016</a>)</cite>, Multilingual Index (M-Index) <cite class="ltx_cite ltx_citemacro_cite">Barnett et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib2" title="">2000</a>)</cite>, Probability of Switching (I-Index) <cite class="ltx_cite ltx_citemacro_cite">Guzmán et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib16" title="">2017</a>)</cite>, Burstiness <cite class="ltx_cite ltx_citemacro_cite">Goh and Barabási (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib15" title="">2008</a>)</cite>, and Complexity Factor (CF1-3) <cite class="ltx_cite ltx_citemacro_cite">Ghosh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib14" title="">2017</a>)</cite>. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.T1" title="Table 1 ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">1</span></a> shows the value of each metric for our genuine CSW dataset, as well as for these 3 synthetic CSW datasets. We can see that the LLM prompting-based dataset was superior in its similarity to the authentic CSW data. Using this method, we generated a corpus of 73,293 utterances covering over 20 English language pairs, including the 6 language pairs in the original dataset.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>The LLM does not always generate the language pairs we ask for. However, these sentences are still included in the dataset categorised under their actual language pair.</span></span></span></p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Step 2: Synthetic Error Generation</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Several works have shown the effectiveness of rule-based error injection for GEC data generation. Many use the PIE-synthetic dataset <cite class="ltx_cite ltx_citemacro_cite">Awasthi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib1" title="">2019</a>)</cite>, a perturbed version of the 1BW corpus <cite class="ltx_cite ltx_citemacro_cite">Chelba et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib8" title="">2013</a>)</cite>. For each sentence, the authors introduce between 0 and 4 errors of random type. We extended this work by introducing a new subset of error types that are not only more common in ESL learners, but also are areas where the SOTA performance collapses when faced with CSW text: noun, pronoun, word order, determiner, and punctuation errors.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Error type analysis is presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A2" title="Appendix B Error Type Analysis of SOTA ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">B</span></a>.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">To increase the diversity of errors, we adopted a second style of error injection, Backtranslation <cite class="ltx_cite ltx_citemacro_cite">Stahlberg and Kumar (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib25" title="">2021</a>)</cite>. By swapping the source and target sentences of a monolingual dataset, we trained a GECToR-based system to induce errors in our synthetic CSW sentences.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1">Using these methods, we created two datasets: Syn-CSW PIE and Syn-CSW Rev-GECToR. After removing pairs containing no injected errors, we are left with 70,180 and 18,159 sentences each.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.3.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.3.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" colspan="3" id="S2.T2.2.2.3.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.3.1.2.1">BEA-2019 Test</span></td>
<td class="ltx_td ltx_align_center" colspan="3" id="S2.T2.2.2.3.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.3.1.3.1">Genuine CSW</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.2.3"></th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.2.4.1">P</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.2.5.1">R</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.1.1.1.1"><math alttext="\text{{F}}_{0.5}" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.m1.1a"><msub id="S2.T2.1.1.1.1.m1.1.1" xref="S2.T2.1.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.T2.1.1.1.1.m1.1.1.2" xref="S2.T2.1.1.1.1.m1.1.1.2a.cmml">F</mtext><mn id="S2.T2.1.1.1.1.m1.1.1.3" xref="S2.T2.1.1.1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.m1.1b"><apply id="S2.T2.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.1.1.1.1.m1.1.1.1.cmml" xref="S2.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.T2.1.1.1.1.m1.1.1.2a.cmml" xref="S2.T2.1.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.T2.1.1.1.1.m1.1.1.2.cmml" xref="S2.T2.1.1.1.1.m1.1.1.2">F</mtext></ci><cn id="S2.T2.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T2.1.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.m1.1c">\text{{F}}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.2.6.1">P</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.2.7.1">R</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.2.2"><math alttext="\text{{F}}_{0.5}" class="ltx_Math" display="inline" id="S2.T2.2.2.2.2.m1.1"><semantics id="S2.T2.2.2.2.2.m1.1a"><msub id="S2.T2.2.2.2.2.m1.1.1" xref="S2.T2.2.2.2.2.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S2.T2.2.2.2.2.m1.1.1.2" xref="S2.T2.2.2.2.2.m1.1.1.2a.cmml">F</mtext><mn id="S2.T2.2.2.2.2.m1.1.1.3" xref="S2.T2.2.2.2.2.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.2.m1.1b"><apply id="S2.T2.2.2.2.2.m1.1.1.cmml" xref="S2.T2.2.2.2.2.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.2.2.2.2.m1.1.1.1.cmml" xref="S2.T2.2.2.2.2.m1.1.1">subscript</csymbol><ci id="S2.T2.2.2.2.2.m1.1.1.2a.cmml" xref="S2.T2.2.2.2.2.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S2.T2.2.2.2.2.m1.1.1.2.cmml" xref="S2.T2.2.2.2.2.m1.1.1.2">F</mtext></ci><cn id="S2.T2.2.2.2.2.m1.1.1.3.cmml" type="float" xref="S2.T2.2.2.2.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.2.m1.1c">\text{{F}}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.2.2.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.2.2.4.2.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.4.2.1.1">Existing GEC systems</span></th>
<td class="ltx_td ltx_border_t" colspan="6" id="S2.T2.2.2.4.2.2"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.5.3.1">GECToR</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.5.3.2">77.88</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.5.3.3">53.07</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.5.3.4">71.22</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.5.3.5">71.14</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.5.3.6">27.08</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.5.3.7">53.67</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.6.4.1">T5-Small</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.6.4.2">62.03</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.6.4.3">47.19</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.6.4.4">58.34</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.6.4.5">11.70</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.6.4.6">24.98</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.6.4.7">13.09</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.2.2.7.5.1"><span class="ltx_text ltx_font_bold" id="S2.T2.2.2.7.5.1.1">Our CSW GEC systems</span></th>
<td class="ltx_td ltx_border_t" colspan="6" id="S2.T2.2.2.7.5.2"></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.8.6.1">Stage 1</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.8.6.2">67.23</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.8.6.3">53.88</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.8.6.4">64.05</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.8.6.5">66.15</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.8.6.6">26.04</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.8.6.7">50.57</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.9.7.1">Stage 2</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.9.7.2">72.64</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.9.7.3">51.73</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.9.7.4">67.20</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.9.7.5">65.41</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.9.7.6">29.93</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.9.7.7">52.87</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.10.8.1">Stage 3</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.10.8.2">74.32</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.10.8.3">53.40</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.10.8.4">68.92</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.10.8.5">84.66</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.10.8.6">22.92</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.10.8.7">55.02</td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S2.T2.2.2.11.9.1">Inference Tweaks</th>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.11.9.2">69.01</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.11.9.3">58.40</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.11.9.4">66.59</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.11.9.5">76.02</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.11.9.6">38.67</td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.2.11.9.7">63.71</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>ERRANT-based Precision, Recall and <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="S2.T2.4.m1.1"><semantics id="S2.T2.4.m1.1b"><msub id="S2.T2.4.m1.1.1" xref="S2.T2.4.m1.1.1.cmml"><mi id="S2.T2.4.m1.1.1.2" xref="S2.T2.4.m1.1.1.2.cmml">F</mi><mn id="S2.T2.4.m1.1.1.3" xref="S2.T2.4.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S2.T2.4.m1.1c"><apply id="S2.T2.4.m1.1.1.cmml" xref="S2.T2.4.m1.1.1"><csymbol cd="ambiguous" id="S2.T2.4.m1.1.1.1.cmml" xref="S2.T2.4.m1.1.1">subscript</csymbol><ci id="S2.T2.4.m1.1.1.2.cmml" xref="S2.T2.4.m1.1.1.2">𝐹</ci><cn id="S2.T2.4.m1.1.1.3.cmml" type="float" xref="S2.T2.4.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.m1.1d">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S2.T2.4.m1.1e">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> Scores of Baselines and Our Model Throughout Training</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>CSW GEC Systems</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">For our GEC system targeting CSW texts, we chose a GECToR model <cite class="ltx_cite ltx_citemacro_cite">Omelianchuk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib22" title="">2020</a>)</cite>, with a RoBERTa-base foundation, due to its proven efficacy with limited training data and stronger performance on CSW texts compared to seq2seq models. We added a new CSW class to the error detection head, adding the ability to detect CSW tokens.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Following <cite class="ltx_cite ltx_citemacro_citet">Tarnavskyi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib26" title="">2022</a>)</cite>, we used a 3-stage training schedule. In the first, we used the same distilled 1BW corpus, and added all our synthetic CSW GEC data. For the second, we used several GEC datasets: NUCLE <cite class="ltx_cite ltx_citemacro_cite">Dahlmeier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib9" title="">2023</a>)</cite>, FCE <cite class="ltx_cite ltx_citemacro_cite">Yannakoudakis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib27" title="">2011</a>)</cite>, W&amp;I Locness <cite class="ltx_cite ltx_citemacro_cite">Bryant et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib5" title="">2019</a>)</cite>, Lang-8 and our 2 synthetic CSW datasets. As our genuine CSW dataset is a subset of the Lang-8 corpus, we checked and removed any duplicates. Following previous works, we finished training using the W&amp;I Locness dataset due to its superior quality. In this final stage, we added a sampled subset of our synthetic CSW sentences and 90% of our authentic CSW data, ensuring exposure to synthetic and genuine CSW text. At each stage, we reserved 5% of the data for validation.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>An exact breakdown of contributions by each dataset is given in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3" title="Appendix C Training Data Schedule ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">C</span></a>.</span></span></span> Finally, we tuned inference parameters using a grid-search to optimise the <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">F</mi><mn id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">𝐹</ci><cn id="S3.p2.1.m1.1.1.3.cmml" type="float" xref="S3.p2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> on the final validation set. By beginning with pre-training on large amounts of lower-quality data in the early stages, this multi-stage learning process allows the model to first build a robust GEC foundation before refining it with high quality data in the latter stages. This approach allows the model to learn incrementally, reducing the risk of the model being overwhelmed by the complexity of the task from the outset.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Analysis</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baseline Comparisons</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We compared our model against two well-established systems: a RoBERTa-base GECToR model <cite class="ltx_cite ltx_citemacro_cite">Omelianchuk et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib22" title="">2020</a>)</cite>, with near SOTA performance on the BEA-2019 test set <cite class="ltx_cite ltx_citemacro_cite">Bryant et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib5" title="">2019</a>)</cite>, and a seq2seq T5 model <cite class="ltx_cite ltx_citemacro_cite">Rothe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib24" title="">2021</a>)</cite>. To assess these models, we evaluated their performance on the BEA-2019 test set and the remaining 10% of our authentic CSW data. The ERRANT <cite class="ltx_cite ltx_citemacro_cite">Bryant et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib6" title="">2017</a>)</cite> GEC evaluation results, as outlined in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.T2" title="Table 2 ‣ 2.2.2 Step 2: Synthetic Error Generation ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">2</span></a>, demonstrate a clear degradation in performance when these two systems are applied to CSW texts. The ERRANT toolkit detects and classifies edits between source and target sentence pairs into predefined error categories. It enables the comparison of a proposed set of edits with a reference set, providing a way of calculating metrics, such as precision and recall, across these categories.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Detailed Model Performance</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The progression of our model throughout training provided insights into its evolving capabilities and effectiveness of our synthetic data. We monitored several metrics, including the ERRANT precision, recall and <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝐹</ci><cn id="S4.SS2.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> score, for the BEA-2019 test set and the remaining unused 10% of our genuine CSW dataset. These metrics, as displayed in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.T2" title="Table 2 ‣ 2.2.2 Step 2: Synthetic Error Generation ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">2</span></a>, indicate a steady improvement in the ability to handle CSW texts. Notably, the performance on the CSW dataset shows a significant leap in the final stages, where the contribution of our synthetic dataset is largest. This improvement in CSW text handling did slightly compromise the model’s performance on monolingual GEC tasks, as seen on the BEA-2019 test set. This suggests a trade-off inherent in specialising the model for CSW contexts. However, our model remains competitive amongst SOTA monolingual GEC systems of its size.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Three illustrative examples of our model’s corrections, taken from the CSW test set, can be seen in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S4.F2" title="Figure 2 ‣ 4.2 Detailed Model Performance ‣ 4 Results and Analysis ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">2</span></a>. The first example demonstrates a case where the model has correctly identified all of the changes required, including the incorrect capitalisation of a word, a missing word, and some missing punctuation. The second example shows a “near miss”; here, the model has correctly identified the majority of the changes required but dropped the “I” whilst rearranging the start of the sentence. Finally, the third example presents a scenario where the model has fallen slightly short, failing to recognise the need for “were” instead of “was” in this hypothetical context.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="S4.F2.29" style="width:390.3pt;">
<p class="ltx_p" id="S4.F2.9.9"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.F2.9.9.1">Gold Correction 1:</em><span class="ltx_ERROR undefined" id="S4.F2.9.9.2">{CJK}</span>UTF8mj We have many [<span class="ltx_text" id="S4.F2.9.9.3" style="color:#FF0000;">New</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.1.1.m1.1"><semantics id="S4.F2.1.1.m1.1a"><mo id="S4.F2.1.1.m1.1.1" stretchy="false" xref="S4.F2.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.1.1.m1.1b"><ci id="S4.F2.1.1.m1.1.1.cmml" xref="S4.F2.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.1.1.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.9.9.4" style="color:#00FF00;">new</span>] words for [<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.2.2.m2.1"><semantics id="S4.F2.2.2.m2.1a"><mi id="S4.F2.2.2.m2.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.2.2.m2.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.2.2.m2.1b"><emptyset id="S4.F2.2.2.m2.1.1.cmml" xref="S4.F2.2.2.m2.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.2.m2.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.2.2.m2.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.3.3.m3.1"><semantics id="S4.F2.3.3.m3.1a"><mo id="S4.F2.3.3.m3.1.1" stretchy="false" xref="S4.F2.3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.3.3.m3.1b"><ci id="S4.F2.3.3.m3.1.1.cmml" xref="S4.F2.3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.3.3.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.3.3.m3.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.9.9.5" style="color:#00FF00;">the</span>] unemployed[<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.4.4.m4.1"><semantics id="S4.F2.4.4.m4.1a"><mi id="S4.F2.4.4.m4.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.4.4.m4.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.4.4.m4.1b"><emptyset id="S4.F2.4.4.m4.1.1.cmml" xref="S4.F2.4.4.m4.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.4.4.m4.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.4.4.m4.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.5.5.m5.1"><semantics id="S4.F2.5.5.m5.1a"><mo id="S4.F2.5.5.m5.1.1" stretchy="false" xref="S4.F2.5.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.5.5.m5.1b"><ci id="S4.F2.5.5.m5.1.1.cmml" xref="S4.F2.5.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.5.5.m5.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.5.5.m5.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.9.9.6" style="color:#00FF00;">:</span>] "이태백"[<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.6.6.m6.1"><semantics id="S4.F2.6.6.m6.1a"><mi id="S4.F2.6.6.m6.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.6.6.m6.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.6.6.m6.1b"><emptyset id="S4.F2.6.6.m6.1.1.cmml" xref="S4.F2.6.6.m6.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.6.6.m6.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.6.6.m6.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.7.7.m7.1"><semantics id="S4.F2.7.7.m7.1a"><mo id="S4.F2.7.7.m7.1.1" stretchy="false" xref="S4.F2.7.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.7.7.m7.1b"><ci id="S4.F2.7.7.m7.1.1.cmml" xref="S4.F2.7.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.7.7.m7.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.7.7.m7.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.9.9.7" style="color:#00FF00;">,</span>] "백수"[<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.8.8.m8.1"><semantics id="S4.F2.8.8.m8.1a"><mi id="S4.F2.8.8.m8.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.8.8.m8.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.8.8.m8.1b"><emptyset id="S4.F2.8.8.m8.1.1.cmml" xref="S4.F2.8.8.m8.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.8.8.m8.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.8.8.m8.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.9.9.m9.1"><semantics id="S4.F2.9.9.m9.1a"><mo id="S4.F2.9.9.m9.1.1" stretchy="false" xref="S4.F2.9.9.m9.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.9.9.m9.1b"><ci id="S4.F2.9.9.m9.1.1.cmml" xref="S4.F2.9.9.m9.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.9.9.m9.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.9.9.m9.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.9.9.8" style="color:#00FF00;">,</span>] "백조"</p>
<p class="ltx_p" id="S4.F2.18.18"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.F2.18.18.1">Proposed Correction 1:</em><span class="ltx_ERROR undefined" id="S4.F2.18.18.2">{CJK}</span>UTF8mj We have many [<span class="ltx_text" id="S4.F2.18.18.3" style="color:#FF0000;">New</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.10.10.m1.1"><semantics id="S4.F2.10.10.m1.1a"><mo id="S4.F2.10.10.m1.1.1" stretchy="false" xref="S4.F2.10.10.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.10.10.m1.1b"><ci id="S4.F2.10.10.m1.1.1.cmml" xref="S4.F2.10.10.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.10.10.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.10.10.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.18.18.4" style="color:#00FF00;">new</span>] words for [<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.11.11.m2.1"><semantics id="S4.F2.11.11.m2.1a"><mi id="S4.F2.11.11.m2.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.11.11.m2.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.11.11.m2.1b"><emptyset id="S4.F2.11.11.m2.1.1.cmml" xref="S4.F2.11.11.m2.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.11.11.m2.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.11.11.m2.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.12.12.m3.1"><semantics id="S4.F2.12.12.m3.1a"><mo id="S4.F2.12.12.m3.1.1" stretchy="false" xref="S4.F2.12.12.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.12.12.m3.1b"><ci id="S4.F2.12.12.m3.1.1.cmml" xref="S4.F2.12.12.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.12.12.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.12.12.m3.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.18.18.5" style="color:#00FF00;">the</span>] unemployed[<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.13.13.m4.1"><semantics id="S4.F2.13.13.m4.1a"><mi id="S4.F2.13.13.m4.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.13.13.m4.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.13.13.m4.1b"><emptyset id="S4.F2.13.13.m4.1.1.cmml" xref="S4.F2.13.13.m4.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.13.13.m4.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.13.13.m4.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.14.14.m5.1"><semantics id="S4.F2.14.14.m5.1a"><mo id="S4.F2.14.14.m5.1.1" stretchy="false" xref="S4.F2.14.14.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.14.14.m5.1b"><ci id="S4.F2.14.14.m5.1.1.cmml" xref="S4.F2.14.14.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.14.14.m5.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.14.14.m5.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.18.18.6" style="color:#00FF00;">:</span>] "이태백"[<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.15.15.m6.1"><semantics id="S4.F2.15.15.m6.1a"><mi id="S4.F2.15.15.m6.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.15.15.m6.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.15.15.m6.1b"><emptyset id="S4.F2.15.15.m6.1.1.cmml" xref="S4.F2.15.15.m6.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.15.15.m6.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.15.15.m6.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.16.16.m7.1"><semantics id="S4.F2.16.16.m7.1a"><mo id="S4.F2.16.16.m7.1.1" stretchy="false" xref="S4.F2.16.16.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.16.16.m7.1b"><ci id="S4.F2.16.16.m7.1.1.cmml" xref="S4.F2.16.16.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.16.16.m7.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.16.16.m7.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.18.18.7" style="color:#00FF00;">,</span>] "백수"[<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.17.17.m8.1"><semantics id="S4.F2.17.17.m8.1a"><mi id="S4.F2.17.17.m8.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.17.17.m8.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.17.17.m8.1b"><emptyset id="S4.F2.17.17.m8.1.1.cmml" xref="S4.F2.17.17.m8.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.17.17.m8.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.17.17.m8.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.18.18.m9.1"><semantics id="S4.F2.18.18.m9.1a"><mo id="S4.F2.18.18.m9.1.1" stretchy="false" xref="S4.F2.18.18.m9.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.18.18.m9.1b"><ci id="S4.F2.18.18.m9.1.1.cmml" xref="S4.F2.18.18.m9.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.18.18.m9.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.18.18.m9.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.18.18.8" style="color:#00FF00;">,</span>] "백조"</p>
<p class="ltx_p" id="S4.F2.21.21"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.F2.21.21.1">Gold Correction 2:</em><span class="ltx_ERROR undefined" id="S4.F2.21.21.2">{CJK}</span>UTF8min [<span class="ltx_text" id="S4.F2.21.21.3" style="color:#FF0000;">I and my girlfriend</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.19.19.m1.1"><semantics id="S4.F2.19.19.m1.1a"><mo id="S4.F2.19.19.m1.1.1" stretchy="false" xref="S4.F2.19.19.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.19.19.m1.1b"><ci id="S4.F2.19.19.m1.1.1.cmml" xref="S4.F2.19.19.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.19.19.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.19.19.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.21.21.4" style="color:#00FF00;">My girlfriend and I</span>] looked [<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.20.20.m2.1"><semantics id="S4.F2.20.20.m2.1a"><mi id="S4.F2.20.20.m2.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.20.20.m2.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.20.20.m2.1b"><emptyset id="S4.F2.20.20.m2.1.1.cmml" xref="S4.F2.20.20.m2.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.20.20.m2.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.20.20.m2.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.21.21.m3.1"><semantics id="S4.F2.21.21.m3.1a"><mo id="S4.F2.21.21.m3.1.1" stretchy="false" xref="S4.F2.21.21.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.21.21.m3.1b"><ci id="S4.F2.21.21.m3.1.1.cmml" xref="S4.F2.21.21.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.21.21.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.21.21.m3.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.21.21.5" style="color:#00FF00;">at a</span>] picture called "無原罪の聖母" (Immaculate Conception).</p>
<p class="ltx_p" id="S4.F2.24.24"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.F2.24.24.1">Proposed Correction 2:</em><span class="ltx_ERROR undefined" id="S4.F2.24.24.2">{CJK}</span>UTF8min [<span class="ltx_text" id="S4.F2.24.24.3" style="color:#FF0000;">I and my girlfriend</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.22.22.m1.1"><semantics id="S4.F2.22.22.m1.1a"><mo id="S4.F2.22.22.m1.1.1" stretchy="false" xref="S4.F2.22.22.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.22.22.m1.1b"><ci id="S4.F2.22.22.m1.1.1.cmml" xref="S4.F2.22.22.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.22.22.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.22.22.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.24.24.4" style="color:#00FF00;">My girlfriend and</span>] looked [<math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.23.23.m2.1"><semantics id="S4.F2.23.23.m2.1a"><mi id="S4.F2.23.23.m2.1.1" mathcolor="#FF0000" mathvariant="normal" xref="S4.F2.23.23.m2.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.23.23.m2.1b"><emptyset id="S4.F2.23.23.m2.1.1.cmml" xref="S4.F2.23.23.m2.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.23.23.m2.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.23.23.m2.1d">∅</annotation></semantics></math> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.24.24.m3.1"><semantics id="S4.F2.24.24.m3.1a"><mo id="S4.F2.24.24.m3.1.1" stretchy="false" xref="S4.F2.24.24.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.24.24.m3.1b"><ci id="S4.F2.24.24.m3.1.1.cmml" xref="S4.F2.24.24.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.24.24.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.24.24.m3.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.24.24.5" style="color:#00FF00;">at a</span>] picture called "無原罪の聖母" (Immaculate Conception).</p>
<p class="ltx_p" id="S4.F2.26.26"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.F2.26.26.1">Gold Correction 3</em>: <span class="ltx_ERROR undefined" id="S4.F2.26.26.2">{CJK}</span>UTF8minIf he [<span class="ltx_text" id="S4.F2.26.26.3" style="color:#FF0000;">was a</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.25.25.m1.1"><semantics id="S4.F2.25.25.m1.1a"><mo id="S4.F2.25.25.m1.1.1" stretchy="false" xref="S4.F2.25.25.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.25.25.m1.1b"><ci id="S4.F2.25.25.m1.1.1.cmml" xref="S4.F2.25.25.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.25.25.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.25.25.m1.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.26.26.4" style="color:#00FF00;">were</span>] Japanese, I suppose I [<span class="ltx_text" id="S4.F2.26.26.5" style="color:#FF0000;">replied</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.26.26.m2.1"><semantics id="S4.F2.26.26.m2.1a"><mo id="S4.F2.26.26.m2.1.1" stretchy="false" xref="S4.F2.26.26.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.26.26.m2.1b"><ci id="S4.F2.26.26.m2.1.1.cmml" xref="S4.F2.26.26.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.26.26.m2.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.26.26.m2.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.26.26.6" style="color:#00FF00;">would reply</span>] like this: "ああ 、駅ならこの道を真っすぐ行けばすぐですよ 、800mくらい先です".</p>
<p class="ltx_p" id="S4.F2.29.29"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="S4.F2.29.29.1">Proposed Correction 3</em>: <span class="ltx_ERROR undefined" id="S4.F2.29.29.2">{CJK}</span>UTF8minIf he was [<span class="ltx_text" id="S4.F2.29.29.3" style="color:#FF0000;">a</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.27.27.m1.1"><semantics id="S4.F2.27.27.m1.1a"><mo id="S4.F2.27.27.m1.1.1" stretchy="false" xref="S4.F2.27.27.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.27.27.m1.1b"><ci id="S4.F2.27.27.m1.1.1.cmml" xref="S4.F2.27.27.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.27.27.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.27.27.m1.1d">→</annotation></semantics></math> <math alttext="\emptyset" class="ltx_Math" display="inline" id="S4.F2.28.28.m2.1"><semantics id="S4.F2.28.28.m2.1a"><mi id="S4.F2.28.28.m2.1.1" mathcolor="#00FF00" mathvariant="normal" xref="S4.F2.28.28.m2.1.1.cmml">∅</mi><annotation-xml encoding="MathML-Content" id="S4.F2.28.28.m2.1b"><emptyset id="S4.F2.28.28.m2.1.1.cmml" xref="S4.F2.28.28.m2.1.1"></emptyset></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.28.28.m2.1c">\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.F2.28.28.m2.1d">∅</annotation></semantics></math>] Japanese, I suppose I [<span class="ltx_text" id="S4.F2.29.29.4" style="color:#FF0000;">replied</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.F2.29.29.m3.1"><semantics id="S4.F2.29.29.m3.1a"><mo id="S4.F2.29.29.m3.1.1" stretchy="false" xref="S4.F2.29.29.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.F2.29.29.m3.1b"><ci id="S4.F2.29.29.m3.1.1.cmml" xref="S4.F2.29.29.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.29.29.m3.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.F2.29.29.m3.1d">→</annotation></semantics></math> <span class="ltx_text" id="S4.F2.29.29.5" style="color:#00FF00;">would reply</span>] like this: "ああ 、駅ならこの道を真っすぐ行けばすぐですよ 、800mくらい先です".</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Three examples of model’s proposed corrections from the CSW test set.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Inference Tweaking and Error Thresholds</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The inference tweaking phase was crucial in tuning the balance between precision and recall. The changes made here, particularly lowering the minimum error thresholds before the model makes an edit, indicated a clear attempt to force the model to make more corrections.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Implementation details are presented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A4" title="Appendix D Inference Hyperparameters ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span> While this slightly lowered precision on monolingual errors, it significantly enhanced the performance on CSW text.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To determine that the improved performance of our proposed model was not entirely due to the different inference configuration, we conducted a similar grid search for the existing GECToR model. However, instead of using the Stage 3 validation dataset, as we did with our model, we used the CSW test set directly. The highest <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><msub id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">𝐹</ci><cn id="S4.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="S4.SS3.p2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> achieved by the baseline model was 56.46, providing evidence that our proposed model beats all inference configurations of the previous GECToR system when applied to CSW texts.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Synthetic Data Impact</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The synthetic CSW text and error injection methods were central to this project. The resemblance of our synthetic text to real ESL learner data, as shown by the similarity metrics in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#S2.T1" title="Table 1 ‣ 2.2 Synthetic CSW GEC Data Generation ‣ 2 Data ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">1</span></a>, is a testament to the effectiveness of our chosen generation method. The improvements in <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">F</mi><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">𝐹</ci><cn id="S4.SS4.p1.1.m1.1.1.3.cmml" type="float" xref="S4.SS4.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> scores provide further evidence of this.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Our extended PIE-synthetic dataset aimed to introduce four error types common in ESL students: noun, pronoun, punctuation and word errors. When compared to the monolingual GECToR, our model is stronger in all of these areas.<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Error type analysis of our model is given in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A5" title="Appendix E Error Type Analysis of Proposed Model ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">E</span></a>.</span></span></span> This provides strong evidence that the targeted approach to error injection was successful in boosting the model’s ability in these areas.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The primary aim of this paper was to build a GEC system capable of effectively correcting English errors in CSW text, whilst maintaining competitive performance on monolingual data. To address the scarcity of CSW data, we explored methods of generating synthetic CSW text. We used several CSW metrics to establish that the LLM prompting-based approach was the most capable of generating text resembling the content in our genuine dataset. From there, we used two error injection methods to create the first substantial datasets labelled for CSW GEC. This significantly expanded the training data available. Importantly, it also opened up opportunities for future research in CSW GEC and CSW NLP more generally. We demonstrated the efficacy of our synthetic data generation techniques by training the first GEC model aimed at correcting errors in CSW texts. Our model showed a clear improvement in performance on CSW data, surpassing the SOTA in this area.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This research, while comprehensive, encounters several limitations that highlight areas for potential improvement. One primary limitation lies in the overrepresentation of Japanese in the genuine CSW dataset. This raises questions about the model’s applicability to a broader range of language pairs. This is an unfortunate consequence of using a dataset sourced from Lang-8, a Japanese language learning network. Although our method demonstrated that it could generate texts from a wider range of language pairs, it is possible that all CSW data used shows a bias towards Japanese styles of CSW. Such a bias in our system could inadvertently lead to reduced accessibility and effectiveness for ESL learners who CSW with languages other than Japanese. If this system were to be used as an aid in ESL education, steps should be taken to ensure that it does not contribute to existing inequalities present in language learning platforms. English learning tools should be accessible regardless of the student’s native language and future work should focus on developing more inclusive datasets to help mitigate these risks.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Another possible limitation relates to the style of model chosen. The sequence tagging method was selected due to its lower data requirements, but this decision may have constrained the capabilities of the model. Many of the errors typical of ESL students require complex restructuring of the sentence - a notably difficult task for edit-based GEC systems. Although the data needs are more substantial, it is likely that NMT GEC systems may fare better as they are not constrained by a limited vocabulary of edits.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">To assess the likeness of our generated CSW text, we introduced several common CSW metrics. Although useful, these metrics are not very sophisticated, and often struggle to accurately capture the nuances of CSW patterns across different subpopulations. These language patterns can have a substantial impact on the optimal approaches to problems across CSW NLP, and hence, the field would benefit from further research in this area. Ideally, we would have conducted a human study to evaluate the quality of our synthetic data. However, given the constraints of the project, it was not possible, and we acknowledge this as a limitation of our work.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Finally, we reported results for a RoBERTa-base GECToR system. Although we also tested other base models, including BERT, DeBERTa and ELECTRA, we did not look at larger models or ensemble systems. Future extensions could explore this area, building upon the observation that larger models or simple voting ensembles can yield better results than the smaller base models <cite class="ltx_cite ltx_citemacro_cite">Tarnavskyi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib26" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">In summary, whilst the current work makes significant contributions to the field of GEC for CSW text, these limitations indicate crucial areas for further research and development.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awasthi et al. (2019)</span>
<span class="ltx_bibblock">
Abhijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh, and Vihari Piratla. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1435" title="">Parallel iterative edit models for local sequence transduction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 4259–4269, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barnett et al. (2000)</span>
<span class="ltx_bibblock">
Ruthanna Barnett, Eva Codó, Eva Eppler, Montse Forcadell, Penelope Gardner-Chloros, Roeland van Hout, Melissa Moyer, Maria Carme Torras, Maria Teresa Turell, Mark Sebba, Marianne Starren, and Sietse Wensing. 2000.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1177/13670069000040020101" title="">The lides coding manual: A document for preparing and analyzing language interaction data version 1.1—july, 1999</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">International Journal of Bilingualism</em>, 4(2):131–132.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beatty-Martínez et al. (2020)</span>
<span class="ltx_bibblock">
Anne L Beatty-Martínez, Christian A Navarro-Torres, and Paola E Dussias. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fpsyg.2020.01699" title="">Codeswitching: A bilingual toolkit for opportunistic speech planning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Frontiers in Psychology</em>, 11:1699.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.14165" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">CoRR</em>, abs/2005.14165.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bryant et al. (2019)</span>
<span class="ltx_bibblock">
Christopher Bryant, Mariano Felice, Øistein E. Andersen, and Ted Briscoe. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W19-4406" title="">The BEA-2019 shared task on grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 52–75, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bryant et al. (2017)</span>
<span class="ltx_bibblock">
Christopher Bryant, Mariano Felice, and Ted Briscoe. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1074" title="">Automatic annotation and evaluation of error types for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 793–805, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2024)</span>
<span class="ltx_bibblock">
Kelvin Wey Han Chan, Christopher Bryant, Li Nguyen, Andrew Caines, and Zheng Yuan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.lrec-main.698" title="">Grammatical error correction for code-switched sentences by learners of English</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>, pages 7926–7938, Torino, Italia. ELRA and ICCL.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chelba et al. (2013)</span>
<span class="ltx_bibblock">
Ciprian Chelba, Tomás Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, and Phillipp Koehn. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1312.3005" title="">One billion word benchmark for measuring progress in statistical language modeling</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em>, abs/1312.3005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dahlmeier et al. (2023)</span>
<span class="ltx_bibblock">
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu, et al. 2023.

</span>
<span class="ltx_bibblock">Nus corpus of learner english (nucle).

</span>
<span class="ltx_bibblock">National University of Singapore, NLP Group.

</span>
<span class="ltx_bibblock">Available: https://www.comp.nus.edu.sg/ nlp/corpora.html.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dou and Neubig (2021)</span>
<span class="ltx_bibblock">
Zi-Yi Dou and Graham Neubig. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-main.181" title="">Word alignment by fine-tuning embeddings on parallel corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 2112–2128, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Falbo and LaCroix (2021)</span>
<span class="ltx_bibblock">
Arianna Falbo and Travis LaCroix. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.08256" title="">Est-ce que vous compute? code-switching, cultural identity, and AI</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/2112.08256.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finlay (2023)</span>
<span class="ltx_bibblock">
P.J. Finlay. 2023.

</span>
<span class="ltx_bibblock">Argos translate.

</span>
<span class="ltx_bibblock">Open-source offline translation library written in Python.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gambäck and Das (2016)</span>
<span class="ltx_bibblock">
Björn Gambäck and Amitava Das. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/L16-1292" title="">Comparing the level of code-switching in corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16)</em>, pages 1850–1855, Portorož, Slovenia. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al. (2017)</span>
<span class="ltx_bibblock">
Souvick Ghosh, Satanu Ghosh, and Dipankar Das. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.13053/cys-21-4-2852" title="">Complexity metric for code-mixed social media text</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Computación y Sistemas</em>, 21(4):693–701.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goh and Barabási (2008)</span>
<span class="ltx_bibblock">
K.-I. Goh and A.-L. Barabási. 2008.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1209/0295-5075/81/48002" title="">Burstiness and memory in complex systems</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Europhysics Letters</em>, 81(4):48002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guzmán et al. (2017)</span>
<span class="ltx_bibblock">
Gualberto Guzmán, Joseph Ricard, Jacqueline Serigos, Barbara E. Bullock, and Almeida Jacqueline Toribio. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.21437/Interspeech.2017-1429" title="">Metrics for Modeling Code-Switching Across Corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proc. Interspeech 2017</em>, pages 67–71.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein et al. (2017)</span>
<span class="ltx_bibblock">
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M. Rush. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-4012" title="">OpenNMT: Open-source toolkit for neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), System Demonstrations</em>, pages 67–72, Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et al. (2012)</span>
<span class="ltx_bibblock">
Philipp Koehn et al. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.statmt.org/europarl/" title="">European parliament proceedings parallel corpus</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manning et al. (2014)</span>
<span class="ltx_bibblock">
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P14-5010" title="">The Stanford CoreNLP natural language processing toolkit</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>, pages 55–60, Baltimore, Maryland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mizumoto et al. (2013)</span>
<span class="ltx_bibblock">
Tomoya Mizumoto, Toshikazu Tajiri, Takuya Fujino, Seiji Kasahara, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://sites.google.com/site/naistlang8corpora" title="">NAIST Lang-8 Learner Corpora</a>.

</span>
<span class="ltx_bibblock">Language Learner Corpora compiled from Lang-8 SNS.

</span>
<span class="ltx_bibblock">Available for research and educational purposes.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. (2022)</span>
<span class="ltx_bibblock">
Li Nguyen, Zheng Yuan, and Graham Seed. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3390/languages7030220" title="">Building educational technologies for code-switching: Current practices, difficulties and future directions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Languages</em>, 7:220.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Omelianchuk et al. (2020)</span>
<span class="ltx_bibblock">
Kostiantyn Omelianchuk, Vitaliy Atrasevych, Artem Chernodub, and Oleksandr Skurzhanskyi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.bea-1.16" title="">GECToR – grammatical error correction: Tag, not rewrite</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 163–170, Seattle, WA, USA → Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rizvi et al. (2021)</span>
<span class="ltx_bibblock">
Mohd Sanad Zaki Rizvi, Anirudh Srinivasan, Tanuja Ganu, Monojit Choudhury, and Sunayana Sitaram. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.eacl-demos.24" title="">GCM: A toolkit for generating synthetic code-mixed text</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</em>, pages 205–211, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rothe et al. (2021)</span>
<span class="ltx_bibblock">
Sascha Rothe, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-short.89" title="">A simple recipe for multilingual grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, pages 702–707, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stahlberg and Kumar (2021)</span>
<span class="ltx_bibblock">
Felix Stahlberg and Shankar Kumar. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.bea-1.4" title="">Synthetic data generation for grammatical error correction with tagged corruption models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 16th Workshop on Innovative Use of NLP for Building Educational Applications</em>, pages 37–47, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarnavskyi et al. (2022)</span>
<span class="ltx_bibblock">
Maksym Tarnavskyi, Artem Chernodub, and Kostiantyn Omelianchuk. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/pdf/2203.13064.pdf" title="">Ensembling and knowledge distilling of large sequence taggers for grammatical error correction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Accepted for publication at 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022)</em>, Dublin, Ireland.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yannakoudakis et al. (2011)</span>
<span class="ltx_bibblock">
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock. 2011.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P11-1019" title="">A new dataset and method for automatically grading ESOL texts</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, pages 180–189, Portland, Oregon, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yow et al. (2018)</span>
<span class="ltx_bibblock">
W. Quin Yow, Jessica S. H. Tan, and Suzanne Flynn. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1017/S1366728917000335" title="">Code-switching as a marker of linguistic competence in bilingual children</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Bilingualism: Language and Cognition</em>, 21(5):1075–1090.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Example LLM Prompt</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A1.F3" title="Figure 3 ‣ Appendix A Example LLM Prompt ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example LLM prompt used to generate synthetic CSW sentences from genuine examples. As we are using a private subset of the Lang-8 dataset, we are not permitted to share any of the CSW texts.</p>
</div>
<figure class="ltx_figure" id="A1.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.1">Settings: [no prose]
<br class="ltx_break"/>For each of the following code-switched sentences, generate a new sentence that uses the same two languages and a similar style of code-switching. The topic should be different. Ensure you use the correct grammar in the English portion of the sentence. Make sure that each sentence contains 2 languages. Only return the sentences and their number. You must follow all of the instructions.
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.2">For example, given the source sentence and label: 
<br class="ltx_break"/>1. This food is called <span class="ltx_ERROR undefined" id="A1.F3.2.1">{CJK}</span>UTF8min
“ラーメン”
.
<br class="ltx_break"/>An acceptable answer would be: 
<br class="ltx_break"/>1. This animal is called a <span class="ltx_ERROR undefined" id="A1.F3.2.2">{CJK}</span>UTF8min
“犬”
.
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.3">Do not include any other information in the generated sentences. The 10 real examples are as follows:
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="A1.F3.4">1. [CSW SENTENCE]
<br class="ltx_break"/>2. [CSW SENTENCE]
<br class="ltx_break"/>3. [CSW SENTENCE]
<br class="ltx_break"/>4. [CSW SENTENCE]
<br class="ltx_break"/>5. [CSW SENTENCE]
<br class="ltx_break"/>6. [CSW SENTENCE]
<br class="ltx_break"/>7. [CSW SENTENCE]
<br class="ltx_break"/>8. [CSW SENTENCE]
<br class="ltx_break"/>9. [CSW SENTENCE]
<br class="ltx_break"/>10.[CSW SENTENCE]
<br class="ltx_break"/></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An Example LLM Prompt Used to Generate CSW Text</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Error Type Analysis of SOTA</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A2.T3" title="Table 3 ‣ Appendix B Error Type Analysis of SOTA ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">3</span></a> shows a breakdown of the performance of a single RoBERTa Large-based GECToR system trained purely on monolingual GEC data when applied to two datasets, our genuine CSW dataset and the BEA-2019 <cite class="ltx_cite ltx_citemacro_cite">Bryant et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib5" title="">2019</a>)</cite> test set. These datasets are approximately the same size. The model used represents a current near-SOTA single model sequence tagging-based GEC system measured using <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="A2.p1.1.m1.1"><semantics id="A2.p1.1.m1.1a"><msub id="A2.p1.1.m1.1.1" xref="A2.p1.1.m1.1.1.cmml"><mi id="A2.p1.1.m1.1.1.2" xref="A2.p1.1.m1.1.1.2.cmml">F</mi><mn id="A2.p1.1.m1.1.1.3" xref="A2.p1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A2.p1.1.m1.1b"><apply id="A2.p1.1.m1.1.1.cmml" xref="A2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.p1.1.m1.1.1.1.cmml" xref="A2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.p1.1.m1.1.1.2.cmml" xref="A2.p1.1.m1.1.1.2">𝐹</ci><cn id="A2.p1.1.m1.1.1.3.cmml" type="float" xref="A2.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p1.1.m1.1c">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> on the BEA-2019 test set. For brevity, we have removed categories with a low number of examples in either dataset or where performance is not significantly different.</p>
</div>
<figure class="ltx_table" id="A2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T3.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r" id="A2.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.2.1">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="A2.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.3.1">BEA-2019 Test</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="A2.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.4.1">Genuine CSW</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.1.1.1.1">
<math alttext="\text{F}_{0.5}" class="ltx_Math" display="inline" id="A2.T3.1.1.1.1.m1.1"><semantics id="A2.T3.1.1.1.1.m1.1a"><msub id="A2.T3.1.1.1.1.m1.1.1" xref="A2.T3.1.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="A2.T3.1.1.1.1.m1.1.1.2" xref="A2.T3.1.1.1.1.m1.1.1.2a.cmml">F</mtext><mn id="A2.T3.1.1.1.1.m1.1.1.3" xref="A2.T3.1.1.1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.1.m1.1b"><apply id="A2.T3.1.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T3.1.1.1.1.m1.1.1.1.cmml" xref="A2.T3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A2.T3.1.1.1.1.m1.1.1.2a.cmml" xref="A2.T3.1.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="A2.T3.1.1.1.1.m1.1.1.2.cmml" xref="A2.T3.1.1.1.1.m1.1.1.2">F</mtext></ci><cn id="A2.T3.1.1.1.1.m1.1.1.3.cmml" type="float" xref="A2.T3.1.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.1.m1.1c">\text{F}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.1.1.1.1.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.1.1"> Decrease</span>
</th>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r" id="A2.T3.3.3.3.3"></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.2.2.2.1"><math alttext="\text{F}_{0.5}" class="ltx_Math" display="inline" id="A2.T3.2.2.2.1.m1.1"><semantics id="A2.T3.2.2.2.1.m1.1a"><msub id="A2.T3.2.2.2.1.m1.1.1" xref="A2.T3.2.2.2.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="A2.T3.2.2.2.1.m1.1.1.2" xref="A2.T3.2.2.2.1.m1.1.1.2a.cmml">F</mtext><mn id="A2.T3.2.2.2.1.m1.1.1.3" xref="A2.T3.2.2.2.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.2.1.m1.1b"><apply id="A2.T3.2.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T3.2.2.2.1.m1.1.1.1.cmml" xref="A2.T3.2.2.2.1.m1.1.1">subscript</csymbol><ci id="A2.T3.2.2.2.1.m1.1.1.2a.cmml" xref="A2.T3.2.2.2.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="A2.T3.2.2.2.1.m1.1.1.2.cmml" xref="A2.T3.2.2.2.1.m1.1.1.2">F</mtext></ci><cn id="A2.T3.2.2.2.1.m1.1.1.3.cmml" type="float" xref="A2.T3.2.2.2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.2.1.m1.1c">\text{F}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.2.2.1.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.3.3.3.4"><span class="ltx_text ltx_font_bold" id="A2.T3.3.3.3.4.1">TP</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.3.3.3.5"><span class="ltx_text ltx_font_bold" id="A2.T3.3.3.3.5.1">FP</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r" id="A2.T3.3.3.3.6"><span class="ltx_text ltx_font_bold" id="A2.T3.3.3.3.6.1">FN</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.3.3.3.2"><math alttext="\text{F}_{0.5}" class="ltx_Math" display="inline" id="A2.T3.3.3.3.2.m1.1"><semantics id="A2.T3.3.3.3.2.m1.1a"><msub id="A2.T3.3.3.3.2.m1.1.1" xref="A2.T3.3.3.3.2.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="A2.T3.3.3.3.2.m1.1.1.2" xref="A2.T3.3.3.3.2.m1.1.1.2a.cmml">F</mtext><mn id="A2.T3.3.3.3.2.m1.1.1.3" xref="A2.T3.3.3.3.2.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.3.2.m1.1b"><apply id="A2.T3.3.3.3.2.m1.1.1.cmml" xref="A2.T3.3.3.3.2.m1.1.1"><csymbol cd="ambiguous" id="A2.T3.3.3.3.2.m1.1.1.1.cmml" xref="A2.T3.3.3.3.2.m1.1.1">subscript</csymbol><ci id="A2.T3.3.3.3.2.m1.1.1.2a.cmml" xref="A2.T3.3.3.3.2.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="A2.T3.3.3.3.2.m1.1.1.2.cmml" xref="A2.T3.3.3.3.2.m1.1.1.2">F</mtext></ci><cn id="A2.T3.3.3.3.2.m1.1.1.3.cmml" type="float" xref="A2.T3.3.3.3.2.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.3.2.m1.1c">\text{F}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.3.3.3.2.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.3.3.3.7"><span class="ltx_text ltx_font_bold" id="A2.T3.3.3.3.7.1">TP</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A2.T3.3.3.3.8"><span class="ltx_text ltx_font_bold" id="A2.T3.3.3.3.8.1">FP</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r" id="A2.T3.3.3.3.9"><span class="ltx_text ltx_font_bold" id="A2.T3.3.3.3.9.1">FN</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="A2.T3.3.3.3.10"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.3.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A2.T3.3.3.4.1.1">DET</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.2">80.45</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.3">432</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.4">80</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A2.T3.3.3.4.1.5">205</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.6">46.27</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.7">472</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.8">351</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A2.T3.3.3.4.1.9">1336</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A2.T3.3.3.4.1.10">34.18</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.5.2.1">NOUN</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.2">47.85</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.3">29</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.4">16</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.5.2.5">94</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.6">4.34</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.7">21</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.8">147</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.5.2.9">1725</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.5.2.10">43.51</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.6.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.6.3.1">ORTH</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.2">75.96</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.3">201</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.4">30</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.6.3.5">198</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.6">36.45</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.7">181</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.8">264</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.6.3.9">522</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.6.3.10">39.51</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.7.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.7.4.1">OTHER</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.2">39.51</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.3">113</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.4">77</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.7.4.5">557</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.6">3.55</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.7">39</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.8">241</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.7.4.9">4333</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.7.4.10">35.96</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.8.5.1">PREP</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.2">75.44</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.3">263</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.4">58</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.8.5.5">196</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.6">39.14</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.7">241</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.8">251</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.8.5.9">870</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.8.5.10">36.30</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.9.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.9.6.1">PRON</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.2">66.38</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.3">62</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.4">19</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.9.6.5">81</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.6">20.71</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.7">21</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.8">32</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.9.6.9">274</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.9.6.10">45.67</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.10.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.10.7.1">PUNCT</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.2">80.93</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.3">786</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.4">165</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.10.7.5">266</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.6">0.35</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.7">1</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.8">286</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.10.7.9">284</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.10.7.10">80.58</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.11.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.11.8.1">VERB</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.2">52.59</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.3">61</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.4">27</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.11.8.5">167</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.6">18.08</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.7">49</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.8">77</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.11.8.9">802</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.11.8.10">34.51</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.12.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.12.9.1">VERB:FORM</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.2">81.62</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.3">151</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.4">30</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.12.9.5">50</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.6">37.20</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.7">61</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.8">90</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.12.9.9">155</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.12.9.10">44.42</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.13.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.13.10.1">VERB:SVA</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.2">88.64</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.3">128</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.4">14</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.13.10.5">26</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.6">57.58</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.7">114</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.8">79</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.13.10.9">104</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.13.10.10">31.06</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.14.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="A2.T3.3.3.14.11.1">VERB:TENSE</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.2">65.55</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.3">145</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.4">62</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.14.11.5">133</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.6">33.80</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.7">116</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.8">172</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="A2.T3.3.3.14.11.9">448</td>
<td class="ltx_td ltx_align_right" id="A2.T3.3.3.14.11.10">31.75</td>
</tr>
<tr class="ltx_tr" id="A2.T3.3.3.15.12">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="A2.T3.3.3.15.12.1">WO</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.2">58.08</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.3">23</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.4">5</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="A2.T3.3.3.15.12.5">63</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.6">6.33</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.7">2</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.8">11</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r" id="A2.T3.3.3.15.12.9">104</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="A2.T3.3.3.15.12.10">51.75</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><math alttext="F_{0.5}" class="ltx_Math" display="inline" id="A2.T3.6.m1.1"><semantics id="A2.T3.6.m1.1b"><msub id="A2.T3.6.m1.1.1" xref="A2.T3.6.m1.1.1.cmml"><mi id="A2.T3.6.m1.1.1.2" xref="A2.T3.6.m1.1.1.2.cmml">F</mi><mn id="A2.T3.6.m1.1.1.3" xref="A2.T3.6.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A2.T3.6.m1.1c"><apply id="A2.T3.6.m1.1.1.cmml" xref="A2.T3.6.m1.1.1"><csymbol cd="ambiguous" id="A2.T3.6.m1.1.1.1.cmml" xref="A2.T3.6.m1.1.1">subscript</csymbol><ci id="A2.T3.6.m1.1.1.2.cmml" xref="A2.T3.6.m1.1.1.2">𝐹</ci><cn id="A2.T3.6.m1.1.1.3.cmml" type="float" xref="A2.T3.6.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.6.m1.1d">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.6.m1.1e">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> Scores, TP, FP, FN, and Differences in <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="A2.T3.7.m2.1"><semantics id="A2.T3.7.m2.1b"><msub id="A2.T3.7.m2.1.1" xref="A2.T3.7.m2.1.1.cmml"><mi id="A2.T3.7.m2.1.1.2" xref="A2.T3.7.m2.1.1.2.cmml">F</mi><mn id="A2.T3.7.m2.1.1.3" xref="A2.T3.7.m2.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A2.T3.7.m2.1c"><apply id="A2.T3.7.m2.1.1.cmml" xref="A2.T3.7.m2.1.1"><csymbol cd="ambiguous" id="A2.T3.7.m2.1.1.1.cmml" xref="A2.T3.7.m2.1.1">subscript</csymbol><ci id="A2.T3.7.m2.1.1.2.cmml" xref="A2.T3.7.m2.1.1.2">𝐹</ci><cn id="A2.T3.7.m2.1.1.3.cmml" type="float" xref="A2.T3.7.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.7.m2.1d">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.7.m2.1e">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> Scores (BEA - CSW) for Different Categories in the BEA-2019 Test Split and our Genuine CSW Dataset.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Training Data Schedule</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section, we explicitly detail the data used at each stage of the training process.</p>
</div>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Stage 1</h5>
<div class="ltx_para" id="A3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px1.p1.1">For the initial pre-training stage, we used the distilled dataset proposed by the SOTA <cite class="ltx_cite ltx_citemacro_cite">Tarnavskyi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib26" title="">2022</a>)</cite>. This dataset was constructed by extracting corrections from the monolingual 1BW corpus <cite class="ltx_cite ltx_citemacro_cite">Chelba et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib8" title="">2013</a>)</cite> using the highest performing GECToR ensemble. Through this dataset, we shuffled our PIE-synthetic CSW dataset. We deemed this dataset to be of lower quality than its Rev-GECToR counterpart. Consequently, it was used earlier in the training process. This provided roughly 1,200,000 examples for the initial training phase of which we split between train and validation sets according to a ratio of 19:1. Our synthetic CSW sentences comprised approximately 5.65% of this dataset. We aimed to keep this percentage small in this phase of the training process to allow the model to first learn to correct errors in monolingual texts. In later stages, we boosted the contribution of the CSW data.</p>
</div>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Stage 2</h5>
<div class="ltx_para" id="A3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px2.p1.1">For the second stage, we shuffled several GEC datasets. These are NUCLE <cite class="ltx_cite ltx_citemacro_cite">Dahlmeier et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib9" title="">2023</a>)</cite>, FCE <cite class="ltx_cite ltx_citemacro_cite">Yannakoudakis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib27" title="">2011</a>)</cite>, W&amp;I Locness <cite class="ltx_cite ltx_citemacro_cite">Bryant et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib5" title="">2019</a>)</cite>, Lang-8 <cite class="ltx_cite ltx_citemacro_cite">Mizumoto et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#bib.bib20" title="">2013</a>)</cite> and our 2 newly created CSW datasets. As our genuine CSW dataset is a subset of the private Lang-8 corpus, we checked and removed any duplicates. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3.T4" title="Table 4 ‣ Stage 2 ‣ Appendix C Training Data Schedule ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">4</span></a> shows the overall contributions of each corpus towards the stage 2 dataset. Similar to the previous stage, the data was split into train and validation sets.</p>
</div>
<figure class="ltx_table" id="A3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A3.T4.1.1.1.1" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A3.T4.1.1.1.2" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="A3.T4.1.1.1.2.1">Sentences</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T4.1.2.1.1">Lang-8</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A3.T4.1.2.1.2">985,683 <span class="ltx_text ltx_font_italic" id="A3.T4.1.2.1.2.1">(80.54%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.1.3.2.1">W&amp;I Locness</th>
<td class="ltx_td ltx_align_right" id="A3.T4.1.3.2.2">68,608 <span class="ltx_text ltx_font_italic" id="A3.T4.1.3.2.2.1">(5.61%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.1.4.3.1">NUCLE</th>
<td class="ltx_td ltx_align_right" id="A3.T4.1.4.3.2">54,258 <span class="ltx_text ltx_font_italic" id="A3.T4.1.4.3.2.1">(4.43%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.1.5.4.1">FCE</th>
<td class="ltx_td ltx_align_right" id="A3.T4.1.5.4.2">26,929 <span class="ltx_text ltx_font_italic" id="A3.T4.1.5.4.2.1">(2.20%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T4.1.6.5.1">Syn-CSW PIE</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A3.T4.1.6.5.2">70,181 <span class="ltx_text ltx_font_italic" id="A3.T4.1.6.5.2.1">(5.73%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T4.1.7.6.1">Syn-CSW Rev-GECToR</th>
<td class="ltx_td ltx_align_right" id="A3.T4.1.7.6.2">18,160 <span class="ltx_text ltx_font_italic" id="A3.T4.1.7.6.2.1">(1.48%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T4.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A3.T4.1.8.7.1">Total</th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="A3.T4.1.8.7.2">1,223,819</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Sentence Count and Contribution of Stage 2 Datasets</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="A3.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Stage 3</h5>
<div class="ltx_para" id="A3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A3.SS0.SSS0.Px3.p1.1">For the final stage, we combined the high quality W&amp;I Locness dataset with a sampled subset of the genuine CSW data and a sampled subset of the synthetic CSW texts. Again, the stage 3 dataset is split into train and validation sets. The remaining unused subset of the genuine CSW dataset was retained for testing purposes. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A3.T5" title="Table 5 ‣ Stage 3 ‣ Appendix C Training Data Schedule ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">5</span></a> details the contributions to this stage from each dataset.</p>
</div>
<figure class="ltx_table" id="A3.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A3.T5.1.1.1.1" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="A3.T5.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="A3.T5.1.1.1.2" style="padding-bottom:2.15277pt;"><span class="ltx_text ltx_font_bold" id="A3.T5.1.1.1.2.1">Sentences</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T5.1.2.1.1">W&amp;I Locness</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A3.T5.1.2.1.2">68,608 <span class="ltx_text ltx_font_italic" id="A3.T5.1.2.1.2.1">(67.23%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T5.1.3.2.1">Syn-CSW Rev-GECToR</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A3.T5.1.3.2.2">18,160 <span class="ltx_text ltx_font_italic" id="A3.T5.1.3.2.2.1">(17.80%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T5.1.4.3.1">Syn-CSW PIE</th>
<td class="ltx_td ltx_align_right" id="A3.T5.1.4.3.2">10,000 <span class="ltx_text ltx_font_italic" id="A3.T5.1.4.3.2.1">(9.80%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T5.1.5.4.1">CSW Genuine</th>
<td class="ltx_td ltx_align_right" id="A3.T5.1.5.4.2">5,279 <span class="ltx_text ltx_font_italic" id="A3.T5.1.5.4.2.1">(5.17%)</span>
</td>
</tr>
<tr class="ltx_tr" id="A3.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t" id="A3.T5.1.6.5.1">Total</th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="A3.T5.1.6.5.2">102,047</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Sentence Count and Contribution of Stage 3 Datasets</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Inference Hyperparameters</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">After training our model, we used the validation dataset from stage 3 to tune 2 inference parameters. These are:</p>
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1">additional_confidence — This value is added to the probability of the <span class="ltx_text ltx_font_italic" id="A4.I1.i1.p1.1.1">$KEEP</span> token. If this value is high, recall is likely to decrease and precision increase. The grid search found the best value of this to be 0.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1">min_error_probability — For a change to be made to a sentence, the probability of at least one token in the sentence being an error must be higher than the min_error_probability. If this value is high, then precision is likely to be higher and recall lower. The grid search found the best value of this to be 0.4.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Error Type Analysis of Proposed Model</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">By exploring the ERRANT error classifications of our proposed model when applied to the CSW test dataset, we can further explore the effectiveness of our synthetic data in addressing the problematic areas identified in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A2" title="Appendix B Error Type Analysis of SOTA ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">B</span></a>. A breakdown of the precision, recall and <math alttext="\text{F}_{0.5}" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><msub id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><mtext id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2a.cmml">F</mtext><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1">subscript</csymbol><ci id="A5.p1.1.m1.1.1.2a.cmml" xref="A5.p1.1.m1.1.1.2"><mtext id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2">F</mtext></ci><cn id="A5.p1.1.m1.1.1.3.cmml" type="float" xref="A5.p1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">\text{F}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> score for each of the previously identified categories is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.10349v1#A5.T6" title="Table 6 ‣ Appendix E Error Type Analysis of Proposed Model ‣ LLM-based Code-Switched Text Generation for Grammatical Error Correction"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_table" id="A5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="A5.T6.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T6.1.1.2.1">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A5.T6.1.1.3"><span class="ltx_text ltx_font_bold" id="A5.T6.1.1.3.1">P</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A5.T6.1.1.4"><span class="ltx_text ltx_font_bold" id="A5.T6.1.1.4.1">R</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A5.T6.1.1.1"><math alttext="\text{{F}}_{0.5}" class="ltx_Math" display="inline" id="A5.T6.1.1.1.m1.1"><semantics id="A5.T6.1.1.1.m1.1a"><msub id="A5.T6.1.1.1.m1.1.1" xref="A5.T6.1.1.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="A5.T6.1.1.1.m1.1.1.2" xref="A5.T6.1.1.1.m1.1.1.2a.cmml">F</mtext><mn id="A5.T6.1.1.1.m1.1.1.3" xref="A5.T6.1.1.1.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T6.1.1.1.m1.1b"><apply id="A5.T6.1.1.1.m1.1.1.cmml" xref="A5.T6.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A5.T6.1.1.1.m1.1.1.1.cmml" xref="A5.T6.1.1.1.m1.1.1">subscript</csymbol><ci id="A5.T6.1.1.1.m1.1.1.2a.cmml" xref="A5.T6.1.1.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="A5.T6.1.1.1.m1.1.1.2.cmml" xref="A5.T6.1.1.1.m1.1.1.2">F</mtext></ci><cn id="A5.T6.1.1.1.m1.1.1.3.cmml" type="float" xref="A5.T6.1.1.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T6.1.1.1.m1.1c">\text{{F}}_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A5.T6.1.1.1.m1.1d">F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T6.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T6.1.2.1.1">NOUN</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T6.1.2.1.2">0.2857</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T6.1.2.1.3">0.0833</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T6.1.2.1.4">0.1923</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T6.1.3.2.1">PRON</th>
<td class="ltx_td ltx_align_center" id="A5.T6.1.3.2.2">0.7647</td>
<td class="ltx_td ltx_align_center" id="A5.T6.1.3.2.3">0.4643</td>
<td class="ltx_td ltx_align_center" id="A5.T6.1.3.2.4">0.6771</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T6.1.4.3.1">PUNCT</th>
<td class="ltx_td ltx_align_center" id="A5.T6.1.4.3.2">0.7143</td>
<td class="ltx_td ltx_align_center" id="A5.T6.1.4.3.3">0.1139</td>
<td class="ltx_td ltx_align_center" id="A5.T6.1.4.3.4">0.3460</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A5.T6.1.5.4.1">WO</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T6.1.5.4.2">0.7778</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T6.1.5.4.3">0.2000</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T6.1.5.4.4">0.4930</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Precision (P), Recall (R) and <math alttext="F_{0.5}" class="ltx_Math" display="inline" id="A5.T6.3.m1.1"><semantics id="A5.T6.3.m1.1b"><msub id="A5.T6.3.m1.1.1" xref="A5.T6.3.m1.1.1.cmml"><mi id="A5.T6.3.m1.1.1.2" xref="A5.T6.3.m1.1.1.2.cmml">F</mi><mn id="A5.T6.3.m1.1.1.3" xref="A5.T6.3.m1.1.1.3.cmml">0.5</mn></msub><annotation-xml encoding="MathML-Content" id="A5.T6.3.m1.1c"><apply id="A5.T6.3.m1.1.1.cmml" xref="A5.T6.3.m1.1.1"><csymbol cd="ambiguous" id="A5.T6.3.m1.1.1.1.cmml" xref="A5.T6.3.m1.1.1">subscript</csymbol><ci id="A5.T6.3.m1.1.1.2.cmml" xref="A5.T6.3.m1.1.1.2">𝐹</ci><cn id="A5.T6.3.m1.1.1.3.cmml" type="float" xref="A5.T6.3.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.T6.3.m1.1d">F_{0.5}</annotation><annotation encoding="application/x-llamapun" id="A5.T6.3.m1.1e">italic_F start_POSTSUBSCRIPT 0.5 end_POSTSUBSCRIPT</annotation></semantics></math> Score of Our Proposed Model for Targeted Error Types in the Genuine CSW Test Dataset</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Oct 14 10:51:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
