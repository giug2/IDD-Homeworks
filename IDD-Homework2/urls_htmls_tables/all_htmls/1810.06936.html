<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1810.06936] UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation</title><meta property="og:description" content="Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1810.06936">

<!--Generated on Sat Mar  2 00:28:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Robotics,  Synthetic Data,  Grasping,  Virtual Reality,  Simulation.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">UnrealROX
<br class="ltx_break">An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pablo Martinez-Gonzalez,
Sergiu Oprea,
Alberto Garcia-Garcia,
Alvaro Jover-Alvarez,
Sergio Orts-Escolano,
and Jose Garcia-Rodriguez
</span><span class="ltx_author_notes">P. Martinez-Gonzalez, S. Oprea, A. Garcia-Garcia, A. Jover-Alvarez, S. Orts-Escolano, and J. Garcia-Rodriguez are with the 3D Perception Lab (http://labs.iuii.ua.es/3dperceptionlab) at the University of Alicante .
<br class="ltx_break">E-mail: pmartinez@dtic.ua.es, soprea@dtic.ua.es, agarcia@dtic.ua.es, ajover@dtic.ua.es, sorts@ua.es, jgarcia@dtic.ua.es</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. Those problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Robotics, Synthetic Data, Grasping, Virtual Reality, Simulation.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Vision-based robotics tasks have made a huge leap forward mainly due to the development of machine learning techniques (e.g. deep architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> such as Convolutional Neural Networks or Recurrent Neural Networks) which are continuously rising the performance bar for various problems such as semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, and visual grasping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> among others. Those data-driven methods are in need of vast amounts of annotated samples to achieve those exceptional results. Gathering that sheer quantity of images with ground truth is a tedious, expensive, and sometimes nearby impossible task in the real world. On the contrary, synthetic environments streamline the data generation process and are usually able to automatically provide annotations for various tasks. Because of that, simulated environments are becoming increasingly popular and widely used to train those models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Learning on virtual or simulated worlds allows faster, low-cost, and more scalable data collection. However, synthetic environments face a huge obstacle to be actually useful despite their inherent advantages: models trained in that simulated domain must also be able to perform properly on real-world test scenarios which often feature numerous discrepancies between them and their synthetic counterparts. That set of differences is widely known as the reality gap. In most cases, this gap is big enough so that transferring knowledge from one domain to another is an extremely difficult task either because renderers are not able to produce images like real-world sensors (due to the implicit noise or the richness of the scene) or either the physical behavior of the scene elements and sensors is not as accurate as it should be.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In order to address this reality gap, two methods have been proven to be effective: extreme realism and domain randomization. On the one hand, extreme realism refers to the process of making the simulation as similar as the real-world environment in which the robot will be deployed as possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. That can be achieved through a combination of various techniques, e.g., photorealistic rendering (which implies realistic geometry, textures, lighting and also simulating camera-specific noise, distortion and other parameters) and accurate physics (complex collisions with high-fidelity calculations). On the other hand, domain randomization is a kind of domain adaptation technique that aims for exposing the model to a huge range of simulated environments at training time instead of just to a single synthetic one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. By doing that, and if the variability is enough, the model will be able to identify the real world as just another variation thus being able to generalize <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we propose an extremely photorealistic virtual reality environment for generating synthetic data for various robotic vision tasks. In such environment, a human operator can be embodied, in virtual reality, as a robot agent inside a scene to freely navigate and interact with objects as if it was a real-world robot. Our environment is built on top of  <span title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Unreal Engine 4</span></span> (<abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>) to take advantage of its advanced  <span title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Virtual Reality</span></span> (<abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr>), rendering, and physics capabilities. Our system provides the following features: (1) a visually plausible grasping system for robot manipulation which is modular enough to be applied to various finger configurations, (2) routines for controlling robotic hands and bodies with commercial <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> setups such as Oculus Rift and HTC Vive Pro, (3) a sequence recorder component to store all the information about the scene, robot, and cameras while the human operator is embodied as a robot, (4) a sequence playback component to reproduce the previously recorded sequence offline to generate raw data such as RGB, depth, normals, or instance segmentation images, (5) a multi-camera component to ease the camera placement process and enable the user to attach them to specific robot joints and configure their parameters (resolution, noise model, field of view), and (6) open-source code, assets, and tutorials for all those components and other subsystems that tie them together.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper is organized as follows. Section <a href="#S2" title="2 Related Works ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> analyzes already existing environments for synthetic data generation and puts our proposal in context. Next, Section <a href="#S3" title="3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes our proposal and provides in-depth details for each one of its components. After that, we briefly discuss application scenarios for our environment in Section <a href="#S4" title="4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. At last, in Section <a href="#S6" title="6 Conclusion ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we draw conclusions about this work and in Section <a href="#S7" title="7 Limitations and Future Works ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we go over current limitations of our work and propose future works to improve it.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic environments have been used for a long time to benchmark vision and robotic algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Recently, their importance has been highlighted for training and evaluating machine learning models for robotic vision problems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Due to the increasing need for samples to train such data-driven architectures, there exists an increasing number of synthetic datasets, environments, and simulation platforms to generate data for indoor robotic tasks and evaluate those learned models. In this section, we briefly review the most relevant ones according to the scope of our proposal. We describe both the most important features and main flaws for the following works: <abbr title="Cornell House Agent Learning Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CHALET</span></abbr>, <abbr title="Household Multimodal Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HoME</span></abbr>, AI2-<abbr title="THe House of inteRactions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">THOR</span></abbr>, and <abbr title="Multimodal Indoor Simulator" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MINOS</span></abbr>. In addition, we also describe two other related tools such as UnrealCV, Gazebo, and NVIDIA’s Isaac Sim which are not strictly similar but relevant enough to be mentioned. At last, we put our proposal in context taking into account all the analyzed strong points and weaknesses.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span title="Cornell House Agent Learning Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Cornell House Agent Learning Environment</span></span> (<abbr title="Cornell House Agent Learning Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">CHALET</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> is a 3D house simulator for manipulation and navigation learning. It is built upon Unity 3D so it supports physics and interactions with objects and the scene itself thanks to its built-in physics engine. CHALET features three modes of operation: standalone (navigate with keyboard and mouse input), replay (reproduce the trajectory generated on standalone mode), and client (use the framework’s API to control the agent and obtain information). On the other hand, CHALET presents various weak points such as its lack of realism, the absence of a robot’s body or mesh, and the limitation in the number of cameras.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span title="Household Multimodal Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Household Multimodal Environment</span></span> (<abbr title="Household Multimodal Environment" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HoME</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is a multimodal household environment for AI learning from visual, auditive, and physical information within realistic synthetic environments sourced from SUNCG. HoME provides RGB, depth, and semantic maps based on 3D renderings produced by Panda3D, acoustic renderings based on EVERT, language descriptions of objects, and physics simulations based on Bullet. It also provides a Python framework compatible with OpenAI gym. However, HoME is not anywhere close to photorealism, there is no phyisical representation of the robot itself, and interactions are discrete.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">AI2- <span title="THe House of inteRactions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">THe House of inteRactions</span></span> (<abbr title="THe House of inteRactions" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">THOR</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is a framework for visual AI research which consists of near-photorealistic synthetic 3D indoor scenes in which agents can navigate and change the state of actionable objects. It is built over Unity so it also integrates a physics engine which enables modeling complex physical interactions. The framework also provides a Python interface to communicate with the engine through HTTP commands to control the agent and obtain visual information and annotations. Some of the weaknesses of this environment are the lack of a 3D robot model and hands, only a first-person view camera, and the discrete nature of its actions with binary states.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p"><span title="Multimodal Indoor Simulator" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Multimodal Indoor Simulator</span></span> (<abbr title="Multimodal Indoor Simulator" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MINOS</span></abbr>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is a simulator for navigation in complex indoor environments. An agent, represented by a cylinder proxy geometry, is able to navigate (in a discrete or continuous way) on scenes sourced from existing synthetic and reconstructed datasets of indoor scenes such as SUNCG and Matterport respectively. Such agent can obtain information from multimodal sensory inputs: RGB, depth, surface normals, contact forces, semantic segmentation, and various egocentric measurements such as velocity and acceleration. The simulator provides both Python and web client APIs to control the agent and set the parameters of the scene. However, this simulator lacks some features such as a fully 3D robot model instead of a geometry proxy, photorealism, configurable cameras and points of view, and interactions with the scene.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Other Tools and Environments</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Although not strictly related, we would like to remark a couple of tools from which we drew inspiration to shape our proposal: UnrealCV, Gazebo, and NVIDIA’s Isaac Sim.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">On the one hand, UnrealCV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> is a project that extends <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> to create virtual worlds and ease communication with computer vision applications. UnrealCV consists of two parts: server and client. The server is a plugin that runs embedded into an <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> game. It uses sockets to listen to high-level UnrealCV commands issued by a client and communicates with UE4 through its C++ API to provide advanced functionality for each command, e.g., rendering per-instance segmentation masks. The client is a Python API which communicates with the server using plain text protocol. It just sends those available commands to the server and waits for a response. A detailed list of commands can be consulted in the official documentation of the plugin. We took the main concept and design behind UnrealCV and implemented the whole pipeline inside <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> itself to be more efficient and customizable. Another framework which helped us design our environment was Gazebo <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://http://gazebosim.org/</span></span></span></span> a well-known robot simulator that enables accurate and efficient simulation of robots in indoor and outdoor environments. It integrates a robust physics engine (Bullet, ODE, Simbody, and DART), advanced 3D graphics (using OGRE), and sensors and noise modelling. On the other hand, NVIDIA’s Isaac Sim is a yet to be released virtual simulator for robotics that lets developers train and test their robot software using highly realistic virtual simulation environments. However, its software development kit is still in early access at the time this work was carried out.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Our Proposal in Context</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">After analyzing the strong points and weaknesses of the most popular indoor robotic environments, we aimed to combine the strengths of all of them while addressing their weaknesses and introducing new features. In this regard, our work focuses on simulating a wide range of common indoor robot actions, both in terms of poses and object interactions, by leveraging a human operator to generate plausible trajectories and grasps in virtual reality. To the best of our knowledge, this is the first extremely photorealistic environment for robotic vision in which interactions and movements can be realistically simulated in virtual reality. Furthermore, we make possible the generation of raw data (RGB-D/3D/Stereo) and ground truth (2D/3D class and instance segmentation, 6D poses, and 2D/3D bounding boxes) for many vision problems. Although UnrealCV is fairly similar to our work, since both aim to connect Unreal Engine and computer vision/robotics, we took radically different design decisions: while its architecture is a Python client/server, ours is contained entirely inside <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> in C++. That architecture allows us to place objects, cameras, and skeletons, and generate images in a more efficient way than other frameworks. Finally, the whole pipeline and tools are released as open-source software with extensive documentation<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/3dperceptionlab/unrealrox</span></span></span></span>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">System</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The rendering engine we chose to generate photorealistic RGB images and immerse the agent in <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> is  <span title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Unreal Engine 4</span></span> (<abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>). The reasons for this choice are the following ones: (1) it is arguably one of the best game engines able to produce extremely realistic renderings, (2) beyond gaming, it has become widely adopted by <span title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Virtual Reality</span></span> developers and indoor/architectural visualization experts so a whole lot of tools, examples, documentation, and assets are available; (3) due to its impact across various communities, many hardware solutions offer plugins for <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> that make them work out-of-the-box; and (4) Epic Games provides the full C++ source code and updates to it so the full suite can be used and easily modified for free. Arguably, the most attractive feature of <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> that made us take that decision is its capability to render photorealistic scenes like the one shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Some UE4 features that enable this realism are: physically-based materials, pre-calculated bounce light via Lightmass, stationary lights, post-processing, and reflections.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1810.06936/assets/realistic_rendering.jpg" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="160" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1810.06936/assets/realistic_rendering_night.jpg" id="S3.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="269" height="160" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.4.2" class="ltx_text" style="font-size:90%;">Snapshots of the daylight and night room setup for the <em id="S3.F1.4.2.1" class="ltx_emph ltx_font_italic">Realistic Rendering</em> released by Epic Games to showcase the realistic rendering capabilities of <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">It is also important to remark that we do have strict real-time constraints for rendering since we need to immerse a human agent in virtual reality, i.e., we require extremely realistic and complex scenes rendered at very high framerates (usually more than 80 FPS). By design, <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> is engineered for virtual reality so it provides a specific rendering solution for it named Forward Renderer. That renderer is able to generate images that meet our quality standards at 90 FPS thanks to high-quality lighting features,  <span title="Multi-Sample Anti-Aliasing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Multi-Sample Anti-Aliasing</span></span> (<abbr title="Multi-Sample Anti-Aliasing" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">MSAA</span></abbr>), and instanced stereo rendering.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The whole system is built over <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> taking advantage of various existing features, extending certain ones with to suit our specific needs, and implementing others from scratch to devise a more efficient and cleaner project that abides to software design principles. A general overview of our proposal is shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In this section we describe each one of the subsystems that our proposal is composed of: robotic pawns, controller, <abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr>, grasping, multi-camera, recording, and playback.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/1810.06936/assets/diagramsimple.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="529" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">System diagram showing the various subsystems and their abstract relationships: Robotic Pawn, Controller, <abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr>, Multi-camera, Grasping, Recording, and Playback.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Robotic Pawns</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">One of the most important parts of the system is the representation of the robots in the virtual environment. Robots are represented by the mesh that models them, the control and movement logic, the animations that it triggers, and the grasping system (explained later in its corresponding section). To encapsulate all this, we have created a base class that contains all the common behavior that any robot would have in our system, which can then be extended by child classes that implement specific things such as the mesh or the configuration of the fingers for the grasping system. Using that encapsulation, we introduced two sample robots in our environment: <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>’s mannequin and Aldebaran’s Pepper (see Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Robotic Pawns ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/pepper_bg.jpg" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="242" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/mannequin_bg.jpg" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="293" height="242" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Pepper and Mannequin integrated with colliders and constraints.</span></figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> there is a hierarchy of predefined classes ready to work together that should be used properly in order to take advantage of the facilities offered by the engine. For example, any element that we want to place in a scene must extend the <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">Actor</em> class, and at the same time, an <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Actor</em> that is supposed to receive inputs from the user must extend the <em id="S3.SS1.p2.1.3" class="ltx_emph ltx_font_italic">Pawn</em> class (and optionally can have a <em id="S3.SS1.p2.1.4" class="ltx_emph ltx_font_italic">Controller</em> class to abstract input events, as we will see in the next section). This means that our base class that represents the common behavior of robots must extend <em id="S3.SS1.p2.1.5" class="ltx_emph ltx_font_italic">Pawn</em> class.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The meshes that model characters with joints like our robots are called <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">SkeletalMesh</em> in <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>. In addition to the mesh that defines their geometry, they incorporate a skeleton that defines how that geometry will be deformed according to the relative position and rotation of its bones. An <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">SkeletalMesh</em> is added as a component to our class (actually, an instance of <em id="S3.SS1.p3.1.3" class="ltx_emph ltx_font_italic">SkeletalMeshComponent</em>).</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">There are two types of inputs to which our robots must react to, those that come from pressing buttons or axes, and those that come from moving the <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> motion controllers. The latter is managed by an <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> component that must be added to our <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Pawn</em> class and that will modify its position according to the real-world motion controllers movement. We will be able to access the position of these components from the animation class, associate it with the hand bones of the robot <em id="S3.SS1.p4.1.2" class="ltx_emph ltx_font_italic">SkeletalMesh</em>, and move the whole arm by inverse kinematics.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">The animation class is created from the <em id="S3.SS1.p5.1.1" class="ltx_emph ltx_font_italic">SkeletalMesh</em>, so it is separate from the <em id="S3.SS1.p5.1.2" class="ltx_emph ltx_font_italic">Pawn</em> class, although the first has to access information from the second. Specifically, our animation classes handles the hand closing animation for the grasping system, and, in the case of robots with legs, it also takes control of the displacement speed to execute the walking animation at different speeds. Finally, the animation class is also used by the playback system (described below) to recover the <em id="S3.SS1.p5.1.3" class="ltx_emph ltx_font_italic">SkeletalMesh</em> pose for a single frame, since it is from where the position and rotation of each joint of the <em id="S3.SS1.p5.1.4" class="ltx_emph ltx_font_italic">SkeletalMesh</em> is accessible for modification.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Controller Subsystem</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We would like our system to seamlessly support a wide range of <span title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Virtual Reality</span></span> setups to reach a potentially higher number of users. In this regard, it is important to decouple the controller system from the rest of the environment so that we can use any device (such as the Oculus Rift and the HTC Vive Pro shown in Figure <a href="#S3.F4" title="Figure 4 ‣ 3.2 Controller Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) without excessive effort. To that end, it is common to have a class that handles all the inputs from the user (in an event-driven way) and then distributes the execution to other classes depending on that input. The very same <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> provides the base class for this purpose,
namely <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">PlayerController</em>. Many of these user inputs are focused on controlling the movement and behavior of a character in the scene, usually represented in <span title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Unreal Engine 4</span></span> as a <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">Pawn</em> class. This means that the <em id="S3.SS2.p1.1.3" class="ltx_emph ltx_font_italic">PlayerController</em> class is closely related to the <em id="S3.SS2.p1.1.4" class="ltx_emph ltx_font_italic">Pawn</em> one. Decoupling input management from functionality is useful as it allows us switching among different controllers for the same <em id="S3.SS2.p1.1.5" class="ltx_emph ltx_font_italic">Pawn</em> (different control types for example), or use the same controller for several ones (if they have the same behavior for inputs).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Our controller system extends the base class <em id="S3.SS2.p2.1.1" class="ltx_emph ltx_font_italic">PlayerController</em> and handles all kind of user inputs, both from keyboard and <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> controllers (we have tested our system with Oculus Rift and HTC Vive). This is configured in the <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> editor, more specifically in the <em id="S3.SS2.p2.1.2" class="ltx_emph ltx_font_italic">Input Project Preferences</em> panel, where several keys, buttons, or axes can be associated with an event name, which is later binded to a handler function in the custom <em id="S3.SS2.p2.1.3" class="ltx_emph ltx_font_italic">PlayerController</em> class. The controller calls the movement and grasping functionalities from the pawn, and also global system functions as toggling the recording system, restarting the scene, and resetting the <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> headset position. It also controls the  <span title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Head-Up Display</span></span> (<abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr>) system for showing input debugging feedback.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/vivepro.jpg" id="S3.F4.g1" class="ltx_graphics ltx_figure_panel ltx_img_square" width="287" height="287" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/oculus.jpg" id="S3.F4.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="287" height="162" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Seamlessly supported <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> headsets thanks to the decoupled controller subsystem: HTC Vive Pro and Oculus Rift.</span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">HUD Subsystem</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">It is convenient for any system to feature a debug system that provides feedback about the application state to the user. In UnrealROX, we offer an interface to show information at various levels to the user if requested. This information is presented in a <abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr> which can be turned off or on to the user’s will. It can even be completely decoupled from the system as a whole for maximum performance. The main information modalities provided by the <abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr> are the following ones:</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Recording: A line of text with the recording state is always shown in the HUD in order to let the user know if his movements through the scene are being recorded.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">States: Notifies the user with a message on the screen of the relevant buttons pressed, the joints in contact with an object, the profiling being activated, etc. The amount of seconds these messages last on screen can be established independently. Most of them are printed for 5 seconds.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Error: Prints a red message indicating an error that lasts in screen for 30 seconds (or until another error occurs). An example of this would be trying to record without the tracker on the scene (as seen in Figure <a href="#S3.F5" title="Figure 5 ‣ 3.3 HUD Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Scene Capture: It allows us to establish a debugging point of view so that we can see our robot from a different point of view than the first person camera.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/1810.06936/assets/NoTrackerError.jpg" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.3.2" class="ltx_text" style="font-size:90%;">Information and error messages shown in the <abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr>.</span></figcaption>
</figure>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We have implemented this functionality extending the <abbr title="Head-Up Display" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">HUD</span></abbr> class that <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> provides, and we also made it fully decoupled from the rest of the system in a simple way by implementing an interface<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.unrealengine.com/en-US/Programming/UnrealArchitecture/Reference/Interfaces</span></span></span></span>. Classes that inherite from HUD class have a canvas and a debug canvas on which primitive shapes can be drawn. It provides some simple methods for rendering text, textures, rectangles, and materials which can also be accessed from blueprints. An example of texture drawing in practice in our project is the Scene Capture, which consists in drawing a texture in the viewport captured from an arbitrary camera (as shown in Figure <a href="#S3.F6" title="Figure 6 ‣ 3.3 HUD Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). This will be useful for the user to see if the animations are being played correctly in a <span title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Virtual Reality</span></span> environment.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/1810.06936/assets/CamTexture.jpg" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.3.2" class="ltx_text" style="font-size:90%;">Scene Capture drawn in the viewport.</span></figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span id="S3.SS4.1.1" class="ltx_text ltx_font_italic">Grasping Subsystem</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Grasping subsystem is considered one of the core components of UnrealROX. We have focused on providing a realistic grasping, both in the way the robot grasp an object and in the movements it makes. When grasping an object we need to simulate a real robot behaviour, thus smooth and plausible movements are needed. The grasping action is fully controlled by the user through the controls, naturally limited to the degrees of freedom of the human body. In this way, we achieve a good representation of a humanoid robot interacting in a realistic home environment, also known as assistive robots which are the current trend in the field of social robotics.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Current approaches for grasping in <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> environments are animation-driven, and based on predefined movements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. This will restrict the system to only a few pre-defined object geometries hindering user’s interaction with the environment resulting also in a unrealistic grasping. In contrast with these approaches, the main idea of our grasping subsystem consists in manipulating and interacting with different objects, regardless of their geometry and pose. In this way, the user can freely decide which object to interact with without restrictions. The robot can manipulate an object with each hand, and change an object from a hand to the other. It can also manipulate two different objects at the same time, drop them freely or throw them around the scene.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">At the implementation level of this subsystem, we make use of <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>’s <em id="S3.SS4.p3.1.1" class="ltx_emph ltx_font_italic">trigger volumes</em> placed on each one of the finger phalanges as we can see in Figure <a href="#S3.F7" title="Figure 7 ‣ 3.4 Grasping Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. These <em id="S3.SS4.p3.1.2" class="ltx_emph ltx_font_italic">triggers</em> act as sensors that will determine if we are manipulating an object in order to grasp it. With the controllers we are able to close robot’s hands limiting individually each finger according to the <em id="S3.SS4.p3.1.3" class="ltx_emph ltx_font_italic">triggers</em>. We also implement a logic for determine when to grasp or release an object based on the <em id="S3.SS4.p3.1.4" class="ltx_emph ltx_font_italic">triggers</em> state. Fingers position change smoothly in order to replicate a real robot hand behaviour and to avoid pass through an object.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">A sequence example grasping two objects with our custom system is shown in Figure <a href="#S3.F8" title="Figure 8 ‣ 3.4 Grasping Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/1810.06936/assets/handTriggers.jpg" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><em id="S3.F7.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Sphere trigger volumes</em><span id="S3.F7.5.3" class="ltx_text" style="font-size:90%;"> placed on finger phalanges of both hands represented in yellow.</span></figcaption>
</figure>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/5100.jpg" id="S3.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F8.sf1.3.2" class="ltx_text" style="font-size:90%;">Frame 0</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/5135.jpg" id="S3.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F8.sf2.3.2" class="ltx_text" style="font-size:90%;">Frame 35</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/5170.jpg" id="S3.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F8.sf3.3.2" class="ltx_text" style="font-size:90%;">Frame 70</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/5175.jpg" id="S3.F8.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F8.sf4.3.2" class="ltx_text" style="font-size:90%;">Frame 75</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/5180.jpg" id="S3.F8.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S3.F8.sf5.3.2" class="ltx_text" style="font-size:90%;">Frame 80</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/5185.jpg" id="S3.F8.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S3.F8.sf6.3.2" class="ltx_text" style="font-size:90%;">Frame 85</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.4.2.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S3.F8.2.1" class="ltx_text" style="font-size:90%;">Sequence of 6 frames (<math id="S3.F8.2.1.m1.6" class="ltx_Math" alttext="Seq={F_{0},F_{35},F_{70},F_{75},F_{80},F_{85}}" display="inline"><semantics id="S3.F8.2.1.m1.6b"><mrow id="S3.F8.2.1.m1.6.6" xref="S3.F8.2.1.m1.6.6.cmml"><mrow id="S3.F8.2.1.m1.6.6.8" xref="S3.F8.2.1.m1.6.6.8.cmml"><mi id="S3.F8.2.1.m1.6.6.8.2" xref="S3.F8.2.1.m1.6.6.8.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.F8.2.1.m1.6.6.8.1" xref="S3.F8.2.1.m1.6.6.8.1.cmml">​</mo><mi id="S3.F8.2.1.m1.6.6.8.3" xref="S3.F8.2.1.m1.6.6.8.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.F8.2.1.m1.6.6.8.1b" xref="S3.F8.2.1.m1.6.6.8.1.cmml">​</mo><mi id="S3.F8.2.1.m1.6.6.8.4" xref="S3.F8.2.1.m1.6.6.8.4.cmml">q</mi></mrow><mo id="S3.F8.2.1.m1.6.6.7" xref="S3.F8.2.1.m1.6.6.7.cmml">=</mo><mrow id="S3.F8.2.1.m1.6.6.6.6" xref="S3.F8.2.1.m1.6.6.6.7.cmml"><msub id="S3.F8.2.1.m1.1.1.1.1.1" xref="S3.F8.2.1.m1.1.1.1.1.1.cmml"><mi id="S3.F8.2.1.m1.1.1.1.1.1.2" xref="S3.F8.2.1.m1.1.1.1.1.1.2.cmml">F</mi><mn id="S3.F8.2.1.m1.1.1.1.1.1.3" xref="S3.F8.2.1.m1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.F8.2.1.m1.6.6.6.6.7" xref="S3.F8.2.1.m1.6.6.6.7.cmml">,</mo><msub id="S3.F8.2.1.m1.2.2.2.2.2" xref="S3.F8.2.1.m1.2.2.2.2.2.cmml"><mi id="S3.F8.2.1.m1.2.2.2.2.2.2" xref="S3.F8.2.1.m1.2.2.2.2.2.2.cmml">F</mi><mn id="S3.F8.2.1.m1.2.2.2.2.2.3" xref="S3.F8.2.1.m1.2.2.2.2.2.3.cmml">35</mn></msub><mo id="S3.F8.2.1.m1.6.6.6.6.8" xref="S3.F8.2.1.m1.6.6.6.7.cmml">,</mo><msub id="S3.F8.2.1.m1.3.3.3.3.3" xref="S3.F8.2.1.m1.3.3.3.3.3.cmml"><mi id="S3.F8.2.1.m1.3.3.3.3.3.2" xref="S3.F8.2.1.m1.3.3.3.3.3.2.cmml">F</mi><mn id="S3.F8.2.1.m1.3.3.3.3.3.3" xref="S3.F8.2.1.m1.3.3.3.3.3.3.cmml">70</mn></msub><mo id="S3.F8.2.1.m1.6.6.6.6.9" xref="S3.F8.2.1.m1.6.6.6.7.cmml">,</mo><msub id="S3.F8.2.1.m1.4.4.4.4.4" xref="S3.F8.2.1.m1.4.4.4.4.4.cmml"><mi id="S3.F8.2.1.m1.4.4.4.4.4.2" xref="S3.F8.2.1.m1.4.4.4.4.4.2.cmml">F</mi><mn id="S3.F8.2.1.m1.4.4.4.4.4.3" xref="S3.F8.2.1.m1.4.4.4.4.4.3.cmml">75</mn></msub><mo id="S3.F8.2.1.m1.6.6.6.6.10" xref="S3.F8.2.1.m1.6.6.6.7.cmml">,</mo><msub id="S3.F8.2.1.m1.5.5.5.5.5" xref="S3.F8.2.1.m1.5.5.5.5.5.cmml"><mi id="S3.F8.2.1.m1.5.5.5.5.5.2" xref="S3.F8.2.1.m1.5.5.5.5.5.2.cmml">F</mi><mn id="S3.F8.2.1.m1.5.5.5.5.5.3" xref="S3.F8.2.1.m1.5.5.5.5.5.3.cmml">80</mn></msub><mo id="S3.F8.2.1.m1.6.6.6.6.11" xref="S3.F8.2.1.m1.6.6.6.7.cmml">,</mo><msub id="S3.F8.2.1.m1.6.6.6.6.6" xref="S3.F8.2.1.m1.6.6.6.6.6.cmml"><mi id="S3.F8.2.1.m1.6.6.6.6.6.2" xref="S3.F8.2.1.m1.6.6.6.6.6.2.cmml">F</mi><mn id="S3.F8.2.1.m1.6.6.6.6.6.3" xref="S3.F8.2.1.m1.6.6.6.6.6.3.cmml">85</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F8.2.1.m1.6c"><apply id="S3.F8.2.1.m1.6.6.cmml" xref="S3.F8.2.1.m1.6.6"><eq id="S3.F8.2.1.m1.6.6.7.cmml" xref="S3.F8.2.1.m1.6.6.7"></eq><apply id="S3.F8.2.1.m1.6.6.8.cmml" xref="S3.F8.2.1.m1.6.6.8"><times id="S3.F8.2.1.m1.6.6.8.1.cmml" xref="S3.F8.2.1.m1.6.6.8.1"></times><ci id="S3.F8.2.1.m1.6.6.8.2.cmml" xref="S3.F8.2.1.m1.6.6.8.2">𝑆</ci><ci id="S3.F8.2.1.m1.6.6.8.3.cmml" xref="S3.F8.2.1.m1.6.6.8.3">𝑒</ci><ci id="S3.F8.2.1.m1.6.6.8.4.cmml" xref="S3.F8.2.1.m1.6.6.8.4">𝑞</ci></apply><list id="S3.F8.2.1.m1.6.6.6.7.cmml" xref="S3.F8.2.1.m1.6.6.6.6"><apply id="S3.F8.2.1.m1.1.1.1.1.1.cmml" xref="S3.F8.2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.F8.2.1.m1.1.1.1.1.1.1.cmml" xref="S3.F8.2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.F8.2.1.m1.1.1.1.1.1.2.cmml" xref="S3.F8.2.1.m1.1.1.1.1.1.2">𝐹</ci><cn type="integer" id="S3.F8.2.1.m1.1.1.1.1.1.3.cmml" xref="S3.F8.2.1.m1.1.1.1.1.1.3">0</cn></apply><apply id="S3.F8.2.1.m1.2.2.2.2.2.cmml" xref="S3.F8.2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.F8.2.1.m1.2.2.2.2.2.1.cmml" xref="S3.F8.2.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.F8.2.1.m1.2.2.2.2.2.2.cmml" xref="S3.F8.2.1.m1.2.2.2.2.2.2">𝐹</ci><cn type="integer" id="S3.F8.2.1.m1.2.2.2.2.2.3.cmml" xref="S3.F8.2.1.m1.2.2.2.2.2.3">35</cn></apply><apply id="S3.F8.2.1.m1.3.3.3.3.3.cmml" xref="S3.F8.2.1.m1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.F8.2.1.m1.3.3.3.3.3.1.cmml" xref="S3.F8.2.1.m1.3.3.3.3.3">subscript</csymbol><ci id="S3.F8.2.1.m1.3.3.3.3.3.2.cmml" xref="S3.F8.2.1.m1.3.3.3.3.3.2">𝐹</ci><cn type="integer" id="S3.F8.2.1.m1.3.3.3.3.3.3.cmml" xref="S3.F8.2.1.m1.3.3.3.3.3.3">70</cn></apply><apply id="S3.F8.2.1.m1.4.4.4.4.4.cmml" xref="S3.F8.2.1.m1.4.4.4.4.4"><csymbol cd="ambiguous" id="S3.F8.2.1.m1.4.4.4.4.4.1.cmml" xref="S3.F8.2.1.m1.4.4.4.4.4">subscript</csymbol><ci id="S3.F8.2.1.m1.4.4.4.4.4.2.cmml" xref="S3.F8.2.1.m1.4.4.4.4.4.2">𝐹</ci><cn type="integer" id="S3.F8.2.1.m1.4.4.4.4.4.3.cmml" xref="S3.F8.2.1.m1.4.4.4.4.4.3">75</cn></apply><apply id="S3.F8.2.1.m1.5.5.5.5.5.cmml" xref="S3.F8.2.1.m1.5.5.5.5.5"><csymbol cd="ambiguous" id="S3.F8.2.1.m1.5.5.5.5.5.1.cmml" xref="S3.F8.2.1.m1.5.5.5.5.5">subscript</csymbol><ci id="S3.F8.2.1.m1.5.5.5.5.5.2.cmml" xref="S3.F8.2.1.m1.5.5.5.5.5.2">𝐹</ci><cn type="integer" id="S3.F8.2.1.m1.5.5.5.5.5.3.cmml" xref="S3.F8.2.1.m1.5.5.5.5.5.3">80</cn></apply><apply id="S3.F8.2.1.m1.6.6.6.6.6.cmml" xref="S3.F8.2.1.m1.6.6.6.6.6"><csymbol cd="ambiguous" id="S3.F8.2.1.m1.6.6.6.6.6.1.cmml" xref="S3.F8.2.1.m1.6.6.6.6.6">subscript</csymbol><ci id="S3.F8.2.1.m1.6.6.6.6.6.2.cmml" xref="S3.F8.2.1.m1.6.6.6.6.6.2">𝐹</ci><cn type="integer" id="S3.F8.2.1.m1.6.6.6.6.6.3.cmml" xref="S3.F8.2.1.m1.6.6.6.6.6.3">85</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F8.2.1.m1.6d">Seq={F_{0},F_{35},F_{70},F_{75},F_{80},F_{85}}</annotation></semantics></math>) representing a grasping action with right hand meanwhile holding a fruit with the left hand. Frames are left-right and top-down ordered</span></figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><span id="S3.SS5.1.1" class="ltx_text ltx_font_italic">Multi-camera Subsystem</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Most robots in the public market (such as Pepper<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotetext: </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.softbankrobotics.com/emea/en/robots/pepper</span></span></span></span> or Baxter<span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotetext: </span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.rethinkrobotics.com/baxter/</span></span></span></span>) integrate multiple cameras in different parts of their bodies. In addition, external cameras are usually added to the system to provide data from different points of view, e.g., ambient assisted living environments tend to feature various camera feeds for different rooms to provide the robot with information that it is not able to perceive directly. In UnrealROX, we want to simulate the ability to add multiple cameras in a synthetic environment with the goal in mind of having the same or more amount of data that we would have in a real environment. For instance, in order to train a data-driven grasping algorithm it would be needed to generate synthetic images from a certain point of view: the wrist of the robot. To simulate this situation in our synthetic scenario, we give the user the ability to place cameras attached to sockets in the robot’s body, e.g., the wrist itself or the end-effector (eye-in-hand). Furthermore, we also provide the functionality to add static cameras over the scene.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">To implement this subsystem, we make use of <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>’s <em id="S3.SS5.p2.1.1" class="ltx_emph ltx_font_italic">CameraActor</em> as the camera class and the <em id="S3.SS5.p2.1.2" class="ltx_emph ltx_font_italic">Pawn</em> class as the entity to which we will attach them. By default, <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> does not allow us to precisely attach components in the editor so it is necessary to define a socket-camera relationship in the <em id="S3.SS5.p2.1.3" class="ltx_emph ltx_font_italic">Pawn</em> class. This is due to the fact that it has direct access to the skeleton to which we will be attaching specific cameras.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">The objective of the <em id="S3.SS5.p3.1.1" class="ltx_emph ltx_font_italic">CameraActor</em> class is to render any scene from a specific point of view. This actor can be placed and rotated at the user’s discretion in the viewport, which makes them ideal for recording any type of scene from any desired point of view. The <em id="S3.SS5.p3.1.2" class="ltx_emph ltx_font_italic">CameraActor</em> is represented in <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> by a 3D camera model and like any other actor, it can be moved and rotated in the viewport. Apart from handling attached and static cameras, UnrealROX exposes the most demanded camera settings through its interface (projection mode,  <span title="Field of View" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Field of View</span></span> (<abbr title="Field of View" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">FoV</span></abbr>), color grading, tone mapping, lens, and various rendering effects), as well as providing additional features such as creating stereo-vision setups.</p>
</div>
<figure id="S3.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/1810.06936/assets/ArrayStructBone.jpg" id="S3.F9.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="314" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F9.sf1.3.2" class="ltx_text" style="font-size:90%;">Representation of the array.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F9.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/1810.06936/assets/ArrayStructBoneRep.jpg" id="S3.F9.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="335" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F9.sf2.3.2" class="ltx_text" style="font-size:90%;">Pawn Actor with cameras.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S3.F9.3.2" class="ltx_text" style="font-size:90%;">In-engine representation of the array of structs. In (<a href="#S3.F9.sf1" title="In Figure 9 ‣ 3.5 Multi-camera Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(a)</span></a>) we can see the representation of the array struct in the instance of the object, while in (<a href="#S3.F9.sf2" title="In Figure 9 ‣ 3.5 Multi-camera Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(b)</span></a>) we see its visual representation in the engine.</span></figcaption>
</figure>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">To implement the camera attachment functionality we make extensive use of the <em id="S3.SS5.p4.1.1" class="ltx_emph ltx_font_italic">AttachToActor</em> function provided by <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr>, which is in charge of parenting one actor with another following some attachment rules. We can specify the socket to which we want to attach the object. This means that when the selected socket changes its transform, the attached object will change it too according to the specified <em id="S3.SS5.p4.1.2" class="ltx_emph ltx_font_italic">AttachmentRules</em>. These rules dictate how this new attached actor will behave when the socket it is linked moves or rotates. The <em id="S3.SS5.p4.1.3" class="ltx_emph ltx_font_italic">AttachmentRules</em> can be defined separately for location, rotation, and scale. This lead us to define an implicit relationship between the <em id="S3.SS5.p4.1.4" class="ltx_emph ltx_font_italic">CameraActor</em> and the socket it is attached to. For that, the <em id="S3.SS5.p4.1.5" class="ltx_emph ltx_font_italic">Pawn</em> class implements an array of <em id="S3.SS5.p4.1.6" class="ltx_emph ltx_font_italic">USTRUCT</em> so that each relationship has two parameters: the camera itself and the socket name. These properties are accompanied by the <em id="S3.SS5.p4.1.7" class="ltx_emph ltx_font_italic">EditAnywhere</em> meta, which makes possible the edition of the properties not only on the Class Default Object (CDO) but also on its instances. The user will be in charge of filling the array specified in Listing 3. To make the attachment process easier, we provide a friendly user interface inside the editor (see Figure <a href="#S3.F9" title="Figure 9 ‣ 3.5 Multi-camera Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span><span id="S3.SS6.1.1" class="ltx_text ltx_font_italic">Recording Subsystem</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">UnrealROX decouples the recording and data generation processes so that we can achieve high framerates when gathering data in  <span title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Virtual Reality</span></span> (<abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr>) without decreasing performance due to extra processing tasks such as changing rendering modes, cameras, and writing images to disk. In this regard, the recording subsystem only acts while the agent is embodied as the robot in the virtual environment. When enabled, this mode gathers and dumps, on a per-frame basis, all the information that will be needed to replay and reconstruct the whole sequence, its data, and its ground truth. That information will be later used as input for the playback system to reproduce the sequence and generate all the requested data.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">In order to implement such behavior we created a new <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> <em id="S3.SS6.p2.1.1" class="ltx_emph ltx_font_italic">Actor</em>, namely <em id="S3.SS6.p2.1.2" class="ltx_emph ltx_font_italic">ROXTracker</em>, which overrides the <em id="S3.SS6.p2.1.3" class="ltx_emph ltx_font_italic">Tick</em> function. This new invisible actor is included in the scene we want to record and executes its tick code for each rendered frame. That tick function loops over all cameras, objects, and robots (skeletons) in the scene and writes all the needed information to a text file in an asynchronous way. For each frame, the actor dumps the following information: recorded frame number, timestamp in milliseconds since the start of the game, the position and rotation for each camera, the position, rotation, and bounding box minimum and maximum world-coordinates for each object, and the position and rotation of each joint of the robot’s skeleton.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/1810.06936/assets/roxtrackerui.jpg" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="323" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><em id="S3.F10.4.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ROXTracker</em><span id="S3.F10.5.3" class="ltx_text" style="font-size:90%;"> custom interface showing the robot mannequin, multiple cameras, and various parameters to configure which pawns and cameras are tracked and other sequence details.</span></figcaption>
</figure>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">The information is dumped in raw text format for efficiency, after the sequence is fully recorded, the raw text file is processed and converted into a more structured and readable JSON file so that it can be easily interpreted by the playback system.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span><span id="S3.SS7.1.1" class="ltx_text ltx_font_italic">Playback Subsystem</span>
</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Once the scene has been recorded, we can use the custom user interface in <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> to provide the needed data for the playback mode: the sequence description file in JSON format and an output directory. Other parameters such as frame skipping (to skip a certain amount of frames at the beginning of the sequence) and dropping (keep only a certain amount of frames) can also be customized (see Figure <a href="#S3.F10" title="Figure 10 ‣ 3.6 Recording Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<p id="S3.SS7.p2.1" class="ltx_p">This mode disables any physics simulation and interactions (since object and skeleton poses will be hard-coded by the sequence description itself) and then interprets the sequence file to generate all the raw data from it: RGB images, depth maps, instance segmentation masks, and normals. For each frame, the playback mode moves every object and every robot joint to the previously recorded position and sets their rotation. Once everything is positioned, it loops through each camera. For each one of them, the aforementioned rendering modes (RGB, depth, instance, and normals) are switched and the corresponding images are generated as shown in Figure <a href="#S3.F11" title="Figure 11 ‣ 3.7 Playback Subsystem ‣ 3 System ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure id="S3.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/4_rgb.jpg" id="S3.F11.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F11.sf1.3.2" class="ltx_text" style="font-size:90%;">RGB</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/4_depth.jpg" id="S3.F11.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F11.sf2.3.2" class="ltx_text" style="font-size:90%;">Depth</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F11.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/4.jpg" id="S3.F11.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F11.sf3.3.2" class="ltx_text" style="font-size:90%;">Mask</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F11.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1810.06936/assets/4_normals.jpg" id="S3.F11.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F11.sf4.3.2" class="ltx_text" style="font-size:90%;">Normals</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S3.F11.3.2" class="ltx_text" style="font-size:90%;">Rendering modes cycled by the playback mode.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Applications</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">UnrealROX environment has multiple potential application scenarios to generate data for various robotic vision tasks. Traditional algorithms for solving such tasks can take advantage of the data but the main purpose of this environment is providing the ability to generate large-scale datasets. Having the possibility of generating vast amounts of high-quality annotated data, data-driven algorithms such as deep learning models can especially benefit from it to increase their performance, in terms of accuracy, and improve their generalization capabilities in unseen situations during training. The set of tasks and problems that can be addressed using such data ranges from low to high-level ones, covering the whole spectrum of indoor robotics. Some of the most relevant low-level tasks include:</p>
</div>
<div id="S4.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Stereo Depth Estimation: One of the typical ways of obtaining 3D information for robotics is using a pair of displaced cameras used to obtain two different views from the same scene at the same time frame. By comparing both images, a disparity map can be obtained whose values are inversely proportional to the scene depth. Our multi-camera system allows the placement of stereo pairs at configurable baselines so that the environment is able to generate pairs of RGB images, and the corresponding depth, from calibrated cameras.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Monocular Depth Estimation: Another trending way of obtaining 3D information consists of using machine learning methods to infer depth from a single RGB image instead of a stereo pair. From a practical standpoint, it is specially interesting since it requires far less hardware and avoids the need for calibration strategies. Our multi-camera system generates by default depth information for each RGB frame (see Figure <a href="#S4.F12" title="Figure 12 ‣ 2nd item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>). For instance, this tool was used to generate data in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<figure id="S4.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/rgb0.jpg" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/rgb1.jpg" id="S4.F12.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/rgb2.jpg" id="S4.F12.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/depth0.jpg" id="S4.F12.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/depth1.jpg" id="S4.F12.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/depth2.jpg" id="S4.F12.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F12.2.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S4.F12.3.2" class="ltx_text" style="font-size:90%;">Sample RGB sequence for monocular depth estimation.</span></figcaption>
</figure>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Object Detection and Pose Estimation: Being able not only to identify which objects are in a given scene frame but also their estimate pose and bounding box is of utmost importance for an indoor robot. Our environment is able to produce 2D and 3D bounding boxes for each frame as ground truth. Furthermore, for each frame of a sequence, the full 6D pose of the objects is annotated too (see Figure <a href="#S4.F13" title="Figure 13 ‣ 3rd item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>).</p>
</div>
<figure id="S4.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/bbox0.jpg" id="S4.F13.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/bbox1.jpg" id="S4.F13.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/bbox2.jpg" id="S4.F13.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S4.F13.3.2" class="ltx_text" style="font-size:90%;">Sample bounding box annotations for the RGB sequence shown in Figure <a href="#S4.F12" title="Figure 12 ‣ 2nd item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</span></figcaption>
</figure>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Instance/Class Segmentation: For certain applications, detecting a bounding box for each object is not enough so we need to be able to pinpoint the exact boundaries of the objects. Semantic segmentation of frames provides per-pixel labels that indicate to which instance or class does a particular pixel belong. Our environment generates 2D (per-pixel) and 3D (per-point) labels for instance and class segmentation (see Figure <a href="#S4.F14" title="Figure 14 ‣ 4th item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>).</p>
</div>
<figure id="S4.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/mask0.jpg" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/mask1.jpg" id="S4.F14.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/mask2.jpg" id="S4.F14.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="146" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S4.F14.3.2" class="ltx_text" style="font-size:90%;">Sample instance segmentation sequence for the RGB images shown in Figure <a href="#S4.F12" title="Figure 12 ‣ 2nd item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.</span></figcaption>
</figure>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Normal Estimation: Estimating the normals of a given surface is an important previous step for many other tasks. For instance, certain algorithms require normal information in a point cloud to extract grasping points for a robot. UnrealROX provides per-pixel normal information.</p>
</div>
</li>
</ul>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">That low-level data enables other higher-level tasks that either make use of the output of those systems or take the low-level data as input or even both possibilities:</p>
</div>
<div id="S4.p4" class="ltx_para">
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Hand Pose Estimation: Estimating the 6D pose of each joint of the hands provides useful information for various higher-level tasks such as gesture detection, grasping or collaboration with other robots. We provide per-frame 6D pose annotations for each joint of the robot’s hands.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">Visual Grasping and Dexterous Manipulation: Grasping objects and manipulating them while grasped with one or both hands is a high-level task which can be solved using information from various sources (RGB images, depth maps, segmentation masks, normal maps, and joint estimates to name a few). In our case, we provide sequences in which the robot interacts with objects to displace, grab, and manipulate them so that grasping algorithms can take advantage of such sequences recorded from various points of view (see Figure <a href="#S4.F15" title="Figure 15 ‣ 2nd item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>).</p>
</div>
<figure id="S4.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_rgb_fps.jpg" id="S4.F15.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_rgb_lh.jpg" id="S4.F15.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_rgb_rh.jpg" id="S4.F15.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_depth_fps.jpg" id="S4.F15.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_depth_lh.jpg" id="S4.F15.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_depth_rh.jpg" id="S4.F15.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_mask_fps.jpg" id="S4.F15.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_mask_lh.jpg" id="S4.F15.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_mask_rh.jpg" id="S4.F15.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_normals_fps.jpg" id="S4.F15.g10" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_normals_lh.jpg" id="S4.F15.g11" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_normals_rh.jpg" id="S4.F15.g12" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F15.2.1.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="S4.F15.3.2" class="ltx_text" style="font-size:90%;">Sample hands interaction sequence and its associated data (from top to bottom: RGB, depth, instance masks, and normals).</span></figcaption>
</figure>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">Robot Pose Estimation: As well as providing 6D pose for hand joints, our environment also provides such information for all the joints of a robot on a per-frame basis. This allows training and testing body pose estimation algorithms which can be extremely useful in indoor environments to analyze behaviors and even collaborate with other robots too. To that end, we equipped our multi-camera system with the capability of adding room cameras that capture full bodies typical from assisted indoor living (see Figure <a href="#S4.F16" title="Figure 16 ‣ 3rd item ‣ 4 Applications ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>).</p>
</div>
<figure id="S4.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_rgb_mr.jpg" id="S4.F16.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_depth_mr.jpg" id="S4.F16.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/4174_mask_mr.jpg" id="S4.F16.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="195" height="110" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F16.2.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="S4.F16.3.2" class="ltx_text" style="font-size:90%;">Sample data from an external point of view in the room with the corresponding images (RGB, depth, and instance masks).</span></figcaption>
</figure>
</li>
<li id="S4.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i4.p1" class="ltx_para">
<p id="S4.I2.i4.p1.1" class="ltx_p">Obstacle Avoidance and Navigation: By leveraging various types of low-level information such as RGB images, depth maps, bounding boxes, and semantic segmentation, robots can learn to avoid obstacles (by detecting objects and estimating their distance) and even navigate in indoor environments (by building a map to localize themselves in the indoor scene while avoiding objects and walls and being able to reason semantically to move intelligently).</p>
</div>
</li>
</ul>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">As we can observe, UnrealROX is able to generate data for a significantly wide range of robotic vision applications. Most of them orbit around indoor robotics, although some of them might as well be applied to outdoor situations. In general, their purpose can be grouped into the more general application of  <span title="Ambient Assisted Living" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long">Ambient Assisted Living</span></span> (<abbr title="Ambient Assisted Living" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">AAL</span></abbr>) due to the inherent goal of achieving a robotic system able to operate intelligently in an indoor scenario in an autonomous way to provide support at various social tasks such as in-house rehabilitation, elder care, or even disabled assistance.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the previous section we showed multiple potential applications to which our data generator and the corresponding ground truth could be applied to train machine learning systems. In this section, we selected two of those applications to experiment with them in order to prove the effectiveness of our approach. Those two representative problems are: monocular depth estimation from RGB images and 6D object pose estimation.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">Monocular Depth Estimation</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As we already mentioned, estimating depth from 2D RGB images is an useful technique for many other higher-level applications such as scene reconstruction, object detection, and semantic segmentation. The problem can be formulated as follows: given a colored RGB image from any camera, the goal is to predict a dense depth map for each pixel as accurately as possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">The current trend for monocular depth estimation takes advantage of deep architectures, more concretely deep <span title="Convolutional Neural Network" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Convolutional Neural Networks</span></span>, with or without additional post-processing techniques for further refinement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Arguably, one of the most successful architecture is the Fully Convolutional Residual Network proposed by Laina <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. To prove the usefulness of our simulator, we have trained Laina’s method using a set of samples coming from our simulator (Figure <a href="#S5.F17" title="Figure 17 ‣ 5.1 Monocular Depth Estimation ‣ 5 Experiments ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> shows a random subset of the training images) and then we have tested it on a real-world dataset such as NYUDv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> (Figure <a href="#S5.F18" title="Figure 18 ‣ 5.1 Monocular Depth Estimation ‣ 5 Experiments ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> shows a random subset of testing samples for qualitative visualization).</p>
</div>
<figure id="S5.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_img_orig1.jpg" id="S5.F17.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_gt_depth1.jpg" id="S5.F17.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_out_depth_orig1.jpg" id="S5.F17.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_error_map1.jpg" id="S5.F17.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_img_orig1_1.jpg" id="S5.F17.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_gt_depth1_1.jpg" id="S5.F17.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_out_depth_orig1_1.jpg" id="S5.F17.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_error_map1_1.jpg" id="S5.F17.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_img_orig1_2.jpg" id="S5.F17.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_gt_depth1_2.jpg" id="S5.F17.g10" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_out_depth_orig12.jpg" id="S5.F17.g11" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_error_map1_2.jpg" id="S5.F17.g12" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_img_orig1_25.jpg" id="S5.F17.g13" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_gt_depth1_25.jpg" id="S5.F17.g14" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_out_depth_orig1_25.jpg" id="S5.F17.g15" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/unreal_error_map1_25.jpg" id="S5.F17.g16" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="81" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/color_bar.jpg" id="S5.F17.g17" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="29" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/gt_depth_bar.jpg" id="S5.F17.g18" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="32" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/prediction_bar.jpg" id="S5.F17.g19" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="32" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/error_map_bar.jpg" id="S5.F17.g20" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="32" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F17.2.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="S5.F17.3.2" class="ltx_text" style="font-size:90%;">Qualitative visualization of Fully Convolutional Residual Networks for monocular depth estimation on data generated by our simulator. First column shows the RGB images, second column is the depth ground truth, third column shows the corresponding depth predictions, and the last column is the error map between the predicted and the ground truth depth.</span></figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">As shown in this qualitative evaluation, knowledge learned using our simulated data can be seamlessly transferred to real-world data with adequate results in terms of accuracy and mean error.</p>
</div>
<figure id="S5.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/img_orig.jpg" id="S5.F18.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/gt_depth.jpg" id="S5.F18.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/out_depth_orig.jpg" id="S5.F18.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/error_map.jpg" id="S5.F18.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/img_orig1.jpg" id="S5.F18.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/gt_depth1.jpg" id="S5.F18.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/out_depth_orig1.jpg" id="S5.F18.g7" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/error_map1.jpg" id="S5.F18.g8" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/img_orig2.jpg" id="S5.F18.g9" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/gt_depth2.jpg" id="S5.F18.g10" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/out_depth_orig2.jpg" id="S5.F18.g11" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/error_map2.jpg" id="S5.F18.g12" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/img_orig3.jpg" id="S5.F18.g13" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/gt_depth3.jpg" id="S5.F18.g14" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/out_depth_orig3.jpg" id="S5.F18.g15" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/error_map3.jpg" id="S5.F18.g16" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/color_bar.jpg" id="S5.F18.g17" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="29" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/gt_depth_bar.jpg" id="S5.F18.g18" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="32" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/prediction_bar.jpg" id="S5.F18.g19" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="32" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/1810.06936/assets/error_map_bar.jpg" id="S5.F18.g20" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="144" height="32" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F18.2.1.1" class="ltx_text" style="font-size:90%;">Figure 18</span>: </span><span id="S5.F18.3.2" class="ltx_text" style="font-size:90%;">Qualitative evaluation of Fully Convolutional Residual Networks for monocular depth estimation on test data coming from NYUDv2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. First column shows the RGB images, second column is the depth ground truth, third column shows the corresponding depth predictions, and the last column is the error map between the predicted and the ground truth depth.</span></figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">6D Object Pose Estimation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Another widely used technique for which data generated with our tool can be helpful is 6D pose estimation of objects from 2D RGB images. This approach takes the object location problem one step further since it infers 3D rotation of the detected objects besides its location in an image (traditionally represented with a 2D bounding box). As a result, this estimation gives back a 3D bounding box that will estimate both 3D location (centroid) and rotation of the object.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">This estimation was usually done through multi-stage algorithms that generated a coarse initial estimation that needed to be refined later. However, newer approaches like the one from Tekin <em id="S5.SS2.p2.1.1" class="ltx_emph ltx_font_italic">et al.</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> generate fine estimations which are accurate enough withoutrequiring multiples stages thus making it possible to perform 6D object pose estimation in real time. It is inspired by the YOLO network for semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> to estimate projected 3D bounding boxes that, later, will be converted to a 6D pose by leveraging PnP algorithms.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p">To prove the usefulness of our generator in this problem, the network by Tekin <em id="S5.SS2.p3.1.1" class="ltx_emph ltx_font_italic">et al.</em> has been trained with our simulated data, and then tested with both synthetic and real images in order to see if it can transfer the knowledge to real-world data. First of all, in Figure <a href="#S5.F19" title="Figure 19 ‣ 5.2 6D Object Pose Estimation ‣ 5 Experiments ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> we can observe the pose estimation (through its 3D bounding box) of a banana in a sequence of synthetic images from our simulator.</p>
</div>
<figure id="S5.F19" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/synth1.jpg" id="S5.F19.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/synth2.jpg" id="S5.F19.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="109" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img src="/html/1810.06936/assets/synth3.jpg" id="S5.F19.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="192" height="108" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F19.2.1.1" class="ltx_text" style="font-size:90%;">Figure 19</span>: </span><span id="S5.F19.3.2" class="ltx_text" style="font-size:90%;">Qualitative evaluation of 6D pose estimation over synthetic data with single shot 6D object pose network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> trained with synthetic data from our simulator.</span></figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Later, Figure <a href="#S5.F20" title="Figure 20 ‣ 5.2 6D Object Pose Estimation ‣ 5 Experiments ‣ UnrealROX An eXtremely Photorealistic Virtual Reality Environment for Robotics Simulations and Synthetic Data Generation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> shows the same previously trained network trying to estimate the pose of a real banana on a live sequence captured by a camera.</p>
</div>
<figure id="S5.F20" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/real1.jpg" id="S5.F20.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="181" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/real2.jpg" id="S5.F20.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="179" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/real3.jpg" id="S5.F20.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="182" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/1810.06936/assets/real4.jpg" id="S5.F20.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="240" height="180" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F20.2.1.1" class="ltx_text" style="font-size:90%;">Figure 20</span>: </span><span id="S5.F20.3.2" class="ltx_text" style="font-size:90%;">Qualitative evaluation of 6D pose estimation over real data with single shot 6D object pose network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> trained with synthetic data from our simulator.</span></figcaption>
</figure>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">As shown in this evaluation, it follows the same trend as the depth estimation experiments: knowledge learned from our synthetic data generator can be transferred to real-world data with success.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper presented a virtual reality system, in which a human operator is embodied as a robotic agent using <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> setups such as Oculus Rift or HTC Vive Pro, for generating automatically annotated synthetic data for various robotic vision tasks. This environment leverages photorealism for bridging the reality gap so that models trained on its simulated data can be transferred to a real-world domain while still generalizing properly. The whole project, with all the aforementioned components (recording/playback, multi-camera, HUD, controller, and robotic pawns) is freely available <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/3dperceptionlab/unrealrox</span></span></span></span> with an open-source license and detailed documentation so that any researcher can use to generate custom data or even extend it to suit their particular needs. That data generation process was engineered and designed with efficiency and easiness in mind and it outperforms other existing solutions such as UnrealCV at object, robot, and camera repositioning, and image generation.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The outcome of this work demonstrates the potential of using <abbr title="Virtual Reality" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">VR</span></abbr> for simulating robotic interactions and generating synthetic data that facilitates training data-driven methods for various applications such as semantic segmentation, depth estimation, or object recognition.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Limitations and Future Works</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Currently, the environment still has certain limitations that must be addressed in order to make it applicable to a wider range of robotic vision tasks. One of them is the simulation of non-rigid objects and deformations when grasping such kind of objects. We have limited ourselves to manipulate non-deformable objects in order not to affect realism, since this is a different approach with a non-haptic manipulation and deformations need to be modelled at the object level. We are currently investigating the mechanisms that <abbr title="Unreal Engine 4" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_short">UE4</span></abbr> offers to model those transformations. Another important shortcoming is the absence of tactile information when grasping objects. We plan to include simulated tactile sensors to provide force data when fingers collide with objects and grasp them instead of providing only visual information. Furthermore, although not strictly a limitation, we are working on making the system able to process <span title="Unified Robot Description File" class="ltx_glossaryref"><span class="ltx_text ltx_glossary_long-plural">Unified Robot Description Files</span></span> to automatically import robots, including their constraints, cinematics, and colliders, in the environment instead of doing that manually for each robot model.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work has been funded by the Spanish Government TIN2016-76515-R grant for the COMBAHO project, supported with Feder funds. This work has also been supported by three Spanish national grants for PhD studies (FPU15/04516, FPU17/00166, and ACIF/2018/197), by the University of Alicante project GRE16-19, and by the Valencian Government project GV/2018/022.
Experiments were made possible by a generous hardware donation from NVIDIA.
We would also like to thank Zuria Bauer for her collaboration in the depth estimation experiments.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Zuria Bauer, Alejandro Dominguez, Edmanuel Cruz, Francisco Gomez-Donoso, Sergio
Orts-Escolano, and Miguel Cazorla.

</span>
<span class="ltx_bibblock">Enhancing perception for the visually impaired with deep learning
techniques and low-cost wearable sensors.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Pattern Recognition Letters</span>, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Zuria Bauer, Francisco Gomez-Donoso, Edmanuel Cruz, Sergio Orts-Escolano, and
Miguel Cazorla.

</span>
<span class="ltx_bibblock">Uasol, a large-scale high-resolution outdoor stereo dataset.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Scientific data</span>, 6(1):1–14, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Amlaan Bhoi.

</span>
<span class="ltx_bibblock">Monocular depth estimation: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1901-09402</span>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey,
Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige,
et al.

</span>
<span class="ltx_bibblock">Using simulation and domain adaptation to improve efficiency of deep
robotic grasping.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1709.07857</span>, 2017.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian
Strub, Jean Rouat, Hugo Larochelle, and Aaron Courville.

</span>
<span class="ltx_bibblock">Home: A household multimodal environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.11017</span>, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black.

</span>
<span class="ltx_bibblock">A naturalistic open source movie for optical flow evaluation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 611–625, 2012.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
David Eigen and Rob Fergus.

</span>
<span class="ltx_bibblock">Predicting depth, surface normals and semantic labels with a common
multi-scale convolutional architecture.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span>, pages 2650–2658, 2015.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
David Eigen, Christian Puhrsch, and Rob Fergus.

</span>
<span class="ltx_bibblock">Depth map prediction from a single image using a multi-scale deep
network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NIPS)</span>,
pages 2366–2374, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig.

</span>
<span class="ltx_bibblock">Virtual worlds as proxy for multi-object tracking analysis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 4340–4349, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (CVPR)</span>, pages 2961–2969, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali
Farhadi.

</span>
<span class="ltx_bibblock">Ai2-thor: An interactive 3d environment for visual ai.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.05474</span>, 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and
Nassir Navab.

</span>
<span class="ltx_bibblock">Deeper depth prediction with fully convolutional residual networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on 3D Vision (3DV)</span>, pages
239–248, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Nature</span>, 521(7553):436, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ian Lenz, Honglak Lee, and Ashutosh Saxena.

</span>
<span class="ltx_bibblock">Deep learning for detecting robotic grasps.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">The International Journal of Robotics Research</span>,
34(4-5):705–724, 2015.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Sergey Levine, Peter Pastor, Alex Krizhevsky, Julian Ibarz, and Deirdre
Quillen.

</span>
<span class="ltx_bibblock">Learning hand-eye coordination for robotic grasping with deep
learning and large-scale data collection.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">The International Journal of Robotics Research</span>,
37(4-5):421–436, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 3431–3440, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tom Looman.

</span>
<span class="ltx_bibblock">Vr template.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael Laskey, Richard Doan, Xinyu
Liu, Juan Aparicio Ojea, and Ken Goldberg.

</span>
<span class="ltx_bibblock">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point
clouds and analytic grasp metrics.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1703.09312</span>, 2017.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
John McCormac, Ankur Handa, Stefan Leutenegger, and Andrew J. Davison.

</span>
<span class="ltx_bibblock">Scenenet rgb-d: 5m photorealistic images of synthetic indoor
trajectories with ground truth.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1612.05079</span>, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus.

</span>
<span class="ltx_bibblock">Indoor segmentation and support inference from rgbd images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 746–760, 2012.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Oculus.

</span>
<span class="ltx_bibblock">Distance grab sample now available in oculus unity sample framework.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Oculus.

</span>
<span class="ltx_bibblock">Oculus first contact.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Weichao Qiu and Alan Yuille.

</span>
<span class="ltx_bibblock">Unrealcv: Connecting computer vision to unreal engine.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</span>, pages 909–916, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Weichao Qiu, Fangwei Zhong, Yi Zhang, Siyuan Qiao, Zihao Xiao, Tae Soo Kim, and
Yizhou Wang.

</span>
<span class="ltx_bibblock">Unrealcv: Virtual worlds for computer vision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2017 ACM on Multimedia Conference
(ACMMM)</span>, pages 1221–1224, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.

</span>
<span class="ltx_bibblock">You only look once: Unified, real-time object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition</span>, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">YOLO9000: Better, faster, stronger.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings - 30th IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2017</span>, 2017-Janua:6517–6525, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M.
Lopez.

</span>
<span class="ltx_bibblock">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 3234–3243, 2016.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and
Vladlen Koltun.

</span>
<span class="ltx_bibblock">Minos: Multimodal indoor simulator for navigation in complex
environments.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.03931</span>, 2017.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
B. Tekin, S. N. Sinha, and P. Fua.

</span>
<span class="ltx_bibblock">Real-time seamless single shot 6d object pose prediction.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</span>, pages 292–301, June 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from
simulation to the real world.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on
Intelligent Robots and Systems (IROS)</span>, pages 23–30, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Joshua Tobin, Wojciech Zaremba, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Domain randomization and generative models for robotic grasping.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1710.06425</span>, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.06516</span>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg,
Alexey Dosovitskiy, and Thomas Brox.

</span>
<span class="ltx_bibblock">Demon: Depth and motion network for learning monocular stereo.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 5038–5047, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, and Elisa Ricci.

</span>
<span class="ltx_bibblock">Structured attention guided convolutional neural fields for monocular
depth estimation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 3917–3925, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan Bisk, and
Yoav Artzi.

</span>
<span class="ltx_bibblock">Chalet: Cornell house agent learning environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1801.07357</span>, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1810.06935" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1810.06936" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1810.06936">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1810.06936" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1810.06937" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 00:28:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
