<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</title>
<!--Generated on Thu Oct 17 13:05:31 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.13523v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S1" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S2" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.SS1" title="In 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Exploring Imperfections in Real Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.SS2" title="In 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Generating Synthetic CXR reports and Paired Images.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.SS3" title="In 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic Data Training for MedVLP</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments Configurations</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4.SS1" title="In 4 Experiments Configurations ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Downstream Task Datasets and Configurations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4.SS2" title="In 4 Experiments Configurations ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Experimental Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S6" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A1" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Downstream Tasks Configuration</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A1.SS1" title="In Appendix A Downstream Tasks Configuration ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Dataset Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A1.SS2" title="In Appendix A Downstream Tasks Configuration ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2" title="In Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Extra Visualization</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><math alttext="\text{Che Liu}^{1,2}" class="ltx_Math" display="inline" id="id1.1.m1.2"><semantics id="id1.1.m1.2a"><msup id="id1.1.m1.2.3" xref="id1.1.m1.2.3.cmml"><mtext id="id1.1.m1.2.3.2" xref="id1.1.m1.2.3.2a.cmml">Che Liu</mtext><mrow id="id1.1.m1.2.2.2.4" xref="id1.1.m1.2.2.2.3.cmml"><mn id="id1.1.m1.1.1.1.1" xref="id1.1.m1.1.1.1.1.cmml">1</mn><mo id="id1.1.m1.2.2.2.4.1" xref="id1.1.m1.2.2.2.3.cmml">,</mo><mn id="id1.1.m1.2.2.2.2" xref="id1.1.m1.2.2.2.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.2b"><apply id="id1.1.m1.2.3.cmml" xref="id1.1.m1.2.3"><csymbol cd="ambiguous" id="id1.1.m1.2.3.1.cmml" xref="id1.1.m1.2.3">superscript</csymbol><ci id="id1.1.m1.2.3.2a.cmml" xref="id1.1.m1.2.3.2"><mtext id="id1.1.m1.2.3.2.cmml" xref="id1.1.m1.2.3.2">Che Liu</mtext></ci><list id="id1.1.m1.2.2.2.3.cmml" xref="id1.1.m1.2.2.2.4"><cn id="id1.1.m1.1.1.1.1.cmml" type="integer" xref="id1.1.m1.1.1.1.1">1</cn><cn id="id1.1.m1.2.2.2.2.cmml" type="integer" xref="id1.1.m1.2.2.2.2">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.2c">\text{Che Liu}^{1,2}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.2d">Che Liu start_POSTSUPERSCRIPT 1 , 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\text{Zhongwei Wan}^{3}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mtext id="id2.2.m2.1.1.2" xref="id2.2.m2.1.1.2a.cmml">Zhongwei Wan</mtext><mn id="id2.2.m2.1.1.3" xref="id2.2.m2.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><csymbol cd="ambiguous" id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1">superscript</csymbol><ci id="id2.2.m2.1.1.2a.cmml" xref="id2.2.m2.1.1.2"><mtext id="id2.2.m2.1.1.2.cmml" xref="id2.2.m2.1.1.2">Zhongwei Wan</mtext></ci><cn id="id2.2.m2.1.1.3.cmml" type="integer" xref="id2.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\text{Zhongwei Wan}^{3}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">Zhongwei Wan start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\text{Haozhe Wang}^{4}" class="ltx_Math" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mtext id="id3.3.m3.1.1.2" xref="id3.3.m3.1.1.2a.cmml">Haozhe Wang</mtext><mn id="id3.3.m3.1.1.3" xref="id3.3.m3.1.1.3.cmml">4</mn></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><csymbol cd="ambiguous" id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1">superscript</csymbol><ci id="id3.3.m3.1.1.2a.cmml" xref="id3.3.m3.1.1.2"><mtext id="id3.3.m3.1.1.2.cmml" xref="id3.3.m3.1.1.2">Haozhe Wang</mtext></ci><cn id="id3.3.m3.1.1.3.cmml" type="integer" xref="id3.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">\text{Haozhe Wang}^{4}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">Haozhe Wang start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\text{Yinda Chen}^{5}" class="ltx_Math" display="inline" id="id4.4.m4.1"><semantics id="id4.4.m4.1a"><msup id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml"><mtext id="id4.4.m4.1.1.2" xref="id4.4.m4.1.1.2a.cmml">Yinda Chen</mtext><mn id="id4.4.m4.1.1.3" xref="id4.4.m4.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.1b"><apply id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1"><csymbol cd="ambiguous" id="id4.4.m4.1.1.1.cmml" xref="id4.4.m4.1.1">superscript</csymbol><ci id="id4.4.m4.1.1.2a.cmml" xref="id4.4.m4.1.1.2"><mtext id="id4.4.m4.1.1.2.cmml" xref="id4.4.m4.1.1.2">Yinda Chen</mtext></ci><cn id="id4.4.m4.1.1.3.cmml" type="integer" xref="id4.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.1c">\text{Yinda Chen}^{5}</annotation><annotation encoding="application/x-llamapun" id="id4.4.m4.1d">Yinda Chen start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\textbf{\text{Talha Qaiser}}^{2}" class="ltx_Math" display="inline" id="id5.5.m5.1"><semantics id="id5.5.m5.1a"><msup id="id5.5.m5.1.1" xref="id5.5.m5.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="id5.5.m5.1.1.2" xref="id5.5.m5.1.1.2a.cmml">Talha Qaiser</mtext><mn id="id5.5.m5.1.1.3" xref="id5.5.m5.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id5.5.m5.1b"><apply id="id5.5.m5.1.1.cmml" xref="id5.5.m5.1.1"><csymbol cd="ambiguous" id="id5.5.m5.1.1.1.cmml" xref="id5.5.m5.1.1">superscript</csymbol><ci id="id5.5.m5.1.1.2a.cmml" xref="id5.5.m5.1.1.2"><mtext class="ltx_mathvariant_bold" id="id5.5.m5.1.1.2.cmml" xref="id5.5.m5.1.1.2">Talha Qaiser</mtext></ci><cn id="id5.5.m5.1.1.3.cmml" type="integer" xref="id5.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id5.5.m5.1c">\textbf{\text{Talha Qaiser}}^{2}</annotation><annotation encoding="application/x-llamapun" id="id5.5.m5.1d">Talha Qaiser start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\textbf{\text{Chen Jin}}^{2}" class="ltx_Math" display="inline" id="id6.6.m6.1"><semantics id="id6.6.m6.1a"><msup id="id6.6.m6.1.1" xref="id6.6.m6.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="id6.6.m6.1.1.2" xref="id6.6.m6.1.1.2a.cmml">Chen Jin</mtext><mn id="id6.6.m6.1.1.3" xref="id6.6.m6.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id6.6.m6.1b"><apply id="id6.6.m6.1.1.cmml" xref="id6.6.m6.1.1"><csymbol cd="ambiguous" id="id6.6.m6.1.1.1.cmml" xref="id6.6.m6.1.1">superscript</csymbol><ci id="id6.6.m6.1.1.2a.cmml" xref="id6.6.m6.1.1.2"><mtext class="ltx_mathvariant_bold" id="id6.6.m6.1.1.2.cmml" xref="id6.6.m6.1.1.2">Chen Jin</mtext></ci><cn id="id6.6.m6.1.1.3.cmml" type="integer" xref="id6.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m6.1c">\textbf{\text{Chen Jin}}^{2}</annotation><annotation encoding="application/x-llamapun" id="id6.6.m6.1d">Chen Jin start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, 
<br class="ltx_break"/><math alttext="\textbf{\text{Fariba Yousefi}}^{2}" class="ltx_Math" display="inline" id="id7.7.m7.1"><semantics id="id7.7.m7.1a"><msup id="id7.7.m7.1.1" xref="id7.7.m7.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="id7.7.m7.1.1.2" xref="id7.7.m7.1.1.2a.cmml">Fariba Yousefi</mtext><mn id="id7.7.m7.1.1.3" xref="id7.7.m7.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id7.7.m7.1b"><apply id="id7.7.m7.1.1.cmml" xref="id7.7.m7.1.1"><csymbol cd="ambiguous" id="id7.7.m7.1.1.1.cmml" xref="id7.7.m7.1.1">superscript</csymbol><ci id="id7.7.m7.1.1.2a.cmml" xref="id7.7.m7.1.1.2"><mtext class="ltx_mathvariant_bold" id="id7.7.m7.1.1.2.cmml" xref="id7.7.m7.1.1.2">Fariba Yousefi</mtext></ci><cn id="id7.7.m7.1.1.3.cmml" type="integer" xref="id7.7.m7.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m7.1c">\textbf{\text{Fariba Yousefi}}^{2}</annotation><annotation encoding="application/x-llamapun" id="id7.7.m7.1d">Fariba Yousefi start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\textbf{\text{Nikolay Burlutskiy}}^{2}" class="ltx_Math" display="inline" id="id8.8.m8.1"><semantics id="id8.8.m8.1a"><msup id="id8.8.m8.1.1" xref="id8.8.m8.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="id8.8.m8.1.1.2" xref="id8.8.m8.1.1.2a.cmml">Nikolay Burlutskiy</mtext><mn id="id8.8.m8.1.1.3" xref="id8.8.m8.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="id8.8.m8.1b"><apply id="id8.8.m8.1.1.cmml" xref="id8.8.m8.1.1"><csymbol cd="ambiguous" id="id8.8.m8.1.1.1.cmml" xref="id8.8.m8.1.1">superscript</csymbol><ci id="id8.8.m8.1.1.2a.cmml" xref="id8.8.m8.1.1.2"><mtext class="ltx_mathvariant_bold" id="id8.8.m8.1.1.2.cmml" xref="id8.8.m8.1.1.2">Nikolay Burlutskiy</mtext></ci><cn id="id8.8.m8.1.1.3.cmml" type="integer" xref="id8.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m8.1c">\textbf{\text{Nikolay Burlutskiy}}^{2}</annotation><annotation encoding="application/x-llamapun" id="id8.8.m8.1d">Nikolay Burlutskiy start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>,
<math alttext="\textbf{\text{Rossella Arcucci}}^{1}" class="ltx_Math" display="inline" id="id9.9.m9.1"><semantics id="id9.9.m9.1a"><msup id="id9.9.m9.1.1" xref="id9.9.m9.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="id9.9.m9.1.1.2" xref="id9.9.m9.1.1.2a.cmml">Rossella Arcucci</mtext><mn id="id9.9.m9.1.1.3" xref="id9.9.m9.1.1.3.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="id9.9.m9.1b"><apply id="id9.9.m9.1.1.cmml" xref="id9.9.m9.1.1"><csymbol cd="ambiguous" id="id9.9.m9.1.1.1.cmml" xref="id9.9.m9.1.1">superscript</csymbol><ci id="id9.9.m9.1.1.2a.cmml" xref="id9.9.m9.1.1.2"><mtext class="ltx_mathvariant_bold" id="id9.9.m9.1.1.2.cmml" xref="id9.9.m9.1.1.2">Rossella Arcucci</mtext></ci><cn id="id9.9.m9.1.1.3.cmml" type="integer" xref="id9.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m9.1c">\textbf{\text{Rossella Arcucci}}^{1}</annotation><annotation encoding="application/x-llamapun" id="id9.9.m9.1d">Rossella Arcucci start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT</annotation></semantics></math>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id1">1</sup>Imperial College London, UK 
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.id2">2</sup>AstraZeneca, UK 
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id3">3</sup>Ohio State University, US 
<br class="ltx_break"/><sup class="ltx_sup" id="id13.13.id4">4</sup>Hong Kong University of Science and Technology 
<br class="ltx_break"/><sup class="ltx_sup" id="id14.14.id5">5</sup>University of Science and Technology of China 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id15.15.id6">che.liu21@imperial.ac.uk</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.id1">Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question: <span class="ltx_text ltx_font_bold ltx_font_italic" id="id16.id1.1">Can MedVLP succeed using purely synthetic data?</span> To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective.
Our results show that MedVLP models trained <span class="ltx_text ltx_font_italic" id="id16.id1.2">exclusively on synthetic data</span> outperform those trained on real data by <span class="ltx_text ltx_font_bold" id="id16.id1.3">3.8%</span> in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of <span class="ltx_text ltx_font_bold" id="id16.id1.4">9.07%</span>. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks.
Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>All data and code will be released upon acceptance.</span></span></span>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In medical image analysis, learning representative features typically requires labor-intensive and costly image annotations <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib43" title="">2015</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib31" title="">2023b</a>)</cite>. Medical Vision-Language Pre-training (MedVLP) addresses this challenge by aligning vision and language content using paired datasets of images and clinical reports, reducing the need for manual annotations <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib40" title="">2021</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib62" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib30" title="">2023a</a>)</cite>. However, existing MedVLP models rely heavily on large-scale, high-quality paired data <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib34" title="">2023e</a>)</cite>, which is scarce in practice. Real-world datasets often contain noisy data, such as low-quality images and unpaired image-text samples, degrading model performance <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib64" title="">2024</a>; Bannur et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib3" title="">2023</a>)</cite>.
Recent advancements in Large Language Models (LLMs) and diffusion models enable the generation of large-scale synthetic image-text datasets, offering an alternative to traditional data collection. Although these techniques have shown promise in medical tasks, they are primarily used as auxiliary support for real data via augmentation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib8" title="">2024a</a>; Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib68" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib7" title="">2022</a>; Qin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib39" title="">2023</a>)</cite>, and are often limited to single-modality settings. To the best of our knowledge, no studies have fully explored the potential of using synthetic multimodal data for MedVLP or considered training exclusively on synthetic data <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib34" title="">2023e</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To bridge this gap and showcase synthetic data’s potential for MedVLP, our contributions are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose an automated pipeline to create the <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">SynCXR</span> dataset, which contains 200,000 synthetic images and text generated with quality and distribution control using off-the-shelf models, without relying on real data or manual curation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We successfully demonstrate that MedVLP models trained on our SynCXR dataset, containing only synthetic data, outperform those trained on real data. Moreover, combining synthetic and real data further improves performance, showcasing the effectiveness of our synthetic data generation pipeline.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We identify several issues in the most commonly used real dataset for MedVLP, MIMIC-CXR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib22" title="">2019b</a>)</cite>, that degrade MedVLP performance, including low-quality images and unpaired image-text samples. Furthermore, we identify the long-tailed distribution problem in multimodal datasets, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">1</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We conduct an extensive analysis of the key factors contributing to MedVLP’s success using purely synthetic data. Our method is evaluated on seven downstream tasks using zero-shot learning and linear probing, demonstrating that MedVLP can effectively perform with synthetic data alone.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="266" id="S2.F1.g1" src="extracted/5934507/fig1.png" width="548"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Comparison of real image-text datasets and synthetic datasets.
<span class="ltx_text ltx_font_bold" id="S2.F1.3.1">(a):</span> The real image-text dataset, MIMIC-CXR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib22" title="">2019b</a>)</cite>, while authentic, often contains imperfections such as long-tailed data distribution, unpaired images and text, and low-quality CXR images, which limit the performance of MedVLP models pretrained on this dataset.
<span class="ltx_text ltx_font_bold" id="S2.F1.4.2">(b):</span> The synthetic dataset generation process uses clinical entities as prompts to an LLM (e.g., Llama3.1 <cite class="ltx_cite ltx_citemacro_citep">(AI@Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib1" title="">2024</a>)</cite>) to generate synthetic reports. These reports are then used to create synthetic images through RoentGen <cite class="ltx_cite ltx_citemacro_citep">(Bluethgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib4" title="">2024</a>)</cite>. We propose an automated pipeline to control the dataset distribution, ensuring it is balanced and includes paired image-text samples.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Representation Learning with Synthetic Data.</span>
Synthetic data has been widely employed across various deep learning fields <cite class="ltx_cite ltx_citemacro_citep">(Rossenbach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib45" title="">2020</a>; Varol et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib57" title="">2017</a>; Jahanian et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib19" title="">2022</a>; Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib77" title="">2023</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib67" title="">2020</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib28" title="">2023</a>)</cite>. In visual representation learning, synthetic data has improved model performance in a range of tasks <cite class="ltx_cite ltx_citemacro_citep">(Richter et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib41" title="">2016</a>; Ros et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib44" title="">2016</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib9" title="">2019</a>; Johnson-Roberson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib23" title="">2017</a>; Yuan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib70" title="">2024</a>; Shmelkov et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib51" title="">2018</a>)</cite>. Recent efforts have also focused on using synthetic data from text-to-image models to augment real-world data during training <cite class="ltx_cite ltx_citemacro_citep">(Azizi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib2" title="">2023</a>; Sariyildiz et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib47" title="">2023</a>; He et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib16" title="">2023</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib69" title="">2023</a>)</cite> introduced a framework to generate synthetic images to diversify existing datasets. Notably, methods utilizing text-to-image generative models <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib42" title="">2022</a>)</cite> have demonstrated that synthetic images guided by real captions can effectively train self-supervised models, achieving performance comparable to that of real images <cite class="ltx_cite ltx_citemacro_citep">(Tian et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib54" title="">2023b</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Further advancements like SynCLR <cite class="ltx_cite ltx_citemacro_citep">(Tian et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib53" title="">2023a</a>)</cite> have focused on visual representation learning using only synthetic images, generated with conditioning on various categories. Meanwhile, other recent works <cite class="ltx_cite ltx_citemacro_citep">(Fan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib14" title="">2023</a>; Sharifzadeh et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib48" title="">2024</a>; Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib64" title="">2024</a>)</cite> have explored joint image and text generation for enhanced vision-language pretraining (VLP). However, only one study, SynthCLIP <cite class="ltx_cite ltx_citemacro_citep">(Hammoud et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib15" title="">2024</a>)</cite>, investigates VLP exclusively with synthetic data, and even that work is limited to natural images. To date, no research has explored the potential of MedVLP trained solely on synthetic data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Medical Vision Language Pre-training.</span>
Recent work on MedVLP has focused on integrating visual and textual modalities, particularly for chest X-ray (CXR) images. Studies such as <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib59" title="">2022</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib31" title="">2023b</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib33" title="">d</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib32" title="">c</a>; Wan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib58" title="">2024</a>)</cite> have concentrated on aligning CXR images with paired radiology reports. Some methods also leverage external datasets to boost performance, raising concerns about generalizability <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib62" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib72" title="">2023</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib29" title="">2024</a>; Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib37" title="">2024a</a>)</cite>.
However, all current MedVLP approaches rely heavily on large-scale, real image-text paired datasets like MIMIC-CXR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib22" title="">2019b</a>)</cite>. Some even require additional human-annotated datasets or manual interventions to improve model performance <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib62" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib72" title="">2023</a>; Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib37" title="">2024a</a>)</cite>, which limits their scalability and accessibility.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Synthetic Data for Medical Image Tasks.</span>
Given the scarcity of annotated data, high costs, and privacy concerns in medical data collection, synthetic data has been explored to support various medical image tasks <cite class="ltx_cite ltx_citemacro_citep">(Koetzier et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib25" title="">2024</a>)</cite>. However, most prior work focuses on image modality and supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib8" title="">2024a</a>; Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib68" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib7" title="">2022</a>; Qin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib39" title="">2023</a>)</cite>, using synthetic data solely as augmentation for real datasets <cite class="ltx_cite ltx_citemacro_citep">(Khosravi et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib24" title="">2024</a>; Ktena et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib26" title="">2024</a>)</cite>. Few studies have evaluated models trained entirely on synthetic medical data <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib63" title="">2024</a>)</cite>. Recent efforts have generated synthetic text and images for MedVLP <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib64" title="">2024</a>)</cite>, but still restrict synthetic data usage to augmentation. Consequently, the full potential of synthetic data in MedVLP remains largely unexplored.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">In this work, we generate both synthetic CXR images and reports, then training a MedVLP model solely on synthetic data. We conduct an extensive evaluation of the impact of large-scale synthetic medical data on MedVLP, exploring its performance across various downstream tasks.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="185" id="S3.F2.g1" src="extracted/5934507/badsample.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span class="ltx_text ltx_font_bold" id="S3.F2.4.1">(a):</span> Examples of invalid or low-quality images filtered out by the proposed image curation method described in Sec <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.SS1" title="3.1 Exploring Imperfections in Real Data ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
<span class="ltx_text ltx_font_bold" id="S3.F2.5.2">(b):</span> The image curation pipeline uses InternVL2 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib10" title="">2023</a>)</cite>, a Multimodal Large Language Model (MLLM), to assess CXR image quality. Images that meet the criteria are retained; others are discarded.
<span class="ltx_text ltx_font_bold" id="S3.F2.6.3">(c):</span> Entity frequency distribution in the MIMIC-CXR dataset. Due to space constraints, only the top 50 frequent entities for four categories (Abnormality, Non-Abnormality, Disease, Non-Disease) are shown. A more detailed distribution is presented in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F7" title="Figure 7 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">7</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F8" title="Figure 8 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">8</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F11" title="Figure 11 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">11</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F9" title="Figure 9 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">9</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F10" title="Figure 10 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">10</span></a>.
</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Exploring Imperfections in Real Data</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">For MedVLP, the most commonly used dataset is MIMIC-CXR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib21" title="">2019a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib22" title="">b</a>)</cite>, a collection of chest x-ray (CXR) images paired with their corresponding textual reports. after following the preprocessing steps outlined in previous works <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib72" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib59" title="">2022</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite>, this dataset provides a total of 213,384 image-text pairs for pre-training. And all images must be frontal views according to the preprocessing steps outlined in <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Previous work on VLP with natural images <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib66" title="">2023b</a>)</cite> has shown that data quality, including image fidelity and long-tailed distribution, significantly impacts model performance. However, the quality of MedVLP datasets remains underexplored due to ambiguity in defining medical image quality, stemming from diverse imaging protocols. Additionally, quantifying data distribution is complex, as radiology reports often describe patterns across multiple anatomical regions rather than distinct categories.To address these challenges, we develop a systematic pipeline to thoroughly analyze the data issues in the MIMIC-CXR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib22" title="">2019b</a>)</cite> dataset, as detailed in the following sections.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Low-Quality and Mismatched Image-Text Pairs.</span>
Our aim is to explore and identify issues related to image quality in the MIMIC-CXR dataset <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib21" title="">2019a</a>)</cite>, rather than to completely clean the dataset, as creating a perfect dataset and filtering out all low-quality samples is infeasible for large-scale multimodal datasets <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib65" title="">2023a</a>)</cite>.
Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Bannur et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib3" title="">2023</a>)</cite>, which highlights various issues with poor-quality images, we design six queries for a Multimodal Large Language Model (MLLM), utilizing the InternVL2-26B model<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/OpenGVLab/InternVL2-26B</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib10" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib11" title="">2024b</a>)</cite>. Each CXR image from the MIMIC-CXR dataset is paired with these six queries, and the MLLM process each query independently. The process is depicted in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">2</span></a> (b).</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Detecting Non-CXR Images:</span> <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.2">&lt;CXR Image&gt;, Please check if the given image is a chest X-ray scan. If it is a chest X-ray, return ‘YES’. Otherwise, return ‘NO’.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Detecting Non-Human CXR Images:</span> <span class="ltx_text ltx_font_typewriter" id="S3.I1.i2.p1.1.2">&lt;CXR Image&gt;, Please verify if the given image is a human chest X-ray scan. If it is a chest X-ray, return ‘YES’. Otherwise, return ‘NO’.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Detecting Wrong Views:</span> <span class="ltx_text ltx_font_typewriter" id="S3.I1.i3.p1.1.2">&lt;CXR Image&gt;, Please check if the given image is a frontal chest X-ray view. If it is a frontal view, return ‘YES’. If it is a lateral view or any other view, return ‘NO’.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Assessing Image Quality:</span> <span class="ltx_text ltx_font_typewriter" id="S3.I1.i4.p1.1.2">&lt;CXR Image&gt;, Please analyze the provided chest X-ray (CXR) image and respond with ‘NO’ if the image quality is poor, such as being blurry, containing artifacts, or having poor contrast. Respond with ‘YES’ if the image quality is acceptable.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Detecting Artifacts and Overprocessing:</span> <span class="ltx_text ltx_font_typewriter" id="S3.I1.i5.p1.1.2">&lt;CXR Image&gt;, Please analyze the following chest X-ray image. Respond with ‘YES’ if the image is clear, correctly oriented, and free of artifacts or imperfections that could affect its diagnostic quality. Respond with ‘NO’ if the image is blurry, incorrectly oriented, contains artifacts, or has imperfections that make it unsuitable for further analysis.</span></p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i6.p1.1.1">Checking High-Fidelity:</span> <span class="ltx_text ltx_font_typewriter" id="S3.I1.i6.p1.1.2">&lt;CXR Image&gt;, Please check if the given image is a high-fidelity human chest X-ray scan. If it is a high-fidelity chest X-ray, return ‘YES’. Otherwise, return ‘NO’.</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.SS1.p3.2">After this process, we filter out the CXR images where the answers are all <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.2.1">‘NO’</span> across the six queries. Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">2</span></a> (a) shows examples of images where the answer was <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p3.2.2">‘NO’</span>. We identified and removed 1,448 such images and their corresponding reports from the preprocessed MIMIC-CXR dataset, leaving us with 211,936 image-text pairs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">To further refine the dataset, we use the CXR-specific vision encoder, RAD-DINO <cite class="ltx_cite ltx_citemacro_citep">(Pérez-García et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib36" title="">2024</a>)</cite>, to extract image features from the remaining 211,936 CXR images and from the 1,448 samples identified as bad by MLLM filtering. We then compute the similarity between each image in the cleaned dataset and each of the bad samples. Since each image comes from a different clinical case, we only compare image quality rather than the clinical content (e.g., diagnoses or abnormalities). To do this, we set a similarity threshold of 0.5 and remove all images with a similarity score greater than 0.5. This step resulted in the removal of an additional 5,512 images and their paired reports, reducing the dataset to 206,424 image-text pairs.
Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">2</span></a> (a) also shows the samples removed based on their similarity to bad images using visual features from RAD-DINO<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://huggingface.co/microsoft/rad-dino</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Pérez-García et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib36" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">In our exploration of the MIMIC-CXR dataset, we utilized a rough approach to identify problematic images, such as non-chest images, wrong views, overprocessing, and low-fidelity scans. Our results confirm that many images in the dataset exhibit these issues. While our approach identifies numerous problematic images, fully curating and removing all low-quality cases is unfeasible due to the substantial human effort required and the absence of well-defined criteria for an automated cleaning pipeline.
Furthermore, addressing all instances of low-quality images remains highly challenging through automated processes alone.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Uncovering Long-tailed Distribution in MIMIC-CXR.</span>
As demonstrated in previous work on natural image-text data <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib65" title="">2023a</a>; Hammoud et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib15" title="">2024</a>)</cite>, a long-tailed distribution in VLP datasets negatively impacts model performance.
Therefore, we aim to explore the data distribution of the MIMIC-CXR dataset. However, directly evaluating the text distribution at the sample level, as done in <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib65" title="">2023a</a>)</cite>, is challenging because each radiology report often describes multiple patterns or anatomical regions, unlike natural image captions that typically focus on a single object <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib71" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">Instead, we adopt an alternative approach by using an off-the-shelf Named Entity Recognition (NER) tool to extract all medical entities, treating them as representatives of the report’s concepts and exploring the dataset distribution at the entity level. For this, we use RaTE<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://huggingface.co/Angelakeke/RaTE-NER-Deberta</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib74" title="">2024</a>)</cite>, a model specifically designed for NER tasks on radiology reports.
RaTE automatically classifies the extracted entities into five categories: [<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p7.1.1">ABNORMALITY</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p7.1.2">NON-ABNORMALITY</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p7.1.3">DISEASE</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p7.1.4">NON-DISEASE</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p7.1.5">ANATOMY</span>].
We display the top 50 frequent entiites distribution of each entity type in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.F2" title="Figure 2 ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">2</span></a> (c).
We display the top 50 frequent entiites distribution of each entity type in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F7" title="Figure 7 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">7</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F8" title="Figure 8 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">8</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F11" title="Figure 11 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">11</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F9" title="Figure 9 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">9</span></a>,<a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F10" title="Figure 10 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">10</span></a>. As shown, all entity types exhibit a severe long-tailed distribution. The MIMIC-CXR <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib22" title="">2019b</a>)</cite> includes a total of 154,049 unique entities, with 55,047 Abnormality, 36,365 Non-Abnormality, 23,017 Disease, 22,103 Non-Disease, and 40,517 Anatomy entities.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Generating Synthetic CXR reports and Paired Images.</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Since the MIMIC-CXR dataset <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib21" title="">2019a</a>)</cite> contains various data issues, we generate synthetic radiology reports and CXR images, controlling data quality and distribution during generation to alleviate these problems.
In this work, we aim to explore the effectiveness of pretraining MedVLP on a purely synthetic dataset, rather than attempting to create a perfect dataset, as noisy data is unavoidable in real-world scenarios and an ideal dataset is unrealistic.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">CXR Report Generation.</span>
To generate the synthetic reports, the pipeline is depicted in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F6" title="Figure 6 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">6</span></a>. We select a general LLM, Llama3.1-70B-Instruct<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct</span></span></span> as the report generator, and we extensively ablate the performance of the report generator with other LLMs in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.fig1" title="Figure 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4</span></a>. We query the LLM using prompts that include the entity list, as shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F6" title="Figure 6 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Since we aim to build a synthetic dataset without a long-tailed distribution, we design a balanced sampling strategy to ensure that the appearance frequency of each entity type is approximately equal across the synthetic dataset. Let <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">caligraphic_E</annotation></semantics></math> be the set of entities, categorized into five types: <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">ABNORMALITY</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2">NON-ABNORMALITY</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.3">DISEASE</span>, <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.4">NON-DISEASE</span>, and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.5">ANATOMY</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.3">For each generation, we sample:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{S}_{1}=\{e_{1}^{(i)},e_{2}^{(i)},\ldots,e_{k}^{(i)}\},\quad\forall e_%
{j}^{(i)}\in\{\texttt{{ABNORMALITY}},\texttt{{NON-ABNORMALITY}},\texttt{{%
DISEASE}},\texttt{{NON-DISEASE}}\}" class="ltx_Math" display="block" id="S3.Ex1.m1.11"><semantics id="S3.Ex1.m1.11a"><mrow id="S3.Ex1.m1.11.11.2" xref="S3.Ex1.m1.11.11.3.cmml"><mrow id="S3.Ex1.m1.10.10.1.1" xref="S3.Ex1.m1.10.10.1.1.cmml"><msub id="S3.Ex1.m1.10.10.1.1.5" xref="S3.Ex1.m1.10.10.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.10.10.1.1.5.2" xref="S3.Ex1.m1.10.10.1.1.5.2.cmml">𝒮</mi><mn id="S3.Ex1.m1.10.10.1.1.5.3" xref="S3.Ex1.m1.10.10.1.1.5.3.cmml">1</mn></msub><mo id="S3.Ex1.m1.10.10.1.1.4" xref="S3.Ex1.m1.10.10.1.1.4.cmml">=</mo><mrow id="S3.Ex1.m1.10.10.1.1.3.3" xref="S3.Ex1.m1.10.10.1.1.3.4.cmml"><mo id="S3.Ex1.m1.10.10.1.1.3.3.4" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.3.4.cmml">{</mo><msubsup id="S3.Ex1.m1.10.10.1.1.1.1.1" xref="S3.Ex1.m1.10.10.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.10.10.1.1.1.1.1.2.2" xref="S3.Ex1.m1.10.10.1.1.1.1.1.2.2.cmml">e</mi><mn id="S3.Ex1.m1.10.10.1.1.1.1.1.2.3" xref="S3.Ex1.m1.10.10.1.1.1.1.1.2.3.cmml">1</mn><mrow id="S3.Ex1.m1.1.1.1.3" xref="S3.Ex1.m1.10.10.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.1.1.1.3.1" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.1.1.1.cmml">(</mo><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex1.m1.1.1.1.3.2" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.Ex1.m1.10.10.1.1.3.3.5" xref="S3.Ex1.m1.10.10.1.1.3.4.cmml">,</mo><msubsup id="S3.Ex1.m1.10.10.1.1.2.2.2" xref="S3.Ex1.m1.10.10.1.1.2.2.2.cmml"><mi id="S3.Ex1.m1.10.10.1.1.2.2.2.2.2" xref="S3.Ex1.m1.10.10.1.1.2.2.2.2.2.cmml">e</mi><mn id="S3.Ex1.m1.10.10.1.1.2.2.2.2.3" xref="S3.Ex1.m1.10.10.1.1.2.2.2.2.3.cmml">2</mn><mrow id="S3.Ex1.m1.2.2.1.3" xref="S3.Ex1.m1.10.10.1.1.2.2.2.cmml"><mo id="S3.Ex1.m1.2.2.1.3.1" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.2.2.2.cmml">(</mo><mi id="S3.Ex1.m1.2.2.1.1" xref="S3.Ex1.m1.2.2.1.1.cmml">i</mi><mo id="S3.Ex1.m1.2.2.1.3.2" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.2.2.2.cmml">)</mo></mrow></msubsup><mo id="S3.Ex1.m1.10.10.1.1.3.3.6" xref="S3.Ex1.m1.10.10.1.1.3.4.cmml">,</mo><mi id="S3.Ex1.m1.5.5" mathvariant="normal" xref="S3.Ex1.m1.5.5.cmml">…</mi><mo id="S3.Ex1.m1.10.10.1.1.3.3.7" xref="S3.Ex1.m1.10.10.1.1.3.4.cmml">,</mo><msubsup id="S3.Ex1.m1.10.10.1.1.3.3.3" xref="S3.Ex1.m1.10.10.1.1.3.3.3.cmml"><mi id="S3.Ex1.m1.10.10.1.1.3.3.3.2.2" xref="S3.Ex1.m1.10.10.1.1.3.3.3.2.2.cmml">e</mi><mi id="S3.Ex1.m1.10.10.1.1.3.3.3.2.3" xref="S3.Ex1.m1.10.10.1.1.3.3.3.2.3.cmml">k</mi><mrow id="S3.Ex1.m1.3.3.1.3" xref="S3.Ex1.m1.10.10.1.1.3.3.3.cmml"><mo id="S3.Ex1.m1.3.3.1.3.1" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.3.3.3.cmml">(</mo><mi id="S3.Ex1.m1.3.3.1.1" xref="S3.Ex1.m1.3.3.1.1.cmml">i</mi><mo id="S3.Ex1.m1.3.3.1.3.2" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.3.3.3.cmml">)</mo></mrow></msubsup><mo id="S3.Ex1.m1.10.10.1.1.3.3.8" stretchy="false" xref="S3.Ex1.m1.10.10.1.1.3.4.cmml">}</mo></mrow></mrow><mo id="S3.Ex1.m1.11.11.2.3" rspace="1.167em" xref="S3.Ex1.m1.11.11.3a.cmml">,</mo><mrow id="S3.Ex1.m1.11.11.2.2" xref="S3.Ex1.m1.11.11.2.2.cmml"><mrow id="S3.Ex1.m1.11.11.2.2.2" xref="S3.Ex1.m1.11.11.2.2.2.cmml"><mo id="S3.Ex1.m1.11.11.2.2.2.1" rspace="0.167em" xref="S3.Ex1.m1.11.11.2.2.2.1.cmml">∀</mo><msubsup id="S3.Ex1.m1.11.11.2.2.2.2" xref="S3.Ex1.m1.11.11.2.2.2.2.cmml"><mi id="S3.Ex1.m1.11.11.2.2.2.2.2.2" xref="S3.Ex1.m1.11.11.2.2.2.2.2.2.cmml">e</mi><mi id="S3.Ex1.m1.11.11.2.2.2.2.2.3" xref="S3.Ex1.m1.11.11.2.2.2.2.2.3.cmml">j</mi><mrow id="S3.Ex1.m1.4.4.1.3" xref="S3.Ex1.m1.11.11.2.2.2.2.cmml"><mo id="S3.Ex1.m1.4.4.1.3.1" stretchy="false" xref="S3.Ex1.m1.11.11.2.2.2.2.cmml">(</mo><mi id="S3.Ex1.m1.4.4.1.1" xref="S3.Ex1.m1.4.4.1.1.cmml">i</mi><mo id="S3.Ex1.m1.4.4.1.3.2" stretchy="false" xref="S3.Ex1.m1.11.11.2.2.2.2.cmml">)</mo></mrow></msubsup></mrow><mo id="S3.Ex1.m1.11.11.2.2.1" xref="S3.Ex1.m1.11.11.2.2.1.cmml">∈</mo><mrow id="S3.Ex1.m1.11.11.2.2.3.2" xref="S3.Ex1.m1.11.11.2.2.3.1.cmml"><mo id="S3.Ex1.m1.11.11.2.2.3.2.1" stretchy="false" xref="S3.Ex1.m1.11.11.2.2.3.1.cmml">{</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.6.6" xref="S3.Ex1.m1.6.6a.cmml">ABNORMALITY</mtext><mo id="S3.Ex1.m1.11.11.2.2.3.2.2" xref="S3.Ex1.m1.11.11.2.2.3.1.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.7.7" xref="S3.Ex1.m1.7.7a.cmml">NON-ABNORMALITY</mtext><mo id="S3.Ex1.m1.11.11.2.2.3.2.3" xref="S3.Ex1.m1.11.11.2.2.3.1.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.8.8" xref="S3.Ex1.m1.8.8a.cmml">DISEASE</mtext><mo id="S3.Ex1.m1.11.11.2.2.3.2.4" xref="S3.Ex1.m1.11.11.2.2.3.1.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.9.9" xref="S3.Ex1.m1.9.9a.cmml">NON-DISEASE</mtext><mo id="S3.Ex1.m1.11.11.2.2.3.2.5" stretchy="false" xref="S3.Ex1.m1.11.11.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.11b"><apply id="S3.Ex1.m1.11.11.3.cmml" xref="S3.Ex1.m1.11.11.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.11.11.3a.cmml" xref="S3.Ex1.m1.11.11.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m1.10.10.1.1.cmml" xref="S3.Ex1.m1.10.10.1.1"><eq id="S3.Ex1.m1.10.10.1.1.4.cmml" xref="S3.Ex1.m1.10.10.1.1.4"></eq><apply id="S3.Ex1.m1.10.10.1.1.5.cmml" xref="S3.Ex1.m1.10.10.1.1.5"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.5.1.cmml" xref="S3.Ex1.m1.10.10.1.1.5">subscript</csymbol><ci id="S3.Ex1.m1.10.10.1.1.5.2.cmml" xref="S3.Ex1.m1.10.10.1.1.5.2">𝒮</ci><cn id="S3.Ex1.m1.10.10.1.1.5.3.cmml" type="integer" xref="S3.Ex1.m1.10.10.1.1.5.3">1</cn></apply><set id="S3.Ex1.m1.10.10.1.1.3.4.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3"><apply id="S3.Ex1.m1.10.10.1.1.1.1.1.cmml" xref="S3.Ex1.m1.10.10.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.10.10.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex1.m1.10.10.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.10.10.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.10.10.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.10.10.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.10.10.1.1.1.1.1.2.2">𝑒</ci><cn id="S3.Ex1.m1.10.10.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.Ex1.m1.10.10.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">𝑖</ci></apply><apply id="S3.Ex1.m1.10.10.1.1.2.2.2.cmml" xref="S3.Ex1.m1.10.10.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.2.2.2.1.cmml" xref="S3.Ex1.m1.10.10.1.1.2.2.2">superscript</csymbol><apply id="S3.Ex1.m1.10.10.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.10.10.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.10.10.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.10.10.1.1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.10.10.1.1.2.2.2.2.2">𝑒</ci><cn id="S3.Ex1.m1.10.10.1.1.2.2.2.2.3.cmml" type="integer" xref="S3.Ex1.m1.10.10.1.1.2.2.2.2.3">2</cn></apply><ci id="S3.Ex1.m1.2.2.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1">𝑖</ci></apply><ci id="S3.Ex1.m1.5.5.cmml" xref="S3.Ex1.m1.5.5">…</ci><apply id="S3.Ex1.m1.10.10.1.1.3.3.3.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.3.3.3.1.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3.3">superscript</csymbol><apply id="S3.Ex1.m1.10.10.1.1.3.3.3.2.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.10.10.1.1.3.3.3.2.1.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex1.m1.10.10.1.1.3.3.3.2.2.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3.3.2.2">𝑒</ci><ci id="S3.Ex1.m1.10.10.1.1.3.3.3.2.3.cmml" xref="S3.Ex1.m1.10.10.1.1.3.3.3.2.3">𝑘</ci></apply><ci id="S3.Ex1.m1.3.3.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1">𝑖</ci></apply></set></apply><apply id="S3.Ex1.m1.11.11.2.2.cmml" xref="S3.Ex1.m1.11.11.2.2"><in id="S3.Ex1.m1.11.11.2.2.1.cmml" xref="S3.Ex1.m1.11.11.2.2.1"></in><apply id="S3.Ex1.m1.11.11.2.2.2.cmml" xref="S3.Ex1.m1.11.11.2.2.2"><csymbol cd="latexml" id="S3.Ex1.m1.11.11.2.2.2.1.cmml" xref="S3.Ex1.m1.11.11.2.2.2.1">for-all</csymbol><apply id="S3.Ex1.m1.11.11.2.2.2.2.cmml" xref="S3.Ex1.m1.11.11.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.11.11.2.2.2.2.1.cmml" xref="S3.Ex1.m1.11.11.2.2.2.2">superscript</csymbol><apply id="S3.Ex1.m1.11.11.2.2.2.2.2.cmml" xref="S3.Ex1.m1.11.11.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.11.11.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.11.11.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.11.11.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.11.11.2.2.2.2.2.2">𝑒</ci><ci id="S3.Ex1.m1.11.11.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.11.11.2.2.2.2.2.3">𝑗</ci></apply><ci id="S3.Ex1.m1.4.4.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1">𝑖</ci></apply></apply><set id="S3.Ex1.m1.11.11.2.2.3.1.cmml" xref="S3.Ex1.m1.11.11.2.2.3.2"><ci id="S3.Ex1.m1.6.6a.cmml" xref="S3.Ex1.m1.6.6"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.6.6.cmml" xref="S3.Ex1.m1.6.6">ABNORMALITY</mtext></ci><ci id="S3.Ex1.m1.7.7a.cmml" xref="S3.Ex1.m1.7.7"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.7.7.cmml" xref="S3.Ex1.m1.7.7">NON-ABNORMALITY</mtext></ci><ci id="S3.Ex1.m1.8.8a.cmml" xref="S3.Ex1.m1.8.8"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.8.8.cmml" xref="S3.Ex1.m1.8.8">DISEASE</mtext></ci><ci id="S3.Ex1.m1.9.9a.cmml" xref="S3.Ex1.m1.9.9"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m1.9.9.cmml" xref="S3.Ex1.m1.9.9">NON-DISEASE</mtext></ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.11c">\mathcal{S}_{1}=\{e_{1}^{(i)},e_{2}^{(i)},\ldots,e_{k}^{(i)}\},\quad\forall e_%
{j}^{(i)}\in\{\texttt{{ABNORMALITY}},\texttt{{NON-ABNORMALITY}},\texttt{{%
DISEASE}},\texttt{{NON-DISEASE}}\}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.11d">caligraphic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } , ∀ italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∈ { ABNORMALITY , NON-ABNORMALITY , DISEASE , NON-DISEASE }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p4.1">where <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_k</annotation></semantics></math> is the number of entities sampled from the first four categories. Additionally, we sample:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{S}_{2}=\{a_{1}^{(i)},a_{2}^{(i)},\ldots,a_{m}^{(i)}\},\quad\forall a_%
{j}^{(i)}\in\texttt{{ANATOMY}}" class="ltx_Math" display="block" id="S3.Ex2.m1.7"><semantics id="S3.Ex2.m1.7a"><mrow id="S3.Ex2.m1.7.7.2" xref="S3.Ex2.m1.7.7.3.cmml"><mrow id="S3.Ex2.m1.6.6.1.1" xref="S3.Ex2.m1.6.6.1.1.cmml"><msub id="S3.Ex2.m1.6.6.1.1.5" xref="S3.Ex2.m1.6.6.1.1.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.6.6.1.1.5.2" xref="S3.Ex2.m1.6.6.1.1.5.2.cmml">𝒮</mi><mn id="S3.Ex2.m1.6.6.1.1.5.3" xref="S3.Ex2.m1.6.6.1.1.5.3.cmml">2</mn></msub><mo id="S3.Ex2.m1.6.6.1.1.4" xref="S3.Ex2.m1.6.6.1.1.4.cmml">=</mo><mrow id="S3.Ex2.m1.6.6.1.1.3.3" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml"><mo id="S3.Ex2.m1.6.6.1.1.3.3.4" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml">{</mo><msubsup id="S3.Ex2.m1.6.6.1.1.1.1.1" xref="S3.Ex2.m1.6.6.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.6.6.1.1.1.1.1.2.2" xref="S3.Ex2.m1.6.6.1.1.1.1.1.2.2.cmml">a</mi><mn id="S3.Ex2.m1.6.6.1.1.1.1.1.2.3" xref="S3.Ex2.m1.6.6.1.1.1.1.1.2.3.cmml">1</mn><mrow id="S3.Ex2.m1.1.1.1.3" xref="S3.Ex2.m1.6.6.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.1.1.1.3.1" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.1.1.1.cmml">(</mo><mi id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex2.m1.1.1.1.3.2" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.Ex2.m1.6.6.1.1.3.3.5" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml">,</mo><msubsup id="S3.Ex2.m1.6.6.1.1.2.2.2" xref="S3.Ex2.m1.6.6.1.1.2.2.2.cmml"><mi id="S3.Ex2.m1.6.6.1.1.2.2.2.2.2" xref="S3.Ex2.m1.6.6.1.1.2.2.2.2.2.cmml">a</mi><mn id="S3.Ex2.m1.6.6.1.1.2.2.2.2.3" xref="S3.Ex2.m1.6.6.1.1.2.2.2.2.3.cmml">2</mn><mrow id="S3.Ex2.m1.2.2.1.3" xref="S3.Ex2.m1.6.6.1.1.2.2.2.cmml"><mo id="S3.Ex2.m1.2.2.1.3.1" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.2.2.2.cmml">(</mo><mi id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml">i</mi><mo id="S3.Ex2.m1.2.2.1.3.2" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.2.2.2.cmml">)</mo></mrow></msubsup><mo id="S3.Ex2.m1.6.6.1.1.3.3.6" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml">,</mo><mi id="S3.Ex2.m1.5.5" mathvariant="normal" xref="S3.Ex2.m1.5.5.cmml">…</mi><mo id="S3.Ex2.m1.6.6.1.1.3.3.7" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml">,</mo><msubsup id="S3.Ex2.m1.6.6.1.1.3.3.3" xref="S3.Ex2.m1.6.6.1.1.3.3.3.cmml"><mi id="S3.Ex2.m1.6.6.1.1.3.3.3.2.2" xref="S3.Ex2.m1.6.6.1.1.3.3.3.2.2.cmml">a</mi><mi id="S3.Ex2.m1.6.6.1.1.3.3.3.2.3" xref="S3.Ex2.m1.6.6.1.1.3.3.3.2.3.cmml">m</mi><mrow id="S3.Ex2.m1.3.3.1.3" xref="S3.Ex2.m1.6.6.1.1.3.3.3.cmml"><mo id="S3.Ex2.m1.3.3.1.3.1" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.3.3.3.cmml">(</mo><mi id="S3.Ex2.m1.3.3.1.1" xref="S3.Ex2.m1.3.3.1.1.cmml">i</mi><mo id="S3.Ex2.m1.3.3.1.3.2" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.3.3.3.cmml">)</mo></mrow></msubsup><mo id="S3.Ex2.m1.6.6.1.1.3.3.8" stretchy="false" xref="S3.Ex2.m1.6.6.1.1.3.4.cmml">}</mo></mrow></mrow><mo id="S3.Ex2.m1.7.7.2.3" rspace="1.167em" xref="S3.Ex2.m1.7.7.3a.cmml">,</mo><mrow id="S3.Ex2.m1.7.7.2.2" xref="S3.Ex2.m1.7.7.2.2.cmml"><mrow id="S3.Ex2.m1.7.7.2.2.2" xref="S3.Ex2.m1.7.7.2.2.2.cmml"><mo id="S3.Ex2.m1.7.7.2.2.2.1" rspace="0.167em" xref="S3.Ex2.m1.7.7.2.2.2.1.cmml">∀</mo><msubsup id="S3.Ex2.m1.7.7.2.2.2.2" xref="S3.Ex2.m1.7.7.2.2.2.2.cmml"><mi id="S3.Ex2.m1.7.7.2.2.2.2.2.2" xref="S3.Ex2.m1.7.7.2.2.2.2.2.2.cmml">a</mi><mi id="S3.Ex2.m1.7.7.2.2.2.2.2.3" xref="S3.Ex2.m1.7.7.2.2.2.2.2.3.cmml">j</mi><mrow id="S3.Ex2.m1.4.4.1.3" xref="S3.Ex2.m1.7.7.2.2.2.2.cmml"><mo id="S3.Ex2.m1.4.4.1.3.1" stretchy="false" xref="S3.Ex2.m1.7.7.2.2.2.2.cmml">(</mo><mi id="S3.Ex2.m1.4.4.1.1" xref="S3.Ex2.m1.4.4.1.1.cmml">i</mi><mo id="S3.Ex2.m1.4.4.1.3.2" stretchy="false" xref="S3.Ex2.m1.7.7.2.2.2.2.cmml">)</mo></mrow></msubsup></mrow><mo id="S3.Ex2.m1.7.7.2.2.1" xref="S3.Ex2.m1.7.7.2.2.1.cmml">∈</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex2.m1.7.7.2.2.3" xref="S3.Ex2.m1.7.7.2.2.3a.cmml">ANATOMY</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.7b"><apply id="S3.Ex2.m1.7.7.3.cmml" xref="S3.Ex2.m1.7.7.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.7.7.3a.cmml" xref="S3.Ex2.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S3.Ex2.m1.6.6.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1"><eq id="S3.Ex2.m1.6.6.1.1.4.cmml" xref="S3.Ex2.m1.6.6.1.1.4"></eq><apply id="S3.Ex2.m1.6.6.1.1.5.cmml" xref="S3.Ex2.m1.6.6.1.1.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.5.1.cmml" xref="S3.Ex2.m1.6.6.1.1.5">subscript</csymbol><ci id="S3.Ex2.m1.6.6.1.1.5.2.cmml" xref="S3.Ex2.m1.6.6.1.1.5.2">𝒮</ci><cn id="S3.Ex2.m1.6.6.1.1.5.3.cmml" type="integer" xref="S3.Ex2.m1.6.6.1.1.5.3">2</cn></apply><set id="S3.Ex2.m1.6.6.1.1.3.4.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3"><apply id="S3.Ex2.m1.6.6.1.1.1.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex2.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.6.6.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.1.1.1.2.2">𝑎</ci><cn id="S3.Ex2.m1.6.6.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.Ex2.m1.6.6.1.1.1.1.1.2.3">1</cn></apply><ci id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">𝑖</ci></apply><apply id="S3.Ex2.m1.6.6.1.1.2.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.2.2.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.2.2.2">superscript</csymbol><apply id="S3.Ex2.m1.6.6.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.2.2.2.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.6.6.1.1.2.2.2.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.2.2.2.2.2">𝑎</ci><cn id="S3.Ex2.m1.6.6.1.1.2.2.2.2.3.cmml" type="integer" xref="S3.Ex2.m1.6.6.1.1.2.2.2.2.3">2</cn></apply><ci id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1">𝑖</ci></apply><ci id="S3.Ex2.m1.5.5.cmml" xref="S3.Ex2.m1.5.5">…</ci><apply id="S3.Ex2.m1.6.6.1.1.3.3.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.3.3.3.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.3">superscript</csymbol><apply id="S3.Ex2.m1.6.6.1.1.3.3.3.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.6.6.1.1.3.3.3.2.1.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex2.m1.6.6.1.1.3.3.3.2.2.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.3.2.2">𝑎</ci><ci id="S3.Ex2.m1.6.6.1.1.3.3.3.2.3.cmml" xref="S3.Ex2.m1.6.6.1.1.3.3.3.2.3">𝑚</ci></apply><ci id="S3.Ex2.m1.3.3.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1">𝑖</ci></apply></set></apply><apply id="S3.Ex2.m1.7.7.2.2.cmml" xref="S3.Ex2.m1.7.7.2.2"><in id="S3.Ex2.m1.7.7.2.2.1.cmml" xref="S3.Ex2.m1.7.7.2.2.1"></in><apply id="S3.Ex2.m1.7.7.2.2.2.cmml" xref="S3.Ex2.m1.7.7.2.2.2"><csymbol cd="latexml" id="S3.Ex2.m1.7.7.2.2.2.1.cmml" xref="S3.Ex2.m1.7.7.2.2.2.1">for-all</csymbol><apply id="S3.Ex2.m1.7.7.2.2.2.2.cmml" xref="S3.Ex2.m1.7.7.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.7.7.2.2.2.2.1.cmml" xref="S3.Ex2.m1.7.7.2.2.2.2">superscript</csymbol><apply id="S3.Ex2.m1.7.7.2.2.2.2.2.cmml" xref="S3.Ex2.m1.7.7.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.7.7.2.2.2.2.2.1.cmml" xref="S3.Ex2.m1.7.7.2.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.7.7.2.2.2.2.2.2.cmml" xref="S3.Ex2.m1.7.7.2.2.2.2.2.2">𝑎</ci><ci id="S3.Ex2.m1.7.7.2.2.2.2.2.3.cmml" xref="S3.Ex2.m1.7.7.2.2.2.2.2.3">𝑗</ci></apply><ci id="S3.Ex2.m1.4.4.1.1.cmml" xref="S3.Ex2.m1.4.4.1.1">𝑖</ci></apply></apply><ci id="S3.Ex2.m1.7.7.2.2.3a.cmml" xref="S3.Ex2.m1.7.7.2.2.3"><mtext class="ltx_mathvariant_monospace" id="S3.Ex2.m1.7.7.2.2.3.cmml" xref="S3.Ex2.m1.7.7.2.2.3">ANATOMY</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.7c">\mathcal{S}_{2}=\{a_{1}^{(i)},a_{2}^{(i)},\ldots,a_{m}^{(i)}\},\quad\forall a_%
{j}^{(i)}\in\texttt{{ANATOMY}}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.7d">caligraphic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = { italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT } , ∀ italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∈ ANATOMY</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p4.2">where <math alttext="m" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m1.1"><semantics id="S3.SS2.p4.2.m1.1a"><mi id="S3.SS2.p4.2.m1.1.1" xref="S3.SS2.p4.2.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m1.1b"><ci id="S3.SS2.p4.2.m1.1.1.cmml" xref="S3.SS2.p4.2.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m1.1d">italic_m</annotation></semantics></math> is the number of entities sampled from the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.2.1">ANATOMY</span> category. Thus, the total sampled entity set for each generation is:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{S}=\mathcal{S}_{1}\cup\mathcal{S}_{2}" class="ltx_Math" display="block" id="S3.Ex3.m1.1"><semantics id="S3.Ex3.m1.1a"><mrow id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.1.1.2" xref="S3.Ex3.m1.1.1.2.cmml">𝒮</mi><mo id="S3.Ex3.m1.1.1.1" xref="S3.Ex3.m1.1.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.1.1.3" xref="S3.Ex3.m1.1.1.3.cmml"><msub id="S3.Ex3.m1.1.1.3.2" xref="S3.Ex3.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.1.1.3.2.2" xref="S3.Ex3.m1.1.1.3.2.2.cmml">𝒮</mi><mn id="S3.Ex3.m1.1.1.3.2.3" xref="S3.Ex3.m1.1.1.3.2.3.cmml">1</mn></msub><mo id="S3.Ex3.m1.1.1.3.1" xref="S3.Ex3.m1.1.1.3.1.cmml">∪</mo><msub id="S3.Ex3.m1.1.1.3.3" xref="S3.Ex3.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.1.1.3.3.2" xref="S3.Ex3.m1.1.1.3.3.2.cmml">𝒮</mi><mn id="S3.Ex3.m1.1.1.3.3.3" xref="S3.Ex3.m1.1.1.3.3.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.1b"><apply id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1"><eq id="S3.Ex3.m1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1"></eq><ci id="S3.Ex3.m1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.2">𝒮</ci><apply id="S3.Ex3.m1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.3"><union id="S3.Ex3.m1.1.1.3.1.cmml" xref="S3.Ex3.m1.1.1.3.1"></union><apply id="S3.Ex3.m1.1.1.3.2.cmml" xref="S3.Ex3.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.3.2.1.cmml" xref="S3.Ex3.m1.1.1.3.2">subscript</csymbol><ci id="S3.Ex3.m1.1.1.3.2.2.cmml" xref="S3.Ex3.m1.1.1.3.2.2">𝒮</ci><cn id="S3.Ex3.m1.1.1.3.2.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.3.2.3">1</cn></apply><apply id="S3.Ex3.m1.1.1.3.3.cmml" xref="S3.Ex3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.3.3.1.cmml" xref="S3.Ex3.m1.1.1.3.3">subscript</csymbol><ci id="S3.Ex3.m1.1.1.3.3.2.cmml" xref="S3.Ex3.m1.1.1.3.3.2">𝒮</ci><cn id="S3.Ex3.m1.1.1.3.3.3.cmml" type="integer" xref="S3.Ex3.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.1c">\mathcal{S}=\mathcal{S}_{1}\cup\mathcal{S}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex3.m1.1d">caligraphic_S = caligraphic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∪ caligraphic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.6">We impose a maximum frequency threshold, <math alttext="\tau_{\max}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">τ</mi><mi id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml">max</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝜏</ci><max id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3"></max></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\tau_{\max}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_τ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT</annotation></semantics></math>, for each entity <math alttext="e\in\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2.1"><semantics id="S3.SS2.p5.2.m2.1a"><mrow id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml"><mi id="S3.SS2.p5.2.m2.1.1.2" xref="S3.SS2.p5.2.m2.1.1.2.cmml">e</mi><mo id="S3.SS2.p5.2.m2.1.1.1" xref="S3.SS2.p5.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.2.m2.1.1.3" xref="S3.SS2.p5.2.m2.1.1.3.cmml">ℰ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><apply id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1"><in id="S3.SS2.p5.2.m2.1.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1.1"></in><ci id="S3.SS2.p5.2.m2.1.1.2.cmml" xref="S3.SS2.p5.2.m2.1.1.2">𝑒</ci><ci id="S3.SS2.p5.2.m2.1.1.3.cmml" xref="S3.SS2.p5.2.m2.1.1.3">ℰ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">e\in\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.2.m2.1d">italic_e ∈ caligraphic_E</annotation></semantics></math>. If an entity <math alttext="e_{j}^{(i)}" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3.1"><semantics id="S3.SS2.p5.3.m3.1a"><msubsup id="S3.SS2.p5.3.m3.1.2" xref="S3.SS2.p5.3.m3.1.2.cmml"><mi id="S3.SS2.p5.3.m3.1.2.2.2" xref="S3.SS2.p5.3.m3.1.2.2.2.cmml">e</mi><mi id="S3.SS2.p5.3.m3.1.2.2.3" xref="S3.SS2.p5.3.m3.1.2.2.3.cmml">j</mi><mrow id="S3.SS2.p5.3.m3.1.1.1.3" xref="S3.SS2.p5.3.m3.1.2.cmml"><mo id="S3.SS2.p5.3.m3.1.1.1.3.1" stretchy="false" xref="S3.SS2.p5.3.m3.1.2.cmml">(</mo><mi id="S3.SS2.p5.3.m3.1.1.1.1" xref="S3.SS2.p5.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p5.3.m3.1.1.1.3.2" stretchy="false" xref="S3.SS2.p5.3.m3.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.2.cmml" xref="S3.SS2.p5.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.2.1.cmml" xref="S3.SS2.p5.3.m3.1.2">superscript</csymbol><apply id="S3.SS2.p5.3.m3.1.2.2.cmml" xref="S3.SS2.p5.3.m3.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.2.2.1.cmml" xref="S3.SS2.p5.3.m3.1.2">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.2.2.2.cmml" xref="S3.SS2.p5.3.m3.1.2.2.2">𝑒</ci><ci id="S3.SS2.p5.3.m3.1.2.2.3.cmml" xref="S3.SS2.p5.3.m3.1.2.2.3">𝑗</ci></apply><ci id="S3.SS2.p5.3.m3.1.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">e_{j}^{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.3.m3.1d">italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> in <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4.1"><semantics id="S3.SS2.p5.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><ci id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.4.m4.1d">caligraphic_S</annotation></semantics></math> reaches this threshold, we resample <math alttext="e_{j}^{(i)}" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m5.1"><semantics id="S3.SS2.p5.5.m5.1a"><msubsup id="S3.SS2.p5.5.m5.1.2" xref="S3.SS2.p5.5.m5.1.2.cmml"><mi id="S3.SS2.p5.5.m5.1.2.2.2" xref="S3.SS2.p5.5.m5.1.2.2.2.cmml">e</mi><mi id="S3.SS2.p5.5.m5.1.2.2.3" xref="S3.SS2.p5.5.m5.1.2.2.3.cmml">j</mi><mrow id="S3.SS2.p5.5.m5.1.1.1.3" xref="S3.SS2.p5.5.m5.1.2.cmml"><mo id="S3.SS2.p5.5.m5.1.1.1.3.1" stretchy="false" xref="S3.SS2.p5.5.m5.1.2.cmml">(</mo><mi id="S3.SS2.p5.5.m5.1.1.1.1" xref="S3.SS2.p5.5.m5.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p5.5.m5.1.1.1.3.2" stretchy="false" xref="S3.SS2.p5.5.m5.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><apply id="S3.SS2.p5.5.m5.1.2.cmml" xref="S3.SS2.p5.5.m5.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.2.1.cmml" xref="S3.SS2.p5.5.m5.1.2">superscript</csymbol><apply id="S3.SS2.p5.5.m5.1.2.2.cmml" xref="S3.SS2.p5.5.m5.1.2"><csymbol cd="ambiguous" id="S3.SS2.p5.5.m5.1.2.2.1.cmml" xref="S3.SS2.p5.5.m5.1.2">subscript</csymbol><ci id="S3.SS2.p5.5.m5.1.2.2.2.cmml" xref="S3.SS2.p5.5.m5.1.2.2.2">𝑒</ci><ci id="S3.SS2.p5.5.m5.1.2.2.3.cmml" xref="S3.SS2.p5.5.m5.1.2.2.3">𝑗</ci></apply><ci id="S3.SS2.p5.5.m5.1.1.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">e_{j}^{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.5.m5.1d">italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> while keeping the remaining entities in <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p5.6.m6.1"><semantics id="S3.SS2.p5.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><ci id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.6.m6.1d">caligraphic_S</annotation></semantics></math> unchanged:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{if }f(e_{j}^{(i)})\geq\tau_{\max},\text{ then resample }e_{j}^{(i)}." class="ltx_Math" display="block" id="S3.Ex4.m1.3"><semantics id="S3.Ex4.m1.3a"><mrow id="S3.Ex4.m1.3.3.1" xref="S3.Ex4.m1.3.3.1.1.cmml"><mrow id="S3.Ex4.m1.3.3.1.1" xref="S3.Ex4.m1.3.3.1.1.cmml"><mrow id="S3.Ex4.m1.3.3.1.1.1" xref="S3.Ex4.m1.3.3.1.1.1.cmml"><mtext id="S3.Ex4.m1.3.3.1.1.1.3" xref="S3.Ex4.m1.3.3.1.1.1.3a.cmml">if </mtext><mo id="S3.Ex4.m1.3.3.1.1.1.2" xref="S3.Ex4.m1.3.3.1.1.1.2.cmml">⁢</mo><mi id="S3.Ex4.m1.3.3.1.1.1.4" xref="S3.Ex4.m1.3.3.1.1.1.4.cmml">f</mi><mo id="S3.Ex4.m1.3.3.1.1.1.2a" xref="S3.Ex4.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex4.m1.3.3.1.1.1.1.1" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.Ex4.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.Ex4.m1.3.3.1.1.1.1.1.1" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.2" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.2.cmml">e</mi><mi id="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.3" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.3.cmml">j</mi><mrow id="S3.Ex4.m1.1.1.1.3" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S3.Ex4.m1.1.1.1.3.1" stretchy="false" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mi id="S3.Ex4.m1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex4.m1.1.1.1.3.2" stretchy="false" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></msubsup><mo id="S3.Ex4.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex4.m1.3.3.1.1.4" xref="S3.Ex4.m1.3.3.1.1.4.cmml">≥</mo><mrow id="S3.Ex4.m1.3.3.1.1.3.2" xref="S3.Ex4.m1.3.3.1.1.3.3.cmml"><msub id="S3.Ex4.m1.3.3.1.1.2.1.1" xref="S3.Ex4.m1.3.3.1.1.2.1.1.cmml"><mi id="S3.Ex4.m1.3.3.1.1.2.1.1.2" xref="S3.Ex4.m1.3.3.1.1.2.1.1.2.cmml">τ</mi><mi id="S3.Ex4.m1.3.3.1.1.2.1.1.3" xref="S3.Ex4.m1.3.3.1.1.2.1.1.3.cmml">max</mi></msub><mo id="S3.Ex4.m1.3.3.1.1.3.2.3" xref="S3.Ex4.m1.3.3.1.1.3.3.cmml">,</mo><mrow id="S3.Ex4.m1.3.3.1.1.3.2.2" xref="S3.Ex4.m1.3.3.1.1.3.2.2.cmml"><mtext id="S3.Ex4.m1.3.3.1.1.3.2.2.2" xref="S3.Ex4.m1.3.3.1.1.3.2.2.2a.cmml"> then resample </mtext><mo id="S3.Ex4.m1.3.3.1.1.3.2.2.1" xref="S3.Ex4.m1.3.3.1.1.3.2.2.1.cmml">⁢</mo><msubsup id="S3.Ex4.m1.3.3.1.1.3.2.2.3" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.cmml"><mi id="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.2" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.2.cmml">e</mi><mi id="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.3" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.3.cmml">j</mi><mrow id="S3.Ex4.m1.2.2.1.3" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.cmml"><mo id="S3.Ex4.m1.2.2.1.3.1" stretchy="false" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.cmml">(</mo><mi id="S3.Ex4.m1.2.2.1.1" xref="S3.Ex4.m1.2.2.1.1.cmml">i</mi><mo id="S3.Ex4.m1.2.2.1.3.2" stretchy="false" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow><mo id="S3.Ex4.m1.3.3.1.2" lspace="0em" xref="S3.Ex4.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.3b"><apply id="S3.Ex4.m1.3.3.1.1.cmml" xref="S3.Ex4.m1.3.3.1"><geq id="S3.Ex4.m1.3.3.1.1.4.cmml" xref="S3.Ex4.m1.3.3.1.1.4"></geq><apply id="S3.Ex4.m1.3.3.1.1.1.cmml" xref="S3.Ex4.m1.3.3.1.1.1"><times id="S3.Ex4.m1.3.3.1.1.1.2.cmml" xref="S3.Ex4.m1.3.3.1.1.1.2"></times><ci id="S3.Ex4.m1.3.3.1.1.1.3a.cmml" xref="S3.Ex4.m1.3.3.1.1.1.3"><mtext id="S3.Ex4.m1.3.3.1.1.1.3.cmml" xref="S3.Ex4.m1.3.3.1.1.1.3">if </mtext></ci><ci id="S3.Ex4.m1.3.3.1.1.1.4.cmml" xref="S3.Ex4.m1.3.3.1.1.1.4">𝑓</ci><apply id="S3.Ex4.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.3.3.1.1.1.1.1">superscript</csymbol><apply id="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex4.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.2">𝑒</ci><ci id="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex4.m1.3.3.1.1.1.1.1.1.2.3">𝑗</ci></apply><ci id="S3.Ex4.m1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1">𝑖</ci></apply></apply><list id="S3.Ex4.m1.3.3.1.1.3.3.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2"><apply id="S3.Ex4.m1.3.3.1.1.2.1.1.cmml" xref="S3.Ex4.m1.3.3.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.3.3.1.1.2.1.1.1.cmml" xref="S3.Ex4.m1.3.3.1.1.2.1.1">subscript</csymbol><ci id="S3.Ex4.m1.3.3.1.1.2.1.1.2.cmml" xref="S3.Ex4.m1.3.3.1.1.2.1.1.2">𝜏</ci><max id="S3.Ex4.m1.3.3.1.1.2.1.1.3.cmml" xref="S3.Ex4.m1.3.3.1.1.2.1.1.3"></max></apply><apply id="S3.Ex4.m1.3.3.1.1.3.2.2.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2"><times id="S3.Ex4.m1.3.3.1.1.3.2.2.1.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.1"></times><ci id="S3.Ex4.m1.3.3.1.1.3.2.2.2a.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.2"><mtext id="S3.Ex4.m1.3.3.1.1.3.2.2.2.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.2"> then resample </mtext></ci><apply id="S3.Ex4.m1.3.3.1.1.3.2.2.3.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.3.3.1.1.3.2.2.3.1.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3">superscript</csymbol><apply id="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.1.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3">subscript</csymbol><ci id="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.2.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.2">𝑒</ci><ci id="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.3.cmml" xref="S3.Ex4.m1.3.3.1.1.3.2.2.3.2.3">𝑗</ci></apply><ci id="S3.Ex4.m1.2.2.1.1.cmml" xref="S3.Ex4.m1.2.2.1.1">𝑖</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.3c">\text{if }f(e_{j}^{(i)})\geq\tau_{\max},\text{ then resample }e_{j}^{(i)}.</annotation><annotation encoding="application/x-llamapun" id="S3.Ex4.m1.3d">if italic_f ( italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) ≥ italic_τ start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT , then resample italic_e start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p5.8">Here, <math alttext="f(e)" class="ltx_Math" display="inline" id="S3.SS2.p5.7.m1.1"><semantics id="S3.SS2.p5.7.m1.1a"><mrow id="S3.SS2.p5.7.m1.1.2" xref="S3.SS2.p5.7.m1.1.2.cmml"><mi id="S3.SS2.p5.7.m1.1.2.2" xref="S3.SS2.p5.7.m1.1.2.2.cmml">f</mi><mo id="S3.SS2.p5.7.m1.1.2.1" xref="S3.SS2.p5.7.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p5.7.m1.1.2.3.2" xref="S3.SS2.p5.7.m1.1.2.cmml"><mo id="S3.SS2.p5.7.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.p5.7.m1.1.2.cmml">(</mo><mi id="S3.SS2.p5.7.m1.1.1" xref="S3.SS2.p5.7.m1.1.1.cmml">e</mi><mo id="S3.SS2.p5.7.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.p5.7.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m1.1b"><apply id="S3.SS2.p5.7.m1.1.2.cmml" xref="S3.SS2.p5.7.m1.1.2"><times id="S3.SS2.p5.7.m1.1.2.1.cmml" xref="S3.SS2.p5.7.m1.1.2.1"></times><ci id="S3.SS2.p5.7.m1.1.2.2.cmml" xref="S3.SS2.p5.7.m1.1.2.2">𝑓</ci><ci id="S3.SS2.p5.7.m1.1.1.cmml" xref="S3.SS2.p5.7.m1.1.1">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m1.1c">f(e)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.7.m1.1d">italic_f ( italic_e )</annotation></semantics></math> denotes the current frequency of entity <math alttext="e" class="ltx_Math" display="inline" id="S3.SS2.p5.8.m2.1"><semantics id="S3.SS2.p5.8.m2.1a"><mi id="S3.SS2.p5.8.m2.1.1" xref="S3.SS2.p5.8.m2.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m2.1b"><ci id="S3.SS2.p5.8.m2.1.1.cmml" xref="S3.SS2.p5.8.m2.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m2.1c">e</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.8.m2.1d">italic_e</annotation></semantics></math> in the dataset. This ensures a balanced distribution of entities across the synthetic dataset.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.6">After sampling, we input the selected entities <math alttext="\mathcal{S}=\mathcal{S}_{1}\cup\mathcal{S}_{2}" class="ltx_Math" display="inline" id="S3.SS2.p6.1.m1.1"><semantics id="S3.SS2.p6.1.m1.1a"><mrow id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.1.m1.1.1.2" xref="S3.SS2.p6.1.m1.1.1.2.cmml">𝒮</mi><mo id="S3.SS2.p6.1.m1.1.1.1" xref="S3.SS2.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p6.1.m1.1.1.3" xref="S3.SS2.p6.1.m1.1.1.3.cmml"><msub id="S3.SS2.p6.1.m1.1.1.3.2" xref="S3.SS2.p6.1.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.1.m1.1.1.3.2.2" xref="S3.SS2.p6.1.m1.1.1.3.2.2.cmml">𝒮</mi><mn id="S3.SS2.p6.1.m1.1.1.3.2.3" xref="S3.SS2.p6.1.m1.1.1.3.2.3.cmml">1</mn></msub><mo id="S3.SS2.p6.1.m1.1.1.3.1" xref="S3.SS2.p6.1.m1.1.1.3.1.cmml">∪</mo><msub id="S3.SS2.p6.1.m1.1.1.3.3" xref="S3.SS2.p6.1.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.1.m1.1.1.3.3.2" xref="S3.SS2.p6.1.m1.1.1.3.3.2.cmml">𝒮</mi><mn id="S3.SS2.p6.1.m1.1.1.3.3.3" xref="S3.SS2.p6.1.m1.1.1.3.3.3.cmml">2</mn></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><apply id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1"><eq id="S3.SS2.p6.1.m1.1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1.1"></eq><ci id="S3.SS2.p6.1.m1.1.1.2.cmml" xref="S3.SS2.p6.1.m1.1.1.2">𝒮</ci><apply id="S3.SS2.p6.1.m1.1.1.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3"><union id="S3.SS2.p6.1.m1.1.1.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.3.1"></union><apply id="S3.SS2.p6.1.m1.1.1.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.p6.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.p6.1.m1.1.1.3.2.2">𝒮</ci><cn id="S3.SS2.p6.1.m1.1.1.3.2.3.cmml" type="integer" xref="S3.SS2.p6.1.m1.1.1.3.2.3">1</cn></apply><apply id="S3.SS2.p6.1.m1.1.1.3.3.cmml" xref="S3.SS2.p6.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p6.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p6.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p6.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p6.1.m1.1.1.3.3.2">𝒮</ci><cn id="S3.SS2.p6.1.m1.1.1.3.3.3.cmml" type="integer" xref="S3.SS2.p6.1.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\mathcal{S}=\mathcal{S}_{1}\cup\mathcal{S}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.1.m1.1d">caligraphic_S = caligraphic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∪ caligraphic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> into the LLM and indicate their type. Let the output of the LLM be denoted as <math alttext="R_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p6.2.m2.1"><semantics id="S3.SS2.p6.2.m2.1a"><msub id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml"><mi id="S3.SS2.p6.2.m2.1.1.2" xref="S3.SS2.p6.2.m2.1.1.2.cmml">R</mi><mrow id="S3.SS2.p6.2.m2.1.1.3" xref="S3.SS2.p6.2.m2.1.1.3.cmml"><mi id="S3.SS2.p6.2.m2.1.1.3.2" xref="S3.SS2.p6.2.m2.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p6.2.m2.1.1.3.1" xref="S3.SS2.p6.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.2.m2.1.1.3.3" xref="S3.SS2.p6.2.m2.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p6.2.m2.1.1.3.1a" xref="S3.SS2.p6.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.2.m2.1.1.3.4" xref="S3.SS2.p6.2.m2.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><apply id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.2.m2.1.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p6.2.m2.1.1.2.cmml" xref="S3.SS2.p6.2.m2.1.1.2">𝑅</ci><apply id="S3.SS2.p6.2.m2.1.1.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3"><times id="S3.SS2.p6.2.m2.1.1.3.1.cmml" xref="S3.SS2.p6.2.m2.1.1.3.1"></times><ci id="S3.SS2.p6.2.m2.1.1.3.2.cmml" xref="S3.SS2.p6.2.m2.1.1.3.2">𝑔</ci><ci id="S3.SS2.p6.2.m2.1.1.3.3.cmml" xref="S3.SS2.p6.2.m2.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.2.m2.1.1.3.4.cmml" xref="S3.SS2.p6.2.m2.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">R_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.2.m2.1d">italic_R start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, which represents the synthetic report generated by the model based on the sampled entities. To ensure that the LLM-generated report <math alttext="R_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p6.3.m3.1"><semantics id="S3.SS2.p6.3.m3.1a"><msub id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mi id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">R</mi><mrow id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml"><mi id="S3.SS2.p6.3.m3.1.1.3.2" xref="S3.SS2.p6.3.m3.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p6.3.m3.1.1.3.1" xref="S3.SS2.p6.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.3.m3.1.1.3.3" xref="S3.SS2.p6.3.m3.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p6.3.m3.1.1.3.1a" xref="S3.SS2.p6.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.3.m3.1.1.3.4" xref="S3.SS2.p6.3.m3.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2">𝑅</ci><apply id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3"><times id="S3.SS2.p6.3.m3.1.1.3.1.cmml" xref="S3.SS2.p6.3.m3.1.1.3.1"></times><ci id="S3.SS2.p6.3.m3.1.1.3.2.cmml" xref="S3.SS2.p6.3.m3.1.1.3.2">𝑔</ci><ci id="S3.SS2.p6.3.m3.1.1.3.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.3.m3.1.1.3.4.cmml" xref="S3.SS2.p6.3.m3.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">R_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.3.m3.1d">italic_R start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math> covers and only includes the entities in <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p6.4.m4.1"><semantics id="S3.SS2.p6.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.4.m4.1.1" xref="S3.SS2.p6.4.m4.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.4.m4.1b"><ci id="S3.SS2.p6.4.m4.1.1.cmml" xref="S3.SS2.p6.4.m4.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.4.m4.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.4.m4.1d">caligraphic_S</annotation></semantics></math> (since the inclusion of non-specified entities would disrupt the frequency balance), we use the RaTE model <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib74" title="">2024</a>)</cite> to extract entities from <math alttext="R_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p6.5.m5.1"><semantics id="S3.SS2.p6.5.m5.1a"><msub id="S3.SS2.p6.5.m5.1.1" xref="S3.SS2.p6.5.m5.1.1.cmml"><mi id="S3.SS2.p6.5.m5.1.1.2" xref="S3.SS2.p6.5.m5.1.1.2.cmml">R</mi><mrow id="S3.SS2.p6.5.m5.1.1.3" xref="S3.SS2.p6.5.m5.1.1.3.cmml"><mi id="S3.SS2.p6.5.m5.1.1.3.2" xref="S3.SS2.p6.5.m5.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p6.5.m5.1.1.3.1" xref="S3.SS2.p6.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.5.m5.1.1.3.3" xref="S3.SS2.p6.5.m5.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p6.5.m5.1.1.3.1a" xref="S3.SS2.p6.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.5.m5.1.1.3.4" xref="S3.SS2.p6.5.m5.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.5.m5.1b"><apply id="S3.SS2.p6.5.m5.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.p6.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.p6.5.m5.1.1.2">𝑅</ci><apply id="S3.SS2.p6.5.m5.1.1.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3"><times id="S3.SS2.p6.5.m5.1.1.3.1.cmml" xref="S3.SS2.p6.5.m5.1.1.3.1"></times><ci id="S3.SS2.p6.5.m5.1.1.3.2.cmml" xref="S3.SS2.p6.5.m5.1.1.3.2">𝑔</ci><ci id="S3.SS2.p6.5.m5.1.1.3.3.cmml" xref="S3.SS2.p6.5.m5.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.5.m5.1.1.3.4.cmml" xref="S3.SS2.p6.5.m5.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.5.m5.1c">R_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.5.m5.1d">italic_R start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, denoted as <math alttext="\mathcal{E}_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p6.6.m6.1"><semantics id="S3.SS2.p6.6.m6.1a"><msub id="S3.SS2.p6.6.m6.1.1" xref="S3.SS2.p6.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p6.6.m6.1.1.2" xref="S3.SS2.p6.6.m6.1.1.2.cmml">ℰ</mi><mrow id="S3.SS2.p6.6.m6.1.1.3" xref="S3.SS2.p6.6.m6.1.1.3.cmml"><mi id="S3.SS2.p6.6.m6.1.1.3.2" xref="S3.SS2.p6.6.m6.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p6.6.m6.1.1.3.1" xref="S3.SS2.p6.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.6.m6.1.1.3.3" xref="S3.SS2.p6.6.m6.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p6.6.m6.1.1.3.1a" xref="S3.SS2.p6.6.m6.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p6.6.m6.1.1.3.4" xref="S3.SS2.p6.6.m6.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.6.m6.1b"><apply id="S3.SS2.p6.6.m6.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.6.m6.1.1.1.cmml" xref="S3.SS2.p6.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p6.6.m6.1.1.2.cmml" xref="S3.SS2.p6.6.m6.1.1.2">ℰ</ci><apply id="S3.SS2.p6.6.m6.1.1.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3"><times id="S3.SS2.p6.6.m6.1.1.3.1.cmml" xref="S3.SS2.p6.6.m6.1.1.3.1"></times><ci id="S3.SS2.p6.6.m6.1.1.3.2.cmml" xref="S3.SS2.p6.6.m6.1.1.3.2">𝑔</ci><ci id="S3.SS2.p6.6.m6.1.1.3.3.cmml" xref="S3.SS2.p6.6.m6.1.1.3.3">𝑒</ci><ci id="S3.SS2.p6.6.m6.1.1.3.4.cmml" xref="S3.SS2.p6.6.m6.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.6.m6.1c">\mathcal{E}_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p6.6.m6.1d">caligraphic_E start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.5">We then verify the entity set <math alttext="\mathcal{E}_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p7.1.m1.1"><semantics id="S3.SS2.p7.1.m1.1a"><msub id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.1.m1.1.1.2" xref="S3.SS2.p7.1.m1.1.1.2.cmml">ℰ</mi><mrow id="S3.SS2.p7.1.m1.1.1.3" xref="S3.SS2.p7.1.m1.1.1.3.cmml"><mi id="S3.SS2.p7.1.m1.1.1.3.2" xref="S3.SS2.p7.1.m1.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p7.1.m1.1.1.3.1" xref="S3.SS2.p7.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.3.3" xref="S3.SS2.p7.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p7.1.m1.1.1.3.1a" xref="S3.SS2.p7.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.1.m1.1.1.3.4" xref="S3.SS2.p7.1.m1.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><apply id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p7.1.m1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.1.1.2">ℰ</ci><apply id="S3.SS2.p7.1.m1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.1.1.3"><times id="S3.SS2.p7.1.m1.1.1.3.1.cmml" xref="S3.SS2.p7.1.m1.1.1.3.1"></times><ci id="S3.SS2.p7.1.m1.1.1.3.2.cmml" xref="S3.SS2.p7.1.m1.1.1.3.2">𝑔</ci><ci id="S3.SS2.p7.1.m1.1.1.3.3.cmml" xref="S3.SS2.p7.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.p7.1.m1.1.1.3.4.cmml" xref="S3.SS2.p7.1.m1.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\mathcal{E}_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math> by comparing it with the originally sampled set <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p7.2.m2.1"><semantics id="S3.SS2.p7.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.1b"><ci id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.2.m2.1d">caligraphic_S</annotation></semantics></math>. If <math alttext="\mathcal{E}_{gen}\neq\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p7.3.m3.1"><semantics id="S3.SS2.p7.3.m3.1a"><mrow id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml"><msub id="S3.SS2.p7.3.m3.1.1.2" xref="S3.SS2.p7.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.3.m3.1.1.2.2" xref="S3.SS2.p7.3.m3.1.1.2.2.cmml">ℰ</mi><mrow id="S3.SS2.p7.3.m3.1.1.2.3" xref="S3.SS2.p7.3.m3.1.1.2.3.cmml"><mi id="S3.SS2.p7.3.m3.1.1.2.3.2" xref="S3.SS2.p7.3.m3.1.1.2.3.2.cmml">g</mi><mo id="S3.SS2.p7.3.m3.1.1.2.3.1" xref="S3.SS2.p7.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.3.m3.1.1.2.3.3" xref="S3.SS2.p7.3.m3.1.1.2.3.3.cmml">e</mi><mo id="S3.SS2.p7.3.m3.1.1.2.3.1a" xref="S3.SS2.p7.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.3.m3.1.1.2.3.4" xref="S3.SS2.p7.3.m3.1.1.2.3.4.cmml">n</mi></mrow></msub><mo id="S3.SS2.p7.3.m3.1.1.1" xref="S3.SS2.p7.3.m3.1.1.1.cmml">≠</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.3.m3.1.1.3" xref="S3.SS2.p7.3.m3.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.1b"><apply id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1"><neq id="S3.SS2.p7.3.m3.1.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1.1"></neq><apply id="S3.SS2.p7.3.m3.1.1.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.1.1.2.1.cmml" xref="S3.SS2.p7.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p7.3.m3.1.1.2.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2.2">ℰ</ci><apply id="S3.SS2.p7.3.m3.1.1.2.3.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3"><times id="S3.SS2.p7.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3.1"></times><ci id="S3.SS2.p7.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3.2">𝑔</ci><ci id="S3.SS2.p7.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3.3">𝑒</ci><ci id="S3.SS2.p7.3.m3.1.1.2.3.4.cmml" xref="S3.SS2.p7.3.m3.1.1.2.3.4">𝑛</ci></apply></apply><ci id="S3.SS2.p7.3.m3.1.1.3.cmml" xref="S3.SS2.p7.3.m3.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.1c">\mathcal{E}_{gen}\neq\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.3.m3.1d">caligraphic_E start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT ≠ caligraphic_S</annotation></semantics></math>, we regenerate the report <math alttext="R_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p7.4.m4.1"><semantics id="S3.SS2.p7.4.m4.1a"><msub id="S3.SS2.p7.4.m4.1.1" xref="S3.SS2.p7.4.m4.1.1.cmml"><mi id="S3.SS2.p7.4.m4.1.1.2" xref="S3.SS2.p7.4.m4.1.1.2.cmml">R</mi><mrow id="S3.SS2.p7.4.m4.1.1.3" xref="S3.SS2.p7.4.m4.1.1.3.cmml"><mi id="S3.SS2.p7.4.m4.1.1.3.2" xref="S3.SS2.p7.4.m4.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p7.4.m4.1.1.3.1" xref="S3.SS2.p7.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.4.m4.1.1.3.3" xref="S3.SS2.p7.4.m4.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p7.4.m4.1.1.3.1a" xref="S3.SS2.p7.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.4.m4.1.1.3.4" xref="S3.SS2.p7.4.m4.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m4.1b"><apply id="S3.SS2.p7.4.m4.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.4.m4.1.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p7.4.m4.1.1.2.cmml" xref="S3.SS2.p7.4.m4.1.1.2">𝑅</ci><apply id="S3.SS2.p7.4.m4.1.1.3.cmml" xref="S3.SS2.p7.4.m4.1.1.3"><times id="S3.SS2.p7.4.m4.1.1.3.1.cmml" xref="S3.SS2.p7.4.m4.1.1.3.1"></times><ci id="S3.SS2.p7.4.m4.1.1.3.2.cmml" xref="S3.SS2.p7.4.m4.1.1.3.2">𝑔</ci><ci id="S3.SS2.p7.4.m4.1.1.3.3.cmml" xref="S3.SS2.p7.4.m4.1.1.3.3">𝑒</ci><ci id="S3.SS2.p7.4.m4.1.1.3.4.cmml" xref="S3.SS2.p7.4.m4.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m4.1c">R_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.4.m4.1d">italic_R start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math> by repeating the generation process until <math alttext="\mathcal{E}_{gen}=\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p7.5.m5.1"><semantics id="S3.SS2.p7.5.m5.1a"><mrow id="S3.SS2.p7.5.m5.1.1" xref="S3.SS2.p7.5.m5.1.1.cmml"><msub id="S3.SS2.p7.5.m5.1.1.2" xref="S3.SS2.p7.5.m5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.5.m5.1.1.2.2" xref="S3.SS2.p7.5.m5.1.1.2.2.cmml">ℰ</mi><mrow id="S3.SS2.p7.5.m5.1.1.2.3" xref="S3.SS2.p7.5.m5.1.1.2.3.cmml"><mi id="S3.SS2.p7.5.m5.1.1.2.3.2" xref="S3.SS2.p7.5.m5.1.1.2.3.2.cmml">g</mi><mo id="S3.SS2.p7.5.m5.1.1.2.3.1" xref="S3.SS2.p7.5.m5.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.5.m5.1.1.2.3.3" xref="S3.SS2.p7.5.m5.1.1.2.3.3.cmml">e</mi><mo id="S3.SS2.p7.5.m5.1.1.2.3.1a" xref="S3.SS2.p7.5.m5.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p7.5.m5.1.1.2.3.4" xref="S3.SS2.p7.5.m5.1.1.2.3.4.cmml">n</mi></mrow></msub><mo id="S3.SS2.p7.5.m5.1.1.1" xref="S3.SS2.p7.5.m5.1.1.1.cmml">=</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.5.m5.1.1.3" xref="S3.SS2.p7.5.m5.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m5.1b"><apply id="S3.SS2.p7.5.m5.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1"><eq id="S3.SS2.p7.5.m5.1.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1.1"></eq><apply id="S3.SS2.p7.5.m5.1.1.2.cmml" xref="S3.SS2.p7.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p7.5.m5.1.1.2.1.cmml" xref="S3.SS2.p7.5.m5.1.1.2">subscript</csymbol><ci id="S3.SS2.p7.5.m5.1.1.2.2.cmml" xref="S3.SS2.p7.5.m5.1.1.2.2">ℰ</ci><apply id="S3.SS2.p7.5.m5.1.1.2.3.cmml" xref="S3.SS2.p7.5.m5.1.1.2.3"><times id="S3.SS2.p7.5.m5.1.1.2.3.1.cmml" xref="S3.SS2.p7.5.m5.1.1.2.3.1"></times><ci id="S3.SS2.p7.5.m5.1.1.2.3.2.cmml" xref="S3.SS2.p7.5.m5.1.1.2.3.2">𝑔</ci><ci id="S3.SS2.p7.5.m5.1.1.2.3.3.cmml" xref="S3.SS2.p7.5.m5.1.1.2.3.3">𝑒</ci><ci id="S3.SS2.p7.5.m5.1.1.2.3.4.cmml" xref="S3.SS2.p7.5.m5.1.1.2.3.4">𝑛</ci></apply></apply><ci id="S3.SS2.p7.5.m5.1.1.3.cmml" xref="S3.SS2.p7.5.m5.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m5.1c">\mathcal{E}_{gen}=\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.5.m5.1d">caligraphic_E start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT = caligraphic_S</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{if }\mathcal{E}_{gen}\neq\mathcal{S},\text{ regenerate }R_{gen}\text{ %
until }\mathcal{E}_{gen}=\mathcal{S}." class="ltx_Math" display="block" id="S3.Ex5.m1.1"><semantics id="S3.Ex5.m1.1a"><mrow id="S3.Ex5.m1.1.1.1"><mrow id="S3.Ex5.m1.1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.1.3.cmml"><mrow id="S3.Ex5.m1.1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex5.m1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.1.1.1.2.cmml"><mtext id="S3.Ex5.m1.1.1.1.1.1.1.2.2" xref="S3.Ex5.m1.1.1.1.1.1.1.2.2a.cmml">if </mtext><mo id="S3.Ex5.m1.1.1.1.1.1.1.2.1" xref="S3.Ex5.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S3.Ex5.m1.1.1.1.1.1.1.2.3" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.1.1.1.1.1.1.2.3.2" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.2.cmml">ℰ</mi><mrow id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.2" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.2.cmml">g</mi><mo id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.1" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.3" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.3.cmml">e</mi><mo id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.1a" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.4" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.4.cmml">n</mi></mrow></msub></mrow><mo id="S3.Ex5.m1.1.1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.1.cmml">≠</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.1.1.1.3.cmml">𝒮</mi></mrow><mo id="S3.Ex5.m1.1.1.1.1.2.3" xref="S3.Ex5.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.Ex5.m1.1.1.1.1.2.2" xref="S3.Ex5.m1.1.1.1.1.2.2.cmml"><mrow id="S3.Ex5.m1.1.1.1.1.2.2.2" xref="S3.Ex5.m1.1.1.1.1.2.2.2.cmml"><mtext id="S3.Ex5.m1.1.1.1.1.2.2.2.2" xref="S3.Ex5.m1.1.1.1.1.2.2.2.2a.cmml"> regenerate </mtext><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.1" xref="S3.Ex5.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><msub id="S3.Ex5.m1.1.1.1.1.2.2.2.3" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.3.2" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.2.cmml">R</mi><mrow id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.2" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.2.cmml">g</mi><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.1" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.3" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.3.cmml">e</mi><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.1a" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.4" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.4.cmml">n</mi></mrow></msub><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.1a" xref="S3.Ex5.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mtext id="S3.Ex5.m1.1.1.1.1.2.2.2.4" xref="S3.Ex5.m1.1.1.1.1.2.2.2.4a.cmml"> until </mtext><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.1b" xref="S3.Ex5.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><msub id="S3.Ex5.m1.1.1.1.1.2.2.2.5" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.1.1.1.1.2.2.2.5.2" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.2.cmml">ℰ</mi><mrow id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.cmml"><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.2" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.2.cmml">g</mi><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.1" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.3" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.3.cmml">e</mi><mo id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.1a" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.4" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.4.cmml">n</mi></mrow></msub></mrow><mo id="S3.Ex5.m1.1.1.1.1.2.2.1" xref="S3.Ex5.m1.1.1.1.1.2.2.1.cmml">=</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.1.1.1.1.2.2.3" xref="S3.Ex5.m1.1.1.1.1.2.2.3.cmml">𝒮</mi></mrow></mrow><mo id="S3.Ex5.m1.1.1.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.1b"><apply id="S3.Ex5.m1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.1.3a.cmml" xref="S3.Ex5.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex5.m1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1"><neq id="S3.Ex5.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.1"></neq><apply id="S3.Ex5.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2"><times id="S3.Ex5.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.1"></times><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.2a.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.2"><mtext id="S3.Ex5.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.2">if </mtext></ci><apply id="S3.Ex5.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.2">ℰ</ci><apply id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3"><times id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.2">𝑔</ci><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.3">𝑒</ci><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2.3.3.4">𝑛</ci></apply></apply></apply><ci id="S3.Ex5.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.3">𝒮</ci></apply><apply id="S3.Ex5.m1.1.1.1.1.2.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2"><eq id="S3.Ex5.m1.1.1.1.1.2.2.1.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.1"></eq><apply id="S3.Ex5.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2"><times id="S3.Ex5.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.1"></times><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.2a.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.2"><mtext id="S3.Ex5.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.2"> regenerate </mtext></ci><apply id="S3.Ex5.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.2">𝑅</ci><apply id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3"><times id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.1"></times><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.2">𝑔</ci><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.3">𝑒</ci><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.4.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.3.3.4">𝑛</ci></apply></apply><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.4a.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.4"><mtext id="S3.Ex5.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.4"> until </mtext></ci><apply id="S3.Ex5.m1.1.1.1.1.2.2.2.5.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.1.2.2.2.5.1.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5">subscript</csymbol><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.5.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.2">ℰ</ci><apply id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3"><times id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.1.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.1"></times><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.2.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.2">𝑔</ci><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.3">𝑒</ci><ci id="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.4.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.2.5.3.4">𝑛</ci></apply></apply></apply><ci id="S3.Ex5.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex5.m1.1.1.1.1.2.2.3">𝒮</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.1c">\text{if }\mathcal{E}_{gen}\neq\mathcal{S},\text{ regenerate }R_{gen}\text{ %
until }\mathcal{E}_{gen}=\mathcal{S}.</annotation><annotation encoding="application/x-llamapun" id="S3.Ex5.m1.1d">if caligraphic_E start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT ≠ caligraphic_S , regenerate italic_R start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT until caligraphic_E start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT = caligraphic_S .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.4">Once the synthetic report is successfully generated, it is used as the ‘FINDINGS’ section of the CXR report. We then query the LLM to summarize <math alttext="R_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p8.1.m1.1"><semantics id="S3.SS2.p8.1.m1.1a"><msub id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml">R</mi><mrow id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml"><mi id="S3.SS2.p8.1.m1.1.1.3.2" xref="S3.SS2.p8.1.m1.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p8.1.m1.1.1.3.1" xref="S3.SS2.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.1.m1.1.1.3.3" xref="S3.SS2.p8.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p8.1.m1.1.1.3.1a" xref="S3.SS2.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.1.m1.1.1.3.4" xref="S3.SS2.p8.1.m1.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p8.1.m1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.2">𝑅</ci><apply id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3"><times id="S3.SS2.p8.1.m1.1.1.3.1.cmml" xref="S3.SS2.p8.1.m1.1.1.3.1"></times><ci id="S3.SS2.p8.1.m1.1.1.3.2.cmml" xref="S3.SS2.p8.1.m1.1.1.3.2">𝑔</ci><ci id="S3.SS2.p8.1.m1.1.1.3.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS2.p8.1.m1.1.1.3.4.cmml" xref="S3.SS2.p8.1.m1.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">R_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.1.m1.1d">italic_R start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math> into the ‘IMPRESSION’ section, denoted as <math alttext="R_{imp}" class="ltx_Math" display="inline" id="S3.SS2.p8.2.m2.1"><semantics id="S3.SS2.p8.2.m2.1a"><msub id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml"><mi id="S3.SS2.p8.2.m2.1.1.2" xref="S3.SS2.p8.2.m2.1.1.2.cmml">R</mi><mrow id="S3.SS2.p8.2.m2.1.1.3" xref="S3.SS2.p8.2.m2.1.1.3.cmml"><mi id="S3.SS2.p8.2.m2.1.1.3.2" xref="S3.SS2.p8.2.m2.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p8.2.m2.1.1.3.1" xref="S3.SS2.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.2.m2.1.1.3.3" xref="S3.SS2.p8.2.m2.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p8.2.m2.1.1.3.1a" xref="S3.SS2.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.2.m2.1.1.3.4" xref="S3.SS2.p8.2.m2.1.1.3.4.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><apply id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.2.m2.1.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p8.2.m2.1.1.2.cmml" xref="S3.SS2.p8.2.m2.1.1.2">𝑅</ci><apply id="S3.SS2.p8.2.m2.1.1.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3"><times id="S3.SS2.p8.2.m2.1.1.3.1.cmml" xref="S3.SS2.p8.2.m2.1.1.3.1"></times><ci id="S3.SS2.p8.2.m2.1.1.3.2.cmml" xref="S3.SS2.p8.2.m2.1.1.3.2">𝑖</ci><ci id="S3.SS2.p8.2.m2.1.1.3.3.cmml" xref="S3.SS2.p8.2.m2.1.1.3.3">𝑚</ci><ci id="S3.SS2.p8.2.m2.1.1.3.4.cmml" xref="S3.SS2.p8.2.m2.1.1.3.4">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">R_{imp}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.2.m2.1d">italic_R start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. To ensure consistency between the entities in the ‘FINDINGS’ and ‘IMPRESSION’ sections, we extract entities from the summary <math alttext="R_{imp}" class="ltx_Math" display="inline" id="S3.SS2.p8.3.m3.1"><semantics id="S3.SS2.p8.3.m3.1a"><msub id="S3.SS2.p8.3.m3.1.1" xref="S3.SS2.p8.3.m3.1.1.cmml"><mi id="S3.SS2.p8.3.m3.1.1.2" xref="S3.SS2.p8.3.m3.1.1.2.cmml">R</mi><mrow id="S3.SS2.p8.3.m3.1.1.3" xref="S3.SS2.p8.3.m3.1.1.3.cmml"><mi id="S3.SS2.p8.3.m3.1.1.3.2" xref="S3.SS2.p8.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p8.3.m3.1.1.3.1" xref="S3.SS2.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.3.m3.1.1.3.3" xref="S3.SS2.p8.3.m3.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p8.3.m3.1.1.3.1a" xref="S3.SS2.p8.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.3.m3.1.1.3.4" xref="S3.SS2.p8.3.m3.1.1.3.4.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.3.m3.1b"><apply id="S3.SS2.p8.3.m3.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.3.m3.1.1.1.cmml" xref="S3.SS2.p8.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p8.3.m3.1.1.2.cmml" xref="S3.SS2.p8.3.m3.1.1.2">𝑅</ci><apply id="S3.SS2.p8.3.m3.1.1.3.cmml" xref="S3.SS2.p8.3.m3.1.1.3"><times id="S3.SS2.p8.3.m3.1.1.3.1.cmml" xref="S3.SS2.p8.3.m3.1.1.3.1"></times><ci id="S3.SS2.p8.3.m3.1.1.3.2.cmml" xref="S3.SS2.p8.3.m3.1.1.3.2">𝑖</ci><ci id="S3.SS2.p8.3.m3.1.1.3.3.cmml" xref="S3.SS2.p8.3.m3.1.1.3.3">𝑚</ci><ci id="S3.SS2.p8.3.m3.1.1.3.4.cmml" xref="S3.SS2.p8.3.m3.1.1.3.4">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.3.m3.1c">R_{imp}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.3.m3.1d">italic_R start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT</annotation></semantics></math> using RaTE, denoted as <math alttext="\mathcal{E}_{imp}" class="ltx_Math" display="inline" id="S3.SS2.p8.4.m4.1"><semantics id="S3.SS2.p8.4.m4.1a"><msub id="S3.SS2.p8.4.m4.1.1" xref="S3.SS2.p8.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.4.m4.1.1.2" xref="S3.SS2.p8.4.m4.1.1.2.cmml">ℰ</mi><mrow id="S3.SS2.p8.4.m4.1.1.3" xref="S3.SS2.p8.4.m4.1.1.3.cmml"><mi id="S3.SS2.p8.4.m4.1.1.3.2" xref="S3.SS2.p8.4.m4.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p8.4.m4.1.1.3.1" xref="S3.SS2.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.4.m4.1.1.3.3" xref="S3.SS2.p8.4.m4.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p8.4.m4.1.1.3.1a" xref="S3.SS2.p8.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.4.m4.1.1.3.4" xref="S3.SS2.p8.4.m4.1.1.3.4.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.4.m4.1b"><apply id="S3.SS2.p8.4.m4.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.4.m4.1.1.1.cmml" xref="S3.SS2.p8.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p8.4.m4.1.1.2.cmml" xref="S3.SS2.p8.4.m4.1.1.2">ℰ</ci><apply id="S3.SS2.p8.4.m4.1.1.3.cmml" xref="S3.SS2.p8.4.m4.1.1.3"><times id="S3.SS2.p8.4.m4.1.1.3.1.cmml" xref="S3.SS2.p8.4.m4.1.1.3.1"></times><ci id="S3.SS2.p8.4.m4.1.1.3.2.cmml" xref="S3.SS2.p8.4.m4.1.1.3.2">𝑖</ci><ci id="S3.SS2.p8.4.m4.1.1.3.3.cmml" xref="S3.SS2.p8.4.m4.1.1.3.3">𝑚</ci><ci id="S3.SS2.p8.4.m4.1.1.3.4.cmml" xref="S3.SS2.p8.4.m4.1.1.3.4">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.4.m4.1c">\mathcal{E}_{imp}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.4.m4.1d">caligraphic_E start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. We verify that:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}_{imp}=\mathcal{S}." class="ltx_Math" display="block" id="S3.Ex6.m1.1"><semantics id="S3.Ex6.m1.1a"><mrow id="S3.Ex6.m1.1.1.1" xref="S3.Ex6.m1.1.1.1.1.cmml"><mrow id="S3.Ex6.m1.1.1.1.1" xref="S3.Ex6.m1.1.1.1.1.cmml"><msub id="S3.Ex6.m1.1.1.1.1.2" xref="S3.Ex6.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex6.m1.1.1.1.1.2.2" xref="S3.Ex6.m1.1.1.1.1.2.2.cmml">ℰ</mi><mrow id="S3.Ex6.m1.1.1.1.1.2.3" xref="S3.Ex6.m1.1.1.1.1.2.3.cmml"><mi id="S3.Ex6.m1.1.1.1.1.2.3.2" xref="S3.Ex6.m1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.Ex6.m1.1.1.1.1.2.3.1" xref="S3.Ex6.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.Ex6.m1.1.1.1.1.2.3.3" xref="S3.Ex6.m1.1.1.1.1.2.3.3.cmml">m</mi><mo id="S3.Ex6.m1.1.1.1.1.2.3.1a" xref="S3.Ex6.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.Ex6.m1.1.1.1.1.2.3.4" xref="S3.Ex6.m1.1.1.1.1.2.3.4.cmml">p</mi></mrow></msub><mo id="S3.Ex6.m1.1.1.1.1.1" xref="S3.Ex6.m1.1.1.1.1.1.cmml">=</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex6.m1.1.1.1.1.3" xref="S3.Ex6.m1.1.1.1.1.3.cmml">𝒮</mi></mrow><mo id="S3.Ex6.m1.1.1.1.2" lspace="0em" xref="S3.Ex6.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex6.m1.1b"><apply id="S3.Ex6.m1.1.1.1.1.cmml" xref="S3.Ex6.m1.1.1.1"><eq id="S3.Ex6.m1.1.1.1.1.1.cmml" xref="S3.Ex6.m1.1.1.1.1.1"></eq><apply id="S3.Ex6.m1.1.1.1.1.2.cmml" xref="S3.Ex6.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex6.m1.1.1.1.1.2.1.cmml" xref="S3.Ex6.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex6.m1.1.1.1.1.2.2.cmml" xref="S3.Ex6.m1.1.1.1.1.2.2">ℰ</ci><apply id="S3.Ex6.m1.1.1.1.1.2.3.cmml" xref="S3.Ex6.m1.1.1.1.1.2.3"><times id="S3.Ex6.m1.1.1.1.1.2.3.1.cmml" xref="S3.Ex6.m1.1.1.1.1.2.3.1"></times><ci id="S3.Ex6.m1.1.1.1.1.2.3.2.cmml" xref="S3.Ex6.m1.1.1.1.1.2.3.2">𝑖</ci><ci id="S3.Ex6.m1.1.1.1.1.2.3.3.cmml" xref="S3.Ex6.m1.1.1.1.1.2.3.3">𝑚</ci><ci id="S3.Ex6.m1.1.1.1.1.2.3.4.cmml" xref="S3.Ex6.m1.1.1.1.1.2.3.4">𝑝</ci></apply></apply><ci id="S3.Ex6.m1.1.1.1.1.3.cmml" xref="S3.Ex6.m1.1.1.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex6.m1.1c">\mathcal{E}_{imp}=\mathcal{S}.</annotation><annotation encoding="application/x-llamapun" id="S3.Ex6.m1.1d">caligraphic_E start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT = caligraphic_S .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p8.7">If the entities in <math alttext="R_{imp}" class="ltx_Math" display="inline" id="S3.SS2.p8.5.m1.1"><semantics id="S3.SS2.p8.5.m1.1a"><msub id="S3.SS2.p8.5.m1.1.1" xref="S3.SS2.p8.5.m1.1.1.cmml"><mi id="S3.SS2.p8.5.m1.1.1.2" xref="S3.SS2.p8.5.m1.1.1.2.cmml">R</mi><mrow id="S3.SS2.p8.5.m1.1.1.3" xref="S3.SS2.p8.5.m1.1.1.3.cmml"><mi id="S3.SS2.p8.5.m1.1.1.3.2" xref="S3.SS2.p8.5.m1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p8.5.m1.1.1.3.1" xref="S3.SS2.p8.5.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.5.m1.1.1.3.3" xref="S3.SS2.p8.5.m1.1.1.3.3.cmml">m</mi><mo id="S3.SS2.p8.5.m1.1.1.3.1a" xref="S3.SS2.p8.5.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.5.m1.1.1.3.4" xref="S3.SS2.p8.5.m1.1.1.3.4.cmml">p</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.5.m1.1b"><apply id="S3.SS2.p8.5.m1.1.1.cmml" xref="S3.SS2.p8.5.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p8.5.m1.1.1.1.cmml" xref="S3.SS2.p8.5.m1.1.1">subscript</csymbol><ci id="S3.SS2.p8.5.m1.1.1.2.cmml" xref="S3.SS2.p8.5.m1.1.1.2">𝑅</ci><apply id="S3.SS2.p8.5.m1.1.1.3.cmml" xref="S3.SS2.p8.5.m1.1.1.3"><times id="S3.SS2.p8.5.m1.1.1.3.1.cmml" xref="S3.SS2.p8.5.m1.1.1.3.1"></times><ci id="S3.SS2.p8.5.m1.1.1.3.2.cmml" xref="S3.SS2.p8.5.m1.1.1.3.2">𝑖</ci><ci id="S3.SS2.p8.5.m1.1.1.3.3.cmml" xref="S3.SS2.p8.5.m1.1.1.3.3">𝑚</ci><ci id="S3.SS2.p8.5.m1.1.1.3.4.cmml" xref="S3.SS2.p8.5.m1.1.1.3.4">𝑝</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.5.m1.1c">R_{imp}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.5.m1.1d">italic_R start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT</annotation></semantics></math> do not match <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p8.6.m2.1"><semantics id="S3.SS2.p8.6.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.6.m2.1.1" xref="S3.SS2.p8.6.m2.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.6.m2.1b"><ci id="S3.SS2.p8.6.m2.1.1.cmml" xref="S3.SS2.p8.6.m2.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.6.m2.1c">\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.6.m2.1d">caligraphic_S</annotation></semantics></math>, we regenerate the "IMPRESSION" section until <math alttext="\mathcal{E}_{imp}=\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p8.7.m3.1"><semantics id="S3.SS2.p8.7.m3.1a"><mrow id="S3.SS2.p8.7.m3.1.1" xref="S3.SS2.p8.7.m3.1.1.cmml"><msub id="S3.SS2.p8.7.m3.1.1.2" xref="S3.SS2.p8.7.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.7.m3.1.1.2.2" xref="S3.SS2.p8.7.m3.1.1.2.2.cmml">ℰ</mi><mrow id="S3.SS2.p8.7.m3.1.1.2.3" xref="S3.SS2.p8.7.m3.1.1.2.3.cmml"><mi id="S3.SS2.p8.7.m3.1.1.2.3.2" xref="S3.SS2.p8.7.m3.1.1.2.3.2.cmml">i</mi><mo id="S3.SS2.p8.7.m3.1.1.2.3.1" xref="S3.SS2.p8.7.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.7.m3.1.1.2.3.3" xref="S3.SS2.p8.7.m3.1.1.2.3.3.cmml">m</mi><mo id="S3.SS2.p8.7.m3.1.1.2.3.1a" xref="S3.SS2.p8.7.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS2.p8.7.m3.1.1.2.3.4" xref="S3.SS2.p8.7.m3.1.1.2.3.4.cmml">p</mi></mrow></msub><mo id="S3.SS2.p8.7.m3.1.1.1" xref="S3.SS2.p8.7.m3.1.1.1.cmml">=</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p8.7.m3.1.1.3" xref="S3.SS2.p8.7.m3.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.7.m3.1b"><apply id="S3.SS2.p8.7.m3.1.1.cmml" xref="S3.SS2.p8.7.m3.1.1"><eq id="S3.SS2.p8.7.m3.1.1.1.cmml" xref="S3.SS2.p8.7.m3.1.1.1"></eq><apply id="S3.SS2.p8.7.m3.1.1.2.cmml" xref="S3.SS2.p8.7.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p8.7.m3.1.1.2.1.cmml" xref="S3.SS2.p8.7.m3.1.1.2">subscript</csymbol><ci id="S3.SS2.p8.7.m3.1.1.2.2.cmml" xref="S3.SS2.p8.7.m3.1.1.2.2">ℰ</ci><apply id="S3.SS2.p8.7.m3.1.1.2.3.cmml" xref="S3.SS2.p8.7.m3.1.1.2.3"><times id="S3.SS2.p8.7.m3.1.1.2.3.1.cmml" xref="S3.SS2.p8.7.m3.1.1.2.3.1"></times><ci id="S3.SS2.p8.7.m3.1.1.2.3.2.cmml" xref="S3.SS2.p8.7.m3.1.1.2.3.2">𝑖</ci><ci id="S3.SS2.p8.7.m3.1.1.2.3.3.cmml" xref="S3.SS2.p8.7.m3.1.1.2.3.3">𝑚</ci><ci id="S3.SS2.p8.7.m3.1.1.2.3.4.cmml" xref="S3.SS2.p8.7.m3.1.1.2.3.4">𝑝</ci></apply></apply><ci id="S3.SS2.p8.7.m3.1.1.3.cmml" xref="S3.SS2.p8.7.m3.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.7.m3.1c">\mathcal{E}_{imp}=\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p8.7.m3.1d">caligraphic_E start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT = caligraphic_S</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{if }\mathcal{E}_{imp}\neq\mathcal{S},\text{ regenerate }R_{imp}\text{ %
until }\mathcal{E}_{imp}=\mathcal{S}." class="ltx_Math" display="block" id="S3.Ex7.m1.1"><semantics id="S3.Ex7.m1.1a"><mrow id="S3.Ex7.m1.1.1.1"><mrow id="S3.Ex7.m1.1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.3.cmml"><mrow id="S3.Ex7.m1.1.1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex7.m1.1.1.1.1.1.1.2" xref="S3.Ex7.m1.1.1.1.1.1.1.2.cmml"><mtext id="S3.Ex7.m1.1.1.1.1.1.1.2.2" xref="S3.Ex7.m1.1.1.1.1.1.1.2.2a.cmml">if </mtext><mo id="S3.Ex7.m1.1.1.1.1.1.1.2.1" xref="S3.Ex7.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S3.Ex7.m1.1.1.1.1.1.1.2.3" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex7.m1.1.1.1.1.1.1.2.3.2" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.2.cmml">ℰ</mi><mrow id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.cmml"><mi id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.2" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.2.cmml">i</mi><mo id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.1" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.3" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.3.cmml">m</mi><mo id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.1a" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.4" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.4.cmml">p</mi></mrow></msub></mrow><mo id="S3.Ex7.m1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.1.1.1.1.1.1.1.cmml">≠</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex7.m1.1.1.1.1.1.1.3" xref="S3.Ex7.m1.1.1.1.1.1.1.3.cmml">𝒮</mi></mrow><mo id="S3.Ex7.m1.1.1.1.1.2.3" xref="S3.Ex7.m1.1.1.1.1.3a.cmml">,</mo><mrow id="S3.Ex7.m1.1.1.1.1.2.2" xref="S3.Ex7.m1.1.1.1.1.2.2.cmml"><mrow id="S3.Ex7.m1.1.1.1.1.2.2.2" xref="S3.Ex7.m1.1.1.1.1.2.2.2.cmml"><mtext id="S3.Ex7.m1.1.1.1.1.2.2.2.2" xref="S3.Ex7.m1.1.1.1.1.2.2.2.2a.cmml"> regenerate </mtext><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.1" xref="S3.Ex7.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><msub id="S3.Ex7.m1.1.1.1.1.2.2.2.3" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.3.2" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.2.cmml">R</mi><mrow id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.2" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.2.cmml">i</mi><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.1" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.3" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.3.cmml">m</mi><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.1a" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.1.cmml">⁢</mo><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.4" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.4.cmml">p</mi></mrow></msub><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.1a" xref="S3.Ex7.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><mtext id="S3.Ex7.m1.1.1.1.1.2.2.2.4" xref="S3.Ex7.m1.1.1.1.1.2.2.2.4a.cmml"> until </mtext><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.1b" xref="S3.Ex7.m1.1.1.1.1.2.2.2.1.cmml">⁢</mo><msub id="S3.Ex7.m1.1.1.1.1.2.2.2.5" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex7.m1.1.1.1.1.2.2.2.5.2" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.2.cmml">ℰ</mi><mrow id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.cmml"><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.2" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.2.cmml">i</mi><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.1" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.3" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.3.cmml">m</mi><mo id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.1a" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.1.cmml">⁢</mo><mi id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.4" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.4.cmml">p</mi></mrow></msub></mrow><mo id="S3.Ex7.m1.1.1.1.1.2.2.1" xref="S3.Ex7.m1.1.1.1.1.2.2.1.cmml">=</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex7.m1.1.1.1.1.2.2.3" xref="S3.Ex7.m1.1.1.1.1.2.2.3.cmml">𝒮</mi></mrow></mrow><mo id="S3.Ex7.m1.1.1.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex7.m1.1b"><apply id="S3.Ex7.m1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.3a.cmml" xref="S3.Ex7.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex7.m1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1"><neq id="S3.Ex7.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.1"></neq><apply id="S3.Ex7.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2"><times id="S3.Ex7.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.1"></times><ci id="S3.Ex7.m1.1.1.1.1.1.1.2.2a.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.2"><mtext id="S3.Ex7.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.2">if </mtext></ci><apply id="S3.Ex7.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.Ex7.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.2">ℰ</ci><apply id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3"><times id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.1"></times><ci id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.2">𝑖</ci><ci id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.3">𝑚</ci><ci id="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.4.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.2.3.3.4">𝑝</ci></apply></apply></apply><ci id="S3.Ex7.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.1.1.1.1.1.1.3">𝒮</ci></apply><apply id="S3.Ex7.m1.1.1.1.1.2.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2"><eq id="S3.Ex7.m1.1.1.1.1.2.2.1.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.1"></eq><apply id="S3.Ex7.m1.1.1.1.1.2.2.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2"><times id="S3.Ex7.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.1"></times><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.2a.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.2"><mtext id="S3.Ex7.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.2"> regenerate </mtext></ci><apply id="S3.Ex7.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3">subscript</csymbol><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.2">𝑅</ci><apply id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3"><times id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.1"></times><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.2">𝑖</ci><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.3">𝑚</ci><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.4.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.3.3.4">𝑝</ci></apply></apply><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.4a.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.4"><mtext id="S3.Ex7.m1.1.1.1.1.2.2.2.4.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.4"> until </mtext></ci><apply id="S3.Ex7.m1.1.1.1.1.2.2.2.5.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.1.2.2.2.5.1.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5">subscript</csymbol><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.5.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.2">ℰ</ci><apply id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3"><times id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.1.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.1"></times><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.2.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.2">𝑖</ci><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.3">𝑚</ci><ci id="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.4.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.2.5.3.4">𝑝</ci></apply></apply></apply><ci id="S3.Ex7.m1.1.1.1.1.2.2.3.cmml" xref="S3.Ex7.m1.1.1.1.1.2.2.3">𝒮</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex7.m1.1c">\text{if }\mathcal{E}_{imp}\neq\mathcal{S},\text{ regenerate }R_{imp}\text{ %
until }\mathcal{E}_{imp}=\mathcal{S}.</annotation><annotation encoding="application/x-llamapun" id="S3.Ex7.m1.1d">if caligraphic_E start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT ≠ caligraphic_S , regenerate italic_R start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT until caligraphic_E start_POSTSUBSCRIPT italic_i italic_m italic_p end_POSTSUBSCRIPT = caligraphic_S .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.2">Given that the number of samples in the original MIMIC-CXR dataset cannot be perfectly divided by <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p9.1.m1.1"><semantics id="S3.SS2.p9.1.m1.1a"><mi id="S3.SS2.p9.1.m1.1.1" xref="S3.SS2.p9.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.1.m1.1b"><ci id="S3.SS2.p9.1.m1.1.1.cmml" xref="S3.SS2.p9.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.1.m1.1d">italic_k</annotation></semantics></math> and <math alttext="m" class="ltx_Math" display="inline" id="S3.SS2.p9.2.m2.1"><semantics id="S3.SS2.p9.2.m2.1a"><mi id="S3.SS2.p9.2.m2.1.1" xref="S3.SS2.p9.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p9.2.m2.1b"><ci id="S3.SS2.p9.2.m2.1.1.cmml" xref="S3.SS2.p9.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p9.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p9.2.m2.1d">italic_m</annotation></semantics></math>, we generate a total of 200,000 synthetic samples to ensure a balanced distribution using only off-the-shelf tools, without any specific design for CXR data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p10">
<p class="ltx_p" id="S3.SS2.p10.1">While RadGraph <cite class="ltx_cite ltx_citemacro_citep">(Delbrouck et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib12" title="">2024</a>)</cite> could be used for entity extraction, it relies on human-annotated data from MIMIC-CXR and is limited to 16,117 entities. In contrast, RaTE <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib74" title="">2024</a>)</cite> extracts 154,049 entities, making it more suitable for our goal of creating a general and easily transferable pipeline for synthetic data generation. Thus, we chose RaTE for its broader applicability to various radiology reports.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p11">
<p class="ltx_p" id="S3.SS2.p11.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p11.1.1">CXR Image Generation.</span>
After generating the synthetic radiology reports, we aim to generate paired CXR images conditioned on the synthetic reports. Since general text-to-image (T2I) models (e.g., Stable Diffusion) are not designed for CXR image generation and demonstrate poor performance, as shown in <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib34" title="">2023e</a>; Bluethgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib4" title="">2024</a>)</cite>, we select RoentGen<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://stanfordmimi.github.io/RoentGen/</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Bluethgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib4" title="">2024</a>)</cite>, the most recent and validated CXR-specific T2I model, verified by clinicians, as our image generator.
We use RoentGen’s <cite class="ltx_cite ltx_citemacro_citep">(Bluethgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib4" title="">2024</a>)</cite> official pretrained weights to generate images. Following their implementation, we use only the ‘IMPRESSION’ section from the synthetic reports as the text prompt for the T2I model. The generation process is controlled using the official hyperparameters provided by RoentGen, where the classifier-free guidance (CFG) is set to 4 and the number of denoising steps is set to 50.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p12">
<p class="ltx_p" id="S3.SS2.p12.1">To prevent the synthetic images from exhibiting the same issues found in the real dataset (as discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S3.SS1" title="3.1 Exploring Imperfections in Real Data ‣ 3 Methods ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we apply a similar curation procedure. First, we use the MLLM to filter synthetic images, and then we compute the similarity of visual features between synthetic images and the problematic samples identified from the real dataset. If the visual similarity exceeds a threshold <math alttext="\delta=0.5" class="ltx_Math" display="inline" id="S3.SS2.p12.1.m1.1"><semantics id="S3.SS2.p12.1.m1.1a"><mrow id="S3.SS2.p12.1.m1.1.1" xref="S3.SS2.p12.1.m1.1.1.cmml"><mi id="S3.SS2.p12.1.m1.1.1.2" xref="S3.SS2.p12.1.m1.1.1.2.cmml">δ</mi><mo id="S3.SS2.p12.1.m1.1.1.1" xref="S3.SS2.p12.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS2.p12.1.m1.1.1.3" xref="S3.SS2.p12.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p12.1.m1.1b"><apply id="S3.SS2.p12.1.m1.1.1.cmml" xref="S3.SS2.p12.1.m1.1.1"><eq id="S3.SS2.p12.1.m1.1.1.1.cmml" xref="S3.SS2.p12.1.m1.1.1.1"></eq><ci id="S3.SS2.p12.1.m1.1.1.2.cmml" xref="S3.SS2.p12.1.m1.1.1.2">𝛿</ci><cn id="S3.SS2.p12.1.m1.1.1.3.cmml" type="float" xref="S3.SS2.p12.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p12.1.m1.1c">\delta=0.5</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p12.1.m1.1d">italic_δ = 0.5</annotation></semantics></math>, we regenerate the images by re-querying the T2I model with the same text prompt until they pass the curation procedure.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p13">
<p class="ltx_p" id="S3.SS2.p13.1">We generate 200,000 synthetic CXR images, each paired with a corresponding synthetic report, using only general-purpose, open-source models (e.g., Llama3.1 <cite class="ltx_cite ltx_citemacro_citep">(AI@Meta, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib1" title="">2024</a>)</cite>, InternVL2 <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib10" title="">2023</a>)</cite>) and vision models pre-trained with self-supervised learning (e.g., RAD-DINO <cite class="ltx_cite ltx_citemacro_citep">(Pérez-García et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib36" title="">2024</a>)</cite>). No annotated CXR images or MedVLP models pre-trained on specific CXR image-text datasets are used in this process. This ensures our approach is adaptable and can easily incorporate future advancements in general-purpose models. We refer to this dataset as <span class="ltx_text ltx_font_bold" id="S3.SS2.p13.1.1">SynCXR</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Data Training for MedVLP</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Finally, we use the synthetic dataset, SynCXR, to train a MedVLP model and explore how effectively a model can learn from pure synthetic data. Since there are many existing methods for MedVLP, we select simple baseline models like ConVIRT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>)</cite> and GLoRIA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite> for the following reasons:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">ConVIRT</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>)</cite> jointly trains vision and text encoders on paired medical images and reports using global contrastive learning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">GLoRIA</span> <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite> extends ConVIRT by incorporating both global and regional contrastive learning to train the encoders on paired medical images and reports.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">These models are open-source, straightforward, and minimize the influence of external factors on evaluating synthetic data for MedVLP. For retraining these two methods on our synthetic dataset, SynCXR, we strictly use their official codebases<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://github.com/marshuang80/gloria</span></span></span><span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>https://github.com/edreisMD/ConVIRT-pytorch</span></span></span>. More complex models may introduce unnecessary complications.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Excluding Complex Models.</span>
Recent models like BioViL <cite class="ltx_cite ltx_citemacro_citep">(Boecking et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib5" title="">2022</a>)</cite> and BioViL-T <cite class="ltx_cite ltx_citemacro_citep">(Bannur et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib3" title="">2023</a>)</cite> lack publicly available training code, making them impractical for re-training with synthetic data. Knowledge-enhanced MedVLP models such as MedKLIP, KAD, and MAVL <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib62" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib72" title="">2023</a>; Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite> rely on external tools and human-annotated data to incorporate additional knowledge, making direct implementation with synthetic data challenging and introducing unnecessary variables.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments Configurations</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">For pre-training, we apply the official configurations provided by ConVIRT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>)</cite> and GLoRIA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite> on the MIMIC-CXR dataset to our synthetic CXR image-text dataset, SynCXR.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Downstream Task Datasets and Configurations</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">For downstream tasks, we evaluate the effectiveness of synthetic data for MedVLP across four tasks. Details on the datasets and implementation are provided in Appendix, Sec <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A1" title="Appendix A Downstream Tasks Configuration ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Zero-shot Medical Image Classification.</span>
Following the guidelines in <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib62" title="">2023</a>)</cite>, we perform this task on seven datasets: CheXpert <cite class="ltx_cite ltx_citemacro_citep">(Saporta et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib46" title="">2022</a>)</cite>, ChestXray-14 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib61" title="">2017</a>)</cite>, PadChest-seen, PadChest-unseen, PadChest-rare <cite class="ltx_cite ltx_citemacro_citep">(Bustos et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib6" title="">2020</a>)</cite>, RSNA <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite>, and SIIM <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite>, using the dataset splits from <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>. Evaluation metrics include AUC, F1, and ACC.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Zero-shot Medical Image Visual Grounding.</span>
In line with <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>, this task is conducted on the RSNA <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite>, SIIM <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite>, and Covid-19 Rural <cite class="ltx_cite ltx_citemacro_citep">(Desai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib13" title="">2020</a>)</cite> datasets, using official splits and metrics. Grounding performance is evaluated with IoU, and Dice score.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Medical Image Fine-tuned Classification.</span>
As described in <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>, we use the RSNA <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite>, SIIM <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite>, Covid-19 CXR-2 <cite class="ltx_cite ltx_citemacro_citep">(Pavlova et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib35" title="">2022</a>)</cite>, and ChestXray-14 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib61" title="">2017</a>)</cite> datasets. During fine-tuning, all model parameters, including the pre-trained vision encoder and linear classifier, are updated. The AdamW optimizer is applied with a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S4.SS1.p4.1.m1.1"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mn id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS1.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p4.1.m1.1.1.1.cmml">×</mo><msup id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml"><mn id="S4.SS1.p4.1.m1.1.1.3.2" xref="S4.SS1.p4.1.m1.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p4.1.m1.1.1.3.3" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml"><mo id="S4.SS1.p4.1.m1.1.1.3.3a" xref="S4.SS1.p4.1.m1.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p4.1.m1.1.1.3.3.2" xref="S4.SS1.p4.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><times id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></times><cn id="S4.SS1.p4.1.m1.1.1.2.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.2">1</cn><apply id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p4.1.m1.1.1.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3">superscript</csymbol><cn id="S4.SS1.p4.1.m1.1.1.3.2.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.3.2">10</cn><apply id="S4.SS1.p4.1.m1.1.1.3.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"><minus id="S4.SS1.p4.1.m1.1.1.3.3.1.cmml" xref="S4.SS1.p4.1.m1.1.1.3.3"></minus><cn id="S4.SS1.p4.1.m1.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.p4.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p4.1.m1.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, batch size of 64, and training runs for 50 epochs. Evaluation follows the AUC score protocol in <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib59" title="">2022</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib75" title="">Zhou et al., </a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Medical Image Fine-tuned Segmentation.</span>
This task uses the RSNA <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite>, SIIM <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite>, and Covid-19 Rural <cite class="ltx_cite ltx_citemacro_citep">(Desai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib13" title="">2020</a>)</cite> datasets, following preprocessing from <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib59" title="">2022</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite>. U-Net <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib43" title="">2015</a>)</cite> is used for fine-tuning, freezing the pre-trained vision encoder and updating only the decoder parameters. Performance is measured using the Dice score, adhering to the evaluation protocol from <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Since the MIMIC-CXR dataset already includes several diseases present in downstream tasks, as mentioned in <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib72" title="">2023</a>)</cite>, we split the zero-shot classification task into <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">seen</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">unseen</span> categories, strictly following <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>. Note that all experimental results for ConVIRT and GLoRIA pre-trained with real data (MIMIC-CXR) are directly referenced from <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite> to ensure a fair comparison.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Zero-shot Classification on Seen Diseases.</span>
Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4.T1" title="Table 1 ‣ 4.2 Experimental Results ‣ 4 Experiments Configurations ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">1</span></a> shows the zero-shot classification performance on <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">seen</span> diseases. Across all datasets, both MedVLP methods pretrained on SynCXR (our <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.3">purely synthetic dataset</span>) consistently outperform or achieve comparable performance to their counterparts pretrained on real datasets, with an average improvement of 4.7% in AUC and 4.53% in F1 scores. Furthermore, the methods pretrained on the mixed dataset, which directly combines real and synthetic data, achieve even greater improvements, with 10.08% AUC and 7.62% F1 scores on average across all datasets and methods. This demonstrates that the SynCXR dataset effectively enables MedVLP models to learn representative cross-modal features, enhancing their zero-shot classification capability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Zero-shot Classification on Unseen Diseases.</span>
Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4.F3.sf1" title="In Table 2 ‣ 4.2 Experimental Results ‣ 4 Experiments Configurations ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">3(a)</span></a> reports the zero-shot classification performance on <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">unseen</span> diseases. Similar to the results for seen diseases, MedVLP models pretrained on the synthetic dataset consistently outperform those pretrained on real data, with an average improvement of 2.96% AUC and 0.51% F1 scores. Additionally, models pretrained on the mixed dataset show substantial gains over those trained on real data, with 7.39% AUC and 1.52% F1 scores on average. This indicates that the SynCXR dataset, generated with meticulous quality control and balanced distribution, can increase the generalizability of MedVLP models for unseen diseases prediction.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Zero-shot Visual Grounding.</span>
We further evaluate the effectiveness of synthetic data in improving MedVLP models’ local visual understanding capabilities through zero-shot grounding tasks. Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4.F3.sf2" title="In Table 2 ‣ 4.2 Experimental Results ‣ 4 Experiments Configurations ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">3(b)</span></a> presents the performance of zero-shot grounding on RSNA <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite>, Covid-19 Rural <cite class="ltx_cite ltx_citemacro_citep">(Desai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib13" title="">2020</a>)</cite>, and SIIM <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite>. Across all datasets, MedVLP models pretrained on the SynCXR dataset achieve superior performance compared to those trained on the real dataset, with an average increase of 1.42% IoU and 0.97% Dice scores. The mixed dataset further enhances performance, with 4.06% IoU and 2.92% Dice scores on average. This demonstrates that the SynCXR dataset not only benefits global cross-modal feature learning but also improves local visual understanding for MedVLP models.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.10" style="width:397.5pt;height:112pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-58.7pt,16.4pt) scale(0.772064297020014,0.772064297020014) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.10.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.10.10.11.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.10.10.11.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.10.10.11.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.2.1">Pre-training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.10.10.11.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.3.1">CheXpert</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.10.10.11.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.4.1">ChestXray-14</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.10.10.11.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.5.1">PadChest-seen</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.10.10.11.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.6.1">RSNA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.10.10.11.1.7"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.11.1.7.1">SIIM</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.10">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.10.10.10.11"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.10.10.10.12"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.10.12.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.1.1">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.2.2.2.2">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.3.3.3.3">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.4.4.4.4">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.4.m1.1"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo id="S4.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.5.5.5.5">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.5.5.5.5.m1.1"><semantics id="S4.T1.5.5.5.5.m1.1a"><mo id="S4.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S4.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.6.6.6.6">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.6.6.6.6.m1.1"><semantics id="S4.T1.6.6.6.6.m1.1a"><mo id="S4.T1.6.6.6.6.m1.1.1" stretchy="false" xref="S4.T1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.6.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.7.7.7.7">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.7.7.7.7.m1.1"><semantics id="S4.T1.7.7.7.7.m1.1a"><mo id="S4.T1.7.7.7.7.m1.1.1" stretchy="false" xref="S4.T1.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.7.m1.1b"><ci id="S4.T1.7.7.7.7.m1.1.1.cmml" xref="S4.T1.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.7.7.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.8.8.8.8">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.8.8.8.8.m1.1"><semantics id="S4.T1.8.8.8.8.m1.1a"><mo id="S4.T1.8.8.8.8.m1.1.1" stretchy="false" xref="S4.T1.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.m1.1b"><ci id="S4.T1.8.8.8.8.m1.1.1.cmml" xref="S4.T1.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.8.8.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.9.9.9.9">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.9.9.9.9.m1.1"><semantics id="S4.T1.9.9.9.9.m1.1a"><mo id="S4.T1.9.9.9.9.m1.1.1" stretchy="false" xref="S4.T1.9.9.9.9.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.9.9.m1.1b"><ci id="S4.T1.9.9.9.9.m1.1.1.cmml" xref="S4.T1.9.9.9.9.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.9.9.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.9.9.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.10.10.10.10">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.10.10.10.10.m1.1"><semantics id="S4.T1.10.10.10.10.m1.1a"><mo id="S4.T1.10.10.10.10.m1.1.1" stretchy="false" xref="S4.T1.10.10.10.10.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.10.10.m1.1b"><ci id="S4.T1.10.10.10.10.m1.1.1.cmml" xref="S4.T1.10.10.10.10.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.10.10.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.10.10.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.10.10.12.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.10.10.12.1.1" rowspan="3"><span class="ltx_text" id="S4.T1.10.10.12.1.1.1">ConVIRT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.10.10.12.1.2">MIMIC-CXR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.12.1.3">52.10</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.12.1.4">35.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.12.1.5">53.15</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.12.1.6">12.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.12.1.7">63.72</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.12.1.8">14.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.12.1.9">79.21</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.12.1.10">55.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.12.1.11">64.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.12.1.12">42.87</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.13.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.10.10.13.2.1">SynCXR</th>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.13.2.2">59.49</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.13.2.3">40.51</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.13.2.4">56.07</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.13.2.5">15.43</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.13.2.6">63.43</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.13.2.7">15.10</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.13.2.8">82.08</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.13.2.9">58.38</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.13.2.10">75.55</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.13.2.11">57.43</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.14.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.10.10.14.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.1.1">Mix</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.14.3.2"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.2.1">71.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.14.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.3.1">47.11</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.14.3.4"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.4.1">61.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.14.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.5.1">18.52</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.14.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.6.1">68.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.14.3.7"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.7.1">16.67</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.14.3.8"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.8.1">83.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.14.3.9"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.9.1">61.28</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.14.3.10"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.10.1">78.51</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.14.3.11"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.14.3.11.1">59.10</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.15.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T1.10.10.15.4.1" rowspan="3"><span class="ltx_text" id="S4.T1.10.10.15.4.1.1">GLoRIA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.10.10.15.4.2">MIMIC-CXR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.15.4.3">54.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.15.4.4">37.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.15.4.5">55.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.15.4.6">14.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.15.4.7">64.09</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.15.4.8">14.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.15.4.9">70.37</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.15.4.10">48.19</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.15.4.11">54.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.10.10.15.4.12">40.39</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.16.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.10.10.16.5.1">SynCXR</th>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.16.5.2">61.38</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.16.5.3">41.05</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.16.5.4">57.47</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.16.5.5">15.60</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.16.5.6">64.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.16.5.7">15.02</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.16.5.8">72.34</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.10.10.16.5.9">49.50</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.16.5.10">67.32</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.16.5.11">53.86</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.17.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.10.10.17.6.1"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.1.1">Mix</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.10.17.6.2"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.2.1">72.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.10.10.17.6.3"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.3.1">48.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.10.17.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.4.1">61.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.10.10.17.6.5"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.5.1">17.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.10.17.6.6"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.6.1">68.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.10.10.17.6.7"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.7.1">17.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.10.17.6.8"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.8.1">74.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.10.10.17.6.9"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.9.1">51.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.10.17.6.10"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.10.1">73.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.10.10.17.6.11"><span class="ltx_text ltx_font_bold" id="S4.T1.10.10.17.6.11.1">56.09</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance of zero-shot classification on five datasets for diseases present in the MIMIC-CXR dataset, evaluated on two MedVLP models pretrained on MIMIC-CXR (real) and SynCXR (<span class="ltx_text ltx_font_bold" id="S4.T1.12.1">pure synthetic</span>). ‘Mix’ denotes the direct combination of real and synthetic data for MedVLP pretraining. Best results are highlighted in bold.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf1">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F3.sf1.6" style="width:198.7pt;height:80pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.5pt,32.0pt) scale(0.555663333954919,0.555663333954919) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.F3.sf1.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.F3.sf1.6.6.7.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.F3.sf1.6.6.7.1.1"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.7.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.F3.sf1.6.6.7.1.2"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.7.1.2.1">Pre-training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.F3.sf1.6.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.7.1.3.1">Covid-19 CXR-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.F3.sf1.6.6.7.1.4"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.7.1.4.1">PadChest-unseen</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.F3.sf1.6.6.7.1.5"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.7.1.5.1">PadChest-rare</span></th>
</tr>
<tr class="ltx_tr" id="S4.F3.sf1.6.6.6">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.F3.sf1.6.6.6.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.F3.sf1.6.6.6.8"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.6.8.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf1.1.1.1.1">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf1.1.1.1.1.m1.1"><semantics id="S4.F3.sf1.1.1.1.1.m1.1a"><mo id="S4.F3.sf1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.F3.sf1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.1.1.1.1.m1.1b"><ci id="S4.F3.sf1.1.1.1.1.m1.1.1.cmml" xref="S4.F3.sf1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf1.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F3.sf1.2.2.2.2">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf1.2.2.2.2.m1.1"><semantics id="S4.F3.sf1.2.2.2.2.m1.1a"><mo id="S4.F3.sf1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.F3.sf1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.2.2.2.2.m1.1b"><ci id="S4.F3.sf1.2.2.2.2.m1.1.1.cmml" xref="S4.F3.sf1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf1.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf1.3.3.3.3">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf1.3.3.3.3.m1.1"><semantics id="S4.F3.sf1.3.3.3.3.m1.1a"><mo id="S4.F3.sf1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.F3.sf1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.3.3.3.3.m1.1b"><ci id="S4.F3.sf1.3.3.3.3.m1.1.1.cmml" xref="S4.F3.sf1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf1.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F3.sf1.4.4.4.4">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf1.4.4.4.4.m1.1"><semantics id="S4.F3.sf1.4.4.4.4.m1.1a"><mo id="S4.F3.sf1.4.4.4.4.m1.1.1" stretchy="false" xref="S4.F3.sf1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.4.4.4.4.m1.1b"><ci id="S4.F3.sf1.4.4.4.4.m1.1.1.cmml" xref="S4.F3.sf1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf1.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf1.5.5.5.5">AUC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf1.5.5.5.5.m1.1"><semantics id="S4.F3.sf1.5.5.5.5.m1.1a"><mo id="S4.F3.sf1.5.5.5.5.m1.1.1" stretchy="false" xref="S4.F3.sf1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.5.5.5.5.m1.1b"><ci id="S4.F3.sf1.5.5.5.5.m1.1.1.cmml" xref="S4.F3.sf1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf1.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf1.6.6.6.6">F1 <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf1.6.6.6.6.m1.1"><semantics id="S4.F3.sf1.6.6.6.6.m1.1a"><mo id="S4.F3.sf1.6.6.6.6.m1.1.1" stretchy="false" xref="S4.F3.sf1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf1.6.6.6.6.m1.1b"><ci id="S4.F3.sf1.6.6.6.6.m1.1.1.cmml" xref="S4.F3.sf1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf1.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf1.6.6.6.6.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F3.sf1.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.8.1.1" rowspan="3"><span class="ltx_text" id="S4.F3.sf1.6.6.8.1.1.1">ConVIRT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.8.1.2">MIMIC-CXR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.8.1.3">62.78</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.8.1.4">71.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.8.1.5">51.17</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.8.1.6">4.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.8.1.7">50.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.8.1.8">3.31</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf1.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.F3.sf1.6.6.9.2.1">SynCXR</th>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.9.2.2">64.41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf1.6.6.9.2.3">72.03</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.9.2.4">54.47</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf1.6.6.9.2.5">4.51</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.9.2.6">53.70</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.9.2.7">3.69</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf1.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.F3.sf1.6.6.10.3.1">Mix</th>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.10.3.2"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.10.3.2.1">69.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf1.6.6.10.3.3"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.10.3.3.1">72.85</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.10.3.4"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.10.3.4.1">58.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf1.6.6.10.3.5"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.10.3.5.1">5.35</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.10.3.6"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.10.3.6.1">57.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.10.3.7"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.10.3.7.1">4.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf1.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.11.4.1" rowspan="3"><span class="ltx_text" id="S4.F3.sf1.6.6.11.4.1.1">GLoRIA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.11.4.2">MIMIC-CXR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.11.4.3">64.52</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.11.4.4">70.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.11.4.5">49.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf1.6.6.11.4.6">4.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.11.4.7">48.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf1.6.6.11.4.8">3.41</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf1.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.F3.sf1.6.6.12.5.1">SynCXR</th>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.12.5.2">66.70</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf1.6.6.12.5.3">71.90</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.12.5.4">54.24</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf1.6.6.12.5.5">4.10</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.12.5.6">51.26</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf1.6.6.12.5.7">3.75</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf1.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.F3.sf1.6.6.13.6.1"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.1.1">Mix</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf1.6.6.13.6.2"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.2.1">68.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.F3.sf1.6.6.13.6.3"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.3.1">73.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf1.6.6.13.6.4"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.4.1">58.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.F3.sf1.6.6.13.6.5"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.5.1">5.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf1.6.6.13.6.6"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.6.1">58.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf1.6.6.13.6.7"><span class="ltx_text ltx_font_bold" id="S4.F3.sf1.6.6.13.6.7.1">4.62</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F3.sf2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.F3.sf2.6" style="width:186.8pt;height:74.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-86.5pt,34.6pt) scale(0.519300197594242,0.519300197594242) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.F3.sf2.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.F3.sf2.6.6.7.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.F3.sf2.6.6.7.1.1"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.7.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.F3.sf2.6.6.7.1.2"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.7.1.2.1">Pre-training</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.F3.sf2.6.6.7.1.3"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.7.1.3.1">RSNA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.F3.sf2.6.6.7.1.4"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.7.1.4.1">Covid-19 Rural</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.F3.sf2.6.6.7.1.5"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.7.1.5.1">SIIM</span></th>
</tr>
<tr class="ltx_tr" id="S4.F3.sf2.6.6.6">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.F3.sf2.6.6.6.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.F3.sf2.6.6.6.8"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.6.8.1">Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf2.1.1.1.1">IoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf2.1.1.1.1.m1.1"><semantics id="S4.F3.sf2.1.1.1.1.m1.1a"><mo id="S4.F3.sf2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.F3.sf2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.1.1.1.1.m1.1b"><ci id="S4.F3.sf2.1.1.1.1.m1.1.1.cmml" xref="S4.F3.sf2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf2.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F3.sf2.2.2.2.2">Dice <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf2.2.2.2.2.m1.1"><semantics id="S4.F3.sf2.2.2.2.2.m1.1a"><mo id="S4.F3.sf2.2.2.2.2.m1.1.1" stretchy="false" xref="S4.F3.sf2.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.2.2.2.2.m1.1b"><ci id="S4.F3.sf2.2.2.2.2.m1.1.1.cmml" xref="S4.F3.sf2.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf2.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf2.3.3.3.3">IoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf2.3.3.3.3.m1.1"><semantics id="S4.F3.sf2.3.3.3.3.m1.1a"><mo id="S4.F3.sf2.3.3.3.3.m1.1.1" stretchy="false" xref="S4.F3.sf2.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.3.3.3.3.m1.1b"><ci id="S4.F3.sf2.3.3.3.3.m1.1.1.cmml" xref="S4.F3.sf2.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf2.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.F3.sf2.4.4.4.4">Dice <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf2.4.4.4.4.m1.1"><semantics id="S4.F3.sf2.4.4.4.4.m1.1a"><mo id="S4.F3.sf2.4.4.4.4.m1.1.1" stretchy="false" xref="S4.F3.sf2.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.4.4.4.4.m1.1b"><ci id="S4.F3.sf2.4.4.4.4.m1.1.1.cmml" xref="S4.F3.sf2.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf2.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf2.5.5.5.5">IoU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf2.5.5.5.5.m1.1"><semantics id="S4.F3.sf2.5.5.5.5.m1.1a"><mo id="S4.F3.sf2.5.5.5.5.m1.1.1" stretchy="false" xref="S4.F3.sf2.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.5.5.5.5.m1.1b"><ci id="S4.F3.sf2.5.5.5.5.m1.1.1.cmml" xref="S4.F3.sf2.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf2.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.F3.sf2.6.6.6.6">Dice <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.F3.sf2.6.6.6.6.m1.1"><semantics id="S4.F3.sf2.6.6.6.6.m1.1a"><mo id="S4.F3.sf2.6.6.6.6.m1.1.1" stretchy="false" xref="S4.F3.sf2.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.F3.sf2.6.6.6.6.m1.1b"><ci id="S4.F3.sf2.6.6.6.6.m1.1.1.cmml" xref="S4.F3.sf2.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.sf2.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.F3.sf2.6.6.6.6.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.F3.sf2.6.6.8.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.8.1.1" rowspan="3"><span class="ltx_text" id="S4.F3.sf2.6.6.8.1.1.1">ConVIRT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.8.1.2">MIMIC-CXR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.8.1.3">18.93</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.8.1.4">28.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.8.1.5">7.42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.8.1.6">10.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.8.1.7">3.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.8.1.8">8.74</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf2.6.6.9.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.F3.sf2.6.6.9.2.1">SynCXR</th>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.9.2.2">22.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf2.6.6.9.2.3">31.45</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.9.2.4">8.62</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf2.6.6.9.2.5">10.83</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.9.2.6">3.43</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.9.2.7">9.67</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf2.6.6.10.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.F3.sf2.6.6.10.3.1"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.1.1">Mix</span></th>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.10.3.2"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.2.1">25.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf2.6.6.10.3.3"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.3.1">34.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.10.3.4"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.4.1">12.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf2.6.6.10.3.5"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.5.1">14.12</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.10.3.6"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.6.1">4.58</span></td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.10.3.7"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.10.3.7.1">11.43</span></td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf2.6.6.11.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.11.4.1" rowspan="3"><span class="ltx_text" id="S4.F3.sf2.6.6.11.4.1.1">GLoRIA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.11.4.2">MIMIC-CXR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.11.4.3">21.82</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.11.4.4">34.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.11.4.5">8.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.F3.sf2.6.6.11.4.6">12.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.11.4.7">3.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.F3.sf2.6.6.11.4.8">10.23</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf2.6.6.12.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.F3.sf2.6.6.12.5.1">SynCXR</th>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.12.5.2">23.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf2.6.6.12.5.3">35.25</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.12.5.4">9.47</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.F3.sf2.6.6.12.5.5">13.00</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.12.5.6">3.50</td>
<td class="ltx_td ltx_align_center" id="S4.F3.sf2.6.6.12.5.7">10.75</td>
</tr>
<tr class="ltx_tr" id="S4.F3.sf2.6.6.13.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.F3.sf2.6.6.13.6.1">Mix</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf2.6.6.13.6.2"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.13.6.2.1">26.34</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.F3.sf2.6.6.13.6.3"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.13.6.3.1">36.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf2.6.6.13.6.4"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.13.6.4.1">12.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.F3.sf2.6.6.13.6.5"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.13.6.5.1">14.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf2.6.6.13.6.6"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.13.6.6.1">4.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.F3.sf2.6.6.13.6.7"><span class="ltx_text ltx_font_bold" id="S4.F3.sf2.6.6.13.6.7.1">11.73</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Zero-shot tasks performance of MedVLP models on disease classification (a) and grounding (b) across multiple datasets, using MIMIC-CXR, SynCXR, and Mix datasets for pretraining.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">Fine-tuning Tasks.</span>
To evaluate the representation quality learned by MedVLP, we report the fine-tuned classification and segmentation performance in Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S4.T3" title="Table 3 ‣ 4.2 Experimental Results ‣ 4 Experiments Configurations ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">3</span></a>. Similar to the zero-shot task, MedVLP models pre-trained on SynCXR consistently outperform those trained on the real dataset across all data ratios for both classification and segmentation tasks. Furthermore, the combination of real and synthetic datasets (Mix) further boosts performance, demonstrating that SynCXR data not only enhances cross-modal representation learning but also improves performance in single-modal tasks.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:397.5pt;height:79.2pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-210.5pt,41.7pt) scale(0.485658178031427,0.485658178031427) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" colspan="12" id="S4.T3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.2.1">Classification</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="9" id="S4.T3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.3.1">Segmentation</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T3.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.2.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.2">RSNA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.3">SIIM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.4">Covid19 CXR-2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.5">ChestXray-14</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.6">RSNA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.7">Covid-19 Rural</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T3.1.1.2.2.8">SIIM</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T3.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.3.1.1">Data Ratio</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.2">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.3">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.3.3.4">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.5">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.6">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.3.3.7">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.8">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.9">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.3.3.10">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.11">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.12">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" id="S4.T3.1.1.3.3.13">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.14">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.15">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.3.3.16">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.17">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.18">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.3.3.19">100%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.20">1%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.21">10%</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.3.3.22">100%</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.1">ConVIRT-Real</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.2">78.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.3">85.42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.4">87.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.5">72.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.6">80.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.7">91.67</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.8">90.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.9">97.74</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.10">99.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.11">57.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.12">72.53</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T3.1.1.4.1.13">79.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.14">56.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.15">63.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.16">71.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.17">16.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.18">30.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.19">42.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.20">28.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.21">47.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.22">65.75</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.1.5.2.1">ConVIRT-Syn</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.2">79.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.3">85.58</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.4">87.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.5">73.51</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.6">81.10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.7">91.84</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.8">91.50</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.9">98.80</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.10">99.73</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.11">57.45</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.12">73.60</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.1.1.5.2.13">80.20</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.14">58.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.15">65.10</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.16">72.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.17">17.10</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.18">32.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.19">43.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.20">29.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.21">48.50</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.22">66.81</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.1.6.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.1.1">ConVIRT-Mix</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.2.1">79.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.3.1">86.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.6.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.4.1">88.45</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.5.1">73.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.6.1">82.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.6.3.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.7.1">92.31</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.8.1">91.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.9"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.9.1">99.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.6.3.10"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.10.1">99.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.11.1">57.61</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.12.1">74.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.1.1.6.3.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.13.1">80.51</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.14">58.50</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.15">65.81</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.6.3.16"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.16.1">73.30</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.17"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.17.1">18.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.18"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.18.1">32.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.6.3.19"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.19.1">44.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.20"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.20.1">30.10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.21"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.21.1">48.81</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.6.3.22"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.3.22.1">67.11</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.1">GLoRIA-Real</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.2">79.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.3">85.59</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.4">87.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.5">75.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.6">86.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.7">91.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.8">92.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.9">97.18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.10">99.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.11">58.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.12">72.87</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S4.T3.1.1.7.4.13">79.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.14">58.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.15">67.71</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.16">72.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.17">16.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.18">31.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.19">43.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.20">31.87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.21">40.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.22">64.82</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.1.8.5.1">GLoRIA-Syn</th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.2">80.30</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.3">86.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.4">88.00</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.5">76.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.6">87.40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.7">92.11</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.8">94.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.9">98.41</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.10">99.75</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.11">60.11</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.12">74.01</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S4.T3.1.1.8.5.13">81.11</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.14">60.41</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.15">70.01</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.16">73.51</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.17">17.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.18">32.51</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.19">45.01</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.20">32.91</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.21">41.91</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.22">66.01</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.1.1.9.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.1.1">GLoRIA-Mix</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.2.1">81.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.3.1">87.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.9.6.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.4.1">88.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.5.1">77.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.6.1">88.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.9.6.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.7.1">92.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.8.1">94.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.9"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.9.1">99.61</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.9.6.10"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.10.1">99.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.11"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.11.1">60.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.12"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.12.1">74.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S4.T3.1.1.9.6.13"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.13.1">81.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.14"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.14.1">61.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.15"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.15.1">70.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.9.6.16"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.16.1">74.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.17"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.17.1">17.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.18"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.18.1">33.01</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.9.6.19"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.19.1">45.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.20"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.20.1">33.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.21"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.21.1">42.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.9.6.22"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.22.1">67.51</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results from two MedVLP methods pre-trained on real, synthetic, and mixed datasets are reported for classification (AUC) and segmentation (Dice) tasks. ‘ConVIRT-Real’ and ‘GLoRIA-Real’ refer to models pre-trained on MIMIC-CXR using real data, while ‘ConVIRT-Syn’ and ‘GLoRIA-Syn’ indicate models pre-trained on SynCXR using synthetic data. ‘ConVIRT-Mix’ and ‘GLoRIA-Mix’ represent models trained on a combination of MIMIC-CXR and SynCXR. Best results are in bold.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf1">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.F4.sf1.1" style="width:285.0pt;height:70.8pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-76.7pt,18.9pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.F4.sf1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F4.sf1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.F4.sf1.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.F4.sf1.1.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.F4.sf1.1.1.1.1.2">Entity Sampling</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.F4.sf1.1.1.1.1.3">Avg. Zero-shot</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf1.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F4.sf1.1.1.2.2.1">Strategy</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf1.1.1.2.2.2">Classification</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf1.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.F4.sf1.1.1.3.3.1" rowspan="2"><span class="ltx_text" id="S5.F4.sf1.1.1.3.3.1.1">ConVIRT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.F4.sf1.1.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.F4.sf1.1.1.3.3.2.1">w/ balance Sampling</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf1.1.1.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.F4.sf1.1.1.3.3.3.1">63.65</span></td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf1.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F4.sf1.1.1.4.4.1">w/o balance Sampling</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf1.1.1.4.4.2">60.21</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf1.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.F4.sf1.1.1.5.5.1" rowspan="2"><span class="ltx_text" id="S5.F4.sf1.1.1.5.5.1.1">GLoRIA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.F4.sf1.1.1.5.5.2"><span class="ltx_text ltx_font_bold" id="S5.F4.sf1.1.1.5.5.2.1">w/ balance Sampling</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf1.1.1.5.5.3"><span class="ltx_text ltx_font_bold" id="S5.F4.sf1.1.1.5.5.3.1">61.87</span></td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf1.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.F4.sf1.1.1.6.6.1">w/o balance Sampling</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F4.sf1.1.1.6.6.2">58.42</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.sf2">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.F4.sf2.1" style="width:220.9pt;height:81.4pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.0pt,49.5pt) scale(0.45,0.45) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.F4.sf2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.F4.sf2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.F4.sf2.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.F4.sf2.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F4.sf2.1.1.1.1.2">Real</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F4.sf2.1.1.1.1.3">Syn.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F4.sf2.1.1.1.1.4">Real</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.F4.sf2.1.1.1.1.5">Syn.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.F4.sf2.1.1.1.1.6">Avg. Zero-shot</th>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.F4.sf2.1.1.2.2.1">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.F4.sf2.1.1.2.2.2">Image</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.F4.sf2.1.1.2.2.3">Report</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.F4.sf2.1.1.2.2.4">Report</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.F4.sf2.1.1.2.2.5">Classification</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.F4.sf2.1.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.F4.sf2.1.1.3.1.1" rowspan="4"><span class="ltx_text" id="S5.F4.sf2.1.1.3.1.1.1">ConVIRT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>)</cite></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf2.1.1.3.1.2">✓</td>
<td class="ltx_td ltx_border_t" id="S5.F4.sf2.1.1.3.1.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf2.1.1.3.1.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.F4.sf2.1.1.3.1.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf2.1.1.3.1.6">59.59</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.4.2">
<td class="ltx_td" id="S5.F4.sf2.1.1.4.2.1"></td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.4.2.2">✓</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.4.2.3">✓</td>
<td class="ltx_td ltx_border_r" id="S5.F4.sf2.1.1.4.2.4"></td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.4.2.5">61.04</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.5.3">
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.5.3.1">✓</td>
<td class="ltx_td" id="S5.F4.sf2.1.1.5.3.2"></td>
<td class="ltx_td" id="S5.F4.sf2.1.1.5.3.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F4.sf2.1.1.5.3.4">✓</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.5.3.5">59.36</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.6.4">
<td class="ltx_td" id="S5.F4.sf2.1.1.6.4.1"></td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.6.4.2">✓</td>
<td class="ltx_td" id="S5.F4.sf2.1.1.6.4.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F4.sf2.1.1.6.4.4">✓</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S5.F4.sf2.1.1.6.4.5.1">63.65</span></td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.F4.sf2.1.1.7.5.1" rowspan="4"><span class="ltx_text" id="S5.F4.sf2.1.1.7.5.1.1">GLoRIA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf2.1.1.7.5.2">✓</td>
<td class="ltx_td ltx_border_t" id="S5.F4.sf2.1.1.7.5.3"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf2.1.1.7.5.4">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.F4.sf2.1.1.7.5.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.F4.sf2.1.1.7.5.6">57.83</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.8.6">
<td class="ltx_td" id="S5.F4.sf2.1.1.8.6.1"></td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.8.6.2">✓</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.8.6.3">✓</td>
<td class="ltx_td ltx_border_r" id="S5.F4.sf2.1.1.8.6.4"></td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.8.6.5">58.62</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.9.7">
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.9.7.1">✓</td>
<td class="ltx_td" id="S5.F4.sf2.1.1.9.7.2"></td>
<td class="ltx_td" id="S5.F4.sf2.1.1.9.7.3"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.F4.sf2.1.1.9.7.4">✓</td>
<td class="ltx_td ltx_align_center" id="S5.F4.sf2.1.1.9.7.5">57.69</td>
</tr>
<tr class="ltx_tr" id="S5.F4.sf2.1.1.10.8">
<td class="ltx_td ltx_border_bb" id="S5.F4.sf2.1.1.10.8.1"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F4.sf2.1.1.10.8.2">✓</td>
<td class="ltx_td ltx_border_bb" id="S5.F4.sf2.1.1.10.8.3"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.F4.sf2.1.1.10.8.4">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.F4.sf2.1.1.10.8.5"><span class="ltx_text ltx_font_bold" id="S5.F4.sf2.1.1.10.8.5.1">61.87</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Evaluation of entity sampling strategies for synthetic report generation and the impact of synthetic data types on MedVLP.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Effect of Balanced Entity Sampling in Generating Synthetic Reports.</span>
We evaluate the impact of balanced sampling entities when generating synthetic reports using LLMs. For the synthetic dataset without balanced sampling, we adjust entity frequencies to match their distribution in MIMIC-CXR, leading to a long-tailed distribution. As shown in Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.sf1" title="In Table 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4(a)</span></a>, for both MedVLP methods, the performance improves significantly when using synthetic datasets generated from balanced sampled entities. This demonstrates that balanced sampling of entities leads to a more representative dataset, benefiting MedVLP performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Evaluating the Contribution of Synthetic Images and Reports.</span>
We aim to assess the individual impact of synthetic images and synthetic reports on MedVLP performance. As shown in Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.sf2" title="In Table 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, we generate two partially synthetic datasets by replacing either the image or the text with synthetic data, while keeping the other components real, to evaluate their respective contributions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Real Image, Synthetic Report:</span> In this setting, we use MedVersa<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/hyzhou/MedVersa" title="">https://huggingface.co/hyzhou/MedVersa</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib76" title="">2024</a>)</cite>, a state-of-the-art radiology report generation model, to generate synthetic reports for each real CXR image. We then train MedVLP models using these real image and synthetic report pairs.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Real Report, Synthetic Image:</span> In this setting, we use RoentGen <cite class="ltx_cite ltx_citemacro_citep">(Bluethgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib4" title="">2024</a>)</cite>, a text-to-image model, to generate synthetic CXR images for each real report. The ‘IMPRESSION’ section of each report serves as the prompt for generating synthetic CXR images. These synthetic image and real report pairs are used to train MedVLP models.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">According to Tab <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.sf2" title="In Table 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, for both MedVLP methods, using real images with synthetic reports results in decreased performance, likely due to the persistent long-tailed distribution, as the synthetic reports are generated based on real images. However, using real reports with synthetic images slightly improves performance, as synthetic images can be curated using our image filtering procedure to ensure high quality, avoiding issues commonly found in real datasets. Using both synthetic images and synthetic reports achieves the highest performance, indicating that a well-curated synthetic dataset can significantly enhance MedVLP performance.</p>
</div>
<figure class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="S5.F4.fig1" style="width:147.1pt;">
<div class="ltx_block ltx_figure_panel ltx_parbox ltx_align_center ltx_align_middle" id="S5.F4.1" style="width:242.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="487" id="S5.F4.1.g1" src="extracted/5934507/ablate.png" width="592"/>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Effectiveness of various factors on SynCXR dataset.
<span class="ltx_text ltx_font_bold" id="S5.F4.fig1.5.1">Top:</span> Impact of entity usage ratio on MedVLP performance for ConVIRT and GLoRIA methods.
<span class="ltx_text ltx_font_bold" id="S5.F4.fig1.6.2">Bottom Left:</span> Effectiveness of different LLMs for report generation on both MedVLP methods.
<span class="ltx_text ltx_font_bold" id="S5.F4.fig1.7.3">Bottom Right:</span> Effectiveness of different CXR image generation models for both MedVLP methods.
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.1"><span class="ltx_text ltx_font_bold" id="S5.p5.1.1">Impact of Entity Diversity.</span>
We evaluate the impact of entity diversity by varying the number of entities used for generating the SynCXR dataset. We generate synthetic datasets using 25%, 50%, and 75% of these entities, following the same procedure each time. The results, shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.fig1" title="Figure 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4</span></a> (Top), indicate that zero-shot classification performance improves as more entities are used for report generation. This suggests that increasing dataset diversity positively influences downstream performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p6">
<p class="ltx_p" id="S5.p6.1"><span class="ltx_text ltx_font_bold" id="S5.p6.1.1">Impact of Different Report Generators.</span>
We also examine the impact of using different LLMs for synthetic report generation. As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.fig1" title="Figure 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4</span></a> (Bottom Left), we compare two general LLMs, LLaMA 3.1 (8B and 70B), and two medical-specific LLMs, Meditron3 (8B<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/OpenMeditron/Meditron3-8B" title="">https://huggingface.co/OpenMeditron/Meditron3-8B</a></span></span></span> and 70B<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/OpenMeditron/Meditron3-70B" title="">https://huggingface.co/OpenMeditron/Meditron3-70B</a></span></span></span>). Despite Meditron3 being trained specifically on medical corpora and inheriting weights from LLaMA, the dataset generated by LLaMA 3.1-70B-Instruct achieves the best performance. This indicates that a powerful general LLM is effective for generating synthetic datasets, and using domain-specific fine-tuned versions may degrade the quality of the synthetic data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p7">
<p class="ltx_p" id="S5.p7.1"><span class="ltx_text ltx_font_bold" id="S5.p7.1.1">Impact of Different Image Generators.</span>
We evaluate various text-to-image models for synthetic CXR image generation, including CXR-IRGen <cite class="ltx_cite ltx_citemacro_citep">(Shentu &amp; Al Moubayed, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib49" title="">2024</a>)</cite>, LLM-CXR <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib27" title="">2023</a>)</cite>, and RoentGen <cite class="ltx_cite ltx_citemacro_citep">(Bluethgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib4" title="">2024</a>)</cite>. As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#S5.F4.fig1" title="Figure 4 ‣ 5 Analysis ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">4</span></a> (Bottom Right), datasets generated by RoentGen lead to the best performance for both MedVLP methods. This is likely because RoentGen is the only image generation model verified by clinicians, suggesting that the quality of image generation models is crucial for building synthetic datasets, and models should be validated by clinical experts.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we tackle the question: <span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.p1.1.1">Can MedVLP succeed using purely synthetic data?</span> Our findings demonstrate that the answer is: <span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.p1.1.2">Yes</span>. To the best of our knowledge, this is the first study to comprehensively explore the potential of synthetic data for MedVLP models. We also identify key limitations in existing real-world datasets and introduce SynCXR—a synthetic dataset of 200,000 image-text pairs generated without any manual quality checks.
Our findings show that MedVLP models trained on purely synthetic data outperform those trained on real data. Moreover, combining synthetic and real data further boosts model performance, demonstrating the potential of synthetic data to overcome limitations in real-world datasets. We systematically analyze key factors in SynCXR and validate its effectiveness through extensive ablation studies.
In summary, we show that MedVLP achieves strong performance using a purely synthetic image-text dataset and benefits significantly from a combination of real and synthetic data. We believe this work will inspire the community to fully leverage synthetic data and mitigate the challenges posed by noisy and limited real-world datasets.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)</span>
<span class="ltx_bibblock">
AI@Meta.

</span>
<span class="ltx_bibblock">Llama 3 model card.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="">https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi et al. (2023)</span>
<span class="ltx_bibblock">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">TMLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bannur et al. (2023)</span>
<span class="ltx_bibblock">
Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel C Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al.

</span>
<span class="ltx_bibblock">Learning to exploit temporal structure for biomedical vision-language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  15016–15027, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bluethgen et al. (2024)</span>
<span class="ltx_bibblock">
Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P Langlotz, and Akshay S Chaudhari.

</span>
<span class="ltx_bibblock">A vision–language foundation model for the generation of realistic chest x-ray images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Nature Biomedical Engineering</em>, pp.  1–13, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boecking et al. (2022)</span>
<span class="ltx_bibblock">
Benedikt Boecking, Naoto Usuyama, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer, Stephanie Hyland, Maria Wetscherek, Tristan Naumann, Aditya Nori, Javier Alvarez-Valle, et al.

</span>
<span class="ltx_bibblock">Making the most of text semantics to improve biomedical vision–language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">European conference on computer vision</em>, pp.  1–21. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bustos et al. (2020)</span>
<span class="ltx_bibblock">
Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá.

</span>
<span class="ltx_bibblock">Padchest: A large chest x-ray image dataset with multi-label annotated reports.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Medical image analysis</em>, 66:101797, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Chen Chen, Chen Qin, Cheng Ouyang, Zeju Li, Shuo Wang, Huaqi Qiu, Liang Chen, Giacomo Tarroni, Wenjia Bai, and Daniel Rueckert.

</span>
<span class="ltx_bibblock">Enhancing mr image segmentation with realistic adversarial data augmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Medical Image Analysis</em>, 82:102597, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024a)</span>
<span class="ltx_bibblock">
Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, and Zongwei Zhou.

</span>
<span class="ltx_bibblock">Towards generalizable tumor synthesis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  11147–11158, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CVPR</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023)</span>
<span class="ltx_bibblock">
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2312.14238</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024b)</span>
<span class="ltx_bibblock">
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al.

</span>
<span class="ltx_bibblock">How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2404.16821</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delbrouck et al. (2024)</span>
<span class="ltx_bibblock">
Jean-Benoit Delbrouck, Pierre Chambon, Zhihong Chen, Maya Varma, Andrew Johnston, Louis Blankemeier, Dave Van Veen, Tan Bui, Steven Truong, and Curtis Langlotz.

</span>
<span class="ltx_bibblock">Radgraph-xl: A large-scale expert-annotated dataset for entity and relation extraction from radiology reports.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Findings of the Association for Computational Linguistics ACL 2024</em>, pp.  12902–12915, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai et al. (2020)</span>
<span class="ltx_bibblock">
Shivang Desai, Ahmad Baghal, Thidathip Wongsurawat, Piroon Jenjaroenpun, Thomas Powell, Shaymaa Al-Shukri, Kim Gates, Phillip Farmer, Michael Rutherford, Geri Blake, et al.

</span>
<span class="ltx_bibblock">Chest imaging representing a covid-19 positive rural us population.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Scientific data</em>, 7(1):414, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2023)</span>
<span class="ltx_bibblock">
Lijie Fan, Kaifeng Chen, Dilip Krishnan, Dina Katabi, Phillip Isola, and Yonglong Tian.

</span>
<span class="ltx_bibblock">Scaling laws of synthetic images for model training… for now.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2312.04567</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hammoud et al. (2024)</span>
<span class="ltx_bibblock">
Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, and Bernard Ghanem.

</span>
<span class="ltx_bibblock">Synthclip: Are we ready for a fully synthetic clip training?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2402.01832</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, and XIAOJUAN QI.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for image recognition?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ICLR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2021)</span>
<span class="ltx_bibblock">
Shih-Cheng Huang, Liyue Shen, Matthew P Lungren, and Serena Yeung.

</span>
<span class="ltx_bibblock">Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pp.  3942–3951, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Irvin et al. (2019)</span>
<span class="ltx_bibblock">
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.

</span>
<span class="ltx_bibblock">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volume 33, pp.  590–597, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahanian et al. (2022)</span>
<span class="ltx_bibblock">
Ali Jahanian, Xavier Puig, Yonglong Tian, and Phillip Isola.

</span>
<span class="ltx_bibblock">Generative models as a data source for multiview representation learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">ICLR</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2023)</span>
<span class="ltx_bibblock">
Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and Zhiyong Lu.

</span>
<span class="ltx_bibblock">Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Bioinformatics</em>, 39(11):btad651, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019a)</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Scientific data</em>, 6(1):1–8, 2019a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019b)</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:1901.07042</em>, 2019b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson-Roberson et al. (2017)</span>
<span class="ltx_bibblock">
Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen, and Ram Vasudevan.

</span>
<span class="ltx_bibblock">Driving in the matrix: Can virtual worlds replace human-generated annotations for real world tasks?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">ICRA</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosravi et al. (2024)</span>
<span class="ltx_bibblock">
Bardia Khosravi, Frank Li, Theo Dapamede, Pouria Rouzrokh, Cooper U Gamble, Hari M Trivedi, Cody C Wyles, Andrew B Sellergren, Saptarshi Purkayastha, Bradley J Erickson, et al.

</span>
<span class="ltx_bibblock">Synthetically enhanced: unveiling synthetic data’s potential in medical imaging research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">EBioMedicine</em>, 104, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koetzier et al. (2024)</span>
<span class="ltx_bibblock">
Lennart R Koetzier, Jie Wu, Domenico Mastrodicasa, Aline Lutz, Matthew Chung, W Adam Koszek, Jayanth Pratap, Akshay S Chaudhari, Pranav Rajpurkar, Matthew P Lungren, et al.

</span>
<span class="ltx_bibblock">Generating synthetic data for medical imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Radiology</em>, 312(3):e232471, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ktena et al. (2024)</span>
<span class="ltx_bibblock">
Ira Ktena, Olivia Wiles, Isabela Albuquerque, Sylvestre-Alvise Rebuffi, Ryutaro Tanno, Abhijit Guha Roy, Shekoofeh Azizi, Danielle Belgrave, Pushmeet Kohli, Taylan Cemgil, et al.

</span>
<span class="ltx_bibblock">Generative models improve fairness of medical classifiers under distribution shifts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Nature Medicine</em>, pp.  1–8, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2023)</span>
<span class="ltx_bibblock">
Suhyeon Lee, Won Jun Kim, Jinho Chang, and Jong Chul Ye.

</span>
<span class="ltx_bibblock">Llm-cxr: Instruction-finetuned llm for cxr image understanding and generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2305.11490</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.

</span>
<span class="ltx_bibblock">CAMEL: Communicative agents for ”mind” exploration of large language model society.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">NeurIPS</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024)</span>
<span class="ltx_bibblock">
Zhe Li, Laurence T Yang, Bocheng Ren, Xin Nie, Zhangyang Gao, Cheng Tan, and Stan Z Li.

</span>
<span class="ltx_bibblock">Mlip: Enhancing medical visual representation with divergence encoder and knowledge-guided contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2402.02045</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong Zhang, Anand Shah, Wenjia Bai, and Rossella Arcucci.

</span>
<span class="ltx_bibblock">M-flag: Medical vision-language pre-training with frozen language models and latent space geometry optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2307.08347</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Che Liu, Sibo Cheng, Miaojing Shi, Anand Shah, Wenjia Bai, and Rossella Arcucci.

</span>
<span class="ltx_bibblock">Imitate: Clinical prior guided hierarchical vision-language pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2310.07355</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023c)</span>
<span class="ltx_bibblock">
Che Liu, Cheng Ouyang, Yinda Chen, Cesar César Quilodrán-Casas, Lei Ma, Jie Fu, Yike Guo, Anand Shah, Wenjia Bai, and Rossella Arcucci.

</span>
<span class="ltx_bibblock">T3d: Towards 3d medical image understanding through vision-language pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2312.01529</em>, 2023c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023d)</span>
<span class="ltx_bibblock">
Che Liu, Cheng Ouyang, Sibo Cheng, Anand Shah, Wenjia Bai, and Rossella Arcucci.

</span>
<span class="ltx_bibblock">G2d: From global to dense radiography representation learning via vision-language pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2312.01522</em>, 2023d.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023e)</span>
<span class="ltx_bibblock">
Che Liu, Anand Shah, Wenjia Bai, and Rossella Arcucci.

</span>
<span class="ltx_bibblock">Utilizing synthetic data for medical vision-language pre-training: Bypassing the need for real images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2310.07027</em>, 2023e.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlova et al. (2022)</span>
<span class="ltx_bibblock">
Maya Pavlova, Naomi Terhljan, Audrey G Chung, Andy Zhao, Siddharth Surana, Hossein Aboutalebi, Hayden Gunraj, Ali Sabri, Amer Alaref, and Alexander Wong.

</span>
<span class="ltx_bibblock">Covid-net cxr-2: An enhanced deep convolutional neural network design for detection of covid-19 cases from chest x-ray images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Frontiers in Medicine</em>, 9:861680, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pérez-García et al. (2024)</span>
<span class="ltx_bibblock">
Fernando Pérez-García, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel C Castro, Anton Schwaighofer, Matthew P Lungren, et al.

</span>
<span class="ltx_bibblock">Rad-dino: Exploring scalable medical image encoders beyond text supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2401.10815</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al. (2024a)</span>
<span class="ltx_bibblock">
Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, and Johan W Verjans.

</span>
<span class="ltx_bibblock">Decomposing disease descriptions for enhanced pathology detection: A multi-aspect vision-language matching framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2403.07636</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phan et al. (2024b)</span>
<span class="ltx_bibblock">
Vu Minh Hieu Phan, Yutong Xie, Yuankai Qi, Lingqiao Liu, Liyang Liu, Bowen Zhang, Zhibin Liao, Qi Wu, Minh-Son To, and Johan W Verjans.

</span>
<span class="ltx_bibblock">Decomposing disease descriptions for enhanced pathology detection: A multi-aspect vision-language pre-training framework.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pp.  11492–11501, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023)</span>
<span class="ltx_bibblock">
Chen Qin, Shuo Wang, Chen Chen, Wenjia Bai, and Daniel Rueckert.

</span>
<span class="ltx_bibblock">Generative myocardial motion tracking via latent space exploration with biomechanics-informed prior.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Medical Image Analysis</em>, 83:102682, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">ICML</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richter et al. (2016)</span>
<span class="ltx_bibblock">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Playing for data: Ground truth from computer games.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">ECCV</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pp.  10684–10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>, pp.  234–241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ros et al. (2016)</span>
<span class="ltx_bibblock">
German Ros, Laura Sellart, Joanna Materzynska, David Vazquez, and Antonio M. Lopez.

</span>
<span class="ltx_bibblock">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">CVPR</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rossenbach et al. (2020)</span>
<span class="ltx_bibblock">
Nick Rossenbach, Albert Zeyer, Ralf Schlüter, and Hermann Ney.

</span>
<span class="ltx_bibblock">Generating synthetic audio data for attention-based speech recognition systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ICASSP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saporta et al. (2022)</span>
<span class="ltx_bibblock">
Adriel Saporta, Xiaotong Gui, Ashwin Agrawal, Anuj Pareek, Steven QH Truong, Chanh DT Nguyen, Van-Doan Ngo, Jayne Seekins, Francis G Blankenberg, Andrew Y Ng, et al.

</span>
<span class="ltx_bibblock">Benchmarking saliency methods for chest x-ray interpretation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Nature Machine Intelligence</em>, 4(10):867–878, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sariyildiz et al. (2023)</span>
<span class="ltx_bibblock">
Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis.

</span>
<span class="ltx_bibblock">Fake it till you make it: Learning transferable representations from synthetic imagenet clones.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CVPR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharifzadeh et al. (2024)</span>
<span class="ltx_bibblock">
Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, and Andrea Banino.

</span>
<span class="ltx_bibblock">Synth2: Boosting visual-language models with synthetic captions and image embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2403.07750</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shentu &amp; Al Moubayed (2024)</span>
<span class="ltx_bibblock">
Junjie Shentu and Noura Al Moubayed.

</span>
<span class="ltx_bibblock">Cxr-irgen: An integrated vision and language model for the generation of clinically accurate chest x-ray image-report pairs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pp.  5212–5221, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih et al. (2019)</span>
<span class="ltx_bibblock">
George Shih, Carol C Wu, Safwan S Halabi, Marc D Kohli, Luciano M Prevedello, Tessa S Cook, Arjun Sharma, Judith K Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, et al.

</span>
<span class="ltx_bibblock">Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Radiology: Artificial Intelligence</em>, 1(1):e180041, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shmelkov et al. (2018)</span>
<span class="ltx_bibblock">
Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari.

</span>
<span class="ltx_bibblock">How good is my gan?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">ECCV</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Steven G. Langer &amp; George Shih (2019)</span>
<span class="ltx_bibblock">
CIIP Steven G. Langer, PhD and MS George Shih, MD.

</span>
<span class="ltx_bibblock">Siim-acr pneumothorax segmentation.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2023a)</span>
<span class="ltx_bibblock">
Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola.

</span>
<span class="ltx_bibblock">Learning vision from models rivals learning vision from data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2312.17742</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2023b)</span>
<span class="ltx_bibblock">
Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan.

</span>
<span class="ltx_bibblock">Stablerep: Synthetic images from text-to-image models make strong visual representation learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">NeurIPS</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiu et al. (2022a)</span>
<span class="ltx_bibblock">
Ekin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Nature Biomedical Engineering</em>, pp.  1–8, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiu et al. (2022b)</span>
<span class="ltx_bibblock">
Ekin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">Expert-level detection of pathologies from unannotated chest x-ray images via self-supervised learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Nature Biomedical Engineering</em>, 6(12):1399–1406, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Varol et al. (2017)</span>
<span class="ltx_bibblock">
Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black, Ivan Laptev, and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Learning from synthetic humans.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">CVPR</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al. (2024)</span>
<span class="ltx_bibblock">
Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-Casas, and Rossella Arcucci.

</span>
<span class="ltx_bibblock">Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing bias.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, and Lequan Yu.

</span>
<span class="ltx_bibblock">Multi-granularity cross-modal alignment for generalized medical visual representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2210.06044</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Linda Wang, Zhong Qiu Lin, and Alexander Wong.

</span>
<span class="ltx_bibblock">Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest x-ray images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Scientific reports</em>, 10(1):1–12, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2017)</span>
<span class="ltx_bibblock">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.

</span>
<span class="ltx_bibblock">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pp.  2097–2106, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Medklip: Medical knowledge enhanced language-image pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">medRxiv</em>, pp.  2023–01, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Linshan Wu, Jiaxin Zhuang, Xuefeng Ni, and Hao Chen.

</span>
<span class="ltx_bibblock">Freetumor: Advance tumor segmentation via large-scale tumor synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2406.01264</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2024)</span>
<span class="ltx_bibblock">
Yutong Xie, Qi Chen, Sinuo Wang, Minh-Son To, Iris Lee, Ee Win Khoo, Kerolos Hendy, Daniel Koh, Yong Xia, and Qi Wu.

</span>
<span class="ltx_bibblock">Pairaug: What can augmented image-text pairs do for radiology?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2404.04960</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023a)</span>
<span class="ltx_bibblock">
Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer.

</span>
<span class="ltx_bibblock">Demystifying clip data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2309.16671</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023b)</span>
<span class="ltx_bibblock">
Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer.

</span>
<span class="ltx_bibblock">Demystifying clip data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2309.16671</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2020)</span>
<span class="ltx_bibblock">
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.

</span>
<span class="ltx_bibblock">Generative data augmentation for commonsense reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">EMNLP</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2021)</span>
<span class="ltx_bibblock">
Qingsong Yao, Li Xiao, Peihang Liu, and S Kevin Zhou.

</span>
<span class="ltx_bibblock">Label-free segmentation of covid-19 lesions in lung ct.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">IEEE transactions on medical imaging</em>, 40(10):2808–2819, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Diversify, don’t fine-tune: Scaling up visual recognition training with synthetic images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2312.02253</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2024)</span>
<span class="ltx_bibblock">
Jianhao Yuan, Jie Zhang, Shuyang Sun, Philip Torr, and Bo Zhao.

</span>
<span class="ltx_bibblock">Real-fake: Effective training data synthesis through distribution matching.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">ICLR</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Long-clip: Unlocking the long-text capability of clip.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">arXiv preprint arXiv:2403.15378</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Weidi Xie, and Yanfeng Wang.

</span>
<span class="ltx_bibblock">Knowledge-enhanced visual-language pre-training on chest radiology images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Nature Communications</em>, 14(1):4542, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz.

</span>
<span class="ltx_bibblock">Contrastive learning of medical visual representations from paired images and text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2010.00747</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Ratescore: A metric for radiology report generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2406.16845</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(75)</span>
<span class="ltx_bibblock">
Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, and Yizhou Yu.

</span>
<span class="ltx_bibblock">Advancing radiograph representation learning with masked record modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Hong-Yu Zhou, Subathra Adithan, Julián Nicolás Acosta, Eric J Topol, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">A generalist learner for multifaceted medical image interpretation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">arXiv preprint arXiv:2405.07988</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Yongchao Zhou, Hshmat Sahak, and Jimmy Ba.

</span>
<span class="ltx_bibblock">Training on thin air: Improve image classification with generated data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2305.15316</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Downstream Tasks Configuration</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dataset Details</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">In this section, we provide details on all datasets used. The dataset splits are publicly accessible at<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://github.com/HieuPhan33/CVPR2024_MAVL/tree/main/data</span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p2.1.1">ChestX-ray14</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib61" title="">2017</a>)</cite> includes 112,120 frontal X-rays from 30,805 patients, labeled for 14 diseases. We use the official split and partition it into 80%/10%/10% for train/validation/test.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p3.1.1">PadChest</span> <cite class="ltx_cite ltx_citemacro_citep">(Bustos et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib6" title="">2020</a>)</cite> includes 160,868 X-rays from 67,000 patients, annotated with over 150 findings. As in <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>, three subsets are built based on PadChest: 14 common diseases as <span class="ltx_text ltx_font_bold" id="A1.SS1.p3.1.2">PadChest-seen</span>, rare diseases from the NORD database<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>https://rarediseases.org/rare-diseases/</span></span></span> as <span class="ltx_text ltx_font_bold" id="A1.SS1.p3.1.3">PadChest-rare</span>, and the remaining diseases as <span class="ltx_text ltx_font_bold" id="A1.SS1.p3.1.4">PadChest-unseen</span>. We use the official split provided by <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p4.1.1">RSNA</span> <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite> contains over 260,000 frontal X-rays annotated with pneumonia masks. We divide it into training (60%), validation (20%), and test (20%) sets for segmentation and classification tasks <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib62" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p5.1.1">CheXpert</span> <cite class="ltx_cite ltx_citemacro_citep">(Irvin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib18" title="">2019</a>)</cite> contains 224,316 chest X-rays from 65,240 patients at Stanford Hospital, with an official validation set of 200 studies and a test set of 500 studies, both annotated by board-certified radiologists. Our evaluation on the five observations in the official test set follows protocols from earlier studies <cite class="ltx_cite ltx_citemacro_citep">(Tiu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib56" title="">2022b</a>; Irvin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib18" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p6">
<p class="ltx_p" id="A1.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p6.1.1">SIIM</span> <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite> consists of over 12,000 frontal X-rays annotated with pneumothorax masks, split into training (60%), validation (20%), and test (20%) sets.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p7">
<p class="ltx_p" id="A1.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p7.1.1">COVIDx CXR-2</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib60" title="">2020</a>)</cite> includes 29,986 X-rays from 16,648 COVID-19 patients, divided into training (70%), validation (20%), and test (10%) <cite class="ltx_cite ltx_citemacro_citep">(Pavlova et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib35" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS1.p8">
<p class="ltx_p" id="A1.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p8.1.1">COVID Rural</span> <cite class="ltx_cite ltx_citemacro_citep">(Desai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib13" title="">2020</a>)</cite> contains over 200 X-rays with segmentation masks, divided into training (60%), validation (20%), and test (20%).</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Implementation Details</h3>
<div class="ltx_para ltx_noindent" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.5"><span class="ltx_text ltx_font_bold" id="A1.SS2.p1.5.1">Zero-shot Image Classification.</span>
The CXR images undergo a two-step preprocessing: resizing to <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1.1"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><mn id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml">256</mn><mo id="A1.SS2.p1.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS2.p1.1.m1.1.1.3" xref="A1.SS2.p1.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><times id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1.1"></times><cn id="A1.SS2.p1.1.m1.1.1.2.cmml" type="integer" xref="A1.SS2.p1.1.m1.1.1.2">256</cn><cn id="A1.SS2.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.p1.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.1.m1.1d">256 × 256</annotation></semantics></math>, followed by center cropping to <math alttext="224\times 224" class="ltx_Math" display="inline" id="A1.SS2.p1.2.m2.1"><semantics id="A1.SS2.p1.2.m2.1a"><mrow id="A1.SS2.p1.2.m2.1.1" xref="A1.SS2.p1.2.m2.1.1.cmml"><mn id="A1.SS2.p1.2.m2.1.1.2" xref="A1.SS2.p1.2.m2.1.1.2.cmml">224</mn><mo id="A1.SS2.p1.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="A1.SS2.p1.2.m2.1.1.3" xref="A1.SS2.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.m2.1b"><apply id="A1.SS2.p1.2.m2.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1"><times id="A1.SS2.p1.2.m2.1.1.1.cmml" xref="A1.SS2.p1.2.m2.1.1.1"></times><cn id="A1.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.p1.2.m2.1.1.2">224</cn><cn id="A1.SS2.p1.2.m2.1.1.3.cmml" type="integer" xref="A1.SS2.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.m2.1c">224\times 224</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.2.m2.1d">224 × 224</annotation></semantics></math>. As per <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite>, pixel values are normalized to <math alttext="[0,1]" class="ltx_Math" display="inline" id="A1.SS2.p1.3.m3.2"><semantics id="A1.SS2.p1.3.m3.2a"><mrow id="A1.SS2.p1.3.m3.2.3.2" xref="A1.SS2.p1.3.m3.2.3.1.cmml"><mo id="A1.SS2.p1.3.m3.2.3.2.1" stretchy="false" xref="A1.SS2.p1.3.m3.2.3.1.cmml">[</mo><mn id="A1.SS2.p1.3.m3.1.1" xref="A1.SS2.p1.3.m3.1.1.cmml">0</mn><mo id="A1.SS2.p1.3.m3.2.3.2.2" xref="A1.SS2.p1.3.m3.2.3.1.cmml">,</mo><mn id="A1.SS2.p1.3.m3.2.2" xref="A1.SS2.p1.3.m3.2.2.cmml">1</mn><mo id="A1.SS2.p1.3.m3.2.3.2.3" stretchy="false" xref="A1.SS2.p1.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.3.m3.2b"><interval closure="closed" id="A1.SS2.p1.3.m3.2.3.1.cmml" xref="A1.SS2.p1.3.m3.2.3.2"><cn id="A1.SS2.p1.3.m3.1.1.cmml" type="integer" xref="A1.SS2.p1.3.m3.1.1">0</cn><cn id="A1.SS2.p1.3.m3.2.2.cmml" type="integer" xref="A1.SS2.p1.3.m3.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.3.m3.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.3.m3.2d">[ 0 , 1 ]</annotation></semantics></math>. The processed image is passed through a visual encoder and projector to generate the image embedding <math alttext="\hat{\mathbf{v}}_{i}" class="ltx_Math" display="inline" id="A1.SS2.p1.4.m4.1"><semantics id="A1.SS2.p1.4.m4.1a"><msub id="A1.SS2.p1.4.m4.1.1" xref="A1.SS2.p1.4.m4.1.1.cmml"><mover accent="true" id="A1.SS2.p1.4.m4.1.1.2" xref="A1.SS2.p1.4.m4.1.1.2.cmml"><mi id="A1.SS2.p1.4.m4.1.1.2.2" xref="A1.SS2.p1.4.m4.1.1.2.2.cmml">𝐯</mi><mo id="A1.SS2.p1.4.m4.1.1.2.1" xref="A1.SS2.p1.4.m4.1.1.2.1.cmml">^</mo></mover><mi id="A1.SS2.p1.4.m4.1.1.3" xref="A1.SS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.4.m4.1b"><apply id="A1.SS2.p1.4.m4.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.4.m4.1.1.1.cmml" xref="A1.SS2.p1.4.m4.1.1">subscript</csymbol><apply id="A1.SS2.p1.4.m4.1.1.2.cmml" xref="A1.SS2.p1.4.m4.1.1.2"><ci id="A1.SS2.p1.4.m4.1.1.2.1.cmml" xref="A1.SS2.p1.4.m4.1.1.2.1">^</ci><ci id="A1.SS2.p1.4.m4.1.1.2.2.cmml" xref="A1.SS2.p1.4.m4.1.1.2.2">𝐯</ci></apply><ci id="A1.SS2.p1.4.m4.1.1.3.cmml" xref="A1.SS2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.4.m4.1c">\hat{\mathbf{v}}_{i}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.4.m4.1d">over^ start_ARG bold_v end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Simultaneously, the text prompts are processed through a text encoder to obtain text embeddings <math alttext="\hat{\mathbf{l}}_{i}" class="ltx_Math" display="inline" id="A1.SS2.p1.5.m5.1"><semantics id="A1.SS2.p1.5.m5.1a"><msub id="A1.SS2.p1.5.m5.1.1" xref="A1.SS2.p1.5.m5.1.1.cmml"><mover accent="true" id="A1.SS2.p1.5.m5.1.1.2" xref="A1.SS2.p1.5.m5.1.1.2.cmml"><mi id="A1.SS2.p1.5.m5.1.1.2.2" xref="A1.SS2.p1.5.m5.1.1.2.2.cmml">𝐥</mi><mo id="A1.SS2.p1.5.m5.1.1.2.1" xref="A1.SS2.p1.5.m5.1.1.2.1.cmml">^</mo></mover><mi id="A1.SS2.p1.5.m5.1.1.3" xref="A1.SS2.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.5.m5.1b"><apply id="A1.SS2.p1.5.m5.1.1.cmml" xref="A1.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A1.SS2.p1.5.m5.1.1.1.cmml" xref="A1.SS2.p1.5.m5.1.1">subscript</csymbol><apply id="A1.SS2.p1.5.m5.1.1.2.cmml" xref="A1.SS2.p1.5.m5.1.1.2"><ci id="A1.SS2.p1.5.m5.1.1.2.1.cmml" xref="A1.SS2.p1.5.m5.1.1.2.1">^</ci><ci id="A1.SS2.p1.5.m5.1.1.2.2.cmml" xref="A1.SS2.p1.5.m5.1.1.2.2">𝐥</ci></apply><ci id="A1.SS2.p1.5.m5.1.1.3.cmml" xref="A1.SS2.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.5.m5.1c">\hat{\mathbf{l}}_{i}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p1.5.m5.1d">over^ start_ARG bold_l end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Classification is based on cosine similarity between image and text embeddings. If the similarity between the image embedding and the positive prompt (e.g., <span class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" id="A1.SS2.p1.5.2">disease</span>) is higher than that with the negative prompt (e.g., <span class="ltx_text ltx_font_italic" id="A1.SS2.p1.5.3">No <span class="ltx_text ltx_framed ltx_framed_underline" id="A1.SS2.p1.5.3.1">disease</span></span>), the classification is positive, and vice versa. The prompt design follows <cite class="ltx_cite ltx_citemacro_citep">(Tiu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib55" title="">2022a</a>)</cite> for both ConVIRT and GLoRIA.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.1.1">Zero-shot Visual Grounding.</span>
For this task, we follow the BioViL pipeline as described in <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>, since ConVIRT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib73" title="">2020</a>)</cite> and GLoRIA <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite> do not provide code for visual grounding. This pixel-level classification task relies on the similarity between text embeddings and the dense visual feature map from the final convolutional layer. The cosine similarity generates a similarity map, resized to match the image, and used as segmentation results for grounding evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A1.SS2.p3.1.1">Medical Image Fine-tuned Classification.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p4">
<p class="ltx_p" id="A1.SS2.p4.4">For fine-tuning, we follow the experimental setup from <cite class="ltx_cite ltx_citemacro_citep">(Phan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib38" title="">2024b</a>)</cite>, updating both the visual encoder and linear layer. Images are resized to <math alttext="256\times 256" class="ltx_Math" display="inline" id="A1.SS2.p4.1.m1.1"><semantics id="A1.SS2.p4.1.m1.1a"><mrow id="A1.SS2.p4.1.m1.1.1" xref="A1.SS2.p4.1.m1.1.1.cmml"><mn id="A1.SS2.p4.1.m1.1.1.2" xref="A1.SS2.p4.1.m1.1.1.2.cmml">256</mn><mo id="A1.SS2.p4.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p4.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS2.p4.1.m1.1.1.3" xref="A1.SS2.p4.1.m1.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p4.1.m1.1b"><apply id="A1.SS2.p4.1.m1.1.1.cmml" xref="A1.SS2.p4.1.m1.1.1"><times id="A1.SS2.p4.1.m1.1.1.1.cmml" xref="A1.SS2.p4.1.m1.1.1.1"></times><cn id="A1.SS2.p4.1.m1.1.1.2.cmml" type="integer" xref="A1.SS2.p4.1.m1.1.1.2">256</cn><cn id="A1.SS2.p4.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.p4.1.m1.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p4.1.m1.1c">256\times 256</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p4.1.m1.1d">256 × 256</annotation></semantics></math>, and data augmentation is applied as recommended in <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib72" title="">2023</a>)</cite>. We use the AdamW optimizer with a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="A1.SS2.p4.2.m2.1"><semantics id="A1.SS2.p4.2.m2.1a"><mrow id="A1.SS2.p4.2.m2.1.1" xref="A1.SS2.p4.2.m2.1.1.cmml"><mn id="A1.SS2.p4.2.m2.1.1.2" xref="A1.SS2.p4.2.m2.1.1.2.cmml">1</mn><mo id="A1.SS2.p4.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.p4.2.m2.1.1.1.cmml">×</mo><msup id="A1.SS2.p4.2.m2.1.1.3" xref="A1.SS2.p4.2.m2.1.1.3.cmml"><mn id="A1.SS2.p4.2.m2.1.1.3.2" xref="A1.SS2.p4.2.m2.1.1.3.2.cmml">10</mn><mrow id="A1.SS2.p4.2.m2.1.1.3.3" xref="A1.SS2.p4.2.m2.1.1.3.3.cmml"><mo id="A1.SS2.p4.2.m2.1.1.3.3a" xref="A1.SS2.p4.2.m2.1.1.3.3.cmml">−</mo><mn id="A1.SS2.p4.2.m2.1.1.3.3.2" xref="A1.SS2.p4.2.m2.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p4.2.m2.1b"><apply id="A1.SS2.p4.2.m2.1.1.cmml" xref="A1.SS2.p4.2.m2.1.1"><times id="A1.SS2.p4.2.m2.1.1.1.cmml" xref="A1.SS2.p4.2.m2.1.1.1"></times><cn id="A1.SS2.p4.2.m2.1.1.2.cmml" type="integer" xref="A1.SS2.p4.2.m2.1.1.2">1</cn><apply id="A1.SS2.p4.2.m2.1.1.3.cmml" xref="A1.SS2.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="A1.SS2.p4.2.m2.1.1.3.1.cmml" xref="A1.SS2.p4.2.m2.1.1.3">superscript</csymbol><cn id="A1.SS2.p4.2.m2.1.1.3.2.cmml" type="integer" xref="A1.SS2.p4.2.m2.1.1.3.2">10</cn><apply id="A1.SS2.p4.2.m2.1.1.3.3.cmml" xref="A1.SS2.p4.2.m2.1.1.3.3"><minus id="A1.SS2.p4.2.m2.1.1.3.3.1.cmml" xref="A1.SS2.p4.2.m2.1.1.3.3"></minus><cn id="A1.SS2.p4.2.m2.1.1.3.3.2.cmml" type="integer" xref="A1.SS2.p4.2.m2.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p4.2.m2.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p4.2.m2.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, batch size of 64, for 50 epochs on a single A100 GPU. Early stopping is applied, with a learning rate of 5e-4 and batch size of 8. AdamW is configured with <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A1.SS2.p4.3.m3.1"><semantics id="A1.SS2.p4.3.m3.1a"><mrow id="A1.SS2.p4.3.m3.1.1" xref="A1.SS2.p4.3.m3.1.1.cmml"><msub id="A1.SS2.p4.3.m3.1.1.2" xref="A1.SS2.p4.3.m3.1.1.2.cmml"><mi id="A1.SS2.p4.3.m3.1.1.2.2" xref="A1.SS2.p4.3.m3.1.1.2.2.cmml">β</mi><mn id="A1.SS2.p4.3.m3.1.1.2.3" xref="A1.SS2.p4.3.m3.1.1.2.3.cmml">1</mn></msub><mo id="A1.SS2.p4.3.m3.1.1.1" xref="A1.SS2.p4.3.m3.1.1.1.cmml">=</mo><mn id="A1.SS2.p4.3.m3.1.1.3" xref="A1.SS2.p4.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p4.3.m3.1b"><apply id="A1.SS2.p4.3.m3.1.1.cmml" xref="A1.SS2.p4.3.m3.1.1"><eq id="A1.SS2.p4.3.m3.1.1.1.cmml" xref="A1.SS2.p4.3.m3.1.1.1"></eq><apply id="A1.SS2.p4.3.m3.1.1.2.cmml" xref="A1.SS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p4.3.m3.1.1.2.1.cmml" xref="A1.SS2.p4.3.m3.1.1.2">subscript</csymbol><ci id="A1.SS2.p4.3.m3.1.1.2.2.cmml" xref="A1.SS2.p4.3.m3.1.1.2.2">𝛽</ci><cn id="A1.SS2.p4.3.m3.1.1.2.3.cmml" type="integer" xref="A1.SS2.p4.3.m3.1.1.2.3">1</cn></apply><cn id="A1.SS2.p4.3.m3.1.1.3.cmml" type="float" xref="A1.SS2.p4.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p4.3.m3.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p4.3.m3.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="A1.SS2.p4.4.m4.1"><semantics id="A1.SS2.p4.4.m4.1a"><mrow id="A1.SS2.p4.4.m4.1.1" xref="A1.SS2.p4.4.m4.1.1.cmml"><msub id="A1.SS2.p4.4.m4.1.1.2" xref="A1.SS2.p4.4.m4.1.1.2.cmml"><mi id="A1.SS2.p4.4.m4.1.1.2.2" xref="A1.SS2.p4.4.m4.1.1.2.2.cmml">β</mi><mn id="A1.SS2.p4.4.m4.1.1.2.3" xref="A1.SS2.p4.4.m4.1.1.2.3.cmml">2</mn></msub><mo id="A1.SS2.p4.4.m4.1.1.1" xref="A1.SS2.p4.4.m4.1.1.1.cmml">=</mo><mn id="A1.SS2.p4.4.m4.1.1.3" xref="A1.SS2.p4.4.m4.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p4.4.m4.1b"><apply id="A1.SS2.p4.4.m4.1.1.cmml" xref="A1.SS2.p4.4.m4.1.1"><eq id="A1.SS2.p4.4.m4.1.1.1.cmml" xref="A1.SS2.p4.4.m4.1.1.1"></eq><apply id="A1.SS2.p4.4.m4.1.1.2.cmml" xref="A1.SS2.p4.4.m4.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p4.4.m4.1.1.2.1.cmml" xref="A1.SS2.p4.4.m4.1.1.2">subscript</csymbol><ci id="A1.SS2.p4.4.m4.1.1.2.2.cmml" xref="A1.SS2.p4.4.m4.1.1.2.2">𝛽</ci><cn id="A1.SS2.p4.4.m4.1.1.2.3.cmml" type="integer" xref="A1.SS2.p4.4.m4.1.1.2.3">2</cn></apply><cn id="A1.SS2.p4.4.m4.1.1.3.cmml" type="float" xref="A1.SS2.p4.4.m4.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p4.4.m4.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p4.4.m4.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>, and weight decay of 1e-6.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p5">
<p class="ltx_p" id="A1.SS2.p5.2"><span class="ltx_text ltx_font_bold" id="A1.SS2.p5.2.1">Medical Image Fine-tuned Segmentation.</span>
For segmentation tasks on the RSNA <cite class="ltx_cite ltx_citemacro_citep">(Shih et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib50" title="">2019</a>)</cite>, SIIM <cite class="ltx_cite ltx_citemacro_citep">(Steven G. Langer &amp; George Shih, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib52" title="">2019</a>)</cite>, and Covid-19 Rural <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib60" title="">2020</a>)</cite> datasets, we fine-tune both the pre-trained vision encoder and decoder. Training is performed with early stopping at 50 epochs, using a learning rate of 2e-4 and weight decay of 0.05. AdamW is the optimizer, with <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A1.SS2.p5.1.m1.1"><semantics id="A1.SS2.p5.1.m1.1a"><mrow id="A1.SS2.p5.1.m1.1.1" xref="A1.SS2.p5.1.m1.1.1.cmml"><msub id="A1.SS2.p5.1.m1.1.1.2" xref="A1.SS2.p5.1.m1.1.1.2.cmml"><mi id="A1.SS2.p5.1.m1.1.1.2.2" xref="A1.SS2.p5.1.m1.1.1.2.2.cmml">β</mi><mn id="A1.SS2.p5.1.m1.1.1.2.3" xref="A1.SS2.p5.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="A1.SS2.p5.1.m1.1.1.1" xref="A1.SS2.p5.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS2.p5.1.m1.1.1.3" xref="A1.SS2.p5.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p5.1.m1.1b"><apply id="A1.SS2.p5.1.m1.1.1.cmml" xref="A1.SS2.p5.1.m1.1.1"><eq id="A1.SS2.p5.1.m1.1.1.1.cmml" xref="A1.SS2.p5.1.m1.1.1.1"></eq><apply id="A1.SS2.p5.1.m1.1.1.2.cmml" xref="A1.SS2.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p5.1.m1.1.1.2.1.cmml" xref="A1.SS2.p5.1.m1.1.1.2">subscript</csymbol><ci id="A1.SS2.p5.1.m1.1.1.2.2.cmml" xref="A1.SS2.p5.1.m1.1.1.2.2">𝛽</ci><cn id="A1.SS2.p5.1.m1.1.1.2.3.cmml" type="integer" xref="A1.SS2.p5.1.m1.1.1.2.3">1</cn></apply><cn id="A1.SS2.p5.1.m1.1.1.3.cmml" type="float" xref="A1.SS2.p5.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p5.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p5.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="A1.SS2.p5.2.m2.1"><semantics id="A1.SS2.p5.2.m2.1a"><mrow id="A1.SS2.p5.2.m2.1.1" xref="A1.SS2.p5.2.m2.1.1.cmml"><msub id="A1.SS2.p5.2.m2.1.1.2" xref="A1.SS2.p5.2.m2.1.1.2.cmml"><mi id="A1.SS2.p5.2.m2.1.1.2.2" xref="A1.SS2.p5.2.m2.1.1.2.2.cmml">β</mi><mn id="A1.SS2.p5.2.m2.1.1.2.3" xref="A1.SS2.p5.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="A1.SS2.p5.2.m2.1.1.1" xref="A1.SS2.p5.2.m2.1.1.1.cmml">=</mo><mn id="A1.SS2.p5.2.m2.1.1.3" xref="A1.SS2.p5.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p5.2.m2.1b"><apply id="A1.SS2.p5.2.m2.1.1.cmml" xref="A1.SS2.p5.2.m2.1.1"><eq id="A1.SS2.p5.2.m2.1.1.1.cmml" xref="A1.SS2.p5.2.m2.1.1.1"></eq><apply id="A1.SS2.p5.2.m2.1.1.2.cmml" xref="A1.SS2.p5.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.p5.2.m2.1.1.2.1.cmml" xref="A1.SS2.p5.2.m2.1.1.2">subscript</csymbol><ci id="A1.SS2.p5.2.m2.1.1.2.2.cmml" xref="A1.SS2.p5.2.m2.1.1.2.2">𝛽</ci><cn id="A1.SS2.p5.2.m2.1.1.2.3.cmml" type="integer" xref="A1.SS2.p5.2.m2.1.1.2.3">2</cn></apply><cn id="A1.SS2.p5.2.m2.1.1.3.cmml" type="float" xref="A1.SS2.p5.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p5.2.m2.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p5.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>. Batch sizes are 8 for SIIM and 16 for RSNA. All configurations follow the protocol from <cite class="ltx_cite ltx_citemacro_citep">(Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib17" title="">2021</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Extra Visualization</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">Distribution of Synthetic and Real Data.</span>
We illustrate the distribution of synthetic and real data in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F5" title="Figure 5 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">5</span></a>. For visualization, we use RAD-DINO <cite class="ltx_cite ltx_citemacro_citep">(Pérez-García et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib36" title="">2024</a>)</cite> to extract image features and Med-CPT <cite class="ltx_cite ltx_citemacro_citep">(Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#bib.bib20" title="">2023</a>)</cite> to extract report features. We then apply Principal component analysis (PCA) to reduce the feature dimensions and visualize the first principal component. As shown in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F5" title="Figure 5 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">5</span></a>, the synthetic data covers a broader range than the real data, indicating greater diversity. In contrast, the real data shows a more concentrated distribution, which may limit the generalizability of MedVLP models.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.1"><span class="ltx_text ltx_font_bold" id="A2.p2.1.1">Pipeline of Synthetic Report Generation.</span> The pipeline for generating synthetic reports using LLMs and balanced sampled clinical entities is illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F6" title="Figure 6 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p3">
<p class="ltx_p" id="A2.p3.1"><span class="ltx_text ltx_font_bold" id="A2.p3.1.1">Entities Distribution.</span> We visualize the distribution of each type of entity in the MIMIC-CXR dataset. Due to space constraints, only the top 200 most frequent entities are shown, revealing a clear long-tailed distribution in Fig <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F7" title="Figure 7 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F11" title="Figure 11 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">11</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F9" title="Figure 9 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">9</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F8" title="Figure 8 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">8</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.13523v1#A2.F10" title="Figure 10 ‣ Appendix B Extra Visualization ‣ Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure class="ltx_figure" id="A2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="A2.F5.g1" src="extracted/5934507/dist.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distribution of Synthetic and Real Data.
(a): Comparison of the first principal component distribution of features extracted from RAD-DINO for synthetic and real images.
(b): Comparison of the first principal component distribution of features extracted from Med-CPT for synthetic and real reports.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="547" id="A2.F6.g1" src="extracted/5934507/reportgen.png" width="180"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Pipeline for generating synthetic reports. The process begins by generating the ‘FINDINGS’ section, followed by summarizing it into the ‘IMPRESSION’ section. Both sections are checked to ensure they contain the specified entities; if not, the generation process is repeated. The final dataset includes 200,000 synthetic reports, each containing both ‘FINDINGS’ and ‘IMPRESSION’ sections.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A2.F7.g1" src="extracted/5934507/ABNORMALITY.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Top 200 most frequent abnormality entities.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A2.F8.g1" src="extracted/5934507/NON-ABNORMALITY.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Top 200 most frequent non-abnormality entities.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A2.F9.g1" src="extracted/5934507/DISEASE.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Top 200 most frequent disease entities.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A2.F10.g1" src="extracted/5934507/NON-DISEASE.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Top 200 most frequent non-disease entities.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A2.F11.g1" src="extracted/5934507/ANATOMY.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Top 200 most frequent anatomy entities.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct 17 13:05:31 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
