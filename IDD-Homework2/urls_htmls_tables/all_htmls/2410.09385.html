<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models</title>
<!--Generated on Sat Oct 12 05:59:48 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.09385v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S1" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S2" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS1" title="In 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Background: State Space Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS2" title="In 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Mamba4Cast Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS3" title="In 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic Data Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS1" title="In 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS1.SSS0.Px1" title="In 4.1 Training Details â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title">Architectural choices</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS1.SSS0.Px2" title="In 4.1 Training Details â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title">Training setup</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS2" title="In 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Performance Comparison with Baseline Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS3" title="In 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Qualitative Analysis on Synthetic and Real Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S5" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Synthetic Data Generation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS1" title="In Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>ForecastPFN Prior</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS1.SSS0.Px1" title="In A.1 ForecastPFN Prior â€£ Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title">Trend</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS1.SSS0.Px2" title="In A.1 ForecastPFN Prior â€£ Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title">Seasonal</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS2" title="In Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>GP priors</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS2.SSS0.Px1" title="In A.2 GP priors â€£ Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title">GP model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS2.SSS0.Px2" title="In A.2 GP priors â€£ Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title">Kernels</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS3" title="In Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Signal level noises</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A2" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Dataset Preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A3" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Benchmark Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A4" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Ablation studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5" title="In Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Evaluations on real datasets</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5.SS1" title="In Appendix E Evaluations on real datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Evaluation metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5.SS2" title="In Appendix E Evaluations on real datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.2 </span>Qualitative analysis</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sathya Kamesh Bhethanabhotla 
<br class="ltx_break"/>University of Freiburg
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">bhethans@tf.uni-freiburg.de</span>
<br class="ltx_break"/>&amp;Omar Swelam<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break"/>University of Freiburg
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">swelamo@tf.uni-freiburg.de</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\AND</span>Julien Siems 
<br class="ltx_break"/>University of Freiburg
<br class="ltx_break"/>&amp;David Salinas 
<br class="ltx_break"/>University of Freiburg
<br class="ltx_break"/>&amp;Frank Hutter 
<br class="ltx_break"/>ELLIS Institute TÃ¼bingen &amp;
<br class="ltx_break"/>University of Freiburg
<br class="ltx_break"/>
</span><span class="ltx_author_notes">Both authors contributed equally to this work.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">This paper introduces Mamba4Cast, a zero-shot foundation model for time series forecasting. Based on the Mamba architecture and inspired by Prior-data Fitted Networks (PFNs), Mamba4Cast generalizes robustly across diverse time series tasks without the need for dataset specific fine-tuning. Mamba4Castâ€™s key innovation lies in its ability to achieve strong zero-shot performance on <span class="ltx_text" id="id4.id1.1">real-world</span> datasets while having much lower inference times than time series foundation models based on the transformer architecture. Trained solely on synthetic data, the model generates forecasts for entire horizons in a single pass, outpacing traditional auto-regressive approaches. Our experiments show that Mamba4Cast performs competitively against other state-of-the-art foundation models in various data sets while scaling significantly better with the prediction length. The source code can be accessed at Â <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/automl/Mamba4Cast" title="">https://github.com/automl/Mamba4Cast</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Time series forecasting is a critical task in numerous domains, from finance <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib12" title="">2023</a>)</cite> to healthcare <cite class="ltx_cite ltx_citemacro_citep">(Jung etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib16" title="">2021</a>)</cite>, and has been approached through various deep learning methods in recent years <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib4" title="">2023</a>; Liu &amp; Wang, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib19" title="">2024</a>)</cite>. Time-series data often exhibits complex temporal patterns, varying distributions with many confounding variables, and long-range dependencies, making it more challenging to model than other data paradigms.
Although the recent Cambrian explosion in deep learning, especially foundation models Â <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib34" title="">2023</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib40" title="">2022</a>)</cite>, can be attributed in part to the availability of large amounts of data for training, the same cannot be said about forecasting in some domains <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib35" title="">2023</a>; Sivaroopan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib33" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Forecasting models <cite class="ltx_cite ltx_citemacro_citep">(Salinas etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib31" title="">2020</a>; Zeng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib41" title="">2023</a>; Oreshkin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib25" title="">2019</a>)</cite> have traditionally employed non-zero-shot methods, which typically require customized training or fine-tuning for each specific task. While effective, this approach can be resource-intensive and time-consuming. Transformer-based time series foundation models <cite class="ltx_cite ltx_citemacro_citep">(Ansari etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib2" title="">2024</a>; Rasul etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib30" title="">2023</a>; Dooley etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib6" title="">2023</a>)</cite> have demonstrated significant potential to address these limitations. However, their application to long sequences during inference is constrained by their quadratic sample complexity.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In an effort to address both of these problems, we present Mamba4Cast, a time series foundation model based on two concepts: Prior-data Fitted Networks (PFNs)Â <cite class="ltx_cite ltx_citemacro_citep">(Hollmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib13" title="">2023</a>; Dooley etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib6" title="">2023</a>)</cite> and the MambaÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu &amp; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib11" title="">2024</a>; Dao &amp; Gu, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib5" title="">2024</a>)</cite> architecture. Our contributions are twofold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce Mamba4Cast, a Mamba-based zero-shot forecasting model trained exclusively on synthetic data. It achieves
competitive performance compared to
other state-of-the-art zero-shot models, such as ChronosÂ <cite class="ltx_cite ltx_citemacro_citep">(Ansari etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib2" title="">2024</a>)</cite>, while leveraging Mambaâ€™s architecture for efficient scaling over longer context lengths.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We demonstrate that Mamba4Cast provides accurate point predictions over the entire forecast horizon in a single forward pass, achieving inference speeds several times faster than autoregressive counterparts.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="150" id="S1.F1.g1" src="x1.png" width="478"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic overview of the Mamba4Cast architecture.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Time series forecasting with Transformers</span> â€‚In the last few years, transformer-based models have significantly improved the state of the art in time series forecasting. Works like the InformerÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib42" title="">2021</a>)</cite> and PatchTSTÂ <cite class="ltx_cite ltx_citemacro_citep">(Nie etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib24" title="">2023</a>)</cite> address the issue of long-term forecasting with transformers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Zero-shot forecasting</span> â€‚There have also been several advancements in zero-shot time series forecasting <cite class="ltx_cite ltx_citemacro_citep">(Woo etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib37" title="">2024</a>; Gruver etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib10" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib7" title="">2024</a>)</cite> proposed UNITS, a unified multi-task model handling various predictive and generative tasks, and <cite class="ltx_cite ltx_citemacro_citet">Oreshkin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib26" title="">2021</a>)</cite> proposed a meta-learning framework for zero-shot forecasting. These works highlight the growing trend towards more adaptable generalized time series models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Forecasting LLMs</span> In the wake of the recent success of Large Language Models (LLMs), a novel direction in time series analysis has emerged, focusing on adapting LLM-based architectures for forecasting. Studies such as <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib20" title="">2024</a>); Jin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib15" title="">2024</a>); Rasul etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib30" title="">2023</a>)</cite> have demonstrated the effectiveness of re-purposing LLMs for time series tasks. These approaches involve techniques to align time series data with the text-based input expected by LLMs, such as using text prototypes or encoding time series as strings of numerical digits. Notably, <cite class="ltx_cite ltx_citemacro_cite">Gruver etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib10" title="">2023</a>)</cite> showed that LLMs can perform zero-shot time series forecasting at levels comparable to or exceeding purpose-built models. These developments suggest that LLMs are promising candidates for general-purpose time series analysis, which can offer advantages in flexibility and performance in various forecasting tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p4">
<p class="ltx_p" id="S2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.p4.1.1">Training on Synthetic Data</span> â€‚While pre-training has enhanced the generalization capabilities of many models, their inductive biases often remain constrained to the distributions of their training corpus, potentially necessitating fine-tuning for niche applications. ForecastPFN <cite class="ltx_cite ltx_citemacro_citep">(Dooley etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib6" title="">2023</a>)</cite>, inspired by PFNs <cite class="ltx_cite ltx_citemacro_citep">(Hollmann etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib13" title="">2023</a>; MÃ¼ller etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib23" title="">2022</a>)</cite>, addressed this limitation by training on synthetic data, enabling zero-shot generalization to real-world time series. More recently, Chronos <cite class="ltx_cite ltx_citemacro_citep">(Ansari etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib2" title="">2024</a>)</cite> demonstrated state-of-the-art results by training on both synthetic and real-world time series, introducing a transformer-based foundation model that follows the next-token prediction paradigm of large language models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p5">
<p class="ltx_p" id="S2.p5.1"><span class="ltx_text ltx_font_bold" id="S2.p5.1.1">State Space Models</span> â€‚Despite the success of transformer-based methods, they face scalability challenges due to their quadratic complexity. In contrast, state-space models, such as Mamba <cite class="ltx_cite ltx_citemacro_citep">(Gu &amp; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib11" title="">2024</a>; Dao &amp; Gu, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib5" title="">2024</a>)</cite> or Linear AttentionÂ <cite class="ltx_cite ltx_citemacro_citep">(Katharopoulos etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib17" title="">2020</a>; Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib38" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib39" title="">b</a>)</cite>, have emerged as more efficient architectures, adapting state space models / linear RNNsÂ <cite class="ltx_cite ltx_citemacro_citep">(PÃ¶ppel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib29" title="">2024</a>)</cite> for sequence modeling with linear scaling properties. This efficiency has proven crucial for modeling dense, long-sequence data in vision and time series forecasting <cite class="ltx_cite ltx_citemacro_citep">(Behrouz etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib3" title="">2024</a>; Patro &amp; Agneeswaran, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib28" title="">2024</a>)</cite>. Subsequent works have further demonstrated Mambaâ€™s capacity in multivariate time-series forecasting; e.g., <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib36" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib18" title="">2024</a>)</cite> proposed bi-directional Mamba architectures to capture inter- and intra-series dependencies, with the latter introducing a forget gate for enhancing selective performance on longer ranges. With recent studies showcasing Mambaâ€™s in-context learning capabilities <cite class="ltx_cite ltx_citemacro_citep">(Grazzi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib9" title="">2024</a>; Park etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib27" title="">2024</a>)</cite>, Mamba4Cast attempts to utilize them towards a Mamba-based foundation model for zero-shot time series forecasting. We aim to address this unexplored avenue for univariate time series, by training over a diverse set of synthetic generation procedures that generalize to various real-life datasets.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Background: State Space Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">Mamba4Cast builds upon the Mamba2 state-space model introduced byÂ <cite class="ltx_cite ltx_citemacro_citet">Dao &amp; Gu (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib5" title="">2024</a>)</cite>. <span class="ltx_text" id="S3.SS1.p1.6.1">Mamba2</span> is a linear Recurrent Neural Network described by the following recurrence:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle h_{t}=A_{t}h_{t-1}+B_{t}x_{t};\qquad y_{t}=C_{t}h_{t}" class="ltx_Math" display="inline" id="S3.Ex1.m1.2"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2.2" xref="S3.Ex1.m1.2.2.3.cmml"><mrow id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml"><msub id="S3.Ex1.m1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.2.2" xref="S3.Ex1.m1.1.1.1.1.2.2.cmml">h</mi><mi id="S3.Ex1.m1.1.1.1.1.2.3" xref="S3.Ex1.m1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.3.cmml"><mrow id="S3.Ex1.m1.1.1.1.1.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.cmml"><msub id="S3.Ex1.m1.1.1.1.1.3.2.2" xref="S3.Ex1.m1.1.1.1.1.3.2.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.2.2" xref="S3.Ex1.m1.1.1.1.1.3.2.2.2.cmml">A</mi><mi id="S3.Ex1.m1.1.1.1.1.3.2.2.3" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.3.2.1" xref="S3.Ex1.m1.1.1.1.1.3.2.1.cmml">â¢</mo><msub id="S3.Ex1.m1.1.1.1.1.3.2.3" xref="S3.Ex1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.3.2.cmml">h</mi><mrow id="S3.Ex1.m1.1.1.1.1.3.2.3.3" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.2.3.3.2" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.2.cmml">t</mi><mo id="S3.Ex1.m1.1.1.1.1.3.2.3.3.1" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.1.cmml">âˆ’</mo><mn id="S3.Ex1.m1.1.1.1.1.3.2.3.3.3" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S3.Ex1.m1.1.1.1.1.3.1" xref="S3.Ex1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.Ex1.m1.1.1.1.1.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.cmml"><msub id="S3.Ex1.m1.1.1.1.1.3.3.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.2" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2.cmml">B</mi><mi id="S3.Ex1.m1.1.1.1.1.3.3.2.3" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.1.1.1.1.3.3.1" xref="S3.Ex1.m1.1.1.1.1.3.3.1.cmml">â¢</mo><msub id="S3.Ex1.m1.1.1.1.1.3.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1.3.3.3.2" xref="S3.Ex1.m1.1.1.1.1.3.3.3.2.cmml">x</mi><mi id="S3.Ex1.m1.1.1.1.1.3.3.3.3" xref="S3.Ex1.m1.1.1.1.1.3.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><mo id="S3.Ex1.m1.2.2.2.3" rspace="2.167em" xref="S3.Ex1.m1.2.2.3a.cmml">;</mo><mrow id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml"><msub id="S3.Ex1.m1.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.2.cmml">y</mi><mi id="S3.Ex1.m1.2.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.2.2.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.2.2.2.2.1" xref="S3.Ex1.m1.2.2.2.2.1.cmml">=</mo><mrow id="S3.Ex1.m1.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.2.3.cmml"><msub id="S3.Ex1.m1.2.2.2.2.3.2" xref="S3.Ex1.m1.2.2.2.2.3.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.3.2.2" xref="S3.Ex1.m1.2.2.2.2.3.2.2.cmml">C</mi><mi id="S3.Ex1.m1.2.2.2.2.3.2.3" xref="S3.Ex1.m1.2.2.2.2.3.2.3.cmml">t</mi></msub><mo id="S3.Ex1.m1.2.2.2.2.3.1" xref="S3.Ex1.m1.2.2.2.2.3.1.cmml">â¢</mo><msub id="S3.Ex1.m1.2.2.2.2.3.3" xref="S3.Ex1.m1.2.2.2.2.3.3.cmml"><mi id="S3.Ex1.m1.2.2.2.2.3.3.2" xref="S3.Ex1.m1.2.2.2.2.3.3.2.cmml">h</mi><mi id="S3.Ex1.m1.2.2.2.2.3.3.3" xref="S3.Ex1.m1.2.2.2.2.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.3a.cmml" xref="S3.Ex1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1"><eq id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"></eq><apply id="S3.Ex1.m1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.2.2">â„</ci><ci id="S3.Ex1.m1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.2.3">ğ‘¡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3"><plus id="S3.Ex1.m1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.1"></plus><apply id="S3.Ex1.m1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2"><times id="S3.Ex1.m1.1.1.1.1.3.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.1"></times><apply id="S3.Ex1.m1.1.1.1.1.3.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.2.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.2.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.2">ğ´</ci><ci id="S3.Ex1.m1.1.1.1.1.3.2.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.2.3">ğ‘¡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.3.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.2">â„</ci><apply id="S3.Ex1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3"><minus id="S3.Ex1.m1.1.1.1.1.3.2.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.1"></minus><ci id="S3.Ex1.m1.1.1.1.1.3.2.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.2">ğ‘¡</ci><cn id="S3.Ex1.m1.1.1.1.1.3.2.3.3.3.cmml" type="integer" xref="S3.Ex1.m1.1.1.1.1.3.2.3.3.3">1</cn></apply></apply></apply><apply id="S3.Ex1.m1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3"><times id="S3.Ex1.m1.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.1"></times><apply id="S3.Ex1.m1.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.2">ğµ</ci><ci id="S3.Ex1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.2.3">ğ‘¡</ci></apply><apply id="S3.Ex1.m1.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.3.2">ğ‘¥</ci><ci id="S3.Ex1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex1.m1.1.1.1.1.3.3.3.3">ğ‘¡</ci></apply></apply></apply></apply><apply id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2"><eq id="S3.Ex1.m1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.1"></eq><apply id="S3.Ex1.m1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2">ğ‘¦</ci><ci id="S3.Ex1.m1.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.3">ğ‘¡</ci></apply><apply id="S3.Ex1.m1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.3"><times id="S3.Ex1.m1.2.2.2.2.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.3.1"></times><apply id="S3.Ex1.m1.2.2.2.2.3.2.cmml" xref="S3.Ex1.m1.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.3.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.3.2">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.3.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.3.2.2">ğ¶</ci><ci id="S3.Ex1.m1.2.2.2.2.3.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.3.2.3">ğ‘¡</ci></apply><apply id="S3.Ex1.m1.2.2.2.2.3.3.cmml" xref="S3.Ex1.m1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.3.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.3.3">subscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.3.3.2.cmml" xref="S3.Ex1.m1.2.2.2.2.3.3.2">â„</ci><ci id="S3.Ex1.m1.2.2.2.2.3.3.3.cmml" xref="S3.Ex1.m1.2.2.2.2.3.3.3">ğ‘¡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\displaystyle h_{t}=A_{t}h_{t-1}+B_{t}x_{t};\qquad y_{t}=C_{t}h_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.2d">italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.5">where <math alttext="h_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">h</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">â„</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">h_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">x</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ğ‘¥</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">x_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="y_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ğ‘¦</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">y_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represent the hidden state, input token embedding, and output at index <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_t</annotation></semantics></math>, respectively.
In contrast to MambaÂ <cite class="ltx_cite ltx_citemacro_citep">(Gu &amp; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib11" title="">2024</a>)</cite>, which uses a fully parameterized diagonal state transition matrix <math alttext="A_{t}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><msub id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">A</mi><mi id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">ğ´</ci><ci id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">A_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, Mamba2 employs a scalar multiple of the identity matrix allowing for more efficient computation. The recurrence can be computed in chunks of linear attention blocks that can be pieced together later, leveraging tensor cores through matrix multiplication. This approach differs from Mambaâ€™s evaluation through an associative scan, which is also performed in parallel across the sequence but cannot leverage GPUs as well.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Mamba4Cast Architecture</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our proposed architecture, illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">1</span></a>, consists of four primary components: 
<br class="ltx_break"/>(1) <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">Pre-processing</span>: we scale the input series using a Min-Max Scaler and extract time features for positional embeddings. (2) <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">Embedding</span>: we embed the scaled input values and their temporal information using convolutions with different dilations, ensuring a large receptive field for the representation used by future layers. For more details about data pre-processing and embedding, refer to AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A2" title="Appendix B Dataset Preprocessing â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">B</span></a>. (3) <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.3">Encoder</span>: comprises of Mamba2 blocks with LayerNorm to avoid noisy learning signals followed by another dilated convolution layer.
(4) <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4">Decoder</span>: the final component is a linear projection layer that transforms the embedded token representations into point forecasts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We perform an ablation study, detailed in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A4" title="Appendix D Ablation studies â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">D</span></a>, investigating the role of convolutions, the efficacy of synthetic data generation methods, and the performance of alternative inference strategies.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Data Generation</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The quality and diversity of the data generation process are crucial for Mamba4Castâ€™s performance on real-world data, as it is trained exclusively on synthetic data. We employ two types of data-generating priors: ForecastPFN (FPFN) and Gaussian Process (GP) based.
The <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">FPFN prior</span>, based on <cite class="ltx_cite ltx_citemacro_citet">Dooley etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib6" title="">2023</a>)</cite>, decomposes a time series into trend, seasonality, and noise components reflecting real-life patterns.
The <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.2">GP prior</span>, inspired by <cite class="ltx_cite ltx_citemacro_citet">Ansari etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib2" title="">2024</a>)</cite>, complements the FPFN priors by accounting for patterns not captured therein. Each series is sampled from a GP with either a zero or a linear mean function and a composite kernel drawn from our <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.3">Kernel bank</span>. This allows for generating diverse and realistic synthetic time series that exhibit a wide range of temporal behaviors. Detailed descriptions of these data priors are provided in Appendix Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1" title="Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="179" id="S4.F2.1.g1" src="x2.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="497" id="S4.F2.2.g1" src="x3.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Performance and efficiency comparison of Mamba4Cast against baseline models. (<span class="ltx_text ltx_font_italic" id="S4.F2.5.1">left</span>) Distribution of MASE across 16 evaluation datasets (excluding Covid Deaths) for Mamba4Cast and five baseline models (ForecastPFN was much worse and is on a separate scale). (<span class="ltx_text ltx_font_italic" id="S4.F2.6.2">right</span>) Inference time of Mamba4Cast versus Chronos-Small on synthetically generated time series (2048 series, 512 context length) for increasing prediction lengths and varying batch sizes. The results demonstrate Mamba4Castâ€™s superior efficiency, particularly for longer prediction horizons and larger batch sizes.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training Details</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Architectural choices</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">The Mamba4Cast model is designed with approximately 27M parameters, positioning it between Chronos-Mini (20M) and Chronos-Small (46M) in size. As demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">1</span></a>, Mamba4Cast is built on Mamba2 <cite class="ltx_cite ltx_citemacro_citep">(Dao &amp; Gu, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib5" title="">2024</a>)</cite> with a state expansion factor (N) of 128 and a block expansion factor (E) of 2. It features 2 encoder layers following an input projection to an embedding dimension of 1024. The final layer of the encoder is defined similarly to the stacked convolution layer illustrated in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A2" title="Appendix B Dataset Preprocessing â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">B</span></a> with the difference in the input channels being 1024 for the embedding size. We minimize the mean squared error over the prediction horizon using AdamW <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib22" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Training setup</h4>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">The model is trained for 420K batches of size 64, using data sampled from the priors in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS3" title="3.3 Synthetic Data Generation â€£ 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>, via a parallelized data loader that ensures the same sample is not seen twice. We train on sequence lengths uniformly sampled between 30 and 512 and minimize the mean squared error over a prediction length uniformly sampled between 10 and 60 per batch. 50% of the time we train to predict a contiguous chunk from the middle of the prediction length to improve predictability over the sequence by reducing reliance on previous states and encouraging emphasis on temporal information. The learning rate is cosine annealed <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib21" title="">2017</a>)</cite> from 1e-5 to 1e-7 throughout the training.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1">The model is trained exclusively on synthetic data generated using two methods outlined in Section Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS3" title="3.3 Synthetic Data Generation â€£ 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The data composition is 70% sampled from GP priors and 30% sampled from FPFN priors, leveraging the GP kernelsâ€™ flexibility in capturing diverse patterns. Training was conducted over 3 days on a single Nvidia RTX2080Ti GPU, for 360k training rounds consisting of 64 independently generated samples each. As stated in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A1.SS2" title="A.2 GP priors â€£ Appendix A Synthetic Data Generation â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">A.2</span></a>, we continue training for another 60K rounds with a changed kernel composition and a learning rate of 1e-6.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Performance Comparison with Baseline Models</h3>
<figure class="ltx_figure ltx_align_floatright" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="94" id="S4.F3.g1" src="x4.png" width="299"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Critical difference diagram comparing mean MASE ranks of Mamba4Cast and baseline models across 17 time series datasets. ForecastPFN was much worse and is excluded for the sake of visibility. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We evaluate on 17 publicly available time series datasets from a wide range of domains from the dataset repository of the GluonTS <cite class="ltx_cite ltx_citemacro_citep">(Alexandrov etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib1" title="">2020</a>)</cite> library with a 512 context length. A detailed list of the datasets used is included in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A3" title="Appendix C Benchmark Datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">C</span></a>. Our evaluations involve comparisons with zero-shot baselines trained on synthetic data (Chronos and ForecastPFN), a deep learning baseline (DeepAR), and statistical methods (AutoARIMA and Seasonal Naive).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For our evaluations, we use AutoGluonâ€“TimeSeries <cite class="ltx_cite ltx_citemacro_citep">(Shchur etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib32" title="">2023</a>)</cite> to evaluate the baselines, with the exception of ForecastPFN, whose results are sourced from the Chronos paper <cite class="ltx_cite ltx_citemacro_citep">(Ansari etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib2" title="">2024</a>)</cite>. To ensure fair comparison across datasets with varying scales, we use the MASE metric <cite class="ltx_cite ltx_citemacro_citep">(Hyndman &amp; Koehler, <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib14" title="">2006</a>)</cite>, which is scale-invariant.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The results, as illustrated in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.F2" title="Figure 2 â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.F3" title="Figure 3 â€£ 4.2 Performance Comparison with Baseline Models â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3</span></a>, demonstrate that Mamba4Cast achieves competitive performance with Chronos-Base(200M) and surpasses other baselines. Notably, this performance is achieved without fine-tuning on real-world datasets. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.F3" title="Figure 3 â€£ 4.2 Performance Comparison with Baseline Models â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3</span></a> shows a critical difference diagram, visualizing the mean model rankings based on MASE (Mean Absolute Scaled Error) over the datasets. In this diagram, models are arranged from best (<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">left</span>) to worst (<span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">right</span>), with statistically insignificant performance differences indicated by connecting horizontal lines (at a significance level of <math alttext="\alpha=0.05" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">Î±</mi><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><eq id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1"></eq><ci id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">ğ›¼</ci><cn id="S4.SS2.p3.1.m1.1.1.3.cmml" type="float" xref="S4.SS2.p3.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\alpha=0.05</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_Î± = 0.05</annotation></semantics></math>). Detailed information on the MASE metric and per-dataset results can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5" title="Appendix E Evaluations on real datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Analysis on Synthetic and Real Data</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We conduct a qualitative inspection of Mamba4Cast to evaluate its ability to extrapolate over the forecasting horizon. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.F4" title="Figure 4 â€£ 4.3 Qualitative Analysis on Synthetic and Real Data â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates Mamba4Castâ€™s improvement with increasing context length and its ability to capture real-life patterns. We also visualize the modelâ€™s forecasting capability on additional real-world data in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5" title="Appendix E Evaluations on real datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="297" id="S4.F4.1.g1" src="x5.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="283" id="S4.F4.2.g1" src="x6.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Qualitative analysis of Mamba4Castâ€™s performance. (<span class="ltx_text ltx_font_italic" id="S4.F4.5.1">left</span>) Demonstrates how prediction accuracy improves with increasing context length for multiplicative sine waves. (<span class="ltx_text ltx_font_italic" id="S4.F4.6.2">right</span>) Illustrates the modelâ€™s forecasting capabilities on two real-world time series datasets.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our experiments demonstrate Mambaâ€™s capability in creating a reliable zero-shot time-series foundation model. After training solely on synthetic data, Mamba4Cast achieves near state-of-the-art results while also maintaining scalability and efficient inference. However, Mamba4Cast is limited to the univariate domain, which only forms a small portion of real time series problems, and is heavily reliant on the diversity of its priors. Nevertheless, we believe our work serves as a significant step towards developing highly performant and scalable multivariate zero-shot forecasting models, setting the stage for future advancements in this domain.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.2">This research was partially supported by the following sources: TAILOR, a project funded by EU Horizon 2020 research and innovation programme under GA No 952215; the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828; the European Research Council (ERC) Consolidator Grant â€œDeep Learning 2.0â€ (grant no. 101045765). The authors acknowledge support by the state of Baden-WÃ¼rttemberg through bwHPC and the German Research Foundation (DFG) through grant INST 35/1597-1 FUGG.Frank Hutter acknowledges financial support by the Hector Foundation. The authors acknowledge support from ELLIS and ELIZA. Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the ERC. Neither the European Union nor the ERC can be held responsible for them.</p>
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="38" id="Sx1.p1.1.g1" src="extracted/5920487/images/ERC_grant.jpeg" width="180"/>
<p class="ltx_p" id="Sx1.p1.3">.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alexandrov etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexandrov, A., Benidis, K., Bohlke-Schneider, M., Flunkert, V., Gasthaus, J., Januschowski, T., Maddix, D.Â C., Rangapuram, S., Salinas, D., Schulz, J., etÂ al.

</span>
<span class="ltx_bibblock">Gluonts: Probabilistic and neural time series modeling in python.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Journal of Machine Learning Research</em>, 21(116):1â€“6, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ansari etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ansari, A.Â F., Stella, L., Turkmen, C., Zhang, X., Mercado, P., Shen, H., Shchur, O., Rangapuram, S.Â S., Arango, S.Â P., Kapoor, S., etÂ al.

</span>
<span class="ltx_bibblock">Chronos: Learning the Language of Time Series.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2403.07815</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Behrouz etÂ al. (2024)</span>
<span class="ltx_bibblock">
Behrouz, A., Santacatterina, M., and Zabih, R.

</span>
<span class="ltx_bibblock">MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2403.19888</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chen, Z., Ma, M., Li, T., Wang, H., and Li, C.

</span>
<span class="ltx_bibblock">Long sequence time-series forecasting with deep learning: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Information Fusion</em>, 97:101819, 2023.

</span>
<span class="ltx_bibblock">ISSN 1566-2535.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.inffus.2023.101819</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao &amp; Gu (2024)</span>
<span class="ltx_bibblock">
Dao, T. and Gu, A.

</span>
<span class="ltx_bibblock">Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dooley etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dooley, S., Khurana, G.Â S., Mohapatra, C., Naidu, S.Â V., and White, C.

</span>
<span class="ltx_bibblock">ForecastPFN: Synthetically-Trained Zero-Shot Forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</em>, 37, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Gao, S., Koker, T., Queen, O., Hartvigsen, T., Tsiligkaridis, T., and Zitnik, M.

</span>
<span class="ltx_bibblock">UNITS: A Unified Multi-Task Time Series Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2403.00131</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gardner etÂ al. (2018)</span>
<span class="ltx_bibblock">
Gardner, J., Pleiss, G., Weinberger, K.Â Q., Bindel, D., and Wilson, A.Â G.

</span>
<span class="ltx_bibblock">Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in neural information processing systems</em>, 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grazzi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Grazzi, R., Siems, J.Â N., Schrodi, S., Brox, T., and Hutter, F.

</span>
<span class="ltx_bibblock">Is Mamba Capable of In-Context Learning?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gruver etÂ al. (2023)</span>
<span class="ltx_bibblock">
Gruver, N., Finzi, M., Qiu, S., and Wilson, A.Â G.

</span>
<span class="ltx_bibblock">Large Language Models Are Zero-Shot Time Series Forecasters.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</em>, 37, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu &amp; Dao (2024)</span>
<span class="ltx_bibblock">
Gu, A. and Dao, T.

</span>
<span class="ltx_bibblock">Mamba: Linear-Time Sequence Modeling with Selective State Spaces.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">First Conference on Language Modeling</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. (2023)</span>
<span class="ltx_bibblock">
He, K., Yang, Q., Ji, L., Pan, J., and Zou, Y.

</span>
<span class="ltx_bibblock">Financial time series forecasting with the deep learning ensemble model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Mathematics</em>, 11(4), 2023.

</span>
<span class="ltx_bibblock">ISSN 2227-7390.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/math11041054</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hollmann etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hollmann, N., MÃ¼ller, S., Eggensperger, K., and Hutter, F.

</span>
<span class="ltx_bibblock">TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hyndman &amp; Koehler (2006)</span>
<span class="ltx_bibblock">
Hyndman, R. and Koehler, A.

</span>
<span class="ltx_bibblock">Another look at measures of forecast accuracy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">International Journal of Forecasting</em>, 22:679â€“688, 02 2006.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.ijforecast.2006.03.001</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J.Â Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q.

</span>
<span class="ltx_bibblock">Time-LLM: Time Series Forecasting by Reprogramming Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jung, S., Moon, J., Park, S., and Hwang, E.

</span>
<span class="ltx_bibblock">Self-attention-based deep learning network for regional influenza forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Journal of Biomedical and Health Informatics</em>, 26(2):922â€“933, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katharopoulos etÂ al. (2020)</span>
<span class="ltx_bibblock">
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.

</span>
<span class="ltx_bibblock">Transformers are rnns: Fast autoregressive transformers with linear attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International conference on machine learning</em>, pp.Â  5156â€“5165. PMLR, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Liang, A., Jiang, X., Sun, Y., Shi, X., and Li, K.

</span>
<span class="ltx_bibblock">Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu &amp; Wang (2024)</span>
<span class="ltx_bibblock">
Liu, X. and Wang, W.

</span>
<span class="ltx_bibblock">Deep time series forecasting models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Mathematics</em>, 12(10):1504, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Liu, Y., Qin, G., Huang, X., Wang, J., and Long, M.

</span>
<span class="ltx_bibblock">AutoTimes: Autoregressive Time Series Forecasters via Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2402.02370</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2017)</span>
<span class="ltx_bibblock">
Loshchilov, I. and Hutter, F.

</span>
<span class="ltx_bibblock">SGDR: Stochastic gradient descent with warm restarts.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">International Conference on Learning Representations</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2019)</span>
<span class="ltx_bibblock">
Loshchilov, I. and Hutter, F.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MÃ¼ller etÂ al. (2022)</span>
<span class="ltx_bibblock">
MÃ¼ller, S., Hollmann, N., Arango, S.Â P., Grabocka, J., and Hutter, F.

</span>
<span class="ltx_bibblock">Transformers Can Do Bayesian Inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Conference on Learning Representations</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nie etÂ al. (2023)</span>
<span class="ltx_bibblock">
Nie, Y., Nguyen, N.Â H., Sinthong, P., and Kalagnanam, J.

</span>
<span class="ltx_bibblock">A Time Series is Worth 64 Words: Long-term Forecasting with Transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oreshkin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Oreshkin, B.Â N., Carpov, D., Chapados, N., and Bengio, Y.

</span>
<span class="ltx_bibblock">N-beats: Neural basis expansion analysis for interpretable time series forecasting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:1905.10437</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oreshkin etÂ al. (2021)</span>
<span class="ltx_bibblock">
Oreshkin, B.Â N., Carpov, D., Chapados, N., and Bengio, Y.

</span>
<span class="ltx_bibblock">Meta-learning framework with applications to zero-shot time-series forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 35, pp.Â  9242â€“9250, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park etÂ al. (2024)</span>
<span class="ltx_bibblock">
Park, J., Park, J., Xiong, Z., Lee, N., Cho, J., Oymak, S., Lee, K., and Papailiopoulos, D.

</span>
<span class="ltx_bibblock">Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patro &amp; Agneeswaran (2024)</span>
<span class="ltx_bibblock">
Patro, B.Â N. and Agneeswaran, V.Â S.

</span>
<span class="ltx_bibblock">SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2403.15360</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PÃ¶ppel etÂ al. (2024)</span>
<span class="ltx_bibblock">
PÃ¶ppel, K., Beck, M., Spanring, M., Auer, A., Prudnikova, O., Kopp, M.Â K., Klambauer, G., Brandstetter, J., and Hochreiter, S.

</span>
<span class="ltx_bibblock">xlstm: Extended long short-term memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">First Workshop on Long-Context Foundation Models@ ICML 2024</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasul etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rasul, K., Ashok, A., Williams, A.Â R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., BiloÅ¡, M., Ghonia, H., Hassen, N., Schneider, A., Garg, S., Drouin, A., Chapados, N., Nevmyvaka, Y., and Rish, I.

</span>
<span class="ltx_bibblock">Lag-Llama: Towards Foundation Models for Time Series Forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salinas etÂ al. (2020)</span>
<span class="ltx_bibblock">
Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T.

</span>
<span class="ltx_bibblock">DeepAR: Probabilistic forecasting with autoregressive recurrent networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">International Journal of Forecasting</em>, 36(3):1181â€“1191, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shchur etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shchur, O., Turkmen, A.Â C., Erickson, N., Shen, H., Shirkov, A., Hu, T., and Wang, B.

</span>
<span class="ltx_bibblock">AutoGluonâ€“TimeSeries: AutoML for probabilistic time series forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">International Conference on Automated Machine Learning</em>, pp.Â  9â€“1. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sivaroopan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Sivaroopan, N., Bandara, D., Madarasingha, C., Jourjon, G., Jayasumana, A.Â P., and Thilakarathna, K.

</span>
<span class="ltx_bibblock">Netdiffus: Network traffic generation by diffusion models through time-series imaging.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Computer Networks</em>, 251:110616, 2024.

</span>
<span class="ltx_bibblock">ISSN 1389-1286.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., etÂ al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wang, Y., Han, Y., Wang, H., and Zhang, X.

</span>
<span class="ltx_bibblock">Contrast everything: A hierarchical contrastive framework for medical time-series.

</span>
<span class="ltx_bibblock">In Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems</em>, volumeÂ 36, pp.Â  55694â€“55717. Curran Associates, Inc., 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Wang, Z., Kong, F., Feng, S., Wang, M., Zhao, H., Wang, D., and Zhang, Y.

</span>
<span class="ltx_bibblock">Is Mamba Effective for Time Series Forecasting?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2403.11144</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Woo etÂ al. (2024)</span>
<span class="ltx_bibblock">
Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D.

</span>
<span class="ltx_bibblock">Unified Training of Universal Time Series Forecasting Transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Forty-first International Conference on Machine Learning</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y.

</span>
<span class="ltx_bibblock">Gated linear attention transformers with hardware-efficient training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Forty-first International Conference on Machine Learning</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Yang, S., Wang, B., Zhang, Y., Shen, Y., and Kim, Y.

</span>
<span class="ltx_bibblock">Parallelizing linear transformers with the delta rule over sequence length.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2406.06484</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y.

</span>
<span class="ltx_bibblock">Coca: Contrastive captioners are image-text foundation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Transactions on Machine Learning Research</em>, 2022.

</span>
<span class="ltx_bibblock">ISSN 2835-8856.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zeng, A., Chen, M., Zhang, L., and Xu, Q.

</span>
<span class="ltx_bibblock">Are transformers effective for time series forecasting?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 37, pp.Â  11121â€“11128, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.

</span>
<span class="ltx_bibblock">Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volumeÂ 35, pp.Â  11106â€“11115, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Synthetic Data Generation</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>ForecastPFN Prior</h3>
<div class="ltx_para ltx_noindent" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We adopted the prior generation process from <cite class="ltx_cite ltx_citemacro_citet">Dooley etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib6" title="">2023</a>)</cite> that decomposes the time series into three components as outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS3" title="3.3 Synthetic Data Generation â€£ 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The trend incorporates linear and exponential growth factors, while seasonal components capture periodic variations at multiple time scales (minutely, hourly, daily, weekly, and monthly), reflecting natural cycles in the data. Noise is modeled using a Weibull distribution to maintain a constant expected value. We introduced some modifications to the original procedure that are mentioned below.</p>
</div>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Trend</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.1">In our experiments, we found that training Mamba4Cast on long sequence time series with exponential trends results in suboptimal performance. Therefore we limited the non-linear growth behavior to be polynomial ones represented in the GP priors, while the FPFN prior only models linearly growing signals.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Seasonal</h4>
<div class="ltx_para ltx_noindent" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">Seasonal patterns are generated according to the granularity of the timestamps. For each granularity, we sample sine-wave signals, referred to as harmonics, with periodicities corresponding to that granularity: 60 for minutely, 24 for hourly, 7 for daily, and 12 for monthly data. For each time series, we sample harmonics from both its granularity and the immediate higher granularity. As an example, for minutely data, we sample seasonal signals with both minutely and hourly periodicities.
In the original design, 10 or 6 harmonics were sampled for each granularity, but in our optimal setup, we used 8 and 5 harmonics, respectively. As the number of harmonics increases, their periodicity is scaled down by the harmonic index, allowing the model to capture finer fluctuations in the data.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>GP priors</h3>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">GP model</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">We use GPyTorch <cite class="ltx_cite ltx_citemacro_citep">(Gardner etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib8" title="">2018</a>)</cite> for sampling our time series from composite kernels, with a sampled zero or linear mean, and a Gaussian likelihood. We add the noise using Cholesky jitter, with the jitter level being sampled among 0.1, 0.01, and 0.001, with probabilities of 0.1, 0.2, and 0.7 respectively. This design choice is to generalize Mamba4Cast for different noise levels in the real-life datasets.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Kernels</h4>
<div class="ltx_para ltx_noindent" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">The kernel bank comprises Linear, Polynomial, Matern, and Periodic kernels. To ensure complex time series patterns, we combine up to 6 kernels using sampled <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS0.Px2.p1.1.1">binary operations</span> (addition or multiplication). The best training pipeline involved sampling a number of kernels from 1 to 6. In the first 360K training rounds, for each kernel, we sampled from Periodic, Matern, or Linear kernels with weights of 5, 1.5, and 1 respectively. This prior was inspired by the KernelSynth method outlined by Chronos <cite class="ltx_cite ltx_citemacro_citep">(Ansari etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib2" title="">2024</a>)</cite>.
<br class="ltx_break"/>We also observed that training followed by a <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS0.Px2.p1.1.2">fine-tuning</span> phase of 60K rounds with changed weights of Periodic, Matern, and Polynomial kernels to 5, 2 and 1 respectively, resulted in better generalization.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Signal level noises</h3>
<div class="ltx_para ltx_noindent" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">In addition to the white noise signal incorporated in our priors, we introduce two types of <span class="ltx_text" id="A1.SS3.p1.1.1">multiplicative</span> noise signals: spikes and step noise. <span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.2">Spikes</span> are designed to introduce regular peaks at every interval, <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.3">l</span>. To simulate peaks that occur regularly but are irregularly spaced, we apply a masking window <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.4">m</span>, which masks up to 40% of the spikes within the window. Similarly, multiplicative <span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.5">step</span> functions are applied in an alternating high-low-high-low pattern to enable Mamba4Cast to capture seasonal level shifts.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Dataset Preprocessing</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We adopt a preprocessing approach similar to e.g. ForecastPFNÂ <cite class="ltx_cite ltx_citemacro_citep">(Dooley etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#bib.bib6" title="">2023</a>)</cite>. Time-steps are decomposed into minutely, hourly, day of week, day of month, day of year, monthly and yearly components, encoded using sinusoidal embeddings. These encodings, along with the series value, are linearly projected and concatenated to represent each time-point in a 112 embedding vector. The value of target tokens for model input across the prediction horizon is 0 for the prediction of point value or 1 for the cumulative mean prediction, fixed to 0 during inference.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">With all input and output token embeddings stacked along the sequence dimension, we apply four causal convolution layers with kernel sizes of 5 and dilations of 1, 2, 4, and 8, concatenating their outputs for diverse temporal coverage. This facilitates capturing multi-scale temporal dependencies, enhancing our modelâ€™s forecasting capabilities. The stack of causal convolution projects the tokens up into our desired embedding dimension of 1024 followed by an inception layer to combine the information across the temporal multi-scale for each token while maintaining the embedding size.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Benchmark Datasets</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We use 17 datasets from Chronos zero-shot benchmark while removing datasets with very small context and prediction length, datasets that are very large, and datasets with sub-hourly frequencies. We will extend to support those datasets in future work. We used GluonTS as an interface for these datasets to have a comparable evaluation pipeline to Chronos. The context length (input sequence length) was restricted to be at most 512, while the prediction length varied according to the evaluated dataset as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A3.T1" title="Table 1 â€£ Appendix C Benchmark Datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="A3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Characteristics of Datasets Used for Zero-Shot Evaluation of Mamba4Cast and baselines.</figcaption>
<br class="ltx_break"/>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T1.1.1.1.1">
<span class="ltx_rule" style="width:0.0pt;height:15.0pt;background:black;display:inline-block;"></span> <span class="ltx_text ltx_font_bold" id="A3.T1.1.1.1.1.1" style="position:relative; bottom:3.0pt;">Dataset</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="A3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A3.T1.1.1.1.2.1" style="position:relative; bottom:3.0pt;">Frequency</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A3.T1.1.1.1.3.1" style="position:relative; bottom:3.0pt;">Num. Test Series</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A3.T1.1.1.1.4.1" style="position:relative; bottom:3.0pt;">Prediction Length</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.T1.1.2.1.1">CIF 2016</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A3.T1.1.2.1.2">1M</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T1.1.2.1.3">72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T1.1.2.1.4">12</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.3.2.1">Car Parts</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.3.2.2">1M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.3.2.3">2674</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.3.2.4">12</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.4.3.1">Covid Deaths</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.4.3.2">1D</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.4.3.3">266</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.4.3.4">30</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.5.4.1">ERCOT Load</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.5.4.2">1H</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.5.4.3">8</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.5.4.4">24</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.6.5.1">Exchange Rate</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.6.5.2">1B</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.6.5.3">8</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.6.5.4">30</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.7.6.1">FRED-MD</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.7.6.2">1M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.7.6.3">107</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.7.6.4">12</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.8.7.1">Hospital</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.8.7.2">1M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.8.7.3">767</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.8.7.4">12</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.9.8.1">M1 (Monthly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.9.8.2">1M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.9.8.3">617</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.9.8.4">18</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.10.9.1">M1 (Quarterly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.10.9.2">3M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.10.9.3">203</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.10.9.4">8</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.11.10.1">M3 (Monthly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.11.10.2">1M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.11.10.3">1428</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.11.10.4">18</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.12.11.1">M3 (Quarterly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.12.11.2">3M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.12.11.3">756</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.12.11.4">8</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.13.12.1">NN5 (Daily)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.13.12.2">1D</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.13.12.3">111</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.13.12.4">56</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.14.13.1">NN5 (Weekly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.14.13.2">1W</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.14.13.3">111</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.14.13.4">8</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.15.14.1">Tourism (Monthly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.15.14.2">1M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.15.14.3">366</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.15.14.4">24</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.16.15.1">Tourism (Quarterly)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.16.15.2">3M</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.16.15.3">427</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.16.15.4">8</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.T1.1.17.16.1">Traffic</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="A3.T1.1.17.16.2">1H</th>
<td class="ltx_td ltx_align_center" id="A3.T1.1.17.16.3">862</td>
<td class="ltx_td ltx_align_center" id="A3.T1.1.17.16.4">24</td>
</tr>
<tr class="ltx_tr" id="A3.T1.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.T1.1.18.17.1">Weather</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="A3.T1.1.18.17.2">1D</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T1.1.18.17.3">3010</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T1.1.18.17.4">30</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Ablation studies</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We investigate the robustness of Mamba4Cast in different configurations, which fall into three main categories:</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.1">Architectural Changes:</span> We look into the effectiveness of a stacked causal convolutions layer (CNN) against a linear layer (Linear) in the input embedding and as the encoder-blockâ€™s last layer. While adding the CNN layer as the final layer of the encoder block (<span class="ltx_text ltx_font_italic" id="A4.I1.i1.p1.1.2">baseline</span>) provides superior performance with a significant overhead in model size, the key advantage stems from the CNN layer in the input embedding without overhead in model size.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.I1.i1.p2">
<p class="ltx_p" id="A4.I1.i1.p2.1">The model sizes of the three setups listed in the corresponding section of Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A4.T2" title="Table 2 â€£ Appendix D Ablation studies â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">2</span></a> are 27M, 17M, and 15M, in the same order as in the table.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.1">Prior Mixing Ratios:</span> Given the importance of the distribution of synthetic data, we conducted experiments to explore the impact of each of the two approaches mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S3.SS3" title="3.3 Synthetic Data Generation â€£ 3 Methodology â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3.3</span></a>. The ablation indicates the effectiveness of the GP prior over the FPFN prior, leading to our choice of a GP favoured mixture of data for training.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.1">Inference Modes:</span> Mamba4Cast was designed with efficient zero-shot forecasting in mind following the one-pass multipoint setup, in which the input and target tokens are concatenated together in their respective order. Mamba4Cast also supports autoregressive forecasting, but its performance declines significantly in this setup. A likely reason is that feeding predicted values back into the model causes overconfidence and error propagation. In contrast, the multipoint setup treats all target values as unknown, avoiding this issue.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.I1.i3.p2">
<p class="ltx_p" id="A4.I1.i3.p2.1">We further test the impact of ensembling by averaging the forecasts generated at 5 different levels of dropout, from 0 to 0.5, of the input sequence. However, given the superior performance over longer and more inclusive contexts, demonstrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.F4" title="Figure 4 â€£ 4.3 Qualitative Analysis on Synthetic and Real Data â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">4</span></a>, it follows that including a less accurate forecast can degrade performance in case Mamba4Cast is certain about its forecast as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A4.T2" title="Table 2 â€£ Appendix D Ablation studies â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">The ablation studies were conducted on the first 360K training rounds mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS1" title="4.1 Training Details â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>, as the subsequent 60K were later applied to our chosen setup for the baseline comparisons cited in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS2" title="4.2 Performance Comparison with Baseline Models â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
<figure class="ltx_table" id="A4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study on architectural changes, prior mixing ratios and the inference modes. The value reported is the geometric mean of MASE across all 17 datasets for each setup.</figcaption>
<br class="ltx_break ltx_centering"/>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A4.T2.15" style="width:260.2pt;height:215.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.3pt,10.2pt) scale(0.913381750101445,0.913381750101445) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A4.T2.15.15">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T2.15.15.16.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A4.T2.15.15.16.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_rule" style="width:0.0pt;height:15.0pt;background:black;display:inline-block;"></span> <span class="ltx_text ltx_font_bold" id="A4.T2.15.15.16.1.1.1" style="position:relative; bottom:3.0pt;">Ablation Setup</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T2.15.15.16.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.15.15.16.1.2.1" style="position:relative; bottom:3.0pt;">Mean MASE</span></th>
</tr>
<tr class="ltx_tr" id="A4.T2.15.15.17.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A4.T2.15.15.17.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.15.15.17.2.1.1" style="position:relative; bottom:1.0pt;">Architectural Modifications</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A4.T2.15.15.17.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T2.3.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T2.2.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<math alttext="\text{CNN}_{in\_emb}" class="ltx_Math" display="inline" id="A4.T2.1.1.1.1.m1.1"><semantics id="A4.T2.1.1.1.1.m1.1a"><msub id="A4.T2.1.1.1.1.m1.1.1" xref="A4.T2.1.1.1.1.m1.1.1.cmml"><mtext id="A4.T2.1.1.1.1.m1.1.1.2" xref="A4.T2.1.1.1.1.m1.1.1.2a.cmml">CNN</mtext><mrow id="A4.T2.1.1.1.1.m1.1.1.3" xref="A4.T2.1.1.1.1.m1.1.1.3.cmml"><mi id="A4.T2.1.1.1.1.m1.1.1.3.2" xref="A4.T2.1.1.1.1.m1.1.1.3.2.cmml">i</mi><mo id="A4.T2.1.1.1.1.m1.1.1.3.1" xref="A4.T2.1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.1.1.1.1.m1.1.1.3.3" xref="A4.T2.1.1.1.1.m1.1.1.3.3.cmml">n</mi><mo id="A4.T2.1.1.1.1.m1.1.1.3.1a" xref="A4.T2.1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.1.1.1.1.m1.1.1.3.4" mathvariant="normal" xref="A4.T2.1.1.1.1.m1.1.1.3.4.cmml">_</mi><mo id="A4.T2.1.1.1.1.m1.1.1.3.1b" xref="A4.T2.1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.1.1.1.1.m1.1.1.3.5" xref="A4.T2.1.1.1.1.m1.1.1.3.5.cmml">e</mi><mo id="A4.T2.1.1.1.1.m1.1.1.3.1c" xref="A4.T2.1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.1.1.1.1.m1.1.1.3.6" xref="A4.T2.1.1.1.1.m1.1.1.3.6.cmml">m</mi><mo id="A4.T2.1.1.1.1.m1.1.1.3.1d" xref="A4.T2.1.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.1.1.1.1.m1.1.1.3.7" xref="A4.T2.1.1.1.1.m1.1.1.3.7.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.T2.1.1.1.1.m1.1b"><apply id="A4.T2.1.1.1.1.m1.1.1.cmml" xref="A4.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.T2.1.1.1.1.m1.1.1.1.cmml" xref="A4.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A4.T2.1.1.1.1.m1.1.1.2a.cmml" xref="A4.T2.1.1.1.1.m1.1.1.2"><mtext id="A4.T2.1.1.1.1.m1.1.1.2.cmml" xref="A4.T2.1.1.1.1.m1.1.1.2">CNN</mtext></ci><apply id="A4.T2.1.1.1.1.m1.1.1.3.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3"><times id="A4.T2.1.1.1.1.m1.1.1.3.1.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.1"></times><ci id="A4.T2.1.1.1.1.m1.1.1.3.2.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.2">ğ‘–</ci><ci id="A4.T2.1.1.1.1.m1.1.1.3.3.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.3">ğ‘›</ci><ci id="A4.T2.1.1.1.1.m1.1.1.3.4.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.4">_</ci><ci id="A4.T2.1.1.1.1.m1.1.1.3.5.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.5">ğ‘’</ci><ci id="A4.T2.1.1.1.1.m1.1.1.3.6.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.6">ğ‘š</ci><ci id="A4.T2.1.1.1.1.m1.1.1.3.7.cmml" xref="A4.T2.1.1.1.1.m1.1.1.3.7">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.1.1.1.1.m1.1c">\text{CNN}_{in\_emb}</annotation><annotation encoding="application/x-llamapun" id="A4.T2.1.1.1.1.m1.1d">CNN start_POSTSUBSCRIPT italic_i italic_n _ italic_e italic_m italic_b end_POSTSUBSCRIPT</annotation></semantics></math> / <math alttext="\text{CNN}_{enc\_out}" class="ltx_Math" display="inline" id="A4.T2.2.2.2.2.m2.1"><semantics id="A4.T2.2.2.2.2.m2.1a"><msub id="A4.T2.2.2.2.2.m2.1.1" xref="A4.T2.2.2.2.2.m2.1.1.cmml"><mtext id="A4.T2.2.2.2.2.m2.1.1.2" xref="A4.T2.2.2.2.2.m2.1.1.2a.cmml">CNN</mtext><mrow id="A4.T2.2.2.2.2.m2.1.1.3" xref="A4.T2.2.2.2.2.m2.1.1.3.cmml"><mi id="A4.T2.2.2.2.2.m2.1.1.3.2" xref="A4.T2.2.2.2.2.m2.1.1.3.2.cmml">e</mi><mo id="A4.T2.2.2.2.2.m2.1.1.3.1" xref="A4.T2.2.2.2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.2.2.2.2.m2.1.1.3.3" xref="A4.T2.2.2.2.2.m2.1.1.3.3.cmml">n</mi><mo id="A4.T2.2.2.2.2.m2.1.1.3.1a" xref="A4.T2.2.2.2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.2.2.2.2.m2.1.1.3.4" xref="A4.T2.2.2.2.2.m2.1.1.3.4.cmml">c</mi><mo id="A4.T2.2.2.2.2.m2.1.1.3.1b" xref="A4.T2.2.2.2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.2.2.2.2.m2.1.1.3.5" mathvariant="normal" xref="A4.T2.2.2.2.2.m2.1.1.3.5.cmml">_</mi><mo id="A4.T2.2.2.2.2.m2.1.1.3.1c" xref="A4.T2.2.2.2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.2.2.2.2.m2.1.1.3.6" xref="A4.T2.2.2.2.2.m2.1.1.3.6.cmml">o</mi><mo id="A4.T2.2.2.2.2.m2.1.1.3.1d" xref="A4.T2.2.2.2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.2.2.2.2.m2.1.1.3.7" xref="A4.T2.2.2.2.2.m2.1.1.3.7.cmml">u</mi><mo id="A4.T2.2.2.2.2.m2.1.1.3.1e" xref="A4.T2.2.2.2.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.2.2.2.2.m2.1.1.3.8" xref="A4.T2.2.2.2.2.m2.1.1.3.8.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.T2.2.2.2.2.m2.1b"><apply id="A4.T2.2.2.2.2.m2.1.1.cmml" xref="A4.T2.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="A4.T2.2.2.2.2.m2.1.1.1.cmml" xref="A4.T2.2.2.2.2.m2.1.1">subscript</csymbol><ci id="A4.T2.2.2.2.2.m2.1.1.2a.cmml" xref="A4.T2.2.2.2.2.m2.1.1.2"><mtext id="A4.T2.2.2.2.2.m2.1.1.2.cmml" xref="A4.T2.2.2.2.2.m2.1.1.2">CNN</mtext></ci><apply id="A4.T2.2.2.2.2.m2.1.1.3.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3"><times id="A4.T2.2.2.2.2.m2.1.1.3.1.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.1"></times><ci id="A4.T2.2.2.2.2.m2.1.1.3.2.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.2">ğ‘’</ci><ci id="A4.T2.2.2.2.2.m2.1.1.3.3.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.3">ğ‘›</ci><ci id="A4.T2.2.2.2.2.m2.1.1.3.4.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.4">ğ‘</ci><ci id="A4.T2.2.2.2.2.m2.1.1.3.5.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.5">_</ci><ci id="A4.T2.2.2.2.2.m2.1.1.3.6.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.6">ğ‘œ</ci><ci id="A4.T2.2.2.2.2.m2.1.1.3.7.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.7">ğ‘¢</ci><ci id="A4.T2.2.2.2.2.m2.1.1.3.8.cmml" xref="A4.T2.2.2.2.2.m2.1.1.3.8">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.2.2.2.2.m2.1c">\text{CNN}_{enc\_out}</annotation><annotation encoding="application/x-llamapun" id="A4.T2.2.2.2.2.m2.1d">CNN start_POSTSUBSCRIPT italic_e italic_n italic_c _ italic_o italic_u italic_t end_POSTSUBSCRIPT</annotation></semantics></math> (<span class="ltx_text ltx_font_italic" id="A4.T2.2.2.2.2.1">Baseline</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T2.3.3.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.153" class="ltx_Math" display="inline" id="A4.T2.3.3.3.3.m1.1"><semantics id="A4.T2.3.3.3.3.m1.1a"><mn id="A4.T2.3.3.3.3.m1.1.1" xref="A4.T2.3.3.3.3.m1.1.1.cmml">1.153</mn><annotation-xml encoding="MathML-Content" id="A4.T2.3.3.3.3.m1.1b"><cn id="A4.T2.3.3.3.3.m1.1.1.cmml" type="float" xref="A4.T2.3.3.3.3.m1.1.1">1.153</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.3.3.3.3.m1.1c">1.153</annotation><annotation encoding="application/x-llamapun" id="A4.T2.3.3.3.3.m1.1d">1.153</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.6.6.6">
<td class="ltx_td ltx_align_left" id="A4.T2.5.5.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<math alttext="\text{CNN}_{in\_emb}" class="ltx_Math" display="inline" id="A4.T2.4.4.4.1.m1.1"><semantics id="A4.T2.4.4.4.1.m1.1a"><msub id="A4.T2.4.4.4.1.m1.1.1" xref="A4.T2.4.4.4.1.m1.1.1.cmml"><mtext id="A4.T2.4.4.4.1.m1.1.1.2" xref="A4.T2.4.4.4.1.m1.1.1.2a.cmml">CNN</mtext><mrow id="A4.T2.4.4.4.1.m1.1.1.3" xref="A4.T2.4.4.4.1.m1.1.1.3.cmml"><mi id="A4.T2.4.4.4.1.m1.1.1.3.2" xref="A4.T2.4.4.4.1.m1.1.1.3.2.cmml">i</mi><mo id="A4.T2.4.4.4.1.m1.1.1.3.1" xref="A4.T2.4.4.4.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.4.4.4.1.m1.1.1.3.3" xref="A4.T2.4.4.4.1.m1.1.1.3.3.cmml">n</mi><mo id="A4.T2.4.4.4.1.m1.1.1.3.1a" xref="A4.T2.4.4.4.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.4.4.4.1.m1.1.1.3.4" mathvariant="normal" xref="A4.T2.4.4.4.1.m1.1.1.3.4.cmml">_</mi><mo id="A4.T2.4.4.4.1.m1.1.1.3.1b" xref="A4.T2.4.4.4.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.4.4.4.1.m1.1.1.3.5" xref="A4.T2.4.4.4.1.m1.1.1.3.5.cmml">e</mi><mo id="A4.T2.4.4.4.1.m1.1.1.3.1c" xref="A4.T2.4.4.4.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.4.4.4.1.m1.1.1.3.6" xref="A4.T2.4.4.4.1.m1.1.1.3.6.cmml">m</mi><mo id="A4.T2.4.4.4.1.m1.1.1.3.1d" xref="A4.T2.4.4.4.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.4.4.4.1.m1.1.1.3.7" xref="A4.T2.4.4.4.1.m1.1.1.3.7.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.T2.4.4.4.1.m1.1b"><apply id="A4.T2.4.4.4.1.m1.1.1.cmml" xref="A4.T2.4.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="A4.T2.4.4.4.1.m1.1.1.1.cmml" xref="A4.T2.4.4.4.1.m1.1.1">subscript</csymbol><ci id="A4.T2.4.4.4.1.m1.1.1.2a.cmml" xref="A4.T2.4.4.4.1.m1.1.1.2"><mtext id="A4.T2.4.4.4.1.m1.1.1.2.cmml" xref="A4.T2.4.4.4.1.m1.1.1.2">CNN</mtext></ci><apply id="A4.T2.4.4.4.1.m1.1.1.3.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3"><times id="A4.T2.4.4.4.1.m1.1.1.3.1.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.1"></times><ci id="A4.T2.4.4.4.1.m1.1.1.3.2.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.2">ğ‘–</ci><ci id="A4.T2.4.4.4.1.m1.1.1.3.3.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.3">ğ‘›</ci><ci id="A4.T2.4.4.4.1.m1.1.1.3.4.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.4">_</ci><ci id="A4.T2.4.4.4.1.m1.1.1.3.5.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.5">ğ‘’</ci><ci id="A4.T2.4.4.4.1.m1.1.1.3.6.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.6">ğ‘š</ci><ci id="A4.T2.4.4.4.1.m1.1.1.3.7.cmml" xref="A4.T2.4.4.4.1.m1.1.1.3.7">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.4.4.4.1.m1.1c">\text{CNN}_{in\_emb}</annotation><annotation encoding="application/x-llamapun" id="A4.T2.4.4.4.1.m1.1d">CNN start_POSTSUBSCRIPT italic_i italic_n _ italic_e italic_m italic_b end_POSTSUBSCRIPT</annotation></semantics></math> / <math alttext="\text{Linear}_{enc\_out}" class="ltx_Math" display="inline" id="A4.T2.5.5.5.2.m2.1"><semantics id="A4.T2.5.5.5.2.m2.1a"><msub id="A4.T2.5.5.5.2.m2.1.1" xref="A4.T2.5.5.5.2.m2.1.1.cmml"><mtext id="A4.T2.5.5.5.2.m2.1.1.2" xref="A4.T2.5.5.5.2.m2.1.1.2a.cmml">Linear</mtext><mrow id="A4.T2.5.5.5.2.m2.1.1.3" xref="A4.T2.5.5.5.2.m2.1.1.3.cmml"><mi id="A4.T2.5.5.5.2.m2.1.1.3.2" xref="A4.T2.5.5.5.2.m2.1.1.3.2.cmml">e</mi><mo id="A4.T2.5.5.5.2.m2.1.1.3.1" xref="A4.T2.5.5.5.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.5.5.5.2.m2.1.1.3.3" xref="A4.T2.5.5.5.2.m2.1.1.3.3.cmml">n</mi><mo id="A4.T2.5.5.5.2.m2.1.1.3.1a" xref="A4.T2.5.5.5.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.5.5.5.2.m2.1.1.3.4" xref="A4.T2.5.5.5.2.m2.1.1.3.4.cmml">c</mi><mo id="A4.T2.5.5.5.2.m2.1.1.3.1b" xref="A4.T2.5.5.5.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.5.5.5.2.m2.1.1.3.5" mathvariant="normal" xref="A4.T2.5.5.5.2.m2.1.1.3.5.cmml">_</mi><mo id="A4.T2.5.5.5.2.m2.1.1.3.1c" xref="A4.T2.5.5.5.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.5.5.5.2.m2.1.1.3.6" xref="A4.T2.5.5.5.2.m2.1.1.3.6.cmml">o</mi><mo id="A4.T2.5.5.5.2.m2.1.1.3.1d" xref="A4.T2.5.5.5.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.5.5.5.2.m2.1.1.3.7" xref="A4.T2.5.5.5.2.m2.1.1.3.7.cmml">u</mi><mo id="A4.T2.5.5.5.2.m2.1.1.3.1e" xref="A4.T2.5.5.5.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.5.5.5.2.m2.1.1.3.8" xref="A4.T2.5.5.5.2.m2.1.1.3.8.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.T2.5.5.5.2.m2.1b"><apply id="A4.T2.5.5.5.2.m2.1.1.cmml" xref="A4.T2.5.5.5.2.m2.1.1"><csymbol cd="ambiguous" id="A4.T2.5.5.5.2.m2.1.1.1.cmml" xref="A4.T2.5.5.5.2.m2.1.1">subscript</csymbol><ci id="A4.T2.5.5.5.2.m2.1.1.2a.cmml" xref="A4.T2.5.5.5.2.m2.1.1.2"><mtext id="A4.T2.5.5.5.2.m2.1.1.2.cmml" xref="A4.T2.5.5.5.2.m2.1.1.2">Linear</mtext></ci><apply id="A4.T2.5.5.5.2.m2.1.1.3.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3"><times id="A4.T2.5.5.5.2.m2.1.1.3.1.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.1"></times><ci id="A4.T2.5.5.5.2.m2.1.1.3.2.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.2">ğ‘’</ci><ci id="A4.T2.5.5.5.2.m2.1.1.3.3.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.3">ğ‘›</ci><ci id="A4.T2.5.5.5.2.m2.1.1.3.4.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.4">ğ‘</ci><ci id="A4.T2.5.5.5.2.m2.1.1.3.5.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.5">_</ci><ci id="A4.T2.5.5.5.2.m2.1.1.3.6.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.6">ğ‘œ</ci><ci id="A4.T2.5.5.5.2.m2.1.1.3.7.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.7">ğ‘¢</ci><ci id="A4.T2.5.5.5.2.m2.1.1.3.8.cmml" xref="A4.T2.5.5.5.2.m2.1.1.3.8">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.5.5.5.2.m2.1c">\text{Linear}_{enc\_out}</annotation><annotation encoding="application/x-llamapun" id="A4.T2.5.5.5.2.m2.1d">Linear start_POSTSUBSCRIPT italic_e italic_n italic_c _ italic_o italic_u italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A4.T2.6.6.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.205" class="ltx_Math" display="inline" id="A4.T2.6.6.6.3.m1.1"><semantics id="A4.T2.6.6.6.3.m1.1a"><mn id="A4.T2.6.6.6.3.m1.1.1" xref="A4.T2.6.6.6.3.m1.1.1.cmml">1.205</mn><annotation-xml encoding="MathML-Content" id="A4.T2.6.6.6.3.m1.1b"><cn id="A4.T2.6.6.6.3.m1.1.1.cmml" type="float" xref="A4.T2.6.6.6.3.m1.1.1">1.205</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.6.6.6.3.m1.1c">1.205</annotation><annotation encoding="application/x-llamapun" id="A4.T2.6.6.6.3.m1.1d">1.205</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.9.9.9">
<td class="ltx_td ltx_align_left" id="A4.T2.8.8.8.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<math alttext="\text{Linear}_{in\_emb}" class="ltx_Math" display="inline" id="A4.T2.7.7.7.1.m1.1"><semantics id="A4.T2.7.7.7.1.m1.1a"><msub id="A4.T2.7.7.7.1.m1.1.1" xref="A4.T2.7.7.7.1.m1.1.1.cmml"><mtext id="A4.T2.7.7.7.1.m1.1.1.2" xref="A4.T2.7.7.7.1.m1.1.1.2a.cmml">Linear</mtext><mrow id="A4.T2.7.7.7.1.m1.1.1.3" xref="A4.T2.7.7.7.1.m1.1.1.3.cmml"><mi id="A4.T2.7.7.7.1.m1.1.1.3.2" xref="A4.T2.7.7.7.1.m1.1.1.3.2.cmml">i</mi><mo id="A4.T2.7.7.7.1.m1.1.1.3.1" xref="A4.T2.7.7.7.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.7.7.7.1.m1.1.1.3.3" xref="A4.T2.7.7.7.1.m1.1.1.3.3.cmml">n</mi><mo id="A4.T2.7.7.7.1.m1.1.1.3.1a" xref="A4.T2.7.7.7.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.7.7.7.1.m1.1.1.3.4" mathvariant="normal" xref="A4.T2.7.7.7.1.m1.1.1.3.4.cmml">_</mi><mo id="A4.T2.7.7.7.1.m1.1.1.3.1b" xref="A4.T2.7.7.7.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.7.7.7.1.m1.1.1.3.5" xref="A4.T2.7.7.7.1.m1.1.1.3.5.cmml">e</mi><mo id="A4.T2.7.7.7.1.m1.1.1.3.1c" xref="A4.T2.7.7.7.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.7.7.7.1.m1.1.1.3.6" xref="A4.T2.7.7.7.1.m1.1.1.3.6.cmml">m</mi><mo id="A4.T2.7.7.7.1.m1.1.1.3.1d" xref="A4.T2.7.7.7.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.7.7.7.1.m1.1.1.3.7" xref="A4.T2.7.7.7.1.m1.1.1.3.7.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.T2.7.7.7.1.m1.1b"><apply id="A4.T2.7.7.7.1.m1.1.1.cmml" xref="A4.T2.7.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="A4.T2.7.7.7.1.m1.1.1.1.cmml" xref="A4.T2.7.7.7.1.m1.1.1">subscript</csymbol><ci id="A4.T2.7.7.7.1.m1.1.1.2a.cmml" xref="A4.T2.7.7.7.1.m1.1.1.2"><mtext id="A4.T2.7.7.7.1.m1.1.1.2.cmml" xref="A4.T2.7.7.7.1.m1.1.1.2">Linear</mtext></ci><apply id="A4.T2.7.7.7.1.m1.1.1.3.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3"><times id="A4.T2.7.7.7.1.m1.1.1.3.1.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.1"></times><ci id="A4.T2.7.7.7.1.m1.1.1.3.2.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.2">ğ‘–</ci><ci id="A4.T2.7.7.7.1.m1.1.1.3.3.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.3">ğ‘›</ci><ci id="A4.T2.7.7.7.1.m1.1.1.3.4.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.4">_</ci><ci id="A4.T2.7.7.7.1.m1.1.1.3.5.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.5">ğ‘’</ci><ci id="A4.T2.7.7.7.1.m1.1.1.3.6.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.6">ğ‘š</ci><ci id="A4.T2.7.7.7.1.m1.1.1.3.7.cmml" xref="A4.T2.7.7.7.1.m1.1.1.3.7">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.7.7.7.1.m1.1c">\text{Linear}_{in\_emb}</annotation><annotation encoding="application/x-llamapun" id="A4.T2.7.7.7.1.m1.1d">Linear start_POSTSUBSCRIPT italic_i italic_n _ italic_e italic_m italic_b end_POSTSUBSCRIPT</annotation></semantics></math> / <math alttext="\text{Linear}_{enc\_out}" class="ltx_Math" display="inline" id="A4.T2.8.8.8.2.m2.1"><semantics id="A4.T2.8.8.8.2.m2.1a"><msub id="A4.T2.8.8.8.2.m2.1.1" xref="A4.T2.8.8.8.2.m2.1.1.cmml"><mtext id="A4.T2.8.8.8.2.m2.1.1.2" xref="A4.T2.8.8.8.2.m2.1.1.2a.cmml">Linear</mtext><mrow id="A4.T2.8.8.8.2.m2.1.1.3" xref="A4.T2.8.8.8.2.m2.1.1.3.cmml"><mi id="A4.T2.8.8.8.2.m2.1.1.3.2" xref="A4.T2.8.8.8.2.m2.1.1.3.2.cmml">e</mi><mo id="A4.T2.8.8.8.2.m2.1.1.3.1" xref="A4.T2.8.8.8.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.8.8.8.2.m2.1.1.3.3" xref="A4.T2.8.8.8.2.m2.1.1.3.3.cmml">n</mi><mo id="A4.T2.8.8.8.2.m2.1.1.3.1a" xref="A4.T2.8.8.8.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.8.8.8.2.m2.1.1.3.4" xref="A4.T2.8.8.8.2.m2.1.1.3.4.cmml">c</mi><mo id="A4.T2.8.8.8.2.m2.1.1.3.1b" xref="A4.T2.8.8.8.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.8.8.8.2.m2.1.1.3.5" mathvariant="normal" xref="A4.T2.8.8.8.2.m2.1.1.3.5.cmml">_</mi><mo id="A4.T2.8.8.8.2.m2.1.1.3.1c" xref="A4.T2.8.8.8.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.8.8.8.2.m2.1.1.3.6" xref="A4.T2.8.8.8.2.m2.1.1.3.6.cmml">o</mi><mo id="A4.T2.8.8.8.2.m2.1.1.3.1d" xref="A4.T2.8.8.8.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.8.8.8.2.m2.1.1.3.7" xref="A4.T2.8.8.8.2.m2.1.1.3.7.cmml">u</mi><mo id="A4.T2.8.8.8.2.m2.1.1.3.1e" xref="A4.T2.8.8.8.2.m2.1.1.3.1.cmml">â¢</mo><mi id="A4.T2.8.8.8.2.m2.1.1.3.8" xref="A4.T2.8.8.8.2.m2.1.1.3.8.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A4.T2.8.8.8.2.m2.1b"><apply id="A4.T2.8.8.8.2.m2.1.1.cmml" xref="A4.T2.8.8.8.2.m2.1.1"><csymbol cd="ambiguous" id="A4.T2.8.8.8.2.m2.1.1.1.cmml" xref="A4.T2.8.8.8.2.m2.1.1">subscript</csymbol><ci id="A4.T2.8.8.8.2.m2.1.1.2a.cmml" xref="A4.T2.8.8.8.2.m2.1.1.2"><mtext id="A4.T2.8.8.8.2.m2.1.1.2.cmml" xref="A4.T2.8.8.8.2.m2.1.1.2">Linear</mtext></ci><apply id="A4.T2.8.8.8.2.m2.1.1.3.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3"><times id="A4.T2.8.8.8.2.m2.1.1.3.1.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.1"></times><ci id="A4.T2.8.8.8.2.m2.1.1.3.2.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.2">ğ‘’</ci><ci id="A4.T2.8.8.8.2.m2.1.1.3.3.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.3">ğ‘›</ci><ci id="A4.T2.8.8.8.2.m2.1.1.3.4.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.4">ğ‘</ci><ci id="A4.T2.8.8.8.2.m2.1.1.3.5.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.5">_</ci><ci id="A4.T2.8.8.8.2.m2.1.1.3.6.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.6">ğ‘œ</ci><ci id="A4.T2.8.8.8.2.m2.1.1.3.7.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.7">ğ‘¢</ci><ci id="A4.T2.8.8.8.2.m2.1.1.3.8.cmml" xref="A4.T2.8.8.8.2.m2.1.1.3.8">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.8.8.8.2.m2.1c">\text{Linear}_{enc\_out}</annotation><annotation encoding="application/x-llamapun" id="A4.T2.8.8.8.2.m2.1d">Linear start_POSTSUBSCRIPT italic_e italic_n italic_c _ italic_o italic_u italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A4.T2.9.9.9.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.556" class="ltx_Math" display="inline" id="A4.T2.9.9.9.3.m1.1"><semantics id="A4.T2.9.9.9.3.m1.1a"><mn id="A4.T2.9.9.9.3.m1.1.1" xref="A4.T2.9.9.9.3.m1.1.1.cmml">1.556</mn><annotation-xml encoding="MathML-Content" id="A4.T2.9.9.9.3.m1.1b"><cn id="A4.T2.9.9.9.3.m1.1.1.cmml" type="float" xref="A4.T2.9.9.9.3.m1.1.1">1.556</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.9.9.9.3.m1.1c">1.556</annotation><annotation encoding="application/x-llamapun" id="A4.T2.9.9.9.3.m1.1d">1.556</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.15.15.18.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T2.15.15.18.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.15.15.18.1.1.1" style="position:relative; bottom:1.0pt;">Priors Mixing Ratios</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T2.15.15.18.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span></td>
</tr>
<tr class="ltx_tr" id="A4.T2.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T2.10.10.10.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_text ltx_align_right ltx_inline-block" id="A4.T2.10.10.10.2.1" style="width:20.0pt;">70%</span> GP Prior / <span class="ltx_text ltx_align_right ltx_inline-block" id="A4.T2.10.10.10.2.2" style="width:22.0pt;">30%</span> FPFN Prior (<span class="ltx_text ltx_font_italic" id="A4.T2.10.10.10.2.3">Baseline</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T2.10.10.10.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.153" class="ltx_Math" display="inline" id="A4.T2.10.10.10.1.m1.1"><semantics id="A4.T2.10.10.10.1.m1.1a"><mn id="A4.T2.10.10.10.1.m1.1.1" xref="A4.T2.10.10.10.1.m1.1.1.cmml">1.153</mn><annotation-xml encoding="MathML-Content" id="A4.T2.10.10.10.1.m1.1b"><cn id="A4.T2.10.10.10.1.m1.1.1.cmml" type="float" xref="A4.T2.10.10.10.1.m1.1.1">1.153</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.10.10.10.1.m1.1c">1.153</annotation><annotation encoding="application/x-llamapun" id="A4.T2.10.10.10.1.m1.1d">1.153</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.11.11.11">
<td class="ltx_td ltx_align_left" id="A4.T2.11.11.11.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_text ltx_align_right ltx_inline-block" id="A4.T2.11.11.11.2.1" style="width:20.0pt;">100%</span> GP Prior / <span class="ltx_text ltx_align_right ltx_inline-block" id="A4.T2.11.11.11.2.2" style="width:22.0pt;">0%</span> FPFN Prior</td>
<td class="ltx_td ltx_align_center" id="A4.T2.11.11.11.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.167" class="ltx_Math" display="inline" id="A4.T2.11.11.11.1.m1.1"><semantics id="A4.T2.11.11.11.1.m1.1a"><mn id="A4.T2.11.11.11.1.m1.1.1" xref="A4.T2.11.11.11.1.m1.1.1.cmml">1.167</mn><annotation-xml encoding="MathML-Content" id="A4.T2.11.11.11.1.m1.1b"><cn id="A4.T2.11.11.11.1.m1.1.1.cmml" type="float" xref="A4.T2.11.11.11.1.m1.1.1">1.167</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.11.11.11.1.m1.1c">1.167</annotation><annotation encoding="application/x-llamapun" id="A4.T2.11.11.11.1.m1.1d">1.167</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.12.12.12">
<td class="ltx_td ltx_align_left" id="A4.T2.12.12.12.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_text ltx_align_right ltx_inline-block" id="A4.T2.12.12.12.2.1" style="width:20.0pt;">0%</span> GP Prior / <span class="ltx_text ltx_align_right ltx_inline-block" id="A4.T2.12.12.12.2.2" style="width:22.0pt;">100%</span> FPFN Prior</td>
<td class="ltx_td ltx_align_center" id="A4.T2.12.12.12.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.579" class="ltx_Math" display="inline" id="A4.T2.12.12.12.1.m1.1"><semantics id="A4.T2.12.12.12.1.m1.1a"><mn id="A4.T2.12.12.12.1.m1.1.1" xref="A4.T2.12.12.12.1.m1.1.1.cmml">1.579</mn><annotation-xml encoding="MathML-Content" id="A4.T2.12.12.12.1.m1.1b"><cn id="A4.T2.12.12.12.1.m1.1.1.cmml" type="float" xref="A4.T2.12.12.12.1.m1.1.1">1.579</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.12.12.12.1.m1.1c">1.579</annotation><annotation encoding="application/x-llamapun" id="A4.T2.12.12.12.1.m1.1d">1.579</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.15.15.19.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T2.15.15.19.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.15.15.19.2.1.1" style="position:relative; bottom:1.0pt;">Inference Modes</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T2.15.15.19.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:0.0pt;height:10.0pt;background:black;display:inline-block;"></span></td>
</tr>
<tr class="ltx_tr" id="A4.T2.13.13.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="A4.T2.13.13.13.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Multipoint Forecasting (<span class="ltx_text ltx_font_italic" id="A4.T2.13.13.13.2.1">Baseline</span>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T2.13.13.13.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.153" class="ltx_Math" display="inline" id="A4.T2.13.13.13.1.m1.1"><semantics id="A4.T2.13.13.13.1.m1.1a"><mn id="A4.T2.13.13.13.1.m1.1.1" xref="A4.T2.13.13.13.1.m1.1.1.cmml">1.153</mn><annotation-xml encoding="MathML-Content" id="A4.T2.13.13.13.1.m1.1b"><cn id="A4.T2.13.13.13.1.m1.1.1.cmml" type="float" xref="A4.T2.13.13.13.1.m1.1.1">1.153</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.13.13.13.1.m1.1c">1.153</annotation><annotation encoding="application/x-llamapun" id="A4.T2.13.13.13.1.m1.1d">1.153</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.14.14.14">
<td class="ltx_td ltx_align_left" id="A4.T2.14.14.14.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Autoregressive Forecasting</td>
<td class="ltx_td ltx_align_center" id="A4.T2.14.14.14.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="2.044" class="ltx_Math" display="inline" id="A4.T2.14.14.14.1.m1.1"><semantics id="A4.T2.14.14.14.1.m1.1a"><mn id="A4.T2.14.14.14.1.m1.1.1" xref="A4.T2.14.14.14.1.m1.1.1.cmml">2.044</mn><annotation-xml encoding="MathML-Content" id="A4.T2.14.14.14.1.m1.1b"><cn id="A4.T2.14.14.14.1.m1.1.1.cmml" type="float" xref="A4.T2.14.14.14.1.m1.1.1">2.044</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.14.14.14.1.m1.1c">2.044</annotation><annotation encoding="application/x-llamapun" id="A4.T2.14.14.14.1.m1.1d">2.044</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A4.T2.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_b" id="A4.T2.15.15.15.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Ensemble Forecasting</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T2.15.15.15.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><math alttext="1.558" class="ltx_Math" display="inline" id="A4.T2.15.15.15.1.m1.1"><semantics id="A4.T2.15.15.15.1.m1.1a"><mn id="A4.T2.15.15.15.1.m1.1.1" xref="A4.T2.15.15.15.1.m1.1.1.cmml">1.558</mn><annotation-xml encoding="MathML-Content" id="A4.T2.15.15.15.1.m1.1b"><cn id="A4.T2.15.15.15.1.m1.1.1.cmml" type="float" xref="A4.T2.15.15.15.1.m1.1.1">1.558</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.T2.15.15.15.1.m1.1c">1.558</annotation><annotation encoding="application/x-llamapun" id="A4.T2.15.15.15.1.m1.1d">1.558</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Evaluations on real datasets</h2>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Evaluation metric</h3>
<div class="ltx_para ltx_noindent" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.1">As part of our evaluation, we tested the performance of our model on real-world time series datasets alongside the synthetic data. The primary metric used was the seasonal Mean Absolute Scaled Error (MASE), which scales the forecast error by the mean absolute error of a seasonal naÃ¯ve forecast on the training data. The evaluation of Mamba4Cast on real-world datasets demonstrates the modelâ€™s capability to generalize and perform well in diverse, real-world forecasting scenarios. Detailed evaluations per dataset can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5.T3" title="Table 3 â€£ E.1 Evaluation metric â€£ Appendix E Evaluations on real datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">3</span></a>. We witnessed inconsistencies between the evaluations performed by AutoGluon in our setups and the ones reported in Chronos paper on datasets with daily frequency, specifically on "Covid Deaths." This resulted in the large gap witnessed on ForecastPFNâ€™s results reported here, since the modelâ€™s MASE evaluations are sourced from the Chronos paper. The results reported for Mamba4Cast per dataset are evaluated with the best model trained according to the procedures in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#S4.SS1" title="4.1 Training Details â€£ 4 Experiments â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
<figure class="ltx_table" id="A5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>MASE evaluations on all of the 17 datasets with the lower value the better. The best results per dataset are in bold and the second best results are underlined.</figcaption>
<br class="ltx_break ltx_centering"/>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T3.1" style="width:433.6pt;height:277.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-68.0pt,43.5pt) scale(0.761254940249819,0.761254940249819) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A5.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T3.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T3.1.1.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:0.0pt;height:15.0pt;background:black;display:inline-block;"></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="A5.T3.1.1.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.1.2.1">Zero-shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="A5.T3.1.1.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.1.3.1">Task-specific</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.1.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.1.1.4.1">Statistical Baseline</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.2.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:0.0pt;height:15.0pt;background:black;display:inline-block;"></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.2.1" style="position:relative; bottom:3.0pt;">Mamba4Cast</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.3.1" style="position:relative; bottom:3.0pt;">Chronos-B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.4.1" style="position:relative; bottom:3.0pt;">Chronos-S</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.5.1" style="position:relative; bottom:3.0pt;">ForecastPFN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.6.1" style="position:relative; bottom:3.0pt;">DeepAR</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.7.1" style="position:relative; bottom:3.0pt;">AutoARIMA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.2.2.8" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.2.8.1" style="position:relative; bottom:3.0pt;">S-Naive</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.3.3.1.1" style="position:relative; bottom:3.0pt;">Dataset</span></th>
<td class="ltx_td" id="A5.T3.1.1.3.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="A5.T3.1.1.3.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="A5.T3.1.1.3.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="A5.T3.1.1.3.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="A5.T3.1.1.3.3.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="A5.T3.1.1.3.3.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="A5.T3.1.1.3.3.8" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A5.T3.1.1.4.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Car Parts</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.061</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.4.4.4.1">0.817</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.657</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.4.4.6.1">0.747</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.180</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A5.T3.1.1.4.4.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.127</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.5.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">CIF 2016</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.5.5.2.1">0.925</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.5.5.3.1">0.995</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.016</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.558</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.597</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.062</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.5.5.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.289</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.6.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Covid Deaths</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.6.6.2.1">5.926</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.461</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.376</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">91.515</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.917</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.6.6.7.1">6.059</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.6.6.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">8.977</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.7.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">ERCOT Load</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.657</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.7.7.3.1">0.521</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.7.7.4.1">0.560</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">3.975</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.429</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.112</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.7.7.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.751</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.8.8.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Exchange Rate</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.8.8.2.1">1.329</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.388</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.436</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">7.583</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.214</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.8.8.7.1">1.187</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.8.8.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.460</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.9.9.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">FRED-MD</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.524</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.9.9.3.1">0.399</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.9.9.4.1">0.399</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.621</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.588</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.519</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.9.9.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.935</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.10.10.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Hospital</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.10.10.2.1">0.806</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.815</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.814</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.775</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.10.10.6.1">0.775</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.836</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.10.10.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.921</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.11.11.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">M1 (Monthly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.11.11.2.1">1.100</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.126</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.171</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.172</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.11.11.6.1">1.102</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.239</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.11.11.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.314</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.12.12.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">M1 (Quarterly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.12.12.2.1">1.695</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.778</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.824</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">9.931</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.784</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.12.12.7.1">1.766</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.12.12.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.078</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.13.13.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">M3 (Monthly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.13.13.2.1">0.849</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.13.13.3.1">0.866</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.890</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.240</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.056</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.033</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.13.13.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.146</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.14.14.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">M3 (Quarterly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.251</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.14.14.3.1">1.210</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.285</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">10.176</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.14.14.6.1">1.178</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.323</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.14.14.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.425</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.15.15.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">NN5 (Daily)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.833</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.15.15.3.1">0.809</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.834</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.375</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.15.15.6.1">0.793</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.832</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.15.15.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.952</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.16.16.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">NN5 (Weekly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.956</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.16.16.3.1">0.942</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.950</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.349</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.16.16.6.1">0.861</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.700</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.16.16.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.063</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.17.17.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Tourism (Monthly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.17.17.2.1">1.567</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.836</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.936</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">4.348</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.17.17.6.1">1.430</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.692</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.17.17.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.631</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.18.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.18.18.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Tourism (Quarterly)</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.746</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.799</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.770</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">5.595</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.18.18.6.1">1.686</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.784</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.18.18.8" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.18.18.8.1">1.699</span></td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.19.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A5.T3.1.1.19.19.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Traffic</th>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.120</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.19.19.3.1">0.370</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.19.19.4.1">0.380</span></td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.909</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.482</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1.327</td>
<td class="ltx_td ltx_align_center" id="A5.T3.1.1.19.19.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.753</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="A5.T3.1.1.20.20.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Weather</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.726</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.20.20.3.1">0.589</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="A5.T3.1.1.20.20.4.1">0.627</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">2.003</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.769</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.705</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A5.T3.1.1.20.20.8" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.813</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.2 </span>Qualitative analysis</h3>
<div class="ltx_para ltx_noindent" id="A5.SS2.p1">
<p class="ltx_p" id="A5.SS2.p1.1">An impartial evaluation in time series forecasting applications favors a qualitative evaluation over the datasets in question, to guarantee adequate behavior for point forecasting. For this sake, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.09385v1#A5.F5" title="Figure 5 â€£ E.2 Qualitative analysis â€£ Appendix E Evaluations on real datasets â€£ Mamba4Cast: Efficient Zero-Shot Time Series Forecasting with State Space Models"><span class="ltx_text ltx_ref_tag">5</span></a> demonstrates Mamba4Castâ€™s ability to capture diverse patterns exemplified in the real-life datasets.</p>
</div>
<figure class="ltx_figure" id="A5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1298" id="A5.F5.g1" src="x7.png" width="814"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Qualitative analysis of real-world datasets evaluated by Mamba4Cast. Blue denotes the ground-truth, red the prediction.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Oct 12 05:59:48 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
