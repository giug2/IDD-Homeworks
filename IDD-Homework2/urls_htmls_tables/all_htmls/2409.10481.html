<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.10481] Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance</title><meta property="og:description" content="3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to distinct application scenarios. These assumptions limit their use when acquisition conditions, such as the subject’s distance from …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.10481">

<!--Generated on Sat Oct  5 22:32:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="3D face reconstruction Authentication Surveillance">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\svgsetup</span>
<p id="p1.2" class="ltx_p">inkscapelatex=false





</p>
</div>
<span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>University of Cagliari, Cagliari, Italy
<span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{simonem.lac,sara.concas90c,roberto.casula,giulia.orru,marcialis}@unica.it</span></span></span> </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Autonomous University of Madrid, Madrid, Spain 
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>{ruben.tolosana,julian.fierrez}@uam.es</span></span></span> </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Masaryk University, Brno, Czech Republic 
<br class="ltx_break"><span id="id3.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>drahansky@sci.muni.cz</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simone Maurizio La Cava 
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-6344-1845" title="ORCID identifier" class="ltx_ref">0000-0002-6344-1845</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sara Concas
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-8114-0686" title="ORCID identifier" class="ltx_ref">0000-0001-8114-0686</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruben Tolosana
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9393-3066" title="ORCID identifier" class="ltx_ref">0000-0002-9393-3066</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Roberto Casula
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-3810-5935" title="ORCID identifier" class="ltx_ref">0000-0003-3810-5935</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giulia Orrù
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7802-2483" title="ORCID identifier" class="ltx_ref">0000-0002-7802-2483</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Drahansky
</span><span class="ltx_author_notes">33
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-9321-7385" title="ORCID identifier" class="ltx_ref">0000-0002-9321-7385</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Julian Fierrez
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-6343-5656" title="ORCID identifier" class="ltx_ref">0000-0002-6343-5656</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gian Luca Marcialis
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-8719-9643" title="ORCID identifier" class="ltx_ref">0000-0002-8719-9643</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to distinct application scenarios. These assumptions limit their use when acquisition conditions, such as the subject’s distance from the camera or the camera’s characteristics, are different than expected, as typically happens in video surveillance. Additionally, 3DFR algorithms follow various strategies to address the reconstruction of a 3D shape from 2D data, such as statistical model fitting, photometric stereo, or deep learning. In the present study, we explore the application of three 3DFR algorithms representative of the SOTA, employing each one as the template set generator for a face verification system. The scores provided by each system are combined by score-level fusion.
We show that the complementarity induced by different 3DFR algorithms improves performance when tests are conducted at never-seen-before distances from the camera and camera characteristics (cross-distance and cross-camera settings), thus encouraging further investigations on multiple 3DFR-based approaches.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>3D face reconstruction Authentication Surveillance
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In the last few years, much attention has been paid to the generation of face synthetic images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, in particular, 3D data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>. The acquisition of 3D data has been proven to be robust to adverse factors of uncontrolled environments, such as unfavorable illumination conditions and non-frontal poses of the face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>.
Moreover, high accuracy and efficiency can be achieved when comparing faces due to the complementary information of shape and texture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, in comparison to standard 2D images, the acquisition of such 3D data requires a much more complex enrolment process and expensive hardware <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.
This is the main reason why current face recognition technology is still mostly based on the acquisition of 2D face images, considering popular and cost-effective 2D acquisition devices.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Approaches based on 3D face reconstruction (3DFR) from 2D images and videos can be a good solution to overcome the limits of 2D data, combining the ease of acquiring 2D data with the robustness of 3D facial models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. An example of this can be seen in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
3DFR algorithms have proven to be particularly suitable in video surveillance scenarios, where a face acquired in unconstrained and different acquisition settings, even with arbitrary pose and distance, is compared to the identity using a reference image, <em id="S1.p2.1.1" class="ltx_emph">e.g</em><span id="S1.p2.1.2" class="ltx_text ltx_font_italic">.<span id="S1.p2.1.2.1" class="ltx_text"></span></span>, mugshot or identity document <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/3DreconstructionExample.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="131" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Example of 3D face reconstruction from a single 2D image using EOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, it is important to highlight that each 3DFR algorithm is usually designed for a specific scenario, such as recognition under certain environmental and technological conditions, or even for applications not strictly related to facial recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Therefore, they are not required to generalize well in other contexts. Accordingly, the designer "maximizes" specific characteristics, such as fine or grained details or even single facial components, useful for the specific application scenario. With this in mind, in the present study, we intend to answer the following research question: is it possible to improve face recognition performance in video surveillance scenarios through the fusion of the complementary information provided by different 3DFR algorithms?</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to answer this research question, we propose to jointly use different 3DFR algorithms for individually aiding the training of multiple classifiers, finally adopting a score-level fusion rule to moderate their matching score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> before making the final decision in a verification task, <em id="S1.p4.1.1" class="ltx_emph">i.e</em><span id="S1.p4.1.2" class="ltx_text ltx_font_italic">.<span id="S1.p4.1.2.1" class="ltx_text"></span></span>, determine whether the subject identity associated with the reference data is the same represented in the probe data based on the resulting <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">a posteriori</span> probability.
To summarize, the main contributions of the present study are the following: <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">(i)</span> the analysis and exploitation of the complementary information provided by different 3DFR algorithms (EOS, 3DDFA v2, and NextFace), not strictly proposed for recognition purposes, to enhance a video surveillance scenario using two distinct deep-learning systems with significantly different architectural complexities; <span id="S1.p4.1.5" class="ltx_text ltx_font_italic">(ii)</span> the exploration of the effectiveness, advantages, and disadvantages of various score-level fusion methods to take advantage of the complementary information provided by different 3DFR algorithms; and <span id="S1.p4.1.6" class="ltx_text ltx_font_italic">(iii)</span> the assessment of the robustness of the proposed approach in challenging conditions, <em id="S1.p4.1.7" class="ltx_emph">i.e</em><span id="S1.p4.1.8" class="ltx_text ltx_font_italic">.<span id="S1.p4.1.8.1" class="ltx_text"></span></span>, when dealing with data acquired in a different setting, concerning acquisition distance and surveillance camera, from the one considered for the system design.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rest of the paper is organized as follows. Section <a href="#S2" title="2 Related Works ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> discusses state-of-the-art 3D face reconstruction in video surveillance and fusion methods in face recognition. Section <a href="#S3" title="3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes our proposed approach based on 3DFR algorithms and fusion methods. Section <a href="#S4" title="4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports the experimental protocol considered to investigate the effectiveness in a surveillance scenario. The experimental results are reported in Section <a href="#S5" title="5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Finally, conclusions are drawn in Section <a href="#S6" title="6 Conclusions ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Exploiting 3DFR for Face Recognition</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">3DFR has been mainly proposed to increase the robustness of face recognition to faces acquired with various view angles, as this is the typical acquisition observed in unconstrained scenarios, with non-frontal and looking-down probe faces due to the non-cooperation of the subjects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
However, the performance improvement between 2D and 3DFR strongly depends on the approaches chosen for its application to the face recognition task. These are usually divided into two main categories, namely, model- and view-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The first one synthesizes frontal faces from the 2D images containing non-frontal views. The normalized (or "frontalized") faces are then compared to the frontal ones to determine the subjects’ identity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
This category is prone to produce textural artifacts in the synthesized frontal images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>; thus, it is usually considered in the so-called face identification task rather than for highly accurate authentication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The second one adapts the 2D images containing frontal faces to non-frontal ones; in other words, lateral views derive from the 3D template (or model) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Additionally, the 3D facial model can be projected to various poses in the 2D domain to enhance the representation capability of each subject, considering them as synthesized templates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
In general, view-based approaches are more expensive than model-based ones in terms of computational and storage costs. Thus, they are usually used in the verification task when high reliability is required <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>3DFR in Video Surveillance Scenarios</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Video surveillance scenarios are characterized by faces captured at an extensive range of lighting, pose, and scale due to environmental conditions and the subject’s cooperation level. In other words, we must rely on uncalibrated images.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The 3D reconstruction from uncalibrated images is an inherently ill-posed problem: the facial geometry, the pose of the head, and its texture must be recovered from a single picture, leading to an undetermined problem, while different 3D faces could generate the same 2D image.
Therefore, the research community has proposed that prior knowledge should be included to estimate the actual 3D geometry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. This information can be added through three main ways: photometric stereo, statistical model fitting, and deep learning.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Through the first approach, a 3D template is combined with photometric stereo methods to estimate the facial surface model. Despite the capability of acquiring fine details, the reconstruction is typically performed through multiple images, thus further constraining the problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Statistical model fitting consists of adapting a 3D facial model built from a set of 3D facial scans to the input images. In this context, the most commonly used statistical model is the 3DMM (3D Morphable Model), which consists of a shape model and, optionally, an albedo model, separately constructed using Principal Component Analysis (PCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Despite providing generally convincing results and lower computational complexity, this approach focuses primarily on global characteristics rather than fine details <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">Similarly, the deep learning approach maps 2D to 3D information through a trained deep neural network. This approach provides more detailed results than the previous one at the cost of requiring more 3D scans to train the network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Each of the approaches presented above performs 3DFR from uncalibrated images, as those typically acquired in video surveillance scenarios, following assumptions to make an intrinsically ill-posed problem into a manageable one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. These assumptions and related pros and cons point out an intrinsic complementarity. Moreover, some studies highlight that it is possible to reconstruct highly detailed 3D faces even with a single image by combining the prior knowledge of the global facial shape encoded in the 3DMM and refining it through a photometric or a deep learning approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. This further supports the hypothesis of the complementarity of the 3DFR methods.
Therefore, combining face recognition systems based on multiple 3DFR algorithms could increase the generalization capabilities of the systems. To achieve this, we propose fusion techniques to exploit the complementary information provided by different 3DFR algorithms.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Ensemble and Fusion Methods for Face Recognition</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Ensemble methods and multi-modal or uni-modal fusion approaches are constantly being exploited in many fields of pattern recognition to improve the ability to generalize and deal with intra-class variations and inter-class similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. In biometrics, fusion can be performed at various levels, including sensor, feature, score, and decision levels <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Among others, score-level fusion allows the exploitation of several sources of information without increasing the system’s complexity (as in the case of sensor-level and feature-level fusion) and without relying only on the binary outcome, as in the case of decision-level fusion. A comprehensive set of previous studies supports this since the earlier attempts at face recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a graphical representation of the proposed method to improve the performance of face verification in video surveillance scenarios, <em id="S3.p1.1.1" class="ltx_emph">i.e</em><span id="S3.p1.1.2" class="ltx_text ltx_font_italic">.<span id="S3.p1.1.2.1" class="ltx_text"></span></span>, determine whether the identity in a surveillance image (<em id="S3.p1.1.3" class="ltx_emph">i.e</em><span id="S3.p1.1.4" class="ltx_text ltx_font_italic">.<span id="S3.p1.1.4.1" class="ltx_text"></span></span>, probe) matches that represented in the reference data (<em id="S3.p1.1.5" class="ltx_emph">i.e</em><span id="S3.p1.1.6" class="ltx_text ltx_font_italic">.<span id="S3.p1.1.6.1" class="ltx_text"></span></span>, mugshot or template). In the following, we summarize the main modules.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/proposed_method.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.5.2" class="ltx_text" style="font-size:90%;">Proposed method. The synthetic view generation produces a 2D image from the 3D template (<em id="S3.F2.5.2.1" class="ltx_emph">i.e</em><span id="S3.F2.5.2.2" class="ltx_text ltx_font_italic">.<span id="S3.F2.5.2.2.1" class="ltx_text"></span></span>, with various view angles obtained from gallery enlargement during the system’s training, only in frontal view during inference). The Siamese Neural Networks (same architecture) provide complementary information as they are enhanced through different 3DFR algorithms (EOS, 3DDFA v2, or NextFace). The example images are from the SCface database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">3DFR methods:</span> We have chosen three types of state-of-the-art 3DFR algorithms, each representative of one or a combination of the previously described approaches for reconstructing the 3D templates from high-quality reference data (<em id="S3.p2.1.2" class="ltx_emph">i.e</em><span id="S3.p2.1.3" class="ltx_text ltx_font_italic">.<span id="S3.p2.1.3.1" class="ltx_text"></span></span>, frontal mugshot images):</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">EOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>: based on 3DMM and proposed for reconstructing 3D faces from videos and images in time-critical applications
(Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, b)<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Original implementation from <a target="_blank" href="https://github.com/patrikhuber/eos" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/patrikhuber/eos</a></span></span></span>.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">3DDFA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>: based on a lightweight network for regressing the 3DMM parameters and proposed for 3D dense face alignment (Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, c)<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Original implementation from <a target="_blank" href="https://github.com/cleardusk/3DDFA_V2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/cleardusk/3DDFA_V2</a></span></span></span>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">NextFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>: based on the combination of a statistical 3DMM and a photometric approach for making 3D reconstruction robust to light conditions (Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, d)<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Original implementation from <a target="_blank" href="https://github.com/abdallahdib/NextFace" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/abdallahdib/NextFace</a></span></span></span>.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/3Dmodels.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Examples of personalized 3D templates generated from a mugshot in the SCface database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> (a), through EOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> (b), 3DDFA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> (c), and NextFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (d).</span></figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">3DFR-based enhancement strategy:</span> We focus on view-based approaches for enhancing face verification through 3DFR. In particular, we train multiple face recognition systems by considering a gallery enlargement strategy through a single 3DFR method for each neural network to make the system robust to pose variations (Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Specifically, we use it on each 3D template generated from training mugshots to project the face in multiple view angles (Figure <a href="#S3.F4" title="Figure 4 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). Then, we train each neural network with all view representations to aid the task of learning how to extract useful information for face verification from non-frontal poses. Concerning the evaluation of each face recognition system at the inference stage, we only compare the frontal face of each subject with the corresponding set of probe images.
The computational cost introduced by the gallery enlargement strategy is mainly offline, thus representing a minor issue in many of the application scenarios to which this contribution is intended <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Gallery enlargement from a single personalized 3D template.</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l0" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l0.1.1.1" class="ltx_text" style="font-size:80%;">0:</span></span><span id="alg1.l0.2" class="ltx_text" style="font-size:70%;">  3D template
</span>
</div>
<div id="alg1.l0a" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l0a.1.1.1" class="ltx_text" style="font-size:80%;">0:</span></span><span id="alg1.l0a.2" class="ltx_text" style="font-size:70%;">  2D views of the 3D template
</span>
</div>
<div id="alg1.l1" class="ltx_listingline">
<span id="alg1.l1.1" class="ltx_text" style="font-size:70%;">  </span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="N\leftarrow 30" display="inline"><semantics id="alg1.l1.m1.1a"><mrow id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml"><mi mathsize="70%" id="alg1.l1.m1.1.1.2" xref="alg1.l1.m1.1.1.2.cmml">N</mi><mo mathsize="70%" stretchy="false" id="alg1.l1.m1.1.1.1" xref="alg1.l1.m1.1.1.1.cmml">←</mo><mn mathsize="70%" id="alg1.l1.m1.1.1.3" xref="alg1.l1.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><apply id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1"><ci id="alg1.l1.m1.1.1.1.cmml" xref="alg1.l1.m1.1.1.1">←</ci><ci id="alg1.l1.m1.1.1.2.cmml" xref="alg1.l1.m1.1.1.2">𝑁</ci><cn type="integer" id="alg1.l1.m1.1.1.3.cmml" xref="alg1.l1.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">N\leftarrow 30</annotation></semantics></math><span id="alg1.l1.2" class="ltx_text" style="font-size:70%;">         {Maximum absolute azimuth}
</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span id="alg1.l2.1" class="ltx_text" style="font-size:70%;">  </span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="M\leftarrow 30" display="inline"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi mathsize="70%" id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">M</mi><mo mathsize="70%" stretchy="false" id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">←</mo><mn mathsize="70%" id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml">30</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><ci id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1">←</ci><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝑀</ci><cn type="integer" id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">M\leftarrow 30</annotation></semantics></math><span id="alg1.l2.2" class="ltx_text" style="font-size:70%;">        {Maximum absolute elevation}
</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span id="alg1.l3.1" class="ltx_text" style="font-size:70%;">  </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="\textit{offset}\leftarrow 10" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2a.cmml">offset</mtext><mo mathsize="70%" stretchy="false" id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">←</mo><mn mathsize="70%" id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><ci id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1">←</ci><ci id="alg1.l3.m1.1.1.2a.cmml" xref="alg1.l3.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">offset</mtext></ci><cn type="integer" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\textit{offset}\leftarrow 10</annotation></semantics></math><span id="alg1.l3.2" class="ltx_text" style="font-size:70%;">      {Angle offset}
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span id="alg1.l4.1" class="ltx_text" style="font-size:70%;">  </span><math id="alg1.l4.m1.1" class="ltx_Math" alttext="\textit{el}\leftarrow-M" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2a.cmml">el</mtext><mo mathsize="70%" stretchy="false" id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">←</mo><mrow id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml"><mo mathsize="70%" id="alg1.l4.m1.1.1.3a" xref="alg1.l4.m1.1.1.3.cmml">−</mo><mi mathsize="70%" id="alg1.l4.m1.1.1.3.2" xref="alg1.l4.m1.1.1.3.2.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><ci id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1">←</ci><ci id="alg1.l4.m1.1.1.2a.cmml" xref="alg1.l4.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">el</mtext></ci><apply id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3"><minus id="alg1.l4.m1.1.1.3.1.cmml" xref="alg1.l4.m1.1.1.3"></minus><ci id="alg1.l4.m1.1.1.3.2.cmml" xref="alg1.l4.m1.1.1.3.2">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\textit{el}\leftarrow-M</annotation></semantics></math><span id="alg1.l4.2" class="ltx_text" style="font-size:70%;">       {Initial elevation}
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span id="alg1.l5.1" class="ltx_text" style="font-size:70%;">  </span><span id="alg1.l5.2" class="ltx_text ltx_font_bold" style="font-size:70%;">while</span><span id="alg1.l5.3" class="ltx_text" style="font-size:70%;"> </span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="\textit{el}\leq M" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2a.cmml">el</mtext><mo mathsize="70%" id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">≤</mo><mi mathsize="70%" id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><leq id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1"></leq><ci id="alg1.l5.m1.1.1.2a.cmml" xref="alg1.l5.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">el</mtext></ci><ci id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">\textit{el}\leq M</annotation></semantics></math><span id="alg1.l5.4" class="ltx_text" style="font-size:70%;"> </span><span id="alg1.l5.5" class="ltx_text ltx_font_bold" style="font-size:70%;">do</span><span id="alg1.l5.6" class="ltx_text" style="font-size:70%;">


</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span id="alg1.l6.1" class="ltx_text" style="font-size:70%;">     </span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="\textit{az}\leftarrow-N" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2a.cmml">az</mtext><mo mathsize="70%" stretchy="false" id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">←</mo><mrow id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"><mo mathsize="70%" id="alg1.l6.m1.1.1.3a" xref="alg1.l6.m1.1.1.3.cmml">−</mo><mi mathsize="70%" id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><ci id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1">←</ci><ci id="alg1.l6.m1.1.1.2a.cmml" xref="alg1.l6.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">az</mtext></ci><apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"><minus id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3"></minus><ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">\textit{az}\leftarrow-N</annotation></semantics></math><span id="alg1.l6.2" class="ltx_text" style="font-size:70%;">       {Initial azimuth}
</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span id="alg1.l7.1" class="ltx_text" style="font-size:70%;">     </span><span id="alg1.l7.2" class="ltx_text ltx_font_bold" style="font-size:70%;">while</span><span id="alg1.l7.3" class="ltx_text" style="font-size:70%;"> </span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="\textit{az}\leq N" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2a.cmml">az</mtext><mo mathsize="70%" id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">≤</mo><mi mathsize="70%" id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><leq id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1"></leq><ci id="alg1.l7.m1.1.1.2a.cmml" xref="alg1.l7.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">az</mtext></ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\textit{az}\leq N</annotation></semantics></math><span id="alg1.l7.4" class="ltx_text" style="font-size:70%;"> </span><span id="alg1.l7.5" class="ltx_text ltx_font_bold" style="font-size:70%;">do</span><span id="alg1.l7.6" class="ltx_text" style="font-size:70%;">


</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span id="alg1.l8.1" class="ltx_text" style="font-size:70%;">        Project 3D template in current </span><span id="alg1.l8.2" class="ltx_text ltx_markedasmath ltx_font_italic" style="font-size:70%;">az</span><span id="alg1.l8.3" class="ltx_text" style="font-size:70%;"> and </span><span id="alg1.l8.4" class="ltx_text ltx_markedasmath ltx_font_italic" style="font-size:70%;">el</span><span id="alg1.l8.5" class="ltx_text" style="font-size:70%;"> 
</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span id="alg1.l9.1" class="ltx_text" style="font-size:70%;">        </span><math id="alg1.l9.m1.1" class="ltx_Math" alttext="\textit{az}\leftarrow\textit{az}+\textit{offset}" display="inline"><semantics id="alg1.l9.m1.1a"><mrow id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l9.m1.1.1.2" xref="alg1.l9.m1.1.1.2a.cmml">az</mtext><mo mathsize="70%" stretchy="false" id="alg1.l9.m1.1.1.1" xref="alg1.l9.m1.1.1.1.cmml">←</mo><mrow id="alg1.l9.m1.1.1.3" xref="alg1.l9.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l9.m1.1.1.3.2" xref="alg1.l9.m1.1.1.3.2a.cmml">az</mtext><mo mathsize="70%" id="alg1.l9.m1.1.1.3.1" xref="alg1.l9.m1.1.1.3.1.cmml">+</mo><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l9.m1.1.1.3.3" xref="alg1.l9.m1.1.1.3.3a.cmml">offset</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><apply id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1"><ci id="alg1.l9.m1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1">←</ci><ci id="alg1.l9.m1.1.1.2a.cmml" xref="alg1.l9.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l9.m1.1.1.2.cmml" xref="alg1.l9.m1.1.1.2">az</mtext></ci><apply id="alg1.l9.m1.1.1.3.cmml" xref="alg1.l9.m1.1.1.3"><plus id="alg1.l9.m1.1.1.3.1.cmml" xref="alg1.l9.m1.1.1.3.1"></plus><ci id="alg1.l9.m1.1.1.3.2a.cmml" xref="alg1.l9.m1.1.1.3.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l9.m1.1.1.3.2.cmml" xref="alg1.l9.m1.1.1.3.2">az</mtext></ci><ci id="alg1.l9.m1.1.1.3.3a.cmml" xref="alg1.l9.m1.1.1.3.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l9.m1.1.1.3.3.cmml" xref="alg1.l9.m1.1.1.3.3">offset</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">\textit{az}\leftarrow\textit{az}+\textit{offset}</annotation></semantics></math><span id="alg1.l9.2" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span id="alg1.l10.1" class="ltx_text" style="font-size:70%;">     </span><span id="alg1.l10.2" class="ltx_text ltx_font_bold" style="font-size:70%;">end</span><span id="alg1.l10.3" class="ltx_text" style="font-size:70%;"> </span><span id="alg1.l10.4" class="ltx_text ltx_font_bold" style="font-size:70%;">while</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span id="alg1.l11.1" class="ltx_text" style="font-size:70%;">     </span><math id="alg1.l11.m1.1" class="ltx_Math" alttext="\textit{el}\leftarrow\textit{el}+\textit{offset}" display="inline"><semantics id="alg1.l11.m1.1a"><mrow id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l11.m1.1.1.2" xref="alg1.l11.m1.1.1.2a.cmml">el</mtext><mo mathsize="70%" stretchy="false" id="alg1.l11.m1.1.1.1" xref="alg1.l11.m1.1.1.1.cmml">←</mo><mrow id="alg1.l11.m1.1.1.3" xref="alg1.l11.m1.1.1.3.cmml"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l11.m1.1.1.3.2" xref="alg1.l11.m1.1.1.3.2a.cmml">el</mtext><mo mathsize="70%" id="alg1.l11.m1.1.1.3.1" xref="alg1.l11.m1.1.1.3.1.cmml">+</mo><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l11.m1.1.1.3.3" xref="alg1.l11.m1.1.1.3.3a.cmml">offset</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><apply id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1"><ci id="alg1.l11.m1.1.1.1.cmml" xref="alg1.l11.m1.1.1.1">←</ci><ci id="alg1.l11.m1.1.1.2a.cmml" xref="alg1.l11.m1.1.1.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l11.m1.1.1.2.cmml" xref="alg1.l11.m1.1.1.2">el</mtext></ci><apply id="alg1.l11.m1.1.1.3.cmml" xref="alg1.l11.m1.1.1.3"><plus id="alg1.l11.m1.1.1.3.1.cmml" xref="alg1.l11.m1.1.1.3.1"></plus><ci id="alg1.l11.m1.1.1.3.2a.cmml" xref="alg1.l11.m1.1.1.3.2"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l11.m1.1.1.3.2.cmml" xref="alg1.l11.m1.1.1.3.2">el</mtext></ci><ci id="alg1.l11.m1.1.1.3.3a.cmml" xref="alg1.l11.m1.1.1.3.3"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="alg1.l11.m1.1.1.3.3.cmml" xref="alg1.l11.m1.1.1.3.3">offset</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">\textit{el}\leftarrow\textit{el}+\textit{offset}</annotation></semantics></math><span id="alg1.l11.2" class="ltx_text" style="font-size:70%;">
</span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span id="alg1.l12.1" class="ltx_text" style="font-size:70%;">  </span><span id="alg1.l12.2" class="ltx_text ltx_font_bold" style="font-size:70%;">end</span><span id="alg1.l12.3" class="ltx_text" style="font-size:70%;"> </span><span id="alg1.l12.4" class="ltx_text ltx_font_bold" style="font-size:70%;">while</span>
</div>
</div>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/enlargement001.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="127" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">Example of gallery enlargement from personalized 3D template obtained from a mugshot in the SCface database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> using the EOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> 3DFR algorithm.</span></figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Face recognition methods:</span> We consider two state-of-the-art deep learning networks, XceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, with different computational complexities to simulate various application scenarios. In particular, we selected a Siamese architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, which has demonstrated accurate results in face recognition using low-resolution images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.4" class="ltx_p">In order to determine the <span id="S3.p5.4.1" class="ltx_text ltx_font_italic">a posteriori</span> probability <math id="S3.p5.1.m1.3" class="ltx_Math" alttext="P(\textrm{match}|X,Y)" display="inline"><semantics id="S3.p5.1.m1.3a"><mrow id="S3.p5.1.m1.3.3" xref="S3.p5.1.m1.3.3.cmml"><mi id="S3.p5.1.m1.3.3.3" xref="S3.p5.1.m1.3.3.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.p5.1.m1.3.3.2" xref="S3.p5.1.m1.3.3.2.cmml">​</mo><mrow id="S3.p5.1.m1.3.3.1.1" xref="S3.p5.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.p5.1.m1.3.3.1.1.2" xref="S3.p5.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.p5.1.m1.3.3.1.1.1" xref="S3.p5.1.m1.3.3.1.1.1.cmml"><mtext id="S3.p5.1.m1.3.3.1.1.1.2" xref="S3.p5.1.m1.3.3.1.1.1.2a.cmml">match</mtext><mo fence="false" id="S3.p5.1.m1.3.3.1.1.1.1" xref="S3.p5.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.p5.1.m1.3.3.1.1.1.3.2" xref="S3.p5.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">X</mi><mo id="S3.p5.1.m1.3.3.1.1.1.3.2.1" xref="S3.p5.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.p5.1.m1.2.2" xref="S3.p5.1.m1.2.2.cmml">Y</mi></mrow></mrow><mo stretchy="false" id="S3.p5.1.m1.3.3.1.1.3" xref="S3.p5.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.3b"><apply id="S3.p5.1.m1.3.3.cmml" xref="S3.p5.1.m1.3.3"><times id="S3.p5.1.m1.3.3.2.cmml" xref="S3.p5.1.m1.3.3.2"></times><ci id="S3.p5.1.m1.3.3.3.cmml" xref="S3.p5.1.m1.3.3.3">𝑃</ci><apply id="S3.p5.1.m1.3.3.1.1.1.cmml" xref="S3.p5.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.p5.1.m1.3.3.1.1.1.1.cmml" xref="S3.p5.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S3.p5.1.m1.3.3.1.1.1.2a.cmml" xref="S3.p5.1.m1.3.3.1.1.1.2"><mtext id="S3.p5.1.m1.3.3.1.1.1.2.cmml" xref="S3.p5.1.m1.3.3.1.1.1.2">match</mtext></ci><list id="S3.p5.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.p5.1.m1.3.3.1.1.1.3.2"><ci id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">𝑋</ci><ci id="S3.p5.1.m1.2.2.cmml" xref="S3.p5.1.m1.2.2">𝑌</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.3c">P(\textrm{match}|X,Y)</annotation></semantics></math> between the representation obtained from the reconstructed 3D reference model <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.p5.2.m2.1a"><mi id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><ci id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">X</annotation></semantics></math> and the probe image <math id="S3.p5.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.p5.3.m3.1a"><mi id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><ci id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">Y</annotation></semantics></math> match, we calculate the similarity between such a pair of images using the Euclidean distance value <math id="S3.p5.4.m4.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S3.p5.4.m4.1a"><mi id="S3.p5.4.m4.1.1" xref="S3.p5.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.p5.4.m4.1b"><ci id="S3.p5.4.m4.1.1.cmml" xref="S3.p5.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m4.1c">d</annotation></semantics></math> between their feature embeddings (<em id="S3.p5.4.2" class="ltx_emph">i.e</em><span id="S3.p5.4.3" class="ltx_text ltx_font_italic">.<span id="S3.p5.4.3.1" class="ltx_text"></span></span>, the output of the Siamese Network) as follows:</p>
</div>
<div id="S3.p6" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="P(\textrm{match}|X,Y)=\frac{1}{d+1}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mtext mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2a.cmml">match</mtext><mo fence="false" mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">X</mi><mo mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.3.2.1" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml">,</mo><mi mathsize="90%" id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Y</mi></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml">=</mo><mfrac id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mn mathsize="90%" id="S3.E1.m1.3.3.3.2" xref="S3.E1.m1.3.3.3.2.cmml">1</mn><mrow id="S3.E1.m1.3.3.3.3" xref="S3.E1.m1.3.3.3.3.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.3.3.2" xref="S3.E1.m1.3.3.3.3.2.cmml">d</mi><mo mathsize="90%" id="S3.E1.m1.3.3.3.3.1" xref="S3.E1.m1.3.3.3.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E1.m1.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.cmml">1</mn></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><eq id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"></eq><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">𝑃</ci><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2a.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><mtext mathsize="90%" id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">match</mtext></ci><list id="S3.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑋</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑌</ci></list></apply></apply><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><divide id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3"></divide><cn type="integer" id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.2">1</cn><apply id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"><plus id="S3.E1.m1.3.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3.3.1"></plus><ci id="S3.E1.m1.3.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3.3.2">𝑑</ci><cn type="integer" id="S3.E1.m1.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">P(\textrm{match}|X,Y)=\frac{1}{d+1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">In particular, we estimate the <span id="S3.p7.1.1" class="ltx_text ltx_font_italic">a posteriori</span> probability through the previous formula to limit the range to <math id="S3.p7.1.m1.1" class="ltx_math_unparsed" alttext="]0,1]" display="inline"><semantics id="S3.p7.1.m1.1a"><mrow id="S3.p7.1.m1.1b"><mo stretchy="false" id="S3.p7.1.m1.1.1">]</mo><mn id="S3.p7.1.m1.1.2">0</mn><mo id="S3.p7.1.m1.1.3">,</mo><mn id="S3.p7.1.m1.1.4">1</mn><mo stretchy="false" id="S3.p7.1.m1.1.5">]</mo></mrow><annotation encoding="application/x-tex" id="S3.p7.1.m1.1c">]0,1]</annotation></semantics></math>.</p>
</div>
<div id="S3.p8" class="ltx_para">
<p id="S3.p8.1" class="ltx_p"><span id="S3.p8.1.1" class="ltx_text ltx_font_bold">Fusion methods:</span> To the best of our knowledge, the score-level fusion between face recognition systems enhanced through various 3DFR algorithms is still missing in the literature. Accordingly, we explore different non-parametric fusion methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> by applying a set of rules to the <span id="S3.p8.1.2" class="ltx_text ltx_font_italic">a posteriori</span> probability values, namely the scores predicted by the single Siamese Neural Networks. Hence, the final <span id="S3.p8.1.3" class="ltx_text ltx_font_italic">a posteriori</span> probability obtained from the comparison between a reference and a probe represents a combination of such scores.</p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.5" class="ltx_p">Here, we introduce a common notation to ease the understanding of such fusion rules. Let us consider the fusion of <math id="S3.p9.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p9.1.m1.1a"><mi id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><ci id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">N</annotation></semantics></math> classifier scores, where <math id="S3.p9.2.m2.3" class="ltx_Math" alttext="P_{i}(\textrm{match}|X,Y)" display="inline"><semantics id="S3.p9.2.m2.3a"><mrow id="S3.p9.2.m2.3.3" xref="S3.p9.2.m2.3.3.cmml"><msub id="S3.p9.2.m2.3.3.3" xref="S3.p9.2.m2.3.3.3.cmml"><mi id="S3.p9.2.m2.3.3.3.2" xref="S3.p9.2.m2.3.3.3.2.cmml">P</mi><mi id="S3.p9.2.m2.3.3.3.3" xref="S3.p9.2.m2.3.3.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.p9.2.m2.3.3.2" xref="S3.p9.2.m2.3.3.2.cmml">​</mo><mrow id="S3.p9.2.m2.3.3.1.1" xref="S3.p9.2.m2.3.3.1.1.1.cmml"><mo stretchy="false" id="S3.p9.2.m2.3.3.1.1.2" xref="S3.p9.2.m2.3.3.1.1.1.cmml">(</mo><mrow id="S3.p9.2.m2.3.3.1.1.1" xref="S3.p9.2.m2.3.3.1.1.1.cmml"><mtext id="S3.p9.2.m2.3.3.1.1.1.2" xref="S3.p9.2.m2.3.3.1.1.1.2a.cmml">match</mtext><mo fence="false" id="S3.p9.2.m2.3.3.1.1.1.1" xref="S3.p9.2.m2.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.p9.2.m2.3.3.1.1.1.3.2" xref="S3.p9.2.m2.3.3.1.1.1.3.1.cmml"><mi id="S3.p9.2.m2.1.1" xref="S3.p9.2.m2.1.1.cmml">X</mi><mo id="S3.p9.2.m2.3.3.1.1.1.3.2.1" xref="S3.p9.2.m2.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.p9.2.m2.2.2" xref="S3.p9.2.m2.2.2.cmml">Y</mi></mrow></mrow><mo stretchy="false" id="S3.p9.2.m2.3.3.1.1.3" xref="S3.p9.2.m2.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p9.2.m2.3b"><apply id="S3.p9.2.m2.3.3.cmml" xref="S3.p9.2.m2.3.3"><times id="S3.p9.2.m2.3.3.2.cmml" xref="S3.p9.2.m2.3.3.2"></times><apply id="S3.p9.2.m2.3.3.3.cmml" xref="S3.p9.2.m2.3.3.3"><csymbol cd="ambiguous" id="S3.p9.2.m2.3.3.3.1.cmml" xref="S3.p9.2.m2.3.3.3">subscript</csymbol><ci id="S3.p9.2.m2.3.3.3.2.cmml" xref="S3.p9.2.m2.3.3.3.2">𝑃</ci><ci id="S3.p9.2.m2.3.3.3.3.cmml" xref="S3.p9.2.m2.3.3.3.3">𝑖</ci></apply><apply id="S3.p9.2.m2.3.3.1.1.1.cmml" xref="S3.p9.2.m2.3.3.1.1"><csymbol cd="latexml" id="S3.p9.2.m2.3.3.1.1.1.1.cmml" xref="S3.p9.2.m2.3.3.1.1.1.1">conditional</csymbol><ci id="S3.p9.2.m2.3.3.1.1.1.2a.cmml" xref="S3.p9.2.m2.3.3.1.1.1.2"><mtext id="S3.p9.2.m2.3.3.1.1.1.2.cmml" xref="S3.p9.2.m2.3.3.1.1.1.2">match</mtext></ci><list id="S3.p9.2.m2.3.3.1.1.1.3.1.cmml" xref="S3.p9.2.m2.3.3.1.1.1.3.2"><ci id="S3.p9.2.m2.1.1.cmml" xref="S3.p9.2.m2.1.1">𝑋</ci><ci id="S3.p9.2.m2.2.2.cmml" xref="S3.p9.2.m2.2.2">𝑌</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.2.m2.3c">P_{i}(\textrm{match}|X,Y)</annotation></semantics></math> represents the score of the <math id="S3.p9.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p9.3.m3.1a"><mi id="S3.p9.3.m3.1.1" xref="S3.p9.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p9.3.m3.1b"><ci id="S3.p9.3.m3.1.1.cmml" xref="S3.p9.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.3.m3.1c">i</annotation></semantics></math>-th classifier when comparing <math id="S3.p9.4.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.p9.4.m4.1a"><mi id="S3.p9.4.m4.1.1" xref="S3.p9.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p9.4.m4.1b"><ci id="S3.p9.4.m4.1.1.cmml" xref="S3.p9.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.4.m4.1c">X</annotation></semantics></math> and <math id="S3.p9.5.m5.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.p9.5.m5.1a"><mi id="S3.p9.5.m5.1.1" xref="S3.p9.5.m5.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.p9.5.m5.1b"><ci id="S3.p9.5.m5.1.1.cmml" xref="S3.p9.5.m5.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.5.m5.1c">Y</annotation></semantics></math>. According to this notation, it is possible to compute the fusion scores through the following formulas that represent, in order, the simple average between the scores obtained from the single recognition systems, their maximum, and their minimum:</p>
</div>
<div id="S3.p10" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="avg\_score=\frac{1}{N}\sum\limits_{i=1}^{N}{P_{i}(\textrm{match}|X,Y)}" display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><mi mathsize="90%" id="S3.E2.m1.3.3.3.2" xref="S3.E2.m1.3.3.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1a" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.4" xref="S3.E2.m1.3.3.3.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1b" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S3.E2.m1.3.3.3.5" xref="S3.E2.m1.3.3.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1c" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.6" xref="S3.E2.m1.3.3.3.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1d" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.7" xref="S3.E2.m1.3.3.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1e" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.8" xref="S3.E2.m1.3.3.3.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1f" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.9" xref="S3.E2.m1.3.3.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.3.1g" xref="S3.E2.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E2.m1.3.3.3.10" xref="S3.E2.m1.3.3.3.10.cmml">e</mi></mrow><mo mathsize="90%" id="S3.E2.m1.3.3.2" xref="S3.E2.m1.3.3.2.cmml">=</mo><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mfrac id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.3.3.1.3.cmml"><mn mathsize="90%" id="S3.E2.m1.3.3.1.3.2" xref="S3.E2.m1.3.3.1.3.2.cmml">1</mn><mi mathsize="90%" id="S3.E2.m1.3.3.1.3.3" xref="S3.E2.m1.3.3.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><munderover id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E2.m1.3.3.1.1.2.2.2" xref="S3.E2.m1.3.3.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.2.2.3" xref="S3.E2.m1.3.3.1.1.2.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.3.3.1.1.2.2.3.2" xref="S3.E2.m1.3.3.1.1.2.2.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E2.m1.3.3.1.1.2.2.3.1" xref="S3.E2.m1.3.3.1.1.2.2.3.1.cmml">=</mo><mn mathsize="90%" id="S3.E2.m1.3.3.1.1.2.2.3.3" xref="S3.E2.m1.3.3.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E2.m1.3.3.1.1.2.3" xref="S3.E2.m1.3.3.1.1.2.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml"><msub id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E2.m1.3.3.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.3.2.cmml">P</mi><mi mathsize="90%" id="S3.E2.m1.3.3.1.1.1.3.3" xref="S3.E2.m1.3.3.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.3.3.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml"><mtext mathsize="90%" id="S3.E2.m1.3.3.1.1.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2a.cmml">match</mtext><mo fence="false" mathsize="90%" id="S3.E2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">X</mi><mo mathsize="90%" id="S3.E2.m1.3.3.1.1.1.1.1.1.3.2.1" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml">,</mo><mi mathsize="90%" id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">Y</mi></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E2.m1.3.3.1.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><eq id="S3.E2.m1.3.3.2.cmml" xref="S3.E2.m1.3.3.2"></eq><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><times id="S3.E2.m1.3.3.3.1.cmml" xref="S3.E2.m1.3.3.3.1"></times><ci id="S3.E2.m1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.2">𝑎</ci><ci id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3">𝑣</ci><ci id="S3.E2.m1.3.3.3.4.cmml" xref="S3.E2.m1.3.3.3.4">𝑔</ci><ci id="S3.E2.m1.3.3.3.5.cmml" xref="S3.E2.m1.3.3.3.5">_</ci><ci id="S3.E2.m1.3.3.3.6.cmml" xref="S3.E2.m1.3.3.3.6">𝑠</ci><ci id="S3.E2.m1.3.3.3.7.cmml" xref="S3.E2.m1.3.3.3.7">𝑐</ci><ci id="S3.E2.m1.3.3.3.8.cmml" xref="S3.E2.m1.3.3.3.8">𝑜</ci><ci id="S3.E2.m1.3.3.3.9.cmml" xref="S3.E2.m1.3.3.3.9">𝑟</ci><ci id="S3.E2.m1.3.3.3.10.cmml" xref="S3.E2.m1.3.3.3.10">𝑒</ci></apply><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><times id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.2"></times><apply id="S3.E2.m1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.3"><divide id="S3.E2.m1.3.3.1.3.1.cmml" xref="S3.E2.m1.3.3.1.3"></divide><cn type="integer" id="S3.E2.m1.3.3.1.3.2.cmml" xref="S3.E2.m1.3.3.1.3.2">1</cn><ci id="S3.E2.m1.3.3.1.3.3.cmml" xref="S3.E2.m1.3.3.1.3.3">𝑁</ci></apply><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1"><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3"><eq id="S3.E2.m1.3.3.1.1.2.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.2.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.2.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.3.3.1.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.2.3">𝑁</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"><times id="S3.E2.m1.3.3.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.2"></times><apply id="S3.E2.m1.3.3.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.1.3.2">𝑃</ci><ci id="S3.E2.m1.3.3.1.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.1.1.2a.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2"><mtext mathsize="90%" id="S3.E2.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.2">match</mtext></ci><list id="S3.E2.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1.1.1.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑋</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑌</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">avg\_score=\frac{1}{N}\sum\limits_{i=1}^{N}{P_{i}(\textrm{match}|X,Y)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p11" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.3" class="ltx_Math" alttext="max\_score=max_{i}(P_{i}(\textrm{match}|X,Y))" display="block"><semantics id="S3.E3.m1.3a"><mrow id="S3.E3.m1.3.3" xref="S3.E3.m1.3.3.cmml"><mrow id="S3.E3.m1.3.3.3" xref="S3.E3.m1.3.3.3.cmml"><mi mathsize="90%" id="S3.E3.m1.3.3.3.2" xref="S3.E3.m1.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.3" xref="S3.E3.m1.3.3.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1a" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.4" xref="S3.E3.m1.3.3.3.4.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1b" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S3.E3.m1.3.3.3.5" xref="S3.E3.m1.3.3.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1c" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.6" xref="S3.E3.m1.3.3.3.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1d" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.7" xref="S3.E3.m1.3.3.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1e" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.8" xref="S3.E3.m1.3.3.3.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1f" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.9" xref="S3.E3.m1.3.3.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.3.1g" xref="S3.E3.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.3.10" xref="S3.E3.m1.3.3.3.10.cmml">e</mi></mrow><mo mathsize="90%" id="S3.E3.m1.3.3.2" xref="S3.E3.m1.3.3.2.cmml">=</mo><mrow id="S3.E3.m1.3.3.1" xref="S3.E3.m1.3.3.1.cmml"><mi mathsize="90%" id="S3.E3.m1.3.3.1.3" xref="S3.E3.m1.3.3.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.2" xref="S3.E3.m1.3.3.1.2.cmml">​</mo><mi mathsize="90%" id="S3.E3.m1.3.3.1.4" xref="S3.E3.m1.3.3.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.2a" xref="S3.E3.m1.3.3.1.2.cmml">​</mo><msub id="S3.E3.m1.3.3.1.5" xref="S3.E3.m1.3.3.1.5.cmml"><mi mathsize="90%" id="S3.E3.m1.3.3.1.5.2" xref="S3.E3.m1.3.3.1.5.2.cmml">x</mi><mi mathsize="90%" id="S3.E3.m1.3.3.1.5.3" xref="S3.E3.m1.3.3.1.5.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.2b" xref="S3.E3.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.3.3.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.cmml"><msub id="S3.E3.m1.3.3.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E3.m1.3.3.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.3.2.cmml">P</mi><mi mathsize="90%" id="S3.E3.m1.3.3.1.1.1.1.3.3" xref="S3.E3.m1.3.3.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.3.3.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E3.m1.3.3.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml"><mtext mathsize="90%" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2a.cmml">match</mtext><mo fence="false" mathsize="90%" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E3.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.3.1.cmml"><mi mathsize="90%" id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">X</mi><mo mathsize="90%" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.3.2.1" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi mathsize="90%" id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml">Y</mi></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E3.m1.3.3.1.1.1.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E3.m1.3.3.1.1.1.3" xref="S3.E3.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.3b"><apply id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3"><eq id="S3.E3.m1.3.3.2.cmml" xref="S3.E3.m1.3.3.2"></eq><apply id="S3.E3.m1.3.3.3.cmml" xref="S3.E3.m1.3.3.3"><times id="S3.E3.m1.3.3.3.1.cmml" xref="S3.E3.m1.3.3.3.1"></times><ci id="S3.E3.m1.3.3.3.2.cmml" xref="S3.E3.m1.3.3.3.2">𝑚</ci><ci id="S3.E3.m1.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3">𝑎</ci><ci id="S3.E3.m1.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.4">𝑥</ci><ci id="S3.E3.m1.3.3.3.5.cmml" xref="S3.E3.m1.3.3.3.5">_</ci><ci id="S3.E3.m1.3.3.3.6.cmml" xref="S3.E3.m1.3.3.3.6">𝑠</ci><ci id="S3.E3.m1.3.3.3.7.cmml" xref="S3.E3.m1.3.3.3.7">𝑐</ci><ci id="S3.E3.m1.3.3.3.8.cmml" xref="S3.E3.m1.3.3.3.8">𝑜</ci><ci id="S3.E3.m1.3.3.3.9.cmml" xref="S3.E3.m1.3.3.3.9">𝑟</ci><ci id="S3.E3.m1.3.3.3.10.cmml" xref="S3.E3.m1.3.3.3.10">𝑒</ci></apply><apply id="S3.E3.m1.3.3.1.cmml" xref="S3.E3.m1.3.3.1"><times id="S3.E3.m1.3.3.1.2.cmml" xref="S3.E3.m1.3.3.1.2"></times><ci id="S3.E3.m1.3.3.1.3.cmml" xref="S3.E3.m1.3.3.1.3">𝑚</ci><ci id="S3.E3.m1.3.3.1.4.cmml" xref="S3.E3.m1.3.3.1.4">𝑎</ci><apply id="S3.E3.m1.3.3.1.5.cmml" xref="S3.E3.m1.3.3.1.5"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.5.1.cmml" xref="S3.E3.m1.3.3.1.5">subscript</csymbol><ci id="S3.E3.m1.3.3.1.5.2.cmml" xref="S3.E3.m1.3.3.1.5.2">𝑥</ci><ci id="S3.E3.m1.3.3.1.5.3.cmml" xref="S3.E3.m1.3.3.1.5.3">𝑖</ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1"><times id="S3.E3.m1.3.3.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.2"></times><apply id="S3.E3.m1.3.3.1.1.1.1.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3.2">𝑃</ci><ci id="S3.E3.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E3.m1.3.3.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2a.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2"><mtext mathsize="90%" id="S3.E3.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.2">match</mtext></ci><list id="S3.E3.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.3.3.1.1.1.1.1.1.1.3.2"><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝑋</ci><ci id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2">𝑌</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.3c">max\_score=max_{i}(P_{i}(\textrm{match}|X,Y))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p12" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.3" class="ltx_Math" alttext="min\_score=min_{i}(P_{i}(\textrm{match}|X,Y))" display="block"><semantics id="S3.E4.m1.3a"><mrow id="S3.E4.m1.3.3" xref="S3.E4.m1.3.3.cmml"><mrow id="S3.E4.m1.3.3.3" xref="S3.E4.m1.3.3.3.cmml"><mi mathsize="90%" id="S3.E4.m1.3.3.3.2" xref="S3.E4.m1.3.3.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.3" xref="S3.E4.m1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1a" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.4" xref="S3.E4.m1.3.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1b" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="S3.E4.m1.3.3.3.5" xref="S3.E4.m1.3.3.3.5.cmml">_</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1c" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.6" xref="S3.E4.m1.3.3.3.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1d" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.7" xref="S3.E4.m1.3.3.3.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1e" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.8" xref="S3.E4.m1.3.3.3.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1f" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.9" xref="S3.E4.m1.3.3.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.3.1g" xref="S3.E4.m1.3.3.3.1.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.3.10" xref="S3.E4.m1.3.3.3.10.cmml">e</mi></mrow><mo mathsize="90%" id="S3.E4.m1.3.3.2" xref="S3.E4.m1.3.3.2.cmml">=</mo><mrow id="S3.E4.m1.3.3.1" xref="S3.E4.m1.3.3.1.cmml"><mi mathsize="90%" id="S3.E4.m1.3.3.1.3" xref="S3.E4.m1.3.3.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.2" xref="S3.E4.m1.3.3.1.2.cmml">​</mo><mi mathsize="90%" id="S3.E4.m1.3.3.1.4" xref="S3.E4.m1.3.3.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.2a" xref="S3.E4.m1.3.3.1.2.cmml">​</mo><msub id="S3.E4.m1.3.3.1.5" xref="S3.E4.m1.3.3.1.5.cmml"><mi mathsize="90%" id="S3.E4.m1.3.3.1.5.2" xref="S3.E4.m1.3.3.1.5.2.cmml">n</mi><mi mathsize="90%" id="S3.E4.m1.3.3.1.5.3" xref="S3.E4.m1.3.3.1.5.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.2b" xref="S3.E4.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E4.m1.3.3.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E4.m1.3.3.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.cmml"><msub id="S3.E4.m1.3.3.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E4.m1.3.3.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.3.2.cmml">P</mi><mi mathsize="90%" id="S3.E4.m1.3.3.1.1.1.1.3.3" xref="S3.E4.m1.3.3.1.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.3.3.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E4.m1.3.3.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mtext mathsize="90%" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2a.cmml">match</mtext><mo fence="false" mathsize="90%" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">X</mi><mo mathsize="90%" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2.1" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml">,</mo><mi mathsize="90%" id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml">Y</mi></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E4.m1.3.3.1.1.1.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E4.m1.3.3.1.1.1.3" xref="S3.E4.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.3b"><apply id="S3.E4.m1.3.3.cmml" xref="S3.E4.m1.3.3"><eq id="S3.E4.m1.3.3.2.cmml" xref="S3.E4.m1.3.3.2"></eq><apply id="S3.E4.m1.3.3.3.cmml" xref="S3.E4.m1.3.3.3"><times id="S3.E4.m1.3.3.3.1.cmml" xref="S3.E4.m1.3.3.3.1"></times><ci id="S3.E4.m1.3.3.3.2.cmml" xref="S3.E4.m1.3.3.3.2">𝑚</ci><ci id="S3.E4.m1.3.3.3.3.cmml" xref="S3.E4.m1.3.3.3.3">𝑖</ci><ci id="S3.E4.m1.3.3.3.4.cmml" xref="S3.E4.m1.3.3.3.4">𝑛</ci><ci id="S3.E4.m1.3.3.3.5.cmml" xref="S3.E4.m1.3.3.3.5">_</ci><ci id="S3.E4.m1.3.3.3.6.cmml" xref="S3.E4.m1.3.3.3.6">𝑠</ci><ci id="S3.E4.m1.3.3.3.7.cmml" xref="S3.E4.m1.3.3.3.7">𝑐</ci><ci id="S3.E4.m1.3.3.3.8.cmml" xref="S3.E4.m1.3.3.3.8">𝑜</ci><ci id="S3.E4.m1.3.3.3.9.cmml" xref="S3.E4.m1.3.3.3.9">𝑟</ci><ci id="S3.E4.m1.3.3.3.10.cmml" xref="S3.E4.m1.3.3.3.10">𝑒</ci></apply><apply id="S3.E4.m1.3.3.1.cmml" xref="S3.E4.m1.3.3.1"><times id="S3.E4.m1.3.3.1.2.cmml" xref="S3.E4.m1.3.3.1.2"></times><ci id="S3.E4.m1.3.3.1.3.cmml" xref="S3.E4.m1.3.3.1.3">𝑚</ci><ci id="S3.E4.m1.3.3.1.4.cmml" xref="S3.E4.m1.3.3.1.4">𝑖</ci><apply id="S3.E4.m1.3.3.1.5.cmml" xref="S3.E4.m1.3.3.1.5"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.5.1.cmml" xref="S3.E4.m1.3.3.1.5">subscript</csymbol><ci id="S3.E4.m1.3.3.1.5.2.cmml" xref="S3.E4.m1.3.3.1.5.2">𝑛</ci><ci id="S3.E4.m1.3.3.1.5.3.cmml" xref="S3.E4.m1.3.3.1.5.3">𝑖</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1"><times id="S3.E4.m1.3.3.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.2"></times><apply id="S3.E4.m1.3.3.1.1.1.1.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.3.2">𝑃</ci><ci id="S3.E4.m1.3.3.1.1.1.1.3.3.cmml" xref="S3.E4.m1.3.3.1.1.1.1.3.3">𝑖</ci></apply><apply id="S3.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2a.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2"><mtext mathsize="90%" id="S3.E4.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.2">match</mtext></ci><list id="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.3.3.1.1.1.1.1.1.1.3.2"><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">𝑋</ci><ci id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2">𝑌</ci></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.3c">min\_score=min_{i}(P_{i}(\textrm{match}|X,Y))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Framework</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The description of the experimental framework is divided into three parts.
Section <a href="#S4.SS1" title="4.1 Database ‣ 4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> describes the database used in our experimental framework. Section <a href="#S4.SS2" title="4.2 Face Recognition Systems: Setup ‣ 4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> explains the experimental protocol considered for the configurations regarding the face recognition systems. It is important to highlight that, as indicated in Section <a href="#S3" title="3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, no additional data was used to re-train the 3DFR modules’ parameters, nor the one described in Section <a href="#S4.SS1" title="4.1 Database ‣ 4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. We consider the original versions of the 3DFR algorithms available in the corresponding GitHub repositories. Finally, Section <a href="#S4.SS3" title="4.3 Performance Evaluation ‣ 4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> discusses the analyzed performance metrics.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Database</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">State-of-the-art face recognition systems, even the ones in the 2D domain, achieve nearly perfect performance on traditional benchmark databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. However, the most significant advantage of 3DFR algorithms in recognition tasks is in more challenging scenarios, such as surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Hence, we use the SCface database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, containing both high-quality mugshot-like images and lower-quality RGB and grayscale surveillance images of 130 subjects. The surveillance images were acquired through five different camera models at three varying distances from the subject, ranging from 1 to 4.2 meters. Each subject was captured once for every possible camera-distance combination. The observed head poses are typically found in surveillance footage, with the camera slightly above the subject’s head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Therefore, SCface is considered to analyze the effect of different quality and resolution cameras on face recognition performance and the robustness to different distances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, thus making this database suitable for evaluating interoperability across settings using data from specific cameras or at certain distances. We refer to these evaluations as "cross-settings" experiments, while experiments involving training and testing data from the same camera and distance are referred to as "intra-settings".</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Concretely, for both experimental protocols, we use RGB samples related to 25 identities (<em id="S4.SS1.p3.1.1" class="ltx_emph">i.e</em><span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_italic">.<span id="S4.SS1.p3.1.2.1" class="ltx_text"></span></span>, about 20% of the subjects) as the test set, while the samples associated with the remaining 105 subjects are further divided, using 90% as training samples and 10% as validation ones.
In any experiment concerning the single camera-distance settings for training and test sets, we perform such divisions randomly to limit the possible bias on performance and introduce a face detection stage following the setting used in the same database in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Face Recognition Systems: Setup</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">All Siamese networks are pre-trained on the LFW database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Then, a fine-tuning through the training set of the SCFace is done, using a validation set for early stopping, according to partitions described in Section <a href="#S4.SS1" title="4.1 Database ‣ 4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. As shown in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Face Recognition Systems: Setup ‣ 4 Experimental Framework ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the inputs to the networks are the probe image and the representations obtained from a 3D template of the face related to the claimed identity. For comparability reasons, all the models have been trained on the images resampled at a resolution of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="128\times 128" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">128</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">128\times 128</annotation></semantics></math>, on up to 256 epochs with the patience of 5 epochs, batches of 128 triplets, and the Adam optimizer with a learning rate equal to 0.001. Note that this configuration does not necessarily represent the best parameters and preprocessing stages; rather, it is a general configuration that might be further improved in future work.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/subsystem_verification.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="395" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.5.2" class="ltx_text" style="font-size:90%;">Example of the proposed face verification system using a Siamese architecture, introducing as input the probe image and the facial representation in a non-frontal view obtained from a mugshot image through the EOS method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The images are related to a subject in the SCface database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. EMB refers to the feature embeddings obtained from the backbone (<em id="S4.F5.5.2.1" class="ltx_emph">i.e</em><span id="S4.F5.5.2.2" class="ltx_text ltx_font_italic">.<span id="S4.F5.5.2.2.1" class="ltx_text"></span></span>, VGG19 or XceptionNet). Only frontal views are used in the final inference stage.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Performance Evaluation</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">After training the individual face recognition systems using information from the different 3DFR algorithms, we investigate the potential of combining them through a correlation analysis between the sets of scores obtained using the test data in intra-setting experiments. In particular, we evaluate the linear correlation between pairs of face recognition systems composed of the same network architecture but enhanced by two different 3DFR algorithms. Therefore, we discuss the complementary information provided by the different 3DFR algorithms in relation to the obtained Pearson Correlation Coefficient (PCC).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Then, we pursue two sets of experiments to evaluate the effectiveness of the fusion methods (<em id="S4.SS3.p2.1.1" class="ltx_emph">i.e</em><span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">.<span id="S4.SS3.p2.1.2.1" class="ltx_text"></span></span>, the fusion of face recognition systems trained with information provided by different 3DFR algorithms) in the RGB domain, namely, intra-setting and cross-setting.
The first one consists of training and testing the single systems on data acquired by a specific camera and at a fixed distance from the subjects, therefore evaluating how models behave under controlled conditions. The cross-setting protocol involves training and testing the systems on images acquired with different cameras and distances.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.9" class="ltx_p">In particular, we evaluate the reliability of the examined fusion methods through metrics commonly used in face recognition.
From the matching scores, we compute the percentage of False Match Rate (FMR), <em id="S4.SS3.p3.9.1" class="ltx_emph">i.e</em><span id="S4.SS3.p3.9.2" class="ltx_text ltx_font_italic">.<span id="S4.SS3.p3.9.2.1" class="ltx_text"></span></span>, the rate of non-matching pairs of samples classified as match, and the percentage of False Non-Match Rate (FNMR), <em id="S4.SS3.p3.9.3" class="ltx_emph">i.e</em><span id="S4.SS3.p3.9.4" class="ltx_text ltx_font_italic">.<span id="S4.SS3.p3.9.4.1" class="ltx_text"></span></span>, the rate of matching pairs of samples that have been incorrectly classified, at various threshold values. This allows us to obtain the Receiver Operating Characteristic (ROC) curve as a function of FMR and <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="1-\textrm{FNMR}" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">1</mn><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">−</mo><mtext id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3a.cmml">FNMR</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><minus id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></minus><cn type="integer" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">1</cn><ci id="S4.SS3.p3.1.m1.1.1.3a.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><mtext id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">FNMR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">1-\textrm{FNMR}</annotation></semantics></math> and, therefore, the Area Under the Curve (AUC).
Similarly, from the distributions of the scores related to matching and non-matching identities, we also assess the effect size through Cohen’s <math id="S4.SS3.p3.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS3.p3.2.m2.1a"><mi id="S4.SS3.p3.2.m2.1.1" xref="S4.SS3.p3.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.2.m2.1b"><ci id="S4.SS3.p3.2.m2.1.1.cmml" xref="S4.SS3.p3.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.2.m2.1c">d</annotation></semantics></math> to further highlight the discriminatory ability of the evaluated systems. In fact, even if two verification systems report similar AUC values, Cohen’s <math id="S4.SS3.p3.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S4.SS3.p3.3.m3.1a"><mi id="S4.SS3.p3.3.m3.1.1" xref="S4.SS3.p3.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.3.m3.1b"><ci id="S4.SS3.p3.3.m3.1.1.cmml" xref="S4.SS3.p3.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.3.m3.1c">d</annotation></semantics></math> can also provide information about the magnitude of the differences between the score distributions.
In addition to these performance metrics, we also include for completeness the Equal Error Rate (EER), commonly used for assessing the performance of a biometric system as the percentage of mistakes when the proportions of errors on matching identities and non-matching ones are balanced. Finally, we include the <math id="S4.SS3.p3.4.m4.1" class="ltx_math_unparsed" alttext="\%\textrm{FNMR}@\textrm{FMR}=1\%" display="inline"><semantics id="S4.SS3.p3.4.m4.1a"><mrow id="S4.SS3.p3.4.m4.1b"><mo id="S4.SS3.p3.4.m4.1.1">%</mo><mtext id="S4.SS3.p3.4.m4.1.2">FNMR</mtext><mi mathvariant="normal" id="S4.SS3.p3.4.m4.1.3">@</mi><mtext id="S4.SS3.p3.4.m4.1.4">FMR</mtext><mo id="S4.SS3.p3.4.m4.1.5">=</mo><mn id="S4.SS3.p3.4.m4.1.6">1</mn><mo id="S4.SS3.p3.4.m4.1.7">%</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.4.m4.1c">\%\textrm{FNMR}@\textrm{FMR}=1\%</annotation></semantics></math> (<math id="S4.SS3.p3.5.m5.1" class="ltx_math_unparsed" alttext="\%\textrm{FNMR}" display="inline"><semantics id="S4.SS3.p3.5.m5.1a"><mrow id="S4.SS3.p3.5.m5.1b"><mo id="S4.SS3.p3.5.m5.1.1">%</mo><mtext id="S4.SS3.p3.5.m5.1.2">FNMR</mtext></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.5.m5.1c">\%\textrm{FNMR}</annotation></semantics></math> at FMR equal to <math id="S4.SS3.p3.6.m6.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S4.SS3.p3.6.m6.1a"><mrow id="S4.SS3.p3.6.m6.1.1" xref="S4.SS3.p3.6.m6.1.1.cmml"><mn id="S4.SS3.p3.6.m6.1.1.2" xref="S4.SS3.p3.6.m6.1.1.2.cmml">1</mn><mo id="S4.SS3.p3.6.m6.1.1.1" xref="S4.SS3.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.6.m6.1b"><apply id="S4.SS3.p3.6.m6.1.1.cmml" xref="S4.SS3.p3.6.m6.1.1"><csymbol cd="latexml" id="S4.SS3.p3.6.m6.1.1.1.cmml" xref="S4.SS3.p3.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.6.m6.1.1.2.cmml" xref="S4.SS3.p3.6.m6.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.6.m6.1c">1\%</annotation></semantics></math>) and <math id="S4.SS3.p3.7.m7.1" class="ltx_math_unparsed" alttext="\%\textrm{FMR}@\textrm{FNMR}=1\%" display="inline"><semantics id="S4.SS3.p3.7.m7.1a"><mrow id="S4.SS3.p3.7.m7.1b"><mo id="S4.SS3.p3.7.m7.1.1">%</mo><mtext id="S4.SS3.p3.7.m7.1.2">FMR</mtext><mi mathvariant="normal" id="S4.SS3.p3.7.m7.1.3">@</mi><mtext id="S4.SS3.p3.7.m7.1.4">FNMR</mtext><mo id="S4.SS3.p3.7.m7.1.5">=</mo><mn id="S4.SS3.p3.7.m7.1.6">1</mn><mo id="S4.SS3.p3.7.m7.1.7">%</mo></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.7.m7.1c">\%\textrm{FMR}@\textrm{FNMR}=1\%</annotation></semantics></math> (<math id="S4.SS3.p3.8.m8.1" class="ltx_math_unparsed" alttext="\%\textrm{FMR}" display="inline"><semantics id="S4.SS3.p3.8.m8.1a"><mrow id="S4.SS3.p3.8.m8.1b"><mo id="S4.SS3.p3.8.m8.1.1">%</mo><mtext id="S4.SS3.p3.8.m8.1.2">FMR</mtext></mrow><annotation encoding="application/x-tex" id="S4.SS3.p3.8.m8.1c">\%\textrm{FMR}</annotation></semantics></math> at FNMR equal to <math id="S4.SS3.p3.9.m9.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="S4.SS3.p3.9.m9.1a"><mrow id="S4.SS3.p3.9.m9.1.1" xref="S4.SS3.p3.9.m9.1.1.cmml"><mn id="S4.SS3.p3.9.m9.1.1.2" xref="S4.SS3.p3.9.m9.1.1.2.cmml">1</mn><mo id="S4.SS3.p3.9.m9.1.1.1" xref="S4.SS3.p3.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.9.m9.1b"><apply id="S4.SS3.p3.9.m9.1.1.cmml" xref="S4.SS3.p3.9.m9.1.1"><csymbol cd="latexml" id="S4.SS3.p3.9.m9.1.1.1.cmml" xref="S4.SS3.p3.9.m9.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p3.9.m9.1.1.2.cmml" xref="S4.SS3.p3.9.m9.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.9.m9.1c">1\%</annotation></semantics></math>) to assess the performance under stringent accuracy constraints, such as high-security and high-usable applications, respectively.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section reports the results obtained through the previously described experimental setup.
Section <a href="#S5.SS1" title="5.1 Correlation Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> provides an analysis of the correlation between the single classification models enhanced through different 3DFR methods. Section <a href="#S5.SS2" title="5.2 Intra-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> describes the outcome of the intra-setting analysis. Finally, Section <a href="#S5.SS3" title="5.3 Cross-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a> shows the performance obtained through the cross-setting analysis.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Correlation Analysis</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Figure <a href="#S5.F6" title="Figure 6 ‣ 5.1 Correlation Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows that recognition systems based on the VGG19 backbone are mainly poorly correlated, with the highest correlation obtained from the systems enhanced through 3DDFA v2 and EOS (0.27). Despite the higher values observed, a similar trend is visible in systems based on XceptionNet. This was expected due to the highest quality of the 3D templates generated by 3DDFA v2 and EOS algorithms, as can be seen in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Proposed Method ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/correlation3.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="377" height="141" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.3.2" class="ltx_text" style="font-size:90%;">Pearson correlation coefficient (PCC) between the set of scores obtained in the intra-setting experiments from pairs of face recognition systems based on the same backbone but enhanced by different 3DFR algorithms.</span></figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Hence, the general weak correlation highlighted by this analysis enforces the hypothesis of potential complementary information provided by different 3DFR algorithms. However, despite providing clues on the linear correlation between the scores of pairs of systems, the PCC needs to be paired with an analysis of the effectiveness of the individual models to observe if a low correlation could be related to a relevant difference in the overall performance or mostly to differences in single types of errors, which could therefore be exploited through a combination between them.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Intra-Setting Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Intra-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> reports the average performance values in the intra-setting scenario (<em id="S5.SS2.p1.1.1" class="ltx_emph">i.e</em><span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">.<span id="S5.SS2.p1.1.2.1" class="ltx_text"></span></span>, fixed camera-distance settings) for the individual 3DFR algorithms (3DDFA v2, EOS, and NextFace) and their fusion (Avg, Min, and Max). The best values for each metric and backbone (VGG19 and XceptionNet) are highlighted in bold within the single 3DFR-enhanced models and the fusion approach.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S5.T1.3.2" class="ltx_text" style="font-size:90%;">Average results in the intra-settings context. For each metric and backbone (VGG19 and XceptionNet), we highlight the best result achieved in the individual system (3DDFA v2, EOS, or NextFace) and the best fusion approach (Avg, Min, or Max).</span></figcaption>
<div id="S5.T1.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:110pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-124.1pt,35.0pt) scale(0.61125854696422,0.61125854696422) ;">
<table id="S5.T1.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.4.1.1.1" class="ltx_tr">
<th id="S5.T1.4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T1.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="S5.T1.4.1.1.1.2.1" class="ltx_text ltx_font_bold">VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite></span></td>
<td id="S5.T1.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="S5.T1.4.1.1.1.3.1" class="ltx_text ltx_font_bold">XceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span></td>
</tr>
<tr id="S5.T1.4.1.2.2" class="ltx_tr">
<th id="S5.T1.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T1.4.1.2.2.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S5.T1.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.2.1" class="ltx_text ltx_font_bold">AUC</span></td>
<td id="S5.T1.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.3.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S5.T1.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.4.1" class="ltx_text ltx_font_bold">Cohen’s</span></td>
<td id="S5.T1.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.5.1" class="ltx_text ltx_font_bold">%FMR at</span></td>
<td id="S5.T1.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.4.1.2.2.6.1" class="ltx_text ltx_font_bold">%FNMR at</span></td>
<td id="S5.T1.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.7.1" class="ltx_text ltx_font_bold">AUC</span></td>
<td id="S5.T1.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.8.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S5.T1.4.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.9.1" class="ltx_text ltx_font_bold">Cohen’s</span></td>
<td id="S5.T1.4.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.2.2.10.1" class="ltx_text ltx_font_bold">%FMR at</span></td>
<td id="S5.T1.4.1.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.4.1.2.2.11.1" class="ltx_text ltx_font_bold">%FNMR at</span></td>
</tr>
<tr id="S5.T1.4.1.3.3" class="ltx_tr">
<td id="S5.T1.4.1.3.3.1" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.1.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T1.4.1.3.3.2" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.2.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T1.4.1.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.3.1" class="ltx_text ltx_font_bold">d</span></td>
<td id="S5.T1.4.1.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.4.1" class="ltx_text ltx_font_bold">FNMR=1%</span></td>
<td id="S5.T1.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.4.1.3.3.5.1" class="ltx_text ltx_font_bold">FMR=1%</span></td>
<td id="S5.T1.4.1.3.3.6" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.6.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T1.4.1.3.3.7" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.7.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T1.4.1.3.3.8" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.8.1" class="ltx_text ltx_font_bold">d</span></td>
<td id="S5.T1.4.1.3.3.9" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.3.3.9.1" class="ltx_text ltx_font_bold">FNMR=1%</span></td>
<td id="S5.T1.4.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.4.1.3.3.10.1" class="ltx_text ltx_font_bold">FMR=1%</span></td>
</tr>
<tr id="S5.T1.4.1.4.4" class="ltx_tr">
<th id="S5.T1.4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.4.1.4.4.1.1" class="ltx_text ltx_font_bold">Baseline</span></th>
<td id="S5.T1.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">77.27</td>
<td id="S5.T1.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">15.52</td>
<td id="S5.T1.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.91</td>
<td id="S5.T1.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">85.37</td>
<td id="S5.T1.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.42</td>
<td id="S5.T1.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">71.89</td>
<td id="S5.T1.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">22.77</td>
<td id="S5.T1.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">0.79</td>
<td id="S5.T1.4.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">81.37</td>
<td id="S5.T1.4.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.57</td>
</tr>
<tr id="S5.T1.4.1.5.5" class="ltx_tr">
<th id="S5.T1.4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.4.1.5.5.1.1" class="ltx_text ltx_font_bold">3DDFA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span></th>
<td id="S5.T1.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">81.51</td>
<td id="S5.T1.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.5.5.3.1" class="ltx_text ltx_font_bold">12.25</span></td>
<td id="S5.T1.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">1.09</td>
<td id="S5.T1.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.5.5.5.1" class="ltx_text ltx_font_bold">75.72</span></td>
<td id="S5.T1.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.4.1.5.5.6.1" class="ltx_text ltx_font_bold">88.75</span></td>
<td id="S5.T1.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">74.99</td>
<td id="S5.T1.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.5.5.8.1" class="ltx_text ltx_font_bold">16.33</span></td>
<td id="S5.T1.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">0.77</td>
<td id="S5.T1.4.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t">86.83</td>
<td id="S5.T1.4.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">91.85</td>
</tr>
<tr id="S5.T1.4.1.6.6" class="ltx_tr">
<th id="S5.T1.4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.4.1.6.6.1.1" class="ltx_text ltx_font_bold">EOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span></th>
<td id="S5.T1.4.1.6.6.2" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.6.6.2.1" class="ltx_text ltx_font_bold">82.06</span></td>
<td id="S5.T1.4.1.6.6.3" class="ltx_td ltx_align_center">12.50</td>
<td id="S5.T1.4.1.6.6.4" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.6.6.4.1" class="ltx_text ltx_font_bold">1.21</span></td>
<td id="S5.T1.4.1.6.6.5" class="ltx_td ltx_align_center">80.46</td>
<td id="S5.T1.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r">90.66</td>
<td id="S5.T1.4.1.6.6.7" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.6.6.7.1" class="ltx_text ltx_font_bold">78.18</span></td>
<td id="S5.T1.4.1.6.6.8" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.6.6.8.1" class="ltx_text ltx_font_bold">16.33</span></td>
<td id="S5.T1.4.1.6.6.9" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.6.6.9.1" class="ltx_text ltx_font_bold">0.92</span></td>
<td id="S5.T1.4.1.6.6.10" class="ltx_td ltx_align_center"><span id="S5.T1.4.1.6.6.10.1" class="ltx_text ltx_font_bold">86.43</span></td>
<td id="S5.T1.4.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T1.4.1.6.6.11.1" class="ltx_text ltx_font_bold">90.68</span></td>
</tr>
<tr id="S5.T1.4.1.7.7" class="ltx_tr">
<th id="S5.T1.4.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.4.1.7.7.1.1" class="ltx_text ltx_font_bold">NextFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span></th>
<td id="S5.T1.4.1.7.7.2" class="ltx_td ltx_align_center">72.58</td>
<td id="S5.T1.4.1.7.7.3" class="ltx_td ltx_align_center">17.67</td>
<td id="S5.T1.4.1.7.7.4" class="ltx_td ltx_align_center">0.78</td>
<td id="S5.T1.4.1.7.7.5" class="ltx_td ltx_align_center">86.45</td>
<td id="S5.T1.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">93.99</td>
<td id="S5.T1.4.1.7.7.7" class="ltx_td ltx_align_center">71.02</td>
<td id="S5.T1.4.1.7.7.8" class="ltx_td ltx_align_center">25.83</td>
<td id="S5.T1.4.1.7.7.9" class="ltx_td ltx_align_center">0.62</td>
<td id="S5.T1.4.1.7.7.10" class="ltx_td ltx_align_center">90.09</td>
<td id="S5.T1.4.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r">95.58</td>
</tr>
<tr id="S5.T1.4.1.8.8" class="ltx_tr">
<th id="S5.T1.4.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T1.4.1.8.8.1.1" class="ltx_text ltx_font_bold">Fusion (Avg)</span></th>
<td id="S5.T1.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.2.1" class="ltx_text ltx_font_bold">86.94</span></td>
<td id="S5.T1.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.3.1" class="ltx_text ltx_font_bold">10.78</span></td>
<td id="S5.T1.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.4.1" class="ltx_text ltx_font_bold">1.53</span></td>
<td id="S5.T1.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.5.1" class="ltx_text ltx_font_bold">56.25</span></td>
<td id="S5.T1.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.4.1.8.8.6.1" class="ltx_text ltx_font_bold">80.41</span></td>
<td id="S5.T1.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.7.1" class="ltx_text ltx_font_bold">79.92</span></td>
<td id="S5.T1.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.8.1" class="ltx_text ltx_font_bold">15.67</span></td>
<td id="S5.T1.4.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T1.4.1.8.8.9.1" class="ltx_text ltx_font_bold">1.04</span></td>
<td id="S5.T1.4.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t">71.57</td>
<td id="S5.T1.4.1.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.4.1.8.8.11.1" class="ltx_text ltx_font_bold">90.19</span></td>
</tr>
<tr id="S5.T1.4.1.9.9" class="ltx_tr">
<th id="S5.T1.4.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T1.4.1.9.9.1.1" class="ltx_text ltx_font_bold">Fusion (Min)</span></th>
<td id="S5.T1.4.1.9.9.2" class="ltx_td ltx_align_center">84.37</td>
<td id="S5.T1.4.1.9.9.3" class="ltx_td ltx_align_center">13.36</td>
<td id="S5.T1.4.1.9.9.4" class="ltx_td ltx_align_center">1.02</td>
<td id="S5.T1.4.1.9.9.5" class="ltx_td ltx_align_center">84.49</td>
<td id="S5.T1.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r">81.78</td>
<td id="S5.T1.4.1.9.9.7" class="ltx_td ltx_align_center">76.52</td>
<td id="S5.T1.4.1.9.9.8" class="ltx_td ltx_align_center">18.17</td>
<td id="S5.T1.4.1.9.9.9" class="ltx_td ltx_align_center">0.61</td>
<td id="S5.T1.4.1.9.9.10" class="ltx_td ltx_align_center">95.05</td>
<td id="S5.T1.4.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r">91.27</td>
</tr>
<tr id="S5.T1.4.1.10.10" class="ltx_tr">
<th id="S5.T1.4.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S5.T1.4.1.10.10.1.1" class="ltx_text ltx_font_bold">Fusion (Max)</span></th>
<td id="S5.T1.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b">74.07</td>
<td id="S5.T1.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b">11.39</td>
<td id="S5.T1.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b">1.08</td>
<td id="S5.T1.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b">68.12</td>
<td id="S5.T1.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">93.03</td>
<td id="S5.T1.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b">77.95</td>
<td id="S5.T1.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_b">17.75</td>
<td id="S5.T1.4.1.10.10.9" class="ltx_td ltx_align_center ltx_border_b">0.99</td>
<td id="S5.T1.4.1.10.10.10" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T1.4.1.10.10.10.1" class="ltx_text ltx_font_bold">71.26</span></td>
<td id="S5.T1.4.1.10.10.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">94.99</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.6" class="ltx_p">We can observe that, in terms of AUC and Cohen’s <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">d</annotation></semantics></math>, the best-performing single models are those enhanced through the EOS algorithm. For example, for the VGG19 backbone, the performance achieved when including information from the EOS algorithm is <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="82.06\%" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mn id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">82.06</mn><mo id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">82.06</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">82.06\%</annotation></semantics></math> AUC, an absolute improvement of <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="4.79\%" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mrow id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><mn id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">4.79</mn><mo id="S5.SS2.p2.3.m3.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.2">4.79</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">4.79\%</annotation></semantics></math> AUC in comparison with the Baseline system (<math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="77.27\%" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mrow id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml"><mn id="S5.SS2.p2.4.m4.1.1.2" xref="S5.SS2.p2.4.m4.1.1.2.cmml">77.27</mn><mo id="S5.SS2.p2.4.m4.1.1.1" xref="S5.SS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><apply id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p2.4.m4.1.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p2.4.m4.1.1.2.cmml" xref="S5.SS2.p2.4.m4.1.1.2">77.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">77.27\%</annotation></semantics></math> AUC), proving to be 3DFR algorithms a good solution to increase the performance in video surveillance scenarios. A similar trend is observed for XceptionNet. However, for some specific operational points, for example, EER, <math id="S5.SS2.p2.5.m5.1" class="ltx_math_unparsed" alttext="\%\textrm{FMR}@\textrm{FNMR}=1\%" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><mrow id="S5.SS2.p2.5.m5.1b"><mo id="S5.SS2.p2.5.m5.1.1">%</mo><mtext id="S5.SS2.p2.5.m5.1.2">FMR</mtext><mi mathvariant="normal" id="S5.SS2.p2.5.m5.1.3">@</mi><mtext id="S5.SS2.p2.5.m5.1.4">FNMR</mtext><mo id="S5.SS2.p2.5.m5.1.5">=</mo><mn id="S5.SS2.p2.5.m5.1.6">1</mn><mo id="S5.SS2.p2.5.m5.1.7">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">\%\textrm{FMR}@\textrm{FNMR}=1\%</annotation></semantics></math>, and <math id="S5.SS2.p2.6.m6.1" class="ltx_math_unparsed" alttext="\%\textrm{FNMR}@\textrm{FMR}=1\%" display="inline"><semantics id="S5.SS2.p2.6.m6.1a"><mrow id="S5.SS2.p2.6.m6.1b"><mo id="S5.SS2.p2.6.m6.1.1">%</mo><mtext id="S5.SS2.p2.6.m6.1.2">FNMR</mtext><mi mathvariant="normal" id="S5.SS2.p2.6.m6.1.3">@</mi><mtext id="S5.SS2.p2.6.m6.1.4">FMR</mtext><mo id="S5.SS2.p2.6.m6.1.5">=</mo><mn id="S5.SS2.p2.6.m6.1.6">1</mn><mo id="S5.SS2.p2.6.m6.1.7">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">\%\textrm{FNMR}@\textrm{FMR}=1\%</annotation></semantics></math>, and the VGG19 backbone, the information provided by the 3DDFA v2 algorithm performs better than the EOS algorithm, enforcing the hypothesis that the fusion of face verification systems trained with different 3DFR algorithms may be beneficial.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.6" class="ltx_p">Analyzing the proposed fusion (Avg, Min, and Max) of different 3DFR algorithms, we can observe that, in general, the Avg score-level fusion achieves the best results, revealing improved performance both at a global (<em id="S5.SS2.p3.6.1" class="ltx_emph">i.e</em><span id="S5.SS2.p3.6.2" class="ltx_text ltx_font_italic">.<span id="S5.SS2.p3.6.2.1" class="ltx_text"></span></span>, AUC and Cohen’s <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">d</annotation></semantics></math>) and local level (<em id="S5.SS2.p3.6.3" class="ltx_emph">i.e</em><span id="S5.SS2.p3.6.4" class="ltx_text ltx_font_italic">.<span id="S5.SS2.p3.6.4.1" class="ltx_text"></span></span>, EER, <math id="S5.SS2.p3.2.m2.1" class="ltx_math_unparsed" alttext="\%\textrm{FMR}@\textrm{FNMR}=1\%" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mrow id="S5.SS2.p3.2.m2.1b"><mo id="S5.SS2.p3.2.m2.1.1">%</mo><mtext id="S5.SS2.p3.2.m2.1.2">FMR</mtext><mi mathvariant="normal" id="S5.SS2.p3.2.m2.1.3">@</mi><mtext id="S5.SS2.p3.2.m2.1.4">FNMR</mtext><mo id="S5.SS2.p3.2.m2.1.5">=</mo><mn id="S5.SS2.p3.2.m2.1.6">1</mn><mo id="S5.SS2.p3.2.m2.1.7">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">\%\textrm{FMR}@\textrm{FNMR}=1\%</annotation></semantics></math>, and <math id="S5.SS2.p3.3.m3.1" class="ltx_math_unparsed" alttext="\%\textrm{FNMR}@\textrm{FMR}=1\%" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><mrow id="S5.SS2.p3.3.m3.1b"><mo id="S5.SS2.p3.3.m3.1.1">%</mo><mtext id="S5.SS2.p3.3.m3.1.2">FNMR</mtext><mi mathvariant="normal" id="S5.SS2.p3.3.m3.1.3">@</mi><mtext id="S5.SS2.p3.3.m3.1.4">FMR</mtext><mo id="S5.SS2.p3.3.m3.1.5">=</mo><mn id="S5.SS2.p3.3.m3.1.6">1</mn><mo id="S5.SS2.p3.3.m3.1.7">%</mo></mrow><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">\%\textrm{FNMR}@\textrm{FMR}=1\%</annotation></semantics></math>). For example, for the VGG19 backbone, the Avg score-level fusion achieves an AUC of <math id="S5.SS2.p3.4.m4.1" class="ltx_Math" alttext="86.94\%" display="inline"><semantics id="S5.SS2.p3.4.m4.1a"><mrow id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml"><mn id="S5.SS2.p3.4.m4.1.1.2" xref="S5.SS2.p3.4.m4.1.1.2.cmml">86.94</mn><mo id="S5.SS2.p3.4.m4.1.1.1" xref="S5.SS2.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><apply id="S5.SS2.p3.4.m4.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p3.4.m4.1.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.4.m4.1.1.2.cmml" xref="S5.SS2.p3.4.m4.1.1.2">86.94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">86.94\%</annotation></semantics></math>, an absolute improvement of <math id="S5.SS2.p3.5.m5.1" class="ltx_Math" alttext="9.67\%" display="inline"><semantics id="S5.SS2.p3.5.m5.1a"><mrow id="S5.SS2.p3.5.m5.1.1" xref="S5.SS2.p3.5.m5.1.1.cmml"><mn id="S5.SS2.p3.5.m5.1.1.2" xref="S5.SS2.p3.5.m5.1.1.2.cmml">9.67</mn><mo id="S5.SS2.p3.5.m5.1.1.1" xref="S5.SS2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.5.m5.1b"><apply id="S5.SS2.p3.5.m5.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1"><csymbol cd="latexml" id="S5.SS2.p3.5.m5.1.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.5.m5.1.1.2.cmml" xref="S5.SS2.p3.5.m5.1.1.2">9.67</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.5.m5.1c">9.67\%</annotation></semantics></math> and <math id="S5.SS2.p3.6.m6.1" class="ltx_Math" alttext="4.88\%" display="inline"><semantics id="S5.SS2.p3.6.m6.1a"><mrow id="S5.SS2.p3.6.m6.1.1" xref="S5.SS2.p3.6.m6.1.1.cmml"><mn id="S5.SS2.p3.6.m6.1.1.2" xref="S5.SS2.p3.6.m6.1.1.2.cmml">4.88</mn><mo id="S5.SS2.p3.6.m6.1.1.1" xref="S5.SS2.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.6.m6.1b"><apply id="S5.SS2.p3.6.m6.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1"><csymbol cd="latexml" id="S5.SS2.p3.6.m6.1.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p3.6.m6.1.1.2.cmml" xref="S5.SS2.p3.6.m6.1.1.2">4.88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.6.m6.1c">4.88\%</annotation></semantics></math> AUC compared to the Baseline system and the best individual 3DFR algorithm, respectively.
In particular, this fusion rule shows improvements in the separation between the score distributions related to genuine and impostor subjects, also solving the issues related to the worst performance at challenging thresholds after introducing the 3DFR algorithm in the pipeline.
Furthermore, it is worth noting that this fusion rule is also able to deal with the variability in performance observed by the 3DFR-enhanced recognition systems between the different camera-distance settings. This can be seen in Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Intra-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
The more stable results across the experiments represent an important feature for the aimed application field <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and this is especially achieved with the proposed Avg score-level fusion.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2409.10481/assets/Figures/aucs_intra_rgb2.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="383" height="248" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.3.2" class="ltx_text" style="font-size:90%;">AUC values obtained for the intra-setting scenario for the different camera-distance configurations using VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> (a) and XceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (b) as backbones.</span></figcaption>
</figure>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Furthermore, as can be seen in Table <a href="#S5.T1" title="Table 1 ‣ 5.2 Intra-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 Intra-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, the performance of the deep neural networks trained with mugshots or enhanced with single 3DFR algorithms is more robust when employing the VGG19 architecture as the backbone.
This outcome was expected, considering the significantly higher performance of this network in face recognition tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Moreover, the fusion methods related to such architecture also reveal the greatest improvement with respect to the single systems.
Regarding the different fusion methods studied, the minimum and the maximum rules are generally less performing, as they inherently tend to produce more false matches and false non-matches, respectively.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">Finally, for completeness, we analyze in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 Intra-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> the robustness of the proposed systems with an increasing distance of the camera during the acquisition (<em id="S5.SS2.p5.1.1" class="ltx_emph">i.e</em><span id="S5.SS2.p5.1.2" class="ltx_text ltx_font_italic">.<span id="S5.SS2.p5.1.2.1" class="ltx_text"></span></span>, 4.2 meters, 2.6 meters, and 1 meter). As expected, both the systems trained through single 3DFR algorithms and their fusion through the Avg rule suffer with an increasing acquisition distance. Similar to our previous analysis, we can observe that the single 3DFR algorithms show differences in enhancement capability, revealing that there is not a single best choice for all settings and, therefore, further highlighting a certain degree of fusion between 3D templates, as can be seen with the Avg score-level fusion.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2409.10481/assets/Figures/auc_intra_distance_vgg.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2409.10481/assets/Figures/auc_intra_distance_xception.png" id="S5.F8.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="293" height="220" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.6.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.7.2" class="ltx_text" style="font-size:90%;">Summary of <span id="S5.F8.7.2.1" class="ltx_text ltx_font_italic">AUC</span> values obtained from all the intra-setting camera-distance configurations (<em id="S5.F8.7.2.2" class="ltx_emph">i.e</em><span id="S5.F8.7.2.3" class="ltx_text ltx_font_italic">.<span id="S5.F8.7.2.3.1" class="ltx_text"></span></span>, 4.2 meters, 2.6 meters, and 1 meter) through the system based on the VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> (a) and XceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (b), trained with mugshots, single 3DFR algorithms, and fusion of them through the Average rule (<span id="S5.F8.7.2.4" class="ltx_text ltx_font_italic">Avg</span>).</span></figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Cross-Setting Analysis</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.2" class="ltx_p">Table <a href="#S5.T2" title="Table 2 ‣ 5.3 Cross-Setting Analysis ‣ 5 Results ‣ Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results achieved when considering different acquisition distances (cross-distance setting) and surveillance cameras (cross-camera setting) between training and test sets.
As expected, both single 3DFR-enhanced and fused systems suffer from a significant decay in performance compared to the intra-setting scenario, remarking how challenging the task is. For example, for the VGG19 backbone and Avg score-level fusion, the AUC decreases from <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="86.94\%" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">86.94</mn><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">86.94</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">86.94\%</annotation></semantics></math> in the intra-setting scenario to <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="74.55\%" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">74.55</mn><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">74.55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">74.55\%</annotation></semantics></math> in the cross-setting scenario. Still, the Avg fusion rule is confirmed to be able to improve the overall performance, outperforming the single systems and the Baseline one from both a global and a local perspective.
A notable finding is that, unlike the intra-setting scenario, the XceptionNet backbone enhanced with the considered 3DFR algorithms achieves better generalization capability in terms of AUC and EER than the VGG19 one.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.3.2" class="ltx_text" style="font-size:90%;">Average results in cross-settings context. For each metric and backbone (VGG19 and XceptionNet), we highlight the best result achieved in the individual system (3DDFA v2, EOS, NextFace) and the best fusion approach (Avg, Min, or Max).</span></figcaption>
<div id="S5.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:110pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-124.1pt,35.0pt) scale(0.61125854696422,0.61125854696422) ;">
<table id="S5.T2.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.4.1.1.1" class="ltx_tr">
<th id="S5.T2.4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r"></th>
<td id="S5.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="S5.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">VGG19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite></span></td>
<td id="S5.T2.4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5"><span id="S5.T2.4.1.1.1.3.1" class="ltx_text ltx_font_bold">XceptionNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span></td>
</tr>
<tr id="S5.T2.4.1.2.2" class="ltx_tr">
<th id="S5.T2.4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S5.T2.4.1.2.2.1.1" class="ltx_text ltx_font_bold">Method</span></th>
<td id="S5.T2.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.2.1" class="ltx_text ltx_font_bold">AUC</span></td>
<td id="S5.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.3.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S5.T2.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.4.1" class="ltx_text ltx_font_bold">Cohen’s</span></td>
<td id="S5.T2.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.5.1" class="ltx_text ltx_font_bold">%FMR at</span></td>
<td id="S5.T2.4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.4.1.2.2.6.1" class="ltx_text ltx_font_bold">%FNMR at</span></td>
<td id="S5.T2.4.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.7.1" class="ltx_text ltx_font_bold">AUC</span></td>
<td id="S5.T2.4.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.8.1" class="ltx_text ltx_font_bold">EER</span></td>
<td id="S5.T2.4.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.9.1" class="ltx_text ltx_font_bold">Cohen’s</span></td>
<td id="S5.T2.4.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.2.2.10.1" class="ltx_text ltx_font_bold">%FMR at</span></td>
<td id="S5.T2.4.1.2.2.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.4.1.2.2.11.1" class="ltx_text ltx_font_bold">%FNMR at</span></td>
</tr>
<tr id="S5.T2.4.1.3.3" class="ltx_tr">
<td id="S5.T2.4.1.3.3.1" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.1.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T2.4.1.3.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.2.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T2.4.1.3.3.3" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.3.1" class="ltx_text ltx_font_bold">d</span></td>
<td id="S5.T2.4.1.3.3.4" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.4.1" class="ltx_text ltx_font_bold">FNMR=1%</span></td>
<td id="S5.T2.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.3.3.5.1" class="ltx_text ltx_font_bold">FMR=1%</span></td>
<td id="S5.T2.4.1.3.3.6" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.6.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T2.4.1.3.3.7" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.7.1" class="ltx_text ltx_font_bold">[%]</span></td>
<td id="S5.T2.4.1.3.3.8" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.8.1" class="ltx_text ltx_font_bold">d</span></td>
<td id="S5.T2.4.1.3.3.9" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.3.3.9.1" class="ltx_text ltx_font_bold">FNMR=1%</span></td>
<td id="S5.T2.4.1.3.3.10" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.3.3.10.1" class="ltx_text ltx_font_bold">FMR=1%</span></td>
</tr>
<tr id="S5.T2.4.1.4.4" class="ltx_tr">
<th id="S5.T2.4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.4.1.4.4.1.1" class="ltx_text ltx_font_bold">Baseline</span></th>
<td id="S5.T2.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">67.30</td>
<td id="S5.T2.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">36.27</td>
<td id="S5.T2.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">0.55</td>
<td id="S5.T2.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">99.24</td>
<td id="S5.T2.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">98.15</td>
<td id="S5.T2.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">68.97</td>
<td id="S5.T2.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">36.21</td>
<td id="S5.T2.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">0.63</td>
<td id="S5.T2.4.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">91.81</td>
<td id="S5.T2.4.1.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">96.53</td>
</tr>
<tr id="S5.T2.4.1.5.5" class="ltx_tr">
<th id="S5.T2.4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.4.1.5.5.1.1" class="ltx_text ltx_font_bold">3DDFA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span></th>
<td id="S5.T2.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">68.84</td>
<td id="S5.T2.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">20.69</td>
<td id="S5.T2.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.5.5.4.1" class="ltx_text ltx_font_bold">0.64</span></td>
<td id="S5.T2.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">95.23</td>
<td id="S5.T2.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">94.57</td>
<td id="S5.T2.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">70.95</td>
<td id="S5.T2.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">19.70</td>
<td id="S5.T2.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.5.5.9.1" class="ltx_text ltx_font_bold">0.69</span></td>
<td id="S5.T2.4.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.5.5.10.1" class="ltx_text ltx_font_bold">95.22</span></td>
<td id="S5.T2.4.1.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">92.53</td>
</tr>
<tr id="S5.T2.4.1.6.6" class="ltx_tr">
<th id="S5.T2.4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T2.4.1.6.6.1.1" class="ltx_text ltx_font_bold">EOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span></th>
<td id="S5.T2.4.1.6.6.2" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.6.2.1" class="ltx_text ltx_font_bold">70.13</span></td>
<td id="S5.T2.4.1.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.6.3.1" class="ltx_text ltx_font_bold">19.70</span></td>
<td id="S5.T2.4.1.6.6.4" class="ltx_td ltx_align_center">0.63</td>
<td id="S5.T2.4.1.6.6.5" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.6.5.1" class="ltx_text ltx_font_bold">93.16</span></td>
<td id="S5.T2.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.6.6.6.1" class="ltx_text ltx_font_bold">93.87</span></td>
<td id="S5.T2.4.1.6.6.7" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.6.7.1" class="ltx_text ltx_font_bold">71.21</span></td>
<td id="S5.T2.4.1.6.6.8" class="ltx_td ltx_align_center"><span id="S5.T2.4.1.6.6.8.1" class="ltx_text ltx_font_bold">19.09</span></td>
<td id="S5.T2.4.1.6.6.9" class="ltx_td ltx_align_center">0.61</td>
<td id="S5.T2.4.1.6.6.10" class="ltx_td ltx_align_center">96.57</td>
<td id="S5.T2.4.1.6.6.11" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.4.1.6.6.11.1" class="ltx_text ltx_font_bold">91.58</span></td>
</tr>
<tr id="S5.T2.4.1.7.7" class="ltx_tr">
<th id="S5.T2.4.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T2.4.1.7.7.1.1" class="ltx_text ltx_font_bold">NextFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span></th>
<td id="S5.T2.4.1.7.7.2" class="ltx_td ltx_align_center">64.91</td>
<td id="S5.T2.4.1.7.7.3" class="ltx_td ltx_align_center">22.29</td>
<td id="S5.T2.4.1.7.7.4" class="ltx_td ltx_align_center">0.47</td>
<td id="S5.T2.4.1.7.7.5" class="ltx_td ltx_align_center">93.71</td>
<td id="S5.T2.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">96.90</td>
<td id="S5.T2.4.1.7.7.7" class="ltx_td ltx_align_center">68.47</td>
<td id="S5.T2.4.1.7.7.8" class="ltx_td ltx_align_center">23.46</td>
<td id="S5.T2.4.1.7.7.9" class="ltx_td ltx_align_center">0.57</td>
<td id="S5.T2.4.1.7.7.10" class="ltx_td ltx_align_center">95.99</td>
<td id="S5.T2.4.1.7.7.11" class="ltx_td ltx_align_center ltx_border_r">94.97</td>
</tr>
<tr id="S5.T2.4.1.8.8" class="ltx_tr">
<th id="S5.T2.4.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S5.T2.4.1.8.8.1.1" class="ltx_text ltx_font_bold">Fusion (Avg)</span></th>
<td id="S5.T2.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.2.1" class="ltx_text ltx_font_bold">74.55</span></td>
<td id="S5.T2.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.3.1" class="ltx_text ltx_font_bold">19.18</span></td>
<td id="S5.T2.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.4.1" class="ltx_text ltx_font_bold">0.86</span></td>
<td id="S5.T2.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.5.1" class="ltx_text ltx_font_bold">84.70</span></td>
<td id="S5.T2.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.4.1.8.8.6.1" class="ltx_text ltx_font_bold">90.74</span></td>
<td id="S5.T2.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.7.1" class="ltx_text ltx_font_bold">75.77</span></td>
<td id="S5.T2.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_t">16.44</td>
<td id="S5.T2.4.1.8.8.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.9.1" class="ltx_text ltx_font_bold">0.86</span></td>
<td id="S5.T2.4.1.8.8.10" class="ltx_td ltx_align_center ltx_border_t"><span id="S5.T2.4.1.8.8.10.1" class="ltx_text ltx_font_bold">83.41</span></td>
<td id="S5.T2.4.1.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.4.1.8.8.11.1" class="ltx_text ltx_font_bold">89.84</span></td>
</tr>
<tr id="S5.T2.4.1.9.9" class="ltx_tr">
<th id="S5.T2.4.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r"><span id="S5.T2.4.1.9.9.1.1" class="ltx_text ltx_font_bold">Fusion (Min)</span></th>
<td id="S5.T2.4.1.9.9.2" class="ltx_td ltx_align_center">70.49</td>
<td id="S5.T2.4.1.9.9.3" class="ltx_td ltx_align_center">20.14</td>
<td id="S5.T2.4.1.9.9.4" class="ltx_td ltx_align_center">0.58</td>
<td id="S5.T2.4.1.9.9.5" class="ltx_td ltx_align_center">96.38</td>
<td id="S5.T2.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r">91.50</td>
<td id="S5.T2.4.1.9.9.7" class="ltx_td ltx_align_center">70.70</td>
<td id="S5.T2.4.1.9.9.8" class="ltx_td ltx_align_center">17.24</td>
<td id="S5.T2.4.1.9.9.9" class="ltx_td ltx_align_center">0.57</td>
<td id="S5.T2.4.1.9.9.10" class="ltx_td ltx_align_center">96.67</td>
<td id="S5.T2.4.1.9.9.11" class="ltx_td ltx_align_center ltx_border_r">91.18</td>
</tr>
<tr id="S5.T2.4.1.10.10" class="ltx_tr">
<th id="S5.T2.4.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span id="S5.T2.4.1.10.10.1.1" class="ltx_text ltx_font_bold">Fusion (Max)</span></th>
<td id="S5.T2.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b">68.04</td>
<td id="S5.T2.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b">20.57</td>
<td id="S5.T2.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b">0.66</td>
<td id="S5.T2.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b">88.18</td>
<td id="S5.T2.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">95.66</td>
<td id="S5.T2.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b">74.82</td>
<td id="S5.T2.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.4.1.10.10.8.1" class="ltx_text ltx_font_bold">14.96</span></td>
<td id="S5.T2.4.1.10.10.9" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T2.4.1.10.10.9.1" class="ltx_text ltx_font_bold">0.86</span></td>
<td id="S5.T2.4.1.10.10.10" class="ltx_td ltx_align_center ltx_border_b">84.13</td>
<td id="S5.T2.4.1.10.10.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">92.89</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">To summarize, despite the expected performance degradation observed in the challenging cross-setting experiments, the proposed fusion of face verification systems trained with complementary information from different 3DFR algorithms demonstrates significant improvement compared to the Baseline and individual 3DFR-enhanced systems. Therefore, the proposed fusion strategy represents a good solution for video surveillance contexts.
The effects of the differences between the technical characteristics of the surveillance cameras and the impact of the different acquisition distances should be further investigated in future research to highlight their impact on system reliability and, therefore, the suitability in specific application scenarios.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we investigate the complementary information provided by three state-of-the-art 3D face reconstruction (3DFR) algorithms and the effectiveness of score-level fusion in leveraging this information to improve face recognition in a video surveillance scenario.
We first assess this complementarity by analyzing the linear correlation between the scores produced by different face verification systems, each utilizing the same Siamese Neural Network, using one between the two examined backbones, but enhanced by a different 3DFR algorithm.
Then, we also employ three non-parametric fusion methods to combine the individual systems based on the same backbone.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Therefore, we evaluate the robustness of the proposed approach using a common experimental setup when dealing with data acquired in intra- and cross-setting, involving variations in acquisition distance and surveillance camera. This allows us to explore different application contexts: the intra-setting experiments simulate a context in which the designer knows the data characteristics and has access to representative data for training the recognition system; the cross-setting experiments simulate a more unfavorable application context in which data characteristics are unknown or representative data cannot be obtained.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The results obtained from our experiments confirm the initial hypothesis that deep-learning face recognition systems can extract distinct information from different state-of-the-art 3DFR algorithms, and the score-level fusion methods effectively leverage this information to enhance inference robustness.
This improvement can be observed even on probe images obtained from surveillance cameras and at distances different from the ones related to training data.
Therefore, this approach could represent a valid tool for improving facial recognition in challenging surveillance scenarios like the one considered in this work.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Future studies should explore other fusion approaches, such as parametric rules and those based on machine-learning models, which may better exploit the observed complementarity.
Similarly, more 3DFR algorithms, enhancement approaches, and deep neural networks could be included in the study, and the impact of the proposed approach on different pre-processing settings should be investigated. Finally, the effects of the differences in terms of acquisition distance and surveillance camera between training and test data should be further investigated.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">This preliminary study provides valuable insights into the effectiveness of fusion methods in exploiting the complementarity among 3D face reconstruction algorithms within video surveillance contexts. We believe this initial exploration holds promise and could pave the way for developing robust multi-modal face recognition systems capable of recognizing never-seen-before subjects, thereby aiding forensic practitioners and security personnel in identifying criminals and finding missing persons.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Funding from Cátedra ENIA UAM-VERIDAS en IA Responsable (NextGenerationEU PRTR TSI-100927-2023-2) and project BBforTAI (PID2021-127641OB-I00 MICINN/FEDER). The work has been conducted within the sAIfer Lab and the ELLIS Unit Madrid.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Abate, A.F., Nappi, M., Riccio, D., Sabatino, G.: 2D and 3D face recognition: A survey. Pattern Recognition Letters <span id="bib.bib1.1.1" class="ltx_text ltx_font_bold">28</span>(14), 1885–1906 (2007), image: Information and Control

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Amor, B.B., Ouji, K., Ardabilian, M., Chen, L.: 3D face recognition by icp-based shape matching. LIRIS Lab, Lyon Research Center for Images and Intelligent Information Systems, UMR <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">5205</span> (2005)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Blanz, V., Vetter, T.: A Morphable Model For The Synthesis Of 3D Faces. Association for Computing Machinery, New York, NY, USA, 1 edn. (2023)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cao, J., Hu, Y., Zhang, H., He, R., Sun, Z.: Towards high fidelity face frontalization in the wild. International Journal of Computer Vision <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">128</span>(5), 1485–1504 (2020)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Castro, H.F., Cardoso, J.S., Andrade, M.T.: A systematic survey of ml datasets for prime cv research areas—media and metadata. Data <span id="bib.bib5.1.1" class="ltx_text ltx_font_bold">6</span>(2),  12 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1251–1258 (2017)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Concas, S., Gao, J., Cuccu, C., Orrù, G., Feng, X., Marcialis, G.L., Puglisi, G., Roli, F.: Experimental results on multi-modal deepfake detection. In: International Conference on Image Analysis and Processing. pp. 164–175. Springer (2022)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Creusot, C., Pears, N., Austin, J.: A machine-learning approach to keypoint detection and landmarking on 3D meshes. International Journal of Computer Vision <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">102</span>(1), 146–179 (2013)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dass, S.C., Nandakumar, K., Jain, A.K.: A principled approach to score level fusion in multimodal biometric systems. In: International Conference on Audio- and Video-based Biometric Person Authentication. pp. 1049–1058. Springer (2005)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
DeAndres-Tame, I., Tolosana, R., Melzi, P., Vera-Rodriguez, R., Kim, M., Rathgeb, C., Liu, X., Morales, A., Fierrez, J., Ortega-Garcia, J., et al.: Second Edition FRCSyn challenge at CVPR 2024: face recognition challenge in the era of synthetic data. In: Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Dib, A., Bharaj, G., Ahn, J., Thébault, C., Gosselin, P., Romeo, M., Chevallier, L.: Practical face reconstruction via differentiable ray tracing. In: Computer Graphics Forum. vol. 40, pp. 153–164. Wiley Online Library (2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fuad, M.T.H., Fime, A.A., Sikder, D., Iftee, M.A.R., Rabbi, J., Al-Rakhami, M.S., Gumaei, A., Sen, O., Fuad, M., Islam, M.N.: Recent advances in deep learning techniques for face recognition. IEEE Access <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">9</span>, 99112–99142 (2021)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Geng, Z., Cao, C., Tulyakov, S.: Towards photo-realistic facial expression manipulation. International Journal of Computer Vision <span id="bib.bib13.1.1" class="ltx_text ltx_font_bold">128</span>, 2744–2761 (2020)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Grgic, M., Delac, K., Grgic, S.: Scface–surveillance cameras face database. Multimedia Tools and Applications <span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">51</span>(3), 863–879 (2011)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z., Li, S.Z.: Towards fast, accurate and stable 3D dense face alignment. In: Proceedings of the European Conference on Computer Vision (ECCV) (2020)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Gwyn, T., Roy, K., Atay, M.: Face recognition using popular deep net architectures: A brief comparative study. Future Internet <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">13</span>(7),  164 (2021)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Han, H., Jain, A.K.: 3D face texture modeling from uncalibrated frontal and profile images. In: 2012 IEEE Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS). pp. 223–230. IEEE (2012)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Hassner, T., Harel, S., Paz, E., Enbar, R.: Effective face frontalization in unconstrained images. In: Proceedings of the IEEE Conference on Computer Vision and Pttern Recognition. pp. 4295–4304 (2015)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Heike, C.L., Upson, K., Stuhaug, E., Weinberg, S.M.: 3D digital stereophotogrammetry: a practical guide to facial image acquisition. Head &amp; Face Medicine <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">6</span>(1), 1–11 (2010)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Hu, X., Peng, S., Wang, L., Yang, Z., Li, Z.: Surveillance video face recognition with single sample per person based on 3D modeling and blurring. Neurocomputing <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">235</span>, 46–58 (2017)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Huang, G.B., Mattar, M., Berg, T., Learned-Miller, E.: Labeled faces in the wild: A database for studying face recognition in unconstrained environments. In: Workshop on Faces in ’Real-Life’ Images: Detection, Alignment, and Recognition (2008)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Huber, P., Hu, G., Tena, R., Mortazavian, P., Koppen, P., Christmas, W.J., Ratsch, M., Kittler, J.: A multiresolution 3D morphable face model and fitting framework. In: Proceedings of the 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (2016)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kumar, M., Mann, R.: Masked face recognition using deep learning model. In: 2021 3rd International Conference on Advances in Computing, Communication Control and Networking (ICAC3N). pp. 428–432. IEEE (2021)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
La Cava, S.M., Orrù, G., Drahansky, M., Marcialis, G.L., Roli, F.: 3D face reconstruction: the road to forensics. ACM Computing Surveys (CSUR) <span id="bib.bib24.1.1" class="ltx_text ltx_font_bold">56</span>(3), 1–38 (2023)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
La Cava, S.M., Orrù, G., Goldmann, T., Drahansky, M., Marcialis, G.L.: 3D face reconstruction for forensic recognition-a survey. In: 2022 26th International Conference on Pattern Recognition (ICPR). pp. 930–937. IEEE (2022)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Lai, S.C., Lam, K.M.: Deep siamese network for low-resolution face recognition. In: 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 1444–1449. IEEE (2021)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Liang, J., Liu, F., Tu, H., Zhao, Q., Jain, A.K.: On mugshot-based arbitrary view face recognition. In: 2018 24th International Conference on Pattern Recognition (ICPR). pp. 3126–3131. IEEE (2018)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Marcialis, G.L., Roli, F.: Fusion of appearance-based face recognition algorithms. Pattern Analysis and Applications <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">7</span>, 151–163 (2004)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Melzi, P., Tolosana, R., Vera-Rodriguez, R., Kim, M., Rathgeb, C., Liu, X., DeAndres-Tame, I., Morales, A., Fierrez, J., Ortega-Garcia, J., et al.: FRCSyn-onGoing: benchmarking and comprehensive evaluation of real and synthetic data to improve face recognition systems. Information Fusion <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">107</span>, 102322 (2024)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Morales, A., Piella, G., Sukno, F.M.: Survey on 3D face reconstruction from uncalibrated images. Computer Science Review <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">40</span>, 100400 (2021)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Noore, A., Singh, R., Vasta, M.: Fusion, Sensor-Level, pp. 616–621. Springer US, Boston, MA (2009)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Osadciw, L., Veeramachaneni, K.: Fusion, Decision-Level, pp. 593–597. Springer US, Boston, MA (2009)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Peng, J., Abd El-Latif, A.A., Li, Q., Niu, X.: Multimodal biometric authentication based on score level fusion of finger biometrics. Optik <span id="bib.bib33.1.1" class="ltx_text ltx_font_bold">125</span>(23), 6891–6897 (2014)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Punyani, P., Kumar, A.: Evaluation of fusion at different levels for face recognition. In: 2017 Int. Conference on Computing, Communication and Automation (ICCCA). pp. 1052–1055 (May 2017)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Richardson, E., Sela, M., Kimmel, R.: 3D face reconstruction by learning from synthetic data. In: Proc. Fourth International Conference on 3D Vision (2016)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Ross, A., Nandakumar, K.: Fusion, Score-Level, pp. 611–616. Springer US, Boston, MA (2009)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Shahreza, H.O., Ecabert, C., George, A., Unnervik, A., Marcel, S., Di Domenico, N., Borghi, G., Maltoni, D., Boutros, F., Vogel, J., et al.: SDFR: synthetic data for face recognition competition. In: Proc. IEEE International Conference on Automatic Face and Gesture Recognition (2024)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Soltanpour, S., Boufama, B., Wu, Q.J.: A survey of local feature methods for 3D face recognition. Pattern Recognition <span id="bib.bib39.1.1" class="ltx_text ltx_font_bold">72</span>, 391–406 (2017)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Sun, K., Wu, S., Huang, Z., Zhang, N., Wang, Q., Li, H.: Controllable 3D face synthesis with conditional generative occupancy fields. In: Proc. Advances in Neural Information Processing Systems (2022)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Ortega-Garcia, J.: Exploring recurrent neural networks for on-line handwritten signature biometrics. IEEE Access <span id="bib.bib41.1.1" class="ltx_text ltx_font_bold">6</span>, 5128–5138 (2018)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Vishi, K., Mavroeidis, V.: An Evaluation of Score Level Fusion Approaches for Fingerprint and Finger-vein Biometrics (2018)

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Yang, F., Yang, W., Gao, R., Liao, Q.: Discriminative multidimensional scaling for low-resolution face recognition. IEEE Signal Processing Letters <span id="bib.bib43.1.1" class="ltx_text ltx_font_bold">25</span>(3), 388–392 (2017)

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Zhang, X., Gao, Y., Leung, M.K.: Recognizing rotated faces from frontal and side views: An approach toward effective use of mugshot databases. IEEE Transactions on Information Forensics and Security <span id="bib.bib44.1.1" class="ltx_text ltx_font_bold">3</span>(4), 684–697 (2008)

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Zhou, H., Liu, J., Liu, Z., Liu, Y., Wang, X.: Rotate-and-render: Unsupervised photorealistic face rotation from single-view images. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 5911–5920 (2020)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.10480" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.10481" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.10481">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.10481" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.10482" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Oct  5 22:32:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
