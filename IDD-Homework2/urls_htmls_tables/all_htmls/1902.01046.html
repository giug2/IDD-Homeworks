<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1902.01046] 1 Introduction</title><meta property="og:description" content="Federated Learning is a distributed machine learning approach which
enables model training on a large corpus of decentralized data. We have
built a scalable production system for Federated Learning in the
domain of mob…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1 Introduction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="1 Introduction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1902.01046">

<!--Generated on Fri Mar  8 11:27:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">marginparsep has been altered.
<br class="ltx_break">topmargin has been altered.
<br class="ltx_break">marginparwidth has been altered.
<br class="ltx_break">marginparpush has been altered.
<br class="ltx_break"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">The page layout violates the ICML style.</span>
Please do not change the page layout, or include packages like geometry,
savetrees, or fullpage, which change it for you.
We’re not able to reliably undo arbitrary changes to the style. Please remove
the offending package(s), or layout-changing commands and try again.</p>
</div>
<div id="p3" class="ltx_para ltx_align_center">
<p id="p3.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p4" class="ltx_para ltx_align_center">
<p id="p4.1" class="ltx_p"><span id="p4.1.1" class="ltx_text ltx_font_smallcaps">Towards Federated Learning at Scale: System Design</span></p>
</div>
<div id="p5" class="ltx_para ltx_align_center">
<p id="p5.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></p>
</div>
<div id="p6" class="ltx_para">
<p id="p6.1" class="ltx_p ltx_align_center"><span id="p6.1.1" class="ltx_text ltx_font_bold">Anonymous Authors</span><sup id="p6.1.2" class="ltx_sup">1 </sup></p>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated Learning is a distributed machine learning approach which
enables model training on a large corpus of decentralized data. We have
built a scalable production system for Federated Learning in the
domain of mobile devices, based on TensorFlow. In this paper, we describe
the resulting high-level design, sketch some of the challenges
and their solutions, and touch upon the open problems and
future directions.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>
<sup id="footnotex1.1" class="ltx_sup">1</sup>Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country.
Correspondence to: Anonymous Author &lt;anon.email@domain.com&gt;.
 
<br class="ltx_break">Preliminary work. Under review by the
Systems and Machine Learning (SysML) Conference. Do not distribute.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> is a distributed machine learning
approach which enables training on a large corpus of decentralized
data residing on devices like mobile phones. FL is one instance of
the more general approach of “bringing the code to the data, instead
of the data to the code” and addresses the fundamental problems of
privacy, ownership, and locality of data. The general description of
FL has been given by <cite class="ltx_cite ltx_citemacro_citet">McMahan &amp; Ramage (<a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>, and its theory has been explored
in <cite class="ltx_cite ltx_citemacro_citet">Konečný et al. (<a href="#bib.bib15" title="" class="ltx_ref">2016a</a>); McMahan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>; <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">A basic design decision for a Federated Learning infrastructure is
whether to focus on asynchronous or synchronous training
algorithms. While much successful work on deep learning has used
asynchronous training, e.g., <cite class="ltx_cite ltx_citemacro_citet">Dean et al. (<a href="#bib.bib9" title="" class="ltx_ref">2012</a>)</cite>, recently there has
been a consistent trend towards synchronous large batch training, even
in the data center <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a href="#bib.bib10" title="" class="ltx_ref">2017</a>; Smith et al., <a href="#bib.bib25" title="" class="ltx_ref">2018</a>)</cite>. The Federated
Averaging algorithm of <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> takes a similar approach. Further,
several approaches to enhancing privacy guarantees for FL, including
differential privacy <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> and Secure Aggregation
<cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>, essentially require some notion of synchronization on
a fixed set of devices, so that the server side of the learning
algorithm only consumes a simple aggregate of the updates from many
users. For all these reasons, we chose to focus on support for
synchronous rounds, while mitigating potential synchronization
overhead via several techniques we describe subsequently. Our system
is thus amenable to running large-batch SGD-style algorithms as well
as Federated Averaging, the primary algorithm we run in
production; pseudo-code is given in Appendix <a href="#A2" title="Appendix B Federated Averaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> for completeness.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this paper, we report on a system design for such algorithms in the
domain of mobile phones (Android). This work is still in an early
stage, and we do not have all problems solved, nor are we able to give
a comprehensive discussion of all required components. Rather, we
attempt to sketch the major components of the system, describe the
challenges, and identify the open issues, in the hope that this will
be useful to spark further systems research.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">Our system enables one to train a deep neural network, using
TensorFlow <cite class="ltx_cite ltx_citemacro_citep">(Abadi et al., <a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>, on data stored on the phone which will never
leave the device. The weights are combined in the cloud with Federated
Averaging, constructing a global model which is pushed back to phones
for inference. An implementation of Secure Aggregation <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al., <a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite>
ensures that on a global level individual updates from phones are
uninspectable. The system has been applied in large scale
applications, for instance in the realm of a phone keyboard.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Our work addresses numerous practical issues: device availability
that correlates with the local data distribution in complex ways
(e.g., time zone dependency); unreliable device connectivity and
interrupted execution; orchestration of lock-step execution across
devices with varying availability; and limited device storage and
compute resources. These issues are addressed at the communication
protocol, device, and server levels. We have reached a state of
maturity sufficient to deploy the system in production and solve
applied learning problems over tens of millions of real-world devices;
we anticipate uses where the number of devices reaches billions.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/1902.01046/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="518" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated Learning Protocol</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Protocol</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">To understand the system architecture, it is best to start from the
network protocol.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Basic Notions</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">The participants in the protocol are <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">devices</span> (currently Android
phones) and the <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_italic">FL server</span>, which is a cloud-based distributed
service.
Devices announce to the server that they are ready to run an <span id="S2.SS1.p1.1.3" class="ltx_text ltx_font_italic">FL
task</span> for a given <span id="S2.SS1.p1.1.4" class="ltx_text ltx_font_italic">FL population</span>. An FL population is specified
by a globally unique name which identifies the learning problem, or
application, which is worked upon. An FL task is a specific computation
for an FL population, such as training to be performed with given
hyperparameters, or evaluation of trained models on local device data.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">From the potential tens of thousands of devices announcing
availability to the server during a certain time window, the server
selects a subset of typically a few hundred which are invited to work
on a specific FL task (we discuss the reason for this subsetting in
Sec. <a href="#S2.SS2" title="2.2 Phases ‣ 2 Protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>). We call this rendezvous between devices and
server a <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">round</span>. Devices stay connected to the server for the
duration of the round.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p">The server tells the selected devices what computation to run with an
<span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_italic">FL plan</span>, a data structure that includes a TensorFlow graph
and instructions for how to execute it. Once a round is established,
the server next sends to each participant the current global model
parameters and any other necessary state as an <span id="S2.SS1.p3.1.2" class="ltx_text ltx_font_italic">FL checkpoint</span>
(essentially the serialized state of a TensorFlow session). Each
participant then performs a local computation based on the global
state and its local dataset, and sends an update in the form of an FL
checkpoint back to the server. The server incorporates these updates
into its global state, and the process repeats.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Phases</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">The communication protocol enables devices to advance the global,
singleton model of an FL population between rounds where each round
consists of the three phases shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
For simplicity, the description below does not include Secure
Aggregation, which is described in Sec. <a href="#S6" title="6 Secure Aggregation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Note that
even in the absence of Secure Aggregation, all network traffic is
encrypted on the wire.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<dl id="S2.I1" class="ltx_description">
<dt id="S2.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Selection</span></span></dt>
<dd class="ltx_item">
<div id="S2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.ix1.p1.1" class="ltx_p">Periodically, devices that meet the eligibility
criteria (e.g., charging and connected to an unmetered network; see
Sec. <a href="#S3" title="3 Device" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) check in to the server by opening a
bidirectional stream. The stream is used to track liveness and
orchestrate multi-step communication. The server selects a subset of
connected devices based on certain goals like the optimal number of
participating devices (typically a few hundred devices participate
in each round). If a device
is not selected for participation, the server responds with
instructions to reconnect at a later point in time.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>In the
current implementation, selection is done by simple reservoir
sampling, but the protocol is amenable to more sophisticated
methods which address selection bias.</span></span></span></p>
</div>
</dd>
<dt id="S2.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Configuration</span></span></dt>
<dd class="ltx_item">
<div id="S2.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.ix2.p1.1" class="ltx_p">The server is configured based on the
aggregation mechanism selected (e.g., simple or Secure Aggregation)
for the selected devices. The server sends the FL plan and an FL checkpoint
with the global model to each of the devices.</p>
</div>
</dd>
<dt id="S2.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S2.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Reporting</span></span></dt>
<dd class="ltx_item">
<div id="S2.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S2.I1.ix3.p1.1" class="ltx_p">The server waits for the participating devices to
report updates. As updates are received, the server aggregates them
using Federated Averaging and instructs the reporting devices when
to reconnect (see also Sec. <a href="#S2.SS3" title="2.3 Pace Steering ‣ 2 Protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>). If enough
devices report in time, the round will be successfully completed
and the server will update its global model, otherwise the round is
abandoned.</p>
</div>
</dd>
</dl>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p">As seen in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, straggling devices which do
not report back in time or do not react on configuration by the
server will simply be ignored. The protocol has a certain tolerance
for such drop-outs which is configurable per FL task.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p">The selection and reporting phases are specified by a set of
parameters which spawn flexible time windows. For example, for the
selection phase the server considers a device participant goal count,
a timeout, and a minimal percentage of the goal count which is
required to run the round. The selection phase lasts until the goal
count is reached or a timeout occurs; in the latter case, the round
will be started or abandoned depending on whether the minimal goal
count has been reached.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Pace Steering</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Pace steering is a flow control mechanism regulating the
pattern of device connections. It enables the FL server both to scale
down to handle small FL populations as well to scale up to very large
FL populations.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Pace steering is based on the simple mechanism of the server suggesting
to the device the optimum time window to reconnect. The device attempts to
respect this, modulo its eligibility.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p">In the case of small FL populations, pace steering is used to ensure
that a sufficient number of devices connect to the server
simultaneously. This is important both for the rate of task progress
and for the security properties of the Secure Aggregation
protocol. The server uses a stateless probabilistic algorithm
requiring no additional device/server communication to suggest reconnection
times to rejected devices so that subsequent checkins are likely to
arrive contemporaneously.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.1" class="ltx_p">For large FL populations, pace steering is used to randomize device
check-in times, avoiding the “thundering herd” problem, and
instructing devices to connect as frequently as needed to run all
scheduled FL tasks, but not more.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para ltx_noindent">
<p id="S2.SS3.p5.1" class="ltx_p">Pace steering also takes into account the diurnal oscillation in the
number of active devices, and is able to adjust the time window
accordingly, avoiding excessive activity during peak hours and without
hurting FL performance during other times of the day.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Device</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/1902.01046/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="460" height="352" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Device Architecture</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">This section describes the software architecture running on a device
participating in FL. This describes our Android implementation but note
that the architectural choices made here are not particularly
platform-specific.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">The device’s first responsibility in on-device learning is to maintain
a repository of locally collected data for model training and
evaluation. Applications are responsible for making their data
available to the FL runtime as an <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">example store</span> by
implementing an API we provide. An application’s example store might,
for example, be an SQLite database recording action suggestions shown
to the user and whether or not those suggestions were accepted. We
recommend that applications limit the total storage footprint of their
example stores, and automatically remove old data after a
pre-designated expiration time, where appropriate. We provide
utilities to make these tasks easy. Data stored on devices may be
vulnerable to threats like malware or physical disassembly of the
phone, so we recommend that applications follow the best practices for
on-device data security, including ensuring that data is encrypted at
rest in the platform-recommended manner.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">The FL runtime, when provided a task by the FL server, accesses an appropriate
example store to compute model updates, or evaluate model quality on held out
data. Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Device" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the relationship between the
example store and the FL runtime. Control flow consists of the following steps:</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<dl id="S3.I1" class="ltx_description">
<dt id="S3.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Programmatic Configuration</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.ix1.p1.1" class="ltx_p">An application configures the FL
runtime by providing an FL population name and registering its
example stores. This schedules a periodic FL runtime job using
Android’s JobScheduler. Possibly the most important requirement for
training machine learning (ML) models on end users’ devices is to
avoid any negative impact on the user experience, data usage, or
battery life. The FL runtime requests that the job scheduler only
invoke the job when the phone is idle, charging, and connected to an
unmetered network such as WiFi. Once started, the FL runtime will abort,
freeing the allocated resources, if these conditions are no longer met.</p>
</div>
</dd>
<dt id="S3.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Job Invocation</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.ix2.p1.1" class="ltx_p">Upon invocation by the job scheduler in a
separate process, the FL runtime contacts the FL server to announce
that it is ready to run tasks for the given FL population. The
server decides whether any FL tasks are available for the population
and will either return an FL plan or a suggested time to check in
later.</p>
</div>
</dd>
<dt id="S3.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Task Execution</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.ix3.p1.1" class="ltx_p">If the device has been selected, the FL runtime receives
the FL plan, queries the app’s example store for data requested by the plan,
and computes plan-determined model updates and metrics.</p>
</div>
</dd>
<dt id="S3.I1.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix4.1.1.1" class="ltx_text ltx_font_bold">Reporting</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix4.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.ix4.p1.1" class="ltx_p">After FL plan execution, the FL runtime reports computed
updates and metrics to the server and cleans up any temporary resources.</p>
</div>
</dd>
</dl>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">As already mentioned, FL plans are not specialized to training, but can also
encode <span id="S3.p5.1.1" class="ltx_text ltx_font_italic">evaluation</span> tasks - computing quality metrics from
held out data that wasn’t used for training, analogous to the
validation step in data center training.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p">Our design enables the FL runtime to either run within the application that
configured it or in a centralized service hosted in another app. Choosing
between these two requires minimal code changes. Communication between the
application, the FL runtime, and the application’s example store as depicted in
Fig. <a href="#S3.F2" title="Figure 2 ‣ 3 Device" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is implemented via Android’s AIDL IPC mechanism, which
works both within a single app and across apps.</p>
</div>
<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Multi-Tenancy</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Our implementation provides a <span id="S3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">multi-tenant</span> architecture,
supporting training of multiple FL populations in the same app (or service).
This allows for coordination between multiple training activities, avoiding
the device being overloaded by many simultaneous training sessions at once.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attestation</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">We want devices to participate in FL anonymously, which excludes the
possibility of authenticating them via a user identity.
Without verifying user identity, we need to protect against attacks to
influence the FL result from non-genuine devices. We do so by using Android’s
remote attestation mechanism <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">Android Documentation, </a>)</cite>, which helps to ensure
that only genuine devices and applications participate in FL, and
gives us some protection against data poisoning <cite class="ltx_cite ltx_citemacro_citep">(Bagdasaryan et al., <a href="#bib.bib4" title="" class="ltx_ref">2018</a>)</cite>
via compromised devices. Other forms of model manipulation – such as content
farms using uncompromised phones to steer a model – are also potential
areas of concern that we do not address in the scope of this paper.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Server</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">The design of the FL server is driven by the necessity to operate
over many orders of magnitude of population sizes and other
dimensions. The server must work with FL populations whose sizes range
from tens of devices (during development) to hundreds of millions, and
be able to process rounds with participant count ranging from tens of
devices to tens of thousands. Also, the size of the updates collected
and communicated during each round can range in size from kilobytes to
tens of megabytes. Finally, the amount of traffic coming into or out
of any given geographic region can vary dramatically over a day based
on when devices are idle and charging. This section details the design
of the FL server infrastructure given these requirements.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Actor Model</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">The FL server is designed around the <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">Actor Programming Model</span>
<cite class="ltx_cite ltx_citemacro_citep">(Hewitt et al., <a href="#bib.bib12" title="" class="ltx_ref">1973</a>)</cite>. Actors are universal primitives of concurrent computation which
use message passing as the sole communication mechanism.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">Each actor handles a stream of messages/events strictly sequentially,
leading to a simple programming model. Running multiple instances of
actors of the same type allows a natural scaling to large number of
processors/machines. In response to a message, an actor can make
local decisions, send messages to other actors, or create more actors
dynamically. Depending on the function and scalability requirements, actor
instances can be co-located on the same process/machine or distributed
across data centers in multiple geographic regions, using either
explicit or automatic configuration mechanisms. Creating and placing
fine-grained ephemeral instances of actors just for the duration of a
given FL task enables dynamic resource management and load-balancing decisions.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The main actors in the system are shown in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.2 Architecture ‣ 4 Server" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1902.01046/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="416" height="458" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Actors in the FL Server Architecture</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<dl id="S4.I1" class="ltx_description">
<dt id="S4.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Coordinators</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p">are the top-level actors which enable global
synchronization and advancing rounds in lockstep. There are multiple
Coordinators, and each one is responsible for an FL population of
devices. A Coordinator registers its address and the FL population it
manages in a shared locking service, so there is always a single
owner for every FL population which is reachable by other actors in the
system, notably the Selectors. The Coordinator receives information
about how many devices are connected to each Selector and instructs them
how many devices to accept for participation, based on which FL tasks
are scheduled. Coordinators spawn Master Aggregators to manage the
rounds of each FL task.</p>
</div>
</dd>
<dt id="S4.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Selectors</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p">are responsible for accepting and forwarding device
connections. They periodically receive information from the
Coordinator about how many devices are needed for each FL
population, which they use to make local decisions about whether or
not to accept each device. After the Master Aggregator and set of
Aggregators are spawned, the Coordinator instructs the Selectors to
forward a subset of its connected devices to the Aggregators,
allowing the Coordinator to efficiently allocate devices to FL tasks
regardless of how many devices are available. The approach also
allows the Selectors to be globally distributed (close to devices)
and limit communication with the remote Coordinator.</p>
</div>
</dd>
<dt id="S4.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S4.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Master Aggregators</span></span></dt>
<dd class="ltx_item">
<div id="S4.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.ix3.p1.1" class="ltx_p">manage the rounds of each FL task. In order
to scale with the number of devices and update size, they make
dynamic decisions to spawn one or more <span id="S4.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Aggregators</span> to which
work is delegated.</p>
</div>
</dd>
</dl>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">No information for a round is written to persistent storage until it is fully
aggregated by the Master Aggregator. Specifically, all actors keep their state
in memory and are ephemeral. Ephemeral actors improve scalability by removing
the latency normally incurred by distributed storage. In-memory aggregation also
removes the possibility of attacks within the data center that target persistent
logs of per-device updates, because no such logs exist.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pipelining</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">While Selection, Configuration and Reporting phases of a round
(Sec. <a href="#S2" title="2 Protocol" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) are sequential, the Selection phase doesn’t
depend on any input from a previous round. This enables latency
optimization by running the Selection phase of the next round of the
protocol in parallel with the Configuration/Reporting phases of a previous
round. Our system architecture enables such pipelining without adding
extra complexity, as parallelism is achieved simply by the virtue of
Selector actors running the selection process continuously.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Failure Modes</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">In all failure cases the system will continue to make progress, either
by completing the current round or restarting from the results of the
previously committed round. In many cases, the loss of an actor will
not prevent the round from succeeding. For example, if an Aggregator
or Selector crashes, only the devices connected to that actor will be
lost. If the Master Aggregator fails, the current round of the FL task
it manages will fail, but will then be restarted by the
Coordinator. Finally, if the Coordinator dies, the Selector
layer will detect this and respawn it. Because the Coordinators are
registered in a shared locking service, this will happen exactly once.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analytics</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">There are many factors and failsafes in the interaction between
devices and servers. Moreover, much of the platform activity happens on
devices that we neither control nor have access to.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">For this reason, we rely on analytics to understand what is actually
going on in the field, and monitor devices’ health statistics. On the
device side we perform computation-intensive operations, and must
avoid wasting the phone’s battery or bandwidth, or degrading the
performance of the phone. To ensure this, we log several activity and
health parameters to the cloud. For example: the device state in which
training was activated, how often and how long it ran, how much memory
it used, which errors where detected, which phone model / OS / FL
runtime version was used, and so on. These log entries do not contain
any personally identifiable information (PII). They are aggregated and
presented in dashboards to be analyzed, and fed into automatic
time-series monitors that trigger alerts on substantial deviations.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">We also log an event for every state in a training round, and use
these logs to generate ASCII visualizations of the sequence of state
transitions happening across all devices (see
Table <a href="#A1.T1" title="Table 1 ‣ Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in the appendix). We chart
counts of these sequence visualizations in our dashboards, which
allows us to quickly distinguish between different types of
issues. For example, the sequence “checking in, downloaded plan,
started training, ended training, starting upload, error” is
visualized as “<span id="S5.p3.1.1" class="ltx_text ltx_font_typewriter">-v[]+*</span>”, while the shorter sequence
“checking in, downloaded plan, started training, error” is
“<span id="S5.p3.1.2" class="ltx_text ltx_font_typewriter">-v[*</span>”. The first indicates that a model trained
successfully but the results upload failed (a network issue),
whereas the second indicates that a training round failed right
after loading the model (a model issue).</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">Server side, we similarly collect information such as how many
devices where accepted and rejected per training round, the timing of
the various phases of the round, throughput in terms of uploaded and
downloaded data, errors, and so on.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p">Since the platform’s deployment, we have relied on the analytics layer
repeatedly to discover issues and verify that they were resolved. Some
of the incidents we discovered were device health related, for example
discovering that training was happening when it shouldn’t have, while
others were functional, for example discovering that the drop out
rates of training participants were much higher than expected.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p">Federated training does not impact the user experience,
so both device and server <span id="S5.p6.1.1" class="ltx_text ltx_font_italic">functional</span> failures do not have an
immediate negative impact. But failures to operate properly could have
secondary consequences leading to utility degradation of the device. Device
utility to the user is mission critical, and degradations are difficult
to pinpoint and easy to wrongly diagnose.
Using accurate analytics to prevent federated training
from negatively impacting the device’s utility
to the user accounts for a substantial part of our engineering
and risk mitigation costs.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Secure Aggregation</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.2" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Bonawitz et al. (<a href="#bib.bib5" title="" class="ltx_ref">2017</a>)</cite> introduced <span id="S6.p1.2.1" class="ltx_text ltx_font_italic">Secure Aggregation</span>, a Secure
Multi-Party Computation protocol that uses encryption to make
individual devices’ updates uninspectable by a server, instead only
revealing the sum after a sufficient number of updates have been
received. We can deploy Secure Aggregation as a privacy enhancement to
the FL service that protects against additional threats within the
data center by ensuring that individual devices’ updates remain
encrypted even in-memory. Formally, Secure Aggregation protects from
“honest but curious” attackers that may have access to the memory of
Aggregator instances. Importantly, the only aggregates needed for
model evaluation, SGD, or Federated Averaging are sums (e.g.,
<math id="S6.p1.1.m1.1" class="ltx_Math" alttext="\bar{w}_{t}" display="inline"><semantics id="S6.p1.1.m1.1a"><msub id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mover accent="true" id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml"><mi id="S6.p1.1.m1.1.1.2.2" xref="S6.p1.1.m1.1.1.2.2.cmml">w</mi><mo id="S6.p1.1.m1.1.1.2.1" xref="S6.p1.1.m1.1.1.2.1.cmml">¯</mo></mover><mi id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1">subscript</csymbol><apply id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2"><ci id="S6.p1.1.m1.1.1.2.1.cmml" xref="S6.p1.1.m1.1.1.2.1">¯</ci><ci id="S6.p1.1.m1.1.1.2.2.cmml" xref="S6.p1.1.m1.1.1.2.2">𝑤</ci></apply><ci id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">\bar{w}_{t}</annotation></semantics></math> and <math id="S6.p1.2.m2.1" class="ltx_Math" alttext="\bar{n}_{t}" display="inline"><semantics id="S6.p1.2.m2.1a"><msub id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mover accent="true" id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml"><mi id="S6.p1.2.m2.1.1.2.2" xref="S6.p1.2.m2.1.1.2.2.cmml">n</mi><mo id="S6.p1.2.m2.1.1.2.1" xref="S6.p1.2.m2.1.1.2.1.cmml">¯</mo></mover><mi id="S6.p1.2.m2.1.1.3" xref="S6.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1">subscript</csymbol><apply id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2"><ci id="S6.p1.2.m2.1.1.2.1.cmml" xref="S6.p1.2.m2.1.1.2.1">¯</ci><ci id="S6.p1.2.m2.1.1.2.2.cmml" xref="S6.p1.2.m2.1.1.2.2">𝑛</ci></apply><ci id="S6.p1.2.m2.1.1.3.cmml" xref="S6.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">\bar{n}_{t}</annotation></semantics></math> in Appendix <a href="#alg1" title="Algorithm 1 ‣ Appendix B Federated Averaging" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
It is important to note that the
goal of our system is to provide the tools to build privacy
preserving applications. Privacy is enhanced by the ephemeral and
focused nature of the FL updates, and can be further augmented with
Secure Aggregation and/or differential privacy — e.g., the
techniques of <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> are currently implemented.
However, while the platform is designed to support a variety of
privacy-enhancing technologies, stating specific privacy guarantees
depends on the details of the application and the details of how
these technologies are used; such a discussion is beyond the scope
of the current work.</span></span></span></p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">Secure Aggregation is a four-round interactive protocol optionally
enabled during the reporting phase of a given FL round. In each
protocol round, the server gathers messages from all devices in the FL
round, then uses the set of device messages to compute an independent
response to return to each device. The protocol is designed to be
robust to a significant fraction of devices dropping out before the
protocol is complete. The first two rounds constitute a Prepare phase,
in which shared secrets are established and during which devices who
drop out will not have their updates included in the final
aggregation. The third round constitutes a Commit phase, during which
devices upload cryptographically masked model updates and the server
accumulates a sum of the masked updates. All devices who complete
this round will have their model update included in the protocol’s
final aggregate update, or else the entire aggregation will fail. The
last round of the protocol constitutes a Finalization phase, during
which devices reveal sufficient cryptographic secrets to allow the
server to unmask the aggregated model update. Not all committed
devices are required to complete this round; so long as a sufficient
number of the devices who started to protocol survive through the
Finalization phase, the entire protocol succeeds.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.2" class="ltx_p">Several costs for Secure Aggregation grow quadratically with the
number of users, most notably the computational cost for the
server. In practice, this limits the maximum size of a Secure
Aggregation to hundreds of users. So as
not to constrain the number of users that may participate in each
round of federated computation, we run an instance of Secure
Aggregation on each Aggregator actor (see Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.2 Architecture ‣ 4 Server" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) to
aggregate inputs from that Aggregator’s devices into an intermediate
sum; FL tasks define a parameter <math id="S6.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.p3.1.m1.1a"><mi id="S6.p3.1.m1.1.1" xref="S6.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p3.1.m1.1b"><ci id="S6.p3.1.m1.1.1.cmml" xref="S6.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.1.m1.1c">k</annotation></semantics></math> so that all updates
are securely aggregated over groups of size at least <math id="S6.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.p3.2.m2.1a"><mi id="S6.p3.2.m2.1.1" xref="S6.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.p3.2.m2.1b"><ci id="S6.p3.2.m2.1.1.cmml" xref="S6.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p3.2.m2.1c">k</annotation></semantics></math>. The Master
Aggregator then further aggregates the intermediate aggregators’
results into a final aggregate for the round, without Secure
Aggregation.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Tools and Workflow</h2>

<figure id="S7.F4" class="ltx_figure"><img src="/html/1902.01046/assets/x4.png" id="S7.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="518" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Model Engineer Workflow</figcaption>
</figure>
<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">Compared to the standard model engineer workflows on centrally collected
data, on-device training poses multiple novel challenges. First, individual
training examples are not directly inspectable, requiring tooling to work with
proxy data in testing and simulation (Sec. <a href="#S7.SS1" title="7.1 Modeling and Simulation ‣ 7 Tools and Workflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.1</span></a>). Second, models
cannot be run interactively but must instead be compiled into FL plans to be
deployed via the FL server (Sec. <a href="#S7.SS2" title="7.2 Plan Generation ‣ 7 Tools and Workflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.2</span></a>). Finally, because FL plans
run on real devices, model resource consumption and runtime compatibility must
be verified automatically by the infrastructure (Sec. <a href="#S7.SS3" title="7.3 Versioning, Testing, and Deployment ‣ 7 Tools and Workflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a>).</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p">The primary developer surface of model engineers working with the FL system
is a set of Python interfaces and tools to define, test, and deploy
TensorFlow-based FL tasks to the fleet of mobile devices via the FL server.
The workflow of a model engineer for FL is depicted in Fig. <a href="#S7.F4" title="Figure 4 ‣ 7 Tools and Workflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
and described below.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Modeling and Simulation</h3>

<div id="S7.SS1.p1" class="ltx_para ltx_noindent">
<p id="S7.SS1.p1.1" class="ltx_p">Model engineers begin by defining the FL tasks that they would like to run
on a given FL population in Python. Our library enables model engineers to
declare Federated Learning and evaluation tasks using engineer-provided
TensorFlow functions. The role of these functions is to map input tensors
to output metrics like loss or accuracy. During development, model engineers
may use sample test data or other proxy data as inputs. When deployed, the
inputs will be provided from the on-device example store via the FL runtime.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para ltx_noindent">
<p id="S7.SS1.p2.1" class="ltx_p">The role of the modeling infrastructure is to enable model engineers to focus
on their model, using our libraries to build and test the corresponding FL
tasks. FL tasks are validated against engineer-provided test data and
expectations, similar in nature to unit tests. FL task tests are ultimately
required in order to deploy a model as described below in
Sec. <a href="#S7.SS3" title="7.3 Versioning, Testing, and Deployment ‣ 7 Tools and Workflow" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7.3</span></a>.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para ltx_noindent">
<p id="S7.SS1.p3.1" class="ltx_p">The configuration of tasks is also written in Python and includes runtime
parameters such as the optimal number of devices in a round as well as model
hyperparameters like learning rate. FL tasks may be defined in groups: for
example, to evaluate a grid search over learning rates. When more than
one FL task is deployed in an FL population, the FL service chooses among them
using a dynamic strategy that allows alternating between training and evaluation
of a single model or A/B comparisons between models.</p>
</div>
<div id="S7.SS1.p4" class="ltx_para ltx_noindent">
<p id="S7.SS1.p4.1" class="ltx_p">Initial hyperparameter exploration is sometimes done in simulation using proxy
data. Proxy data is similar in shape to the on-device data but drawn from a
different distribution – for example, text from Wikipedia may be viewed
as proxy data for text typed on a mobile keyboard. Our modeling tools
allow deployment of FL tasks to a simulated FL server and a fleet of cloud
jobs emulating devices on a large proxy dataset. The simulation executes the
same code as we run on device and communicates with the server using simulated
FL populations. Simulation can scale to a large number of devices and is
sometimes used to pre-train models on proxy data before it is refined by FL
in the field.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Plan Generation</h3>

<div id="S7.SS2.p1" class="ltx_para ltx_noindent">
<p id="S7.SS2.p1.1" class="ltx_p">Each FL task is associated with an FL plan. Plans are automatically
generated from the combination of model and configuration supplied by
the model engineer.
Typically, in data center training, the information which is encoded
in the FL plan would be represented by a Python program which
orchestrates a TensorFlow graph. However, we do not execute Python
directly on the server or devices. The FL plan’s purpose is to describe the desired
orchestration independent of Python.</p>
</div>
<div id="S7.SS2.p2" class="ltx_para ltx_noindent">
<p id="S7.SS2.p2.1" class="ltx_p">An FL plan consists of two parts: one for the device and one for the
server. The device portion of the FL plan contains, among other things:
the TensorFlow graph itself, selection criteria for
training data in the example store, instructions on how to batch data and
how many epochs to run on the device, labels for the nodes in the graph which
represent certain computations like loading and saving weights, and so
on. The server part contains the aggregation logic, which is encoded
in a similar way. Our libraries automatically split the
part of a provided model’s computation which runs on device from the
part that runs on the server (the aggregation).</p>
</div>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>Versioning, Testing, and Deployment</h3>

<div id="S7.SS3.p1" class="ltx_para ltx_noindent">
<p id="S7.SS3.p1.1" class="ltx_p">Model engineers working in the federated system are able to work productively
and safely, launching or ending multiple experiments per day. But because each
FL task may potentially be RAM-hogging or incompatible with version(s) of
TensorFlow running on the fleet, engineers rely on the FL system’s versioning,
testing, and deployment infrastructure for automated safety checks.</p>
</div>
<div id="S7.SS3.p2" class="ltx_para ltx_noindent">
<p id="S7.SS3.p2.1" class="ltx_p">An FL task that has been translated into an FL plan is not accepted by the
server for deployment unless certain conditions are met. First, it must
have been built from auditable, peer reviewed code. Second, it must have bundled test
predicates for each FL task that pass in simulation. Third, the resources
consumed during testing must be within a safe range of expected resources
for the target population. And finally, the FL task tests must pass on every
version of the TensorFlow runtime that the FL task claims to support,
as verified by testing the FL task’s plan in an Android emulator.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para ltx_noindent">
<p id="S7.SS3.p3.1" class="ltx_p"><span id="S7.SS3.p3.1.1" class="ltx_text ltx_font_italic">Versioning</span> is a specific challenge for on-device machine learning. In
contrast to data-center training, where the TensorFlow runtime and graphs can
generally be rebuilt as needed, devices may be running a version of the
TensorFlow runtime that is many months older than what is required by the FL plan
generated by modelers today. For example, the old runtime may be missing a
particular TensorFlow operator, or the signature of an operator may have changed
in an incompatible way. The FL infrastructure deals with this problem by
generating <span id="S7.SS3.p3.1.2" class="ltx_text ltx_font_italic">versioned</span> FL plans for each task. Each versioned FL plan
is derived from the default (unversioned) FL plan by transforming its
computation graph to achieve compatibility with a deployed TensorFlow version.
Versioned and unversioned plans must pass the same release tests, and are
therefore treated as semantically equivalent.
We encounter about one incompatible change that can be fixed with a graph
transformation every three months, and a slightly smaller number that cannot be
fixed without complex workarounds.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>Metrics</h3>

<div id="S7.SS4.p1" class="ltx_para ltx_noindent">
<p id="S7.SS4.p1.1" class="ltx_p">As soon as an FL task has been accepted for deployment, devices checking in
may be served the appropriate (versioned) plan. As soon as an FL round closes,
that round’s aggregated model parameters and metrics are written to the
server storage location chosen by the model engineer.</p>
</div>
<div id="S7.SS4.p2" class="ltx_para ltx_noindent">
<p id="S7.SS4.p2.1" class="ltx_p">Materialized model metrics are annotated with additional data, including
metadata like the source FL task’s name, FL round number within
the FL task, and other basic operational data. The metrics themselves are
summaries of device reports within the round via approximate order statistics
and moments like mean. The FL system provides analysis tools for model engineers
to load these metrics into standard Python numerical data science packages
for visualization and exploration.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Applications</h2>

<div id="S8.p1" class="ltx_para ltx_noindent">
<p id="S8.p1.1" class="ltx_p">Federated Learning applies best in situations where the on-device data
is more relevant than the data that exists on servers (e.g., the
devices generate the data in the first place), is privacy-sensitive,
or otherwise undesirable or infeasible to transmit to servers. Current
applications of Federated Learning are for supervised learning tasks, typically
using labels inferred from user activity (e.g., clicks or typed words).</p>
</div>
<div id="S8.p2" class="ltx_para ltx_noindent">
<dl id="S8.I1" class="ltx_description">
<dt id="S8.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S8.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">On-device item ranking</span></span></dt>
<dd class="ltx_item">
<div id="S8.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S8.I1.ix1.p1.1" class="ltx_p">A common use of machine learning in mobile applications is
selecting and ranking items from an on-device inventory. For example,
apps may expose a search mechanism for information retrieval
or in-app navigation, for example settings search on Google Pixel devices
<cite class="ltx_cite ltx_citemacro_citep">(ai.google, <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. By ranking these results on-device, expensive
calls to the server (in e.g., latency, bandwidth or power consumption
dimensions) are eliminated, and any potentially private information from
the search query and user selection remains on the device. Each user
interaction with the ranking feature can become a labeled
data point, since it’s possible to observe the user’s interaction with the
preferred item in the context of the full ranked list.</p>
</div>
</dd>
<dt id="S8.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S8.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Content suggestions for on-device keyboards</span></span></dt>
<dd class="ltx_item">
<div id="S8.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S8.I1.ix2.p1.1" class="ltx_p">On-device keyboard implementations can add value to users by suggesting
relevant content – for example, search queries that are related to the input
text. Federated Learning can be used to train ML models
for triggering the suggestion feature, as well as ranking the items
that can be suggested in the current context. This approach has been taken
by Google’s Gboard mobile keyboard team, using our FL system <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib27" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</dd>
<dt id="S8.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S8.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Next word prediction</span></span></dt>
<dd class="ltx_item">
<div id="S8.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S8.I1.ix3.p1.2" class="ltx_p">Gboard also used our FL platform to train
a recurrent neural network (RNN) for next-word-prediction
<cite class="ltx_cite ltx_citemacro_citep">(Hard et al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>. This model, which has about 1.4 million parameters,
converges in 3000 FL rounds after processing 6e8 sentences from
1.5e6 users over 5 days of training (so each round takes about 2–3
minutes).<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This is roughly <math id="footnote3.m1.1" class="ltx_math_unparsed" alttext="7\times" display="inline"><semantics id="footnote3.m1.1b"><mrow id="footnote3.m1.1c"><mn id="footnote3.m1.1.1">7</mn><mo lspace="0.222em" id="footnote3.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="footnote3.m1.1d">7\times</annotation></semantics></math> slower than in
comparable data center training of the same model. However, we do
not believe this type of comparison is the primary one – our main
goal is to enable training on data that is not available in the
data center. In fact, for the model mentioned different proxy
data was used for data center training. Nevertheless, fast
wall-clock convergence time is important for enabling model
engineers to iterate rapidly, and hence we are continuing to
optimize both our system and algorithms to decrease convergence
times.</span></span></span> It improves top-1 recall over a baseline <math id="S8.I1.ix3.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S8.I1.ix3.p1.1.m1.1a"><mi id="S8.I1.ix3.p1.1.m1.1.1" xref="S8.I1.ix3.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S8.I1.ix3.p1.1.m1.1b"><ci id="S8.I1.ix3.p1.1.m1.1.1.cmml" xref="S8.I1.ix3.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.I1.ix3.p1.1.m1.1c">n</annotation></semantics></math>-gram model
from 13.0% to 16.4%, and matches the performance of a
server-trained RNN which required 1.2e8 SGD steps. In live A/B
experiments, the FL model outperforms both the <math id="S8.I1.ix3.p1.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S8.I1.ix3.p1.2.m2.1a"><mi id="S8.I1.ix3.p1.2.m2.1.1" xref="S8.I1.ix3.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S8.I1.ix3.p1.2.m2.1b"><ci id="S8.I1.ix3.p1.2.m2.1.1.cmml" xref="S8.I1.ix3.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S8.I1.ix3.p1.2.m2.1c">n</annotation></semantics></math>-gram and the
server-trained RNN models.</p>
</div>
</dd>
</dl>
</div>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Operational Profile</h2>

<div id="S9.p1" class="ltx_para ltx_noindent">
<p id="S9.p1.1" class="ltx_p">In this section we provide a brief overview of some key operational
metrics of the deployed FL system, running production workloads for
over a year; Appendix <a href="#A1" title="Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> provides additional details. These
numbers are examples only, since we have not yet applied FL to a
diverse enough set of applications to provide a complete
characterization. Further, all data was collected in the process of
operating a production system, rather than under controlled conditions
explicitly for the purpose of measurement. Many of the performance
metrics here depend on the device and network speed (which can vary by
region); FL plan, global model and update sizes (varies per
application); number of samples per round and computational complexity
per sample.</p>
</div>
<div id="S9.p2" class="ltx_para ltx_noindent">
<p id="S9.p2.1" class="ltx_p">We designed the FL system to elastically scale with the number and
sizes of the FL populations, potentially up into the
billions. Currently the system is handling a cumulative FL population
size of approximately 10M daily active devices, spanning several
different applications.</p>
</div>
<div id="S9.p3" class="ltx_para ltx_noindent">
<p id="S9.p3.1" class="ltx_p">As discussed before, at any point in time only a subset of devices
connect to the server due to device eligibility and pace steering.
Given this, in practice we observe that up to 10k devices are
participating simultaneously. It is worth noting that the number of
participating devices depends on the (local) time of day (see
Fig. <a href="#S9.F5" title="Figure 5 ‣ 9 Operational Profile" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Devices are more likely idle and
charging at night, and hence more likely to participate. We have observed a
<math id="S9.p3.1.m1.1" class="ltx_math_unparsed" alttext="4\times" display="inline"><semantics id="S9.p3.1.m1.1a"><mrow id="S9.p3.1.m1.1b"><mn id="S9.p3.1.m1.1.1">4</mn><mo lspace="0.222em" id="S9.p3.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S9.p3.1.m1.1c">4\times</annotation></semantics></math> difference between low and high numbers of participating
devices over a 24 hours period for a US-centric population.</p>
</div>
<figure id="S9.F5" class="ltx_figure"><img src="/html/1902.01046/assets/round_completion_rate.png" id="S9.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Round Completion Rate</figcaption>
</figure>
<div id="S9.p4" class="ltx_para ltx_noindent">
<p id="S9.p4.1" class="ltx_p">Based on the previous work of <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> and experiments we have conducted
on production FL populations, for most models receiving updates from a
few hundred devices per FL round is sufficient (that is, we see
diminishing improvements in the convergence rate from training on
larger numbers of devices). We also observe that on average the
portion of devices that drop out due to computation errors, network
failures, or changes in eligibility varies between 6% and
10%. Therefore, in order to compensate for device drop out as well as
to allow stragglers to be discarded, the server typically selects
130% of the target number of devices to initially participate. This
parameter can be tuned based on the empirical distribution of device
reporting times and the target number of stragglers to ignore.</p>
</div>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Related Work</h2>

<div id="S10.p1" class="ltx_para ltx_noindent">
<dl id="S10.I1" class="ltx_description">
<dt id="S10.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S10.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Alternative Approaches</span></span></dt>
<dd class="ltx_item">
<div id="S10.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S10.I1.ix1.p1.1" class="ltx_p">To the best of our knowledge, the system we described is the first
production-level Federated Learning implementation, focusing primarily
on the Federated Averaging algorithm running on mobile phones. Nevertheless,
there are other ways to learn from data stored on mobile phones, and other
settings in which FL as a concept could be relevant.</p>
</div>
<div id="S10.I1.ix1.p2" class="ltx_para ltx_noindent">
<p id="S10.I1.ix1.p2.1" class="ltx_p">In particular, <cite class="ltx_cite ltx_citemacro_citet">Pihur et al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> proposes an algorithm
that learns from users’ data without performing aggregation on the
server and with additional formal privacy guarantees. However, their
work focuses on generalized linear models, and argues that their
approach is highly scalable due to avoidance of synchronization and
not requiring to store updates from devices. Our server design
described in Sec. <a href="#S4" title="4 Server" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, rebuts the concerns about scalability
of the synchronous approach we are using, and in particular shows that
updates can be processed online as they are received without a need to
store them. Alternative proposals for FL algorithms include
<cite class="ltx_cite ltx_citemacro_citet">Smith et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Kamp et al. (<a href="#bib.bib14" title="" class="ltx_ref">2018</a>)</cite>, which would be on the
high-level compatible with the system design described here.</p>
</div>
<div id="S10.I1.ix1.p3" class="ltx_para ltx_noindent">
<p id="S10.I1.ix1.p3.1" class="ltx_p">In addition, Federated Learning has already been proposed in the context of
vehicle-to-vehicle communication <cite class="ltx_cite ltx_citemacro_citep">(Samarakoon et al., <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> and medical
applications <cite class="ltx_cite ltx_citemacro_citep">(Brisimi et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. While the system described in this
work as a whole does not directly apply to these scenarios, many aspects of
it would likely be relevant for production application.</p>
</div>
<div id="S10.I1.ix1.p4" class="ltx_para ltx_noindent">
<p id="S10.I1.ix1.p4.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Nishio &amp; Yonetani (<a href="#bib.bib22" title="" class="ltx_ref">2018</a>)</cite> focuses on applying FL in different
environmental conditions, namely where the server can reach any subset of
heterogeneous devices to initiate a round, but receives updates sequentially due
to cellular bandwidth limit. The work offers a resource-aware selection algorithm
maximizing the number of participants in a round, which is implementable within our
system.</p>
</div>
</dd>
<dt id="S10.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S10.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Distributed ML</span></span></dt>
<dd class="ltx_item">
<div id="S10.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S10.I1.ix2.p1.1" class="ltx_p">There has been significant work on distributed machine learning, and large-scale
cloud-based systems have been described and are used in practice. Many
systems support multiple distribution schemes, including <span id="S10.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">model
parallelism</span> and <span id="S10.I1.ix2.p1.1.2" class="ltx_text ltx_font_italic">data parallelism</span>, e.g., <cite class="ltx_cite ltx_citemacro_citet">Dean et al. (<a href="#bib.bib9" title="" class="ltx_ref">2012</a>)</cite>
and <cite class="ltx_cite ltx_citemacro_citet">Low et al. (<a href="#bib.bib18" title="" class="ltx_ref">2012</a>)</cite>. Our system imposes a more structured approach
fitting to the domain of mobile devices, which have much lower
bandwidth and reliability compared to datacenter nodes. We do not
allow for arbitrary distributed computation but rather focus on a
synchronous FL protocol. This domain specialization allows us, from
the system viewpoint, to optimize for the specific use case.</p>
</div>
<div id="S10.I1.ix2.p2" class="ltx_para ltx_noindent">
<p id="S10.I1.ix2.p2.2" class="ltx_p">A particularly common approach in the datacenter is the
<span id="S10.I1.ix2.p2.2.1" class="ltx_text ltx_font_italic">parameter server</span>, e.g., <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib17" title="" class="ltx_ref">2014</a>); Dean et al. (<a href="#bib.bib9" title="" class="ltx_ref">2012</a>); Abadi et al. (<a href="#bib.bib1" title="" class="ltx_ref">2016</a>)</cite>, which allows a large number of workers to collaborate on a
shared global model, the parameter vector. Focus in that line of work
is put on an efficient server architecture for dealing with vectors of
the size of <math id="S10.I1.ix2.p2.1.m1.1" class="ltx_Math" alttext="10^{9}" display="inline"><semantics id="S10.I1.ix2.p2.1.m1.1a"><msup id="S10.I1.ix2.p2.1.m1.1.1" xref="S10.I1.ix2.p2.1.m1.1.1.cmml"><mn id="S10.I1.ix2.p2.1.m1.1.1.2" xref="S10.I1.ix2.p2.1.m1.1.1.2.cmml">10</mn><mn id="S10.I1.ix2.p2.1.m1.1.1.3" xref="S10.I1.ix2.p2.1.m1.1.1.3.cmml">9</mn></msup><annotation-xml encoding="MathML-Content" id="S10.I1.ix2.p2.1.m1.1b"><apply id="S10.I1.ix2.p2.1.m1.1.1.cmml" xref="S10.I1.ix2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S10.I1.ix2.p2.1.m1.1.1.1.cmml" xref="S10.I1.ix2.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S10.I1.ix2.p2.1.m1.1.1.2.cmml" xref="S10.I1.ix2.p2.1.m1.1.1.2">10</cn><cn type="integer" id="S10.I1.ix2.p2.1.m1.1.1.3.cmml" xref="S10.I1.ix2.p2.1.m1.1.1.3">9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.I1.ix2.p2.1.m1.1c">10^{9}</annotation></semantics></math> to <math id="S10.I1.ix2.p2.2.m2.1" class="ltx_Math" alttext="10^{12}" display="inline"><semantics id="S10.I1.ix2.p2.2.m2.1a"><msup id="S10.I1.ix2.p2.2.m2.1.1" xref="S10.I1.ix2.p2.2.m2.1.1.cmml"><mn id="S10.I1.ix2.p2.2.m2.1.1.2" xref="S10.I1.ix2.p2.2.m2.1.1.2.cmml">10</mn><mn id="S10.I1.ix2.p2.2.m2.1.1.3" xref="S10.I1.ix2.p2.2.m2.1.1.3.cmml">12</mn></msup><annotation-xml encoding="MathML-Content" id="S10.I1.ix2.p2.2.m2.1b"><apply id="S10.I1.ix2.p2.2.m2.1.1.cmml" xref="S10.I1.ix2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S10.I1.ix2.p2.2.m2.1.1.1.cmml" xref="S10.I1.ix2.p2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S10.I1.ix2.p2.2.m2.1.1.2.cmml" xref="S10.I1.ix2.p2.2.m2.1.1.2">10</cn><cn type="integer" id="S10.I1.ix2.p2.2.m2.1.1.3.cmml" xref="S10.I1.ix2.p2.2.m2.1.1.3">12</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S10.I1.ix2.p2.2.m2.1c">10^{12}</annotation></semantics></math>. The parameter server provides global
state which workers access and update asynchronously. Our approach
inherently cannot work with such a global state, because we require a
specific rendezvous between a set of devices and the FL server to
perform a synchronous update with Secure Aggregation.</p>
</div>
</dd>
<dt id="S10.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S10.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">MapReduce</span></span></dt>
<dd class="ltx_item">
<div id="S10.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S10.I1.ix3.p1.1" class="ltx_p">For datacenter applications, it is now commonly accepted that
MapReduce <cite class="ltx_cite ltx_citemacro_citep">(Dean &amp; Ghemawat, <a href="#bib.bib8" title="" class="ltx_ref">2008</a>)</cite> is not the right framework for ML
training. For the problem space of FL, MapReduce is a close
relative. One can interpret the FL server as the Reducer, and FL
devices as Mappers. However, there are also fundamental technical
differences compared to a generic MapReduce framework. In our system,
FL devices own the data on which they are working. They are fully
self-controlled actors which attend and leave computation rounds at
will. In turn, the FL server actively scans for available FL devices,
and brings only selected subsets of them together for a round of
computation. The server needs to work with the fact that many devices
drop out during computation, and that availability of FL devices
varies drastically over time. These very specific requirements are
better dealt with by a domain specific framework than a generic
MapReduce.</p>
</div>
</dd>
</dl>
</div>
</section>
<section id="S11" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Future Work</h2>

<div id="S11.p1" class="ltx_para ltx_noindent">
<dl id="S11.I1" class="ltx_description">
<dt id="S11.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S11.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">Bias</span></span></dt>
<dd class="ltx_item">
<div id="S11.I1.ix1.p1" class="ltx_para ltx_noindent">
<p id="S11.I1.ix1.p1.1" class="ltx_p">The Federated Averaging <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> protocol assumes that all devices
are equally likely to participate and complete each round. In
practice, our system potentially introduces bias by the fact that
devices only train when they are on an unmetered network and
charging. In some countries the majority of people rarely have access
to an unmetered network. Also, we limit the deployment of our device
code only to certain phones, currently with recent Android versions
and at least 2 GB of memory, another source of potential bias.</p>
</div>
<div id="S11.I1.ix1.p2" class="ltx_para ltx_noindent">
<p id="S11.I1.ix1.p2.1" class="ltx_p">We address this possibility in the current system as follows: During
FL training, the models are not used to make user-visible predictions;
instead, once a model is trained, it is evaluated in live A/B
experiments using multiple application-specific metrics (just as with
a datacenter model). If bias in device participation or other issues
lead to an inferior model, it will be detected at this point. So far,
we have not observed this to be an issue in practice, but this is
likely application and population dependent.
Further quantification of these possible effects across a wider set of
applications, and if needed algorithmic or systems approaches to
mitigate them, are important directions for future work.</p>
</div>
</dd>
<dt id="S11.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S11.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">Convergence Time</span></span></dt>
<dd class="ltx_item">
<div id="S11.I1.ix2.p1" class="ltx_para ltx_noindent">
<p id="S11.I1.ix2.p1.1" class="ltx_p">We noted in Sec. <a href="#S8" title="8 Applications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> that we currently
observe a slower convergence time for Federated Learning compared to ML
on centralized data where training is backed by the power of a data
center. Current FL algorithms such as Federated Averaging can only efficiently
utilize 100s of devices in parallel, but many more are available; FL would
greatly benefit from new algorithms that can utilize increased parallelism.</p>
</div>
<div id="S11.I1.ix2.p2" class="ltx_para ltx_noindent">
<p id="S11.I1.ix2.p2.1" class="ltx_p">On the operational side, there is also more which can be done. For
example, the time windows to select devices for training and wait for
their reporting is currently configured statically per FL population. It
should be dynamically adjusted to reduce the drop out rate and increase
round frequency. We should ideally use online ML for tuning this and
other parameters of the protocol configuration, bringing in e.g. time
of the day as context.</p>
</div>
</dd>
<dt id="S11.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S11.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">Device Scheduling</span></span></dt>
<dd class="ltx_item">
<div id="S11.I1.ix3.p1" class="ltx_para ltx_noindent">
<p id="S11.I1.ix3.p1.1" class="ltx_p">Currently, our multi-tenant on-device scheduler uses a simple worker
queue for determining which training session to run next (we avoid
running training sessions on-device in parallel because of their high
resource consumption). This approach is blind to aspects like which
apps the user has been frequently using. It’s possible for us to end up
repeatedly training on older data (up to the expiration date) for some apps,
while also neglecting training on newer data for the apps the user is frequently
using. Any optimization here, though, has to be carefully evaluated
against the biases it may introduce.</p>
</div>
</dd>
<dt id="S11.I1.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S11.I1.ix4.1.1.1" class="ltx_text ltx_font_bold">Bandwidth</span></span></dt>
<dd class="ltx_item">
<div id="S11.I1.ix4.p1" class="ltx_para ltx_noindent">
<p id="S11.I1.ix4.p1.1" class="ltx_p">When working with certain types of models, for example recurrent
networks for language modeling, even small amounts of raw data can
result in large amounts of information (weight updates) being
communicated. In particular, this might be more than if we would just
upload the raw data. While this could be viewed as a tradeoff for
better privacy, there is also much which can be improved. To reduce
the bandwidth necessary, we implement compression techniques such as those of
<cite class="ltx_cite ltx_citemacro_citet">Konečný et al. (<a href="#bib.bib16" title="" class="ltx_ref">2016b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Caldas et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>.
In addition to that, we can modify the training
algorithms to obtain models in quantized representation
<cite class="ltx_cite ltx_citemacro_citep">(Jacob et al., <a href="#bib.bib13" title="" class="ltx_ref">2017</a>)</cite>, which will have synergetic effect with bandwidth
savings and be important for efficient deployment for inference.</p>
</div>
</dd>
<dt id="S11.I1.ix5" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S11.I1.ix5.1.1.1" class="ltx_text ltx_font_bold">Federated Computation</span></span></dt>
<dd class="ltx_item">
<div id="S11.I1.ix5.p1" class="ltx_para ltx_noindent">
<p id="S11.I1.ix5.p1.1" class="ltx_p">We believe there are more applications besides ML for the general
device/server architecture we have described in this paper. This is also
apparent from the fact that this paper contains no explicit
mentioning of any ML logic. Instead, we refer abstractly to ’plans’, ’models’,
’updates’ and so on.</p>
</div>
<div id="S11.I1.ix5.p2" class="ltx_para ltx_noindent">
<p id="S11.I1.ix5.p2.1" class="ltx_p">We aim to generalize our system from Federated Learning to
<span id="S11.I1.ix5.p2.1.1" class="ltx_text ltx_font_italic">Federated Computation</span>, which follows the same basic
principles as described in this paper, but does not restrict
computation to ML with TensorFlow, but general MapReduce like
workloads. One application area we are seeing is in <span id="S11.I1.ix5.p2.1.2" class="ltx_text ltx_font_italic">Federated
Analytics</span>, which would allow us to monitor aggregate device
statistics without logging raw device data to the cloud.</p>
</div>
</dd>
</dl>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para ltx_noindent">
<p id="Sx1.p1.1" class="ltx_p">Like most production scale systems, there are many more contributors
than the authors of this paper. The following people, at least, have
directly contributed to design and implementation: 
Galen Andrew,
Blaise Agüera y Arcas,
Sean Augenstein,
Dave Bacon,
Françoise Beaufays,
Amlan Chakraborty,
Arlie Davis,
Stefan Dierauf,
Randy Dodgen,
Emily Glanz,
Shiyu Hu,
Ben Kreuter,
Eric Loewenthal,
Antonio Marcedone,
Jason Hunter,
Krzysztof Ostrowski,
Sarvar Patel,
Peter Kairouz,
Kanishka Rao,
Michael Reneer,
Aaron Segal,
Karn Seth,
Wei Huang,
Nicholas Kong,
Haicheng Sun,
Vivian Lee,
Tim Yang,
Yuanbo Zhang.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abadi et al. (2016)</span>
<span class="ltx_bibblock">
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C.,
Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
Vanhoucke, V., Vasudevan, V., Viégas, F., Vinyals, O., Warden, P.,
Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.

</span>
<span class="ltx_bibblock">TensorFlow: Large-scale machine learning on heterogeneous systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">OSDI</em>, volume 16, pp.  265–283, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ai.google (2018)</span>
<span class="ltx_bibblock">
ai.google.

</span>
<span class="ltx_bibblock">Under the hood of the pixel 2: How ai is supercharging hardware,
2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ai.google/stories/ai-in-hardware/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.google/stories/ai-in-hardware/</a>.

</span>
<span class="ltx_bibblock">Retrieved Nov 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(3)</span>
<span class="ltx_bibblock">
Android Documentation.

</span>
<span class="ltx_bibblock">SafetyNet Attestation API.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://developer.android.com/training/safetynet/attestation" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developer.android.com/training/safetynet/attestation</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagdasaryan et al. (2018)</span>
<span class="ltx_bibblock">
Bagdasaryan, E., Veit, A., Hua, Y., Estrin, D., and Shmatikov, V.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.00459</em>, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2017)</span>
<span class="ltx_bibblock">
Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H. B., Patel,
S., Ramage, D., Segal, A., and Seth, K.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em>, pp.  1175–1191. ACM, 2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brisimi et al. (2018)</span>
<span class="ltx_bibblock">
Brisimi, T. S., Chen, R., Mela, T., Olshevsky, A., Paschalidis, I. C., and Shi,
W.

</span>
<span class="ltx_bibblock">Federated learning of predictive models from federated electronic
health records.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International journal of medical informatics</em>, 112:59–67, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. (2018)</span>
<span class="ltx_bibblock">
Caldas, S., Konecný, J., McMahan, H. B., and Talwalkar, A.

</span>
<span class="ltx_bibblock">Expanding the reach of federated learning by reducing client resource
requirements.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint 1812.07210</em>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dean &amp; Ghemawat (2008)</span>
<span class="ltx_bibblock">
Dean, J. and Ghemawat, S.

</span>
<span class="ltx_bibblock">MapReduce: Simplified data processing on large clusters.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, 51(1):107–113,
2008.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dean et al. (2012)</span>
<span class="ltx_bibblock">
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q. V., Mao, M.,
Ranzato, M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y.

</span>
<span class="ltx_bibblock">Large scale distributed deep networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pp. 1223–1231, 2012.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
A., Tulloch, A., Jia, Y., and He, K.

</span>
<span class="ltx_bibblock">Accurate, large minibatch SGD: Training imagenet in 1 hour.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.02677</em>, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Hard, A., Rao, K., Mathews, R., Beaufays, F., Augenstein, S., Eichner, H.,
Kiddon, C., and Ramage, D.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint 1811.03604</em>, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt et al. (1973)</span>
<span class="ltx_bibblock">
Hewitt, C., Bishop, P. B., and Steiger, R.

</span>
<span class="ltx_bibblock">A universal modular ACTOR formalism for artificial intelligence.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd International Joint Conference on
Artificial Intelligence. Stanford, CA, USA, August 20-23, 1973</em>, pp. 235–245, 1973.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacob et al. (2017)</span>
<span class="ltx_bibblock">
Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., and
Kalenichenko, D.

</span>
<span class="ltx_bibblock">Quantization and training of neural networks for efficient
integer-arithmetic-only inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1712.05877</em>, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamp et al. (2018)</span>
<span class="ltx_bibblock">
Kamp, M., Adilova, L., Sicking, J., Hüger, F., Schlicht, P., Wirtz, T., and
Wrobel, S.

</span>
<span class="ltx_bibblock">Efficient decentralized deep learning by dynamic model averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.03210</em>, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al. (2016a)</span>
<span class="ltx_bibblock">
Konečný, J., McMahan, H. B., Ramage, D., and Richtárik, P.

</span>
<span class="ltx_bibblock">Federated optimization: Distributed machine learning for on-device
intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016a.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečný et al. (2016b)</span>
<span class="ltx_bibblock">
Konečný, J., McMahan, H. B., Yu, F. X., Richtárik, P., Suresh,
A. T., and Bacon, D.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016b.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2014)</span>
<span class="ltx_bibblock">
Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V.,
Long, J., Shekita, E. J., and Su, B.-Y.

</span>
<span class="ltx_bibblock">Scaling distributed machine learning with the parameter server.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">11th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 14)</em>, pp.  583–598. USENIX Association, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Low et al. (2012)</span>
<span class="ltx_bibblock">
Low, Y., Bickson, D., Gonzalez, J., Guestrin, C., Kyrola, A., and Hellerstein,
J. M.

</span>
<span class="ltx_bibblock">Distributed graphlab: A framework for machine learning and data
mining in the cloud.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, 5(8):716–727, April
2012.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan &amp; Ramage (2017)</span>
<span class="ltx_bibblock">
McMahan, H. B. and Ramage, D.

</span>
<span class="ltx_bibblock">Federated learning: Collaborative machine learning without
centralized training data, April 2017.

</span>
<span class="ltx_bibblock">URL
<a target="_blank" href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.googleblog.com/2017/04/federated-learning-collaborative.html</a>.

</span>
<span class="ltx_bibblock">Google AI Blog.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017)</span>
<span class="ltx_bibblock">
McMahan, H. B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em>, pp.  1273–1282, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2018)</span>
<span class="ltx_bibblock">
McMahan, H. B., Ramage, D., Talwar, K., and Zhang, L.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>, 2018.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nishio &amp; Yonetani (2018)</span>
<span class="ltx_bibblock">
Nishio, T. and Yonetani, R.

</span>
<span class="ltx_bibblock">Client selection for federated learning with heterogeneous resources
in mobile edge.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.08333</em>, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pihur et al. (2018)</span>
<span class="ltx_bibblock">
Pihur, V., Korolova, A., Liu, F., Sankuratripati, S., Yung, M., Huang, D., and
Zeng, R.

</span>
<span class="ltx_bibblock">Differentially-private “draw and discard” machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.04369</em>, 2018.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Samarakoon et al. (2018)</span>
<span class="ltx_bibblock">
Samarakoon, S., Bennis, M., Saad, W., and Debbah, M.

</span>
<span class="ltx_bibblock">Federated learning for ultra-reliable low-latency v2v communications.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1805.09253</em>, 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. (2018)</span>
<span class="ltx_bibblock">
Smith, S., jan Kindermans, P., Ying, C., and Le, Q. V.

</span>
<span class="ltx_bibblock">Don’t decay the learning rate, increase the batch size.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. (2017)</span>
<span class="ltx_bibblock">
Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A. S.

</span>
<span class="ltx_bibblock">Federated multi-task learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pp. 4424–4434, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., and
Beaufays, F.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint 1812.02903</em>, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Operational Profile Data</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">In this section we present operational profile data for one of the FL
populations that are currently active in the deployed FL system,
augmenting the discussion in Sec. <a href="#S9" title="9 Operational Profile" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. The subject FL
population primarily comes from the same time zone.</p>
</div>
<div id="A1.p2" class="ltx_para ltx_noindent">
<p id="A1.p2.1" class="ltx_p">Fig. <a href="#A1.F6" title="Figure 6 ‣ Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates how
availability of the devices varies through the day and its impact on
the round completion rate. Because the FL server schedules an FL task
for execution only once a desired number of devices are available and
selected, the round completion rate oscillates in sync with device
availability.</p>
</div>
<figure id="A1.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1902.01046/assets/connected_devices.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="215" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1902.01046/assets/round_completion_rate.png" id="A1.F6.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="598" height="215" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A subset of the connected devices over three days (top) in states
“participating” (blue) and “waiting” (purple). Other states
(“closing” and “attesting”) are too rare to be visible in this graph.
The rate of successful round completions (green, bottom) is also shown,
along with the rate of other outcomes (“failure”, “retry”, and
“abort”) plotted on the same graph but too low to be visible.</figcaption>
</figure>
<div id="A1.p3" class="ltx_para ltx_noindent">
<p id="A1.p3.1" class="ltx_p">Fig. <a href="#A1.F7" title="Figure 7 ‣ Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates the average number of devices
participating in an FL task round and the outcomes of the participation. Note
that in each round the FL server selects more devices for the participation than
desired to complete to offset the devices that drop out during execution.
Therefore in each round there are devices that were aborted after a desired
number of devices successfully complete. Another noteworthy aspect is drop out
rate correlation with the time of day, specifically the drop out rate is higher
during the day time compared to the night time. This is explained by higher
probability of the device eligibility criteria changes due interaction with
a device.</p>
</div>
<figure id="A1.F7" class="ltx_figure"><img src="/html/1902.01046/assets/devices_per_round.png" id="A1.F7.g1" class="ltx_graphics ltx_img_landscape" width="598" height="215" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Average number of devices completed, aborted and dropped out from
round execution</figcaption>
</figure>
<div id="A1.p4" class="ltx_para ltx_noindent">
<p id="A1.p4.1" class="ltx_p">Fig. <a href="#A1.F8" title="Figure 8 ‣ Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows distribution
of round run and device participation time. There are two noteworthy
observations. First is that the round run time is roughly equal to the majority
of the device participation time which is explained by the fact that the
FL server selects more than needed devices for participation and stops execution
when enough devices complete. Second is that device participation time is
capped. This is a mechanism used by the FL server to deal with straggler
devices; i.e., the round run time capped by the server.</p>
</div>
<figure id="A1.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1902.01046/assets/round_execution_time.png" id="A1.F8.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="215" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/1902.01046/assets/device_participation_time_per_round.png" id="A1.F8.g2" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="215" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Round execution and device participation time</figcaption>
</figure>
<div id="A1.p5" class="ltx_para ltx_noindent">
<p id="A1.p5.1" class="ltx_p">Fig. <a href="#A1.F9" title="Figure 9 ‣ Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> illustrates the asymmetry in server network
traffic, specifically that download from server dominates upload. There are
several aspects that contribute. Namely each device downloads both an FL task
plan and current global model (plan size is comparable with the global model)
whereas it uploads only updates to the global model; the model updates
are inherently more compressible compared to the global model.</p>
</div>
<figure id="A1.F9" class="ltx_figure"><img src="/html/1902.01046/assets/server_network_traffic.png" id="A1.F9.g1" class="ltx_graphics ltx_img_landscape" width="598" height="215" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Server network traffic</figcaption>
</figure>
<div id="A1.p6" class="ltx_para ltx_noindent">
<p id="A1.p6.1" class="ltx_p">Tab. <a href="#A1.T1" title="Table 1 ‣ Appendix A Operational Profile Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the training round
session shape visualizations generated from the clients’ training
state event logs. As shown, 75% of clients complete their training
rounds successfully, 22% of clients complete their training rounds
but have their results rejected by the server (these are the devices
which report back after the reporting window already closed), and 2%
of clients are interrupted before being able to complete their round
(e.g., because the device exited the idle state).</p>
</div>
<figure id="A1.T1" class="ltx_table">
<table id="A1.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T1.1.1.1" class="ltx_tr">
<th id="A1.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Session Shape</th>
<th id="A1.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Count</th>
<th id="A1.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">Percent</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T1.1.2.1" class="ltx_tr">
<th id="A1.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">-v[]+^</th>
<td id="A1.T1.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">1,116,401</td>
<td id="A1.T1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">75%</td>
</tr>
<tr id="A1.T1.1.3.2" class="ltx_tr">
<th id="A1.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">-v[]+#</th>
<td id="A1.T1.1.3.2.2" class="ltx_td ltx_align_right">327,478</td>
<td id="A1.T1.1.3.2.3" class="ltx_td ltx_align_right">22%</td>
</tr>
<tr id="A1.T1.1.4.3" class="ltx_tr">
<th id="A1.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">-v[!</th>
<td id="A1.T1.1.4.3.2" class="ltx_td ltx_align_right ltx_border_b">29,771</td>
<td id="A1.T1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_b">2%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Distribution of on-device training round sessions. Legend: - = FL
server checkin, v = downloaded plan, [ = training started, ] = training
completed, + = upload started, ^ = upload completed,
# = upload rejected, ! = interrupted.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Federated Averaging</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">In this section, we show the Federated Averaging algorithm from <cite class="ltx_cite ltx_citemacro_citet">McMahan et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>
for the interested reader.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<div id="alg1.4" class="ltx_listing ltx_listing">
<div id="alg0.l0" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">0:</span>   <span id="alg0.l0.1" class="ltx_text ltx_font_bold">Server executes:</span>

</div>
<div id="alg0.l1" class="ltx_listingline">  initialize <math id="alg0.l1.m1.1" class="ltx_Math" alttext="w_{0}" display="inline"><semantics id="alg0.l1.m1.1a"><msub id="alg0.l1.m1.1.1" xref="alg0.l1.m1.1.1.cmml"><mi id="alg0.l1.m1.1.1.2" xref="alg0.l1.m1.1.1.2.cmml">w</mi><mn id="alg0.l1.m1.1.1.3" xref="alg0.l1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg0.l1.m1.1b"><apply id="alg0.l1.m1.1.1.cmml" xref="alg0.l1.m1.1.1"><csymbol cd="ambiguous" id="alg0.l1.m1.1.1.1.cmml" xref="alg0.l1.m1.1.1">subscript</csymbol><ci id="alg0.l1.m1.1.1.2.cmml" xref="alg0.l1.m1.1.1.2">𝑤</ci><cn type="integer" id="alg0.l1.m1.1.1.3.cmml" xref="alg0.l1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l1.m1.1c">w_{0}</annotation></semantics></math>

</div>
<div id="alg0.l2" class="ltx_listingline">  <span id="alg0.l2.1" class="ltx_text ltx_font_bold">for</span> each round <math id="alg0.l2.m1.3" class="ltx_Math" alttext="t=1,2,\dots" display="inline"><semantics id="alg0.l2.m1.3a"><mrow id="alg0.l2.m1.3.4" xref="alg0.l2.m1.3.4.cmml"><mi id="alg0.l2.m1.3.4.2" xref="alg0.l2.m1.3.4.2.cmml">t</mi><mo id="alg0.l2.m1.3.4.1" xref="alg0.l2.m1.3.4.1.cmml">=</mo><mrow id="alg0.l2.m1.3.4.3.2" xref="alg0.l2.m1.3.4.3.1.cmml"><mn id="alg0.l2.m1.1.1" xref="alg0.l2.m1.1.1.cmml">1</mn><mo id="alg0.l2.m1.3.4.3.2.1" xref="alg0.l2.m1.3.4.3.1.cmml">,</mo><mn id="alg0.l2.m1.2.2" xref="alg0.l2.m1.2.2.cmml">2</mn><mo id="alg0.l2.m1.3.4.3.2.2" xref="alg0.l2.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="alg0.l2.m1.3.3" xref="alg0.l2.m1.3.3.cmml">…</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l2.m1.3b"><apply id="alg0.l2.m1.3.4.cmml" xref="alg0.l2.m1.3.4"><eq id="alg0.l2.m1.3.4.1.cmml" xref="alg0.l2.m1.3.4.1"></eq><ci id="alg0.l2.m1.3.4.2.cmml" xref="alg0.l2.m1.3.4.2">𝑡</ci><list id="alg0.l2.m1.3.4.3.1.cmml" xref="alg0.l2.m1.3.4.3.2"><cn type="integer" id="alg0.l2.m1.1.1.cmml" xref="alg0.l2.m1.1.1">1</cn><cn type="integer" id="alg0.l2.m1.2.2.cmml" xref="alg0.l2.m1.2.2">2</cn><ci id="alg0.l2.m1.3.3.cmml" xref="alg0.l2.m1.3.3">…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l2.m1.3c">t=1,2,\dots</annotation></semantics></math> <span id="alg0.l2.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg0.l3" class="ltx_listingline">     Select <math id="alg0.l3.m1.1" class="ltx_Math" alttext="1.3K" display="inline"><semantics id="alg0.l3.m1.1a"><mrow id="alg0.l3.m1.1.1" xref="alg0.l3.m1.1.1.cmml"><mn id="alg0.l3.m1.1.1.2" xref="alg0.l3.m1.1.1.2.cmml">1.3</mn><mo lspace="0em" rspace="0em" id="alg0.l3.m1.1.1.1" xref="alg0.l3.m1.1.1.1.cmml">​</mo><mi id="alg0.l3.m1.1.1.3" xref="alg0.l3.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="alg0.l3.m1.1b"><apply id="alg0.l3.m1.1.1.cmml" xref="alg0.l3.m1.1.1"><times id="alg0.l3.m1.1.1.1.cmml" xref="alg0.l3.m1.1.1.1"></times><cn type="float" id="alg0.l3.m1.1.1.2.cmml" xref="alg0.l3.m1.1.1.2">1.3</cn><ci id="alg0.l3.m1.1.1.3.cmml" xref="alg0.l3.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l3.m1.1c">1.3K</annotation></semantics></math> eligible clients to compute updates

</div>
<div id="alg0.l4" class="ltx_listingline">     Wait for updates from <math id="alg0.l4.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="alg0.l4.m1.1a"><mi id="alg0.l4.m1.1.1" xref="alg0.l4.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg0.l4.m1.1b"><ci id="alg0.l4.m1.1.1.cmml" xref="alg0.l4.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l4.m1.1c">K</annotation></semantics></math> clients (indexed <math id="alg0.l4.m2.3" class="ltx_math_unparsed" alttext="1,\dots,K)" display="inline"><semantics id="alg0.l4.m2.3a"><mrow id="alg0.l4.m2.3b"><mn id="alg0.l4.m2.1.1">1</mn><mo id="alg0.l4.m2.3.4">,</mo><mi mathvariant="normal" id="alg0.l4.m2.2.2">…</mi><mo id="alg0.l4.m2.3.5">,</mo><mi id="alg0.l4.m2.3.3">K</mi><mo stretchy="false" id="alg0.l4.m2.3.6">)</mo></mrow><annotation encoding="application/x-tex" id="alg0.l4.m2.3c">1,\dots,K)</annotation></semantics></math>

</div>
<div id="alg0.l5" class="ltx_listingline">     <math id="alg0.l5.m1.3" class="ltx_Math" alttext="(\rule{0.0pt}{9.47217pt}\Delta^{k},n^{k})=\text{ClientUpdate}(w)" display="inline"><semantics id="alg0.l5.m1.3a"><mrow id="alg0.l5.m1.3.3" xref="alg0.l5.m1.3.3.cmml"><mrow id="alg0.l5.m1.3.3.2.2" xref="alg0.l5.m1.3.3.2.3.cmml"><mo stretchy="false" id="alg0.l5.m1.3.3.2.2.3" xref="alg0.l5.m1.3.3.2.3.cmml">(</mo><mrow id="alg0.l5.m1.2.2.1.1.1" xref="alg0.l5.m1.2.2.1.1.1.cmml"><mtext id="alg0.l5.m1.2.2.1.1.1.2" xref="alg0.l5.m1.2.2.1.1.1.2b.cmml"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext><mo lspace="0em" rspace="0em" id="alg0.l5.m1.2.2.1.1.1.1" xref="alg0.l5.m1.2.2.1.1.1.1.cmml">​</mo><msup id="alg0.l5.m1.2.2.1.1.1.3" xref="alg0.l5.m1.2.2.1.1.1.3.cmml"><mi mathvariant="normal" id="alg0.l5.m1.2.2.1.1.1.3.2" xref="alg0.l5.m1.2.2.1.1.1.3.2.cmml">Δ</mi><mi id="alg0.l5.m1.2.2.1.1.1.3.3" xref="alg0.l5.m1.2.2.1.1.1.3.3.cmml">k</mi></msup></mrow><mo id="alg0.l5.m1.3.3.2.2.4" xref="alg0.l5.m1.3.3.2.3.cmml">,</mo><msup id="alg0.l5.m1.3.3.2.2.2" xref="alg0.l5.m1.3.3.2.2.2.cmml"><mi id="alg0.l5.m1.3.3.2.2.2.2" xref="alg0.l5.m1.3.3.2.2.2.2.cmml">n</mi><mi id="alg0.l5.m1.3.3.2.2.2.3" xref="alg0.l5.m1.3.3.2.2.2.3.cmml">k</mi></msup><mo stretchy="false" id="alg0.l5.m1.3.3.2.2.5" xref="alg0.l5.m1.3.3.2.3.cmml">)</mo></mrow><mo id="alg0.l5.m1.3.3.3" xref="alg0.l5.m1.3.3.3.cmml">=</mo><mrow id="alg0.l5.m1.3.3.4" xref="alg0.l5.m1.3.3.4.cmml"><mtext id="alg0.l5.m1.3.3.4.2" xref="alg0.l5.m1.3.3.4.2a.cmml">ClientUpdate</mtext><mo lspace="0em" rspace="0em" id="alg0.l5.m1.3.3.4.1" xref="alg0.l5.m1.3.3.4.1.cmml">​</mo><mrow id="alg0.l5.m1.3.3.4.3.2" xref="alg0.l5.m1.3.3.4.cmml"><mo stretchy="false" id="alg0.l5.m1.3.3.4.3.2.1" xref="alg0.l5.m1.3.3.4.cmml">(</mo><mi id="alg0.l5.m1.1.1" xref="alg0.l5.m1.1.1.cmml">w</mi><mo stretchy="false" id="alg0.l5.m1.3.3.4.3.2.2" xref="alg0.l5.m1.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l5.m1.3b"><apply id="alg0.l5.m1.3.3.cmml" xref="alg0.l5.m1.3.3"><eq id="alg0.l5.m1.3.3.3.cmml" xref="alg0.l5.m1.3.3.3"></eq><interval closure="open" id="alg0.l5.m1.3.3.2.3.cmml" xref="alg0.l5.m1.3.3.2.2"><apply id="alg0.l5.m1.2.2.1.1.1.cmml" xref="alg0.l5.m1.2.2.1.1.1"><times id="alg0.l5.m1.2.2.1.1.1.1.cmml" xref="alg0.l5.m1.2.2.1.1.1.1"></times><ci id="alg0.l5.m1.2.2.1.1.1.2b.cmml" xref="alg0.l5.m1.2.2.1.1.1.2"><mtext id="alg0.l5.m1.2.2.1.1.1.2.cmml" xref="alg0.l5.m1.2.2.1.1.1.2"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext></ci><apply id="alg0.l5.m1.2.2.1.1.1.3.cmml" xref="alg0.l5.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="alg0.l5.m1.2.2.1.1.1.3.1.cmml" xref="alg0.l5.m1.2.2.1.1.1.3">superscript</csymbol><ci id="alg0.l5.m1.2.2.1.1.1.3.2.cmml" xref="alg0.l5.m1.2.2.1.1.1.3.2">Δ</ci><ci id="alg0.l5.m1.2.2.1.1.1.3.3.cmml" xref="alg0.l5.m1.2.2.1.1.1.3.3">𝑘</ci></apply></apply><apply id="alg0.l5.m1.3.3.2.2.2.cmml" xref="alg0.l5.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="alg0.l5.m1.3.3.2.2.2.1.cmml" xref="alg0.l5.m1.3.3.2.2.2">superscript</csymbol><ci id="alg0.l5.m1.3.3.2.2.2.2.cmml" xref="alg0.l5.m1.3.3.2.2.2.2">𝑛</ci><ci id="alg0.l5.m1.3.3.2.2.2.3.cmml" xref="alg0.l5.m1.3.3.2.2.2.3">𝑘</ci></apply></interval><apply id="alg0.l5.m1.3.3.4.cmml" xref="alg0.l5.m1.3.3.4"><times id="alg0.l5.m1.3.3.4.1.cmml" xref="alg0.l5.m1.3.3.4.1"></times><ci id="alg0.l5.m1.3.3.4.2a.cmml" xref="alg0.l5.m1.3.3.4.2"><mtext id="alg0.l5.m1.3.3.4.2.cmml" xref="alg0.l5.m1.3.3.4.2">ClientUpdate</mtext></ci><ci id="alg0.l5.m1.1.1.cmml" xref="alg0.l5.m1.1.1">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l5.m1.3c">(\rule{0.0pt}{9.47217pt}\Delta^{k},n^{k})=\text{ClientUpdate}(w)</annotation></semantics></math> from client <math id="alg0.l5.m2.1" class="ltx_Math" alttext="k\in[K]" display="inline"><semantics id="alg0.l5.m2.1a"><mrow id="alg0.l5.m2.1.2" xref="alg0.l5.m2.1.2.cmml"><mi id="alg0.l5.m2.1.2.2" xref="alg0.l5.m2.1.2.2.cmml">k</mi><mo id="alg0.l5.m2.1.2.1" xref="alg0.l5.m2.1.2.1.cmml">∈</mo><mrow id="alg0.l5.m2.1.2.3.2" xref="alg0.l5.m2.1.2.3.1.cmml"><mo stretchy="false" id="alg0.l5.m2.1.2.3.2.1" xref="alg0.l5.m2.1.2.3.1.1.cmml">[</mo><mi id="alg0.l5.m2.1.1" xref="alg0.l5.m2.1.1.cmml">K</mi><mo stretchy="false" id="alg0.l5.m2.1.2.3.2.2" xref="alg0.l5.m2.1.2.3.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l5.m2.1b"><apply id="alg0.l5.m2.1.2.cmml" xref="alg0.l5.m2.1.2"><in id="alg0.l5.m2.1.2.1.cmml" xref="alg0.l5.m2.1.2.1"></in><ci id="alg0.l5.m2.1.2.2.cmml" xref="alg0.l5.m2.1.2.2">𝑘</ci><apply id="alg0.l5.m2.1.2.3.1.cmml" xref="alg0.l5.m2.1.2.3.2"><csymbol cd="latexml" id="alg0.l5.m2.1.2.3.1.1.cmml" xref="alg0.l5.m2.1.2.3.2.1">delimited-[]</csymbol><ci id="alg0.l5.m2.1.1.cmml" xref="alg0.l5.m2.1.1">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l5.m2.1c">k\in[K]</annotation></semantics></math>.

</div>
<div id="alg0.l6" class="ltx_listingline">     <math id="alg0.l6.m1.1" class="ltx_Math" alttext="\rule{0.0pt}{9.47217pt}\bar{w}_{t}=\sum_{k}\Delta^{k}" display="inline"><semantics id="alg0.l6.m1.1a"><mrow id="alg0.l6.m1.1.1" xref="alg0.l6.m1.1.1.cmml"><mrow id="alg0.l6.m1.1.1.2" xref="alg0.l6.m1.1.1.2.cmml"><mtext id="alg0.l6.m1.1.1.2.2" xref="alg0.l6.m1.1.1.2.2b.cmml"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext><mo lspace="0em" rspace="0em" id="alg0.l6.m1.1.1.2.1" xref="alg0.l6.m1.1.1.2.1.cmml">​</mo><msub id="alg0.l6.m1.1.1.2.3" xref="alg0.l6.m1.1.1.2.3.cmml"><mover accent="true" id="alg0.l6.m1.1.1.2.3.2" xref="alg0.l6.m1.1.1.2.3.2.cmml"><mi id="alg0.l6.m1.1.1.2.3.2.2" xref="alg0.l6.m1.1.1.2.3.2.2.cmml">w</mi><mo id="alg0.l6.m1.1.1.2.3.2.1" xref="alg0.l6.m1.1.1.2.3.2.1.cmml">¯</mo></mover><mi id="alg0.l6.m1.1.1.2.3.3" xref="alg0.l6.m1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo rspace="0.111em" id="alg0.l6.m1.1.1.1" xref="alg0.l6.m1.1.1.1.cmml">=</mo><mrow id="alg0.l6.m1.1.1.3" xref="alg0.l6.m1.1.1.3.cmml"><msub id="alg0.l6.m1.1.1.3.1" xref="alg0.l6.m1.1.1.3.1.cmml"><mo id="alg0.l6.m1.1.1.3.1.2" xref="alg0.l6.m1.1.1.3.1.2.cmml">∑</mo><mi id="alg0.l6.m1.1.1.3.1.3" xref="alg0.l6.m1.1.1.3.1.3.cmml">k</mi></msub><msup id="alg0.l6.m1.1.1.3.2" xref="alg0.l6.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="alg0.l6.m1.1.1.3.2.2" xref="alg0.l6.m1.1.1.3.2.2.cmml">Δ</mi><mi id="alg0.l6.m1.1.1.3.2.3" xref="alg0.l6.m1.1.1.3.2.3.cmml">k</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l6.m1.1b"><apply id="alg0.l6.m1.1.1.cmml" xref="alg0.l6.m1.1.1"><eq id="alg0.l6.m1.1.1.1.cmml" xref="alg0.l6.m1.1.1.1"></eq><apply id="alg0.l6.m1.1.1.2.cmml" xref="alg0.l6.m1.1.1.2"><times id="alg0.l6.m1.1.1.2.1.cmml" xref="alg0.l6.m1.1.1.2.1"></times><ci id="alg0.l6.m1.1.1.2.2b.cmml" xref="alg0.l6.m1.1.1.2.2"><mtext id="alg0.l6.m1.1.1.2.2.cmml" xref="alg0.l6.m1.1.1.2.2"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext></ci><apply id="alg0.l6.m1.1.1.2.3.cmml" xref="alg0.l6.m1.1.1.2.3"><csymbol cd="ambiguous" id="alg0.l6.m1.1.1.2.3.1.cmml" xref="alg0.l6.m1.1.1.2.3">subscript</csymbol><apply id="alg0.l6.m1.1.1.2.3.2.cmml" xref="alg0.l6.m1.1.1.2.3.2"><ci id="alg0.l6.m1.1.1.2.3.2.1.cmml" xref="alg0.l6.m1.1.1.2.3.2.1">¯</ci><ci id="alg0.l6.m1.1.1.2.3.2.2.cmml" xref="alg0.l6.m1.1.1.2.3.2.2">𝑤</ci></apply><ci id="alg0.l6.m1.1.1.2.3.3.cmml" xref="alg0.l6.m1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="alg0.l6.m1.1.1.3.cmml" xref="alg0.l6.m1.1.1.3"><apply id="alg0.l6.m1.1.1.3.1.cmml" xref="alg0.l6.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg0.l6.m1.1.1.3.1.1.cmml" xref="alg0.l6.m1.1.1.3.1">subscript</csymbol><sum id="alg0.l6.m1.1.1.3.1.2.cmml" xref="alg0.l6.m1.1.1.3.1.2"></sum><ci id="alg0.l6.m1.1.1.3.1.3.cmml" xref="alg0.l6.m1.1.1.3.1.3">𝑘</ci></apply><apply id="alg0.l6.m1.1.1.3.2.cmml" xref="alg0.l6.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg0.l6.m1.1.1.3.2.1.cmml" xref="alg0.l6.m1.1.1.3.2">superscript</csymbol><ci id="alg0.l6.m1.1.1.3.2.2.cmml" xref="alg0.l6.m1.1.1.3.2.2">Δ</ci><ci id="alg0.l6.m1.1.1.3.2.3.cmml" xref="alg0.l6.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l6.m1.1c">\rule{0.0pt}{9.47217pt}\bar{w}_{t}=\sum_{k}\Delta^{k}</annotation></semantics></math>      // <em id="alg0.l6.1" class="ltx_emph ltx_font_italic">Sum of weighted updates</em>

</div>
<div id="alg0.l7" class="ltx_listingline">     <math id="alg0.l7.m1.1" class="ltx_Math" alttext="\rule{0.0pt}{9.47217pt}\bar{n}_{t}=\sum_{k}n^{k}" display="inline"><semantics id="alg0.l7.m1.1a"><mrow id="alg0.l7.m1.1.1" xref="alg0.l7.m1.1.1.cmml"><mrow id="alg0.l7.m1.1.1.2" xref="alg0.l7.m1.1.1.2.cmml"><mtext id="alg0.l7.m1.1.1.2.2" xref="alg0.l7.m1.1.1.2.2b.cmml"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext><mo lspace="0em" rspace="0em" id="alg0.l7.m1.1.1.2.1" xref="alg0.l7.m1.1.1.2.1.cmml">​</mo><msub id="alg0.l7.m1.1.1.2.3" xref="alg0.l7.m1.1.1.2.3.cmml"><mover accent="true" id="alg0.l7.m1.1.1.2.3.2" xref="alg0.l7.m1.1.1.2.3.2.cmml"><mi id="alg0.l7.m1.1.1.2.3.2.2" xref="alg0.l7.m1.1.1.2.3.2.2.cmml">n</mi><mo id="alg0.l7.m1.1.1.2.3.2.1" xref="alg0.l7.m1.1.1.2.3.2.1.cmml">¯</mo></mover><mi id="alg0.l7.m1.1.1.2.3.3" xref="alg0.l7.m1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo rspace="0.111em" id="alg0.l7.m1.1.1.1" xref="alg0.l7.m1.1.1.1.cmml">=</mo><mrow id="alg0.l7.m1.1.1.3" xref="alg0.l7.m1.1.1.3.cmml"><msub id="alg0.l7.m1.1.1.3.1" xref="alg0.l7.m1.1.1.3.1.cmml"><mo id="alg0.l7.m1.1.1.3.1.2" xref="alg0.l7.m1.1.1.3.1.2.cmml">∑</mo><mi id="alg0.l7.m1.1.1.3.1.3" xref="alg0.l7.m1.1.1.3.1.3.cmml">k</mi></msub><msup id="alg0.l7.m1.1.1.3.2" xref="alg0.l7.m1.1.1.3.2.cmml"><mi id="alg0.l7.m1.1.1.3.2.2" xref="alg0.l7.m1.1.1.3.2.2.cmml">n</mi><mi id="alg0.l7.m1.1.1.3.2.3" xref="alg0.l7.m1.1.1.3.2.3.cmml">k</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l7.m1.1b"><apply id="alg0.l7.m1.1.1.cmml" xref="alg0.l7.m1.1.1"><eq id="alg0.l7.m1.1.1.1.cmml" xref="alg0.l7.m1.1.1.1"></eq><apply id="alg0.l7.m1.1.1.2.cmml" xref="alg0.l7.m1.1.1.2"><times id="alg0.l7.m1.1.1.2.1.cmml" xref="alg0.l7.m1.1.1.2.1"></times><ci id="alg0.l7.m1.1.1.2.2b.cmml" xref="alg0.l7.m1.1.1.2.2"><mtext id="alg0.l7.m1.1.1.2.2.cmml" xref="alg0.l7.m1.1.1.2.2"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext></ci><apply id="alg0.l7.m1.1.1.2.3.cmml" xref="alg0.l7.m1.1.1.2.3"><csymbol cd="ambiguous" id="alg0.l7.m1.1.1.2.3.1.cmml" xref="alg0.l7.m1.1.1.2.3">subscript</csymbol><apply id="alg0.l7.m1.1.1.2.3.2.cmml" xref="alg0.l7.m1.1.1.2.3.2"><ci id="alg0.l7.m1.1.1.2.3.2.1.cmml" xref="alg0.l7.m1.1.1.2.3.2.1">¯</ci><ci id="alg0.l7.m1.1.1.2.3.2.2.cmml" xref="alg0.l7.m1.1.1.2.3.2.2">𝑛</ci></apply><ci id="alg0.l7.m1.1.1.2.3.3.cmml" xref="alg0.l7.m1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="alg0.l7.m1.1.1.3.cmml" xref="alg0.l7.m1.1.1.3"><apply id="alg0.l7.m1.1.1.3.1.cmml" xref="alg0.l7.m1.1.1.3.1"><csymbol cd="ambiguous" id="alg0.l7.m1.1.1.3.1.1.cmml" xref="alg0.l7.m1.1.1.3.1">subscript</csymbol><sum id="alg0.l7.m1.1.1.3.1.2.cmml" xref="alg0.l7.m1.1.1.3.1.2"></sum><ci id="alg0.l7.m1.1.1.3.1.3.cmml" xref="alg0.l7.m1.1.1.3.1.3">𝑘</ci></apply><apply id="alg0.l7.m1.1.1.3.2.cmml" xref="alg0.l7.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg0.l7.m1.1.1.3.2.1.cmml" xref="alg0.l7.m1.1.1.3.2">superscript</csymbol><ci id="alg0.l7.m1.1.1.3.2.2.cmml" xref="alg0.l7.m1.1.1.3.2.2">𝑛</ci><ci id="alg0.l7.m1.1.1.3.2.3.cmml" xref="alg0.l7.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l7.m1.1c">\rule{0.0pt}{9.47217pt}\bar{n}_{t}=\sum_{k}n^{k}</annotation></semantics></math>      // <em id="alg0.l7.1" class="ltx_emph ltx_font_italic">Sum of weights</em>

</div>
<div id="alg0.l8" class="ltx_listingline">     <math id="alg0.l8.m1.1" class="ltx_Math" alttext="\rule{0.0pt}{9.47217pt}\Delta_{t}=\Delta^{k}_{t}/\bar{n}_{t}" display="inline"><semantics id="alg0.l8.m1.1a"><mrow id="alg0.l8.m1.1.1" xref="alg0.l8.m1.1.1.cmml"><mrow id="alg0.l8.m1.1.1.2" xref="alg0.l8.m1.1.1.2.cmml"><mtext id="alg0.l8.m1.1.1.2.2" xref="alg0.l8.m1.1.1.2.2b.cmml"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext><mo lspace="0em" rspace="0em" id="alg0.l8.m1.1.1.2.1" xref="alg0.l8.m1.1.1.2.1.cmml">​</mo><msub id="alg0.l8.m1.1.1.2.3" xref="alg0.l8.m1.1.1.2.3.cmml"><mi mathvariant="normal" id="alg0.l8.m1.1.1.2.3.2" xref="alg0.l8.m1.1.1.2.3.2.cmml">Δ</mi><mi id="alg0.l8.m1.1.1.2.3.3" xref="alg0.l8.m1.1.1.2.3.3.cmml">t</mi></msub></mrow><mo id="alg0.l8.m1.1.1.1" xref="alg0.l8.m1.1.1.1.cmml">=</mo><mrow id="alg0.l8.m1.1.1.3" xref="alg0.l8.m1.1.1.3.cmml"><msubsup id="alg0.l8.m1.1.1.3.2" xref="alg0.l8.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="alg0.l8.m1.1.1.3.2.2.2" xref="alg0.l8.m1.1.1.3.2.2.2.cmml">Δ</mi><mi id="alg0.l8.m1.1.1.3.2.3" xref="alg0.l8.m1.1.1.3.2.3.cmml">t</mi><mi id="alg0.l8.m1.1.1.3.2.2.3" xref="alg0.l8.m1.1.1.3.2.2.3.cmml">k</mi></msubsup><mo id="alg0.l8.m1.1.1.3.1" xref="alg0.l8.m1.1.1.3.1.cmml">/</mo><msub id="alg0.l8.m1.1.1.3.3" xref="alg0.l8.m1.1.1.3.3.cmml"><mover accent="true" id="alg0.l8.m1.1.1.3.3.2" xref="alg0.l8.m1.1.1.3.3.2.cmml"><mi id="alg0.l8.m1.1.1.3.3.2.2" xref="alg0.l8.m1.1.1.3.3.2.2.cmml">n</mi><mo id="alg0.l8.m1.1.1.3.3.2.1" xref="alg0.l8.m1.1.1.3.3.2.1.cmml">¯</mo></mover><mi id="alg0.l8.m1.1.1.3.3.3" xref="alg0.l8.m1.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l8.m1.1b"><apply id="alg0.l8.m1.1.1.cmml" xref="alg0.l8.m1.1.1"><eq id="alg0.l8.m1.1.1.1.cmml" xref="alg0.l8.m1.1.1.1"></eq><apply id="alg0.l8.m1.1.1.2.cmml" xref="alg0.l8.m1.1.1.2"><times id="alg0.l8.m1.1.1.2.1.cmml" xref="alg0.l8.m1.1.1.2.1"></times><ci id="alg0.l8.m1.1.1.2.2b.cmml" xref="alg0.l8.m1.1.1.2.2"><mtext id="alg0.l8.m1.1.1.2.2.cmml" xref="alg0.l8.m1.1.1.2.2"><span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span></mtext></ci><apply id="alg0.l8.m1.1.1.2.3.cmml" xref="alg0.l8.m1.1.1.2.3"><csymbol cd="ambiguous" id="alg0.l8.m1.1.1.2.3.1.cmml" xref="alg0.l8.m1.1.1.2.3">subscript</csymbol><ci id="alg0.l8.m1.1.1.2.3.2.cmml" xref="alg0.l8.m1.1.1.2.3.2">Δ</ci><ci id="alg0.l8.m1.1.1.2.3.3.cmml" xref="alg0.l8.m1.1.1.2.3.3">𝑡</ci></apply></apply><apply id="alg0.l8.m1.1.1.3.cmml" xref="alg0.l8.m1.1.1.3"><divide id="alg0.l8.m1.1.1.3.1.cmml" xref="alg0.l8.m1.1.1.3.1"></divide><apply id="alg0.l8.m1.1.1.3.2.cmml" xref="alg0.l8.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg0.l8.m1.1.1.3.2.1.cmml" xref="alg0.l8.m1.1.1.3.2">subscript</csymbol><apply id="alg0.l8.m1.1.1.3.2.2.cmml" xref="alg0.l8.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg0.l8.m1.1.1.3.2.2.1.cmml" xref="alg0.l8.m1.1.1.3.2">superscript</csymbol><ci id="alg0.l8.m1.1.1.3.2.2.2.cmml" xref="alg0.l8.m1.1.1.3.2.2.2">Δ</ci><ci id="alg0.l8.m1.1.1.3.2.2.3.cmml" xref="alg0.l8.m1.1.1.3.2.2.3">𝑘</ci></apply><ci id="alg0.l8.m1.1.1.3.2.3.cmml" xref="alg0.l8.m1.1.1.3.2.3">𝑡</ci></apply><apply id="alg0.l8.m1.1.1.3.3.cmml" xref="alg0.l8.m1.1.1.3.3"><csymbol cd="ambiguous" id="alg0.l8.m1.1.1.3.3.1.cmml" xref="alg0.l8.m1.1.1.3.3">subscript</csymbol><apply id="alg0.l8.m1.1.1.3.3.2.cmml" xref="alg0.l8.m1.1.1.3.3.2"><ci id="alg0.l8.m1.1.1.3.3.2.1.cmml" xref="alg0.l8.m1.1.1.3.3.2.1">¯</ci><ci id="alg0.l8.m1.1.1.3.3.2.2.cmml" xref="alg0.l8.m1.1.1.3.3.2.2">𝑛</ci></apply><ci id="alg0.l8.m1.1.1.3.3.3.cmml" xref="alg0.l8.m1.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l8.m1.1c">\rule{0.0pt}{9.47217pt}\Delta_{t}=\Delta^{k}_{t}/\bar{n}_{t}</annotation></semantics></math>      // <em id="alg0.l8.1" class="ltx_emph ltx_font_italic">Average update</em>

</div>
<div id="alg0.l9" class="ltx_listingline">     <math id="alg0.l9.m1.1" class="ltx_Math" alttext="w_{t+1}\leftarrow w_{t}+\Delta_{t}" display="inline"><semantics id="alg0.l9.m1.1a"><mrow id="alg0.l9.m1.1.1" xref="alg0.l9.m1.1.1.cmml"><msub id="alg0.l9.m1.1.1.2" xref="alg0.l9.m1.1.1.2.cmml"><mi id="alg0.l9.m1.1.1.2.2" xref="alg0.l9.m1.1.1.2.2.cmml">w</mi><mrow id="alg0.l9.m1.1.1.2.3" xref="alg0.l9.m1.1.1.2.3.cmml"><mi id="alg0.l9.m1.1.1.2.3.2" xref="alg0.l9.m1.1.1.2.3.2.cmml">t</mi><mo id="alg0.l9.m1.1.1.2.3.1" xref="alg0.l9.m1.1.1.2.3.1.cmml">+</mo><mn id="alg0.l9.m1.1.1.2.3.3" xref="alg0.l9.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="alg0.l9.m1.1.1.1" xref="alg0.l9.m1.1.1.1.cmml">←</mo><mrow id="alg0.l9.m1.1.1.3" xref="alg0.l9.m1.1.1.3.cmml"><msub id="alg0.l9.m1.1.1.3.2" xref="alg0.l9.m1.1.1.3.2.cmml"><mi id="alg0.l9.m1.1.1.3.2.2" xref="alg0.l9.m1.1.1.3.2.2.cmml">w</mi><mi id="alg0.l9.m1.1.1.3.2.3" xref="alg0.l9.m1.1.1.3.2.3.cmml">t</mi></msub><mo id="alg0.l9.m1.1.1.3.1" xref="alg0.l9.m1.1.1.3.1.cmml">+</mo><msub id="alg0.l9.m1.1.1.3.3" xref="alg0.l9.m1.1.1.3.3.cmml"><mi mathvariant="normal" id="alg0.l9.m1.1.1.3.3.2" xref="alg0.l9.m1.1.1.3.3.2.cmml">Δ</mi><mi id="alg0.l9.m1.1.1.3.3.3" xref="alg0.l9.m1.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l9.m1.1b"><apply id="alg0.l9.m1.1.1.cmml" xref="alg0.l9.m1.1.1"><ci id="alg0.l9.m1.1.1.1.cmml" xref="alg0.l9.m1.1.1.1">←</ci><apply id="alg0.l9.m1.1.1.2.cmml" xref="alg0.l9.m1.1.1.2"><csymbol cd="ambiguous" id="alg0.l9.m1.1.1.2.1.cmml" xref="alg0.l9.m1.1.1.2">subscript</csymbol><ci id="alg0.l9.m1.1.1.2.2.cmml" xref="alg0.l9.m1.1.1.2.2">𝑤</ci><apply id="alg0.l9.m1.1.1.2.3.cmml" xref="alg0.l9.m1.1.1.2.3"><plus id="alg0.l9.m1.1.1.2.3.1.cmml" xref="alg0.l9.m1.1.1.2.3.1"></plus><ci id="alg0.l9.m1.1.1.2.3.2.cmml" xref="alg0.l9.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="alg0.l9.m1.1.1.2.3.3.cmml" xref="alg0.l9.m1.1.1.2.3.3">1</cn></apply></apply><apply id="alg0.l9.m1.1.1.3.cmml" xref="alg0.l9.m1.1.1.3"><plus id="alg0.l9.m1.1.1.3.1.cmml" xref="alg0.l9.m1.1.1.3.1"></plus><apply id="alg0.l9.m1.1.1.3.2.cmml" xref="alg0.l9.m1.1.1.3.2"><csymbol cd="ambiguous" id="alg0.l9.m1.1.1.3.2.1.cmml" xref="alg0.l9.m1.1.1.3.2">subscript</csymbol><ci id="alg0.l9.m1.1.1.3.2.2.cmml" xref="alg0.l9.m1.1.1.3.2.2">𝑤</ci><ci id="alg0.l9.m1.1.1.3.2.3.cmml" xref="alg0.l9.m1.1.1.3.2.3">𝑡</ci></apply><apply id="alg0.l9.m1.1.1.3.3.cmml" xref="alg0.l9.m1.1.1.3.3"><csymbol cd="ambiguous" id="alg0.l9.m1.1.1.3.3.1.cmml" xref="alg0.l9.m1.1.1.3.3">subscript</csymbol><ci id="alg0.l9.m1.1.1.3.3.2.cmml" xref="alg0.l9.m1.1.1.3.3.2">Δ</ci><ci id="alg0.l9.m1.1.1.3.3.3.cmml" xref="alg0.l9.m1.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l9.m1.1c">w_{t+1}\leftarrow w_{t}+\Delta_{t}</annotation></semantics></math>

</div>
<div id="alg0.l10" class="ltx_listingline">  
</div>
<div id="alg0.l10a" class="ltx_listingline">   <span id="alg0.l10a.1" class="ltx_text ltx_font_bold">ClientUpdate(<math id="alg0.l10a.1.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="alg0.l10a.1.m1.1a"><mi id="alg0.l10a.1.m1.1.1" xref="alg0.l10a.1.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="alg0.l10a.1.m1.1b"><ci id="alg0.l10a.1.m1.1.1.cmml" xref="alg0.l10a.1.m1.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l10a.1.m1.1c">w</annotation></semantics></math>):</span> 
</div>
<div id="alg0.l11" class="ltx_listingline">  <math id="alg0.l11.m1.1" class="ltx_Math" alttext="\mathcal{B}\leftarrow" display="inline"><semantics id="alg0.l11.m1.1a"><mrow id="alg0.l11.m1.1.1" xref="alg0.l11.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="alg0.l11.m1.1.1.2" xref="alg0.l11.m1.1.1.2.cmml">ℬ</mi><mo stretchy="false" id="alg0.l11.m1.1.1.1" xref="alg0.l11.m1.1.1.1.cmml">←</mo><mi id="alg0.l11.m1.1.1.3" xref="alg0.l11.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg0.l11.m1.1b"><apply id="alg0.l11.m1.1.1.cmml" xref="alg0.l11.m1.1.1"><ci id="alg0.l11.m1.1.1.1.cmml" xref="alg0.l11.m1.1.1.1">←</ci><ci id="alg0.l11.m1.1.1.2.cmml" xref="alg0.l11.m1.1.1.2">ℬ</ci><csymbol cd="latexml" id="alg0.l11.m1.1.1.3.cmml" xref="alg0.l11.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l11.m1.1c">\mathcal{B}\leftarrow</annotation></semantics></math> (local data divided into minibatches)

</div>
<div id="alg0.l12" class="ltx_listingline">  <math id="alg0.l12.m1.1" class="ltx_Math" alttext="n\leftarrow|\mathcal{B}|" display="inline"><semantics id="alg0.l12.m1.1a"><mrow id="alg0.l12.m1.1.2" xref="alg0.l12.m1.1.2.cmml"><mi id="alg0.l12.m1.1.2.2" xref="alg0.l12.m1.1.2.2.cmml">n</mi><mo stretchy="false" id="alg0.l12.m1.1.2.1" xref="alg0.l12.m1.1.2.1.cmml">←</mo><mrow id="alg0.l12.m1.1.2.3.2" xref="alg0.l12.m1.1.2.3.1.cmml"><mo stretchy="false" id="alg0.l12.m1.1.2.3.2.1" xref="alg0.l12.m1.1.2.3.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="alg0.l12.m1.1.1" xref="alg0.l12.m1.1.1.cmml">ℬ</mi><mo stretchy="false" id="alg0.l12.m1.1.2.3.2.2" xref="alg0.l12.m1.1.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l12.m1.1b"><apply id="alg0.l12.m1.1.2.cmml" xref="alg0.l12.m1.1.2"><ci id="alg0.l12.m1.1.2.1.cmml" xref="alg0.l12.m1.1.2.1">←</ci><ci id="alg0.l12.m1.1.2.2.cmml" xref="alg0.l12.m1.1.2.2">𝑛</ci><apply id="alg0.l12.m1.1.2.3.1.cmml" xref="alg0.l12.m1.1.2.3.2"><abs id="alg0.l12.m1.1.2.3.1.1.cmml" xref="alg0.l12.m1.1.2.3.2.1"></abs><ci id="alg0.l12.m1.1.1.cmml" xref="alg0.l12.m1.1.1">ℬ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l12.m1.1c">n\leftarrow|\mathcal{B}|</annotation></semantics></math>      // <em id="alg0.l12.1" class="ltx_emph ltx_font_italic">Update weight</em>

</div>
<div id="alg0.l13" class="ltx_listingline">  <math id="alg0.l13.m1.1" class="ltx_Math" alttext="w_{\text{init}}\leftarrow w" display="inline"><semantics id="alg0.l13.m1.1a"><mrow id="alg0.l13.m1.1.1" xref="alg0.l13.m1.1.1.cmml"><msub id="alg0.l13.m1.1.1.2" xref="alg0.l13.m1.1.1.2.cmml"><mi id="alg0.l13.m1.1.1.2.2" xref="alg0.l13.m1.1.1.2.2.cmml">w</mi><mtext id="alg0.l13.m1.1.1.2.3" xref="alg0.l13.m1.1.1.2.3a.cmml">init</mtext></msub><mo stretchy="false" id="alg0.l13.m1.1.1.1" xref="alg0.l13.m1.1.1.1.cmml">←</mo><mi id="alg0.l13.m1.1.1.3" xref="alg0.l13.m1.1.1.3.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="alg0.l13.m1.1b"><apply id="alg0.l13.m1.1.1.cmml" xref="alg0.l13.m1.1.1"><ci id="alg0.l13.m1.1.1.1.cmml" xref="alg0.l13.m1.1.1.1">←</ci><apply id="alg0.l13.m1.1.1.2.cmml" xref="alg0.l13.m1.1.1.2"><csymbol cd="ambiguous" id="alg0.l13.m1.1.1.2.1.cmml" xref="alg0.l13.m1.1.1.2">subscript</csymbol><ci id="alg0.l13.m1.1.1.2.2.cmml" xref="alg0.l13.m1.1.1.2.2">𝑤</ci><ci id="alg0.l13.m1.1.1.2.3a.cmml" xref="alg0.l13.m1.1.1.2.3"><mtext mathsize="70%" id="alg0.l13.m1.1.1.2.3.cmml" xref="alg0.l13.m1.1.1.2.3">init</mtext></ci></apply><ci id="alg0.l13.m1.1.1.3.cmml" xref="alg0.l13.m1.1.1.3">𝑤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l13.m1.1c">w_{\text{init}}\leftarrow w</annotation></semantics></math>

</div>
<div id="alg0.l14" class="ltx_listingline">  <span id="alg0.l14.1" class="ltx_text ltx_font_bold">for</span> batch <math id="alg0.l14.m1.1" class="ltx_Math" alttext="b\in\mathcal{B}" display="inline"><semantics id="alg0.l14.m1.1a"><mrow id="alg0.l14.m1.1.1" xref="alg0.l14.m1.1.1.cmml"><mi id="alg0.l14.m1.1.1.2" xref="alg0.l14.m1.1.1.2.cmml">b</mi><mo id="alg0.l14.m1.1.1.1" xref="alg0.l14.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="alg0.l14.m1.1.1.3" xref="alg0.l14.m1.1.1.3.cmml">ℬ</mi></mrow><annotation-xml encoding="MathML-Content" id="alg0.l14.m1.1b"><apply id="alg0.l14.m1.1.1.cmml" xref="alg0.l14.m1.1.1"><in id="alg0.l14.m1.1.1.1.cmml" xref="alg0.l14.m1.1.1.1"></in><ci id="alg0.l14.m1.1.1.2.cmml" xref="alg0.l14.m1.1.1.2">𝑏</ci><ci id="alg0.l14.m1.1.1.3.cmml" xref="alg0.l14.m1.1.1.3">ℬ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l14.m1.1c">b\in\mathcal{B}</annotation></semantics></math> <span id="alg0.l14.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg0.l15" class="ltx_listingline">     <math id="alg0.l15.m1.2" class="ltx_Math" alttext="w\leftarrow w-\eta\triangledown\ell(w;b)" display="inline"><semantics id="alg0.l15.m1.2a"><mrow id="alg0.l15.m1.2.3" xref="alg0.l15.m1.2.3.cmml"><mi id="alg0.l15.m1.2.3.2" xref="alg0.l15.m1.2.3.2.cmml">w</mi><mo stretchy="false" id="alg0.l15.m1.2.3.1" xref="alg0.l15.m1.2.3.1.cmml">←</mo><mrow id="alg0.l15.m1.2.3.3" xref="alg0.l15.m1.2.3.3.cmml"><mi id="alg0.l15.m1.2.3.3.2" xref="alg0.l15.m1.2.3.3.2.cmml">w</mi><mo id="alg0.l15.m1.2.3.3.1" xref="alg0.l15.m1.2.3.3.1.cmml">−</mo><mrow id="alg0.l15.m1.2.3.3.3" xref="alg0.l15.m1.2.3.3.3.cmml"><mi id="alg0.l15.m1.2.3.3.3.2" xref="alg0.l15.m1.2.3.3.3.2.cmml">η</mi><mo lspace="0em" rspace="0em" id="alg0.l15.m1.2.3.3.3.1" xref="alg0.l15.m1.2.3.3.3.1.cmml">​</mo><mi mathvariant="normal" id="alg0.l15.m1.2.3.3.3.3" xref="alg0.l15.m1.2.3.3.3.3.cmml">▽</mi><mo lspace="0em" rspace="0em" id="alg0.l15.m1.2.3.3.3.1a" xref="alg0.l15.m1.2.3.3.3.1.cmml">​</mo><mi mathvariant="normal" id="alg0.l15.m1.2.3.3.3.4" xref="alg0.l15.m1.2.3.3.3.4.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="alg0.l15.m1.2.3.3.3.1b" xref="alg0.l15.m1.2.3.3.3.1.cmml">​</mo><mrow id="alg0.l15.m1.2.3.3.3.5.2" xref="alg0.l15.m1.2.3.3.3.5.1.cmml"><mo stretchy="false" id="alg0.l15.m1.2.3.3.3.5.2.1" xref="alg0.l15.m1.2.3.3.3.5.1.cmml">(</mo><mi id="alg0.l15.m1.1.1" xref="alg0.l15.m1.1.1.cmml">w</mi><mo id="alg0.l15.m1.2.3.3.3.5.2.2" xref="alg0.l15.m1.2.3.3.3.5.1.cmml">;</mo><mi id="alg0.l15.m1.2.2" xref="alg0.l15.m1.2.2.cmml">b</mi><mo stretchy="false" id="alg0.l15.m1.2.3.3.3.5.2.3" xref="alg0.l15.m1.2.3.3.3.5.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l15.m1.2b"><apply id="alg0.l15.m1.2.3.cmml" xref="alg0.l15.m1.2.3"><ci id="alg0.l15.m1.2.3.1.cmml" xref="alg0.l15.m1.2.3.1">←</ci><ci id="alg0.l15.m1.2.3.2.cmml" xref="alg0.l15.m1.2.3.2">𝑤</ci><apply id="alg0.l15.m1.2.3.3.cmml" xref="alg0.l15.m1.2.3.3"><minus id="alg0.l15.m1.2.3.3.1.cmml" xref="alg0.l15.m1.2.3.3.1"></minus><ci id="alg0.l15.m1.2.3.3.2.cmml" xref="alg0.l15.m1.2.3.3.2">𝑤</ci><apply id="alg0.l15.m1.2.3.3.3.cmml" xref="alg0.l15.m1.2.3.3.3"><times id="alg0.l15.m1.2.3.3.3.1.cmml" xref="alg0.l15.m1.2.3.3.3.1"></times><ci id="alg0.l15.m1.2.3.3.3.2.cmml" xref="alg0.l15.m1.2.3.3.3.2">𝜂</ci><ci id="alg0.l15.m1.2.3.3.3.3.cmml" xref="alg0.l15.m1.2.3.3.3.3">▽</ci><ci id="alg0.l15.m1.2.3.3.3.4.cmml" xref="alg0.l15.m1.2.3.3.3.4">ℓ</ci><list id="alg0.l15.m1.2.3.3.3.5.1.cmml" xref="alg0.l15.m1.2.3.3.3.5.2"><ci id="alg0.l15.m1.1.1.cmml" xref="alg0.l15.m1.1.1">𝑤</ci><ci id="alg0.l15.m1.2.2.cmml" xref="alg0.l15.m1.2.2">𝑏</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l15.m1.2c">w\leftarrow w-\eta\triangledown\ell(w;b)</annotation></semantics></math>

</div>
<div id="alg0.l16" class="ltx_listingline">  <math id="alg0.l16.m1.1" class="ltx_Math" alttext="\Delta\leftarrow n\cdot(w-w_{\text{init}})" display="inline"><semantics id="alg0.l16.m1.1a"><mrow id="alg0.l16.m1.1.1" xref="alg0.l16.m1.1.1.cmml"><mi mathvariant="normal" id="alg0.l16.m1.1.1.3" xref="alg0.l16.m1.1.1.3.cmml">Δ</mi><mo stretchy="false" id="alg0.l16.m1.1.1.2" xref="alg0.l16.m1.1.1.2.cmml">←</mo><mrow id="alg0.l16.m1.1.1.1" xref="alg0.l16.m1.1.1.1.cmml"><mi id="alg0.l16.m1.1.1.1.3" xref="alg0.l16.m1.1.1.1.3.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="alg0.l16.m1.1.1.1.2" xref="alg0.l16.m1.1.1.1.2.cmml">⋅</mo><mrow id="alg0.l16.m1.1.1.1.1.1" xref="alg0.l16.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg0.l16.m1.1.1.1.1.1.2" xref="alg0.l16.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="alg0.l16.m1.1.1.1.1.1.1" xref="alg0.l16.m1.1.1.1.1.1.1.cmml"><mi id="alg0.l16.m1.1.1.1.1.1.1.2" xref="alg0.l16.m1.1.1.1.1.1.1.2.cmml">w</mi><mo id="alg0.l16.m1.1.1.1.1.1.1.1" xref="alg0.l16.m1.1.1.1.1.1.1.1.cmml">−</mo><msub id="alg0.l16.m1.1.1.1.1.1.1.3" xref="alg0.l16.m1.1.1.1.1.1.1.3.cmml"><mi id="alg0.l16.m1.1.1.1.1.1.1.3.2" xref="alg0.l16.m1.1.1.1.1.1.1.3.2.cmml">w</mi><mtext id="alg0.l16.m1.1.1.1.1.1.1.3.3" xref="alg0.l16.m1.1.1.1.1.1.1.3.3a.cmml">init</mtext></msub></mrow><mo stretchy="false" id="alg0.l16.m1.1.1.1.1.1.3" xref="alg0.l16.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg0.l16.m1.1b"><apply id="alg0.l16.m1.1.1.cmml" xref="alg0.l16.m1.1.1"><ci id="alg0.l16.m1.1.1.2.cmml" xref="alg0.l16.m1.1.1.2">←</ci><ci id="alg0.l16.m1.1.1.3.cmml" xref="alg0.l16.m1.1.1.3">Δ</ci><apply id="alg0.l16.m1.1.1.1.cmml" xref="alg0.l16.m1.1.1.1"><ci id="alg0.l16.m1.1.1.1.2.cmml" xref="alg0.l16.m1.1.1.1.2">⋅</ci><ci id="alg0.l16.m1.1.1.1.3.cmml" xref="alg0.l16.m1.1.1.1.3">𝑛</ci><apply id="alg0.l16.m1.1.1.1.1.1.1.cmml" xref="alg0.l16.m1.1.1.1.1.1"><minus id="alg0.l16.m1.1.1.1.1.1.1.1.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.1"></minus><ci id="alg0.l16.m1.1.1.1.1.1.1.2.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.2">𝑤</ci><apply id="alg0.l16.m1.1.1.1.1.1.1.3.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="alg0.l16.m1.1.1.1.1.1.1.3.1.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="alg0.l16.m1.1.1.1.1.1.1.3.2.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.3.2">𝑤</ci><ci id="alg0.l16.m1.1.1.1.1.1.1.3.3a.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.3.3"><mtext mathsize="70%" id="alg0.l16.m1.1.1.1.1.1.1.3.3.cmml" xref="alg0.l16.m1.1.1.1.1.1.1.3.3">init</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg0.l16.m1.1c">\Delta\leftarrow n\cdot(w-w_{\text{init}})</annotation></semantics></math>      // <em id="alg0.l16.1" class="ltx_emph ltx_font_italic">Weighted update</em>

</div>
<div id="alg0.l17" class="ltx_listingline">  // <em id="alg0.l17.2" class="ltx_emph ltx_font_italic">Note <math id="alg0.l17.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="alg0.l17.1.m1.1a"><mi mathvariant="normal" id="alg0.l17.1.m1.1.1" xref="alg0.l17.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="alg0.l17.1.m1.1b"><ci id="alg0.l17.1.m1.1.1.cmml" xref="alg0.l17.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l17.1.m1.1c">\Delta</annotation></semantics></math> is more amenable to compression than <math id="alg0.l17.2.m2.1" class="ltx_Math" alttext="w" display="inline"><semantics id="alg0.l17.2.m2.1a"><mi id="alg0.l17.2.m2.1.1" xref="alg0.l17.2.m2.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="alg0.l17.2.m2.1b"><ci id="alg0.l17.2.m2.1.1.cmml" xref="alg0.l17.2.m2.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="alg0.l17.2.m2.1c">w</annotation></semantics></math></em>

</div>
<div id="alg0.l18" class="ltx_listingline">  return <math id="alg0.l18.m1.2" class="ltx_Math" alttext="(\Delta,n)" display="inline"><semantics id="alg0.l18.m1.2a"><mrow id="alg0.l18.m1.2.3.2" xref="alg0.l18.m1.2.3.1.cmml"><mo stretchy="false" id="alg0.l18.m1.2.3.2.1" xref="alg0.l18.m1.2.3.1.cmml">(</mo><mi mathvariant="normal" id="alg0.l18.m1.1.1" xref="alg0.l18.m1.1.1.cmml">Δ</mi><mo id="alg0.l18.m1.2.3.2.2" xref="alg0.l18.m1.2.3.1.cmml">,</mo><mi id="alg0.l18.m1.2.2" xref="alg0.l18.m1.2.2.cmml">n</mi><mo stretchy="false" id="alg0.l18.m1.2.3.2.3" xref="alg0.l18.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg0.l18.m1.2b"><interval closure="open" id="alg0.l18.m1.2.3.1.cmml" xref="alg0.l18.m1.2.3.2"><ci id="alg0.l18.m1.1.1.cmml" xref="alg0.l18.m1.1.1">Δ</ci><ci id="alg0.l18.m1.2.2.cmml" xref="alg0.l18.m1.2.2">𝑛</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="alg0.l18.m1.2c">(\Delta,n)</annotation></semantics></math> to server

</div>
</div>
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.6.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> <span id="alg1.7.2" class="ltx_text ltx_font_typewriter">FederatedAveraging</span> targeting updates from <math id="alg1.2.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="alg1.2.m1.1b"><mi id="alg1.2.m1.1.1" xref="alg1.2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.2.m1.1c"><ci id="alg1.2.m1.1.1.cmml" xref="alg1.2.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.2.m1.1d">K</annotation></semantics></math> clients per
round.</figcaption>
</figure>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="Anonymous Authors"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="Machine Learning, SysML, Federated Learning"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="SysML 2019"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="Towards Federated Learning at Scale: System Design"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/1902.01045" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1902.01046" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1902.01046">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1902.01046" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1902.01047" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  8 11:27:54 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
