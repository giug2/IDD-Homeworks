<!DOCTYPE html><html lang="ru">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.08815] NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks</title><meta property="og:description" content="Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks.
Federated learning (FL) provi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.08815">

<!--Generated on Thu Mar  7 01:03:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="ru">
<h1 class="ltx_title ltx_title_document">

 NAACL 2022 Findings
<br class="ltx_break">
FedNLP: Benchmarking Federated Learning Methods
<br class="ltx_break">for Natural Language Processing Tasks</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bill Yuchen Lin<sup id="id16.16.id1" class="ltx_sup">1</sup>, 
Chaoyang He<sup id="id17.17.id2" class="ltx_sup">1</sup><sup id="id18.18.id3" class="ltx_sup">∗</sup>, Zihang Zeng<sup id="id19.19.id4" class="ltx_sup">1</sup>, Hulin Wang<sup id="id20.20.id5" class="ltx_sup">1</sup>, 
<br class="ltx_break"><span id="id8.8.3" class="ltx_text ltx_font_bold">Yufen Huang<sup id="id8.8.3.1" class="ltx_sup"><span id="id8.8.3.1.1" class="ltx_text ltx_font_medium">1</span></sup>, Christophe Dupuy<sup id="id8.8.3.2" class="ltx_sup"><span id="id8.8.3.2.1" class="ltx_text ltx_font_medium">2</span></sup>, Rahul Gupta<sup id="id8.8.3.3" class="ltx_sup"><span id="id8.8.3.3.1" class="ltx_text ltx_font_medium">2</span></sup>,</span> 
<br class="ltx_break"><span id="id13.13.8" class="ltx_text ltx_font_bold">Mahdi Soltanolkotabi<sup id="id13.13.8.1" class="ltx_sup"><span id="id13.13.8.1.1" class="ltx_text ltx_font_medium">1</span></sup>, Xiang Ren<sup id="id13.13.8.2" class="ltx_sup"><span id="id13.13.8.2.1" class="ltx_text ltx_font_medium">1</span></sup><sup id="id13.13.8.3" class="ltx_sup"><span id="id13.13.8.3.1" class="ltx_text ltx_font_medium">∗</span></sup>, Salman Avestimehr<sup id="id13.13.8.4" class="ltx_sup"><span id="id13.13.8.4.1" class="ltx_text ltx_font_medium">1</span></sup><sup id="id13.13.8.5" class="ltx_sup"><span id="id13.13.8.5.1" class="ltx_text ltx_font_medium">∗</span></sup>
<br class="ltx_break"></span>University of Southern California<sup id="id21.21.id6" class="ltx_sup">1</sup>   Amazon Alexa AI<sup id="id22.22.id7" class="ltx_sup">2</sup> 
<br class="ltx_break"><span id="id23.23.id8" class="ltx_text ltx_font_typewriter">{yuchen.lin,chaoyang.he,saltanol,xiangren,avestime}@usc.edu gupra@amazon.com </span> 
<br class="ltx_break">
</span><span class="ltx_author_notes"> Bill and Chaoyang contributed equally; Xiang and Salman are equal advisors for this work.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Аннотация</h6>
<p id="id24.id1" class="ltx_p"><span id="id24.id1.1" class="ltx_text">Increasing concerns and regulations about data privacy and sparsity necessitate the study of privacy-preserving, decentralized learning methods for natural language processing (NLP) tasks.
Federated learning (FL) provides promising approaches for a large number of clients (e.g., personal devices or organizations) to collaboratively learn a shared global model to benefit all clients while allowing users to keep their data locally.
Despite interest in studying FL methods for NLP tasks, a systematic comparison and analysis is lacking in the literature.
Herein, we present the FedNLP<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/FedML-AI/FedNLP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/FedML-AI/FedNLP</a></span></span></span>, a benchmarking framework for evaluating federated learning methods on four common formulations of NLP tasks: text classification, sequence tagging, question answering, and seq2seq generation.
We propose a universal interface between Transformer-based language models (e.g., BERT, BART) and FL methods under various non-IID partitioning strategies.
Our extensive experiments with FedNLP provide empirical comparisons between FL methods and help us better understand the inherent challenges of this direction.
The comprehensive analysis points to intriguing and exciting future research aimed at developing FL methods for NLP tasks.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Fine-tuning large pre-trained language models (LMs) such as BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite> often leads to state-of-the-art performance in many realistic NLP applications (e.g., text classification, named entity recognition, question answering, summarization, etc.), when large-scale, <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">centralized</span> training datasets are available.
However, due to the increasing concerns and regulations about data privacy (<span id="S1.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> GPDR <cite class="ltx_cite ltx_citemacro_cite">Regulation (<a href="#bib.bib50" title="" class="ltx_ref">2016</a>)</cite>)
emerging data from realistic users have been much more <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">fragmented</span> and <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">distributed</span>,
forming <span id="S1.p1.1.5" class="ltx_text ltx_font_italic">decentralized private datasets</span> of multiple ‘‘data silos’’
(a <span id="S1.p1.1.6" class="ltx_text ltx_font_italic">data silo</span> can be viewed as an individual dataset) — across different clients (e.g., organizations or personal devices).</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2104.08815/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="462" height="475" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 1: </span>The FedNLP benchmarking framework. </figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To respect the privacy of the users and abide by these regulations,
we must assume that users’ data in a <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">silo</span> are not allowed to transfer to a centralized server or other clients.
For example, a client cannot share its private user data (e.g., documents, conversations, questions asked on the website/app) with other clients.
This is a common concern for <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">organizations</span> such as hospitals, financial institutions, or legal firms, as well as <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">personal computing devices</span> such as smartphones, virtual assistants (e.g., Amazon Alexa, Google Assistant, etc.), or a personal computer.
However, from a machine learning perspective, models trained on a centralized dataset that combine the data from all organizations or devices usually result in better performance in the NLP domain.
Therefore, it is of vital importance to study NLP problems in such a realistic yet more challenging scenario —i.e., training data are distributed across different clients and cannot be shared for privacy concerns.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The nascent field of <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">federated learning</span> <cite class="ltx_cite ltx_citemacro_cite">et al (<a href="#bib.bib11" title="" class="ltx_ref">2019</a>); Li et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020a</a>)</cite> (FL) aims to enable many individual clients to train their models jointly while keeping their local data <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">decentralized</span> and completely <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">private</span> from other users or a centralized server.
A common training schema of FL methods is that each client sends its model parameters to the server,
which updates and sends back the global model to all clients in each round.
Since the raw data of one client has never been exposed to others, FL is promising as an effective way to address the above challenges, particularly in the NLP domain, where many user-generated text data contain sensitive and/or personal information.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Despite the growing progress in the FL domain, research into and application for NLP has been rather limited.
There are indeed several recent works on using FL methods for processing medical information extraction tasks <cite class="ltx_cite ltx_citemacro_cite">Sui et al. (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite>.
However, such prior work usually has its experimental setup and specific task, making it difficult to fairly compare these FL methods and analyze their performance in other NLP tasks.
We argue that future research in this promising direction (FL for NLP) would highly benefit from a universal benchmarking platform for systematically comparing different FL methods for NLP.
To the best of our knowledge, such a benchmarking platform is still absent from the literature.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Therefore, our goal in this paper is to provide comprehensive comparisons between popular FL methods (e.g., FedAvg <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib40" title="" class="ltx_ref">2017a</a>)</cite>, FedOPT <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020b</a>)</cite>) for four mainstream formulations of NLP tasks: text classification, sequence tagging, question answering, and seq2seq generation.
Although there are few available realistic FL datasets for NLP due to privacy concerns,
we manage to use existing NLP datasets to create various non-IID data partitions over clients.
These non-IID partitions simulate various kinds of distribution shifts (e.g., label, features, quantities, etc.) over the clients, which often happen in real-world NLP applications.
As for the base NLP models, we use the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib61" title="" class="ltx_ref">2017</a>)</cite> as the backbone and support a wide range of pre-trained LMs such as DistilBERT <cite class="ltx_cite ltx_citemacro_cite">Sanh et al. (<a href="#bib.bib53" title="" class="ltx_ref">2019</a>)</cite>,
BERT <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>, BART <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>, etc. To conduct extensive experiments,
we need to support the experiments with multiple options on dimensions such as (1) <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">task formulations</span>, (2) <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">NLP models</span>, (3) <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">FL algorithms</span>, and (4) <span id="S1.p5.1.4" class="ltx_text ltx_font_italic">non-IID partitions</span>.
Therefore, we propose FedNLP, a modular framework with universal interfaces among the above four components, which is thus more extensible for supporting future research in FL for NLP.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We aim to unblock the research of FL for NLP with the following two-fold contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Evaluation and analysis.</span>   We systematically compare popular federated learning algorithms for mainstream NLP task formulations under multiple non-IID data partitions, which thus provides the first comprehensive understanding.
Our analysis reveals that there is a considerably large gap between centralized and decentralized training in various settings. We also analyze the efficiency of different FL methods and model sizes. With our analysis, we highlight several directions to advance FL for NLP.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Resource.</span>  The implementation of our experiments also forms a general open-source framework named FedNLP, which is capable of evaluating, analyzing, and developing FL methods for NLP. We also provide decentralized NLP datasets of various task formulations created by various non-IID partitioning strategies for future research.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The remainder of this paper is structured as follows.
We introduce the background knowledge of federated learning and several typical FL algorithms
in §<a href="#S2" title="2 Federated Learning for NLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Then, we present the proposed non-IID partitioning strategies to create synthetic datasets for different task formulations
in §<a href="#S3" title="3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
Our results, analysis, and findings
are in §<a href="#S4" title="4 Experimental Results and Analysis ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
Finally, we discuss related work
(§<a href="#S5" title="5 Related Work ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>)
and conclusions
(§<a href="#S6" title="6 Conclusion and Future Directions ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning for NLP</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first introduce the background knowledge of federated learning (FL) in the context of NLP tasks.
Then, we illustrate a unified FL framework that we used to study typical FL algorithms.
Based on this, we build our research framework, a general pipeline for benchmarking and developing FL methods for NLP.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Federated Learning Concepts</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Federated learning</span> (FL) is a machine learning paradigm where multiple entities (clients) collaborate in solving a machine learning problem under the coordination of a central server or service provider.
Each client’s raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objectives <cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>.
Therefore, federated learning has been seen as a promising direction to decrease the risk of attack and leakage, reduce the difficulty and cost of data movement, and meet the privacy-related data storage regulations.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.8" class="ltx_p">In the basic conception of federated learning, we would like to minimize the objective function,</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.38" class="ltx_Math" alttext="\begin{split}F(\bm{x})&amp;=\mathbb{E}_{i\sim\mathcal{P}}[F_{i}(\bm{x})],\\
\text{where}\quad F_{i}(\bm{x})&amp;=\mathbb{E}_{\xi\sim\mathcal{D}_{i}}[f_{i}(\bm{x},\xi)].\end{split}" display="block"><semantics id="S2.E1.m1.38a"><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" id="S2.E1.m1.38.38.4"><mtr id="S2.E1.m1.38.38.4a"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.38.38.4b"><mrow id="S2.E1.m1.4.4.4.4.4"><mi id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml">F</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.4.4.4.5">​</mo><mrow id="S2.E1.m1.4.4.4.4.4.6"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.2.2">(</mo><mi id="S2.E1.m1.3.3.3.3.3.3" xref="S2.E1.m1.3.3.3.3.3.3.cmml">𝒙</mi><mo stretchy="false" id="S2.E1.m1.4.4.4.4.4.4">)</mo></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.38.38.4c"><mrow id="S2.E1.m1.36.36.2.35.16.12.12"><mrow id="S2.E1.m1.36.36.2.35.16.12.12.1"><mi id="S2.E1.m1.36.36.2.35.16.12.12.1.2"></mi><mo id="S2.E1.m1.5.5.5.5.1.1" xref="S2.E1.m1.5.5.5.5.1.1.cmml">=</mo><mrow id="S2.E1.m1.36.36.2.35.16.12.12.1.1"><msub id="S2.E1.m1.36.36.2.35.16.12.12.1.1.3"><mi id="S2.E1.m1.6.6.6.6.2.2" xref="S2.E1.m1.6.6.6.6.2.2.cmml">𝔼</mi><mrow id="S2.E1.m1.7.7.7.7.3.3.1" xref="S2.E1.m1.7.7.7.7.3.3.1.cmml"><mi id="S2.E1.m1.7.7.7.7.3.3.1.2" xref="S2.E1.m1.7.7.7.7.3.3.1.2.cmml">i</mi><mo id="S2.E1.m1.7.7.7.7.3.3.1.1" xref="S2.E1.m1.7.7.7.7.3.3.1.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.7.7.7.7.3.3.1.3" xref="S2.E1.m1.7.7.7.7.3.3.1.3.cmml">𝒫</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.36.36.2.35.16.12.12.1.1.2">​</mo><mrow id="S2.E1.m1.36.36.2.35.16.12.12.1.1.1.1"><mo stretchy="false" id="S2.E1.m1.8.8.8.8.4.4">[</mo><mrow id="S2.E1.m1.36.36.2.35.16.12.12.1.1.1.1.1"><msub id="S2.E1.m1.36.36.2.35.16.12.12.1.1.1.1.1.2"><mi id="S2.E1.m1.9.9.9.9.5.5" xref="S2.E1.m1.9.9.9.9.5.5.cmml">F</mi><mi id="S2.E1.m1.10.10.10.10.6.6.1" xref="S2.E1.m1.10.10.10.10.6.6.1.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.36.36.2.35.16.12.12.1.1.1.1.1.1">​</mo><mrow id="S2.E1.m1.36.36.2.35.16.12.12.1.1.1.1.1.3"><mo stretchy="false" id="S2.E1.m1.11.11.11.11.7.7">(</mo><mi id="S2.E1.m1.12.12.12.12.8.8" xref="S2.E1.m1.12.12.12.12.8.8.cmml">𝒙</mi><mo stretchy="false" id="S2.E1.m1.13.13.13.13.9.9">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.14.14.14.14.10.10">]</mo></mrow></mrow></mrow><mo id="S2.E1.m1.15.15.15.15.11.11">,</mo></mrow></mtd></mtr><mtr id="S2.E1.m1.38.38.4d"><mtd class="ltx_align_right" columnalign="right" id="S2.E1.m1.38.38.4e"><mrow id="S2.E1.m1.37.37.3.36.20.7.7"><mtext id="S2.E1.m1.16.16.16.1.1.1" xref="S2.E1.m1.16.16.16.1.1.1a.cmml">where</mtext><mspace width="1em" id="S2.E1.m1.37.37.3.36.20.7.7.2"></mspace><mrow id="S2.E1.m1.37.37.3.36.20.7.7.1"><msub id="S2.E1.m1.37.37.3.36.20.7.7.1.2"><mi id="S2.E1.m1.17.17.17.2.2.2" xref="S2.E1.m1.17.17.17.2.2.2.cmml">F</mi><mi id="S2.E1.m1.18.18.18.3.3.3.1" xref="S2.E1.m1.18.18.18.3.3.3.1.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.37.37.3.36.20.7.7.1.1">​</mo><mrow id="S2.E1.m1.37.37.3.36.20.7.7.1.3"><mo stretchy="false" id="S2.E1.m1.19.19.19.4.4.4">(</mo><mi id="S2.E1.m1.20.20.20.5.5.5" xref="S2.E1.m1.20.20.20.5.5.5.cmml">𝒙</mi><mo stretchy="false" id="S2.E1.m1.21.21.21.6.6.6">)</mo></mrow></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E1.m1.38.38.4f"><mrow id="S2.E1.m1.38.38.4.37.21.14.14"><mrow id="S2.E1.m1.38.38.4.37.21.14.14.1"><mi id="S2.E1.m1.38.38.4.37.21.14.14.1.2"></mi><mo id="S2.E1.m1.22.22.22.7.1.1" xref="S2.E1.m1.22.22.22.7.1.1.cmml">=</mo><mrow id="S2.E1.m1.38.38.4.37.21.14.14.1.1"><msub id="S2.E1.m1.38.38.4.37.21.14.14.1.1.3"><mi id="S2.E1.m1.23.23.23.8.2.2" xref="S2.E1.m1.23.23.23.8.2.2.cmml">𝔼</mi><mrow id="S2.E1.m1.24.24.24.9.3.3.1" xref="S2.E1.m1.24.24.24.9.3.3.1.cmml"><mi id="S2.E1.m1.24.24.24.9.3.3.1.2" xref="S2.E1.m1.24.24.24.9.3.3.1.2.cmml">ξ</mi><mo id="S2.E1.m1.24.24.24.9.3.3.1.1" xref="S2.E1.m1.24.24.24.9.3.3.1.1.cmml">∼</mo><msub id="S2.E1.m1.24.24.24.9.3.3.1.3" xref="S2.E1.m1.24.24.24.9.3.3.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.24.24.24.9.3.3.1.3.2" xref="S2.E1.m1.24.24.24.9.3.3.1.3.2.cmml">𝒟</mi><mi id="S2.E1.m1.24.24.24.9.3.3.1.3.3" xref="S2.E1.m1.24.24.24.9.3.3.1.3.3.cmml">i</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.38.38.4.37.21.14.14.1.1.2">​</mo><mrow id="S2.E1.m1.38.38.4.37.21.14.14.1.1.1.1"><mo stretchy="false" id="S2.E1.m1.25.25.25.10.4.4">[</mo><mrow id="S2.E1.m1.38.38.4.37.21.14.14.1.1.1.1.1"><msub id="S2.E1.m1.38.38.4.37.21.14.14.1.1.1.1.1.2"><mi id="S2.E1.m1.26.26.26.11.5.5" xref="S2.E1.m1.26.26.26.11.5.5.cmml">f</mi><mi id="S2.E1.m1.27.27.27.12.6.6.1" xref="S2.E1.m1.27.27.27.12.6.6.1.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.38.38.4.37.21.14.14.1.1.1.1.1.1">​</mo><mrow id="S2.E1.m1.38.38.4.37.21.14.14.1.1.1.1.1.3"><mo stretchy="false" id="S2.E1.m1.28.28.28.13.7.7">(</mo><mi id="S2.E1.m1.29.29.29.14.8.8" xref="S2.E1.m1.29.29.29.14.8.8.cmml">𝒙</mi><mo id="S2.E1.m1.30.30.30.15.9.9">,</mo><mi id="S2.E1.m1.31.31.31.16.10.10" xref="S2.E1.m1.31.31.31.16.10.10.cmml">ξ</mi><mo stretchy="false" id="S2.E1.m1.32.32.32.17.11.11">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.33.33.33.18.12.12">]</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.E1.m1.34.34.34.19.13.13">.</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.E1.m1.38b"><apply id="S2.E1.m1.35.35.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.35.35.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S2.E1.m1.35.35.1.1.1.1.1.cmml"><eq id="S2.E1.m1.5.5.5.5.1.1.cmml" xref="S2.E1.m1.5.5.5.5.1.1"></eq><apply id="S2.E1.m1.35.35.1.1.1.1.1.3.cmml"><times id="S2.E1.m1.35.35.1.1.1.1.1.3.1.cmml"></times><ci id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1">𝐹</ci><ci id="S2.E1.m1.3.3.3.3.3.3.cmml" xref="S2.E1.m1.3.3.3.3.3.3">𝒙</ci></apply><list id="S2.E1.m1.35.35.1.1.1.1.1.1.2.cmml"><apply id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.cmml"><times id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.2.cmml"></times><apply id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.3.1.cmml">subscript</csymbol><ci id="S2.E1.m1.6.6.6.6.2.2.cmml" xref="S2.E1.m1.6.6.6.6.2.2">𝔼</ci><apply id="S2.E1.m1.7.7.7.7.3.3.1.cmml" xref="S2.E1.m1.7.7.7.7.3.3.1"><csymbol cd="latexml" id="S2.E1.m1.7.7.7.7.3.3.1.1.cmml" xref="S2.E1.m1.7.7.7.7.3.3.1.1">similar-to</csymbol><ci id="S2.E1.m1.7.7.7.7.3.3.1.2.cmml" xref="S2.E1.m1.7.7.7.7.3.3.1.2">𝑖</ci><ci id="S2.E1.m1.7.7.7.7.3.3.1.3.cmml" xref="S2.E1.m1.7.7.7.7.3.3.1.3">𝒫</ci></apply></apply><apply id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="latexml" id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.1.2.1.cmml">delimited-[]</csymbol><apply id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.1.1.1.cmml"><times id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.1.1.1.1.cmml"></times><apply id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.35.35.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">subscript</csymbol><ci id="S2.E1.m1.9.9.9.9.5.5.cmml" xref="S2.E1.m1.9.9.9.9.5.5">𝐹</ci><ci id="S2.E1.m1.10.10.10.10.6.6.1.cmml" xref="S2.E1.m1.10.10.10.10.6.6.1">𝑖</ci></apply><ci id="S2.E1.m1.12.12.12.12.8.8.cmml" xref="S2.E1.m1.12.12.12.12.8.8">𝒙</ci></apply></apply></apply><ci id="S2.E1.m1.16.16.16.1.1.1a.cmml" xref="S2.E1.m1.16.16.16.1.1.1"><mtext id="S2.E1.m1.16.16.16.1.1.1.cmml" xref="S2.E1.m1.16.16.16.1.1.1">where</mtext></ci></list></apply><apply id="S2.E1.m1.35.35.1.1.1.2.2.cmml"><eq id="S2.E1.m1.22.22.22.7.1.1.cmml" xref="S2.E1.m1.22.22.22.7.1.1"></eq><apply id="S2.E1.m1.35.35.1.1.1.2.2.3.cmml"><times id="S2.E1.m1.35.35.1.1.1.2.2.3.1.cmml"></times><apply id="S2.E1.m1.35.35.1.1.1.2.2.3.2.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.35.35.1.1.1.2.2.3.2.1.cmml">subscript</csymbol><ci id="S2.E1.m1.17.17.17.2.2.2.cmml" xref="S2.E1.m1.17.17.17.2.2.2">𝐹</ci><ci id="S2.E1.m1.18.18.18.3.3.3.1.cmml" xref="S2.E1.m1.18.18.18.3.3.3.1">𝑖</ci></apply><ci id="S2.E1.m1.20.20.20.5.5.5.cmml" xref="S2.E1.m1.20.20.20.5.5.5">𝒙</ci></apply><apply id="S2.E1.m1.35.35.1.1.1.2.2.1.cmml"><times id="S2.E1.m1.35.35.1.1.1.2.2.1.2.cmml"></times><apply id="S2.E1.m1.35.35.1.1.1.2.2.1.3.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.35.35.1.1.1.2.2.1.3.1.cmml">subscript</csymbol><ci id="S2.E1.m1.23.23.23.8.2.2.cmml" xref="S2.E1.m1.23.23.23.8.2.2">𝔼</ci><apply id="S2.E1.m1.24.24.24.9.3.3.1.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1"><csymbol cd="latexml" id="S2.E1.m1.24.24.24.9.3.3.1.1.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1.1">similar-to</csymbol><ci id="S2.E1.m1.24.24.24.9.3.3.1.2.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1.2">𝜉</ci><apply id="S2.E1.m1.24.24.24.9.3.3.1.3.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.24.24.24.9.3.3.1.3.1.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1.3">subscript</csymbol><ci id="S2.E1.m1.24.24.24.9.3.3.1.3.2.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1.3.2">𝒟</ci><ci id="S2.E1.m1.24.24.24.9.3.3.1.3.3.cmml" xref="S2.E1.m1.24.24.24.9.3.3.1.3.3">𝑖</ci></apply></apply></apply><apply id="S2.E1.m1.35.35.1.1.1.2.2.1.1.2.cmml"><csymbol cd="latexml" id="S2.E1.m1.35.35.1.1.1.2.2.1.1.2.1.cmml">delimited-[]</csymbol><apply id="S2.E1.m1.35.35.1.1.1.2.2.1.1.1.1.cmml"><times id="S2.E1.m1.35.35.1.1.1.2.2.1.1.1.1.1.cmml"></times><apply id="S2.E1.m1.35.35.1.1.1.2.2.1.1.1.1.2.cmml"><csymbol cd="ambiguous" id="S2.E1.m1.35.35.1.1.1.2.2.1.1.1.1.2.1.cmml">subscript</csymbol><ci id="S2.E1.m1.26.26.26.11.5.5.cmml" xref="S2.E1.m1.26.26.26.11.5.5">𝑓</ci><ci id="S2.E1.m1.27.27.27.12.6.6.1.cmml" xref="S2.E1.m1.27.27.27.12.6.6.1">𝑖</ci></apply><interval closure="open" id="S2.E1.m1.35.35.1.1.1.2.2.1.1.1.1.3.cmml"><ci id="S2.E1.m1.29.29.29.14.8.8.cmml" xref="S2.E1.m1.29.29.29.14.8.8">𝒙</ci><ci id="S2.E1.m1.31.31.31.16.10.10.cmml" xref="S2.E1.m1.31.31.31.16.10.10">𝜉</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.38c">\begin{split}F(\bm{x})&amp;=\mathbb{E}_{i\sim\mathcal{P}}[F_{i}(\bm{x})],\\
\text{where}\quad F_{i}(\bm{x})&amp;=\mathbb{E}_{\xi\sim\mathcal{D}_{i}}[f_{i}(\bm{x},\xi)].\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.7" class="ltx_p"><math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="\bm{x}\in\mathbb{R}^{d}" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml">𝒙</mi><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><in id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></in><ci id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2">𝒙</ci><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">ℝ</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\bm{x}\in\mathbb{R}^{d}</annotation></semantics></math> represents the parameter for the global model, <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="F_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><msub id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2.2" xref="S2.SS1.p2.2.m2.1.1.2.2.cmml">F</mi><mi id="S2.SS1.p2.2.m2.1.1.2.3" xref="S2.SS1.p2.2.m2.1.1.2.3.cmml">i</mi></msub><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">:</mo><mrow id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml"><msup id="S2.SS1.p2.2.m2.1.1.3.2" xref="S2.SS1.p2.2.m2.1.1.3.2.cmml"><mi id="S2.SS1.p2.2.m2.1.1.3.2.2" xref="S2.SS1.p2.2.m2.1.1.3.2.2.cmml">ℝ</mi><mi id="S2.SS1.p2.2.m2.1.1.3.2.3" xref="S2.SS1.p2.2.m2.1.1.3.2.3.cmml">d</mi></msup><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.3.1" xref="S2.SS1.p2.2.m2.1.1.3.1.cmml">→</mo><mi id="S2.SS1.p2.2.m2.1.1.3.3" xref="S2.SS1.p2.2.m2.1.1.3.3.cmml">ℝ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><ci id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1">:</ci><apply id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.2.1.cmml" xref="S2.SS1.p2.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.2.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2.2">𝐹</ci><ci id="S2.SS1.p2.2.m2.1.1.2.3.cmml" xref="S2.SS1.p2.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3"><ci id="S2.SS1.p2.2.m2.1.1.3.1.cmml" xref="S2.SS1.p2.2.m2.1.1.3.1">→</ci><apply id="S2.SS1.p2.2.m2.1.1.3.2.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.1.1.3.2.1.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2">superscript</csymbol><ci id="S2.SS1.p2.2.m2.1.1.3.2.2.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2.2">ℝ</ci><ci id="S2.SS1.p2.2.m2.1.1.3.2.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3.2.3">𝑑</ci></apply><ci id="S2.SS1.p2.2.m2.1.1.3.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3.3">ℝ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">F_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}</annotation></semantics></math> denotes the local objective function at client <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">i</annotation></semantics></math>, and <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">\mathcal{P}</annotation></semantics></math> denotes a distribution on the collection of clients <math id="S2.SS1.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.SS1.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">\mathcal{I}</annotation></semantics></math>. The local loss functions <math id="S2.SS1.p2.6.m6.2" class="ltx_Math" alttext="f_{i}(\bm{x},\xi)" display="inline"><semantics id="S2.SS1.p2.6.m6.2a"><mrow id="S2.SS1.p2.6.m6.2.3" xref="S2.SS1.p2.6.m6.2.3.cmml"><msub id="S2.SS1.p2.6.m6.2.3.2" xref="S2.SS1.p2.6.m6.2.3.2.cmml"><mi id="S2.SS1.p2.6.m6.2.3.2.2" xref="S2.SS1.p2.6.m6.2.3.2.2.cmml">f</mi><mi id="S2.SS1.p2.6.m6.2.3.2.3" xref="S2.SS1.p2.6.m6.2.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p2.6.m6.2.3.1" xref="S2.SS1.p2.6.m6.2.3.1.cmml">​</mo><mrow id="S2.SS1.p2.6.m6.2.3.3.2" xref="S2.SS1.p2.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S2.SS1.p2.6.m6.2.3.3.2.1" xref="S2.SS1.p2.6.m6.2.3.3.1.cmml">(</mo><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">𝒙</mi><mo id="S2.SS1.p2.6.m6.2.3.3.2.2" xref="S2.SS1.p2.6.m6.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p2.6.m6.2.2" xref="S2.SS1.p2.6.m6.2.2.cmml">ξ</mi><mo stretchy="false" id="S2.SS1.p2.6.m6.2.3.3.2.3" xref="S2.SS1.p2.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.2b"><apply id="S2.SS1.p2.6.m6.2.3.cmml" xref="S2.SS1.p2.6.m6.2.3"><times id="S2.SS1.p2.6.m6.2.3.1.cmml" xref="S2.SS1.p2.6.m6.2.3.1"></times><apply id="S2.SS1.p2.6.m6.2.3.2.cmml" xref="S2.SS1.p2.6.m6.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.p2.6.m6.2.3.2.1.cmml" xref="S2.SS1.p2.6.m6.2.3.2">subscript</csymbol><ci id="S2.SS1.p2.6.m6.2.3.2.2.cmml" xref="S2.SS1.p2.6.m6.2.3.2.2">𝑓</ci><ci id="S2.SS1.p2.6.m6.2.3.2.3.cmml" xref="S2.SS1.p2.6.m6.2.3.2.3">𝑖</ci></apply><interval closure="open" id="S2.SS1.p2.6.m6.2.3.3.1.cmml" xref="S2.SS1.p2.6.m6.2.3.3.2"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">𝒙</ci><ci id="S2.SS1.p2.6.m6.2.2.cmml" xref="S2.SS1.p2.6.m6.2.2">𝜉</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.2c">f_{i}(\bm{x},\xi)</annotation></semantics></math> are often the same across all clients, but the local data distribution <math id="S2.SS1.p2.7.m7.1" class="ltx_Math" alttext="\mathcal{D}_{i}" display="inline"><semantics id="S2.SS1.p2.7.m7.1a"><msub id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.7.m7.1.1.2" xref="S2.SS1.p2.7.m7.1.1.2.cmml">𝒟</mi><mi id="S2.SS1.p2.7.m7.1.1.3" xref="S2.SS1.p2.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><apply id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.7.m7.1.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p2.7.m7.1.1.2.cmml" xref="S2.SS1.p2.7.m7.1.1.2">𝒟</ci><ci id="S2.SS1.p2.7.m7.1.1.3.cmml" xref="S2.SS1.p2.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">\mathcal{D}_{i}</annotation></semantics></math> will often vary, capturing data heterogeneity.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.12" class="ltx_p"><em id="S2.SS1.p3.12.1" class="ltx_emph ltx_font_italic">Federated averaging</em> (FedAvg) <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib40" title="" class="ltx_ref">2017a</a>)</cite> is a common algorithm to solve (<a href="#S2.E1" title="Equation 1 ‣ 2.1 Federated Learning Concepts ‣ 2 Federated Learning for NLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) by dividing the training process into rounds. At the beginning of the <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">t</annotation></semantics></math>-th round (<math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="t\geq 0" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mrow id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml"><mi id="S2.SS1.p3.2.m2.1.1.2" xref="S2.SS1.p3.2.m2.1.1.2.cmml">t</mi><mo id="S2.SS1.p3.2.m2.1.1.1" xref="S2.SS1.p3.2.m2.1.1.1.cmml">≥</mo><mn id="S2.SS1.p3.2.m2.1.1.3" xref="S2.SS1.p3.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><apply id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1"><geq id="S2.SS1.p3.2.m2.1.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1.1"></geq><ci id="S2.SS1.p3.2.m2.1.1.2.cmml" xref="S2.SS1.p3.2.m2.1.1.2">𝑡</ci><cn type="integer" id="S2.SS1.p3.2.m2.1.1.3.cmml" xref="S2.SS1.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">t\geq 0</annotation></semantics></math>), the server broadcasts the current global model <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="\bm{x}^{(t)}" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><msup id="S2.SS1.p3.3.m3.1.2" xref="S2.SS1.p3.3.m3.1.2.cmml"><mi id="S2.SS1.p3.3.m3.1.2.2" xref="S2.SS1.p3.3.m3.1.2.2.cmml">𝒙</mi><mrow id="S2.SS1.p3.3.m3.1.1.1.3" xref="S2.SS1.p3.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.3.m3.1.1.1.3.1" xref="S2.SS1.p3.3.m3.1.2.cmml">(</mo><mi id="S2.SS1.p3.3.m3.1.1.1.1" xref="S2.SS1.p3.3.m3.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.3.m3.1.1.1.3.2" xref="S2.SS1.p3.3.m3.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><apply id="S2.SS1.p3.3.m3.1.2.cmml" xref="S2.SS1.p3.3.m3.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.3.m3.1.2.1.cmml" xref="S2.SS1.p3.3.m3.1.2">superscript</csymbol><ci id="S2.SS1.p3.3.m3.1.2.2.cmml" xref="S2.SS1.p3.3.m3.1.2.2">𝒙</ci><ci id="S2.SS1.p3.3.m3.1.1.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">\bm{x}^{(t)}</annotation></semantics></math> to a <em id="S2.SS1.p3.12.2" class="ltx_emph ltx_font_italic">cohort</em> of participants: a random subset of clients from <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{S}^{(t)}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><msup id="S2.SS1.p3.4.m4.1.2" xref="S2.SS1.p3.4.m4.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.4.m4.1.2.2" xref="S2.SS1.p3.4.m4.1.2.2.cmml">𝒮</mi><mrow id="S2.SS1.p3.4.m4.1.1.1.3" xref="S2.SS1.p3.4.m4.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.4.m4.1.1.1.3.1" xref="S2.SS1.p3.4.m4.1.2.cmml">(</mo><mi id="S2.SS1.p3.4.m4.1.1.1.1" xref="S2.SS1.p3.4.m4.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.4.m4.1.1.1.3.2" xref="S2.SS1.p3.4.m4.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><apply id="S2.SS1.p3.4.m4.1.2.cmml" xref="S2.SS1.p3.4.m4.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.4.m4.1.2.1.cmml" xref="S2.SS1.p3.4.m4.1.2">superscript</csymbol><ci id="S2.SS1.p3.4.m4.1.2.2.cmml" xref="S2.SS1.p3.4.m4.1.2.2">𝒮</ci><ci id="S2.SS1.p3.4.m4.1.1.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">\mathcal{S}^{(t)}</annotation></semantics></math> which includes <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><mi id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><ci id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">M</annotation></semantics></math> clients in total. Then, each sampled client in the round’s cohort performs <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="\tau_{i}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msub id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">τ</mi><mi id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">𝜏</ci><ci id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">\tau_{i}</annotation></semantics></math> local SGD updates on its own local dataset and sends the local model changes <math id="S2.SS1.p3.7.m7.4" class="ltx_Math" alttext="\Delta_{i}^{(t)}=\bm{x}_{i}^{(t,\tau_{i})}-\bm{x}^{(t)}" display="inline"><semantics id="S2.SS1.p3.7.m7.4a"><mrow id="S2.SS1.p3.7.m7.4.5" xref="S2.SS1.p3.7.m7.4.5.cmml"><msubsup id="S2.SS1.p3.7.m7.4.5.2" xref="S2.SS1.p3.7.m7.4.5.2.cmml"><mi mathvariant="normal" id="S2.SS1.p3.7.m7.4.5.2.2.2" xref="S2.SS1.p3.7.m7.4.5.2.2.2.cmml">Δ</mi><mi id="S2.SS1.p3.7.m7.4.5.2.2.3" xref="S2.SS1.p3.7.m7.4.5.2.2.3.cmml">i</mi><mrow id="S2.SS1.p3.7.m7.1.1.1.3" xref="S2.SS1.p3.7.m7.4.5.2.cmml"><mo stretchy="false" id="S2.SS1.p3.7.m7.1.1.1.3.1" xref="S2.SS1.p3.7.m7.4.5.2.cmml">(</mo><mi id="S2.SS1.p3.7.m7.1.1.1.1" xref="S2.SS1.p3.7.m7.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.7.m7.1.1.1.3.2" xref="S2.SS1.p3.7.m7.4.5.2.cmml">)</mo></mrow></msubsup><mo id="S2.SS1.p3.7.m7.4.5.1" xref="S2.SS1.p3.7.m7.4.5.1.cmml">=</mo><mrow id="S2.SS1.p3.7.m7.4.5.3" xref="S2.SS1.p3.7.m7.4.5.3.cmml"><msubsup id="S2.SS1.p3.7.m7.4.5.3.2" xref="S2.SS1.p3.7.m7.4.5.3.2.cmml"><mi id="S2.SS1.p3.7.m7.4.5.3.2.2.2" xref="S2.SS1.p3.7.m7.4.5.3.2.2.2.cmml">𝒙</mi><mi id="S2.SS1.p3.7.m7.4.5.3.2.2.3" xref="S2.SS1.p3.7.m7.4.5.3.2.2.3.cmml">i</mi><mrow id="S2.SS1.p3.7.m7.3.3.2.2" xref="S2.SS1.p3.7.m7.3.3.2.3.cmml"><mo stretchy="false" id="S2.SS1.p3.7.m7.3.3.2.2.2" xref="S2.SS1.p3.7.m7.3.3.2.3.cmml">(</mo><mi id="S2.SS1.p3.7.m7.2.2.1.1" xref="S2.SS1.p3.7.m7.2.2.1.1.cmml">t</mi><mo id="S2.SS1.p3.7.m7.3.3.2.2.3" xref="S2.SS1.p3.7.m7.3.3.2.3.cmml">,</mo><msub id="S2.SS1.p3.7.m7.3.3.2.2.1" xref="S2.SS1.p3.7.m7.3.3.2.2.1.cmml"><mi id="S2.SS1.p3.7.m7.3.3.2.2.1.2" xref="S2.SS1.p3.7.m7.3.3.2.2.1.2.cmml">τ</mi><mi id="S2.SS1.p3.7.m7.3.3.2.2.1.3" xref="S2.SS1.p3.7.m7.3.3.2.2.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.SS1.p3.7.m7.3.3.2.2.4" xref="S2.SS1.p3.7.m7.3.3.2.3.cmml">)</mo></mrow></msubsup><mo id="S2.SS1.p3.7.m7.4.5.3.1" xref="S2.SS1.p3.7.m7.4.5.3.1.cmml">−</mo><msup id="S2.SS1.p3.7.m7.4.5.3.3" xref="S2.SS1.p3.7.m7.4.5.3.3.cmml"><mi id="S2.SS1.p3.7.m7.4.5.3.3.2" xref="S2.SS1.p3.7.m7.4.5.3.3.2.cmml">𝒙</mi><mrow id="S2.SS1.p3.7.m7.4.4.1.3" xref="S2.SS1.p3.7.m7.4.5.3.3.cmml"><mo stretchy="false" id="S2.SS1.p3.7.m7.4.4.1.3.1" xref="S2.SS1.p3.7.m7.4.5.3.3.cmml">(</mo><mi id="S2.SS1.p3.7.m7.4.4.1.1" xref="S2.SS1.p3.7.m7.4.4.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.7.m7.4.4.1.3.2" xref="S2.SS1.p3.7.m7.4.5.3.3.cmml">)</mo></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.4b"><apply id="S2.SS1.p3.7.m7.4.5.cmml" xref="S2.SS1.p3.7.m7.4.5"><eq id="S2.SS1.p3.7.m7.4.5.1.cmml" xref="S2.SS1.p3.7.m7.4.5.1"></eq><apply id="S2.SS1.p3.7.m7.4.5.2.cmml" xref="S2.SS1.p3.7.m7.4.5.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.4.5.2.1.cmml" xref="S2.SS1.p3.7.m7.4.5.2">superscript</csymbol><apply id="S2.SS1.p3.7.m7.4.5.2.2.cmml" xref="S2.SS1.p3.7.m7.4.5.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.4.5.2.2.1.cmml" xref="S2.SS1.p3.7.m7.4.5.2">subscript</csymbol><ci id="S2.SS1.p3.7.m7.4.5.2.2.2.cmml" xref="S2.SS1.p3.7.m7.4.5.2.2.2">Δ</ci><ci id="S2.SS1.p3.7.m7.4.5.2.2.3.cmml" xref="S2.SS1.p3.7.m7.4.5.2.2.3">𝑖</ci></apply><ci id="S2.SS1.p3.7.m7.1.1.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1.1.1">𝑡</ci></apply><apply id="S2.SS1.p3.7.m7.4.5.3.cmml" xref="S2.SS1.p3.7.m7.4.5.3"><minus id="S2.SS1.p3.7.m7.4.5.3.1.cmml" xref="S2.SS1.p3.7.m7.4.5.3.1"></minus><apply id="S2.SS1.p3.7.m7.4.5.3.2.cmml" xref="S2.SS1.p3.7.m7.4.5.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.4.5.3.2.1.cmml" xref="S2.SS1.p3.7.m7.4.5.3.2">superscript</csymbol><apply id="S2.SS1.p3.7.m7.4.5.3.2.2.cmml" xref="S2.SS1.p3.7.m7.4.5.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.4.5.3.2.2.1.cmml" xref="S2.SS1.p3.7.m7.4.5.3.2">subscript</csymbol><ci id="S2.SS1.p3.7.m7.4.5.3.2.2.2.cmml" xref="S2.SS1.p3.7.m7.4.5.3.2.2.2">𝒙</ci><ci id="S2.SS1.p3.7.m7.4.5.3.2.2.3.cmml" xref="S2.SS1.p3.7.m7.4.5.3.2.2.3">𝑖</ci></apply><interval closure="open" id="S2.SS1.p3.7.m7.3.3.2.3.cmml" xref="S2.SS1.p3.7.m7.3.3.2.2"><ci id="S2.SS1.p3.7.m7.2.2.1.1.cmml" xref="S2.SS1.p3.7.m7.2.2.1.1">𝑡</ci><apply id="S2.SS1.p3.7.m7.3.3.2.2.1.cmml" xref="S2.SS1.p3.7.m7.3.3.2.2.1"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.3.3.2.2.1.1.cmml" xref="S2.SS1.p3.7.m7.3.3.2.2.1">subscript</csymbol><ci id="S2.SS1.p3.7.m7.3.3.2.2.1.2.cmml" xref="S2.SS1.p3.7.m7.3.3.2.2.1.2">𝜏</ci><ci id="S2.SS1.p3.7.m7.3.3.2.2.1.3.cmml" xref="S2.SS1.p3.7.m7.3.3.2.2.1.3">𝑖</ci></apply></interval></apply><apply id="S2.SS1.p3.7.m7.4.5.3.3.cmml" xref="S2.SS1.p3.7.m7.4.5.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.7.m7.4.5.3.3.1.cmml" xref="S2.SS1.p3.7.m7.4.5.3.3">superscript</csymbol><ci id="S2.SS1.p3.7.m7.4.5.3.3.2.cmml" xref="S2.SS1.p3.7.m7.4.5.3.3.2">𝒙</ci><ci id="S2.SS1.p3.7.m7.4.4.1.1.cmml" xref="S2.SS1.p3.7.m7.4.4.1.1">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.4c">\Delta_{i}^{(t)}=\bm{x}_{i}^{(t,\tau_{i})}-\bm{x}^{(t)}</annotation></semantics></math> to the server. Finally, the server uses the aggregated <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="\Delta_{i}^{(t)}" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><msubsup id="S2.SS1.p3.8.m8.1.2" xref="S2.SS1.p3.8.m8.1.2.cmml"><mi mathvariant="normal" id="S2.SS1.p3.8.m8.1.2.2.2" xref="S2.SS1.p3.8.m8.1.2.2.2.cmml">Δ</mi><mi id="S2.SS1.p3.8.m8.1.2.2.3" xref="S2.SS1.p3.8.m8.1.2.2.3.cmml">i</mi><mrow id="S2.SS1.p3.8.m8.1.1.1.3" xref="S2.SS1.p3.8.m8.1.2.cmml"><mo stretchy="false" id="S2.SS1.p3.8.m8.1.1.1.3.1" xref="S2.SS1.p3.8.m8.1.2.cmml">(</mo><mi id="S2.SS1.p3.8.m8.1.1.1.1" xref="S2.SS1.p3.8.m8.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.8.m8.1.1.1.3.2" xref="S2.SS1.p3.8.m8.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><apply id="S2.SS1.p3.8.m8.1.2.cmml" xref="S2.SS1.p3.8.m8.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.2.1.cmml" xref="S2.SS1.p3.8.m8.1.2">superscript</csymbol><apply id="S2.SS1.p3.8.m8.1.2.2.cmml" xref="S2.SS1.p3.8.m8.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.8.m8.1.2.2.1.cmml" xref="S2.SS1.p3.8.m8.1.2">subscript</csymbol><ci id="S2.SS1.p3.8.m8.1.2.2.2.cmml" xref="S2.SS1.p3.8.m8.1.2.2.2">Δ</ci><ci id="S2.SS1.p3.8.m8.1.2.2.3.cmml" xref="S2.SS1.p3.8.m8.1.2.2.3">𝑖</ci></apply><ci id="S2.SS1.p3.8.m8.1.1.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">\Delta_{i}^{(t)}</annotation></semantics></math> to update the global model:
<math id="S2.SS1.p3.9.m9.6" class="ltx_Math" alttext="\bm{x}^{(t+1)}=\bm{x}^{(t)}+\frac{\sum_{i\in\mathcal{S}^{(t)}}p_{i}\Delta_{i}^{(t)}}{\sum_{i\in\mathcal{S}^{(t)}}p_{i}}." display="inline"><semantics id="S2.SS1.p3.9.m9.6a"><mrow id="S2.SS1.p3.9.m9.6.6.1" xref="S2.SS1.p3.9.m9.6.6.1.1.cmml"><mrow id="S2.SS1.p3.9.m9.6.6.1.1" xref="S2.SS1.p3.9.m9.6.6.1.1.cmml"><msup id="S2.SS1.p3.9.m9.6.6.1.1.2" xref="S2.SS1.p3.9.m9.6.6.1.1.2.cmml"><mi id="S2.SS1.p3.9.m9.6.6.1.1.2.2" xref="S2.SS1.p3.9.m9.6.6.1.1.2.2.cmml">𝒙</mi><mrow id="S2.SS1.p3.9.m9.1.1.1.1" xref="S2.SS1.p3.9.m9.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.9.m9.1.1.1.1.2" xref="S2.SS1.p3.9.m9.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.9.m9.1.1.1.1.1" xref="S2.SS1.p3.9.m9.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.9.m9.1.1.1.1.1.2" xref="S2.SS1.p3.9.m9.1.1.1.1.1.2.cmml">t</mi><mo id="S2.SS1.p3.9.m9.1.1.1.1.1.1" xref="S2.SS1.p3.9.m9.1.1.1.1.1.1.cmml">+</mo><mn id="S2.SS1.p3.9.m9.1.1.1.1.1.3" xref="S2.SS1.p3.9.m9.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.SS1.p3.9.m9.1.1.1.1.3" xref="S2.SS1.p3.9.m9.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S2.SS1.p3.9.m9.6.6.1.1.1" xref="S2.SS1.p3.9.m9.6.6.1.1.1.cmml">=</mo><mrow id="S2.SS1.p3.9.m9.6.6.1.1.3" xref="S2.SS1.p3.9.m9.6.6.1.1.3.cmml"><msup id="S2.SS1.p3.9.m9.6.6.1.1.3.2" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2.cmml"><mi id="S2.SS1.p3.9.m9.6.6.1.1.3.2.2" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2.2.cmml">𝒙</mi><mrow id="S2.SS1.p3.9.m9.2.2.1.3" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2.cmml"><mo stretchy="false" id="S2.SS1.p3.9.m9.2.2.1.3.1" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2.cmml">(</mo><mi id="S2.SS1.p3.9.m9.2.2.1.1" xref="S2.SS1.p3.9.m9.2.2.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.9.m9.2.2.1.3.2" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2.cmml">)</mo></mrow></msup><mo id="S2.SS1.p3.9.m9.6.6.1.1.3.1" xref="S2.SS1.p3.9.m9.6.6.1.1.3.1.cmml">+</mo><mfrac id="S2.SS1.p3.9.m9.5.5" xref="S2.SS1.p3.9.m9.5.5.cmml"><mrow id="S2.SS1.p3.9.m9.4.4.2" xref="S2.SS1.p3.9.m9.4.4.2.cmml"><mstyle displaystyle="false" id="S2.SS1.p3.9.m9.4.4.2.3" xref="S2.SS1.p3.9.m9.4.4.2.3.cmml"><msub id="S2.SS1.p3.9.m9.4.4.2.3a" xref="S2.SS1.p3.9.m9.4.4.2.3.cmml"><mo id="S2.SS1.p3.9.m9.4.4.2.3.2" xref="S2.SS1.p3.9.m9.4.4.2.3.2.cmml">∑</mo><mrow id="S2.SS1.p3.9.m9.3.3.1.1.1" xref="S2.SS1.p3.9.m9.3.3.1.1.1.cmml"><mi id="S2.SS1.p3.9.m9.3.3.1.1.1.3" xref="S2.SS1.p3.9.m9.3.3.1.1.1.3.cmml">i</mi><mo id="S2.SS1.p3.9.m9.3.3.1.1.1.2" xref="S2.SS1.p3.9.m9.3.3.1.1.1.2.cmml">∈</mo><msup id="S2.SS1.p3.9.m9.3.3.1.1.1.4" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.9.m9.3.3.1.1.1.4.2" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4.2.cmml">𝒮</mi><mrow id="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.3" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4.cmml"><mo stretchy="false" id="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.3.1" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4.cmml">(</mo><mi id="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.1" xref="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.3.2" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4.cmml">)</mo></mrow></msup></mrow></msub></mstyle><mrow id="S2.SS1.p3.9.m9.4.4.2.4" xref="S2.SS1.p3.9.m9.4.4.2.4.cmml"><msub id="S2.SS1.p3.9.m9.4.4.2.4.2" xref="S2.SS1.p3.9.m9.4.4.2.4.2.cmml"><mi id="S2.SS1.p3.9.m9.4.4.2.4.2.2" xref="S2.SS1.p3.9.m9.4.4.2.4.2.2.cmml">p</mi><mi id="S2.SS1.p3.9.m9.4.4.2.4.2.3" xref="S2.SS1.p3.9.m9.4.4.2.4.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p3.9.m9.4.4.2.4.1" xref="S2.SS1.p3.9.m9.4.4.2.4.1.cmml">​</mo><msubsup id="S2.SS1.p3.9.m9.4.4.2.4.3" xref="S2.SS1.p3.9.m9.4.4.2.4.3.cmml"><mi mathvariant="normal" id="S2.SS1.p3.9.m9.4.4.2.4.3.2.2" xref="S2.SS1.p3.9.m9.4.4.2.4.3.2.2.cmml">Δ</mi><mi id="S2.SS1.p3.9.m9.4.4.2.4.3.2.3" xref="S2.SS1.p3.9.m9.4.4.2.4.3.2.3.cmml">i</mi><mrow id="S2.SS1.p3.9.m9.4.4.2.2.1.3" xref="S2.SS1.p3.9.m9.4.4.2.4.3.cmml"><mo stretchy="false" id="S2.SS1.p3.9.m9.4.4.2.2.1.3.1" xref="S2.SS1.p3.9.m9.4.4.2.4.3.cmml">(</mo><mi id="S2.SS1.p3.9.m9.4.4.2.2.1.1" xref="S2.SS1.p3.9.m9.4.4.2.2.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.9.m9.4.4.2.2.1.3.2" xref="S2.SS1.p3.9.m9.4.4.2.4.3.cmml">)</mo></mrow></msubsup></mrow></mrow><mrow id="S2.SS1.p3.9.m9.5.5.3" xref="S2.SS1.p3.9.m9.5.5.3.cmml"><mstyle displaystyle="false" id="S2.SS1.p3.9.m9.5.5.3.2" xref="S2.SS1.p3.9.m9.5.5.3.2.cmml"><msub id="S2.SS1.p3.9.m9.5.5.3.2a" xref="S2.SS1.p3.9.m9.5.5.3.2.cmml"><mo id="S2.SS1.p3.9.m9.5.5.3.2.2" xref="S2.SS1.p3.9.m9.5.5.3.2.2.cmml">∑</mo><mrow id="S2.SS1.p3.9.m9.5.5.3.1.1" xref="S2.SS1.p3.9.m9.5.5.3.1.1.cmml"><mi id="S2.SS1.p3.9.m9.5.5.3.1.1.3" xref="S2.SS1.p3.9.m9.5.5.3.1.1.3.cmml">i</mi><mo id="S2.SS1.p3.9.m9.5.5.3.1.1.2" xref="S2.SS1.p3.9.m9.5.5.3.1.1.2.cmml">∈</mo><msup id="S2.SS1.p3.9.m9.5.5.3.1.1.4" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.9.m9.5.5.3.1.1.4.2" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4.2.cmml">𝒮</mi><mrow id="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.3" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4.cmml"><mo stretchy="false" id="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.3.1" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4.cmml">(</mo><mi id="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.1" xref="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.3.2" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4.cmml">)</mo></mrow></msup></mrow></msub></mstyle><msub id="S2.SS1.p3.9.m9.5.5.3.3" xref="S2.SS1.p3.9.m9.5.5.3.3.cmml"><mi id="S2.SS1.p3.9.m9.5.5.3.3.2" xref="S2.SS1.p3.9.m9.5.5.3.3.2.cmml">p</mi><mi id="S2.SS1.p3.9.m9.5.5.3.3.3" xref="S2.SS1.p3.9.m9.5.5.3.3.3.cmml">i</mi></msub></mrow></mfrac></mrow></mrow><mo lspace="0em" id="S2.SS1.p3.9.m9.6.6.1.2" xref="S2.SS1.p3.9.m9.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m9.6b"><apply id="S2.SS1.p3.9.m9.6.6.1.1.cmml" xref="S2.SS1.p3.9.m9.6.6.1"><eq id="S2.SS1.p3.9.m9.6.6.1.1.1.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.1"></eq><apply id="S2.SS1.p3.9.m9.6.6.1.1.2.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.6.6.1.1.2.1.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.2">superscript</csymbol><ci id="S2.SS1.p3.9.m9.6.6.1.1.2.2.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.2.2">𝒙</ci><apply id="S2.SS1.p3.9.m9.1.1.1.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1.1.1"><plus id="S2.SS1.p3.9.m9.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.9.m9.1.1.1.1.1.1"></plus><ci id="S2.SS1.p3.9.m9.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.9.m9.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S2.SS1.p3.9.m9.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.9.m9.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.SS1.p3.9.m9.6.6.1.1.3.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.3"><plus id="S2.SS1.p3.9.m9.6.6.1.1.3.1.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.3.1"></plus><apply id="S2.SS1.p3.9.m9.6.6.1.1.3.2.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.6.6.1.1.3.2.1.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2">superscript</csymbol><ci id="S2.SS1.p3.9.m9.6.6.1.1.3.2.2.cmml" xref="S2.SS1.p3.9.m9.6.6.1.1.3.2.2">𝒙</ci><ci id="S2.SS1.p3.9.m9.2.2.1.1.cmml" xref="S2.SS1.p3.9.m9.2.2.1.1">𝑡</ci></apply><apply id="S2.SS1.p3.9.m9.5.5.cmml" xref="S2.SS1.p3.9.m9.5.5"><divide id="S2.SS1.p3.9.m9.5.5.4.cmml" xref="S2.SS1.p3.9.m9.5.5"></divide><apply id="S2.SS1.p3.9.m9.4.4.2.cmml" xref="S2.SS1.p3.9.m9.4.4.2"><apply id="S2.SS1.p3.9.m9.4.4.2.3.cmml" xref="S2.SS1.p3.9.m9.4.4.2.3"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.4.4.2.3.1.cmml" xref="S2.SS1.p3.9.m9.4.4.2.3">subscript</csymbol><sum id="S2.SS1.p3.9.m9.4.4.2.3.2.cmml" xref="S2.SS1.p3.9.m9.4.4.2.3.2"></sum><apply id="S2.SS1.p3.9.m9.3.3.1.1.1.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1"><in id="S2.SS1.p3.9.m9.3.3.1.1.1.2.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1.2"></in><ci id="S2.SS1.p3.9.m9.3.3.1.1.1.3.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1.3">𝑖</ci><apply id="S2.SS1.p3.9.m9.3.3.1.1.1.4.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.3.3.1.1.1.4.1.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4">superscript</csymbol><ci id="S2.SS1.p3.9.m9.3.3.1.1.1.4.2.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1.4.2">𝒮</ci><ci id="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.9.m9.3.3.1.1.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="S2.SS1.p3.9.m9.4.4.2.4.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4"><times id="S2.SS1.p3.9.m9.4.4.2.4.1.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.1"></times><apply id="S2.SS1.p3.9.m9.4.4.2.4.2.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.2"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.4.4.2.4.2.1.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.2">subscript</csymbol><ci id="S2.SS1.p3.9.m9.4.4.2.4.2.2.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.2.2">𝑝</ci><ci id="S2.SS1.p3.9.m9.4.4.2.4.2.3.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.2.3">𝑖</ci></apply><apply id="S2.SS1.p3.9.m9.4.4.2.4.3.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.3"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.4.4.2.4.3.1.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.3">superscript</csymbol><apply id="S2.SS1.p3.9.m9.4.4.2.4.3.2.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.3"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.4.4.2.4.3.2.1.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.3">subscript</csymbol><ci id="S2.SS1.p3.9.m9.4.4.2.4.3.2.2.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.3.2.2">Δ</ci><ci id="S2.SS1.p3.9.m9.4.4.2.4.3.2.3.cmml" xref="S2.SS1.p3.9.m9.4.4.2.4.3.2.3">𝑖</ci></apply><ci id="S2.SS1.p3.9.m9.4.4.2.2.1.1.cmml" xref="S2.SS1.p3.9.m9.4.4.2.2.1.1">𝑡</ci></apply></apply></apply><apply id="S2.SS1.p3.9.m9.5.5.3.cmml" xref="S2.SS1.p3.9.m9.5.5.3"><apply id="S2.SS1.p3.9.m9.5.5.3.2.cmml" xref="S2.SS1.p3.9.m9.5.5.3.2"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.5.5.3.2.1.cmml" xref="S2.SS1.p3.9.m9.5.5.3.2">subscript</csymbol><sum id="S2.SS1.p3.9.m9.5.5.3.2.2.cmml" xref="S2.SS1.p3.9.m9.5.5.3.2.2"></sum><apply id="S2.SS1.p3.9.m9.5.5.3.1.1.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1"><in id="S2.SS1.p3.9.m9.5.5.3.1.1.2.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1.2"></in><ci id="S2.SS1.p3.9.m9.5.5.3.1.1.3.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1.3">𝑖</ci><apply id="S2.SS1.p3.9.m9.5.5.3.1.1.4.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.5.5.3.1.1.4.1.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4">superscript</csymbol><ci id="S2.SS1.p3.9.m9.5.5.3.1.1.4.2.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1.4.2">𝒮</ci><ci id="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.1.cmml" xref="S2.SS1.p3.9.m9.5.5.3.1.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="S2.SS1.p3.9.m9.5.5.3.3.cmml" xref="S2.SS1.p3.9.m9.5.5.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.9.m9.5.5.3.3.1.cmml" xref="S2.SS1.p3.9.m9.5.5.3.3">subscript</csymbol><ci id="S2.SS1.p3.9.m9.5.5.3.3.2.cmml" xref="S2.SS1.p3.9.m9.5.5.3.3.2">𝑝</ci><ci id="S2.SS1.p3.9.m9.5.5.3.3.3.cmml" xref="S2.SS1.p3.9.m9.5.5.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m9.6c">\bm{x}^{(t+1)}=\bm{x}^{(t)}+\frac{\sum_{i\in\mathcal{S}^{(t)}}p_{i}\Delta_{i}^{(t)}}{\sum_{i\in\mathcal{S}^{(t)}}p_{i}}.</annotation></semantics></math>
where <math id="S2.SS1.p3.10.m10.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S2.SS1.p3.10.m10.1a"><msub id="S2.SS1.p3.10.m10.1.1" xref="S2.SS1.p3.10.m10.1.1.cmml"><mi id="S2.SS1.p3.10.m10.1.1.2" xref="S2.SS1.p3.10.m10.1.1.2.cmml">p</mi><mi id="S2.SS1.p3.10.m10.1.1.3" xref="S2.SS1.p3.10.m10.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.10.m10.1b"><apply id="S2.SS1.p3.10.m10.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.10.m10.1.1.1.cmml" xref="S2.SS1.p3.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p3.10.m10.1.1.2.cmml" xref="S2.SS1.p3.10.m10.1.1.2">𝑝</ci><ci id="S2.SS1.p3.10.m10.1.1.3.cmml" xref="S2.SS1.p3.10.m10.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.10.m10.1c">p_{i}</annotation></semantics></math> is the relative weight of client <math id="S2.SS1.p3.11.m11.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p3.11.m11.1a"><mi id="S2.SS1.p3.11.m11.1.1" xref="S2.SS1.p3.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.11.m11.1b"><ci id="S2.SS1.p3.11.m11.1.1.cmml" xref="S2.SS1.p3.11.m11.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.11.m11.1c">i</annotation></semantics></math>. The above procedure will repeat until the algorithm converges. In the <em id="S2.SS1.p3.12.3" class="ltx_emph ltx_font_italic">cross-silo</em> setting where all clients participate in training on every round (each cohort is the entire population), we have <math id="S2.SS1.p3.12.m12.5" class="ltx_Math" alttext="\mathcal{S}^{(t)}=\{1,2,\dots,M\}" display="inline"><semantics id="S2.SS1.p3.12.m12.5a"><mrow id="S2.SS1.p3.12.m12.5.6" xref="S2.SS1.p3.12.m12.5.6.cmml"><msup id="S2.SS1.p3.12.m12.5.6.2" xref="S2.SS1.p3.12.m12.5.6.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.12.m12.5.6.2.2" xref="S2.SS1.p3.12.m12.5.6.2.2.cmml">𝒮</mi><mrow id="S2.SS1.p3.12.m12.1.1.1.3" xref="S2.SS1.p3.12.m12.5.6.2.cmml"><mo stretchy="false" id="S2.SS1.p3.12.m12.1.1.1.3.1" xref="S2.SS1.p3.12.m12.5.6.2.cmml">(</mo><mi id="S2.SS1.p3.12.m12.1.1.1.1" xref="S2.SS1.p3.12.m12.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p3.12.m12.1.1.1.3.2" xref="S2.SS1.p3.12.m12.5.6.2.cmml">)</mo></mrow></msup><mo id="S2.SS1.p3.12.m12.5.6.1" xref="S2.SS1.p3.12.m12.5.6.1.cmml">=</mo><mrow id="S2.SS1.p3.12.m12.5.6.3.2" xref="S2.SS1.p3.12.m12.5.6.3.1.cmml"><mo stretchy="false" id="S2.SS1.p3.12.m12.5.6.3.2.1" xref="S2.SS1.p3.12.m12.5.6.3.1.cmml">{</mo><mn id="S2.SS1.p3.12.m12.2.2" xref="S2.SS1.p3.12.m12.2.2.cmml">1</mn><mo id="S2.SS1.p3.12.m12.5.6.3.2.2" xref="S2.SS1.p3.12.m12.5.6.3.1.cmml">,</mo><mn id="S2.SS1.p3.12.m12.3.3" xref="S2.SS1.p3.12.m12.3.3.cmml">2</mn><mo id="S2.SS1.p3.12.m12.5.6.3.2.3" xref="S2.SS1.p3.12.m12.5.6.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p3.12.m12.4.4" xref="S2.SS1.p3.12.m12.4.4.cmml">…</mi><mo id="S2.SS1.p3.12.m12.5.6.3.2.4" xref="S2.SS1.p3.12.m12.5.6.3.1.cmml">,</mo><mi id="S2.SS1.p3.12.m12.5.5" xref="S2.SS1.p3.12.m12.5.5.cmml">M</mi><mo stretchy="false" id="S2.SS1.p3.12.m12.5.6.3.2.5" xref="S2.SS1.p3.12.m12.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.12.m12.5b"><apply id="S2.SS1.p3.12.m12.5.6.cmml" xref="S2.SS1.p3.12.m12.5.6"><eq id="S2.SS1.p3.12.m12.5.6.1.cmml" xref="S2.SS1.p3.12.m12.5.6.1"></eq><apply id="S2.SS1.p3.12.m12.5.6.2.cmml" xref="S2.SS1.p3.12.m12.5.6.2"><csymbol cd="ambiguous" id="S2.SS1.p3.12.m12.5.6.2.1.cmml" xref="S2.SS1.p3.12.m12.5.6.2">superscript</csymbol><ci id="S2.SS1.p3.12.m12.5.6.2.2.cmml" xref="S2.SS1.p3.12.m12.5.6.2.2">𝒮</ci><ci id="S2.SS1.p3.12.m12.1.1.1.1.cmml" xref="S2.SS1.p3.12.m12.1.1.1.1">𝑡</ci></apply><set id="S2.SS1.p3.12.m12.5.6.3.1.cmml" xref="S2.SS1.p3.12.m12.5.6.3.2"><cn type="integer" id="S2.SS1.p3.12.m12.2.2.cmml" xref="S2.SS1.p3.12.m12.2.2">1</cn><cn type="integer" id="S2.SS1.p3.12.m12.3.3.cmml" xref="S2.SS1.p3.12.m12.3.3">2</cn><ci id="S2.SS1.p3.12.m12.4.4.cmml" xref="S2.SS1.p3.12.m12.4.4">…</ci><ci id="S2.SS1.p3.12.m12.5.5.cmml" xref="S2.SS1.p3.12.m12.5.5">𝑀</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.12.m12.5c">\mathcal{S}^{(t)}=\{1,2,\dots,M\}</annotation></semantics></math>.
Consequently,
we can learn a global model to benefit all clients while preserving their data privacy.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Our Unified Framework for FL</h3>

<figure id="algorithm1" class="ltx_float ltx_algorithm">
<div id="algorithm1.11" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm1.1.1" class="ltx_listingline">

<span id="algorithm1.1.1.1" class="ltx_text"><span id="algorithm1.1.1.1.1" class="ltx_text ltx_font_bold">Input:</span> </span>Initial model <math id="algorithm1.1.1.m1.1" class="ltx_Math" alttext="\bm{x}^{(0)}" display="inline"><semantics id="algorithm1.1.1.m1.1a"><msup id="algorithm1.1.1.m1.1.2" xref="algorithm1.1.1.m1.1.2.cmml"><mi id="algorithm1.1.1.m1.1.2.2" xref="algorithm1.1.1.m1.1.2.2.cmml">𝒙</mi><mrow id="algorithm1.1.1.m1.1.1.1.3" xref="algorithm1.1.1.m1.1.2.cmml"><mo stretchy="false" id="algorithm1.1.1.m1.1.1.1.3.1" xref="algorithm1.1.1.m1.1.2.cmml">(</mo><mn id="algorithm1.1.1.m1.1.1.1.1" xref="algorithm1.1.1.m1.1.1.1.1.cmml">0</mn><mo stretchy="false" id="algorithm1.1.1.m1.1.1.1.3.2" xref="algorithm1.1.1.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="algorithm1.1.1.m1.1b"><apply id="algorithm1.1.1.m1.1.2.cmml" xref="algorithm1.1.1.m1.1.2"><csymbol cd="ambiguous" id="algorithm1.1.1.m1.1.2.1.cmml" xref="algorithm1.1.1.m1.1.2">superscript</csymbol><ci id="algorithm1.1.1.m1.1.2.2.cmml" xref="algorithm1.1.1.m1.1.2.2">𝒙</ci><cn type="integer" id="algorithm1.1.1.m1.1.1.1.1.cmml" xref="algorithm1.1.1.m1.1.1.1.1">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.1.1.m1.1c">\bm{x}^{(0)}</annotation></semantics></math>, <span id="algorithm1.1.1.2" class="ltx_text ltx_font_smallcaps">ClientOpt</span>, <span id="algorithm1.1.1.3" class="ltx_text ltx_font_smallcaps">ServerOpt</span>
</div>
<div id="algorithm1.2.2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">1</span>
<span id="algorithm1.2.2.2" class="ltx_text ltx_font_bold">for</span> <em id="algorithm1.2.2.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.2.2.1.m1.4" class="ltx_Math" alttext="t\in\{0,1,\dots,T-1\}" display="inline"><semantics id="algorithm1.2.2.1.m1.4a"><mrow id="algorithm1.2.2.1.m1.4.4" xref="algorithm1.2.2.1.m1.4.4.cmml"><mi id="algorithm1.2.2.1.m1.4.4.3" xref="algorithm1.2.2.1.m1.4.4.3.cmml">t</mi><mo id="algorithm1.2.2.1.m1.4.4.2" xref="algorithm1.2.2.1.m1.4.4.2.cmml">∈</mo><mrow id="algorithm1.2.2.1.m1.4.4.1.1" xref="algorithm1.2.2.1.m1.4.4.1.2.cmml"><mo stretchy="false" id="algorithm1.2.2.1.m1.4.4.1.1.2" xref="algorithm1.2.2.1.m1.4.4.1.2.cmml">{</mo><mn id="algorithm1.2.2.1.m1.1.1" xref="algorithm1.2.2.1.m1.1.1.cmml">0</mn><mo id="algorithm1.2.2.1.m1.4.4.1.1.3" xref="algorithm1.2.2.1.m1.4.4.1.2.cmml">,</mo><mn id="algorithm1.2.2.1.m1.2.2" xref="algorithm1.2.2.1.m1.2.2.cmml">1</mn><mo id="algorithm1.2.2.1.m1.4.4.1.1.4" xref="algorithm1.2.2.1.m1.4.4.1.2.cmml">,</mo><mi mathvariant="normal" id="algorithm1.2.2.1.m1.3.3" xref="algorithm1.2.2.1.m1.3.3.cmml">…</mi><mo id="algorithm1.2.2.1.m1.4.4.1.1.5" xref="algorithm1.2.2.1.m1.4.4.1.2.cmml">,</mo><mrow id="algorithm1.2.2.1.m1.4.4.1.1.1" xref="algorithm1.2.2.1.m1.4.4.1.1.1.cmml"><mi id="algorithm1.2.2.1.m1.4.4.1.1.1.2" xref="algorithm1.2.2.1.m1.4.4.1.1.1.2.cmml">T</mi><mo id="algorithm1.2.2.1.m1.4.4.1.1.1.1" xref="algorithm1.2.2.1.m1.4.4.1.1.1.1.cmml">−</mo><mn id="algorithm1.2.2.1.m1.4.4.1.1.1.3" xref="algorithm1.2.2.1.m1.4.4.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="algorithm1.2.2.1.m1.4.4.1.1.6" xref="algorithm1.2.2.1.m1.4.4.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.2.2.1.m1.4b"><apply id="algorithm1.2.2.1.m1.4.4.cmml" xref="algorithm1.2.2.1.m1.4.4"><in id="algorithm1.2.2.1.m1.4.4.2.cmml" xref="algorithm1.2.2.1.m1.4.4.2"></in><ci id="algorithm1.2.2.1.m1.4.4.3.cmml" xref="algorithm1.2.2.1.m1.4.4.3">𝑡</ci><set id="algorithm1.2.2.1.m1.4.4.1.2.cmml" xref="algorithm1.2.2.1.m1.4.4.1.1"><cn type="integer" id="algorithm1.2.2.1.m1.1.1.cmml" xref="algorithm1.2.2.1.m1.1.1">0</cn><cn type="integer" id="algorithm1.2.2.1.m1.2.2.cmml" xref="algorithm1.2.2.1.m1.2.2">1</cn><ci id="algorithm1.2.2.1.m1.3.3.cmml" xref="algorithm1.2.2.1.m1.3.3">…</ci><apply id="algorithm1.2.2.1.m1.4.4.1.1.1.cmml" xref="algorithm1.2.2.1.m1.4.4.1.1.1"><minus id="algorithm1.2.2.1.m1.4.4.1.1.1.1.cmml" xref="algorithm1.2.2.1.m1.4.4.1.1.1.1"></minus><ci id="algorithm1.2.2.1.m1.4.4.1.1.1.2.cmml" xref="algorithm1.2.2.1.m1.4.4.1.1.1.2">𝑇</ci><cn type="integer" id="algorithm1.2.2.1.m1.4.4.1.1.1.3.cmml" xref="algorithm1.2.2.1.m1.4.4.1.1.1.3">1</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.2.2.1.m1.4c">t\in\{0,1,\dots,T-1\}</annotation></semantics></math> </em> <span id="algorithm1.2.2.3" class="ltx_text ltx_font_bold">do</span>
</div>
<div id="algorithm1.3.3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">2</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Sample a subset <math id="algorithm1.3.3.m1.1" class="ltx_Math" alttext="\mathcal{S}^{(t)}" display="inline"><semantics id="algorithm1.3.3.m1.1a"><msup id="algorithm1.3.3.m1.1.2" xref="algorithm1.3.3.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.3.3.m1.1.2.2" xref="algorithm1.3.3.m1.1.2.2.cmml">𝒮</mi><mrow id="algorithm1.3.3.m1.1.1.1.3" xref="algorithm1.3.3.m1.1.2.cmml"><mo stretchy="false" id="algorithm1.3.3.m1.1.1.1.3.1" xref="algorithm1.3.3.m1.1.2.cmml">(</mo><mi id="algorithm1.3.3.m1.1.1.1.1" xref="algorithm1.3.3.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.3.3.m1.1.1.1.3.2" xref="algorithm1.3.3.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="algorithm1.3.3.m1.1b"><apply id="algorithm1.3.3.m1.1.2.cmml" xref="algorithm1.3.3.m1.1.2"><csymbol cd="ambiguous" id="algorithm1.3.3.m1.1.2.1.cmml" xref="algorithm1.3.3.m1.1.2">superscript</csymbol><ci id="algorithm1.3.3.m1.1.2.2.cmml" xref="algorithm1.3.3.m1.1.2.2">𝒮</ci><ci id="algorithm1.3.3.m1.1.1.1.1.cmml" xref="algorithm1.3.3.m1.1.1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.3.3.m1.1c">\mathcal{S}^{(t)}</annotation></semantics></math> of clients
</div>
<div id="algorithm1.11.12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">3</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.4.4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">4</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   <span id="algorithm1.4.4.2" class="ltx_text ltx_font_bold">for</span> <em id="algorithm1.4.4.1" class="ltx_emph ltx_font_italic"><span id="algorithm1.4.4.1.1" class="ltx_text ltx_font_bold ltx_font_upright">client</span> <math id="algorithm1.4.4.1.m1.1" class="ltx_Math" alttext="i\in\mathcal{S}^{(t)}" display="inline"><semantics id="algorithm1.4.4.1.m1.1a"><mrow id="algorithm1.4.4.1.m1.1.2" xref="algorithm1.4.4.1.m1.1.2.cmml"><mi id="algorithm1.4.4.1.m1.1.2.2" xref="algorithm1.4.4.1.m1.1.2.2.cmml">i</mi><mo id="algorithm1.4.4.1.m1.1.2.1" xref="algorithm1.4.4.1.m1.1.2.1.cmml">∈</mo><msup id="algorithm1.4.4.1.m1.1.2.3" xref="algorithm1.4.4.1.m1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.4.4.1.m1.1.2.3.2" xref="algorithm1.4.4.1.m1.1.2.3.2.cmml">𝒮</mi><mrow id="algorithm1.4.4.1.m1.1.1.1.3" xref="algorithm1.4.4.1.m1.1.2.3.cmml"><mo stretchy="false" id="algorithm1.4.4.1.m1.1.1.1.3.1" xref="algorithm1.4.4.1.m1.1.2.3.cmml">(</mo><mi id="algorithm1.4.4.1.m1.1.1.1.1" xref="algorithm1.4.4.1.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.4.4.1.m1.1.1.1.3.2" xref="algorithm1.4.4.1.m1.1.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.4.4.1.m1.1b"><apply id="algorithm1.4.4.1.m1.1.2.cmml" xref="algorithm1.4.4.1.m1.1.2"><in id="algorithm1.4.4.1.m1.1.2.1.cmml" xref="algorithm1.4.4.1.m1.1.2.1"></in><ci id="algorithm1.4.4.1.m1.1.2.2.cmml" xref="algorithm1.4.4.1.m1.1.2.2">𝑖</ci><apply id="algorithm1.4.4.1.m1.1.2.3.cmml" xref="algorithm1.4.4.1.m1.1.2.3"><csymbol cd="ambiguous" id="algorithm1.4.4.1.m1.1.2.3.1.cmml" xref="algorithm1.4.4.1.m1.1.2.3">superscript</csymbol><ci id="algorithm1.4.4.1.m1.1.2.3.2.cmml" xref="algorithm1.4.4.1.m1.1.2.3.2">𝒮</ci><ci id="algorithm1.4.4.1.m1.1.1.1.1.cmml" xref="algorithm1.4.4.1.m1.1.1.1.1">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.4.4.1.m1.1c">i\in\mathcal{S}^{(t)}</annotation></semantics></math> <span id="algorithm1.4.4.1.2" class="ltx_text ltx_font_bold ltx_font_upright">in parallel</span></em> <span id="algorithm1.4.4.3" class="ltx_text ltx_font_bold">do</span>
</div>
<div id="algorithm1.5.5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">5</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Initialize local model <math id="algorithm1.5.5.m1.3" class="ltx_Math" alttext="\bm{x}_{i}^{(t,0)}=\bm{x}^{(t)}" display="inline"><semantics id="algorithm1.5.5.m1.3a"><mrow id="algorithm1.5.5.m1.3.4" xref="algorithm1.5.5.m1.3.4.cmml"><msubsup id="algorithm1.5.5.m1.3.4.2" xref="algorithm1.5.5.m1.3.4.2.cmml"><mi id="algorithm1.5.5.m1.3.4.2.2.2" xref="algorithm1.5.5.m1.3.4.2.2.2.cmml">𝒙</mi><mi id="algorithm1.5.5.m1.3.4.2.2.3" xref="algorithm1.5.5.m1.3.4.2.2.3.cmml">i</mi><mrow id="algorithm1.5.5.m1.2.2.2.4" xref="algorithm1.5.5.m1.2.2.2.3.cmml"><mo stretchy="false" id="algorithm1.5.5.m1.2.2.2.4.1" xref="algorithm1.5.5.m1.2.2.2.3.cmml">(</mo><mi id="algorithm1.5.5.m1.1.1.1.1" xref="algorithm1.5.5.m1.1.1.1.1.cmml">t</mi><mo id="algorithm1.5.5.m1.2.2.2.4.2" xref="algorithm1.5.5.m1.2.2.2.3.cmml">,</mo><mn id="algorithm1.5.5.m1.2.2.2.2" xref="algorithm1.5.5.m1.2.2.2.2.cmml">0</mn><mo stretchy="false" id="algorithm1.5.5.m1.2.2.2.4.3" xref="algorithm1.5.5.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><mo id="algorithm1.5.5.m1.3.4.1" xref="algorithm1.5.5.m1.3.4.1.cmml">=</mo><msup id="algorithm1.5.5.m1.3.4.3" xref="algorithm1.5.5.m1.3.4.3.cmml"><mi id="algorithm1.5.5.m1.3.4.3.2" xref="algorithm1.5.5.m1.3.4.3.2.cmml">𝒙</mi><mrow id="algorithm1.5.5.m1.3.3.1.3" xref="algorithm1.5.5.m1.3.4.3.cmml"><mo stretchy="false" id="algorithm1.5.5.m1.3.3.1.3.1" xref="algorithm1.5.5.m1.3.4.3.cmml">(</mo><mi id="algorithm1.5.5.m1.3.3.1.1" xref="algorithm1.5.5.m1.3.3.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.5.5.m1.3.3.1.3.2" xref="algorithm1.5.5.m1.3.4.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.5.5.m1.3b"><apply id="algorithm1.5.5.m1.3.4.cmml" xref="algorithm1.5.5.m1.3.4"><eq id="algorithm1.5.5.m1.3.4.1.cmml" xref="algorithm1.5.5.m1.3.4.1"></eq><apply id="algorithm1.5.5.m1.3.4.2.cmml" xref="algorithm1.5.5.m1.3.4.2"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.3.4.2.1.cmml" xref="algorithm1.5.5.m1.3.4.2">superscript</csymbol><apply id="algorithm1.5.5.m1.3.4.2.2.cmml" xref="algorithm1.5.5.m1.3.4.2"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.3.4.2.2.1.cmml" xref="algorithm1.5.5.m1.3.4.2">subscript</csymbol><ci id="algorithm1.5.5.m1.3.4.2.2.2.cmml" xref="algorithm1.5.5.m1.3.4.2.2.2">𝒙</ci><ci id="algorithm1.5.5.m1.3.4.2.2.3.cmml" xref="algorithm1.5.5.m1.3.4.2.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.5.5.m1.2.2.2.3.cmml" xref="algorithm1.5.5.m1.2.2.2.4"><ci id="algorithm1.5.5.m1.1.1.1.1.cmml" xref="algorithm1.5.5.m1.1.1.1.1">𝑡</ci><cn type="integer" id="algorithm1.5.5.m1.2.2.2.2.cmml" xref="algorithm1.5.5.m1.2.2.2.2">0</cn></interval></apply><apply id="algorithm1.5.5.m1.3.4.3.cmml" xref="algorithm1.5.5.m1.3.4.3"><csymbol cd="ambiguous" id="algorithm1.5.5.m1.3.4.3.1.cmml" xref="algorithm1.5.5.m1.3.4.3">superscript</csymbol><ci id="algorithm1.5.5.m1.3.4.3.2.cmml" xref="algorithm1.5.5.m1.3.4.3.2">𝒙</ci><ci id="algorithm1.5.5.m1.3.3.1.1.cmml" xref="algorithm1.5.5.m1.3.3.1.1">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.5.5.m1.3c">\bm{x}_{i}^{(t,0)}=\bm{x}^{(t)}</annotation></semantics></math>
</div>
<div id="algorithm1.6.6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">6</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
<span id="algorithm1.6.6.2" class="ltx_text ltx_font_bold">for</span> <em id="algorithm1.6.6.1" class="ltx_emph ltx_font_italic"><math id="algorithm1.6.6.1.m1.3" class="ltx_Math" alttext="k=0,\dots,\tau_{i}-1" display="inline"><semantics id="algorithm1.6.6.1.m1.3a"><mrow id="algorithm1.6.6.1.m1.3.3" xref="algorithm1.6.6.1.m1.3.3.cmml"><mi id="algorithm1.6.6.1.m1.3.3.3" xref="algorithm1.6.6.1.m1.3.3.3.cmml">k</mi><mo id="algorithm1.6.6.1.m1.3.3.2" xref="algorithm1.6.6.1.m1.3.3.2.cmml">=</mo><mrow id="algorithm1.6.6.1.m1.3.3.1.1" xref="algorithm1.6.6.1.m1.3.3.1.2.cmml"><mn id="algorithm1.6.6.1.m1.1.1" xref="algorithm1.6.6.1.m1.1.1.cmml">0</mn><mo id="algorithm1.6.6.1.m1.3.3.1.1.2" xref="algorithm1.6.6.1.m1.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="algorithm1.6.6.1.m1.2.2" xref="algorithm1.6.6.1.m1.2.2.cmml">…</mi><mo id="algorithm1.6.6.1.m1.3.3.1.1.3" xref="algorithm1.6.6.1.m1.3.3.1.2.cmml">,</mo><mrow id="algorithm1.6.6.1.m1.3.3.1.1.1" xref="algorithm1.6.6.1.m1.3.3.1.1.1.cmml"><msub id="algorithm1.6.6.1.m1.3.3.1.1.1.2" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2.cmml"><mi id="algorithm1.6.6.1.m1.3.3.1.1.1.2.2" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2.2.cmml">τ</mi><mi id="algorithm1.6.6.1.m1.3.3.1.1.1.2.3" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2.3.cmml">i</mi></msub><mo id="algorithm1.6.6.1.m1.3.3.1.1.1.1" xref="algorithm1.6.6.1.m1.3.3.1.1.1.1.cmml">−</mo><mn id="algorithm1.6.6.1.m1.3.3.1.1.1.3" xref="algorithm1.6.6.1.m1.3.3.1.1.1.3.cmml">1</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.6.6.1.m1.3b"><apply id="algorithm1.6.6.1.m1.3.3.cmml" xref="algorithm1.6.6.1.m1.3.3"><eq id="algorithm1.6.6.1.m1.3.3.2.cmml" xref="algorithm1.6.6.1.m1.3.3.2"></eq><ci id="algorithm1.6.6.1.m1.3.3.3.cmml" xref="algorithm1.6.6.1.m1.3.3.3">𝑘</ci><list id="algorithm1.6.6.1.m1.3.3.1.2.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1"><cn type="integer" id="algorithm1.6.6.1.m1.1.1.cmml" xref="algorithm1.6.6.1.m1.1.1">0</cn><ci id="algorithm1.6.6.1.m1.2.2.cmml" xref="algorithm1.6.6.1.m1.2.2">…</ci><apply id="algorithm1.6.6.1.m1.3.3.1.1.1.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1"><minus id="algorithm1.6.6.1.m1.3.3.1.1.1.1.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1.1"></minus><apply id="algorithm1.6.6.1.m1.3.3.1.1.1.2.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="algorithm1.6.6.1.m1.3.3.1.1.1.2.1.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2">subscript</csymbol><ci id="algorithm1.6.6.1.m1.3.3.1.1.1.2.2.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2.2">𝜏</ci><ci id="algorithm1.6.6.1.m1.3.3.1.1.1.2.3.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1.2.3">𝑖</ci></apply><cn type="integer" id="algorithm1.6.6.1.m1.3.3.1.1.1.3.cmml" xref="algorithm1.6.6.1.m1.3.3.1.1.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.6.6.1.m1.3c">k=0,\dots,\tau_{i}-1</annotation></semantics></math></em> <span id="algorithm1.6.6.3" class="ltx_text ltx_font_bold">do</span>
</div>
<div id="algorithm1.7.7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">7</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Compute local stochastic gradient <math id="algorithm1.7.7.m1.3" class="ltx_Math" alttext="g_{i}(\bm{x}_{i}^{(t,k)})" display="inline"><semantics id="algorithm1.7.7.m1.3a"><mrow id="algorithm1.7.7.m1.3.3" xref="algorithm1.7.7.m1.3.3.cmml"><msub id="algorithm1.7.7.m1.3.3.3" xref="algorithm1.7.7.m1.3.3.3.cmml"><mi id="algorithm1.7.7.m1.3.3.3.2" xref="algorithm1.7.7.m1.3.3.3.2.cmml">g</mi><mi id="algorithm1.7.7.m1.3.3.3.3" xref="algorithm1.7.7.m1.3.3.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="algorithm1.7.7.m1.3.3.2" xref="algorithm1.7.7.m1.3.3.2.cmml">​</mo><mrow id="algorithm1.7.7.m1.3.3.1.1" xref="algorithm1.7.7.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="algorithm1.7.7.m1.3.3.1.1.2" xref="algorithm1.7.7.m1.3.3.1.1.1.cmml">(</mo><msubsup id="algorithm1.7.7.m1.3.3.1.1.1" xref="algorithm1.7.7.m1.3.3.1.1.1.cmml"><mi id="algorithm1.7.7.m1.3.3.1.1.1.2.2" xref="algorithm1.7.7.m1.3.3.1.1.1.2.2.cmml">𝒙</mi><mi id="algorithm1.7.7.m1.3.3.1.1.1.2.3" xref="algorithm1.7.7.m1.3.3.1.1.1.2.3.cmml">i</mi><mrow id="algorithm1.7.7.m1.2.2.2.4" xref="algorithm1.7.7.m1.2.2.2.3.cmml"><mo stretchy="false" id="algorithm1.7.7.m1.2.2.2.4.1" xref="algorithm1.7.7.m1.2.2.2.3.cmml">(</mo><mi id="algorithm1.7.7.m1.1.1.1.1" xref="algorithm1.7.7.m1.1.1.1.1.cmml">t</mi><mo id="algorithm1.7.7.m1.2.2.2.4.2" xref="algorithm1.7.7.m1.2.2.2.3.cmml">,</mo><mi id="algorithm1.7.7.m1.2.2.2.2" xref="algorithm1.7.7.m1.2.2.2.2.cmml">k</mi><mo stretchy="false" id="algorithm1.7.7.m1.2.2.2.4.3" xref="algorithm1.7.7.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="algorithm1.7.7.m1.3.3.1.1.3" xref="algorithm1.7.7.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.7.7.m1.3b"><apply id="algorithm1.7.7.m1.3.3.cmml" xref="algorithm1.7.7.m1.3.3"><times id="algorithm1.7.7.m1.3.3.2.cmml" xref="algorithm1.7.7.m1.3.3.2"></times><apply id="algorithm1.7.7.m1.3.3.3.cmml" xref="algorithm1.7.7.m1.3.3.3"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.3.3.3.1.cmml" xref="algorithm1.7.7.m1.3.3.3">subscript</csymbol><ci id="algorithm1.7.7.m1.3.3.3.2.cmml" xref="algorithm1.7.7.m1.3.3.3.2">𝑔</ci><ci id="algorithm1.7.7.m1.3.3.3.3.cmml" xref="algorithm1.7.7.m1.3.3.3.3">𝑖</ci></apply><apply id="algorithm1.7.7.m1.3.3.1.1.1.cmml" xref="algorithm1.7.7.m1.3.3.1.1"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.3.3.1.1.1.1.cmml" xref="algorithm1.7.7.m1.3.3.1.1">superscript</csymbol><apply id="algorithm1.7.7.m1.3.3.1.1.1.2.cmml" xref="algorithm1.7.7.m1.3.3.1.1"><csymbol cd="ambiguous" id="algorithm1.7.7.m1.3.3.1.1.1.2.1.cmml" xref="algorithm1.7.7.m1.3.3.1.1">subscript</csymbol><ci id="algorithm1.7.7.m1.3.3.1.1.1.2.2.cmml" xref="algorithm1.7.7.m1.3.3.1.1.1.2.2">𝒙</ci><ci id="algorithm1.7.7.m1.3.3.1.1.1.2.3.cmml" xref="algorithm1.7.7.m1.3.3.1.1.1.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.7.7.m1.2.2.2.3.cmml" xref="algorithm1.7.7.m1.2.2.2.4"><ci id="algorithm1.7.7.m1.1.1.1.1.cmml" xref="algorithm1.7.7.m1.1.1.1.1">𝑡</ci><ci id="algorithm1.7.7.m1.2.2.2.2.cmml" xref="algorithm1.7.7.m1.2.2.2.2">𝑘</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.7.7.m1.3c">g_{i}(\bm{x}_{i}^{(t,k)})</annotation></semantics></math>
</div>
<div id="algorithm1.8.8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">8</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Perform local update <math id="algorithm1.8.8.m1.10" class="ltx_Math" alttext="\bm{x}_{i}^{(t,k+1)}=\hbox{\pagecolor{blue!18}{ClientOpt}}(\bm{x}_{i}^{(t,k)},g_{i}(\bm{x}_{i}^{(t,k)}),\eta,t)" display="inline"><semantics id="algorithm1.8.8.m1.10a"><mrow id="algorithm1.8.8.m1.10.10" xref="algorithm1.8.8.m1.10.10.cmml"><msubsup id="algorithm1.8.8.m1.10.10.4" xref="algorithm1.8.8.m1.10.10.4.cmml"><mi id="algorithm1.8.8.m1.10.10.4.2.2" xref="algorithm1.8.8.m1.10.10.4.2.2.cmml">𝒙</mi><mi id="algorithm1.8.8.m1.10.10.4.2.3" xref="algorithm1.8.8.m1.10.10.4.2.3.cmml">i</mi><mrow id="algorithm1.8.8.m1.2.2.2.2" xref="algorithm1.8.8.m1.2.2.2.3.cmml"><mo stretchy="false" id="algorithm1.8.8.m1.2.2.2.2.2" xref="algorithm1.8.8.m1.2.2.2.3.cmml">(</mo><mi id="algorithm1.8.8.m1.1.1.1.1" xref="algorithm1.8.8.m1.1.1.1.1.cmml">t</mi><mo id="algorithm1.8.8.m1.2.2.2.2.3" xref="algorithm1.8.8.m1.2.2.2.3.cmml">,</mo><mrow id="algorithm1.8.8.m1.2.2.2.2.1" xref="algorithm1.8.8.m1.2.2.2.2.1.cmml"><mi id="algorithm1.8.8.m1.2.2.2.2.1.2" xref="algorithm1.8.8.m1.2.2.2.2.1.2.cmml">k</mi><mo id="algorithm1.8.8.m1.2.2.2.2.1.1" xref="algorithm1.8.8.m1.2.2.2.2.1.1.cmml">+</mo><mn id="algorithm1.8.8.m1.2.2.2.2.1.3" xref="algorithm1.8.8.m1.2.2.2.2.1.3.cmml">1</mn></mrow><mo stretchy="false" id="algorithm1.8.8.m1.2.2.2.2.4" xref="algorithm1.8.8.m1.2.2.2.3.cmml">)</mo></mrow></msubsup><mo id="algorithm1.8.8.m1.10.10.3" xref="algorithm1.8.8.m1.10.10.3.cmml">=</mo><mrow id="algorithm1.8.8.m1.10.10.2" xref="algorithm1.8.8.m1.10.10.2.cmml"><mtext class="ltx_font_smallcaps" mathbackground="#D1D1FF" id="algorithm1.8.8.m1.10.10.2.4" xref="algorithm1.8.8.m1.10.10.2.4a.cmml">ClientOpt</mtext><mo lspace="0em" rspace="0em" id="algorithm1.8.8.m1.10.10.2.3" xref="algorithm1.8.8.m1.10.10.2.3.cmml">​</mo><mrow id="algorithm1.8.8.m1.10.10.2.2.2" xref="algorithm1.8.8.m1.10.10.2.2.3.cmml"><mo stretchy="false" id="algorithm1.8.8.m1.10.10.2.2.2.3" xref="algorithm1.8.8.m1.10.10.2.2.3.cmml">(</mo><msubsup id="algorithm1.8.8.m1.9.9.1.1.1.1" xref="algorithm1.8.8.m1.9.9.1.1.1.1.cmml"><mi id="algorithm1.8.8.m1.9.9.1.1.1.1.2.2" xref="algorithm1.8.8.m1.9.9.1.1.1.1.2.2.cmml">𝒙</mi><mi id="algorithm1.8.8.m1.9.9.1.1.1.1.2.3" xref="algorithm1.8.8.m1.9.9.1.1.1.1.2.3.cmml">i</mi><mrow id="algorithm1.8.8.m1.4.4.2.4" xref="algorithm1.8.8.m1.4.4.2.3.cmml"><mo stretchy="false" id="algorithm1.8.8.m1.4.4.2.4.1" xref="algorithm1.8.8.m1.4.4.2.3.cmml">(</mo><mi id="algorithm1.8.8.m1.3.3.1.1" xref="algorithm1.8.8.m1.3.3.1.1.cmml">t</mi><mo id="algorithm1.8.8.m1.4.4.2.4.2" xref="algorithm1.8.8.m1.4.4.2.3.cmml">,</mo><mi id="algorithm1.8.8.m1.4.4.2.2" xref="algorithm1.8.8.m1.4.4.2.2.cmml">k</mi><mo stretchy="false" id="algorithm1.8.8.m1.4.4.2.4.3" xref="algorithm1.8.8.m1.4.4.2.3.cmml">)</mo></mrow></msubsup><mo id="algorithm1.8.8.m1.10.10.2.2.2.4" xref="algorithm1.8.8.m1.10.10.2.2.3.cmml">,</mo><mrow id="algorithm1.8.8.m1.10.10.2.2.2.2" xref="algorithm1.8.8.m1.10.10.2.2.2.2.cmml"><msub id="algorithm1.8.8.m1.10.10.2.2.2.2.3" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3.cmml"><mi id="algorithm1.8.8.m1.10.10.2.2.2.2.3.2" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3.2.cmml">g</mi><mi id="algorithm1.8.8.m1.10.10.2.2.2.2.3.3" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="algorithm1.8.8.m1.10.10.2.2.2.2.2" xref="algorithm1.8.8.m1.10.10.2.2.2.2.2.cmml">​</mo><mrow id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.2" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.cmml">(</mo><msubsup id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.cmml"><mi id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.2" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.2.cmml">𝒙</mi><mi id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.3" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.3.cmml">i</mi><mrow id="algorithm1.8.8.m1.6.6.2.4" xref="algorithm1.8.8.m1.6.6.2.3.cmml"><mo stretchy="false" id="algorithm1.8.8.m1.6.6.2.4.1" xref="algorithm1.8.8.m1.6.6.2.3.cmml">(</mo><mi id="algorithm1.8.8.m1.5.5.1.1" xref="algorithm1.8.8.m1.5.5.1.1.cmml">t</mi><mo id="algorithm1.8.8.m1.6.6.2.4.2" xref="algorithm1.8.8.m1.6.6.2.3.cmml">,</mo><mi id="algorithm1.8.8.m1.6.6.2.2" xref="algorithm1.8.8.m1.6.6.2.2.cmml">k</mi><mo stretchy="false" id="algorithm1.8.8.m1.6.6.2.4.3" xref="algorithm1.8.8.m1.6.6.2.3.cmml">)</mo></mrow></msubsup><mo stretchy="false" id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.3" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="algorithm1.8.8.m1.10.10.2.2.2.5" xref="algorithm1.8.8.m1.10.10.2.2.3.cmml">,</mo><mi id="algorithm1.8.8.m1.7.7" xref="algorithm1.8.8.m1.7.7.cmml">η</mi><mo id="algorithm1.8.8.m1.10.10.2.2.2.6" xref="algorithm1.8.8.m1.10.10.2.2.3.cmml">,</mo><mi id="algorithm1.8.8.m1.8.8" xref="algorithm1.8.8.m1.8.8.cmml">t</mi><mo stretchy="false" id="algorithm1.8.8.m1.10.10.2.2.2.7" xref="algorithm1.8.8.m1.10.10.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.8.8.m1.10b"><apply id="algorithm1.8.8.m1.10.10.cmml" xref="algorithm1.8.8.m1.10.10"><eq id="algorithm1.8.8.m1.10.10.3.cmml" xref="algorithm1.8.8.m1.10.10.3"></eq><apply id="algorithm1.8.8.m1.10.10.4.cmml" xref="algorithm1.8.8.m1.10.10.4"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.10.10.4.1.cmml" xref="algorithm1.8.8.m1.10.10.4">superscript</csymbol><apply id="algorithm1.8.8.m1.10.10.4.2.cmml" xref="algorithm1.8.8.m1.10.10.4"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.10.10.4.2.1.cmml" xref="algorithm1.8.8.m1.10.10.4">subscript</csymbol><ci id="algorithm1.8.8.m1.10.10.4.2.2.cmml" xref="algorithm1.8.8.m1.10.10.4.2.2">𝒙</ci><ci id="algorithm1.8.8.m1.10.10.4.2.3.cmml" xref="algorithm1.8.8.m1.10.10.4.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.8.8.m1.2.2.2.3.cmml" xref="algorithm1.8.8.m1.2.2.2.2"><ci id="algorithm1.8.8.m1.1.1.1.1.cmml" xref="algorithm1.8.8.m1.1.1.1.1">𝑡</ci><apply id="algorithm1.8.8.m1.2.2.2.2.1.cmml" xref="algorithm1.8.8.m1.2.2.2.2.1"><plus id="algorithm1.8.8.m1.2.2.2.2.1.1.cmml" xref="algorithm1.8.8.m1.2.2.2.2.1.1"></plus><ci id="algorithm1.8.8.m1.2.2.2.2.1.2.cmml" xref="algorithm1.8.8.m1.2.2.2.2.1.2">𝑘</ci><cn type="integer" id="algorithm1.8.8.m1.2.2.2.2.1.3.cmml" xref="algorithm1.8.8.m1.2.2.2.2.1.3">1</cn></apply></interval></apply><apply id="algorithm1.8.8.m1.10.10.2.cmml" xref="algorithm1.8.8.m1.10.10.2"><times id="algorithm1.8.8.m1.10.10.2.3.cmml" xref="algorithm1.8.8.m1.10.10.2.3"></times><ci id="algorithm1.8.8.m1.10.10.2.4a.cmml" xref="algorithm1.8.8.m1.10.10.2.4"><mtext class="ltx_font_smallcaps" mathbackground="#D1D1FF" id="algorithm1.8.8.m1.10.10.2.4.cmml" xref="algorithm1.8.8.m1.10.10.2.4">ClientOpt</mtext></ci><vector id="algorithm1.8.8.m1.10.10.2.2.3.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2"><apply id="algorithm1.8.8.m1.9.9.1.1.1.1.cmml" xref="algorithm1.8.8.m1.9.9.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.9.9.1.1.1.1.1.cmml" xref="algorithm1.8.8.m1.9.9.1.1.1.1">superscript</csymbol><apply id="algorithm1.8.8.m1.9.9.1.1.1.1.2.cmml" xref="algorithm1.8.8.m1.9.9.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.9.9.1.1.1.1.2.1.cmml" xref="algorithm1.8.8.m1.9.9.1.1.1.1">subscript</csymbol><ci id="algorithm1.8.8.m1.9.9.1.1.1.1.2.2.cmml" xref="algorithm1.8.8.m1.9.9.1.1.1.1.2.2">𝒙</ci><ci id="algorithm1.8.8.m1.9.9.1.1.1.1.2.3.cmml" xref="algorithm1.8.8.m1.9.9.1.1.1.1.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.8.8.m1.4.4.2.3.cmml" xref="algorithm1.8.8.m1.4.4.2.4"><ci id="algorithm1.8.8.m1.3.3.1.1.cmml" xref="algorithm1.8.8.m1.3.3.1.1">𝑡</ci><ci id="algorithm1.8.8.m1.4.4.2.2.cmml" xref="algorithm1.8.8.m1.4.4.2.2">𝑘</ci></interval></apply><apply id="algorithm1.8.8.m1.10.10.2.2.2.2.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2"><times id="algorithm1.8.8.m1.10.10.2.2.2.2.2.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.2"></times><apply id="algorithm1.8.8.m1.10.10.2.2.2.2.3.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.10.10.2.2.2.2.3.1.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3">subscript</csymbol><ci id="algorithm1.8.8.m1.10.10.2.2.2.2.3.2.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3.2">𝑔</ci><ci id="algorithm1.8.8.m1.10.10.2.2.2.2.3.3.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.3.3">𝑖</ci></apply><apply id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.1.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1">superscript</csymbol><apply id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1"><csymbol cd="ambiguous" id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.1.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1">subscript</csymbol><ci id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.2.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.2">𝒙</ci><ci id="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.3.cmml" xref="algorithm1.8.8.m1.10.10.2.2.2.2.1.1.1.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.8.8.m1.6.6.2.3.cmml" xref="algorithm1.8.8.m1.6.6.2.4"><ci id="algorithm1.8.8.m1.5.5.1.1.cmml" xref="algorithm1.8.8.m1.5.5.1.1">𝑡</ci><ci id="algorithm1.8.8.m1.6.6.2.2.cmml" xref="algorithm1.8.8.m1.6.6.2.2">𝑘</ci></interval></apply></apply><ci id="algorithm1.8.8.m1.7.7.cmml" xref="algorithm1.8.8.m1.7.7">𝜂</ci><ci id="algorithm1.8.8.m1.8.8.cmml" xref="algorithm1.8.8.m1.8.8">𝑡</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.8.8.m1.10c">\bm{x}_{i}^{(t,k+1)}=\hbox{\pagecolor{blue!18}{ClientOpt}}(\bm{x}_{i}^{(t,k)},g_{i}(\bm{x}_{i}^{(t,k)}),\eta,t)</annotation></semantics></math>
</div>
<div id="algorithm1.11.13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">9</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.9.9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">10</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   Compute local model changes <math id="algorithm1.9.9.m1.5" class="ltx_Math" alttext="\Delta_{i}^{(t)}=\bm{x}_{i}^{(t,\tau_{i})}-\bm{x}_{i}^{(t,0)}" display="inline"><semantics id="algorithm1.9.9.m1.5a"><mrow id="algorithm1.9.9.m1.5.6" xref="algorithm1.9.9.m1.5.6.cmml"><msubsup id="algorithm1.9.9.m1.5.6.2" xref="algorithm1.9.9.m1.5.6.2.cmml"><mi mathvariant="normal" id="algorithm1.9.9.m1.5.6.2.2.2" xref="algorithm1.9.9.m1.5.6.2.2.2.cmml">Δ</mi><mi id="algorithm1.9.9.m1.5.6.2.2.3" xref="algorithm1.9.9.m1.5.6.2.2.3.cmml">i</mi><mrow id="algorithm1.9.9.m1.1.1.1.3" xref="algorithm1.9.9.m1.5.6.2.cmml"><mo stretchy="false" id="algorithm1.9.9.m1.1.1.1.3.1" xref="algorithm1.9.9.m1.5.6.2.cmml">(</mo><mi id="algorithm1.9.9.m1.1.1.1.1" xref="algorithm1.9.9.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.9.9.m1.1.1.1.3.2" xref="algorithm1.9.9.m1.5.6.2.cmml">)</mo></mrow></msubsup><mo id="algorithm1.9.9.m1.5.6.1" xref="algorithm1.9.9.m1.5.6.1.cmml">=</mo><mrow id="algorithm1.9.9.m1.5.6.3" xref="algorithm1.9.9.m1.5.6.3.cmml"><msubsup id="algorithm1.9.9.m1.5.6.3.2" xref="algorithm1.9.9.m1.5.6.3.2.cmml"><mi id="algorithm1.9.9.m1.5.6.3.2.2.2" xref="algorithm1.9.9.m1.5.6.3.2.2.2.cmml">𝒙</mi><mi id="algorithm1.9.9.m1.5.6.3.2.2.3" xref="algorithm1.9.9.m1.5.6.3.2.2.3.cmml">i</mi><mrow id="algorithm1.9.9.m1.3.3.2.2" xref="algorithm1.9.9.m1.3.3.2.3.cmml"><mo stretchy="false" id="algorithm1.9.9.m1.3.3.2.2.2" xref="algorithm1.9.9.m1.3.3.2.3.cmml">(</mo><mi id="algorithm1.9.9.m1.2.2.1.1" xref="algorithm1.9.9.m1.2.2.1.1.cmml">t</mi><mo id="algorithm1.9.9.m1.3.3.2.2.3" xref="algorithm1.9.9.m1.3.3.2.3.cmml">,</mo><msub id="algorithm1.9.9.m1.3.3.2.2.1" xref="algorithm1.9.9.m1.3.3.2.2.1.cmml"><mi id="algorithm1.9.9.m1.3.3.2.2.1.2" xref="algorithm1.9.9.m1.3.3.2.2.1.2.cmml">τ</mi><mi id="algorithm1.9.9.m1.3.3.2.2.1.3" xref="algorithm1.9.9.m1.3.3.2.2.1.3.cmml">i</mi></msub><mo stretchy="false" id="algorithm1.9.9.m1.3.3.2.2.4" xref="algorithm1.9.9.m1.3.3.2.3.cmml">)</mo></mrow></msubsup><mo id="algorithm1.9.9.m1.5.6.3.1" xref="algorithm1.9.9.m1.5.6.3.1.cmml">−</mo><msubsup id="algorithm1.9.9.m1.5.6.3.3" xref="algorithm1.9.9.m1.5.6.3.3.cmml"><mi id="algorithm1.9.9.m1.5.6.3.3.2.2" xref="algorithm1.9.9.m1.5.6.3.3.2.2.cmml">𝒙</mi><mi id="algorithm1.9.9.m1.5.6.3.3.2.3" xref="algorithm1.9.9.m1.5.6.3.3.2.3.cmml">i</mi><mrow id="algorithm1.9.9.m1.5.5.2.4" xref="algorithm1.9.9.m1.5.5.2.3.cmml"><mo stretchy="false" id="algorithm1.9.9.m1.5.5.2.4.1" xref="algorithm1.9.9.m1.5.5.2.3.cmml">(</mo><mi id="algorithm1.9.9.m1.4.4.1.1" xref="algorithm1.9.9.m1.4.4.1.1.cmml">t</mi><mo id="algorithm1.9.9.m1.5.5.2.4.2" xref="algorithm1.9.9.m1.5.5.2.3.cmml">,</mo><mn id="algorithm1.9.9.m1.5.5.2.2" xref="algorithm1.9.9.m1.5.5.2.2.cmml">0</mn><mo stretchy="false" id="algorithm1.9.9.m1.5.5.2.4.3" xref="algorithm1.9.9.m1.5.5.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.9.9.m1.5b"><apply id="algorithm1.9.9.m1.5.6.cmml" xref="algorithm1.9.9.m1.5.6"><eq id="algorithm1.9.9.m1.5.6.1.cmml" xref="algorithm1.9.9.m1.5.6.1"></eq><apply id="algorithm1.9.9.m1.5.6.2.cmml" xref="algorithm1.9.9.m1.5.6.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.5.6.2.1.cmml" xref="algorithm1.9.9.m1.5.6.2">superscript</csymbol><apply id="algorithm1.9.9.m1.5.6.2.2.cmml" xref="algorithm1.9.9.m1.5.6.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.5.6.2.2.1.cmml" xref="algorithm1.9.9.m1.5.6.2">subscript</csymbol><ci id="algorithm1.9.9.m1.5.6.2.2.2.cmml" xref="algorithm1.9.9.m1.5.6.2.2.2">Δ</ci><ci id="algorithm1.9.9.m1.5.6.2.2.3.cmml" xref="algorithm1.9.9.m1.5.6.2.2.3">𝑖</ci></apply><ci id="algorithm1.9.9.m1.1.1.1.1.cmml" xref="algorithm1.9.9.m1.1.1.1.1">𝑡</ci></apply><apply id="algorithm1.9.9.m1.5.6.3.cmml" xref="algorithm1.9.9.m1.5.6.3"><minus id="algorithm1.9.9.m1.5.6.3.1.cmml" xref="algorithm1.9.9.m1.5.6.3.1"></minus><apply id="algorithm1.9.9.m1.5.6.3.2.cmml" xref="algorithm1.9.9.m1.5.6.3.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.5.6.3.2.1.cmml" xref="algorithm1.9.9.m1.5.6.3.2">superscript</csymbol><apply id="algorithm1.9.9.m1.5.6.3.2.2.cmml" xref="algorithm1.9.9.m1.5.6.3.2"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.5.6.3.2.2.1.cmml" xref="algorithm1.9.9.m1.5.6.3.2">subscript</csymbol><ci id="algorithm1.9.9.m1.5.6.3.2.2.2.cmml" xref="algorithm1.9.9.m1.5.6.3.2.2.2">𝒙</ci><ci id="algorithm1.9.9.m1.5.6.3.2.2.3.cmml" xref="algorithm1.9.9.m1.5.6.3.2.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.9.9.m1.3.3.2.3.cmml" xref="algorithm1.9.9.m1.3.3.2.2"><ci id="algorithm1.9.9.m1.2.2.1.1.cmml" xref="algorithm1.9.9.m1.2.2.1.1">𝑡</ci><apply id="algorithm1.9.9.m1.3.3.2.2.1.cmml" xref="algorithm1.9.9.m1.3.3.2.2.1"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.3.3.2.2.1.1.cmml" xref="algorithm1.9.9.m1.3.3.2.2.1">subscript</csymbol><ci id="algorithm1.9.9.m1.3.3.2.2.1.2.cmml" xref="algorithm1.9.9.m1.3.3.2.2.1.2">𝜏</ci><ci id="algorithm1.9.9.m1.3.3.2.2.1.3.cmml" xref="algorithm1.9.9.m1.3.3.2.2.1.3">𝑖</ci></apply></interval></apply><apply id="algorithm1.9.9.m1.5.6.3.3.cmml" xref="algorithm1.9.9.m1.5.6.3.3"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.5.6.3.3.1.cmml" xref="algorithm1.9.9.m1.5.6.3.3">superscript</csymbol><apply id="algorithm1.9.9.m1.5.6.3.3.2.cmml" xref="algorithm1.9.9.m1.5.6.3.3"><csymbol cd="ambiguous" id="algorithm1.9.9.m1.5.6.3.3.2.1.cmml" xref="algorithm1.9.9.m1.5.6.3.3">subscript</csymbol><ci id="algorithm1.9.9.m1.5.6.3.3.2.2.cmml" xref="algorithm1.9.9.m1.5.6.3.3.2.2">𝒙</ci><ci id="algorithm1.9.9.m1.5.6.3.3.2.3.cmml" xref="algorithm1.9.9.m1.5.6.3.3.2.3">𝑖</ci></apply><interval closure="open" id="algorithm1.9.9.m1.5.5.2.3.cmml" xref="algorithm1.9.9.m1.5.5.2.4"><ci id="algorithm1.9.9.m1.4.4.1.1.cmml" xref="algorithm1.9.9.m1.4.4.1.1">𝑡</ci><cn type="integer" id="algorithm1.9.9.m1.5.5.2.2.cmml" xref="algorithm1.9.9.m1.5.5.2.2">0</cn></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.9.9.m1.5c">\Delta_{i}^{(t)}=\bm{x}_{i}^{(t,\tau_{i})}-\bm{x}_{i}^{(t,0)}</annotation></semantics></math>
</div>
<div id="algorithm1.11.14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">11</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>     <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.10.10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">12</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   Aggregate local changes <math id="algorithm1.10.10.m1.4" class="ltx_Math" alttext="\Delta^{(t)}=\sum_{i\in\mathcal{S}^{(t)}}p_{i}\Delta_{i}^{(t)}/\sum_{i\in\mathcal{S}^{(t)}}p_{i}" display="inline"><semantics id="algorithm1.10.10.m1.4a"><mrow id="algorithm1.10.10.m1.4.5" xref="algorithm1.10.10.m1.4.5.cmml"><msup id="algorithm1.10.10.m1.4.5.2" xref="algorithm1.10.10.m1.4.5.2.cmml"><mi mathvariant="normal" id="algorithm1.10.10.m1.4.5.2.2" xref="algorithm1.10.10.m1.4.5.2.2.cmml">Δ</mi><mrow id="algorithm1.10.10.m1.1.1.1.3" xref="algorithm1.10.10.m1.4.5.2.cmml"><mo stretchy="false" id="algorithm1.10.10.m1.1.1.1.3.1" xref="algorithm1.10.10.m1.4.5.2.cmml">(</mo><mi id="algorithm1.10.10.m1.1.1.1.1" xref="algorithm1.10.10.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.10.10.m1.1.1.1.3.2" xref="algorithm1.10.10.m1.4.5.2.cmml">)</mo></mrow></msup><mo rspace="0.111em" id="algorithm1.10.10.m1.4.5.1" xref="algorithm1.10.10.m1.4.5.1.cmml">=</mo><mrow id="algorithm1.10.10.m1.4.5.3" xref="algorithm1.10.10.m1.4.5.3.cmml"><msub id="algorithm1.10.10.m1.4.5.3.1" xref="algorithm1.10.10.m1.4.5.3.1.cmml"><mo id="algorithm1.10.10.m1.4.5.3.1.2" xref="algorithm1.10.10.m1.4.5.3.1.2.cmml">∑</mo><mrow id="algorithm1.10.10.m1.2.2.1" xref="algorithm1.10.10.m1.2.2.1.cmml"><mi id="algorithm1.10.10.m1.2.2.1.3" xref="algorithm1.10.10.m1.2.2.1.3.cmml">i</mi><mo id="algorithm1.10.10.m1.2.2.1.2" xref="algorithm1.10.10.m1.2.2.1.2.cmml">∈</mo><msup id="algorithm1.10.10.m1.2.2.1.4" xref="algorithm1.10.10.m1.2.2.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.10.10.m1.2.2.1.4.2" xref="algorithm1.10.10.m1.2.2.1.4.2.cmml">𝒮</mi><mrow id="algorithm1.10.10.m1.2.2.1.1.1.3" xref="algorithm1.10.10.m1.2.2.1.4.cmml"><mo stretchy="false" id="algorithm1.10.10.m1.2.2.1.1.1.3.1" xref="algorithm1.10.10.m1.2.2.1.4.cmml">(</mo><mi id="algorithm1.10.10.m1.2.2.1.1.1.1" xref="algorithm1.10.10.m1.2.2.1.1.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.10.10.m1.2.2.1.1.1.3.2" xref="algorithm1.10.10.m1.2.2.1.4.cmml">)</mo></mrow></msup></mrow></msub><mrow id="algorithm1.10.10.m1.4.5.3.2" xref="algorithm1.10.10.m1.4.5.3.2.cmml"><mrow id="algorithm1.10.10.m1.4.5.3.2.2" xref="algorithm1.10.10.m1.4.5.3.2.2.cmml"><msub id="algorithm1.10.10.m1.4.5.3.2.2.2" xref="algorithm1.10.10.m1.4.5.3.2.2.2.cmml"><mi id="algorithm1.10.10.m1.4.5.3.2.2.2.2" xref="algorithm1.10.10.m1.4.5.3.2.2.2.2.cmml">p</mi><mi id="algorithm1.10.10.m1.4.5.3.2.2.2.3" xref="algorithm1.10.10.m1.4.5.3.2.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="algorithm1.10.10.m1.4.5.3.2.2.1" xref="algorithm1.10.10.m1.4.5.3.2.2.1.cmml">​</mo><msubsup id="algorithm1.10.10.m1.4.5.3.2.2.3" xref="algorithm1.10.10.m1.4.5.3.2.2.3.cmml"><mi mathvariant="normal" id="algorithm1.10.10.m1.4.5.3.2.2.3.2.2" xref="algorithm1.10.10.m1.4.5.3.2.2.3.2.2.cmml">Δ</mi><mi id="algorithm1.10.10.m1.4.5.3.2.2.3.2.3" xref="algorithm1.10.10.m1.4.5.3.2.2.3.2.3.cmml">i</mi><mrow id="algorithm1.10.10.m1.3.3.1.3" xref="algorithm1.10.10.m1.4.5.3.2.2.3.cmml"><mo stretchy="false" id="algorithm1.10.10.m1.3.3.1.3.1" xref="algorithm1.10.10.m1.4.5.3.2.2.3.cmml">(</mo><mi id="algorithm1.10.10.m1.3.3.1.1" xref="algorithm1.10.10.m1.3.3.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.10.10.m1.3.3.1.3.2" xref="algorithm1.10.10.m1.4.5.3.2.2.3.cmml">)</mo></mrow></msubsup></mrow><mo rspace="0.055em" id="algorithm1.10.10.m1.4.5.3.2.1" xref="algorithm1.10.10.m1.4.5.3.2.1.cmml">/</mo><mrow id="algorithm1.10.10.m1.4.5.3.2.3" xref="algorithm1.10.10.m1.4.5.3.2.3.cmml"><msub id="algorithm1.10.10.m1.4.5.3.2.3.1" xref="algorithm1.10.10.m1.4.5.3.2.3.1.cmml"><mo id="algorithm1.10.10.m1.4.5.3.2.3.1.2" xref="algorithm1.10.10.m1.4.5.3.2.3.1.2.cmml">∑</mo><mrow id="algorithm1.10.10.m1.4.4.1" xref="algorithm1.10.10.m1.4.4.1.cmml"><mi id="algorithm1.10.10.m1.4.4.1.3" xref="algorithm1.10.10.m1.4.4.1.3.cmml">i</mi><mo id="algorithm1.10.10.m1.4.4.1.2" xref="algorithm1.10.10.m1.4.4.1.2.cmml">∈</mo><msup id="algorithm1.10.10.m1.4.4.1.4" xref="algorithm1.10.10.m1.4.4.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="algorithm1.10.10.m1.4.4.1.4.2" xref="algorithm1.10.10.m1.4.4.1.4.2.cmml">𝒮</mi><mrow id="algorithm1.10.10.m1.4.4.1.1.1.3" xref="algorithm1.10.10.m1.4.4.1.4.cmml"><mo stretchy="false" id="algorithm1.10.10.m1.4.4.1.1.1.3.1" xref="algorithm1.10.10.m1.4.4.1.4.cmml">(</mo><mi id="algorithm1.10.10.m1.4.4.1.1.1.1" xref="algorithm1.10.10.m1.4.4.1.1.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.10.10.m1.4.4.1.1.1.3.2" xref="algorithm1.10.10.m1.4.4.1.4.cmml">)</mo></mrow></msup></mrow></msub><msub id="algorithm1.10.10.m1.4.5.3.2.3.2" xref="algorithm1.10.10.m1.4.5.3.2.3.2.cmml"><mi id="algorithm1.10.10.m1.4.5.3.2.3.2.2" xref="algorithm1.10.10.m1.4.5.3.2.3.2.2.cmml">p</mi><mi id="algorithm1.10.10.m1.4.5.3.2.3.2.3" xref="algorithm1.10.10.m1.4.5.3.2.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.10.10.m1.4b"><apply id="algorithm1.10.10.m1.4.5.cmml" xref="algorithm1.10.10.m1.4.5"><eq id="algorithm1.10.10.m1.4.5.1.cmml" xref="algorithm1.10.10.m1.4.5.1"></eq><apply id="algorithm1.10.10.m1.4.5.2.cmml" xref="algorithm1.10.10.m1.4.5.2"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.2.1.cmml" xref="algorithm1.10.10.m1.4.5.2">superscript</csymbol><ci id="algorithm1.10.10.m1.4.5.2.2.cmml" xref="algorithm1.10.10.m1.4.5.2.2">Δ</ci><ci id="algorithm1.10.10.m1.1.1.1.1.cmml" xref="algorithm1.10.10.m1.1.1.1.1">𝑡</ci></apply><apply id="algorithm1.10.10.m1.4.5.3.cmml" xref="algorithm1.10.10.m1.4.5.3"><apply id="algorithm1.10.10.m1.4.5.3.1.cmml" xref="algorithm1.10.10.m1.4.5.3.1"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.3.1.1.cmml" xref="algorithm1.10.10.m1.4.5.3.1">subscript</csymbol><sum id="algorithm1.10.10.m1.4.5.3.1.2.cmml" xref="algorithm1.10.10.m1.4.5.3.1.2"></sum><apply id="algorithm1.10.10.m1.2.2.1.cmml" xref="algorithm1.10.10.m1.2.2.1"><in id="algorithm1.10.10.m1.2.2.1.2.cmml" xref="algorithm1.10.10.m1.2.2.1.2"></in><ci id="algorithm1.10.10.m1.2.2.1.3.cmml" xref="algorithm1.10.10.m1.2.2.1.3">𝑖</ci><apply id="algorithm1.10.10.m1.2.2.1.4.cmml" xref="algorithm1.10.10.m1.2.2.1.4"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.2.2.1.4.1.cmml" xref="algorithm1.10.10.m1.2.2.1.4">superscript</csymbol><ci id="algorithm1.10.10.m1.2.2.1.4.2.cmml" xref="algorithm1.10.10.m1.2.2.1.4.2">𝒮</ci><ci id="algorithm1.10.10.m1.2.2.1.1.1.1.cmml" xref="algorithm1.10.10.m1.2.2.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="algorithm1.10.10.m1.4.5.3.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2"><divide id="algorithm1.10.10.m1.4.5.3.2.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.1"></divide><apply id="algorithm1.10.10.m1.4.5.3.2.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2"><times id="algorithm1.10.10.m1.4.5.3.2.2.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.1"></times><apply id="algorithm1.10.10.m1.4.5.3.2.2.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.2"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.3.2.2.2.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.2">subscript</csymbol><ci id="algorithm1.10.10.m1.4.5.3.2.2.2.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.2.2">𝑝</ci><ci id="algorithm1.10.10.m1.4.5.3.2.2.2.3.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.2.3">𝑖</ci></apply><apply id="algorithm1.10.10.m1.4.5.3.2.2.3.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.3"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.3.2.2.3.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.3">superscript</csymbol><apply id="algorithm1.10.10.m1.4.5.3.2.2.3.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.3"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.3.2.2.3.2.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.3">subscript</csymbol><ci id="algorithm1.10.10.m1.4.5.3.2.2.3.2.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.3.2.2">Δ</ci><ci id="algorithm1.10.10.m1.4.5.3.2.2.3.2.3.cmml" xref="algorithm1.10.10.m1.4.5.3.2.2.3.2.3">𝑖</ci></apply><ci id="algorithm1.10.10.m1.3.3.1.1.cmml" xref="algorithm1.10.10.m1.3.3.1.1">𝑡</ci></apply></apply><apply id="algorithm1.10.10.m1.4.5.3.2.3.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3"><apply id="algorithm1.10.10.m1.4.5.3.2.3.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.1"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.3.2.3.1.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.1">subscript</csymbol><sum id="algorithm1.10.10.m1.4.5.3.2.3.1.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.1.2"></sum><apply id="algorithm1.10.10.m1.4.4.1.cmml" xref="algorithm1.10.10.m1.4.4.1"><in id="algorithm1.10.10.m1.4.4.1.2.cmml" xref="algorithm1.10.10.m1.4.4.1.2"></in><ci id="algorithm1.10.10.m1.4.4.1.3.cmml" xref="algorithm1.10.10.m1.4.4.1.3">𝑖</ci><apply id="algorithm1.10.10.m1.4.4.1.4.cmml" xref="algorithm1.10.10.m1.4.4.1.4"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.4.1.4.1.cmml" xref="algorithm1.10.10.m1.4.4.1.4">superscript</csymbol><ci id="algorithm1.10.10.m1.4.4.1.4.2.cmml" xref="algorithm1.10.10.m1.4.4.1.4.2">𝒮</ci><ci id="algorithm1.10.10.m1.4.4.1.1.1.1.cmml" xref="algorithm1.10.10.m1.4.4.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="algorithm1.10.10.m1.4.5.3.2.3.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.2"><csymbol cd="ambiguous" id="algorithm1.10.10.m1.4.5.3.2.3.2.1.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.2">subscript</csymbol><ci id="algorithm1.10.10.m1.4.5.3.2.3.2.2.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.2.2">𝑝</ci><ci id="algorithm1.10.10.m1.4.5.3.2.3.2.3.cmml" xref="algorithm1.10.10.m1.4.5.3.2.3.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.10.10.m1.4c">\Delta^{(t)}=\sum_{i\in\mathcal{S}^{(t)}}p_{i}\Delta_{i}^{(t)}/\sum_{i\in\mathcal{S}^{(t)}}p_{i}</annotation></semantics></math>
</div>
<div id="algorithm1.11.11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">13</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   
Update global model <math id="algorithm1.11.11.m1.7" class="ltx_Math" alttext="\bm{x}^{(t+1)}=\hbox{\pagecolor{green!18}{ServerOpt}}(\bm{x}^{(t)},-\Delta^{(t)},\eta_{s},t)" display="inline"><semantics id="algorithm1.11.11.m1.7a"><mrow id="algorithm1.11.11.m1.7.7" xref="algorithm1.11.11.m1.7.7.cmml"><msup id="algorithm1.11.11.m1.7.7.5" xref="algorithm1.11.11.m1.7.7.5.cmml"><mi id="algorithm1.11.11.m1.7.7.5.2" xref="algorithm1.11.11.m1.7.7.5.2.cmml">𝒙</mi><mrow id="algorithm1.11.11.m1.1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="algorithm1.11.11.m1.1.1.1.1.2" xref="algorithm1.11.11.m1.1.1.1.1.1.cmml">(</mo><mrow id="algorithm1.11.11.m1.1.1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.1.1.cmml"><mi id="algorithm1.11.11.m1.1.1.1.1.1.2" xref="algorithm1.11.11.m1.1.1.1.1.1.2.cmml">t</mi><mo id="algorithm1.11.11.m1.1.1.1.1.1.1" xref="algorithm1.11.11.m1.1.1.1.1.1.1.cmml">+</mo><mn id="algorithm1.11.11.m1.1.1.1.1.1.3" xref="algorithm1.11.11.m1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="algorithm1.11.11.m1.1.1.1.1.3" xref="algorithm1.11.11.m1.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="algorithm1.11.11.m1.7.7.4" xref="algorithm1.11.11.m1.7.7.4.cmml">=</mo><mrow id="algorithm1.11.11.m1.7.7.3" xref="algorithm1.11.11.m1.7.7.3.cmml"><mtext class="ltx_font_smallcaps" mathbackground="#D1FFD1" id="algorithm1.11.11.m1.7.7.3.5" xref="algorithm1.11.11.m1.7.7.3.5a.cmml">ServerOpt</mtext><mo lspace="0em" rspace="0em" id="algorithm1.11.11.m1.7.7.3.4" xref="algorithm1.11.11.m1.7.7.3.4.cmml">​</mo><mrow id="algorithm1.11.11.m1.7.7.3.3.3" xref="algorithm1.11.11.m1.7.7.3.3.4.cmml"><mo stretchy="false" id="algorithm1.11.11.m1.7.7.3.3.3.4" xref="algorithm1.11.11.m1.7.7.3.3.4.cmml">(</mo><msup id="algorithm1.11.11.m1.5.5.1.1.1.1" xref="algorithm1.11.11.m1.5.5.1.1.1.1.cmml"><mi id="algorithm1.11.11.m1.5.5.1.1.1.1.2" xref="algorithm1.11.11.m1.5.5.1.1.1.1.2.cmml">𝒙</mi><mrow id="algorithm1.11.11.m1.2.2.1.3" xref="algorithm1.11.11.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="algorithm1.11.11.m1.2.2.1.3.1" xref="algorithm1.11.11.m1.5.5.1.1.1.1.cmml">(</mo><mi id="algorithm1.11.11.m1.2.2.1.1" xref="algorithm1.11.11.m1.2.2.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.11.11.m1.2.2.1.3.2" xref="algorithm1.11.11.m1.5.5.1.1.1.1.cmml">)</mo></mrow></msup><mo id="algorithm1.11.11.m1.7.7.3.3.3.5" xref="algorithm1.11.11.m1.7.7.3.3.4.cmml">,</mo><mrow id="algorithm1.11.11.m1.6.6.2.2.2.2" xref="algorithm1.11.11.m1.6.6.2.2.2.2.cmml"><mo id="algorithm1.11.11.m1.6.6.2.2.2.2a" xref="algorithm1.11.11.m1.6.6.2.2.2.2.cmml">−</mo><msup id="algorithm1.11.11.m1.6.6.2.2.2.2.2" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2.cmml"><mi mathvariant="normal" id="algorithm1.11.11.m1.6.6.2.2.2.2.2.2" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2.2.cmml">Δ</mi><mrow id="algorithm1.11.11.m1.3.3.1.3" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2.cmml"><mo stretchy="false" id="algorithm1.11.11.m1.3.3.1.3.1" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2.cmml">(</mo><mi id="algorithm1.11.11.m1.3.3.1.1" xref="algorithm1.11.11.m1.3.3.1.1.cmml">t</mi><mo stretchy="false" id="algorithm1.11.11.m1.3.3.1.3.2" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2.cmml">)</mo></mrow></msup></mrow><mo id="algorithm1.11.11.m1.7.7.3.3.3.6" xref="algorithm1.11.11.m1.7.7.3.3.4.cmml">,</mo><msub id="algorithm1.11.11.m1.7.7.3.3.3.3" xref="algorithm1.11.11.m1.7.7.3.3.3.3.cmml"><mi id="algorithm1.11.11.m1.7.7.3.3.3.3.2" xref="algorithm1.11.11.m1.7.7.3.3.3.3.2.cmml">η</mi><mi id="algorithm1.11.11.m1.7.7.3.3.3.3.3" xref="algorithm1.11.11.m1.7.7.3.3.3.3.3.cmml">s</mi></msub><mo id="algorithm1.11.11.m1.7.7.3.3.3.7" xref="algorithm1.11.11.m1.7.7.3.3.4.cmml">,</mo><mi id="algorithm1.11.11.m1.4.4" xref="algorithm1.11.11.m1.4.4.cmml">t</mi><mo stretchy="false" id="algorithm1.11.11.m1.7.7.3.3.3.8" xref="algorithm1.11.11.m1.7.7.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algorithm1.11.11.m1.7b"><apply id="algorithm1.11.11.m1.7.7.cmml" xref="algorithm1.11.11.m1.7.7"><eq id="algorithm1.11.11.m1.7.7.4.cmml" xref="algorithm1.11.11.m1.7.7.4"></eq><apply id="algorithm1.11.11.m1.7.7.5.cmml" xref="algorithm1.11.11.m1.7.7.5"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.7.7.5.1.cmml" xref="algorithm1.11.11.m1.7.7.5">superscript</csymbol><ci id="algorithm1.11.11.m1.7.7.5.2.cmml" xref="algorithm1.11.11.m1.7.7.5.2">𝒙</ci><apply id="algorithm1.11.11.m1.1.1.1.1.1.cmml" xref="algorithm1.11.11.m1.1.1.1.1"><plus id="algorithm1.11.11.m1.1.1.1.1.1.1.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1.1"></plus><ci id="algorithm1.11.11.m1.1.1.1.1.1.2.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="algorithm1.11.11.m1.1.1.1.1.1.3.cmml" xref="algorithm1.11.11.m1.1.1.1.1.1.3">1</cn></apply></apply><apply id="algorithm1.11.11.m1.7.7.3.cmml" xref="algorithm1.11.11.m1.7.7.3"><times id="algorithm1.11.11.m1.7.7.3.4.cmml" xref="algorithm1.11.11.m1.7.7.3.4"></times><ci id="algorithm1.11.11.m1.7.7.3.5a.cmml" xref="algorithm1.11.11.m1.7.7.3.5"><mtext class="ltx_font_smallcaps" mathbackground="#D1FFD1" id="algorithm1.11.11.m1.7.7.3.5.cmml" xref="algorithm1.11.11.m1.7.7.3.5">ServerOpt</mtext></ci><vector id="algorithm1.11.11.m1.7.7.3.3.4.cmml" xref="algorithm1.11.11.m1.7.7.3.3.3"><apply id="algorithm1.11.11.m1.5.5.1.1.1.1.cmml" xref="algorithm1.11.11.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.5.5.1.1.1.1.1.cmml" xref="algorithm1.11.11.m1.5.5.1.1.1.1">superscript</csymbol><ci id="algorithm1.11.11.m1.5.5.1.1.1.1.2.cmml" xref="algorithm1.11.11.m1.5.5.1.1.1.1.2">𝒙</ci><ci id="algorithm1.11.11.m1.2.2.1.1.cmml" xref="algorithm1.11.11.m1.2.2.1.1">𝑡</ci></apply><apply id="algorithm1.11.11.m1.6.6.2.2.2.2.cmml" xref="algorithm1.11.11.m1.6.6.2.2.2.2"><minus id="algorithm1.11.11.m1.6.6.2.2.2.2.1.cmml" xref="algorithm1.11.11.m1.6.6.2.2.2.2"></minus><apply id="algorithm1.11.11.m1.6.6.2.2.2.2.2.cmml" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.6.6.2.2.2.2.2.1.cmml" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2">superscript</csymbol><ci id="algorithm1.11.11.m1.6.6.2.2.2.2.2.2.cmml" xref="algorithm1.11.11.m1.6.6.2.2.2.2.2.2">Δ</ci><ci id="algorithm1.11.11.m1.3.3.1.1.cmml" xref="algorithm1.11.11.m1.3.3.1.1">𝑡</ci></apply></apply><apply id="algorithm1.11.11.m1.7.7.3.3.3.3.cmml" xref="algorithm1.11.11.m1.7.7.3.3.3.3"><csymbol cd="ambiguous" id="algorithm1.11.11.m1.7.7.3.3.3.3.1.cmml" xref="algorithm1.11.11.m1.7.7.3.3.3.3">subscript</csymbol><ci id="algorithm1.11.11.m1.7.7.3.3.3.3.2.cmml" xref="algorithm1.11.11.m1.7.7.3.3.3.3.2">𝜂</ci><ci id="algorithm1.11.11.m1.7.7.3.3.3.3.3.cmml" xref="algorithm1.11.11.m1.7.7.3.3.3.3.3">𝑠</ci></apply><ci id="algorithm1.11.11.m1.4.4.cmml" xref="algorithm1.11.11.m1.4.4">𝑡</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algorithm1.11.11.m1.7c">\bm{x}^{(t+1)}=\hbox{\pagecolor{green!18}{ServerOpt}}(\bm{x}^{(t)},-\Delta^{(t)},\eta_{s},t)</annotation></semantics></math>
</div>
<div id="algorithm1.11.15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline">14</span>  <span class="ltx_rule" style="width:1px;height:100%;background:black;display:inline-block;"> </span>   

</div>
<div id="algorithm1.11.16" class="ltx_listingline">

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm1.14.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span><span id="algorithm1.15.2" class="ltx_text ltx_font_smallcaps">FedOpt</span> <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>): A Generic FedAvg Algorithm</figcaption>
</figure>
<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.3" class="ltx_p">In this work, we propose to use FedOPT <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al., <a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>, a generalized version of FedAvg, to build the <span id="S2.SS2.p1.3.1" class="ltx_text ltx_font_typewriter">FedNLP</span> platform. As the pseudo-code presented in <a href="#algorithm1" title="In 2.2 Our Unified Framework for FL ‣ 2 Federated Learning for NLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">1</span></a>, the algorithm is parameterized by two gradient-based optimizers: <span id="S2.SS2.p1.3.2" class="ltx_text ltx_font_smallcaps">ClientOpt</span> and <span id="S2.SS2.p1.3.3" class="ltx_text ltx_font_smallcaps">ServerOpt</span> with client learning rate <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">η</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝜂</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\eta</annotation></semantics></math> and server learning rate <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\eta_{s}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">η</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">𝜂</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\eta_{s}</annotation></semantics></math>, respectively. While <span id="S2.SS2.p1.3.4" class="ltx_text ltx_font_smallcaps">ClientOpt</span> is used to update the local models, <span id="S2.SS2.p1.3.5" class="ltx_text ltx_font_smallcaps">ServerOpt</span> treats the negative of aggregated local changes <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="-\Delta^{(t)}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><mrow id="S2.SS2.p1.3.m3.1.2" xref="S2.SS2.p1.3.m3.1.2.cmml"><mo id="S2.SS2.p1.3.m3.1.2a" xref="S2.SS2.p1.3.m3.1.2.cmml">−</mo><msup id="S2.SS2.p1.3.m3.1.2.2" xref="S2.SS2.p1.3.m3.1.2.2.cmml"><mi mathvariant="normal" id="S2.SS2.p1.3.m3.1.2.2.2" xref="S2.SS2.p1.3.m3.1.2.2.2.cmml">Δ</mi><mrow id="S2.SS2.p1.3.m3.1.1.1.3" xref="S2.SS2.p1.3.m3.1.2.2.cmml"><mo stretchy="false" id="S2.SS2.p1.3.m3.1.1.1.3.1" xref="S2.SS2.p1.3.m3.1.2.2.cmml">(</mo><mi id="S2.SS2.p1.3.m3.1.1.1.1" xref="S2.SS2.p1.3.m3.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS2.p1.3.m3.1.1.1.3.2" xref="S2.SS2.p1.3.m3.1.2.2.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.2.cmml" xref="S2.SS2.p1.3.m3.1.2"><minus id="S2.SS2.p1.3.m3.1.2.1.cmml" xref="S2.SS2.p1.3.m3.1.2"></minus><apply id="S2.SS2.p1.3.m3.1.2.2.cmml" xref="S2.SS2.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.2.2.1.cmml" xref="S2.SS2.p1.3.m3.1.2.2">superscript</csymbol><ci id="S2.SS2.p1.3.m3.1.2.2.2.cmml" xref="S2.SS2.p1.3.m3.1.2.2.2">Δ</ci><ci id="S2.SS2.p1.3.m3.1.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1.1.1">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">-\Delta^{(t)}</annotation></semantics></math> as a pseudo-gradient and applies it to the global model.
This optimization framework generalizes to many aggregation-based FL algorithms and simplifies the system design.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.2" class="ltx_p">To make our research general, we explore different combinations of <span id="S2.SS2.p2.2.1" class="ltx_text ltx_font_smallcaps">SeverOpt</span> and <span id="S2.SS2.p2.2.2" class="ltx_text ltx_font_smallcaps">ClientOpt</span>. The original FedAvg algorithm implicitly sets <span id="S2.SS2.p2.2.3" class="ltx_text ltx_font_smallcaps">SeverOpt</span> and <span id="S2.SS2.p2.2.4" class="ltx_text ltx_font_smallcaps">ClientOpt</span> to be SGD, with a fixed server learning rate <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="\eta_{s}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">η</mi><mi id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">𝜂</ci><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\eta_{s}</annotation></semantics></math> of <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mn id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><cn type="float" id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">1.0</annotation></semantics></math>. FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020b</a>)</cite>, tackling statistical heterogeneity by restricting the local model updates to be closer to the initial (global) model, can be easily incorporated into this framework by adding L2 regularization for better stability in training. Moreover, given that AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> is widely used in NLP, we set it for <span id="S2.SS2.p2.2.5" class="ltx_text ltx_font_typewriter">ClientOpt</span> and let the <span id="S2.SS2.p2.2.6" class="ltx_text ltx_font_typewriter">ServerOpt</span> be SGD with momentum to reduce the burden of tuning.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>The Proposed FedNLP Framework</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">To support our research in this paper and other future work in the area of federated learning for NLP,
we build a general research framework named FedNLP, based on the above universal optimization framework.
We here briefly highlight its unique features and leave the details in the following content and a detailed design is shown in App. <a href="#A6" title="Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>.
First, FedNLP is the very first framework that connects multiple FL algorithms with Transformer-based models, to our best knowledge.
Also, we implement a flexible suite of interfaces to support different types of NLP tasks and models, as well as different non-IID partitioning strategies (Sec. <a href="#S3.SS2" title="3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
To study security and privacy guarantees, we incorporate state-of-the-art secure aggregation algorithms such as LightSecAgg (see  <a href="#A6.SS5" title="F.5 Enhancing Security with Secure Aggregation (SA) ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F.5</span></a>).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Benchmarking Setup with FedNLP</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce the creation of our benchmark datasets from a set of chosen NLP tasks with different non-IID partition methods. We evaluate various FL methods on these datasets.
</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Task Formulations, Datasets, and Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">There are numerous NLP applications, but most of them can be categorized based on four mainstream formulations: text classification (TC), sequence tagging (ST), question answering (QA), and seq2seq generation (SS).
The formal definition of each formulation is detailed in Appendix §<a href="#A2" title="Приложение B Basic Formulations of NLP Tasks ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.
To cover all formulations while keeping our experiments in a reasonable scope, we select one representative task for each formulation:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Text Classification</span>: <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_typewriter">20Newsgroup</span> <cite class="ltx_cite ltx_citemacro_cite">Lang (<a href="#bib.bib28" title="" class="ltx_ref">1995</a>)</cite> is a news classification dataset with annotations for 20
labels. We showcase our FedNLP with this dataset as it has a larger output space (20 labels) than sentiment-analysis datasets, which is an important factor for the label-distribution shift scenarios. .</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Sequence Tagging</span>: <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">OntoNotes</span> <cite class="ltx_cite ltx_citemacro_cite">Pradhan et al. (<a href="#bib.bib43" title="" class="ltx_ref">2013</a>)</cite> (5.0) is a corpus where sentences have annotations for the entity spans and types. We use it for the named entity recognition task, which is fundamental to information extraction and other applications.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">QA</span>: <span id="S3.I1.i3.p1.1.2" class="ltx_text ltx_font_typewriter">MRQA</span> <cite class="ltx_cite ltx_citemacro_cite">Fisch et al. (<a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite> is a benchmark consisting of 6 popular datasets<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We only use part of the data to demonstrate and verify our hypothesis; we show the train/test split in brackets.</span></span></span>: <span id="S3.I1.i3.p1.1.3" class="ltx_text ltx_font_typewriter">SQuAD</span> <cite class="ltx_cite ltx_citemacro_cite">Rajpurkar et al. (<a href="#bib.bib47" title="" class="ltx_ref">2016</a>)</cite> (8529/431), <span id="S3.I1.i3.p1.1.4" class="ltx_text ltx_font_typewriter">NewsQA</span> <cite class="ltx_cite ltx_citemacro_cite">Trischler et al. (<a href="#bib.bib60" title="" class="ltx_ref">2017</a>)</cite> (11877/613), <span id="S3.I1.i3.p1.1.5" class="ltx_text ltx_font_typewriter">TriviaQA</span> <cite class="ltx_cite ltx_citemacro_cite">Joshi et al. (<a href="#bib.bib25" title="" class="ltx_ref">2017</a>)</cite>
(4120/176)
, <span id="S3.I1.i3.p1.1.6" class="ltx_text ltx_font_typewriter">SearchQA</span> <cite class="ltx_cite ltx_citemacro_cite">Dunn et al. (<a href="#bib.bib9" title="" class="ltx_ref">2017</a>)</cite>
(9972/499)
,
<span id="S3.I1.i3.p1.1.7" class="ltx_text ltx_font_typewriter">HotpotQA</span> <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib67" title="" class="ltx_ref">2018b</a>)</cite>
, and <span id="S3.I1.i3.p1.1.8" class="ltx_text ltx_font_typewriter">NQ</span> <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> (9617/795).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Seq2Seq Generation</span>: <span id="S3.I1.i4.p1.1.2" class="ltx_text ltx_font_typewriter">Gigaword</span> <cite class="ltx_cite ltx_citemacro_cite">DBL (<a href="#bib.bib1" title="" class="ltx_ref">2012</a>)</cite> is a news corpus with headlines that are often used for testing seq2seq models as a summarization task. Other tasks such as dialogue response generation and machine translation can also be adapted to this format.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">Task</td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Txt.Cls.</td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Seq.Tag.</td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">QA</td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Seq2Seq</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">Dataset</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20News</td>
<td id="S3.T1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Onto.</td>
<td id="S3.T1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MRQA</td>
<td id="S3.T1.1.2.5" class="ltx_td ltx_align_center ltx_border_t">Giga.</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"># Training</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.3k</td>
<td id="S3.T1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">50k</td>
<td id="S3.T1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">53.9k</td>
<td id="S3.T1.1.3.5" class="ltx_td ltx_align_center ltx_border_t">10k</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_center ltx_border_rr"># Test</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_center ltx_border_r">7.5k</td>
<td id="S3.T1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">5k</td>
<td id="S3.T1.1.4.4" class="ltx_td ltx_align_center ltx_border_r">3k</td>
<td id="S3.T1.1.4.5" class="ltx_td ltx_align_center">2k</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_center ltx_border_rr"># Labels</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_r">20</td>
<td id="S3.T1.1.5.3" class="ltx_td ltx_align_center ltx_border_r">37*</td>
<td id="S3.T1.1.5.4" class="ltx_td ltx_align_center ltx_border_r">N/A</td>
<td id="S3.T1.1.5.5" class="ltx_td ltx_align_center">N/A</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t">Metrics</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Acc.</td>
<td id="S3.T1.1.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">F-1</td>
<td id="S3.T1.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">F-1</td>
<td id="S3.T1.1.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">ROUGE</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Таблица 1: </span>
Statistics of the selected datasets for our experiments. *37 is the size of the tag vocabulary.
</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We show the basic statistics of the above datasets in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Task Formulations, Datasets, and Models ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Note that our FedNLP as a research platform supports a much wider range of specific tasks of each formulation, while we only introduce the ones used in our experiments here with typical settings.
Moreover, our contribution is more of a general FL+NLP benchmarking platform instead of particular datasets and partitions.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Base NLP Models.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">Fine-tuning pre-trained LMs has been the <span id="S3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">de facto</span> method for NLP research, so we focus on testing Transformer-based architectures in FedNLP.
Specifically, we choose to use BART <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a href="#bib.bib30" title="" class="ltx_ref">2020</a>)</cite>, a text-to-text Transformer model similar to the T5 model <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>, for seq2seq tasks.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Non-IID Partitioning Strategies</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The existing datasets have been used for centralized training in NLP.
As our focus here is to test decentralized learning methods, we need to distribute the existing datasets to a set of clients.
It is the non-IIDness of the client distribution that makes federated learning a challenging problem.
Thus, we extend the common practice widely used in prior works to the NLP domain for generating synthetic FL benchmarks <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib31" title="" class="ltx_ref">2021a</a>)</cite>.
We first introduce how we control the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">label distribution shift</span> for TC and ST, then the <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">quantity distribution shift</span>, and finally how we model the distribution shift in terms of input features for non-classification NLP tasks (e.g., summarization).</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Non-IID Label Distributions.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.15" class="ltx_p">Here we present how we synthesize the data partitions such that clients share the same (or very similar) number of examples, but have different <span id="S3.SS2.SSS0.Px1.p1.15.1" class="ltx_text ltx_font_italic">label distributions</span> from each other.
We assume that on every client training, examples are drawn independently with labels following a categorical distribution over <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">L</annotation></semantics></math> classes parameterized by a vector <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\bm{q}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">\bm{q}</annotation></semantics></math> <math id="S3.SS2.SSS0.Px1.p1.3.m3.2" class="ltx_math_unparsed" alttext="\left(q_{i}\geq 0,i\in[1,L]\right." display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.2a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.2b"><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.3">(</mo><msub id="S3.SS2.SSS0.Px1.p1.3.m3.2.4"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.4.2">q</mi><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.4.3">i</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.5">≥</mo><mn id="S3.SS2.SSS0.Px1.p1.3.m3.2.6">0</mn><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.7">,</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.8">i</mi><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.9">∈</mo><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.2.10"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.3.m3.2.10.1">[</mo><mn id="S3.SS2.SSS0.Px1.p1.3.m3.1.1">1</mn><mo id="S3.SS2.SSS0.Px1.p1.3.m3.2.10.2">,</mo><mi id="S3.SS2.SSS0.Px1.p1.3.m3.2.2">L</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.3.m3.2.10.3">]</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.2c">\left(q_{i}\geq 0,i\in[1,L]\right.</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px1.p1.4.m4.1" class="ltx_math_unparsed" alttext="\left.\|\bm{q}\|_{1}=1\right)" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px1.p1.4.m4.1b"><mo rspace="0.167em" id="S3.SS2.SSS0.Px1.p1.4.m4.1.2">∥</mo><mi id="S3.SS2.SSS0.Px1.p1.4.m4.1.1">𝒒</mi><msub id="S3.SS2.SSS0.Px1.p1.4.m4.1.3"><mo lspace="0em" rspace="0.0835em" id="S3.SS2.SSS0.Px1.p1.4.m4.1.3.2">∥</mo><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.3.3">1</mn></msub><mo lspace="0.0835em" id="S3.SS2.SSS0.Px1.p1.4.m4.1.4">=</mo><mn id="S3.SS2.SSS0.Px1.p1.4.m4.1.5">1</mn><mo id="S3.SS2.SSS0.Px1.p1.4.m4.1.6">)</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.4.m4.1c">\left.\|\bm{q}\|_{1}=1\right)</annotation></semantics></math>. To synthesize a population of non-identical clients, we draw <math id="S3.SS2.SSS0.Px1.p1.5.m5.2" class="ltx_Math" alttext="\bm{q}\sim\operatorname{Dir}_{L}(\alpha\bm{p})" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.5.m5.2a"><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.4" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.4.cmml">𝒒</mi><mo id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.3.cmml">∼</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml"><msub id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml">Dir</mi><mi id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml">L</mi></msub><mo id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2a" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">⁡</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">(</mo><mrow id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.2" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.1" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.3.cmml">𝒑</mi></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.3" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.5.m5.2b"><apply id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.3">similar-to</csymbol><ci id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.4.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.4">𝒒</ci><apply id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2"><apply id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.2">Dir</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.1.3">𝐿</ci></apply><apply id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1"><times id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.1"></times><ci id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.2">𝛼</ci><ci id="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.2.1.3">𝒑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.5.m5.2c">\bm{q}\sim\operatorname{Dir}_{L}(\alpha\bm{p})</annotation></semantics></math> from a <span id="S3.SS2.SSS0.Px1.p1.15.2" class="ltx_text ltx_font_italic">Dirichlet</span> distribution, where <math id="S3.SS2.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="\bm{p}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.6.m6.1a"><mi id="S3.SS2.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml">𝒑</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.6.m6.1b"><ci id="S3.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.6.m6.1.1">𝒑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.6.m6.1c">\bm{p}</annotation></semantics></math> characterizes a prior class distribution over <math id="S3.SS2.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.7.m7.1a"><mi id="S3.SS2.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.7.m7.1b"><ci id="S3.SS2.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.7.m7.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.7.m7.1c">L</annotation></semantics></math> classes, and <math id="S3.SS2.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="\alpha&gt;0" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.8.m8.1a"><mrow id="S3.SS2.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2.cmml">α</mi><mo id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.8.m8.1b"><apply id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1"><gt id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.1"></gt><ci id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.2">𝛼</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.8.m8.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.8.m8.1c">\alpha&gt;0</annotation></semantics></math> is a concentration parameter controlling the identicalness among clients.
For each client <math id="S3.SS2.SSS0.Px1.p1.9.m9.1" class="ltx_Math" alttext="C_{j}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.9.m9.1a"><msub id="S3.SS2.SSS0.Px1.p1.9.m9.1.1" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.2" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1.2.cmml">C</mi><mi id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.3" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.9.m9.1b"><apply id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1.2">𝐶</ci><ci id="S3.SS2.SSS0.Px1.p1.9.m9.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.9.m9.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.9.m9.1c">C_{j}</annotation></semantics></math>, we draw a <math id="S3.SS2.SSS0.Px1.p1.10.m10.1" class="ltx_Math" alttext="\bm{q}_{j}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.10.m10.1a"><msub id="S3.SS2.SSS0.Px1.p1.10.m10.1.1" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.2" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1.2.cmml">𝒒</mi><mi id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.3" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.10.m10.1b"><apply id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1.2">𝒒</ci><ci id="S3.SS2.SSS0.Px1.p1.10.m10.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.10.m10.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.10.m10.1c">\bm{q}_{j}</annotation></semantics></math> as its label distribution and then sample examples without replacement from the global dataset according to <math id="S3.SS2.SSS0.Px1.p1.11.m11.1" class="ltx_Math" alttext="\bm{q}_{j}" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.11.m11.1a"><msub id="S3.SS2.SSS0.Px1.p1.11.m11.1.1" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.11.m11.1.1.2" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1.2.cmml">𝒒</mi><mi id="S3.SS2.SSS0.Px1.p1.11.m11.1.1.3" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.11.m11.1b"><apply id="S3.SS2.SSS0.Px1.p1.11.m11.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.11.m11.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.11.m11.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1.2">𝒒</ci><ci id="S3.SS2.SSS0.Px1.p1.11.m11.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.11.m11.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.11.m11.1c">\bm{q}_{j}</annotation></semantics></math>.
With <math id="S3.SS2.SSS0.Px1.p1.12.m12.1" class="ltx_Math" alttext="\alpha\rightarrow\infty" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.12.m12.1a"><mrow id="S3.SS2.SSS0.Px1.p1.12.m12.1.1" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.2" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.2.cmml">α</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.1" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.1.cmml">→</mo><mi mathvariant="normal" id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.3" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.3.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.12.m12.1b"><apply id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1"><ci id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.1">→</ci><ci id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.2">𝛼</ci><infinity id="S3.SS2.SSS0.Px1.p1.12.m12.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.12.m12.1.1.3"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.12.m12.1c">\alpha\rightarrow\infty</annotation></semantics></math>, all clients have identical distributions to the prior (i.e., uniform distribution);
with <math id="S3.SS2.SSS0.Px1.p1.13.m13.1" class="ltx_Math" alttext="\alpha\rightarrow 0" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.13.m13.1a"><mrow id="S3.SS2.SSS0.Px1.p1.13.m13.1.1" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.2" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.2.cmml">α</mi><mo stretchy="false" id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.1" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.1.cmml">→</mo><mn id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.3" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.13.m13.1b"><apply id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1"><ci id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.1">→</ci><ci id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.2">𝛼</ci><cn type="integer" id="S3.SS2.SSS0.Px1.p1.13.m13.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.13.m13.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.13.m13.1c">\alpha\rightarrow 0</annotation></semantics></math>, on the other extreme, each client holds examples from only one class chosen at random.
In Fig. <a href="#S3.F2" title="Figure 2 ‣ Non-IID Label Distributions. ‣ 3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show heatmaps for visualizing the distribution differences between each client.
Figure <a href="#S3.F3" title="Figure 3 ‣ Controlling non-IID Features. ‣ 3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example of the concrete label distributions for all clients with different <math id="S3.SS2.SSS0.Px1.p1.14.m14.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.14.m14.1a"><mi id="S3.SS2.SSS0.Px1.p1.14.m14.1.1" xref="S3.SS2.SSS0.Px1.p1.14.m14.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.14.m14.1b"><ci id="S3.SS2.SSS0.Px1.p1.14.m14.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.14.m14.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.14.m14.1c">\alpha</annotation></semantics></math>.
We can see that when <math id="S3.SS2.SSS0.Px1.p1.15.m15.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.15.m15.1a"><mi id="S3.SS2.SSS0.Px1.p1.15.m15.1.1" xref="S3.SS2.SSS0.Px1.p1.15.m15.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.15.m15.1b"><ci id="S3.SS2.SSS0.Px1.p1.15.m15.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.15.m15.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.15.m15.1c">\alpha</annotation></semantics></math> is smaller, the overall label distribution shift becomes larger.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2104.08815/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="133" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Рис. 2: </span> The <span id="S3.F2.13.1" class="ltx_text ltx_font_italic">J-S divergence</span> matrix between 100 clients on the <span id="S3.F2.14.2" class="ltx_text ltx_font_italic">20News</span> dataset when <math id="S3.F2.6.m1.4" class="ltx_Math" alttext="\alpha\in\{1,5,10,100\}" display="inline"><semantics id="S3.F2.6.m1.4b"><mrow id="S3.F2.6.m1.4.5" xref="S3.F2.6.m1.4.5.cmml"><mi id="S3.F2.6.m1.4.5.2" xref="S3.F2.6.m1.4.5.2.cmml">α</mi><mo id="S3.F2.6.m1.4.5.1" xref="S3.F2.6.m1.4.5.1.cmml">∈</mo><mrow id="S3.F2.6.m1.4.5.3.2" xref="S3.F2.6.m1.4.5.3.1.cmml"><mo stretchy="false" id="S3.F2.6.m1.4.5.3.2.1" xref="S3.F2.6.m1.4.5.3.1.cmml">{</mo><mn id="S3.F2.6.m1.1.1" xref="S3.F2.6.m1.1.1.cmml">1</mn><mo id="S3.F2.6.m1.4.5.3.2.2" xref="S3.F2.6.m1.4.5.3.1.cmml">,</mo><mn id="S3.F2.6.m1.2.2" xref="S3.F2.6.m1.2.2.cmml">5</mn><mo id="S3.F2.6.m1.4.5.3.2.3" xref="S3.F2.6.m1.4.5.3.1.cmml">,</mo><mn id="S3.F2.6.m1.3.3" xref="S3.F2.6.m1.3.3.cmml">10</mn><mo id="S3.F2.6.m1.4.5.3.2.4" xref="S3.F2.6.m1.4.5.3.1.cmml">,</mo><mn id="S3.F2.6.m1.4.4" xref="S3.F2.6.m1.4.4.cmml">100</mn><mo stretchy="false" id="S3.F2.6.m1.4.5.3.2.5" xref="S3.F2.6.m1.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.6.m1.4c"><apply id="S3.F2.6.m1.4.5.cmml" xref="S3.F2.6.m1.4.5"><in id="S3.F2.6.m1.4.5.1.cmml" xref="S3.F2.6.m1.4.5.1"></in><ci id="S3.F2.6.m1.4.5.2.cmml" xref="S3.F2.6.m1.4.5.2">𝛼</ci><set id="S3.F2.6.m1.4.5.3.1.cmml" xref="S3.F2.6.m1.4.5.3.2"><cn type="integer" id="S3.F2.6.m1.1.1.cmml" xref="S3.F2.6.m1.1.1">1</cn><cn type="integer" id="S3.F2.6.m1.2.2.cmml" xref="S3.F2.6.m1.2.2">5</cn><cn type="integer" id="S3.F2.6.m1.3.3.cmml" xref="S3.F2.6.m1.3.3">10</cn><cn type="integer" id="S3.F2.6.m1.4.4.cmml" xref="S3.F2.6.m1.4.4">100</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.6.m1.4d">\alpha\in\{1,5,10,100\}</annotation></semantics></math>.
Each sub-figure is a 100x100 symmetric matrix.
The intensity of a cell <math id="S3.F2.7.m2.2" class="ltx_Math" alttext="(i,j)" display="inline"><semantics id="S3.F2.7.m2.2b"><mrow id="S3.F2.7.m2.2.3.2" xref="S3.F2.7.m2.2.3.1.cmml"><mo stretchy="false" id="S3.F2.7.m2.2.3.2.1" xref="S3.F2.7.m2.2.3.1.cmml">(</mo><mi id="S3.F2.7.m2.1.1" xref="S3.F2.7.m2.1.1.cmml">i</mi><mo id="S3.F2.7.m2.2.3.2.2" xref="S3.F2.7.m2.2.3.1.cmml">,</mo><mi id="S3.F2.7.m2.2.2" xref="S3.F2.7.m2.2.2.cmml">j</mi><mo stretchy="false" id="S3.F2.7.m2.2.3.2.3" xref="S3.F2.7.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.7.m2.2c"><interval closure="open" id="S3.F2.7.m2.2.3.1.cmml" xref="S3.F2.7.m2.2.3.2"><ci id="S3.F2.7.m2.1.1.cmml" xref="S3.F2.7.m2.1.1">𝑖</ci><ci id="S3.F2.7.m2.2.2.cmml" xref="S3.F2.7.m2.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.7.m2.2d">(i,j)</annotation></semantics></math>’s color here represents the distance between the label distribution of Client <math id="S3.F2.8.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.F2.8.m3.1b"><mi id="S3.F2.8.m3.1.1" xref="S3.F2.8.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.m3.1c"><ci id="S3.F2.8.m3.1.1.cmml" xref="S3.F2.8.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.m3.1d">i</annotation></semantics></math> and <math id="S3.F2.9.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.F2.9.m4.1b"><mi id="S3.F2.9.m4.1.1" xref="S3.F2.9.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.F2.9.m4.1c"><ci id="S3.F2.9.m4.1.1.cmml" xref="S3.F2.9.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.9.m4.1d">j</annotation></semantics></math>.
It is expected that when <math id="S3.F2.10.m5.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F2.10.m5.1b"><mi id="S3.F2.10.m5.1.1" xref="S3.F2.10.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F2.10.m5.1c"><ci id="S3.F2.10.m5.1.1.cmml" xref="S3.F2.10.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.10.m5.1d">\alpha</annotation></semantics></math> is smaller, the partition over clients is more non-IID in terms of their label distributions. </figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Controlling non-IID Quantity.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.6" class="ltx_p">It is also common that different clients have very different data quantities while sharing similar label distribution.
We thus also provide a quantity-level Dirichlet allocation <math id="S3.SS2.SSS0.Px2.p1.1.m1.2" class="ltx_Math" alttext="\bm{z}\sim\operatorname{Dir}_{N}(\beta)" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.2a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3.cmml">𝒛</mi><mo id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml">∼</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml"><msub id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2.cmml">Dir</mi><mi id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3.cmml">N</mi></msub><mo id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1a" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml">⁡</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">β</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.2b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2">similar-to</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3">𝒛</ci><apply id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2">Dir</ci><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3">𝑁</ci></apply><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝛽</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.2c">\bm{z}\sim\operatorname{Dir}_{N}(\beta)</annotation></semantics></math> where <math id="S3.SS2.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.1c">N</annotation></semantics></math> is the number of clients.
Then, we can allocate examples in a global dataset to all clients according to the distribution <math id="S3.SS2.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="\bm{z}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">𝒛</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">𝒛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">\bm{z}</annotation></semantics></math> — i.e., <math id="S3.SS2.SSS0.Px2.p1.4.m4.2" class="ltx_Math" alttext="|\mathcal{D}_{i}|=z_{i}|\mathcal{D}_{G}|" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.2a"><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.2.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.2.1.cmml">|</mo><msub id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.2.cmml">𝒟</mi><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.2.1.cmml">|</mo></mrow><mo id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.3.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.cmml"><msub id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.2.cmml">z</mi><mi id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.cmml">​</mo><mrow id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.2.1.cmml">|</mo><msub id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.2.cmml">𝒟</mi><mi id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.3.cmml">G</mi></msub><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.2.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.2b"><apply id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2"><eq id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.3"></eq><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1"><abs id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.2"></abs><apply id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.2">𝒟</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2"><times id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.2"></times><apply id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.2">𝑧</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.3.3">𝑖</ci></apply><apply id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1"><abs id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.2"></abs><apply id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.2">𝒟</ci><ci id="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.2.2.2.1.1.1.3">𝐺</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.2c">|\mathcal{D}_{i}|=z_{i}|\mathcal{D}_{G}|</annotation></semantics></math>.
If we would like to model both quantity and label distribution shift, it is also easy to combine both factors.
Note that one could assume it is a uniform distribution <math id="S3.SS2.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\bm{z}\sim U(N)" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.cmml">𝒛</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1.cmml">∼</mo><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.1.cmml">​</mo><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.3.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.3.2.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.cmml">(</mo><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml">N</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.3.2.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2"><csymbol cd="latexml" id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.1">similar-to</csymbol><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.2">𝒛</ci><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3"><times id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.1"></times><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.2.3.2">𝑈</ci><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1">𝑁</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">\bm{z}\sim U(N)</annotation></semantics></math>, (or <math id="S3.SS2.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="\beta\rightarrow\infty" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.1a"><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml">β</mi><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml">→</mo><mi mathvariant="normal" id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1"><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.1">→</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.2">𝛽</ci><infinity id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.3"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.1c">\beta\rightarrow\infty</annotation></semantics></math>) if we expect all clients to share a similar number of examples.
A concrete example is shown in Figure <a href="#A2.F8" title="Figure 8 ‣ Приложение B Basic Formulations of NLP Tasks ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (Appendix).</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Controlling non-IID Features. </h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.1" class="ltx_p">Although straightforward and effective, the above label-based Dirichlet allocation method has a major limitation — it is only suitable for text classification tasks where the outputs can be modeled as category-based random variables.
To create synthetic partitions for other non-classification NLP tasks and model distribution shifts,
we thus propose a partition method based on feature clustering.
Specifically,
we use SentenceBERT <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite> to encode each example to a dense vector by their text then we apply K-Means clustering to get the cluster label of each example; finally, we use these cluster labels (as if they were classification tasks) to follow the steps in modeling <span id="S3.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">label distribution shift</span>.
There are two obvious benefits of this clustering-based Dirichlet partition method:
1) It enables us to easily synthesize the FL datasets for non-classification tasks (i.e., ST, QA, SS) as they do not have discrete labels as output space;
2) The BERT-based clustering results naturally imply different sub-topics of a dataset, and thus feature shift can be seen as a shift of latent labels — we can reuse the same method for the label-based Dirichlet partition method.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2104.08815/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="313" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 3: </span> Visualizing the <span id="S3.F3.11.1" class="ltx_text ltx_font_bold">non-IID label distributions</span> on <span id="S3.F3.12.2" class="ltx_text ltx_font_italic">20News</span> with <math id="S3.F3.5.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F3.5.m1.1b"><mi id="S3.F3.5.m1.1.1" xref="S3.F3.5.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F3.5.m1.1c"><ci id="S3.F3.5.m1.1.1.cmml" xref="S3.F3.5.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.5.m1.1d">\alpha</annotation></semantics></math> being <math id="S3.F3.6.m2.4" class="ltx_Math" alttext="\{1,5,10,100\}" display="inline"><semantics id="S3.F3.6.m2.4b"><mrow id="S3.F3.6.m2.4.5.2" xref="S3.F3.6.m2.4.5.1.cmml"><mo stretchy="false" id="S3.F3.6.m2.4.5.2.1" xref="S3.F3.6.m2.4.5.1.cmml">{</mo><mn id="S3.F3.6.m2.1.1" xref="S3.F3.6.m2.1.1.cmml">1</mn><mo id="S3.F3.6.m2.4.5.2.2" xref="S3.F3.6.m2.4.5.1.cmml">,</mo><mn id="S3.F3.6.m2.2.2" xref="S3.F3.6.m2.2.2.cmml">5</mn><mo id="S3.F3.6.m2.4.5.2.3" xref="S3.F3.6.m2.4.5.1.cmml">,</mo><mn id="S3.F3.6.m2.3.3" xref="S3.F3.6.m2.3.3.cmml">10</mn><mo id="S3.F3.6.m2.4.5.2.4" xref="S3.F3.6.m2.4.5.1.cmml">,</mo><mn id="S3.F3.6.m2.4.4" xref="S3.F3.6.m2.4.4.cmml">100</mn><mo stretchy="false" id="S3.F3.6.m2.4.5.2.5" xref="S3.F3.6.m2.4.5.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.6.m2.4c"><set id="S3.F3.6.m2.4.5.1.cmml" xref="S3.F3.6.m2.4.5.2"><cn type="integer" id="S3.F3.6.m2.1.1.cmml" xref="S3.F3.6.m2.1.1">1</cn><cn type="integer" id="S3.F3.6.m2.2.2.cmml" xref="S3.F3.6.m2.2.2">5</cn><cn type="integer" id="S3.F3.6.m2.3.3.cmml" xref="S3.F3.6.m2.3.3">10</cn><cn type="integer" id="S3.F3.6.m2.4.4.cmml" xref="S3.F3.6.m2.4.4">100</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.6.m2.4d">\{1,5,10,100\}</annotation></semantics></math>. Each sub-figure is a 100x20 matrix, where 100 is the number of clients, and 20 is the number of labels. The intensity of a cell here represents the ratio of a particular label in the local data of a client.
When <math id="S3.F3.7.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.F3.7.m3.1b"><mi id="S3.F3.7.m3.1.1" xref="S3.F3.7.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F3.7.m3.1c"><ci id="S3.F3.7.m3.1.1.cmml" xref="S3.F3.7.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.m3.1d">\alpha</annotation></semantics></math> is smaller (1, 5, 10), each client has a relatively unique label distribution, thus the differences between clients are larger; when <math id="S3.F3.8.m4.1" class="ltx_Math" alttext="\alpha=100" display="inline"><semantics id="S3.F3.8.m4.1b"><mrow id="S3.F3.8.m4.1.1" xref="S3.F3.8.m4.1.1.cmml"><mi id="S3.F3.8.m4.1.1.2" xref="S3.F3.8.m4.1.1.2.cmml">α</mi><mo id="S3.F3.8.m4.1.1.1" xref="S3.F3.8.m4.1.1.1.cmml">=</mo><mn id="S3.F3.8.m4.1.1.3" xref="S3.F3.8.m4.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.8.m4.1c"><apply id="S3.F3.8.m4.1.1.cmml" xref="S3.F3.8.m4.1.1"><eq id="S3.F3.8.m4.1.1.1.cmml" xref="S3.F3.8.m4.1.1.1"></eq><ci id="S3.F3.8.m4.1.1.2.cmml" xref="S3.F3.8.m4.1.1.2">𝛼</ci><cn type="integer" id="S3.F3.8.m4.1.1.3.cmml" xref="S3.F3.8.m4.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.m4.1d">\alpha=100</annotation></semantics></math>, every client has a nearly uniform label distribution.
</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.4.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="S3.T2.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.3.4.2.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S3.T2.3.4.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.4.3.1" class="ltx_text ltx_font_bold">Partition</span></td>
<td id="S3.T2.3.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.3.4.4.1" class="ltx_text ltx_font_bold">Clients</span></td>
<td id="S3.T2.3.4.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.4.5.1" class="ltx_text ltx_font_typewriter">FedAvg</span></td>
<td id="S3.T2.3.4.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.4.6.1" class="ltx_text ltx_font_typewriter">FedProx</span></td>
<td id="S3.T2.3.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.3.4.7.1" class="ltx_text ltx_font_typewriter">FedOPT</span></td>
<td id="S3.T2.3.4.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T2.3.4.8.1" class="ltx_text ltx_font_bold"># Rounds</span></td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Text Classification</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20news</td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<math id="S3.T2.1.1.1.m1.1" class="ltx_Math" alttext="\alpha=" display="inline"><semantics id="S3.T2.1.1.1.m1.1a"><mrow id="S3.T2.1.1.1.m1.1.1" xref="S3.T2.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.m1.1.1.2.cmml">α</mi><mo id="S3.T2.1.1.1.m1.1.1.1" xref="S3.T2.1.1.1.m1.1.1.1.cmml">=</mo><mi id="S3.T2.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1"><eq id="S3.T2.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1.1"></eq><ci id="S3.T2.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.m1.1.1.2">𝛼</ci><csymbol cd="latexml" id="S3.T2.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\alpha=</annotation></semantics></math>1 (label shift)</td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</td>
<td id="S3.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_t">0.5142</td>
<td id="S3.T2.1.1.6" class="ltx_td ltx_align_center ltx_border_t">0.5143</td>
<td id="S3.T2.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.5349</td>
<td id="S3.T2.1.1.8" class="ltx_td ltx_align_center ltx_border_t">22</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_center">Sequence Tagging</td>
<td id="S3.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">OntoNotes</td>
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_center">
<math id="S3.T2.2.2.1.m1.1" class="ltx_Math" alttext="\alpha=" display="inline"><semantics id="S3.T2.2.2.1.m1.1a"><mrow id="S3.T2.2.2.1.m1.1.1" xref="S3.T2.2.2.1.m1.1.1.cmml"><mi id="S3.T2.2.2.1.m1.1.1.2" xref="S3.T2.2.2.1.m1.1.1.2.cmml">α</mi><mo id="S3.T2.2.2.1.m1.1.1.1" xref="S3.T2.2.2.1.m1.1.1.1.cmml">=</mo><mi id="S3.T2.2.2.1.m1.1.1.3" xref="S3.T2.2.2.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.m1.1b"><apply id="S3.T2.2.2.1.m1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1"><eq id="S3.T2.2.2.1.m1.1.1.1.cmml" xref="S3.T2.2.2.1.m1.1.1.1"></eq><ci id="S3.T2.2.2.1.m1.1.1.2.cmml" xref="S3.T2.2.2.1.m1.1.1.2">𝛼</ci><csymbol cd="latexml" id="S3.T2.2.2.1.m1.1.1.3.cmml" xref="S3.T2.2.2.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.m1.1c">\alpha=</annotation></semantics></math>0.1 (label shift)</td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">30</td>
<td id="S3.T2.2.2.5" class="ltx_td ltx_align_center">0.7382</td>
<td id="S3.T2.2.2.6" class="ltx_td ltx_align_center">0.6731</td>
<td id="S3.T2.2.2.7" class="ltx_td ltx_align_center ltx_border_r">0.7918</td>
<td id="S3.T2.2.2.8" class="ltx_td ltx_align_center">17</td>
</tr>
<tr id="S3.T2.3.5" class="ltx_tr">
<td id="S3.T2.3.5.1" class="ltx_td ltx_align_center">Question Answering</td>
<td id="S3.T2.3.5.2" class="ltx_td ltx_align_center ltx_border_r">MRQA</td>
<td id="S3.T2.3.5.3" class="ltx_td ltx_align_center">natural factor</td>
<td id="S3.T2.3.5.4" class="ltx_td ltx_align_center ltx_border_r">6</td>
<td id="S3.T2.3.5.5" class="ltx_td ltx_align_center">0.2707</td>
<td id="S3.T2.3.5.6" class="ltx_td ltx_align_center">0.2706</td>
<td id="S3.T2.3.5.7" class="ltx_td ltx_align_center ltx_border_r">0.3280</td>
<td id="S3.T2.3.5.8" class="ltx_td ltx_align_center">13</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_center ltx_border_bb">Seq2Seq Generation</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Gigaword</td>
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_bb">
<math id="S3.T2.3.3.1.m1.1" class="ltx_Math" alttext="\alpha=" display="inline"><semantics id="S3.T2.3.3.1.m1.1a"><mrow id="S3.T2.3.3.1.m1.1.1" xref="S3.T2.3.3.1.m1.1.1.cmml"><mi id="S3.T2.3.3.1.m1.1.1.2" xref="S3.T2.3.3.1.m1.1.1.2.cmml">α</mi><mo id="S3.T2.3.3.1.m1.1.1.1" xref="S3.T2.3.3.1.m1.1.1.1.cmml">=</mo><mi id="S3.T2.3.3.1.m1.1.1.3" xref="S3.T2.3.3.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.m1.1b"><apply id="S3.T2.3.3.1.m1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1"><eq id="S3.T2.3.3.1.m1.1.1.1.cmml" xref="S3.T2.3.3.1.m1.1.1.1"></eq><ci id="S3.T2.3.3.1.m1.1.1.2.cmml" xref="S3.T2.3.3.1.m1.1.1.2">𝛼</ci><csymbol cd="latexml" id="S3.T2.3.3.1.m1.1.1.3.cmml" xref="S3.T2.3.3.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.m1.1c">\alpha=</annotation></semantics></math>0.1 (feature shift)</td>
<td id="S3.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">100</td>
<td id="S3.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_bb">0.3192</td>
<td id="S3.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_bb">0.3169</td>
<td id="S3.T2.3.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.3037</td>
<td id="S3.T2.3.3.8" class="ltx_td ltx_align_center ltx_border_bb">13</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Таблица 2: </span>The comparisons between different FL methods under the same setting on different NLP tasks. The number of workers per round are 10, expect for the MRQA task, which uses 6. </figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2104.08815/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 4: </span>The learning curves of the three FL Methods on four different task formulations. The metrics used for these tasks are accuracy, span-F1, token-F1, and ROUGE respectively; The x-axis is the number of rounds.</figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Natural Factors</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.1" class="ltx_p">For datasets like MRQA,
we consider a cross-silo setting where each client is associated with a particular sub-dataset (out of the six datasets of the same format), forming a natural distribution shift based on the inherent factors such as data source and annotating style.

</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results and Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we aim to analyze typical federated learning methods (introduced in our benchmark datasets with multiple dimensions with the base NLP models listed previously.
We put more implementation details and additional results in Appendix.
We organize our extensive experimental results and findings from the analysis as a collection of research questions with answers.</p>
</div>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Experimental Setup and Hyper-parameters.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">We use DistilBERT and BART-base for most of our experiments,
as the former is a distilled version of the BERT model and has a 7x speed improvement over BERT-base on mobile devices — a common scenario for FL applications; the BART-base model is the most suitable option considering the trade-off between performance and computation cost.
We leave our implementation details and the selected hyper-parameters in the submitted supplementary materials.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">Our experiments cover both cross-device and cross-silo settings. As shown in Table <a href="#S3.T2" title="Table 2 ‣ Controlling non-IID Features. ‣ 3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, in the cross-device setting, we use uniform sampling to select 10 clients for each round when the client number in a dataset is very large (e.g., 100). For the cross-silo setting, each round will select the same number of clients (we use 6 for the QA task). The local epoch number is set to 1 for all experiments.
To make our results reproducible, we use <span id="S4.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">wandb.ai</span> to store all experiment logs and hyper-parameters as well as running scripts.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p3" class="ltx_para">
<span id="S4.SS0.SSS0.Px1.p3.1" class="ltx_ERROR undefined">{bclogo}</span>
<p id="S4.SS0.SSS0.Px1.p3.2" class="ltx_p">[couleur= msftBlack!03, arrondi=0, logo=, marge=4, couleurBord=msftBlack!10, sousTitre =<span id="S4.SS0.SSS0.Px1.p3.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Q1: How do popular FL methods perform differently under the same setting?</span>]</p>
</div>
<div id="S4.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p4.1" class="ltx_p">We compare the three typical FL methods under the same setting (i.e., data partition, communication rounds, etc.) for each task formulation.
As shown in Table <a href="#S3.T2" title="Table 2 ‣ Controlling non-IID Features. ‣ 3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
we report the results of FedAvg, FedProx, and FedOPT.
We can see that overall FedOPT performs better than the other two methods, with the only exception being in the seq2seq generation task.
FedAvg and FedProx perform similarly with marginal differences, but FedAvg outperforms FedProx in sequence tagging.
These two exceptions are surprising findings, as many prior works in the FL community show that FedOPT is generally better than FedProx and FedAvg on vision tasks and datasets.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p5" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p5.1" class="ltx_p">We conjecture that such inconsistent performance across tasks suggests the difference in terms of the loss functions has a great impact on FL performance.
Seq2seq and sequence tagging tasks usually have more complex loss landscapes than text classification, as they are both typical structured prediction tasks, while the text classification has a much smaller output space.
From Fig. <a href="#S3.F4" title="Figure 4 ‣ Controlling non-IID Features. ‣ 3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>,
we see that the FedOPT outperforms the other two methods at the beginning while gradually becoming worse over time.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p6" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p6.1" class="ltx_p">This tells us that the use of AdamW as the client optimizer may not always be a good choice, especially for a complex task such as the Seq2Seq ones, as its adaptive method for scheduling learning rates might cause implicit conflicts. These observations suggest that federated optimization algorithms need to be tailored for various NLP tasks, and exploring FL-friendly model architecture or loss function can also be promising directions to address these challenges.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2104.08815/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 5: </span>Testing FedOPT with <span id="S4.F5.3.1" class="ltx_text ltx_font_typewriter">DistilBERT</span> for <span id="S4.F5.4.2" class="ltx_text ltx_font_typewriter">20News</span> under different data partition strategies. </figcaption>
</figure>
<div id="S4.SS0.SSS0.Px1.p7" class="ltx_para">
<span id="S4.SS0.SSS0.Px1.p7.1" class="ltx_ERROR undefined">{bclogo}</span>
<p id="S4.SS0.SSS0.Px1.p7.2" class="ltx_p">[couleur= msftBlack!03, arrondi=0, logo=, marge=4, couleurBord=msftBlack!10, sousTitre =<span id="S4.SS0.SSS0.Px1.p7.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Q2: How do different non-IID partitions of the same data influence FL performance?</span>]</p>
</div>
<div id="S4.SS0.SSS0.Px1.p8" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p8.1" class="ltx_p">The FedNLP platform supports users to investigate the performance of an FL algorithm with a wide range of data partitioning strategies, as discussed in §<a href="#S3.SS2" title="3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.
Here we look at the training curves of the FedOPT on different partitions, as shown in Figure <a href="#S4.F5" title="Figure 5 ‣ Experimental Setup and Hyper-parameters. ‣ 4 Experimental Results and Analysis ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
We reveal several findings:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.2" class="ltx_p">When <math id="S4.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.I1.i1.p1.1.m1.1a"><mi id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><ci id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">\alpha</annotation></semantics></math> is smaller (i.e., the partition is more non-IID in terms of their label distribution), the performance tends to degrade, based on the three curves (<math id="S4.I1.i1.p1.2.m2.3" class="ltx_Math" alttext="\alpha=\{1,5,10\}" display="inline"><semantics id="S4.I1.i1.p1.2.m2.3a"><mrow id="S4.I1.i1.p1.2.m2.3.4" xref="S4.I1.i1.p1.2.m2.3.4.cmml"><mi id="S4.I1.i1.p1.2.m2.3.4.2" xref="S4.I1.i1.p1.2.m2.3.4.2.cmml">α</mi><mo id="S4.I1.i1.p1.2.m2.3.4.1" xref="S4.I1.i1.p1.2.m2.3.4.1.cmml">=</mo><mrow id="S4.I1.i1.p1.2.m2.3.4.3.2" xref="S4.I1.i1.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S4.I1.i1.p1.2.m2.3.4.3.2.1" xref="S4.I1.i1.p1.2.m2.3.4.3.1.cmml">{</mo><mn id="S4.I1.i1.p1.2.m2.1.1" xref="S4.I1.i1.p1.2.m2.1.1.cmml">1</mn><mo id="S4.I1.i1.p1.2.m2.3.4.3.2.2" xref="S4.I1.i1.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S4.I1.i1.p1.2.m2.2.2" xref="S4.I1.i1.p1.2.m2.2.2.cmml">5</mn><mo id="S4.I1.i1.p1.2.m2.3.4.3.2.3" xref="S4.I1.i1.p1.2.m2.3.4.3.1.cmml">,</mo><mn id="S4.I1.i1.p1.2.m2.3.3" xref="S4.I1.i1.p1.2.m2.3.3.cmml">10</mn><mo stretchy="false" id="S4.I1.i1.p1.2.m2.3.4.3.2.4" xref="S4.I1.i1.p1.2.m2.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.2.m2.3b"><apply id="S4.I1.i1.p1.2.m2.3.4.cmml" xref="S4.I1.i1.p1.2.m2.3.4"><eq id="S4.I1.i1.p1.2.m2.3.4.1.cmml" xref="S4.I1.i1.p1.2.m2.3.4.1"></eq><ci id="S4.I1.i1.p1.2.m2.3.4.2.cmml" xref="S4.I1.i1.p1.2.m2.3.4.2">𝛼</ci><set id="S4.I1.i1.p1.2.m2.3.4.3.1.cmml" xref="S4.I1.i1.p1.2.m2.3.4.3.2"><cn type="integer" id="S4.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1">1</cn><cn type="integer" id="S4.I1.i1.p1.2.m2.2.2.cmml" xref="S4.I1.i1.p1.2.m2.2.2">5</cn><cn type="integer" id="S4.I1.i1.p1.2.m2.3.3.cmml" xref="S4.I1.i1.p1.2.m2.3.3">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.2.m2.3c">\alpha=\{1,5,10\}</annotation></semantics></math>).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">The variance is also larger when the label distribution shift is larger.
Both uniform and quantity-skew partitions have a smoother curve, while the variance is smaller for a larger <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mi id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><ci id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">\alpha</annotation></semantics></math> (e.g., 10).</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Quantity skew does not introduce a great challenge for federated learning when the label distribution is closer to the uniform one.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS0.SSS0.Px1.p9" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p9.1" class="ltx_p">These findings suggest that it is important to design algorithms to mitigate data heterogeneity.
One promising direction is <span id="S4.SS0.SSS0.Px1.p9.1.1" class="ltx_text ltx_font_italic">personalized</span> FL, which enables each client to learn its personalized model via adapting its local data distribution and system resources <cite class="ltx_cite ltx_citemacro_citep">(Dinh et al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>; Fallah et al., <a href="#bib.bib12" title="" class="ltx_ref">2020</a>; Li et al., <a href="#bib.bib32" title="" class="ltx_ref">2021b</a>)</cite>.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.7" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.7.8" class="ltx_tr">
<td id="S4.T3.7.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Frozen Layers</td>
<td id="S4.T3.7.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"># Tunable Paras.</td>
<td id="S4.T3.7.8.3" class="ltx_td ltx_align_center ltx_border_tt">Cent.</td>
<td id="S4.T3.7.8.4" class="ltx_td ltx_align_center ltx_border_tt">FedOpt.</td>
</tr>
<tr id="S4.T3.7.9" class="ltx_tr">
<td id="S4.T3.7.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T3.7.9.1.1" class="ltx_text ltx_font_typewriter">None</span></td>
<td id="S4.T3.7.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.0M</td>
<td id="S4.T3.7.9.3" class="ltx_td ltx_align_center ltx_border_t">86.86</td>
<td id="S4.T3.7.9.4" class="ltx_td ltx_align_center ltx_border_t">55.11</td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mi id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">E</annotation></semantics></math></td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r">43.1M</td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center">86.19</td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center">54.86</td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<td id="S4.T3.2.2.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.2.2.1.m1.1" class="ltx_Math" alttext="E+L_{0}" display="inline"><semantics id="S4.T3.2.2.1.m1.1a"><mrow id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml"><mi id="S4.T3.2.2.1.m1.1.1.2" xref="S4.T3.2.2.1.m1.1.1.2.cmml">E</mi><mo id="S4.T3.2.2.1.m1.1.1.1" xref="S4.T3.2.2.1.m1.1.1.1.cmml">+</mo><msub id="S4.T3.2.2.1.m1.1.1.3" xref="S4.T3.2.2.1.m1.1.1.3.cmml"><mi id="S4.T3.2.2.1.m1.1.1.3.2" xref="S4.T3.2.2.1.m1.1.1.3.2.cmml">L</mi><mn id="S4.T3.2.2.1.m1.1.1.3.3" xref="S4.T3.2.2.1.m1.1.1.3.3.cmml">0</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><apply id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1"><plus id="S4.T3.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1.1"></plus><ci id="S4.T3.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.1.m1.1.1.2">𝐸</ci><apply id="S4.T3.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.2.2.1.m1.1.1.3.1.cmml" xref="S4.T3.2.2.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.2.2.1.m1.1.1.3.2.cmml" xref="S4.T3.2.2.1.m1.1.1.3.2">𝐿</ci><cn type="integer" id="S4.T3.2.2.1.m1.1.1.3.3.cmml" xref="S4.T3.2.2.1.m1.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">E+L_{0}</annotation></semantics></math></td>
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_r">36.0M</td>
<td id="S4.T3.2.2.3" class="ltx_td ltx_align_center">86.54</td>
<td id="S4.T3.2.2.4" class="ltx_td ltx_align_center">52.91</td>
</tr>
<tr id="S4.T3.3.3" class="ltx_tr">
<td id="S4.T3.3.3.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.3.3.1.m1.1" class="ltx_Math" alttext="E+L_{0\rightarrow 1}" display="inline"><semantics id="S4.T3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.1.m1.1.1" xref="S4.T3.3.3.1.m1.1.1.cmml"><mi id="S4.T3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.1.m1.1.1.2.cmml">E</mi><mo id="S4.T3.3.3.1.m1.1.1.1" xref="S4.T3.3.3.1.m1.1.1.1.cmml">+</mo><msub id="S4.T3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.1.m1.1.1.3.cmml"><mi id="S4.T3.3.3.1.m1.1.1.3.2" xref="S4.T3.3.3.1.m1.1.1.3.2.cmml">L</mi><mrow id="S4.T3.3.3.1.m1.1.1.3.3" xref="S4.T3.3.3.1.m1.1.1.3.3.cmml"><mn id="S4.T3.3.3.1.m1.1.1.3.3.2" xref="S4.T3.3.3.1.m1.1.1.3.3.2.cmml">0</mn><mo stretchy="false" id="S4.T3.3.3.1.m1.1.1.3.3.1" xref="S4.T3.3.3.1.m1.1.1.3.3.1.cmml">→</mo><mn id="S4.T3.3.3.1.m1.1.1.3.3.3" xref="S4.T3.3.3.1.m1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.m1.1b"><apply id="S4.T3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1"><plus id="S4.T3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1.1"></plus><ci id="S4.T3.3.3.1.m1.1.1.2.cmml" xref="S4.T3.3.3.1.m1.1.1.2">𝐸</ci><apply id="S4.T3.3.3.1.m1.1.1.3.cmml" xref="S4.T3.3.3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.3.3.1.m1.1.1.3.1.cmml" xref="S4.T3.3.3.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.3.3.1.m1.1.1.3.2.cmml" xref="S4.T3.3.3.1.m1.1.1.3.2">𝐿</ci><apply id="S4.T3.3.3.1.m1.1.1.3.3.cmml" xref="S4.T3.3.3.1.m1.1.1.3.3"><ci id="S4.T3.3.3.1.m1.1.1.3.3.1.cmml" xref="S4.T3.3.3.1.m1.1.1.3.3.1">→</ci><cn type="integer" id="S4.T3.3.3.1.m1.1.1.3.3.2.cmml" xref="S4.T3.3.3.1.m1.1.1.3.3.2">0</cn><cn type="integer" id="S4.T3.3.3.1.m1.1.1.3.3.3.cmml" xref="S4.T3.3.3.1.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.m1.1c">E+L_{0\rightarrow 1}</annotation></semantics></math></td>
<td id="S4.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">29.0M</td>
<td id="S4.T3.3.3.3" class="ltx_td ltx_align_center">86.52</td>
<td id="S4.T3.3.3.4" class="ltx_td ltx_align_center">53.92</td>
</tr>
<tr id="S4.T3.4.4" class="ltx_tr">
<td id="S4.T3.4.4.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.4.4.1.m1.1" class="ltx_Math" alttext="E+L_{0\rightarrow 2}" display="inline"><semantics id="S4.T3.4.4.1.m1.1a"><mrow id="S4.T3.4.4.1.m1.1.1" xref="S4.T3.4.4.1.m1.1.1.cmml"><mi id="S4.T3.4.4.1.m1.1.1.2" xref="S4.T3.4.4.1.m1.1.1.2.cmml">E</mi><mo id="S4.T3.4.4.1.m1.1.1.1" xref="S4.T3.4.4.1.m1.1.1.1.cmml">+</mo><msub id="S4.T3.4.4.1.m1.1.1.3" xref="S4.T3.4.4.1.m1.1.1.3.cmml"><mi id="S4.T3.4.4.1.m1.1.1.3.2" xref="S4.T3.4.4.1.m1.1.1.3.2.cmml">L</mi><mrow id="S4.T3.4.4.1.m1.1.1.3.3" xref="S4.T3.4.4.1.m1.1.1.3.3.cmml"><mn id="S4.T3.4.4.1.m1.1.1.3.3.2" xref="S4.T3.4.4.1.m1.1.1.3.3.2.cmml">0</mn><mo stretchy="false" id="S4.T3.4.4.1.m1.1.1.3.3.1" xref="S4.T3.4.4.1.m1.1.1.3.3.1.cmml">→</mo><mn id="S4.T3.4.4.1.m1.1.1.3.3.3" xref="S4.T3.4.4.1.m1.1.1.3.3.3.cmml">2</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.1.m1.1b"><apply id="S4.T3.4.4.1.m1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1"><plus id="S4.T3.4.4.1.m1.1.1.1.cmml" xref="S4.T3.4.4.1.m1.1.1.1"></plus><ci id="S4.T3.4.4.1.m1.1.1.2.cmml" xref="S4.T3.4.4.1.m1.1.1.2">𝐸</ci><apply id="S4.T3.4.4.1.m1.1.1.3.cmml" xref="S4.T3.4.4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.4.4.1.m1.1.1.3.1.cmml" xref="S4.T3.4.4.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.4.4.1.m1.1.1.3.2.cmml" xref="S4.T3.4.4.1.m1.1.1.3.2">𝐿</ci><apply id="S4.T3.4.4.1.m1.1.1.3.3.cmml" xref="S4.T3.4.4.1.m1.1.1.3.3"><ci id="S4.T3.4.4.1.m1.1.1.3.3.1.cmml" xref="S4.T3.4.4.1.m1.1.1.3.3.1">→</ci><cn type="integer" id="S4.T3.4.4.1.m1.1.1.3.3.2.cmml" xref="S4.T3.4.4.1.m1.1.1.3.3.2">0</cn><cn type="integer" id="S4.T3.4.4.1.m1.1.1.3.3.3.cmml" xref="S4.T3.4.4.1.m1.1.1.3.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.1.m1.1c">E+L_{0\rightarrow 2}</annotation></semantics></math></td>
<td id="S4.T3.4.4.2" class="ltx_td ltx_align_center ltx_border_r">21.9M</td>
<td id="S4.T3.4.4.3" class="ltx_td ltx_align_center">85.71</td>
<td id="S4.T3.4.4.4" class="ltx_td ltx_align_center">52.01</td>
</tr>
<tr id="S4.T3.5.5" class="ltx_tr">
<td id="S4.T3.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.5.5.1.m1.1" class="ltx_Math" alttext="E+L_{0\rightarrow 3}" display="inline"><semantics id="S4.T3.5.5.1.m1.1a"><mrow id="S4.T3.5.5.1.m1.1.1" xref="S4.T3.5.5.1.m1.1.1.cmml"><mi id="S4.T3.5.5.1.m1.1.1.2" xref="S4.T3.5.5.1.m1.1.1.2.cmml">E</mi><mo id="S4.T3.5.5.1.m1.1.1.1" xref="S4.T3.5.5.1.m1.1.1.1.cmml">+</mo><msub id="S4.T3.5.5.1.m1.1.1.3" xref="S4.T3.5.5.1.m1.1.1.3.cmml"><mi id="S4.T3.5.5.1.m1.1.1.3.2" xref="S4.T3.5.5.1.m1.1.1.3.2.cmml">L</mi><mrow id="S4.T3.5.5.1.m1.1.1.3.3" xref="S4.T3.5.5.1.m1.1.1.3.3.cmml"><mn id="S4.T3.5.5.1.m1.1.1.3.3.2" xref="S4.T3.5.5.1.m1.1.1.3.3.2.cmml">0</mn><mo stretchy="false" id="S4.T3.5.5.1.m1.1.1.3.3.1" xref="S4.T3.5.5.1.m1.1.1.3.3.1.cmml">→</mo><mn id="S4.T3.5.5.1.m1.1.1.3.3.3" xref="S4.T3.5.5.1.m1.1.1.3.3.3.cmml">3</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.1.m1.1b"><apply id="S4.T3.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1"><plus id="S4.T3.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1.1"></plus><ci id="S4.T3.5.5.1.m1.1.1.2.cmml" xref="S4.T3.5.5.1.m1.1.1.2">𝐸</ci><apply id="S4.T3.5.5.1.m1.1.1.3.cmml" xref="S4.T3.5.5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.5.5.1.m1.1.1.3.1.cmml" xref="S4.T3.5.5.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.5.5.1.m1.1.1.3.2.cmml" xref="S4.T3.5.5.1.m1.1.1.3.2">𝐿</ci><apply id="S4.T3.5.5.1.m1.1.1.3.3.cmml" xref="S4.T3.5.5.1.m1.1.1.3.3"><ci id="S4.T3.5.5.1.m1.1.1.3.3.1.cmml" xref="S4.T3.5.5.1.m1.1.1.3.3.1">→</ci><cn type="integer" id="S4.T3.5.5.1.m1.1.1.3.3.2.cmml" xref="S4.T3.5.5.1.m1.1.1.3.3.2">0</cn><cn type="integer" id="S4.T3.5.5.1.m1.1.1.3.3.3.cmml" xref="S4.T3.5.5.1.m1.1.1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.1.m1.1c">E+L_{0\rightarrow 3}</annotation></semantics></math></td>
<td id="S4.T3.5.5.2" class="ltx_td ltx_align_center ltx_border_r">14.8M</td>
<td id="S4.T3.5.5.3" class="ltx_td ltx_align_center">85.47</td>
<td id="S4.T3.5.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.5.4.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">30.68</span></td>
</tr>
<tr id="S4.T3.6.6" class="ltx_tr">
<td id="S4.T3.6.6.1" class="ltx_td ltx_align_center ltx_border_r"><math id="S4.T3.6.6.1.m1.1" class="ltx_Math" alttext="E+L_{0\rightarrow 4}" display="inline"><semantics id="S4.T3.6.6.1.m1.1a"><mrow id="S4.T3.6.6.1.m1.1.1" xref="S4.T3.6.6.1.m1.1.1.cmml"><mi id="S4.T3.6.6.1.m1.1.1.2" xref="S4.T3.6.6.1.m1.1.1.2.cmml">E</mi><mo id="S4.T3.6.6.1.m1.1.1.1" xref="S4.T3.6.6.1.m1.1.1.1.cmml">+</mo><msub id="S4.T3.6.6.1.m1.1.1.3" xref="S4.T3.6.6.1.m1.1.1.3.cmml"><mi id="S4.T3.6.6.1.m1.1.1.3.2" xref="S4.T3.6.6.1.m1.1.1.3.2.cmml">L</mi><mrow id="S4.T3.6.6.1.m1.1.1.3.3" xref="S4.T3.6.6.1.m1.1.1.3.3.cmml"><mn id="S4.T3.6.6.1.m1.1.1.3.3.2" xref="S4.T3.6.6.1.m1.1.1.3.3.2.cmml">0</mn><mo stretchy="false" id="S4.T3.6.6.1.m1.1.1.3.3.1" xref="S4.T3.6.6.1.m1.1.1.3.3.1.cmml">→</mo><mn id="S4.T3.6.6.1.m1.1.1.3.3.3" xref="S4.T3.6.6.1.m1.1.1.3.3.3.cmml">4</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.1.m1.1b"><apply id="S4.T3.6.6.1.m1.1.1.cmml" xref="S4.T3.6.6.1.m1.1.1"><plus id="S4.T3.6.6.1.m1.1.1.1.cmml" xref="S4.T3.6.6.1.m1.1.1.1"></plus><ci id="S4.T3.6.6.1.m1.1.1.2.cmml" xref="S4.T3.6.6.1.m1.1.1.2">𝐸</ci><apply id="S4.T3.6.6.1.m1.1.1.3.cmml" xref="S4.T3.6.6.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.6.6.1.m1.1.1.3.1.cmml" xref="S4.T3.6.6.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.6.6.1.m1.1.1.3.2.cmml" xref="S4.T3.6.6.1.m1.1.1.3.2">𝐿</ci><apply id="S4.T3.6.6.1.m1.1.1.3.3.cmml" xref="S4.T3.6.6.1.m1.1.1.3.3"><ci id="S4.T3.6.6.1.m1.1.1.3.3.1.cmml" xref="S4.T3.6.6.1.m1.1.1.3.3.1">→</ci><cn type="integer" id="S4.T3.6.6.1.m1.1.1.3.3.2.cmml" xref="S4.T3.6.6.1.m1.1.1.3.3.2">0</cn><cn type="integer" id="S4.T3.6.6.1.m1.1.1.3.3.3.cmml" xref="S4.T3.6.6.1.m1.1.1.3.3.3">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.1.m1.1c">E+L_{0\rightarrow 4}</annotation></semantics></math></td>
<td id="S4.T3.6.6.2" class="ltx_td ltx_align_center ltx_border_r">7.7M</td>
<td id="S4.T3.6.6.3" class="ltx_td ltx_align_center">82.76</td>
<td id="S4.T3.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.6.4.1" class="ltx_text ltx_framed ltx_framed_underline">16.63</span></td>
</tr>
<tr id="S4.T3.7.7" class="ltx_tr">
<td id="S4.T3.7.7.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><math id="S4.T3.7.7.1.m1.1" class="ltx_Math" alttext="E+L_{0\rightarrow 5}" display="inline"><semantics id="S4.T3.7.7.1.m1.1a"><mrow id="S4.T3.7.7.1.m1.1.1" xref="S4.T3.7.7.1.m1.1.1.cmml"><mi id="S4.T3.7.7.1.m1.1.1.2" xref="S4.T3.7.7.1.m1.1.1.2.cmml">E</mi><mo id="S4.T3.7.7.1.m1.1.1.1" xref="S4.T3.7.7.1.m1.1.1.1.cmml">+</mo><msub id="S4.T3.7.7.1.m1.1.1.3" xref="S4.T3.7.7.1.m1.1.1.3.cmml"><mi id="S4.T3.7.7.1.m1.1.1.3.2" xref="S4.T3.7.7.1.m1.1.1.3.2.cmml">L</mi><mrow id="S4.T3.7.7.1.m1.1.1.3.3" xref="S4.T3.7.7.1.m1.1.1.3.3.cmml"><mn id="S4.T3.7.7.1.m1.1.1.3.3.2" xref="S4.T3.7.7.1.m1.1.1.3.3.2.cmml">0</mn><mo stretchy="false" id="S4.T3.7.7.1.m1.1.1.3.3.1" xref="S4.T3.7.7.1.m1.1.1.3.3.1.cmml">→</mo><mn id="S4.T3.7.7.1.m1.1.1.3.3.3" xref="S4.T3.7.7.1.m1.1.1.3.3.3.cmml">5</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.1.m1.1b"><apply id="S4.T3.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1"><plus id="S4.T3.7.7.1.m1.1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1.1"></plus><ci id="S4.T3.7.7.1.m1.1.1.2.cmml" xref="S4.T3.7.7.1.m1.1.1.2">𝐸</ci><apply id="S4.T3.7.7.1.m1.1.1.3.cmml" xref="S4.T3.7.7.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.7.7.1.m1.1.1.3.1.cmml" xref="S4.T3.7.7.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.7.7.1.m1.1.1.3.2.cmml" xref="S4.T3.7.7.1.m1.1.1.3.2">𝐿</ci><apply id="S4.T3.7.7.1.m1.1.1.3.3.cmml" xref="S4.T3.7.7.1.m1.1.1.3.3"><ci id="S4.T3.7.7.1.m1.1.1.3.3.1.cmml" xref="S4.T3.7.7.1.m1.1.1.3.3.1">→</ci><cn type="integer" id="S4.T3.7.7.1.m1.1.1.3.3.2.cmml" xref="S4.T3.7.7.1.m1.1.1.3.3.2">0</cn><cn type="integer" id="S4.T3.7.7.1.m1.1.1.3.3.3.cmml" xref="S4.T3.7.7.1.m1.1.1.3.3.3">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.1.m1.1c">E+L_{0\rightarrow 5}</annotation></semantics></math></td>
<td id="S4.T3.7.7.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.6M</td>
<td id="S4.T3.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.3.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">63.83</span></td>
<td id="S4.T3.7.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.7.7.4.1" class="ltx_text ltx_framed ltx_framed_underline">12.97</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Таблица 3: </span>
Performance (Acc.%) on 20news (TC) when different parts of <span id="S4.T3.16.1" class="ltx_text ltx_font_typewriter">DistilBERT</span> are frozen for centralized training and FedOpt (at 28-th round). <math id="S4.T3.11.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.T3.11.m1.1b"><mi id="S4.T3.11.m1.1.1" xref="S4.T3.11.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.T3.11.m1.1c"><ci id="S4.T3.11.m1.1.1.cmml" xref="S4.T3.11.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.11.m1.1d">E</annotation></semantics></math> stands for the embedding layer and <math id="S4.T3.12.m2.1" class="ltx_Math" alttext="L_{i}" display="inline"><semantics id="S4.T3.12.m2.1b"><msub id="S4.T3.12.m2.1.1" xref="S4.T3.12.m2.1.1.cmml"><mi id="S4.T3.12.m2.1.1.2" xref="S4.T3.12.m2.1.1.2.cmml">L</mi><mi id="S4.T3.12.m2.1.1.3" xref="S4.T3.12.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.T3.12.m2.1c"><apply id="S4.T3.12.m2.1.1.cmml" xref="S4.T3.12.m2.1.1"><csymbol cd="ambiguous" id="S4.T3.12.m2.1.1.1.cmml" xref="S4.T3.12.m2.1.1">subscript</csymbol><ci id="S4.T3.12.m2.1.1.2.cmml" xref="S4.T3.12.m2.1.1.2">𝐿</ci><ci id="S4.T3.12.m2.1.1.3.cmml" xref="S4.T3.12.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.12.m2.1d">L_{i}</annotation></semantics></math> means the <math id="S4.T3.13.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.T3.13.m3.1b"><mi id="S4.T3.13.m3.1.1" xref="S4.T3.13.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.T3.13.m3.1c"><ci id="S4.T3.13.m3.1.1.cmml" xref="S4.T3.13.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.13.m3.1d">i</annotation></semantics></math>-th layer. The significant lower accuracy are <span id="S4.T3.17.2" class="ltx_text ltx_framed ltx_framed_underline">underlined</span>.
</figcaption>
</figure>
<div id="S4.SS0.SSS0.Px1.p10" class="ltx_para">
<span id="S4.SS0.SSS0.Px1.p10.1" class="ltx_ERROR undefined">{bclogo}</span>
<p id="S4.SS0.SSS0.Px1.p10.2" class="ltx_p">[couleur= msftBlack!03, arrondi=0, logo=, marge=4, couleurBord=msftBlack!10, sousTitre =<span id="S4.SS0.SSS0.Px1.p10.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Q3: How does freezing of Transformers influence the FL performance?</span>]</p>
</div>
<div id="S4.SS0.SSS0.Px1.p11" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p11.2" class="ltx_p">Communication cost is a major concern in the federated learning process.
It is thus natural to consider freezing some Transformer layers of the client models to reduce the size of the trainable parameters that will be transmitted between servers and clients.
To study the influence of freezing layers on the FL performance,
we conduct a series of experiments that freeze the layers from the embedding layer (<math id="S4.SS0.SSS0.Px1.p11.1.m1.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S4.SS0.SSS0.Px1.p11.1.m1.1a"><mi id="S4.SS0.SSS0.Px1.p11.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p11.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p11.1.m1.1b"><ci id="S4.SS0.SSS0.Px1.p11.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p11.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p11.1.m1.1c">E</annotation></semantics></math>) to the top layer (<math id="S4.SS0.SSS0.Px1.p11.2.m2.1" class="ltx_Math" alttext="L_{5}" display="inline"><semantics id="S4.SS0.SSS0.Px1.p11.2.m2.1a"><msub id="S4.SS0.SSS0.Px1.p11.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1.cmml"><mi id="S4.SS0.SSS0.Px1.p11.2.m2.1.1.2" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1.2.cmml">L</mi><mn id="S4.SS0.SSS0.Px1.p11.2.m2.1.1.3" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p11.2.m2.1b"><apply id="S4.SS0.SSS0.Px1.p11.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px1.p11.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px1.p11.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1.2">𝐿</ci><cn type="integer" id="S4.SS0.SSS0.Px1.p11.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p11.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p11.2.m2.1c">L_{5}</annotation></semantics></math>) of DistilBERT with both centralized training and FedOPT on the text classification task.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2104.08815/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 6: </span>Testing <span id="S4.F6.4.1" class="ltx_text ltx_font_typewriter">FedOPT</span> with <span id="S4.F6.5.2" class="ltx_text ltx_font_typewriter">DistilBERT</span> for <span id="S4.F6.6.3" class="ltx_text ltx_font_typewriter">20News</span> under different frozen layers. </figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2104.08815/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 7: </span><span id="S4.F7.3.1" class="ltx_text ltx_font_typewriter">FedOPT</span> for <span id="S4.F7.4.2" class="ltx_text ltx_font_typewriter">20News</span> with different LMs. </figcaption>
</figure>
<div id="S4.SS0.SSS0.Px1.p12" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p12.1" class="ltx_p">We report our results in Table <a href="#S4.T3" title="Table 3 ‣ Experimental Setup and Hyper-parameters. ‣ 4 Experimental Results and Analysis ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure <a href="#S4.F6" title="Figure 6 ‣ Experimental Setup and Hyper-parameters. ‣ 4 Experimental Results and Analysis ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
We find that in centralized training, the largest performance gain happens when we unfreeze the last layer, while in FedOPT we have to unfreeze the last three layers to enjoy a comparable performance with the full model.
This suggests that reducing communication costs via freezing some layers of Transformer LMs is feasible, though one should be aware that the experience in centralized training may not generalize to the FL experiments.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p13" class="ltx_para">
<span id="S4.SS0.SSS0.Px1.p13.1" class="ltx_ERROR undefined">{bclogo}</span>
<p id="S4.SS0.SSS0.Px1.p13.2" class="ltx_p">[couleur= msftBlack!03, arrondi=0, logo=, marge=4, couleurBord=msftBlack!10, sousTitre =<span id="S4.SS0.SSS0.Px1.p13.2.1" class="ltx_text ltx_font_bold ltx_font_italic">Q4: Are compact model DistilBERT adequate for FL+NLP?</span>]



We know that BERT has a better performance than DistilBERT for its larger model size.
However, is it cost-effective to use BERT rather than DistilBERT?
To study this,
we compare the performance of both models with FedOPT on text classification, sharing the same setting as the above experiments.
As shown in Figure <a href="#S4.F7" title="Figure 7 ‣ Experimental Setup and Hyper-parameters. ‣ 4 Experimental Results and Analysis ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
although BERT-base achieves better performance, the performance of DistilBERT is not significantly worse.
Considering the communication cost (BERT-base is almost 2x larger),
we argue that using DistilBERT is a more cost-effective choice for both experimental analysis and realistic applications.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FL benchmarks and platforms.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">In the last few years a proliferation of frameworks and benchmark datasets have been developed to enable researchers to better explore and study algorithms and modeling for federated learning, both from academia: <span id="S5.SS0.SSS0.Px1.p1.1.1" class="ltx_text">LEAF</span><cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.2" class="ltx_text">FedML</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib19" title="" class="ltx_ref">2020c</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.3" class="ltx_text">Flower</span> <cite class="ltx_cite ltx_citemacro_citep">(Beutel et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>,
and from the industry: <span id="S5.SS0.SSS0.Px1.p1.1.4" class="ltx_text">PySyft</span> <cite class="ltx_cite ltx_citemacro_citep">(Ryffel et al., <a href="#bib.bib52" title="" class="ltx_ref">2018</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.5" class="ltx_text">TensorFlow-Federated</span> (TFF) <cite class="ltx_cite ltx_citemacro_citep">(Ingerman and Ostrowski, <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.6" class="ltx_text">FATE</span> <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib65" title="" class="ltx_ref">2019</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.7" class="ltx_text">Clara</span> <cite class="ltx_cite ltx_citemacro_citep">(NVIDIA, <a href="#bib.bib42" title="" class="ltx_ref">2019</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.8" class="ltx_text">PaddleFL</span> <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>, <span id="S5.SS0.SSS0.Px1.p1.1.9" class="ltx_text">Open FL</span> <cite class="ltx_cite ltx_citemacro_citep">(Intel®, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>)</cite>.
However, most platforms only focus on designing a unified framework for federated learning methods and do not provide a dedicated environment for studying NLP problems with FL methods.
<span id="S5.SS0.SSS0.Px1.p1.1.10" class="ltx_text">LEAF</span> <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al., <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite> contains a few text datasets, however, it is limited to classification and next-word prediction datasets and does not consider the pre-trained language models.
We want to provide a dedicated platform for studying FL methods in realistic NLP applications with state-of-the-art language models.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Federated learning in NLP applications.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">There are a few prior works that have begun to apply FL methods in privacy-oriented NLP applications.
For example,
federated learning has been applied to many keyboard-related applications including <cite class="ltx_cite ltx_citemacro_cite">Hard et al. (<a href="#bib.bib15" title="" class="ltx_ref">2018</a>); Stremmel and Singh (<a href="#bib.bib58" title="" class="ltx_ref">2020</a>); Leroy et al. (<a href="#bib.bib29" title="" class="ltx_ref">2019</a>); Ramaswamy et al. (<a href="#bib.bib48" title="" class="ltx_ref">2019</a>); Yang et al. (<a href="#bib.bib66" title="" class="ltx_ref">2018a</a>)</cite>, sentence-level text intent classification using Text-CNN
<cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib69" title="" class="ltx_ref">2020</a>)</cite>, and pretraining and fine-tuning of BERT using medical data from
multiple silos without fetching all data to the same place <cite class="ltx_cite ltx_citemacro_cite">Liu and Miller (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>.
FL methods also have been proposed to train high-quality language models that can outperform the models trained without federated learning <cite class="ltx_cite ltx_citemacro_cite">Ji et al. (<a href="#bib.bib24" title="" class="ltx_ref">2019</a>); Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2019</a>)</cite>.
Besides these applications, some work has been done in medical relation extractions <cite class="ltx_cite ltx_citemacro_cite">Ge et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> and medical name entity recognition <cite class="ltx_cite ltx_citemacro_cite">Sui et al. (<a href="#bib.bib59" title="" class="ltx_ref">2020</a>)</cite>.
These methods use federated learning to preserve the privacy of sensitive medical data and learn data on different platforms, excluding the need for exchanging data between different platforms.</p>
</div>
<div id="S5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p2.1" class="ltx_p">Our work aims to provide a unified platform for studying various NLP applications in a shared environment so that researchers can better design new FL methods either for a specific NLP task or as a general-purpose model.
The aforementioned prior works would thus be a particular instance of the settings supported by the FedNLP platform.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Our key contribution is providing
a thorough and insightful empirical analysis of existing federated learning algorithms in the context of NLP models.
Notably, We compare typical FL methods for four NLP task formulations under multiple non-IID data partitions.
Our findings reveal both promise and the challenges of FL for NLP.
In addition, we also provide
a suite of resources to support future research in FL for NLP
(e.g., a unifying framework for connecting Transformer models with popular FL methods and different non-IID partition
strategies).
Thus, we believe our well-maintained open-source codebase to support future work in this area.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Promising future directions in FL for NLP include: 1) minimizing the performance gap, 2) improving the system efficiency and scalability, 3) trustworthy and privacy-preserving NLP, 4) personalized FL methods for NLP, etc. (Please see Appendix <a href="#A5" title="Приложение E Future Directions ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> for more details.)</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Considerations and Limitations(*)</h2>

<section id="Sx1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Ethical considerations.</h4>

<div id="Sx1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px1.p1.1" class="ltx_p">The key motivation of FedNLP (and FL) is to protect the data privacy of general users by keeping their data on their own devices while benefiting from a shared model from a broader community.
Among the risks that need to be considered in any deployment of NLP are that responses may be wrong, or biased, in ways that would lead to improperly justified decisions.
Although in our view the current technology is still relatively immature, and unlikely to be fielded in applications that would cause harm of this sort,
it is desirable that FedNLP methods provide audit trails,
and recourse so that their predictions can be explained to and critiqued by affected parties.</p>
</div>
</section>
<section id="Sx1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitations.</h4>

<div id="Sx1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx1.SS0.SSS0.Px2.p1.1" class="ltx_p">One limitation of our work is that we have not analyzed the privacy leakage of FL methods.
We argue that novel privacy-centric measures are orthogonal to the development of FL methods, which is beyond the scope of our work.
How to fairly analyze the privacy leakage is now still an open problem for both FL and NLP, and it is only possible to study this when we have an existing platform like FedNLP.</p>
</div>
</section>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This work is supported in part by a research grant and an Amazon ML Fellowship from USC-Amazon Center on Secure and Trustworthy AI (<a target="_blank" href="https://trustedai.usc.edu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://trustedai.usc.edu</a>).
Xiang Ren is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, the DARPA MCS program under Contract No. N660011924033, the Defense Advanced Research Projects Agency with award W911NF-19-20271, NSF IIS 2048211, NSF SMA 1829268, and gift awards from Google, Amazon, JP Morgan and Sony.
Mahdi Soltanolkotabi is supported by the Packard Fellowship in Science and Engineering,
a Sloan Research Fellowship in Mathematics, an NSF-CAREER under award #1846369, DARPA Learning with Less Labels (LwLL) and FastNICS programs, and NSF-CIF awards #1813877 and #2008443.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">Список литературы</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DBL (2012)</span>
<span class="ltx_bibblock">
2012.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Joint Workshop on Automatic Knowledge Base
Construction and Web-scale Knowledge Extraction, AKBC-WEKEX@NAACL-HLT 2012,
Montrèal, Canada, June 7-8, 2012</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bell et al. (2020)</span>
<span class="ltx_bibblock">
James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède
Lepoint, and Mariana Raykova. 2020.

</span>
<span class="ltx_bibblock">Secure single-server aggregation with (poly) logarithmic overhead.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 ACM SIGSAC Conference on Computer
and Communications Security</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beutel et al. (2020)</span>
<span class="ltx_bibblock">
Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and
Nicholas D Lane. 2020.

</span>
<span class="ltx_bibblock">Flower: A friendly federated learning research framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al. (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečnỳ, H Brendan
McMahan, Virginia Smith, and Ameet Talwalkar. 2018.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Mingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong, Cyril
Allauzen, Françoise Beaufays, and Michael Riley. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/K19-1012" title="" class="ltx_ref ltx_href">Federated learning of
n-gram language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd Conference on Computational Natural
Language Learning (CoNLL)</em>, pages 121–130, Hong Kong, China. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinh et al. (2020)</span>
<span class="ltx_bibblock">
Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html" title="" class="ltx_ref ltx_href">Personalized federated learning with moreau envelopes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn et al. (2017)</span>
<span class="ltx_bibblock">
Matthew Dunn, Levent Sagun, Mike Higgins, V. U. Güney, Volkan Cirik, and
Kyunghyun Cho. 2017.

</span>
<span class="ltx_bibblock">Searchqa: A new q&amp;a dataset augmented with context from a search
engine.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elkordy and Avestimehr (2020)</span>
<span class="ltx_bibblock">
A. Elkordy and A. Avestimehr. 2020.

</span>
<span class="ltx_bibblock">Secure aggregation with heterogeneous quantization in federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al (2019)</span>
<span class="ltx_bibblock">
P. Kairouz et al. 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fallah et al. (2020)</span>
<span class="ltx_bibblock">
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. 2020.

</span>
<span class="ltx_bibblock">Personalized federated learning: A meta-learning approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fisch et al. (2019)</span>
<span class="ltx_bibblock">
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen.
2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-5801" title="" class="ltx_ref ltx_href">MRQA 2019 shared
task: Evaluating generalization in reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Machine Reading for
Question Answering</em>, pages 1–13, Hong Kong, China. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. (2020)</span>
<span class="ltx_bibblock">
Suyu Ge, Fangzhao Wu, Chuhan Wu, Tao Qi, Yongfeng Huang, and X. Xie. 2020.

</span>
<span class="ltx_bibblock">Fedner: Privacy-preserving medical named entity recognition with
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hard et al. (2018)</span>
<span class="ltx_bibblock">
Andrew Hard, K. Rao, Rajiv Mathews, F. Beaufays, S. Augenstein, Hubert Eichner,
Chloé Kiddon, and D. Ramage. 2018.

</span>
<span class="ltx_bibblock">Federated learning for mobile keyboard prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020a)</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr. 2020a.

</span>
<span class="ltx_bibblock">Fednas: Federated deep learning via neural architecture search.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020b)</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/a1d4c20b182ad7137ab3606f0e3fc8a4-Abstract.html" title="" class="ltx_ref ltx_href">Group knowledge transfer: Federated learning of large cnns at the edge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2021)</span>
<span class="ltx_bibblock">
Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Yu Rong, Peilin Zhao, Junzhou
Huang, M. Annavaram, and S. Avestimehr. 2021.

</span>
<span class="ltx_bibblock">Fedgraphnn: A federated learning system and benchmark for graph
neural networks.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020c)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang
Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong
Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang,
Murali Annavaram, and Salman Avestimehr. 2020c.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2019)</span>
<span class="ltx_bibblock">
Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, and Ji Liu. 2019.

</span>
<span class="ltx_bibblock">Central server free federated learning over single-sided trust social
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020d)</span>
<span class="ltx_bibblock">
Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang. 2020d.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR42600.2020.01201" title="" class="ltx_ref ltx_href">Milenas:
Efficient neural architecture search via mixed-level reformulation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020</em>, pages
11990–11999. IEEE.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ingerman and Ostrowski (2019)</span>
<span class="ltx_bibblock">
Alex Ingerman and Krzys Ostrowski. 2019.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">TensorFlow Federated</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Intel® (2021)</span>
<span class="ltx_bibblock">
Intel®. 2021.

</span>
<span class="ltx_bibblock">Intel® open federated learning.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2019)</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, and Zi Huang. 2019.

</span>
<span class="ltx_bibblock">Learning private neural language modeling with attentive aggregation.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1147" title="" class="ltx_ref ltx_href">TriviaQA: A large
scale distantly supervised challenge dataset for reading comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1601–1611,
Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al. (2019)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
Rachel Cummings, et al. 2019.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang,
Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00276" title="" class="ltx_ref ltx_href">Natural questions: A
benchmark for question answering research</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
7:452–466.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lang (1995)</span>
<span class="ltx_bibblock">
Ken Lang. 1995.

</span>
<span class="ltx_bibblock">Newsweeder: Learning to filter netnews.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proc. of ICML</em>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leroy et al. (2019)</span>
<span class="ltx_bibblock">
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph
Dureau. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICASSP.2019.8683546" title="" class="ltx_ref ltx_href">Federated
learning for keyword spotting</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">IEEE International Conference on Acoustics, Speech and
Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019</em>,
pages 6341–6345. IEEE.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="" class="ltx_ref ltx_href">BART:
Denoising sequence-to-sequence pre-training for natural language generation,
translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7871–7880, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021a)</span>
<span class="ltx_bibblock">
Q. Li, Yiqun Diao, Quan Chen, and Bingsheng He. 2021a.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021b)</span>
<span class="ltx_bibblock">
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v139/li21h.html" title="" class="ltx_ref ltx_href">Ditto: Fair and
robust federated learning through personalization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of
<em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 6357–6368. PMLR.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and V. Smith. 2020a.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlsys.org/book/316.pdf" title="" class="ltx_ref ltx_href">Federated
optimization in heterogeneous networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems 2020, MLSys
2020, Austin, TX, USA, March 2-4, 2020</em>. mlsys.org.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.353" title="" class="ltx_ref ltx_href">Prefix-tuning:
Optimizing continuous prompts for generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 4582–4597,
Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Miller (2020)</span>
<span class="ltx_bibblock">
D. Liu and T. Miller. 2020.

</span>
<span class="ltx_bibblock">Federated pretraining and fine tuning of bert using clinical notes
from multiple silos.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="" class="ltx_ref ltx_href">Decoupled weight
decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2020)</span>
<span class="ltx_bibblock">
Lingjuan Lyu, Han Yu, Xingjun Ma, Lichao Sun, Jun Zhao, Qiang Yang, and
Philip S Yu. 2020.

</span>
<span class="ltx_bibblock">Privacy and robustness in federated learning: Attacks and defenses.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2019)</span>
<span class="ltx_bibblock">
Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. 2019.

</span>
<span class="ltx_bibblock">Paddlepaddle: An open-source deep learning platform from industrial
practice.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Frontiers of Data and Domputing</em>, (1).

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017a)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas. 2017a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_href">Communication-efficient learning of deep networks from decentralized data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017,
Fort Lauderdale, FL, USA</em>, volume 54 of <em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine
Learning Research</em>, pages 1273–1282. PMLR.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2017b)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas. 2017b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v54/mcmahan17a.html" title="" class="ltx_ref ltx_href">Communication-efficient learning of deep networks from decentralized data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017,
Fort Lauderdale, FL, USA</em>, volume 54 of <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine
Learning Research</em>, pages 1273–1282. PMLR.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NVIDIA (2019)</span>
<span class="ltx_bibblock">
NVIDIA. 2019.

</span>
<span class="ltx_bibblock">Nvidia clara.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradhan et al. (2013)</span>
<span class="ltx_bibblock">
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders
Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W13-3516" title="" class="ltx_ref ltx_href">Towards robust linguistic
analysis using OntoNotes</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventeenth Conference on Computational
Natural Language Learning</em>, pages 143–152, Sofia, Bulgaria. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prakash and Avestimehr (2020)</span>
<span class="ltx_bibblock">
Saurav Prakash and Amir Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock">Mitigating byzantine attacks in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prakash et al. (2020)</span>
<span class="ltx_bibblock">
Saurav Prakash, Sagar Dhakal, Mustafa Riza Akdeniz, Yair Yona, Shilpa Talwar,
Salman Avestimehr, and Nageen Himayat. 2020.

</span>
<span class="ltx_bibblock">Coded computing for low-latency federated learning over wireless edge
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>, (1).

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, (140).

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D16-1264" title="" class="ltx_ref ltx_href">SQuAD: 100,000+
questions for machine comprehension of text</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2383–2392, Austin, Texas. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramaswamy et al. (2019)</span>
<span class="ltx_bibblock">
Swaroop Indra Ramaswamy, Rajiv Mathews, K. Rao, and Franccoise Beaufays. 2019.

</span>
<span class="ltx_bibblock">Federated learning for emoji prediction in a mobile keyboard.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. (2021)</span>
<span class="ltx_bibblock">
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=LkFG3lB13U5" title="" class="ltx_ref ltx_href">Adaptive
federated optimization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Regulation (2016)</span>
<span class="ltx_bibblock">
General Data Protection Regulation. 2016.

</span>
<span class="ltx_bibblock">Regulation eu 2016/679 of the european parliament and of the council
of 27 april 2016.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Official Journal of the European Union. Available at:
http://ec. europa.
eu/justice/data-protection/reform/files/regulation_oj_en. pdf (accessed 20
September 2017)</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1410" title="" class="ltx_ref ltx_href">Sentence-BERT:
Sentence embeddings using Siamese BERT-networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3982–3992, Hong Kong,
China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al. (2018)</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach. 2018.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">ArXiv preprint</em>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al. (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.

</span>
<span class="ltx_bibblock">Distilbert, a distilled version of bert: smaller, faster, cheaper and
lighter.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith et al. (2017)</span>
<span class="ltx_bibblock">
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/6211080fa89981f66b1a0c9d55c61d0f-Abstract.html" title="" class="ltx_ref ltx_href">Federated multi-task learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA</em>, pages 4424–4434.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">So et al. (2020)</span>
<span class="ltx_bibblock">
Jinhyun So, Başak Güler, and A Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock">Byzantine-resilient secure federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">So et al. (2021a)</span>
<span class="ltx_bibblock">
Jinhyun So, Başak Güler, and A Salman Avestimehr. 2021a.

</span>
<span class="ltx_bibblock">Codedprivateml: A fast and privacy-preserving framework for
distributed machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Information Theory</em>, (1).

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">So et al. (2021b)</span>
<span class="ltx_bibblock">
Jinhyun So, Başak Güler, and A Salman Avestimehr. 2021b.

</span>
<span class="ltx_bibblock">Turbo-aggregate: Breaking the quadratic aggregation barrier in secure
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Information Theory</em>, (1).

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stremmel and Singh (2020)</span>
<span class="ltx_bibblock">
Joel Stremmel and Arjun Singh. 2020.

</span>
<span class="ltx_bibblock">Pretraining federated text models for next word prediction.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sui et al. (2020)</span>
<span class="ltx_bibblock">
Dianbo Sui, Yubo Chen, Jun Zhao, Yantao Jia, Yuantao Xie, and Weijian Sun.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.165" title="" class="ltx_ref ltx_href">FedED:
Federated learning via ensemble distillation for medical relation
extraction</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2118–2128, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trischler et al. (2017)</span>
<span class="ltx_bibblock">
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,
Philip Bachman, and Kaheer Suleman. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W17-2623" title="" class="ltx_ref ltx_href">NewsQA: A machine
comprehension dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Representation Learning
for NLP</em>, pages 191–200, Vancouver, Canada. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual
Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA</em>, pages 5998–6008.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020a)</span>
<span class="ltx_bibblock">
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris S. Papailiopoulos.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html" title="" class="ltx_ref ltx_href">Attack of the tails: Yes, you really can backdoor federated learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020b)</span>
<span class="ltx_bibblock">
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris S. Papailiopoulos, and
Yasaman Khazaeni. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=BkluqlSFDS" title="" class="ltx_ref ltx_href">Federated
learning with matched averaging</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">8th International Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-demos.6" title="" class="ltx_ref ltx_href">Transformers:
State-of-the-art natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 38–45, Online.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
(2).

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018a)</span>
<span class="ltx_bibblock">
T. Yang, G. Andrew, Hubert Eichner, Haicheng Sun, W. Li, Nicholas Kong,
D. Ramage, and F. Beaufays. 2018a.

</span>
<span class="ltx_bibblock">Applied federated learning: Improving google keyboard query
suggestions.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018b)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. 2018b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1259" title="" class="ltx_ref ltx_href">HotpotQA: A dataset
for diverse, explainable multi-hop question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2369–2380, Brussels, Belgium.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2019)</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/60a6c4002cc7b29142def8871531281a-Abstract.html" title="" class="ltx_ref ltx_href">Deep leakage from gradients</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual
Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
December 8-14, 2019, Vancouver, BC, Canada</em>, pages 14747–14756.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Xinghua Zhu, Jianzong Wang, Zhenhou Hong, and Jing Xiao. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.55" title="" class="ltx_ref ltx_href">Empirical
studies of institutional federated learning for natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 625–634, Online. Association for Computational
Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Appendix</span> 
<br class="ltx_break"></p>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Приложение A </span>FL+NLP</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Many realistic NLP services heavily rely on users’ local data (e.g., text messages, documents and their tags, questions and selected answers, etc.), which can be located at either personal devices or larger data-silos for organizations.
These local data are usually regarded as highly private and thus not directly accessible by anyone, according to many data privacy regulations; this makes it difficult to train a high-performance model to benefit users.
Federated learning aims to solve machine learning under such a privacy-preserving use case, thus offering a novel and promising direction to the community: FL+NLP.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">Apart from the goal of learning a shared global model for all clients, FL also provides a new perspective for many other interesting research questions in NLP.
One related direction is to develop personalized models for NLP applications, which requires both protection of data privacy and transferred ability on users’ own input feature distribution caused by language styles, interested topics and so on.
The recent concerns on adversarial attacks and safety issues of NLP models are also highly related to FL+NLP.
We thus believe FL+NLP is of vital importance for applying NLP technologies in realistic use cases and could benefit many relevant research areas.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Challenges of Applying FL in NLP</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">Given the promising benefits of studying FL+NLP, however, this research direction is currently blocked by the lack of a standardized platform providing fundamental building blocks: benchmark datasets, NLP models, FL methods, evaluation protocols, etc.
Most of the current FL platforms either focus on unifying various FL methods and use computer vision models and datasets for their experiments, but lack the ability to connect the study of pre-trained language models, the most popular NLP , and realistic NLP applications of various task formulations.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">The first challenge in developing a comprehensive and universal platform for FL+NLP is to deal with various task formulations for realistic NLP applications, which have different input and output formats (Section <a href="#A2" title="Приложение B Basic Formulations of NLP Tasks ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>).
As the non-IID data partition over clients is the major feature of FL problems, it is also a challenge to simulate the realistic non-IID partition for existing NLP datasets (Section <a href="#S3.SS2" title="3.2 Non-IID Partitioning Strategies ‣ 3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
Finally, a platform also must integrate various FL methods with the Transformer-based NLP models for a variety of task types, and thus a flexible and extensible learning framework is needed.
In particular, the conventional trainer component of Transformers now needs to be modified for efficient and safe communications towards federated learning (Section <a href="#A6" title="Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>).</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Приложение B </span>Basic Formulations of NLP Tasks</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">There are various types of NLP applications, but many of them share a similar task formulation (i.e., input-and-put formats).
We show four common task formulations that can cover most of the mainstream NLP applications: text classification, sequence tagging, question answering, sequence-to-sequence generation.</p>
</div>
<div id="A2.p2" class="ltx_para ltx_noindent">
<p id="A2.p2.3" class="ltx_p"><span id="A2.p2.3.1" class="ltx_text ltx_font_bold">Text Classification (TC)</span>
The input is a sequence of words, <math id="A2.p2.1.m1.3" class="ltx_Math" alttext="x=[w_{1},w_{2},\dots]" display="inline"><semantics id="A2.p2.1.m1.3a"><mrow id="A2.p2.1.m1.3.3" xref="A2.p2.1.m1.3.3.cmml"><mi id="A2.p2.1.m1.3.3.4" xref="A2.p2.1.m1.3.3.4.cmml">x</mi><mo id="A2.p2.1.m1.3.3.3" xref="A2.p2.1.m1.3.3.3.cmml">=</mo><mrow id="A2.p2.1.m1.3.3.2.2" xref="A2.p2.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="A2.p2.1.m1.3.3.2.2.3" xref="A2.p2.1.m1.3.3.2.3.cmml">[</mo><msub id="A2.p2.1.m1.2.2.1.1.1" xref="A2.p2.1.m1.2.2.1.1.1.cmml"><mi id="A2.p2.1.m1.2.2.1.1.1.2" xref="A2.p2.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="A2.p2.1.m1.2.2.1.1.1.3" xref="A2.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="A2.p2.1.m1.3.3.2.2.4" xref="A2.p2.1.m1.3.3.2.3.cmml">,</mo><msub id="A2.p2.1.m1.3.3.2.2.2" xref="A2.p2.1.m1.3.3.2.2.2.cmml"><mi id="A2.p2.1.m1.3.3.2.2.2.2" xref="A2.p2.1.m1.3.3.2.2.2.2.cmml">w</mi><mn id="A2.p2.1.m1.3.3.2.2.2.3" xref="A2.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="A2.p2.1.m1.3.3.2.2.5" xref="A2.p2.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="A2.p2.1.m1.1.1" xref="A2.p2.1.m1.1.1.cmml">…</mi><mo stretchy="false" id="A2.p2.1.m1.3.3.2.2.6" xref="A2.p2.1.m1.3.3.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p2.1.m1.3b"><apply id="A2.p2.1.m1.3.3.cmml" xref="A2.p2.1.m1.3.3"><eq id="A2.p2.1.m1.3.3.3.cmml" xref="A2.p2.1.m1.3.3.3"></eq><ci id="A2.p2.1.m1.3.3.4.cmml" xref="A2.p2.1.m1.3.3.4">𝑥</ci><list id="A2.p2.1.m1.3.3.2.3.cmml" xref="A2.p2.1.m1.3.3.2.2"><apply id="A2.p2.1.m1.2.2.1.1.1.cmml" xref="A2.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p2.1.m1.2.2.1.1.1.1.cmml" xref="A2.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="A2.p2.1.m1.2.2.1.1.1.2.cmml" xref="A2.p2.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="A2.p2.1.m1.2.2.1.1.1.3.cmml" xref="A2.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="A2.p2.1.m1.3.3.2.2.2.cmml" xref="A2.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p2.1.m1.3.3.2.2.2.1.cmml" xref="A2.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="A2.p2.1.m1.3.3.2.2.2.2.cmml" xref="A2.p2.1.m1.3.3.2.2.2.2">𝑤</ci><cn type="integer" id="A2.p2.1.m1.3.3.2.2.2.3.cmml" xref="A2.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="A2.p2.1.m1.1.1.cmml" xref="A2.p2.1.m1.1.1">…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.1.m1.3c">x=[w_{1},w_{2},\dots]</annotation></semantics></math>, and the output is a label <math id="A2.p2.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="A2.p2.2.m2.1a"><mi id="A2.p2.2.m2.1.1" xref="A2.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="A2.p2.2.m2.1b"><ci id="A2.p2.2.m2.1.1.cmml" xref="A2.p2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.2.m2.1c">y</annotation></semantics></math> in a fixed set of labels <math id="A2.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="A2.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="A2.p2.3.m3.1.1" xref="A2.p2.3.m3.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="A2.p2.3.m3.1b"><ci id="A2.p2.3.m3.1.1.cmml" xref="A2.p2.3.m3.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p2.3.m3.1c">\mathcal{L}</annotation></semantics></math>.
Many NLP applications can be formulated as text classification tasks.
For example, we can use TC models for classifying the topic of a news article to be <span id="A2.p2.3.2" class="ltx_text ltx_font_italic">political</span>, <span id="A2.p2.3.3" class="ltx_text ltx_font_italic">sports</span>, <span id="A2.p2.3.4" class="ltx_text ltx_font_italic">entertainment</span>, etc., or analyzing movie reviews to be <span id="A2.p2.3.5" class="ltx_text ltx_font_italic">positive</span>, <span id="A2.p2.3.6" class="ltx_text ltx_font_italic">negative</span> or <span id="A2.p2.3.7" class="ltx_text ltx_font_italic">neutral</span>.</p>
</div>
<div id="A2.p3" class="ltx_para ltx_noindent">
<p id="A2.p3.4" class="ltx_p"><span id="A2.p3.4.1" class="ltx_text ltx_font_bold">Sequence Tagging (ST)</span>
The input is a sequence of words, <math id="A2.p3.1.m1.4" class="ltx_Math" alttext="x=[w_{1},w_{2},\dots,w_{N}]" display="inline"><semantics id="A2.p3.1.m1.4a"><mrow id="A2.p3.1.m1.4.4" xref="A2.p3.1.m1.4.4.cmml"><mi id="A2.p3.1.m1.4.4.5" xref="A2.p3.1.m1.4.4.5.cmml">x</mi><mo id="A2.p3.1.m1.4.4.4" xref="A2.p3.1.m1.4.4.4.cmml">=</mo><mrow id="A2.p3.1.m1.4.4.3.3" xref="A2.p3.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="A2.p3.1.m1.4.4.3.3.4" xref="A2.p3.1.m1.4.4.3.4.cmml">[</mo><msub id="A2.p3.1.m1.2.2.1.1.1" xref="A2.p3.1.m1.2.2.1.1.1.cmml"><mi id="A2.p3.1.m1.2.2.1.1.1.2" xref="A2.p3.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="A2.p3.1.m1.2.2.1.1.1.3" xref="A2.p3.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="A2.p3.1.m1.4.4.3.3.5" xref="A2.p3.1.m1.4.4.3.4.cmml">,</mo><msub id="A2.p3.1.m1.3.3.2.2.2" xref="A2.p3.1.m1.3.3.2.2.2.cmml"><mi id="A2.p3.1.m1.3.3.2.2.2.2" xref="A2.p3.1.m1.3.3.2.2.2.2.cmml">w</mi><mn id="A2.p3.1.m1.3.3.2.2.2.3" xref="A2.p3.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="A2.p3.1.m1.4.4.3.3.6" xref="A2.p3.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml">…</mi><mo id="A2.p3.1.m1.4.4.3.3.7" xref="A2.p3.1.m1.4.4.3.4.cmml">,</mo><msub id="A2.p3.1.m1.4.4.3.3.3" xref="A2.p3.1.m1.4.4.3.3.3.cmml"><mi id="A2.p3.1.m1.4.4.3.3.3.2" xref="A2.p3.1.m1.4.4.3.3.3.2.cmml">w</mi><mi id="A2.p3.1.m1.4.4.3.3.3.3" xref="A2.p3.1.m1.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="A2.p3.1.m1.4.4.3.3.8" xref="A2.p3.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.4b"><apply id="A2.p3.1.m1.4.4.cmml" xref="A2.p3.1.m1.4.4"><eq id="A2.p3.1.m1.4.4.4.cmml" xref="A2.p3.1.m1.4.4.4"></eq><ci id="A2.p3.1.m1.4.4.5.cmml" xref="A2.p3.1.m1.4.4.5">𝑥</ci><list id="A2.p3.1.m1.4.4.3.4.cmml" xref="A2.p3.1.m1.4.4.3.3"><apply id="A2.p3.1.m1.2.2.1.1.1.cmml" xref="A2.p3.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p3.1.m1.2.2.1.1.1.1.cmml" xref="A2.p3.1.m1.2.2.1.1.1">subscript</csymbol><ci id="A2.p3.1.m1.2.2.1.1.1.2.cmml" xref="A2.p3.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="A2.p3.1.m1.2.2.1.1.1.3.cmml" xref="A2.p3.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="A2.p3.1.m1.3.3.2.2.2.cmml" xref="A2.p3.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p3.1.m1.3.3.2.2.2.1.cmml" xref="A2.p3.1.m1.3.3.2.2.2">subscript</csymbol><ci id="A2.p3.1.m1.3.3.2.2.2.2.cmml" xref="A2.p3.1.m1.3.3.2.2.2.2">𝑤</ci><cn type="integer" id="A2.p3.1.m1.3.3.2.2.2.3.cmml" xref="A2.p3.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1">…</ci><apply id="A2.p3.1.m1.4.4.3.3.3.cmml" xref="A2.p3.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p3.1.m1.4.4.3.3.3.1.cmml" xref="A2.p3.1.m1.4.4.3.3.3">subscript</csymbol><ci id="A2.p3.1.m1.4.4.3.3.3.2.cmml" xref="A2.p3.1.m1.4.4.3.3.3.2">𝑤</ci><ci id="A2.p3.1.m1.4.4.3.3.3.3.cmml" xref="A2.p3.1.m1.4.4.3.3.3.3">𝑁</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.4c">x=[w_{1},w_{2},\dots,w_{N}]</annotation></semantics></math>, and the output is a same-length sequence of tags <math id="A2.p3.2.m2.4" class="ltx_Math" alttext="y=[t_{1},t_{2},\dots,t_{N}]" display="inline"><semantics id="A2.p3.2.m2.4a"><mrow id="A2.p3.2.m2.4.4" xref="A2.p3.2.m2.4.4.cmml"><mi id="A2.p3.2.m2.4.4.5" xref="A2.p3.2.m2.4.4.5.cmml">y</mi><mo id="A2.p3.2.m2.4.4.4" xref="A2.p3.2.m2.4.4.4.cmml">=</mo><mrow id="A2.p3.2.m2.4.4.3.3" xref="A2.p3.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="A2.p3.2.m2.4.4.3.3.4" xref="A2.p3.2.m2.4.4.3.4.cmml">[</mo><msub id="A2.p3.2.m2.2.2.1.1.1" xref="A2.p3.2.m2.2.2.1.1.1.cmml"><mi id="A2.p3.2.m2.2.2.1.1.1.2" xref="A2.p3.2.m2.2.2.1.1.1.2.cmml">t</mi><mn id="A2.p3.2.m2.2.2.1.1.1.3" xref="A2.p3.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="A2.p3.2.m2.4.4.3.3.5" xref="A2.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="A2.p3.2.m2.3.3.2.2.2" xref="A2.p3.2.m2.3.3.2.2.2.cmml"><mi id="A2.p3.2.m2.3.3.2.2.2.2" xref="A2.p3.2.m2.3.3.2.2.2.2.cmml">t</mi><mn id="A2.p3.2.m2.3.3.2.2.2.3" xref="A2.p3.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="A2.p3.2.m2.4.4.3.3.6" xref="A2.p3.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="A2.p3.2.m2.1.1" xref="A2.p3.2.m2.1.1.cmml">…</mi><mo id="A2.p3.2.m2.4.4.3.3.7" xref="A2.p3.2.m2.4.4.3.4.cmml">,</mo><msub id="A2.p3.2.m2.4.4.3.3.3" xref="A2.p3.2.m2.4.4.3.3.3.cmml"><mi id="A2.p3.2.m2.4.4.3.3.3.2" xref="A2.p3.2.m2.4.4.3.3.3.2.cmml">t</mi><mi id="A2.p3.2.m2.4.4.3.3.3.3" xref="A2.p3.2.m2.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="A2.p3.2.m2.4.4.3.3.8" xref="A2.p3.2.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.2.m2.4b"><apply id="A2.p3.2.m2.4.4.cmml" xref="A2.p3.2.m2.4.4"><eq id="A2.p3.2.m2.4.4.4.cmml" xref="A2.p3.2.m2.4.4.4"></eq><ci id="A2.p3.2.m2.4.4.5.cmml" xref="A2.p3.2.m2.4.4.5">𝑦</ci><list id="A2.p3.2.m2.4.4.3.4.cmml" xref="A2.p3.2.m2.4.4.3.3"><apply id="A2.p3.2.m2.2.2.1.1.1.cmml" xref="A2.p3.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p3.2.m2.2.2.1.1.1.1.cmml" xref="A2.p3.2.m2.2.2.1.1.1">subscript</csymbol><ci id="A2.p3.2.m2.2.2.1.1.1.2.cmml" xref="A2.p3.2.m2.2.2.1.1.1.2">𝑡</ci><cn type="integer" id="A2.p3.2.m2.2.2.1.1.1.3.cmml" xref="A2.p3.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="A2.p3.2.m2.3.3.2.2.2.cmml" xref="A2.p3.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p3.2.m2.3.3.2.2.2.1.cmml" xref="A2.p3.2.m2.3.3.2.2.2">subscript</csymbol><ci id="A2.p3.2.m2.3.3.2.2.2.2.cmml" xref="A2.p3.2.m2.3.3.2.2.2.2">𝑡</ci><cn type="integer" id="A2.p3.2.m2.3.3.2.2.2.3.cmml" xref="A2.p3.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="A2.p3.2.m2.1.1.cmml" xref="A2.p3.2.m2.1.1">…</ci><apply id="A2.p3.2.m2.4.4.3.3.3.cmml" xref="A2.p3.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p3.2.m2.4.4.3.3.3.1.cmml" xref="A2.p3.2.m2.4.4.3.3.3">subscript</csymbol><ci id="A2.p3.2.m2.4.4.3.3.3.2.cmml" xref="A2.p3.2.m2.4.4.3.3.3.2">𝑡</ci><ci id="A2.p3.2.m2.4.4.3.3.3.3.cmml" xref="A2.p3.2.m2.4.4.3.3.3.3">𝑁</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.2.m2.4c">y=[t_{1},t_{2},\dots,t_{N}]</annotation></semantics></math>, where <math id="A2.p3.3.m3.1" class="ltx_Math" alttext="t_{i}" display="inline"><semantics id="A2.p3.3.m3.1a"><msub id="A2.p3.3.m3.1.1" xref="A2.p3.3.m3.1.1.cmml"><mi id="A2.p3.3.m3.1.1.2" xref="A2.p3.3.m3.1.1.2.cmml">t</mi><mi id="A2.p3.3.m3.1.1.3" xref="A2.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A2.p3.3.m3.1b"><apply id="A2.p3.3.m3.1.1.cmml" xref="A2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="A2.p3.3.m3.1.1.1.cmml" xref="A2.p3.3.m3.1.1">subscript</csymbol><ci id="A2.p3.3.m3.1.1.2.cmml" xref="A2.p3.3.m3.1.1.2">𝑡</ci><ci id="A2.p3.3.m3.1.1.3.cmml" xref="A2.p3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.3.m3.1c">t_{i}</annotation></semantics></math> is in a fixed set of labels <math id="A2.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="A2.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="A2.p3.4.m4.1.1" xref="A2.p3.4.m4.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="A2.p3.4.m4.1b"><ci id="A2.p3.4.m4.1.1.cmml" xref="A2.p3.4.m4.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.4.m4.1c">\mathcal{L}</annotation></semantics></math>.
The main difference between TC and ST is that ST learns to classify the label of each token in a sentence, which is particularly useful in analyzing syntactic structures (e.g., part-of-speech analysis, phrase chunking, and word segmentation) and extracting spans (e.g., named entity recognition).</p>
</div>
<div id="A2.p4" class="ltx_para ltx_noindent">
<p id="A2.p4.4" class="ltx_p"><span id="A2.p4.4.1" class="ltx_text ltx_font_bold">Question Answering (QA)</span>
Given a passage <math id="A2.p4.1.m1.4" class="ltx_Math" alttext="P=[w_{1},w_{2},\dots,w_{N}]" display="inline"><semantics id="A2.p4.1.m1.4a"><mrow id="A2.p4.1.m1.4.4" xref="A2.p4.1.m1.4.4.cmml"><mi id="A2.p4.1.m1.4.4.5" xref="A2.p4.1.m1.4.4.5.cmml">P</mi><mo id="A2.p4.1.m1.4.4.4" xref="A2.p4.1.m1.4.4.4.cmml">=</mo><mrow id="A2.p4.1.m1.4.4.3.3" xref="A2.p4.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="A2.p4.1.m1.4.4.3.3.4" xref="A2.p4.1.m1.4.4.3.4.cmml">[</mo><msub id="A2.p4.1.m1.2.2.1.1.1" xref="A2.p4.1.m1.2.2.1.1.1.cmml"><mi id="A2.p4.1.m1.2.2.1.1.1.2" xref="A2.p4.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="A2.p4.1.m1.2.2.1.1.1.3" xref="A2.p4.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="A2.p4.1.m1.4.4.3.3.5" xref="A2.p4.1.m1.4.4.3.4.cmml">,</mo><msub id="A2.p4.1.m1.3.3.2.2.2" xref="A2.p4.1.m1.3.3.2.2.2.cmml"><mi id="A2.p4.1.m1.3.3.2.2.2.2" xref="A2.p4.1.m1.3.3.2.2.2.2.cmml">w</mi><mn id="A2.p4.1.m1.3.3.2.2.2.3" xref="A2.p4.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="A2.p4.1.m1.4.4.3.3.6" xref="A2.p4.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="A2.p4.1.m1.1.1" xref="A2.p4.1.m1.1.1.cmml">…</mi><mo id="A2.p4.1.m1.4.4.3.3.7" xref="A2.p4.1.m1.4.4.3.4.cmml">,</mo><msub id="A2.p4.1.m1.4.4.3.3.3" xref="A2.p4.1.m1.4.4.3.3.3.cmml"><mi id="A2.p4.1.m1.4.4.3.3.3.2" xref="A2.p4.1.m1.4.4.3.3.3.2.cmml">w</mi><mi id="A2.p4.1.m1.4.4.3.3.3.3" xref="A2.p4.1.m1.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="A2.p4.1.m1.4.4.3.3.8" xref="A2.p4.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.1.m1.4b"><apply id="A2.p4.1.m1.4.4.cmml" xref="A2.p4.1.m1.4.4"><eq id="A2.p4.1.m1.4.4.4.cmml" xref="A2.p4.1.m1.4.4.4"></eq><ci id="A2.p4.1.m1.4.4.5.cmml" xref="A2.p4.1.m1.4.4.5">𝑃</ci><list id="A2.p4.1.m1.4.4.3.4.cmml" xref="A2.p4.1.m1.4.4.3.3"><apply id="A2.p4.1.m1.2.2.1.1.1.cmml" xref="A2.p4.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p4.1.m1.2.2.1.1.1.1.cmml" xref="A2.p4.1.m1.2.2.1.1.1">subscript</csymbol><ci id="A2.p4.1.m1.2.2.1.1.1.2.cmml" xref="A2.p4.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="A2.p4.1.m1.2.2.1.1.1.3.cmml" xref="A2.p4.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="A2.p4.1.m1.3.3.2.2.2.cmml" xref="A2.p4.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p4.1.m1.3.3.2.2.2.1.cmml" xref="A2.p4.1.m1.3.3.2.2.2">subscript</csymbol><ci id="A2.p4.1.m1.3.3.2.2.2.2.cmml" xref="A2.p4.1.m1.3.3.2.2.2.2">𝑤</ci><cn type="integer" id="A2.p4.1.m1.3.3.2.2.2.3.cmml" xref="A2.p4.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="A2.p4.1.m1.1.1.cmml" xref="A2.p4.1.m1.1.1">…</ci><apply id="A2.p4.1.m1.4.4.3.3.3.cmml" xref="A2.p4.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p4.1.m1.4.4.3.3.3.1.cmml" xref="A2.p4.1.m1.4.4.3.3.3">subscript</csymbol><ci id="A2.p4.1.m1.4.4.3.3.3.2.cmml" xref="A2.p4.1.m1.4.4.3.3.3.2">𝑤</ci><ci id="A2.p4.1.m1.4.4.3.3.3.3.cmml" xref="A2.p4.1.m1.4.4.3.3.3.3">𝑁</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.1.m1.4c">P=[w_{1},w_{2},\dots,w_{N}]</annotation></semantics></math> and a question <math id="A2.p4.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="A2.p4.2.m2.1a"><mi id="A2.p4.2.m2.1.1" xref="A2.p4.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="A2.p4.2.m2.1b"><ci id="A2.p4.2.m2.1.1.cmml" xref="A2.p4.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.2.m2.1c">q</annotation></semantics></math> as input, the task is to locate a span in the passage as the answer to the question.
Thus, the output is a pair of token index <math id="A2.p4.3.m3.2" class="ltx_Math" alttext="(s,e)" display="inline"><semantics id="A2.p4.3.m3.2a"><mrow id="A2.p4.3.m3.2.3.2" xref="A2.p4.3.m3.2.3.1.cmml"><mo stretchy="false" id="A2.p4.3.m3.2.3.2.1" xref="A2.p4.3.m3.2.3.1.cmml">(</mo><mi id="A2.p4.3.m3.1.1" xref="A2.p4.3.m3.1.1.cmml">s</mi><mo id="A2.p4.3.m3.2.3.2.2" xref="A2.p4.3.m3.2.3.1.cmml">,</mo><mi id="A2.p4.3.m3.2.2" xref="A2.p4.3.m3.2.2.cmml">e</mi><mo stretchy="false" id="A2.p4.3.m3.2.3.2.3" xref="A2.p4.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.3.m3.2b"><interval closure="open" id="A2.p4.3.m3.2.3.1.cmml" xref="A2.p4.3.m3.2.3.2"><ci id="A2.p4.3.m3.1.1.cmml" xref="A2.p4.3.m3.1.1">𝑠</ci><ci id="A2.p4.3.m3.2.2.cmml" xref="A2.p4.3.m3.2.2">𝑒</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.3.m3.2c">(s,e)</annotation></semantics></math> where <math id="A2.p4.4.m4.6" class="ltx_Math" alttext="s,e\in\{1,2,\dots,N\}" display="inline"><semantics id="A2.p4.4.m4.6a"><mrow id="A2.p4.4.m4.6.7" xref="A2.p4.4.m4.6.7.cmml"><mrow id="A2.p4.4.m4.6.7.2.2" xref="A2.p4.4.m4.6.7.2.1.cmml"><mi id="A2.p4.4.m4.5.5" xref="A2.p4.4.m4.5.5.cmml">s</mi><mo id="A2.p4.4.m4.6.7.2.2.1" xref="A2.p4.4.m4.6.7.2.1.cmml">,</mo><mi id="A2.p4.4.m4.6.6" xref="A2.p4.4.m4.6.6.cmml">e</mi></mrow><mo id="A2.p4.4.m4.6.7.1" xref="A2.p4.4.m4.6.7.1.cmml">∈</mo><mrow id="A2.p4.4.m4.6.7.3.2" xref="A2.p4.4.m4.6.7.3.1.cmml"><mo stretchy="false" id="A2.p4.4.m4.6.7.3.2.1" xref="A2.p4.4.m4.6.7.3.1.cmml">{</mo><mn id="A2.p4.4.m4.1.1" xref="A2.p4.4.m4.1.1.cmml">1</mn><mo id="A2.p4.4.m4.6.7.3.2.2" xref="A2.p4.4.m4.6.7.3.1.cmml">,</mo><mn id="A2.p4.4.m4.2.2" xref="A2.p4.4.m4.2.2.cmml">2</mn><mo id="A2.p4.4.m4.6.7.3.2.3" xref="A2.p4.4.m4.6.7.3.1.cmml">,</mo><mi mathvariant="normal" id="A2.p4.4.m4.3.3" xref="A2.p4.4.m4.3.3.cmml">…</mi><mo id="A2.p4.4.m4.6.7.3.2.4" xref="A2.p4.4.m4.6.7.3.1.cmml">,</mo><mi id="A2.p4.4.m4.4.4" xref="A2.p4.4.m4.4.4.cmml">N</mi><mo stretchy="false" id="A2.p4.4.m4.6.7.3.2.5" xref="A2.p4.4.m4.6.7.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.4.m4.6b"><apply id="A2.p4.4.m4.6.7.cmml" xref="A2.p4.4.m4.6.7"><in id="A2.p4.4.m4.6.7.1.cmml" xref="A2.p4.4.m4.6.7.1"></in><list id="A2.p4.4.m4.6.7.2.1.cmml" xref="A2.p4.4.m4.6.7.2.2"><ci id="A2.p4.4.m4.5.5.cmml" xref="A2.p4.4.m4.5.5">𝑠</ci><ci id="A2.p4.4.m4.6.6.cmml" xref="A2.p4.4.m4.6.6">𝑒</ci></list><set id="A2.p4.4.m4.6.7.3.1.cmml" xref="A2.p4.4.m4.6.7.3.2"><cn type="integer" id="A2.p4.4.m4.1.1.cmml" xref="A2.p4.4.m4.1.1">1</cn><cn type="integer" id="A2.p4.4.m4.2.2.cmml" xref="A2.p4.4.m4.2.2">2</cn><ci id="A2.p4.4.m4.3.3.cmml" xref="A2.p4.4.m4.3.3">…</ci><ci id="A2.p4.4.m4.4.4.cmml" xref="A2.p4.4.m4.4.4">𝑁</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.4.m4.6c">s,e\in\{1,2,\dots,N\}</annotation></semantics></math> for denoting the begin and end of the span in the passage.
This particular formulation is also known as <span id="A2.p4.4.2" class="ltx_text ltx_font_italic">reading comprehension</span>.</p>
</div>
<div id="A2.p5" class="ltx_para ltx_noindent">
<p id="A2.p5.2" class="ltx_p"><span id="A2.p5.2.1" class="ltx_text ltx_font_bold">Natural Language Generation (NLG)</span>
Both input and output are sequence of words, <math id="A2.p5.1.m1.4" class="ltx_Math" alttext="x=[w_{1}^{i},w_{2}^{i},\dots,w_{N}^{i}]" display="inline"><semantics id="A2.p5.1.m1.4a"><mrow id="A2.p5.1.m1.4.4" xref="A2.p5.1.m1.4.4.cmml"><mi id="A2.p5.1.m1.4.4.5" xref="A2.p5.1.m1.4.4.5.cmml">x</mi><mo id="A2.p5.1.m1.4.4.4" xref="A2.p5.1.m1.4.4.4.cmml">=</mo><mrow id="A2.p5.1.m1.4.4.3.3" xref="A2.p5.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="A2.p5.1.m1.4.4.3.3.4" xref="A2.p5.1.m1.4.4.3.4.cmml">[</mo><msubsup id="A2.p5.1.m1.2.2.1.1.1" xref="A2.p5.1.m1.2.2.1.1.1.cmml"><mi id="A2.p5.1.m1.2.2.1.1.1.2.2" xref="A2.p5.1.m1.2.2.1.1.1.2.2.cmml">w</mi><mn id="A2.p5.1.m1.2.2.1.1.1.2.3" xref="A2.p5.1.m1.2.2.1.1.1.2.3.cmml">1</mn><mi id="A2.p5.1.m1.2.2.1.1.1.3" xref="A2.p5.1.m1.2.2.1.1.1.3.cmml">i</mi></msubsup><mo id="A2.p5.1.m1.4.4.3.3.5" xref="A2.p5.1.m1.4.4.3.4.cmml">,</mo><msubsup id="A2.p5.1.m1.3.3.2.2.2" xref="A2.p5.1.m1.3.3.2.2.2.cmml"><mi id="A2.p5.1.m1.3.3.2.2.2.2.2" xref="A2.p5.1.m1.3.3.2.2.2.2.2.cmml">w</mi><mn id="A2.p5.1.m1.3.3.2.2.2.2.3" xref="A2.p5.1.m1.3.3.2.2.2.2.3.cmml">2</mn><mi id="A2.p5.1.m1.3.3.2.2.2.3" xref="A2.p5.1.m1.3.3.2.2.2.3.cmml">i</mi></msubsup><mo id="A2.p5.1.m1.4.4.3.3.6" xref="A2.p5.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="A2.p5.1.m1.1.1" xref="A2.p5.1.m1.1.1.cmml">…</mi><mo id="A2.p5.1.m1.4.4.3.3.7" xref="A2.p5.1.m1.4.4.3.4.cmml">,</mo><msubsup id="A2.p5.1.m1.4.4.3.3.3" xref="A2.p5.1.m1.4.4.3.3.3.cmml"><mi id="A2.p5.1.m1.4.4.3.3.3.2.2" xref="A2.p5.1.m1.4.4.3.3.3.2.2.cmml">w</mi><mi id="A2.p5.1.m1.4.4.3.3.3.2.3" xref="A2.p5.1.m1.4.4.3.3.3.2.3.cmml">N</mi><mi id="A2.p5.1.m1.4.4.3.3.3.3" xref="A2.p5.1.m1.4.4.3.3.3.3.cmml">i</mi></msubsup><mo stretchy="false" id="A2.p5.1.m1.4.4.3.3.8" xref="A2.p5.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p5.1.m1.4b"><apply id="A2.p5.1.m1.4.4.cmml" xref="A2.p5.1.m1.4.4"><eq id="A2.p5.1.m1.4.4.4.cmml" xref="A2.p5.1.m1.4.4.4"></eq><ci id="A2.p5.1.m1.4.4.5.cmml" xref="A2.p5.1.m1.4.4.5">𝑥</ci><list id="A2.p5.1.m1.4.4.3.4.cmml" xref="A2.p5.1.m1.4.4.3.3"><apply id="A2.p5.1.m1.2.2.1.1.1.cmml" xref="A2.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p5.1.m1.2.2.1.1.1.1.cmml" xref="A2.p5.1.m1.2.2.1.1.1">superscript</csymbol><apply id="A2.p5.1.m1.2.2.1.1.1.2.cmml" xref="A2.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p5.1.m1.2.2.1.1.1.2.1.cmml" xref="A2.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="A2.p5.1.m1.2.2.1.1.1.2.2.cmml" xref="A2.p5.1.m1.2.2.1.1.1.2.2">𝑤</ci><cn type="integer" id="A2.p5.1.m1.2.2.1.1.1.2.3.cmml" xref="A2.p5.1.m1.2.2.1.1.1.2.3">1</cn></apply><ci id="A2.p5.1.m1.2.2.1.1.1.3.cmml" xref="A2.p5.1.m1.2.2.1.1.1.3">𝑖</ci></apply><apply id="A2.p5.1.m1.3.3.2.2.2.cmml" xref="A2.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p5.1.m1.3.3.2.2.2.1.cmml" xref="A2.p5.1.m1.3.3.2.2.2">superscript</csymbol><apply id="A2.p5.1.m1.3.3.2.2.2.2.cmml" xref="A2.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p5.1.m1.3.3.2.2.2.2.1.cmml" xref="A2.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="A2.p5.1.m1.3.3.2.2.2.2.2.cmml" xref="A2.p5.1.m1.3.3.2.2.2.2.2">𝑤</ci><cn type="integer" id="A2.p5.1.m1.3.3.2.2.2.2.3.cmml" xref="A2.p5.1.m1.3.3.2.2.2.2.3">2</cn></apply><ci id="A2.p5.1.m1.3.3.2.2.2.3.cmml" xref="A2.p5.1.m1.3.3.2.2.2.3">𝑖</ci></apply><ci id="A2.p5.1.m1.1.1.cmml" xref="A2.p5.1.m1.1.1">…</ci><apply id="A2.p5.1.m1.4.4.3.3.3.cmml" xref="A2.p5.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p5.1.m1.4.4.3.3.3.1.cmml" xref="A2.p5.1.m1.4.4.3.3.3">superscript</csymbol><apply id="A2.p5.1.m1.4.4.3.3.3.2.cmml" xref="A2.p5.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p5.1.m1.4.4.3.3.3.2.1.cmml" xref="A2.p5.1.m1.4.4.3.3.3">subscript</csymbol><ci id="A2.p5.1.m1.4.4.3.3.3.2.2.cmml" xref="A2.p5.1.m1.4.4.3.3.3.2.2">𝑤</ci><ci id="A2.p5.1.m1.4.4.3.3.3.2.3.cmml" xref="A2.p5.1.m1.4.4.3.3.3.2.3">𝑁</ci></apply><ci id="A2.p5.1.m1.4.4.3.3.3.3.cmml" xref="A2.p5.1.m1.4.4.3.3.3.3">𝑖</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.1.m1.4c">x=[w_{1}^{i},w_{2}^{i},\dots,w_{N}^{i}]</annotation></semantics></math> , <math id="A2.p5.2.m2.4" class="ltx_Math" alttext="y=[w_{1}^{o},w_{2}^{o},\dots,w_{M}^{o}]" display="inline"><semantics id="A2.p5.2.m2.4a"><mrow id="A2.p5.2.m2.4.4" xref="A2.p5.2.m2.4.4.cmml"><mi id="A2.p5.2.m2.4.4.5" xref="A2.p5.2.m2.4.4.5.cmml">y</mi><mo id="A2.p5.2.m2.4.4.4" xref="A2.p5.2.m2.4.4.4.cmml">=</mo><mrow id="A2.p5.2.m2.4.4.3.3" xref="A2.p5.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="A2.p5.2.m2.4.4.3.3.4" xref="A2.p5.2.m2.4.4.3.4.cmml">[</mo><msubsup id="A2.p5.2.m2.2.2.1.1.1" xref="A2.p5.2.m2.2.2.1.1.1.cmml"><mi id="A2.p5.2.m2.2.2.1.1.1.2.2" xref="A2.p5.2.m2.2.2.1.1.1.2.2.cmml">w</mi><mn id="A2.p5.2.m2.2.2.1.1.1.2.3" xref="A2.p5.2.m2.2.2.1.1.1.2.3.cmml">1</mn><mi id="A2.p5.2.m2.2.2.1.1.1.3" xref="A2.p5.2.m2.2.2.1.1.1.3.cmml">o</mi></msubsup><mo id="A2.p5.2.m2.4.4.3.3.5" xref="A2.p5.2.m2.4.4.3.4.cmml">,</mo><msubsup id="A2.p5.2.m2.3.3.2.2.2" xref="A2.p5.2.m2.3.3.2.2.2.cmml"><mi id="A2.p5.2.m2.3.3.2.2.2.2.2" xref="A2.p5.2.m2.3.3.2.2.2.2.2.cmml">w</mi><mn id="A2.p5.2.m2.3.3.2.2.2.2.3" xref="A2.p5.2.m2.3.3.2.2.2.2.3.cmml">2</mn><mi id="A2.p5.2.m2.3.3.2.2.2.3" xref="A2.p5.2.m2.3.3.2.2.2.3.cmml">o</mi></msubsup><mo id="A2.p5.2.m2.4.4.3.3.6" xref="A2.p5.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="A2.p5.2.m2.1.1" xref="A2.p5.2.m2.1.1.cmml">…</mi><mo id="A2.p5.2.m2.4.4.3.3.7" xref="A2.p5.2.m2.4.4.3.4.cmml">,</mo><msubsup id="A2.p5.2.m2.4.4.3.3.3" xref="A2.p5.2.m2.4.4.3.3.3.cmml"><mi id="A2.p5.2.m2.4.4.3.3.3.2.2" xref="A2.p5.2.m2.4.4.3.3.3.2.2.cmml">w</mi><mi id="A2.p5.2.m2.4.4.3.3.3.2.3" xref="A2.p5.2.m2.4.4.3.3.3.2.3.cmml">M</mi><mi id="A2.p5.2.m2.4.4.3.3.3.3" xref="A2.p5.2.m2.4.4.3.3.3.3.cmml">o</mi></msubsup><mo stretchy="false" id="A2.p5.2.m2.4.4.3.3.8" xref="A2.p5.2.m2.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p5.2.m2.4b"><apply id="A2.p5.2.m2.4.4.cmml" xref="A2.p5.2.m2.4.4"><eq id="A2.p5.2.m2.4.4.4.cmml" xref="A2.p5.2.m2.4.4.4"></eq><ci id="A2.p5.2.m2.4.4.5.cmml" xref="A2.p5.2.m2.4.4.5">𝑦</ci><list id="A2.p5.2.m2.4.4.3.4.cmml" xref="A2.p5.2.m2.4.4.3.3"><apply id="A2.p5.2.m2.2.2.1.1.1.cmml" xref="A2.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p5.2.m2.2.2.1.1.1.1.cmml" xref="A2.p5.2.m2.2.2.1.1.1">superscript</csymbol><apply id="A2.p5.2.m2.2.2.1.1.1.2.cmml" xref="A2.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p5.2.m2.2.2.1.1.1.2.1.cmml" xref="A2.p5.2.m2.2.2.1.1.1">subscript</csymbol><ci id="A2.p5.2.m2.2.2.1.1.1.2.2.cmml" xref="A2.p5.2.m2.2.2.1.1.1.2.2">𝑤</ci><cn type="integer" id="A2.p5.2.m2.2.2.1.1.1.2.3.cmml" xref="A2.p5.2.m2.2.2.1.1.1.2.3">1</cn></apply><ci id="A2.p5.2.m2.2.2.1.1.1.3.cmml" xref="A2.p5.2.m2.2.2.1.1.1.3">𝑜</ci></apply><apply id="A2.p5.2.m2.3.3.2.2.2.cmml" xref="A2.p5.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p5.2.m2.3.3.2.2.2.1.cmml" xref="A2.p5.2.m2.3.3.2.2.2">superscript</csymbol><apply id="A2.p5.2.m2.3.3.2.2.2.2.cmml" xref="A2.p5.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p5.2.m2.3.3.2.2.2.2.1.cmml" xref="A2.p5.2.m2.3.3.2.2.2">subscript</csymbol><ci id="A2.p5.2.m2.3.3.2.2.2.2.2.cmml" xref="A2.p5.2.m2.3.3.2.2.2.2.2">𝑤</ci><cn type="integer" id="A2.p5.2.m2.3.3.2.2.2.2.3.cmml" xref="A2.p5.2.m2.3.3.2.2.2.2.3">2</cn></apply><ci id="A2.p5.2.m2.3.3.2.2.2.3.cmml" xref="A2.p5.2.m2.3.3.2.2.2.3">𝑜</ci></apply><ci id="A2.p5.2.m2.1.1.cmml" xref="A2.p5.2.m2.1.1">…</ci><apply id="A2.p5.2.m2.4.4.3.3.3.cmml" xref="A2.p5.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p5.2.m2.4.4.3.3.3.1.cmml" xref="A2.p5.2.m2.4.4.3.3.3">superscript</csymbol><apply id="A2.p5.2.m2.4.4.3.3.3.2.cmml" xref="A2.p5.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p5.2.m2.4.4.3.3.3.2.1.cmml" xref="A2.p5.2.m2.4.4.3.3.3">subscript</csymbol><ci id="A2.p5.2.m2.4.4.3.3.3.2.2.cmml" xref="A2.p5.2.m2.4.4.3.3.3.2.2">𝑤</ci><ci id="A2.p5.2.m2.4.4.3.3.3.2.3.cmml" xref="A2.p5.2.m2.4.4.3.3.3.2.3">𝑀</ci></apply><ci id="A2.p5.2.m2.4.4.3.3.3.3.cmml" xref="A2.p5.2.m2.4.4.3.3.3.3">𝑜</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.2.m2.4c">y=[w_{1}^{o},w_{2}^{o},\dots,w_{M}^{o}]</annotation></semantics></math>.
It is shared by many realistic applications such as summarization, response generation in dialogue systems, machine translation, etc.</p>
</div>
<div id="A2.p6" class="ltx_para ltx_noindent">
<p id="A2.p6.3" class="ltx_p"><span id="A2.p6.3.1" class="ltx_text ltx_font_bold">Language Modeling (LM)</span>
The left-to-right language modeling task considers a sequence of words as the input <math id="A2.p6.1.m1.4" class="ltx_Math" alttext="x=[w_{1},w_{2},\dots,w_{n}]" display="inline"><semantics id="A2.p6.1.m1.4a"><mrow id="A2.p6.1.m1.4.4" xref="A2.p6.1.m1.4.4.cmml"><mi id="A2.p6.1.m1.4.4.5" xref="A2.p6.1.m1.4.4.5.cmml">x</mi><mo id="A2.p6.1.m1.4.4.4" xref="A2.p6.1.m1.4.4.4.cmml">=</mo><mrow id="A2.p6.1.m1.4.4.3.3" xref="A2.p6.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="A2.p6.1.m1.4.4.3.3.4" xref="A2.p6.1.m1.4.4.3.4.cmml">[</mo><msub id="A2.p6.1.m1.2.2.1.1.1" xref="A2.p6.1.m1.2.2.1.1.1.cmml"><mi id="A2.p6.1.m1.2.2.1.1.1.2" xref="A2.p6.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="A2.p6.1.m1.2.2.1.1.1.3" xref="A2.p6.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="A2.p6.1.m1.4.4.3.3.5" xref="A2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="A2.p6.1.m1.3.3.2.2.2" xref="A2.p6.1.m1.3.3.2.2.2.cmml"><mi id="A2.p6.1.m1.3.3.2.2.2.2" xref="A2.p6.1.m1.3.3.2.2.2.2.cmml">w</mi><mn id="A2.p6.1.m1.3.3.2.2.2.3" xref="A2.p6.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="A2.p6.1.m1.4.4.3.3.6" xref="A2.p6.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="A2.p6.1.m1.1.1" xref="A2.p6.1.m1.1.1.cmml">…</mi><mo id="A2.p6.1.m1.4.4.3.3.7" xref="A2.p6.1.m1.4.4.3.4.cmml">,</mo><msub id="A2.p6.1.m1.4.4.3.3.3" xref="A2.p6.1.m1.4.4.3.3.3.cmml"><mi id="A2.p6.1.m1.4.4.3.3.3.2" xref="A2.p6.1.m1.4.4.3.3.3.2.cmml">w</mi><mi id="A2.p6.1.m1.4.4.3.3.3.3" xref="A2.p6.1.m1.4.4.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="A2.p6.1.m1.4.4.3.3.8" xref="A2.p6.1.m1.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.1.m1.4b"><apply id="A2.p6.1.m1.4.4.cmml" xref="A2.p6.1.m1.4.4"><eq id="A2.p6.1.m1.4.4.4.cmml" xref="A2.p6.1.m1.4.4.4"></eq><ci id="A2.p6.1.m1.4.4.5.cmml" xref="A2.p6.1.m1.4.4.5">𝑥</ci><list id="A2.p6.1.m1.4.4.3.4.cmml" xref="A2.p6.1.m1.4.4.3.3"><apply id="A2.p6.1.m1.2.2.1.1.1.cmml" xref="A2.p6.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="A2.p6.1.m1.2.2.1.1.1.1.cmml" xref="A2.p6.1.m1.2.2.1.1.1">subscript</csymbol><ci id="A2.p6.1.m1.2.2.1.1.1.2.cmml" xref="A2.p6.1.m1.2.2.1.1.1.2">𝑤</ci><cn type="integer" id="A2.p6.1.m1.2.2.1.1.1.3.cmml" xref="A2.p6.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="A2.p6.1.m1.3.3.2.2.2.cmml" xref="A2.p6.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="A2.p6.1.m1.3.3.2.2.2.1.cmml" xref="A2.p6.1.m1.3.3.2.2.2">subscript</csymbol><ci id="A2.p6.1.m1.3.3.2.2.2.2.cmml" xref="A2.p6.1.m1.3.3.2.2.2.2">𝑤</ci><cn type="integer" id="A2.p6.1.m1.3.3.2.2.2.3.cmml" xref="A2.p6.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="A2.p6.1.m1.1.1.cmml" xref="A2.p6.1.m1.1.1">…</ci><apply id="A2.p6.1.m1.4.4.3.3.3.cmml" xref="A2.p6.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="A2.p6.1.m1.4.4.3.3.3.1.cmml" xref="A2.p6.1.m1.4.4.3.3.3">subscript</csymbol><ci id="A2.p6.1.m1.4.4.3.3.3.2.cmml" xref="A2.p6.1.m1.4.4.3.3.3.2">𝑤</ci><ci id="A2.p6.1.m1.4.4.3.3.3.3.cmml" xref="A2.p6.1.m1.4.4.3.3.3.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.1.m1.4c">x=[w_{1},w_{2},\dots,w_{n}]</annotation></semantics></math> and a token <math id="A2.p6.2.m2.1" class="ltx_Math" alttext="y=w_{n+1}" display="inline"><semantics id="A2.p6.2.m2.1a"><mrow id="A2.p6.2.m2.1.1" xref="A2.p6.2.m2.1.1.cmml"><mi id="A2.p6.2.m2.1.1.2" xref="A2.p6.2.m2.1.1.2.cmml">y</mi><mo id="A2.p6.2.m2.1.1.1" xref="A2.p6.2.m2.1.1.1.cmml">=</mo><msub id="A2.p6.2.m2.1.1.3" xref="A2.p6.2.m2.1.1.3.cmml"><mi id="A2.p6.2.m2.1.1.3.2" xref="A2.p6.2.m2.1.1.3.2.cmml">w</mi><mrow id="A2.p6.2.m2.1.1.3.3" xref="A2.p6.2.m2.1.1.3.3.cmml"><mi id="A2.p6.2.m2.1.1.3.3.2" xref="A2.p6.2.m2.1.1.3.3.2.cmml">n</mi><mo id="A2.p6.2.m2.1.1.3.3.1" xref="A2.p6.2.m2.1.1.3.3.1.cmml">+</mo><mn id="A2.p6.2.m2.1.1.3.3.3" xref="A2.p6.2.m2.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.2.m2.1b"><apply id="A2.p6.2.m2.1.1.cmml" xref="A2.p6.2.m2.1.1"><eq id="A2.p6.2.m2.1.1.1.cmml" xref="A2.p6.2.m2.1.1.1"></eq><ci id="A2.p6.2.m2.1.1.2.cmml" xref="A2.p6.2.m2.1.1.2">𝑦</ci><apply id="A2.p6.2.m2.1.1.3.cmml" xref="A2.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="A2.p6.2.m2.1.1.3.1.cmml" xref="A2.p6.2.m2.1.1.3">subscript</csymbol><ci id="A2.p6.2.m2.1.1.3.2.cmml" xref="A2.p6.2.m2.1.1.3.2">𝑤</ci><apply id="A2.p6.2.m2.1.1.3.3.cmml" xref="A2.p6.2.m2.1.1.3.3"><plus id="A2.p6.2.m2.1.1.3.3.1.cmml" xref="A2.p6.2.m2.1.1.3.3.1"></plus><ci id="A2.p6.2.m2.1.1.3.3.2.cmml" xref="A2.p6.2.m2.1.1.3.3.2">𝑛</ci><cn type="integer" id="A2.p6.2.m2.1.1.3.3.3.cmml" xref="A2.p6.2.m2.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.2.m2.1c">y=w_{n+1}</annotation></semantics></math> as the output.
The output token is expected to be the most plausible next word of the incomplete sentence denoted as <math id="A2.p6.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="A2.p6.3.m3.1a"><mi id="A2.p6.3.m3.1.1" xref="A2.p6.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="A2.p6.3.m3.1b"><ci id="A2.p6.3.m3.1.1.cmml" xref="A2.p6.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.3.m3.1c">x</annotation></semantics></math>.
Although the direct application of LM is limited,
a high-performance pre-trained language model can benefit a wide range of NLP applications (as above) via fine-tuning.
It also serves as an excellent test bed as it requires no human annotations at all.</p>
</div>
<figure id="A2.F8" class="ltx_figure"><img src="/html/2104.08815/assets/x8.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="392" height="380" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 8: </span>The probability density of quantity of training examples in each of the 100 clients on the <span id="A2.F8.8.1" class="ltx_text ltx_font_italic">20News</span> dataset with different <math id="A2.F8.4.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A2.F8.4.m1.1b"><mi id="A2.F8.4.m1.1.1" xref="A2.F8.4.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.F8.4.m1.1c"><ci id="A2.F8.4.m1.1.1.cmml" xref="A2.F8.4.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.F8.4.m1.1d">\beta</annotation></semantics></math>. When <math id="A2.F8.5.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A2.F8.5.m2.1b"><mi id="A2.F8.5.m2.1.1" xref="A2.F8.5.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.F8.5.m2.1c"><ci id="A2.F8.5.m2.1.1.cmml" xref="A2.F8.5.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.F8.5.m2.1d">\beta</annotation></semantics></math> is larger, then all clients share more similar numbers of examples; when <math id="A2.F8.6.m3.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="A2.F8.6.m3.1b"><mi id="A2.F8.6.m3.1.1" xref="A2.F8.6.m3.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="A2.F8.6.m3.1c"><ci id="A2.F8.6.m3.1.1.cmml" xref="A2.F8.6.m3.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.F8.6.m3.1d">\beta</annotation></semantics></math> is smaller, then the range of the quantity is much wider — i.e., the larger differences between clients in terms of their sizes of datasets.</figcaption>
</figure>
<div id="A2.p7" class="ltx_para ltx_noindent">
<p id="A2.p7.1" class="ltx_p"><span id="A2.p7.1.1" class="ltx_text ltx_font_bold">Others.</span> There are some other applications that not are covered by the above four basic formulations, and our extensible platform (detailed in Section <a href="#A6" title="Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>) enables users to easily implement their specific tasks.
For each task formulation, we show which datasets are used in FedNLP and how we partition them in Section <a href="#S3" title="3 Benchmarking Setup with FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Приложение C </span>Implementation Details</h2>

<section id="A3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Non-IID. Label Distribution</h4>

<div id="A3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px1.p1.1" class="ltx_p">Note that this might cause a few clients not to have enough examples to sample for particular labels if they are already used up.
Prior works choose to stop assigning early and remove such clients, but it consequently loses the other unused examples and also causes the inconsistency of client numbers.
Thus, to avoid these issues, we propose a <span id="A3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">dynamic reassigning</span> method which complement the vacancy of a label by filling in the examples of other labels based on their current ratio of remaining unassigned examples.</p>
</div>
</section>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>The FedNLP Training Pipeline: Security and Efficiency</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">Under the definition of federated learning in <a href="#algorithm1" title="In 2.2 Our Unified Framework for FL ‣ 2 Federated Learning for NLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Algorithm</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we design a training system to support the research of NLP in the FL paradigm. We highlight its core capabilities and design as follows.</p>
</div>
<section id="A3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Supporting diverse FL algorithms.</h4>

<div id="A3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A3.SS1.SSS0.Px1.p1.1" class="ltx_p"><span id="A3.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">FedNLP</span> aims to enable flexible customization for future algorithmic innovations. We have supported a number of classical federated learning algorithms, including FedAvg <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib40" title="" class="ltx_ref">2017a</a>)</cite>, FedOPT <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>, and FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020b</a>)</cite>. These algorithms follow the same framework introduced in Algorithm <a href="#algorithm1" title="Algorithm 1 ‣ 2.2 Our Unified Framework for FL ‣ 2 Federated Learning for NLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The algorithmic APIs are modularized: all data loaders follow the same format of input and output arguments, which are compatible with different models and algorithms and are easy to support new datasets; the method of defining the model and related trainer is kept the same as in centralized training to reduce the difficulty of developing the distributed training framework. For new FL algorithm development, worker-oriented programming reduces the difficulty of message passing and definition. More details are introduced in Appendix <a href="#A6.SS3" title="F.3 The Algorithm Layer ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F.3</span></a>.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Enabling secure benchmarking with lightweight secure aggregation.</h4>

<div id="A3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A3.SS1.SSS0.Px2.p1.1" class="ltx_p">In particular, <span id="A3.SS1.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_typewriter">FedNLP</span> enhances the security aspect of federated training, which is not supported by existing non-NLP-oriented benchmarking libraries (e.g., TFF, LEAF). This is motivated by the fact that model weights from clients may still have the risk of privacy leakage <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib68" title="" class="ltx_ref">2019</a>)</cite>. To break this barrier, we integrate secure aggregation (SA) algorithms to the <span id="A3.SS1.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">FedNLP</span> system. NLP researchers do not need to master security-related knowledge and also benefit from a secure distributed training environment. To be more specific, <span id="A3.SS1.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_typewriter">FedNLP</span> supports state-of-the-art SA algorithms <span id="A3.SS1.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_typewriter">LightSecAgg</span>, <span id="A3.SS1.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_typewriter">SecAgg</span> <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>, and <span id="A3.SS1.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_typewriter">SecAgg</span>+ <cite class="ltx_cite ltx_citemacro_cite">Bell et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. At a high-level understanding, SA protects the client model by generating a single random mask and allows their cancellation when aggregated at the server. Consequently, the server can only see the aggregated model and not the raw model from each client. In this work, our main effort is to design and optimize these SA algorithms in the context of the <span id="A3.SS1.SSS0.Px2.p1.1.7" class="ltx_text ltx_font_typewriter">FedNLP</span> system. We provide an algorithmic performance comparison in Appendix <a href="#A6.SS5" title="F.5 Enhancing Security with Secure Aggregation (SA) ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F.5</span></a>.</p>
</div>
</section>
<section id="A3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Realistic evaluation with efficient distributed system design.</h4>

<div id="A3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="A3.SS1.SSS0.Px3.p1.1" class="ltx_p"><span id="A3.SS1.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">FedNLP</span> aims to support distributed training in multiple edge servers (e.g, AWS EC2) or edge devices (e.g., IoTs and smartphones). To achieve this, the system is designed with three layers: the application layer, the algorithm layer, and the infrastructure layer.
At the application layer, FedNLP provides three modules:
data management, model definition, and a single-process trainer for all task formats;
at the algorithm layer, FedNLP supports various FL algorithms;
at the infrastructure layer,
FedNLP aims at integrating single-process trainers with a distributed learning system for FL.
Specifically, we make each layer and module perform its own duties and have a high degree of modularization. We refer readers to Appendix <a href="#A6" title="Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a> for a detailed description of the system architecture and design philosophy.</p>
</div>
</section>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Приложение D </span>More Related Works</h2>

<section id="A4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Federated Learning Methods.</h4>

<div id="A4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p1.1" class="ltx_p">Federated Learning (FL) is a widely disciplinary research area that mainly focuses on three aspects: statistical challenge, trustworthiness, and system optimization. Numerous methods have been proposed to solve statistical challenges, including FedAvg <cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib41" title="" class="ltx_ref">2017b</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020b</a>)</cite>, FedOPT <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>, FedNAS <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020a</a>, <a href="#bib.bib21" title="" class="ltx_ref">d</a>)</cite>, and FedMA <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib63" title="" class="ltx_ref">2020b</a>)</cite> that alleviate the non-IID issue with distributed optimization, and new formulations, MOCHA <cite class="ltx_cite ltx_citemacro_cite">Smith et al. (<a href="#bib.bib54" title="" class="ltx_ref">2017</a>)</cite>, pFedMe <cite class="ltx_cite ltx_citemacro_cite">Dinh et al. (<a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite>, perFedAvg <cite class="ltx_cite ltx_citemacro_cite">Fallah et al. (<a href="#bib.bib12" title="" class="ltx_ref">2020</a>)</cite>, and Ditto <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib32" title="" class="ltx_ref">2021b</a>)</cite>, that consider personalization and fairness in federated training.</p>
</div>
<div id="A4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p2.1" class="ltx_p">For trustworthiness, security and privacy are the two main research directions that are mainly concerned with resisting data or model attacks, reconstruction, and leakage during training <cite class="ltx_cite ltx_citemacro_cite">So et al. (<a href="#bib.bib57" title="" class="ltx_ref">2021b</a>, <a href="#bib.bib56" title="" class="ltx_ref">a</a>, <a href="#bib.bib55" title="" class="ltx_ref">2020</a>); Prakash et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>); Prakash and Avestimehr (<a href="#bib.bib44" title="" class="ltx_ref">2020</a>); Elkordy and Avestimehr (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>); Prakash et al. (<a href="#bib.bib45" title="" class="ltx_ref">2020</a>); Wang et al. (<a href="#bib.bib62" title="" class="ltx_ref">2020a</a>); Lyu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2020</a>)</cite>.
Given that modern deep neural networks are over-parameterized and dominate nearly all learning tasks, researchers also proposed algorithms or systems to improve the efficiency and scalability of edge training <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib17" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib19" title="" class="ltx_ref">c</a>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>, <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>. We refer readers to the canonical survey <cite class="ltx_cite ltx_citemacro_cite">Kairouz et al. (<a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> for details.</p>
</div>
<div id="A4.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p3.1" class="ltx_p">Although tremendous progress has been made in the past few years, these algorithms or systems have not been fully evaluated on realistic NLP tasks introduced in this paper.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Приложение E </span>Future Directions</h2>

<section id="A5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Minimizing the performance gap.</h4>

<div id="A5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px1.p1.1" class="ltx_p">In the FL setting, we demonstrate that federated fine-tuning still has a large accuracy gap in the non-IID dataset compared to centralized fine-tuning. Developing algorithms for Transformer models with NLP tasks is of the highest priority.</p>
</div>
</section>
<section id="A5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Improving the system efficiency and scalability.</h4>

<div id="A5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px2.p1.1" class="ltx_p">Transformer models are usually large, while resource-constrained edge devices may not be able to run large models.
Designing efficient FL methods for NLP tasks is thus a practical problem worth solving.
How to adopt a reasonable user selection mechanism to avoid stragglers and speed up the convergence of training algorithms is also a pressing problem to be solved.</p>
</div>
</section>
<section id="A5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Trustworthy and privacy-preserving NLP.</h4>

<div id="A5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px3.p1.1" class="ltx_p">We argue that it is an important future research direction to analyze and assure the privacy-preserving ability of these methods, although our focus in this paper is the implementation and performance analysis of the FL methods for NLP tasks.
It is now an open problem for both FL and NLP areas, while it is an orthogonal goal for improving the trustworthy of decentralized learning, and it is only possible to study privacy preservation when we have an existing FL+NLP platform. This is also part of our motivation in proposing FedNLP, and we believe our framework provides a set of flexible interfaces for future development to analyze and improve the privacy-preserving ability of FL methods for NLP tasks and beyond.
</p>
</div>
</section>
<section id="A5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Personalized FedNLP.</h4>

<div id="A5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A5.SS0.SSS0.Px4.p1.1" class="ltx_p">From the perspective of the data itself, user-generated text is inherently personalized. Designing personalized algorithms to improve model accuracy or fairness is a very promising direction. In addition, it is also an interesting problem to adapt the heterogeneous model architecture for each client in the FL network. We show that it is feasible to only fine-tune a small amount of the parameters of LMs, so it is promising to adapt recent prefix-tuning methods <cite class="ltx_cite ltx_citemacro_cite">Li and Liang (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> for personalizing the parameters of NLP models within the FedNLP framework.</p>
</div>
</section>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Приложение F </span>The System Design of FedNLP</h2>

<figure id="A6.F9" class="ltx_figure"><img src="/html/2104.08815/assets/figure/FedNLP.png" id="A6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="462" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Рис. 9: </span>The overall workflow and system design of the proposed FedNLP platform. </figcaption>
</figure>
<div id="A6.p1" class="ltx_para">
<p id="A6.p1.1" class="ltx_p">The FedNLP platform consists of three layers:
the application layer, the algorithm layer, and the infrastructure layer.
At the application layer, FedNLP provides three modules:
data management, model definition, and single-process trainer for all task formats;
At the algorithm layer, FedNLP supports various FL algorithms;
At the infrastructure layer,
FedNLP aims at integrating single-process trainers with a distributed learning system for FL.
Specifically, we make each layer and module perform its own duties and have a high degree of modularization.</p>
</div>
<section id="A6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Overall Workflow</h3>

<div id="A6.SS1.p1" class="ltx_para">
<p id="A6.SS1.p1.1" class="ltx_p">The module calling logic flow of the whole framework is shown on the left of Figure <a href="#A6.F9" title="Figure 9 ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.
When we start the federated training,
we first complete the launcher script, device allocation, data loading, and model creation, and finally call the API of the federated learning algorithm. This process is expressed in Python-style code (see Alg. <a href="#algorithm2" title="Algorithm 2 ‣ F.1 Overall Workflow ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="algorithm2" class="ltx_float ltx_algorithm">
<div id="algorithm2.2" class="ltx_listing ltx_lst_numbers_left ltx_listing">
<div id="algorithm2.2.1" class="ltx_listingline">





<div id="algorithm2.2.1.1" class="ltx_listing ltx_lst_language_python ltx_lstlisting ltx_listing" style="background-color:#FFFFFF;">
<div class="ltx_listing_data"><a href="data:text/plain;base64,IyB1c2luZyB0ZXh0IGNsYXNzaWZpY2F0aW9uIChUQykgYXMgYW4gZXhhbXBsZQoKIyBpbml0aWFsaXplIGRpc3RyaWJ1dGVkIGNvbXB1dGluZyBlbnZpcm9ubWVudApwcm9jZXNzX2lkLCAuLi4gPSBGZWROTFBfaW5pdCgpCgojIEdQVSBkZXZpY2UgbWFuYWdlbWVudApkZXZpY2UgPSAgbWFwX3Byb2Nlc3NfdG9fZ3B1KHByb2Nlc3NfaWQsIC4uLikKCiMgZGF0YSBtYW5hZ2VtZW50CmRhdGFfbWFuYWdlciA9IFRDRGF0YU1hbmFnZXIgKHByb2Nlc3NfaWQsIC4uLikKIyBsb2FkIHRoZSBkYXRhIGRpY3Rpb25hcnkgYnkgcHJvY2Vzc19pZApkYXRhX2RpY3QgPSBkbS5sb2FkX2ZlZGVyYXRlZF9kYXRhKHByb2Nlc3NfaWQpCgojIGNyZWF0ZSBtb2RlbCBieSBzcGVjaWZ5aW5nIHRoZSB0YXNrCmNsaWVudF9tb2RlbCwgLi4uID0gY3JlYXRlX21vZGVsKG1vZGVsX2FyZ3MsICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICBmb3JtdWxhdGlvbj0iY2xhc3NpZmljYXRpb24iKQoKIyBkZWZpbmUgYSBjdXN0b21pemVkIE5MUCBUcmFpbmVyCmNsaWVudF90cmFpbmVyID0gVENUcmFpbmVyKGRldmljZSwKICAgICAgICAgICAgICAgICAgICAgICAgICAgY2xpZW50X21vZGVsLCAuLi4pCgojIGxhdW5jaCB0aGUgZmVkZXJhdGVkIHRyYWluaW5nIChlLmcuLCBGZWRBdmcpCkZlZEF2Z19kaXN0cmlidXRlZCguLi4sIGRldmljZSwKICAgICAgICAgICAgICAgICAgICAgICAgY2xpZW50X21vZGVsLAogICAgICAgICAgICAgICAgICAgICAgICBkYXRhX2RpY3QsIC4uLiwKICAgICAgICAgICAgICAgICAgICAgICAgY2xpZW50X3RyYWluZXIpCg==" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
<span id="lstnumberx1.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx1.1.1" class="ltx_text ltx_lst_space"> </span>using<span id="lstnumberx1.1.2" class="ltx_text ltx_lst_space"> </span>text<span id="lstnumberx1.1.3" class="ltx_text ltx_lst_space"> </span>classification<span id="lstnumberx1.1.4" class="ltx_text ltx_lst_space"> </span>(TC)<span id="lstnumberx1.1.5" class="ltx_text ltx_lst_space"> </span>as<span id="lstnumberx1.1.6" class="ltx_text ltx_lst_space"> </span>an<span id="lstnumberx1.1.7" class="ltx_text ltx_lst_space"> </span>example</span>
</div>
<div id="lstnumberx2" class="ltx_listingline">
</div>
<div id="lstnumberx3" class="ltx_listingline">
<span id="lstnumberx3.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx3.1.1" class="ltx_text ltx_lst_space"> </span>initialize<span id="lstnumberx3.1.2" class="ltx_text ltx_lst_space"> </span>distributed<span id="lstnumberx3.1.3" class="ltx_text ltx_lst_space"> </span>computing<span id="lstnumberx3.1.4" class="ltx_text ltx_lst_space"> </span>environment</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">process_id</span><span id="lstnumberx4.2" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx4.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.4" class="ltx_text ltx_font_typewriter">...</span><span id="lstnumberx4.5" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.6" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx4.7" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx4.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">FedNLP_init</span><span id="lstnumberx4.9" class="ltx_text ltx_font_typewriter">()</span>
</div>
<div id="lstnumberx5" class="ltx_listingline">
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx6.1.1" class="ltx_text ltx_lst_space"> </span>GPU<span id="lstnumberx6.1.2" class="ltx_text ltx_lst_space"> </span>device<span id="lstnumberx6.1.3" class="ltx_text ltx_lst_space"> </span>management</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">device</span><span id="lstnumberx7.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx7.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx7.4" class="ltx_text ltx_lst_space ltx_font_typewriter">  </span><span id="lstnumberx7.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">map_process_to_gpu</span><span id="lstnumberx7.6" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx7.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">process_id</span><span id="lstnumberx7.8" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx7.9" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx7.10" class="ltx_text ltx_font_typewriter">...)</span>
</div>
<div id="lstnumberx8" class="ltx_listingline">
</div>
<div id="lstnumberx9" class="ltx_listingline">
<span id="lstnumberx9.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx9.1.1" class="ltx_text ltx_lst_space"> </span>data<span id="lstnumberx9.1.2" class="ltx_text ltx_lst_space"> </span>management</span>
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">data_manager</span><span id="lstnumberx10.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx10.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx10.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx10.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">TCDataManager</span><span id="lstnumberx10.6" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx10.7" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx10.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">process_id</span><span id="lstnumberx10.9" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx10.10" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx10.11" class="ltx_text ltx_font_typewriter">...)</span>
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span id="lstnumberx11.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx11.1.1" class="ltx_text ltx_lst_space"> </span>load<span id="lstnumberx11.1.2" class="ltx_text ltx_lst_space"> </span>the<span id="lstnumberx11.1.3" class="ltx_text ltx_lst_space"> </span>data<span id="lstnumberx11.1.4" class="ltx_text ltx_lst_space"> </span>dictionary<span id="lstnumberx11.1.5" class="ltx_text ltx_lst_space"> </span>by<span id="lstnumberx11.1.6" class="ltx_text ltx_lst_space"> </span>process_id</span>
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span id="lstnumberx12.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">data_dict</span><span id="lstnumberx12.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx12.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx12.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx12.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">dm</span><span id="lstnumberx12.6" class="ltx_text ltx_font_typewriter">.</span><span id="lstnumberx12.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">load_federated_data</span><span id="lstnumberx12.8" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx12.9" class="ltx_text ltx_lst_identifier ltx_font_typewriter">process_id</span><span id="lstnumberx12.10" class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx13" class="ltx_listingline">
</div>
<div id="lstnumberx14" class="ltx_listingline">
<span id="lstnumberx14.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx14.1.1" class="ltx_text ltx_lst_space"> </span>create<span id="lstnumberx14.1.2" class="ltx_text ltx_lst_space"> </span>model<span id="lstnumberx14.1.3" class="ltx_text ltx_lst_space"> </span>by<span id="lstnumberx14.1.4" class="ltx_text ltx_lst_space"> </span>specifying<span id="lstnumberx14.1.5" class="ltx_text ltx_lst_space"> </span>the<span id="lstnumberx14.1.6" class="ltx_text ltx_lst_space"> </span>task</span>
</div>
<div id="lstnumberx15" class="ltx_listingline">
<span id="lstnumberx15.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">client_model</span><span id="lstnumberx15.2" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx15.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx15.4" class="ltx_text ltx_font_typewriter">...</span><span id="lstnumberx15.5" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx15.6" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx15.7" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx15.8" class="ltx_text ltx_lst_identifier ltx_font_typewriter">create_model</span><span id="lstnumberx15.9" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx15.10" class="ltx_text ltx_lst_identifier ltx_font_typewriter">model_args</span><span id="lstnumberx15.11" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx15.12" class="ltx_text ltx_lst_space ltx_font_typewriter">                                                        </span><span id="lstnumberx15.13" class="ltx_text ltx_lst_identifier ltx_font_typewriter">formulation</span><span id="lstnumberx15.14" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx15.15" class="ltx_text ltx_lst_string ltx_font_typewriter">"classification"missing</span><span id="lstnumberx15.16" class="ltx_text ltx_font_typewriter">)</span>
</div>
<div id="lstnumberx16" class="ltx_listingline">
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span id="lstnumberx17.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx17.1.1" class="ltx_text ltx_lst_space"> </span>define<span id="lstnumberx17.1.2" class="ltx_text ltx_lst_space"> </span>a<span id="lstnumberx17.1.3" class="ltx_text ltx_lst_space"> </span>customized<span id="lstnumberx17.1.4" class="ltx_text ltx_lst_space"> </span>NLP<span id="lstnumberx17.1.5" class="ltx_text ltx_lst_space"> </span>Trainer</span>
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span id="lstnumberx18.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">client_trainer</span><span id="lstnumberx18.2" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx18.3" class="ltx_text ltx_font_typewriter">=</span><span id="lstnumberx18.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx18.5" class="ltx_text ltx_lst_identifier ltx_font_typewriter">TCTrainer</span><span id="lstnumberx18.6" class="ltx_text ltx_font_typewriter">(</span><span id="lstnumberx18.7" class="ltx_text ltx_lst_identifier ltx_font_typewriter">device</span><span id="lstnumberx18.8" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span id="lstnumberx19.1" class="ltx_text ltx_lst_space ltx_font_typewriter">                           </span><span id="lstnumberx19.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">client_model</span><span id="lstnumberx19.3" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx19.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx19.5" class="ltx_text ltx_font_typewriter">...)</span>
</div>
<div id="lstnumberx20" class="ltx_listingline">
</div>
<div id="lstnumberx21" class="ltx_listingline">
<span id="lstnumberx21.1" class="ltx_text ltx_lst_comment ltx_font_typewriter" style="color:#408080;">#<span id="lstnumberx21.1.1" class="ltx_text ltx_lst_space"> </span>launch<span id="lstnumberx21.1.2" class="ltx_text ltx_lst_space"> </span>the<span id="lstnumberx21.1.3" class="ltx_text ltx_lst_space"> </span>federated<span id="lstnumberx21.1.4" class="ltx_text ltx_lst_space"> </span>training<span id="lstnumberx21.1.5" class="ltx_text ltx_lst_space"> </span>(e.g.,<span id="lstnumberx21.1.6" class="ltx_text ltx_lst_space"> </span>FedAvg)</span>
</div>
<div id="lstnumberx22" class="ltx_listingline">
<span id="lstnumberx22.1" class="ltx_text ltx_lst_identifier ltx_font_typewriter">FedAvg_distributed</span><span id="lstnumberx22.2" class="ltx_text ltx_font_typewriter">(...,</span><span id="lstnumberx22.3" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx22.4" class="ltx_text ltx_lst_identifier ltx_font_typewriter">device</span><span id="lstnumberx22.5" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span id="lstnumberx23.1" class="ltx_text ltx_lst_space ltx_font_typewriter">                        </span><span id="lstnumberx23.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">client_model</span><span id="lstnumberx23.3" class="ltx_text ltx_font_typewriter">,</span>
</div>
<div id="lstnumberx24" class="ltx_listingline">
<span id="lstnumberx24.1" class="ltx_text ltx_lst_space ltx_font_typewriter">                        </span><span id="lstnumberx24.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">data_dict</span><span id="lstnumberx24.3" class="ltx_text ltx_font_typewriter">,</span><span id="lstnumberx24.4" class="ltx_text ltx_lst_space ltx_font_typewriter"> </span><span id="lstnumberx24.5" class="ltx_text ltx_font_typewriter">...,</span>
</div>
<div id="lstnumberx25" class="ltx_listingline">
<span id="lstnumberx25.1" class="ltx_text ltx_lst_space ltx_font_typewriter">                        </span><span id="lstnumberx25.2" class="ltx_text ltx_lst_identifier ltx_font_typewriter">client_trainer</span><span id="lstnumberx25.3" class="ltx_text ltx_font_typewriter">)</span>
</div>
</div>

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="algorithm2.3.1.1" class="ltx_text ltx_font_bold">Algorithm 2</span> </span>The FedNLP Workflow</figcaption>
</figure>
</section>
<section id="A6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>The Application Layer</h3>

<section id="A6.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Management.</h4>

<div id="A6.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="A6.SS2.SSS0.Px1.p1.1" class="ltx_p">In data management, What <span id="A6.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">DataManager</span> does is control the whole workflow from loading data to returning trainable features. To be specific, <span id="A6.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_typewriter">DataManager</span> is set up for reading h5py data files and driving a <span id="A6.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_typewriter">preprocessor</span> to convert raw data to features. There are four types of <span id="A6.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_typewriter">DataManager</span> according to the task definition. Users can customize their <span id="A6.SS2.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_typewriter">DataManager</span> by inheriting one of the <span id="A6.SS2.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_typewriter">DataManager</span> class, specifying data operation functions, and embedding a particular preprocessor. Note that the raw data’s <span id="A6.SS2.SSS0.Px1.p1.1.7" class="ltx_text ltx_font_typewriter">H5Py</span> file and the non-IID partition file are preprocessed offline, while <span id="A6.SS2.SSS0.Px1.p1.1.8" class="ltx_text ltx_font_typewriter">DataManager</span> only loads them in runtime.</p>
</div>
</section>
<section id="A6.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model Definition.</h4>

<div id="A6.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="A6.SS2.SSS0.Px2.p1.1" class="ltx_p">We support two types of models: Transformer and LSTM. For Transformer models, to dock with the existing NLP ecology, our framework is compatible with the <span id="A6.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">HuggingFace Transformers</span> library <cite class="ltx_cite ltx_citemacro_cite">Wolf et al. (<a href="#bib.bib64" title="" class="ltx_ref">2020</a>)</cite>,
so that various types of Transformers can be directly reused without the need for re-implementation.
Specifically, our code is compatible with the three main classes of <span id="A6.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">Tokenizer</span>, <span id="A6.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_typewriter">Model</span>, and <span id="A6.SS2.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_typewriter">Config</span> in <span id="A6.SS2.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_italic">HuggingFace</span>.
Users can also customize them based on <span id="A6.SS2.SSS0.Px2.p1.1.6" class="ltx_text ltx_font_typewriter">HuggingFace</span>’s code. Although LSTM has gradually deviated from the mainstream, we still support LSTM to reflect the framework’s integrity, which may meet some particular use cases in a federated setting.</p>
</div>
</section>
<section id="A6.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">NLP Trainer (single process perspective).</h4>

<div id="A6.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="A6.SS2.SSS0.Px3.p1.1" class="ltx_p">As for the task-specific NLP <span id="A6.SS2.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_typewriter">Trainer</span>,
the most prominent feature is that it does not require users to have any background in distributed computing.
Users of FedNLP only need to complete single-process code writing.
A user should inherit the <span id="A6.SS2.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_typewriter">Trainer</span> class in the application layer to implement the four methods as shown in the figure: 1. the <span id="A6.SS2.SSS0.Px3.p1.1.3" class="ltx_text ltx_font_typewriter">get_model_params()</span> interface allows the algorithm layer to obtain model parameters and transmit them to the server; 2. the <span id="A6.SS2.SSS0.Px3.p1.1.4" class="ltx_text ltx_font_typewriter">set_model_params()</span> interface obtains the updated model from the server’s aggregation and then updates the model parameters of the local model; 3. the programming of the <span id="A6.SS2.SSS0.Px3.p1.1.5" class="ltx_text ltx_font_typewriter">train()</span> and <span id="A6.SS2.SSS0.Px3.p1.1.6" class="ltx_text ltx_font_typewriter">test()</span> function only needs to consider the data of a single user, meaning that the trainer is completely consistent with the centralized training.</p>
</div>
</section>
</section>
<section id="A6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.3 </span>The Algorithm Layer</h3>

<div id="A6.SS3.p1" class="ltx_para">
<p id="A6.SS3.p1.1" class="ltx_p">In the design of the algorithm layer, we follow the principle of one-line API. The parameters of the API include model, data, and single-process trainer (as shown in Algorithm <a href="#algorithm2" title="Algorithm 2 ‣ F.1 Overall Workflow ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The algorithms we support include:</p>
</div>
<section id="A6.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Centralized Training.</h4>

<div id="A6.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="A6.SS3.SSS0.Px1.p1.1" class="ltx_p">We concatenate all client datasets and use the global data <math id="A6.SS3.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{G}" display="inline"><semantics id="A6.SS3.SSS0.Px1.p1.1.m1.1a"><msub id="A6.SS3.SSS0.Px1.p1.1.m1.1.1" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A6.SS3.SSS0.Px1.p1.1.m1.1.1.2" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml">𝒟</mi><mi id="A6.SS3.SSS0.Px1.p1.1.m1.1.1.3" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="A6.SS3.SSS0.Px1.p1.1.m1.1b"><apply id="A6.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A6.SS3.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="A6.SS3.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1.2">𝒟</ci><ci id="A6.SS3.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="A6.SS3.SSS0.Px1.p1.1.m1.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.SS3.SSS0.Px1.p1.1.m1.1c">\mathcal{D}_{G}</annotation></semantics></math> to train a global model — i.e., the conventional protocol for learning an NLP model on a dataset.</p>
</div>
</section>
<section id="A6.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FedAvg</h4>

<div id="A6.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="A6.SS3.SSS0.Px2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">McMahan et al. (<a href="#bib.bib40" title="" class="ltx_ref">2017a</a>)</cite> is the <span id="A6.SS3.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">de facto</span> method for federated learning, assuming both client and server use the <span id="A6.SS3.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">SGD</span> optimizer for updating model weights.</p>
</div>
</section>
<section id="A6.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FedProx</h4>

<div id="A6.SS3.SSS0.Px3.p1" class="ltx_para">
<p id="A6.SS3.SSS0.Px3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib34" title="" class="ltx_ref">2020b</a>)</cite> can tackle statistical heterogeneity by restricting the local model updates to be closer to the initial (global) model with L2 regularization for better stability in training.</p>
</div>
</section>
<section id="A6.SS3.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FedOPT</h4>

<div id="A6.SS3.SSS0.Px4.p1" class="ltx_para">
<p id="A6.SS3.SSS0.Px4.p1.2" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite> is a generalized version of <span id="A6.SS3.SSS0.Px4.p1.2.1" class="ltx_text ltx_font_typewriter">FedAvg</span>.
There are two gradient-based optimizers in the algorithm:
<span id="A6.SS3.SSS0.Px4.p1.2.2" class="ltx_text ltx_font_typewriter">ClientOpt</span> and <span id="A6.SS3.SSS0.Px4.p1.2.3" class="ltx_text ltx_font_typewriter">ServerOpt</span> (please refer to the pseudo code in the original paper <cite class="ltx_cite ltx_citemacro_cite">Reddi et al. (<a href="#bib.bib49" title="" class="ltx_ref">2021</a>)</cite>).
While <span id="A6.SS3.SSS0.Px4.p1.2.4" class="ltx_text ltx_font_typewriter">ClientOpt</span> is used to update the local models, <span id="A6.SS3.SSS0.Px4.p1.2.5" class="ltx_text ltx_font_typewriter">SerevrOpt</span> treats the negative of aggregated local changes <math id="A6.SS3.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="-\Delta^{(t)}" display="inline"><semantics id="A6.SS3.SSS0.Px4.p1.1.m1.1a"><mrow id="A6.SS3.SSS0.Px4.p1.1.m1.1.2" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.cmml"><mo id="A6.SS3.SSS0.Px4.p1.1.m1.1.2a" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.cmml">−</mo><msup id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.cmml"><mi mathvariant="normal" id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.2" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.2.cmml">Δ</mi><mrow id="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.3" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.cmml"><mo stretchy="false" id="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.3.1" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.cmml">(</mo><mi id="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.1" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.3.2" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A6.SS3.SSS0.Px4.p1.1.m1.1b"><apply id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.cmml" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2"><minus id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.1.cmml" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2"></minus><apply id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.cmml" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.1.cmml" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2">superscript</csymbol><ci id="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.2.cmml" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.2.2.2">Δ</ci><ci id="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.1.cmml" xref="A6.SS3.SSS0.Px4.p1.1.m1.1.1.1.1">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.SS3.SSS0.Px4.p1.1.m1.1c">-\Delta^{(t)}</annotation></semantics></math> as a pseudo-gradient and applies it on the global model.
In our FedNLP framework, by default,
we set the <span id="A6.SS3.SSS0.Px4.p1.2.6" class="ltx_text ltx_font_typewriter">ClientOpt</span> to be AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite> and the <span id="A6.SS3.SSS0.Px4.p1.2.7" class="ltx_text ltx_font_typewriter">SerevrOpt</span> to be SGD with momentum (0.9) and fix server learning rate as <math id="A6.SS3.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="A6.SS3.SSS0.Px4.p1.2.m2.1a"><mn id="A6.SS3.SSS0.Px4.p1.2.m2.1.1" xref="A6.SS3.SSS0.Px4.p1.2.m2.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="A6.SS3.SSS0.Px4.p1.2.m2.1b"><cn type="float" id="A6.SS3.SSS0.Px4.p1.2.m2.1.1.cmml" xref="A6.SS3.SSS0.Px4.p1.2.m2.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.SS3.SSS0.Px4.p1.2.m2.1c">1.0</annotation></semantics></math>.</p>
</div>
<div id="A6.SS3.SSS0.Px4.p2" class="ltx_para">
<p id="A6.SS3.SSS0.Px4.p2.1" class="ltx_p">Each algorithm includes two core objects, <span id="A6.SS3.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_typewriter">ServerManager</span> and <span id="A6.SS3.SSS0.Px4.p2.1.2" class="ltx_text ltx_font_typewriter">ClientManager</span>, which integrate the communication module <span id="A6.SS3.SSS0.Px4.p2.1.3" class="ltx_text ltx_font_typewriter">ComManager</span> from the infrastructure layer and the <span id="A6.SS3.SSS0.Px4.p2.1.4" class="ltx_text ltx_font_typewriter">Trainer</span> of the training engine to complete the distributed algorithm protocol and edge training. Note that users can customize the Trainer by passing a customized <span id="A6.SS3.SSS0.Px4.p2.1.5" class="ltx_text ltx_font_typewriter">Trainer</span> through the algorithm API.</p>
</div>
</section>
</section>
<section id="A6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.4 </span>The Infrastructure Layer</h3>

<div id="A6.SS4.p1" class="ltx_para">
<p id="A6.SS4.p1.1" class="ltx_p">The infrastructure layer includes three modules:</p>
</div>
<div id="A6.SS4.p2" class="ltx_para ltx_noindent">
<p id="A6.SS4.p2.1" class="ltx_p">1) Users can write distributed scripts to manage GPU resource allocation.
In particular, FedNLP provides the GPU assignment API (<span id="A6.SS4.p2.1.1" class="ltx_text ltx_font_typewriter">map_process_to_gpu()</span> in Algorithm <a href="#algorithm2" title="Algorithm 2 ‣ F.1 Overall Workflow ‣ Приложение F The System Design of FedNLP ‣ NAACL 2022 Findings FedNLP: Benchmarking Federated Learning Methods for Natural Language Processing Tasks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) to assign specific GPUs to different FL Clients.</p>
</div>
<div id="A6.SS4.p3" class="ltx_para ltx_noindent">
<p id="A6.SS4.p3.1" class="ltx_p">2) The algorithm layer can use a unified and abstract <span id="A6.SS4.p3.1.1" class="ltx_text ltx_font_typewriter">ComManager</span> to complete a complex algorithmic communication protocol.
Currently, we support MPI (Message Passing Interface), RPC (Remote procedure call), and MQTT (Message Queuing Telemetry Transport) communication backend. MPI meets the distributed training needs in a single cluster; RPC meets the communication needs of cross-data centers (e.g., cross-silo federated learning); MQTT can meet the communication needs of smartphones or IoT devices.</p>
</div>
<div id="A6.SS4.p4" class="ltx_para ltx_noindent">
<p id="A6.SS4.p4.1" class="ltx_p">3) The third part is the training engine, which reuses the existing deep learning training engines by presenting as the <span id="A6.SS4.p4.1.1" class="ltx_text ltx_font_typewriter">Trainer</span> class. Our current version of this module is built on <span id="A6.SS4.p4.1.2" class="ltx_text ltx_font_typewriter">PyTorch</span>, but it can easily support frameworks such as <span id="A6.SS4.p4.1.3" class="ltx_text ltx_font_typewriter">TensorFlow</span>.
In the future, we may consider supporting the lightweight edge training engine optimized by the compiler technology at this level.</p>
</div>
</section>
<section id="A6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.5 </span>Enhancing Security with Secure Aggregation (SA)</h3>

<div id="A6.SS5.p1" class="ltx_para">
<p id="A6.SS5.p1.1" class="ltx_p"><span id="A6.SS5.p1.1.1" class="ltx_text ltx_font_typewriter">FedNLP</span> supports state-of-the-art SA algorithms <span id="A6.SS5.p1.1.2" class="ltx_text ltx_font_typewriter">LightSecAgg</span>, <span id="A6.SS5.p1.1.3" class="ltx_text ltx_font_typewriter">SecAgg</span> <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite>, and <span id="A6.SS5.p1.1.4" class="ltx_text ltx_font_typewriter">SecAgg+</span> <cite class="ltx_cite ltx_citemacro_cite">Bell et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>. Here, we provide a short performance comparison of these three algorithms. In general, <span id="A6.SS5.p1.1.5" class="ltx_text ltx_font_typewriter">LightSecAgg</span> provides the same model privacy guarantees as SecAgg <cite class="ltx_cite ltx_citemacro_cite">Bonawitz et al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> and SecAgg+ <cite class="ltx_cite ltx_citemacro_cite">Bell et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite>) while substantially reducing the aggregation (hence run-time) complexity (Figure <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:fig:runtime_CNN_varDropout</span>). The main idea of <span id="A6.SS5.p1.1.6" class="ltx_text ltx_font_typewriter">LightSecAgg</span> are that each user protects its local model using a locally generated random mask. This mask is then encoded and shared with other users, in such a way that the aggregate mask of any sufficiently large set of surviving users can be directly reconstructed at the server. Our main effort in <span id="A6.SS5.p1.1.7" class="ltx_text ltx_font_typewriter">FedNLP</span> is integrating these algorithms, optimizing its system performance, and designing user-friendly APIs to make them compatible with NLP models and FL algorithms.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.08814" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.08815" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.08815">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.08815" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.08816" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 01:03:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
