<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Multimodal Pretraining and Generation for Recommendation: A Tutorial</title>
<!--Generated on Sat May 11 06:10:13 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender systems,  multimodal pretraining,  multimodal generation,  multimodal adaptation" lang="en" name="keywords"/>
<base href="/html/2405.06927v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S1" title="In Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S1.SS1" title="In 1. Introduction ‣ Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Topic and Relevance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S1.SS2" title="In 1. Introduction ‣ Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Target Audience</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S2" title="In Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Tentative Schedule</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S2.SS1" title="In 2. Tentative Schedule ‣ Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Multimodal Pretraining for Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S2.SS2" title="In 2. Tentative Schedule ‣ Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multimodal Generation for Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S2.SS3" title="In 2. Tentative Schedule ‣ Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Industrial Applications and Open Challenges in Multimodal Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S3" title="In Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Related Tutorials</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#S4" title="In Multimodal Pretraining and Generation for Recommendation: A Tutorial"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>BIOGRAPHY</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\useunder</span>
<p class="ltx_p" id="p1.2"><span class="ltx_text ltx_ulem_uline" id="p1.2.1"></span><span class="ltx_ERROR undefined" id="p1.2.2">\ul</span>
</p>
</div>
<h1 class="ltx_title ltx_title_document">Multimodal Pretraining and Generation for Recommendation: 
<br class="ltx_break"/>A Tutorial</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jieming Zhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Huawei Noah’s Ark Lab</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Shenzhen</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jiemingzhu@ieee.org">jiemingzhu@ieee.org</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chuhan Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Huawei Noah’s Ark Lab</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wuchuhan1@huawei.com">wuchuhan1@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rui Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">www.ruizhang.info</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Shenzhen</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rayteam@yeah.net">rayteam@yeah.net</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhenhua Dong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Huawei Noah’s Ark Lab</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Shenzhen</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dongzhenhua@huawei.com">dongzhenhua@huawei.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">Personalized recommendation stands as a ubiquitous channel for users to explore information or items aligned with their interests. Nevertheless, prevailing recommendation models predominantly rely on unique IDs and categorical features for user-item matching. While this ID-centric approach has witnessed considerable success, it falls short in comprehensively grasping the essence of raw item contents across diverse modalities, such as text, image, audio, and video. This underutilization of multimodal data poses a limitation to recommender systems, particularly in the realm of multimedia services like news, music, and short-video platforms. The recent surge in pretraining and generation techniques presents both opportunities and challenges in the development of multimodal recommender systems. This tutorial seeks to provide a thorough exploration of the latest advancements and future trajectories in multimodal pretraining and generation techniques within the realm of recommender systems. The tutorial comprises three parts: multimodal pretraining, multimodal generation, and industrial applications and open challenges in the field of recommendation. Our target audience encompasses scholars, practitioners, and other parties interested in this domain. By providing a succinct overview of the field, we aspire to facilitate a swift understanding of multimodal recommendation and foster meaningful discussions on the future development of this evolving landscape.</p>
</div>
<div class="ltx_keywords">Recommender systems, multimodal pretraining, multimodal generation, multimodal adaptation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Companion Proceedings of the ACM Web Conference 2024; May 13–17, 2024; Singapore, Singapore</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Companion Proceedings of the ACM Web Conference 2024 (WWW ’24 Companion), May 13–17, 2024, Singapore, Singapore</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3589335.3641248</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0172-6/24/05</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Topic and Relevance</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Nowadays, the emergence of Large Language Models (LLMs) and Multimodal LLMs (MLLMs), such as ChatGPT (GPT-3.5 and GPT-4) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib35" title="">2023</a>)</cite>, Llama2 <cite class="ltx_cite ltx_citemacro_citep">(et al., <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib8" title="">2023</a>)</cite>, BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib24" title="">2023a</a>)</cite>, and MiniGPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib61" title="">2023</a>)</cite>, is reshaping the landscape of technological capabilities. The immense potential of these pretrained large models, particularly MLLMs, introduces both novel opportunities and challenges for the research community, prompting exploration into innovative applications for recommendation tasks. This tutorial aims to comprehensively review and present existing research and practical insights related to multimodal pretraining and generation for recommendation. The tutorial aligns closely with the core themes of the WWW conference and promises valuable takeaways for attendees from both multimodal learning and recommender systems communities.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>Target Audience</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">The tutorial is structured in a <span class="ltx_text ltx_font_bold" id="S1.SS2.p1.1.1">lecture-style</span> format. We welcome participation from academic researchers, industrial practitioners, and other stakeholders with a keen interest in the field. Participants are anticipated to possess a foundational knowledge of the relevant fields. The tutorial uniquely explores the synergy between multimodal learning and recommender system domains. For researchers specializing in multimodal learning, the tutorial offers insights into the applications and challenges associated with integrating multimodal models into recommendation systems. On the other hand, researchers within the recommender systems domain can gain valuable knowledge about recent and prospective research directions in multimodal recommender systems, specifically focusing on how to enhance recommendations through multimodal pretraining and generation techniques. Moreover, we share impactful success stories derived from deploying multimodal models in production systems. These real-world cases can provide practitioners with valuable insights into practical multimodal model deployment.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Tentative Schedule</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The tutorial consists of three talks: The first two talks cover the research topics of multimodal pretraining and multimodal generation in the context of recommender systems. The last one will share some successful applications in practice and present the open challenges from an industrial perspective. The tutorial materials will be made available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mmrec.github.io/tutorial/www2024" style="color:#FF00FF;" title="">https://mmrec.github.io/tutorial/www2024</a>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Opening Remarks (30min), by Dr. Zhenhua Dong.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Multimodal Pretraining for Recommendation (45min), by Dr. Jieming Zhu.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Coffee Break (15min)</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Multimodal Generation for Recommendation (45min), by Prof. Rui Zhang.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1">Industrial Applications and Open Challenges in Multimodal Recommendation (45min), by Dr. Chuhan Wu.</p>
</div>
</li>
</ul>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Multimodal Pretraining for Recommendation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Pretrained models have recently emerged as a groundbreaking approach to achieve the state-of-the-art results in many machine learning tasks. In this talk, we will introduce multimodal pretraining techniques and their applications in recommender systems.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Self-supervised pretraining</span>: We will briefly review the common self-supervised pretraining paradigms, including reconstructive, contrastive, and generative learning tasks <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib32" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">Multimodal pretraining</span>: Multimodal pretraining models have emerged as a rapidly growing trend across various fields, including computing vision, natural language processing, and speech recognition, among others, capturing significant interests within these fields. We will introduce some representative multimodal pretrained models, including both constrative and generative ones, e.g., CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib36" title="">2021</a>)</cite>, Flamingo <cite class="ltx_cite ltx_citemacro_citep">(Alayrac et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib2" title="">2022</a>)</cite>,
GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib35" title="">2023</a>)</cite>, BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib24" title="">2023a</a>)</cite>, ImageBind <cite class="ltx_cite ltx_citemacro_citep">(Girdhar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib13" title="">2023</a>)</cite>,
etc.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i3.p1.1.1">Pretraining for recommendation</span>: This part focuses on recent research that applies pretraining techniques to recommendation. We will summarize the pretrained models for recommendation from four categories: <span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.2">1) Sequence pretraining</span>, which aims to capture users’ sequential behavior patterns from item representations, including Bert4Rec <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib42" title="">2019</a>)</cite>, PeterRec <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib57" title="">2020</a>)</cite>,
UserBert <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib47" title="">2022</a>)</cite>,
S<sup class="ltx_sup" id="S2.I2.i3.p1.1.3">3</sup>-Rec <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib60" title="">2020</a>)</cite>,
SL4Rec <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib54" title="">2021</a>)</cite>.
<span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.4">2) Text-based pretraining</span>, which models semantic-based item representations from text data. Examples include UNBERT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib59" title="">2021</a>)</cite>, PREC <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib31" title="">2022</a>)</cite>, MINER <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib26" title="">2022</a>)</cite>,
UniSRec <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib17" title="">2022</a>)</cite>, Recformer <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib25" title="">2023b</a>)</cite>, and P5 <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib11" title="">2022</a>)</cite>. They are not only valuable for text-rich news recommendation but also can enable knowledge transfer across items and domains. <span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.5">3) Audio-based pretraining</span>, which has been studied in the context of music recommendation and retrieval. They are used to extract latent music representations to enhance recommendation and retrieval tasks, including MusicBert <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib62" title="">2021</a>)</cite>, MART <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib53" title="">2024</a>)</cite>,
PEMR <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib52" title="">2022</a>)</cite>, and UAE <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib7" title="">2021</a>)</cite>. <span class="ltx_text ltx_font_italic" id="S2.I2.i3.p1.1.6">4) Multimodal pretraining</span> that aims to achieve multimodal content understanding and cross-modal alignment. Recent trend emerges to build multimodal foundation models for recommendation, e.g., MMSSL <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib45" title="">2023</a>)</cite>, PMGT <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib33" title="">2021</a>)</cite>, MSM4SR <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib58" title="">2023</a>)</cite>,
MISSRec <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib44" title="">2023</a>)</cite>, VIP5 <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib12" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i4.p1">
<p class="ltx_p" id="S2.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i4.p1.1.1">Model adaptation for recommendation</span>: Given large pretrained models, it is often necessary to adapt the models to a recommendation task with domain-specific data. We will review the common paradigms for model adaptation, including representation-based transfer, fine-tuning, adapter tuning <cite class="ltx_cite ltx_citemacro_citep">(Lialin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib27" title="">2023</a>)</cite>, prompt tuning <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib14" title="">2023</a>)</cite>, and retrieval-augmented adaptation <cite class="ltx_cite ltx_citemacro_citep">(Long et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib34" title="">2023</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Multimodal Generation for Recommendation</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">With the recent advancements in generative models, AI-generated content (AIGC) has gained significant popularity in various applications. In this talk, we will discuss the research directions for applying AIGC techniques in recommendation scenarios.</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i1.p1.1.1">Text generation</span>: With the support of powerful large language models (LLMs), text generation has been applied to many tasks such as news headline generation <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib15" title="">2020</a>; Salemi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib38" title="">2023</a>)</cite> and dialogue generation <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib51" title="">2022</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib19" title="">2020</a>)</cite>.
We will discuss the commonly used sequence-to-sequence generation framework and LLM-based generation methods. More recently, news headline generation has been performed in a personalized manner, such as LaMP <cite class="ltx_cite ltx_citemacro_citep">(Salemi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib38" title="">2023</a>)</cite>, GUE <cite class="ltx_cite ltx_citemacro_citep">(Cai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib6" title="">2023</a>)</cite>,
PENS <cite class="ltx_cite ltx_citemacro_citep">(Ao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib4" title="">2021</a>)</cite>, NHNet <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib15" title="">2020</a>)</cite>, and PNG <cite class="ltx_cite ltx_citemacro_citep">(Ao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib3" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i2.p1.1.1">Image generation</span>: Image generation has achieved remarkable success with prevalence of GAN and diffusion models. We will introduce their applications in poster generation for advertisements and cover image generation of news and e-books. Examples include AutoPoster <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib29" title="">2023</a>)</cite>, TextPainter <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib9" title="">2023</a>)</cite>, and PosterLayout <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib18" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i3.p1.1.1">Personalized generation</span>: While pretrained generation models enable general-domain text and image generation, there is a trend towards personalized generation. This is important for recommendation scenarios where personalized content or identity information needs to be provided. Pioneer work includes personalized image generation (e.g., DreamBooth <cite class="ltx_cite ltx_citemacro_citep">(Ruiz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib37" title="">2023</a>)</cite>, text inversion <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib50" title="">2023</a>)</cite>), personalized text generation (e.g., LaMP <cite class="ltx_cite ltx_citemacro_citep">(Salemi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib38" title="">2023</a>)</cite>, APR <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib22" title="">2023c</a>)</cite>, PTG <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib23" title="">2023d</a>)</cite>), and personalized multimodal generation (e.g., PMG <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib41" title="">2024</a>)</cite>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Industrial Applications and Open Challenges in Multimodal Recommendation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<ul class="ltx_itemize" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I4.i1.p1.1.1">Successful applications</span>: In this talk, we will demonstrate a list of successful applications in industry. We organize the open use cases from Alibaba <cite class="ltx_cite ltx_citemacro_citep">(Ge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib10" title="">2018</a>)</cite>, JD.com <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib30" title="">2020</a>; Xiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib48" title="">2022</a>)</cite>, Tencent <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib7" title="">2021</a>)</cite>, Baidu <cite class="ltx_cite ltx_citemacro_citep">(Wen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib46" title="">2023</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib55" title="">2022</a>)</cite>, Xiaohongshu <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib20" title="">2021</a>)</cite>, Pinterest <cite class="ltx_cite ltx_citemacro_citep">(Baltescu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib5" title="">2022</a>)</cite>, etc. We will also share our industrial experiences that deploying multimodal recommendation models at Huawei <cite class="ltx_cite ltx_citemacro_citep">(Xun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib49" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I4.i2.p1.1.1">Open challenges</span>: We will discuss the open challenges in multimodal recommendation from both research and practice perpectives, such as multimodal representation fusion, multi-domain multimodal pretraining, efficient adaptation of MLLMs, personalized adaptation of MLLMs, multimodal AIGC for recommendation, efficiency and responsibility of multimodal recommendation, open benchmarking <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib63" title="">2022</a>)</cite>, etc.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Related Tutorials</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">There are several related tutorials given at previous conferences:</p>
</div>
<div class="ltx_para" id="S3.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Paul Pu Liang, Louis-Philippe Morency. Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions. ICMI 2023 <cite class="ltx_cite ltx_citemacro_citep">(Liang and Morency, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib28" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Trung-Hoang Le, Quoc-Tuan Truong, Aghiles Salah, Hady W. Lauw. Multi-Modal Recommender Systems: Towards Addressing Sparsity, Comparability, and Explainability. WWW 2023 <cite class="ltx_cite ltx_citemacro_citep">(Le et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib21" title="">2023</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Quoc-Tuan Truong, Aghiles Salah, Hady Lauw. Multi-Modal Recommender Systems: Hands-On Exploration. RecSys 2021 <cite class="ltx_cite ltx_citemacro_citep">(Truong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib43" title="">2021</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Xiangnan He, Hanwang Zhang, Tat-Seng Chua. Recommendation Technologies for Multimedia Content. ICMR 2018 <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib16" title="">2018</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Yi Yu, Kiyoharu Aizawa, Toshihiko Yamasaki, Roger Zimmermann. Emerging Topics on Personalized and Localized Multimedia Information Systems. MM 2014 <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib56" title="">2014</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">Jialie Shen, Xian-Sheng Hua, Emre Sargin. Towards Next Generation Multimedia Recommendation Systems. MM 2013 <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib40" title="">2013</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i7.p1">
<p class="ltx_p" id="S3.I1.i7.p1.1">Jialie Shen, Meng Wang, Shuicheng Yan, Peng Cui. Multimedia Recommendation: Technology and Techniques. SIGIR 2013 <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib40" title="">2013</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i8.p1">
<p class="ltx_p" id="S3.I1.i8.p1.1">Jialie Shen, Meng Wang, Shuicheng Yan, Peng Cui. Multimedia Recommendation. MM 2012 <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib39" title="">2012</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Different from these previous tutorials, our tutorial makes the following novel contributions: 1) Our tutorial builds on recent advances in multimodal pretraining and generation techniques, which differs significantly from the previous tutorials on multimedia recommendaton <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib39" title="">2012</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib40" title="">2013</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib56" title="">2014</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib16" title="">2018</a>)</cite>. 2) As for the recent three tutorials, they either present a technical review on general multimodal learning tasks <cite class="ltx_cite ltx_citemacro_citep">(Liang and Morency, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib28" title="">2023</a>)</cite> or provide introductory to intermediate hands-on projects on multimodal recommendation <cite class="ltx_cite ltx_citemacro_citep">(Truong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib43" title="">2021</a>; Le et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.06927v1#bib.bib21" title="">2023</a>)</cite>. In contrast, we take a look into new research and practice progresses on applying pretrained multimodal models to recommendation tasks.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>BIOGRAPHY</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Dr. Jieming Zhu</span> is a researcher at Huawei Noah’s Ark Lab. He received the Ph.D. degree from The Chinese University of Hong Kong in 2016. His recent research focuses on developing practical AI models for industrial-scale recommender systems. He currently leads a research project on multimodal pretraining and generation for recommender systems.
Please find more information at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jiemingzhu.github.io" title="">https://jiemingzhu.github.io</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Dr. Chuhan Wu</span> is a researcher at Huawei Noah’s Ark Lab. Before that, he got his Ph.D. degree from Tsinghua University in 2023. He focuses on recommender systems and responsible AI. Please find more information at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://wuch15.github.io" title="">https://wuch15.github.io</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Prof. Rui Zhang</span> is a visiting Professor at Tsinghua University and was previously a Professor at the University of Melbourne. His research interests include machine learning and big data. Please find more information at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ruizhang.info" title="">https://www.ruizhang.info</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Dr. Zhenhua Dong</span> is a technology expert and project manager at Huawei Noah’s Ark Lab. He received the B.Eng. degree from Tianjin University in 2006 and the Ph.D. degree from Nankai University in 2012. He leads a research team dedicated to advancing the field of recommender systems and causal inference.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, and et al. 2022.

</span>
<span class="ltx_bibblock">Flamingo: a Visual Language Model for Few-Shot Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">NeurIPS</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiang Ao, Ling Luo, Xiting Wang, Zhao Yang, Jiun-Hung Chen, Ying Qiao, Qing He, and Xing Xie. 2023.

</span>
<span class="ltx_bibblock">Put Your Voice on Stage: Personalized Headline Generation for News Articles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">TKDD</em> 18, 3 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ao et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, and Xing Xie. 2021.

</span>
<span class="ltx_bibblock">PENS: A Dataset and Generic Framework for Personalized News Headline Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of ACL/IJCNLP</em>. 82–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baltescu et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Paul Baltescu, Haoyu Chen, Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022.

</span>
<span class="ltx_bibblock">ItemSage: Learning Product Embeddings for Shopping Recommendations at Pinterest. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</em>. 2703–2711.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Pengshan Cai, Kaiqiang Song, Sangwoo Cho, Hongwei Wang, Xiaoyang Wang, Hong Yu, Fei Liu, and Dong Yu. 2023.

</span>
<span class="ltx_bibblock">Generating User-Engaging News Headlines. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of ACL</em>. 3265–3280.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ke Chen, Beici Liang, Xiaoshuan Ma, and Minwei Gu. 2021.

</span>
<span class="ltx_bibblock">Learning Audio Embeddings with User Listening Data for Content-Based Music Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">ICASSP</em>. 3015–3019.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2023)</span>
<span class="ltx_bibblock">
Touvron et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">CoRR</em> abs/2307.09288 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yifan Gao, Jinpeng Lin, Min Zhou, Chuanbin Liu, Hongtao Xie, Tiezheng Ge, and Yuning Jiang. 2023.

</span>
<span class="ltx_bibblock">TextPainter: Multimodal Text Image Generation with Visual-harmony and Text-comprehension for Poster Design. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 31st ACM International Conference on Multimedia (MM)</em>. 7236–7246.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huiming Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, Pengtao Yi, Sui Huang, Zhiqiang Zhang, Xiaoqiang Zhu, Yu Zhang, and Kun Gai. 2018.

</span>
<span class="ltx_bibblock">Image Matters: Visually Modeling User Behaviors Using Advanced Model Server. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">CIKM</em>. 2087–2095.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5). In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">RecSys</em>. 299–315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">VIP5: Towards Multimodal Foundation Models for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">CoRR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girdhar et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. 2023.

</span>
<span class="ltx_bibblock">ImageBind: One Embedding Space to Bind Them All. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">CVPR</em>. 15180–15190.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip H. S. Torr. 2023.

</span>
<span class="ltx_bibblock">A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">CoRR</em> abs/2307.12980 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, You Wu, Cong Yu, Daniel Finnie, Hongkun Yu, Jiaqi Zhai, and Nicholas Zukoski. 2020.

</span>
<span class="ltx_bibblock">Generating Representative Headlines for News Stories. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">The Web Conference 2020 (WWW)</em>. 1773–1784.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xiangnan He, Hanwang Zhang, and Tat-Seng Chua. 2018.

</span>
<span class="ltx_bibblock">Recommendation Technologies for Multimedia Content. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of ICMR</em>. 8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yupeng Hou, Shanlei Mu, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. 2022.

</span>
<span class="ltx_bibblock">Towards Universal Sequence Representation Learning for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">KDD</em>. 585–593.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
HsiaoYuan Hsu, Xiangteng He, Yuxin Peng, Hao Kong, and Qing Zhang. 2023.

</span>
<span class="ltx_bibblock">PosterLayout: A New Benchmark and Approach for Content-Aware Visual-Textual Presentation Layout. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">CVPR</em>. 6018–6026.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xinting Huang, Jianzhong Qi, Yu Sun, and Rui Zhang. 2020.

</span>
<span class="ltx_bibblock">MALA: Cross-Domain Dialogue Generation with Action Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">AAAI</em>. 7977–7984.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yanhua Huang, Weikun Wang, Lei Zhang, and Ruiwen Xu. 2021.

</span>
<span class="ltx_bibblock">Sliding Spectrum Decomposition for Diversified Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">KDD</em>. 3041–3049.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Trung-Hoang Le, Quoc-Tuan Truong, Aghiles Salah, and Hady W. Lauw. 2023.

</span>
<span class="ltx_bibblock">Multi-Modal Recommender Systems: Towards Addressing Sparsity, Comparability, and Explainability. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">ACM Web Conference (WWW)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. 2023c.

</span>
<span class="ltx_bibblock">Automatic Prompt Rewriting for Personalized Text Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">CoRR</em> abs/2310.00152 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky. 2023d.

</span>
<span class="ltx_bibblock">Teach LLMs to Personalize - An Approach inspired by Writing Education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">CoRR</em> abs/2308.07968 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023a.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">ICML</em>, Vol. 202. 19730–19742.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian J. McAuley. 2023b.

</span>
<span class="ltx_bibblock">Text Is All You Need: Learning Language Representations for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of KDD</em>. 1258–1267.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng Shang, Zhenhua Dong, Xin Jiang, and Qun Liu. 2022.

</span>
<span class="ltx_bibblock">MINER: Multi-Interest Matching Network for News Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Findings of ACL</em>. 343–352.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lialin et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. 2023.

</span>
<span class="ltx_bibblock">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">CoRR</em> abs/2303.15647.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang and Morency (2023)</span>
<span class="ltx_bibblock">
Paul Pu Liang and Louis-Philippe Morency. 2023.

</span>
<span class="ltx_bibblock">Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">International Conference on Multimodal Interaction (ICMI)</em>. 101–104.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinpeng Lin, Min Zhou, Ye Ma, Yifan Gao, Chenxi Fei, Yangjian Chen, Zhang Yu, and Tiezheng Ge. 2023.

</span>
<span class="ltx_bibblock">AutoPoster: A Highly Automatic and Content-aware Design System for Advertising Poster Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">ACM MM</em>. 1250–1260.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hu Liu, Jing Lu, Hao Yang, Xiwei Zhao, Sulong Xu, Hao Peng, Zehua Zhang, Wenjie Niu, Xiaokun Zhu, Yongjun Bao, and Weipeng Yan. 2020.

</span>
<span class="ltx_bibblock">Category-Specific CNN for Visual-aware CTR Prediction at JD.com. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</em>. 2686–2696.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qijiong Liu, Jieming Zhu, Quanyu Dai, and Xiaoming Wu. 2022.

</span>
<span class="ltx_bibblock">Boosting Deep CTR Prediction with a Plug-and-Play Pre-trainer for News Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of COLING</em>. 2823–2833.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. 2023.

</span>
<span class="ltx_bibblock">Self-Supervised Learning: Generative or Contrastive.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">IEEE Trans. Knowl. Data Eng.</em> 35, 1 (2023), 857–876.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yong Liu, Susen Yang, Chenyi Lei, Guoxin Wang, Haihong Tang, Juyong Zhang, Aixin Sun, and Chunyan Miao. 2021.

</span>
<span class="ltx_bibblock">Pre-training Graph Transformer with Multimodal Side Information for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">ACM MM</em>. 2853–2861.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Quanyu Long, Wenya Wang, and Sinno Jialin Pan. 2023.

</span>
<span class="ltx_bibblock">Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of EMNLP</em>. 6525–6542.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em> abs/2303.08774 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of ICML</em>, Vol. 139. 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023.

</span>
<span class="ltx_bibblock">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">CVPR</em>. 22500–22510.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salemi et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">LaMP: When Large Language Models Meet Personalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">CoRR</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Jialie Shen, Meng Wang, Shuicheng Yan, and Peng Cui. 2012.

</span>
<span class="ltx_bibblock">Multimedia Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">ACM Multimedia Conference (MM)</em>. 1535.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Jialie Shen, Meng Wang, Shuicheng Yan, and Peng Cui. 2013.

</span>
<span class="ltx_bibblock">Multimedia recommendation: technology and techniques. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">SIGIR</em>. 1131.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, and Xi Xiao. 2024.

</span>
<span class="ltx_bibblock">PMG: Personalized Multimodal Generation with Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">WWW</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of CIKM</em>. 1441–1450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truong et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Quoc-Tuan Truong, Aghiles Salah, and Hady W. Lauw. 2021.

</span>
<span class="ltx_bibblock">Multi-Modal Recommender Systems: Hands-On Exploration. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">RecSys</em>. 834–837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinpeng Wang, Ziyun Zeng, Yunxiao Wang, Yuting Wang, Xingyu Lu, Tianxiang Li, Jun Yuan, Rui Zhang, Hai-Tao Zheng, and Shu-Tao Xia. 2023.

</span>
<span class="ltx_bibblock">MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of MM</em>. 6548–6557.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wei Wei, Chao Huang, Lianghao Xia, and Chuxu Zhang. 2023.

</span>
<span class="ltx_bibblock">Multi-Modal Self-Supervised Learning for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">WWW</em>. 790–800.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhoufutu Wen, Xinyu Zhao, Zhipeng Jin, Yi Yang, Wei Jia, Xiaodong Chen, Shuanglong Li, and Lin Liu. 2023.

</span>
<span class="ltx_bibblock">Enhancing Dynamic Image Advertising with Vision-Language Pre-training. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">SIGIR</em>. 3310–3314.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2022.

</span>
<span class="ltx_bibblock">UserBERT: Pre-training User Model with Contrastive Self-supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">SIGIR</em>. 2087–2092.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Fangxiong Xiao, Lixi Deng, Jingjing Chen, Houye Ji, Xiaorui Yang, Zhuoye Ding, and Bo Long. 2022.

</span>
<span class="ltx_bibblock">From Abstract to Details: A Generative Multimodal Fusion Framework for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">ACM MM</em>. 258–267.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xun et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jiahao Xun, Shengyu Zhang, Zhou Zhao, Jieming Zhu, Qi Zhang, Jingjie Li, Xiuqiang He, Xiaofei He, Tat-Seng Chua, and Fei Wu. 2021.

</span>
<span class="ltx_bibblock">Why Do We Click: Visual Impression-aware News Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">ACM MM</em>. 3881–3890.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianan Yang, Haobo Wang, Ruixuan Xiao, Sai Wu, Gang Chen, and Junbo Zhao. 2023.

</span>
<span class="ltx_bibblock">Controllable Textual Inversion for Personalized Text-to-Image Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">CoRR</em> abs/2304.05265 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shiquan Yang, Rui Zhang, Sarah M. Erfani, and Jey Han Lau. 2022.

</span>
<span class="ltx_bibblock">An Interpretable Neuro-Symbolic Reasoning Framework for Task-Oriented Dialogue Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">ACL</em>. 4918–4935.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Dong Yao, Zhou Zhao, Shengyu Zhang, Jieming Zhu, Yudong Zhu, Rui Zhang, and Xiuqiang He. 2022.

</span>
<span class="ltx_bibblock">Contrastive Learning with Positive-Negative Frame Mask for Music Representation. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">The ACM Web Conference 2022 (WWW)</em>. 2906–2915.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dong Yao, Jieming Zhu, Jiahao Xun, Shengyu Zhang, Zhou Zhao, Liqun Deng, Wenqiao Zhang, Zhenhua Dong, and Xin Jiang. 2024.

</span>
<span class="ltx_bibblock">MART: Learning Hierarchical Music Audio Representations with Part-Whole Transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">WWW</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix X. Yu, Ting Chen, Aditya Krishna Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi (Jay) Kang, and Evan Ettinger. 2021.

</span>
<span class="ltx_bibblock">Self-supervised Learning for Large-scale Item Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">CIKM</em>. 4321–4330.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tan Yu, Zhipeng Jin, Jie Liu, Yi Yang, Hongliang Fei, and Ping Li. 2022.

</span>
<span class="ltx_bibblock">Boost CTR Prediction for New Advertisements via Modeling Visual Content. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">IEEE International Conference on Big Data (BigData)</em>. 2140–2149.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Yi Yu, Kiyoharu Aizawa, Toshihiko Yamasaki, and Roger Zimmermann. 2014.

</span>
<span class="ltx_bibblock">Emerging Topics on Personalized and Localized Multimedia Information Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">ACM MM</em>. 1233–1234.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020.

</span>
<span class="ltx_bibblock">Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of SIGIR</em>. 1469–1478.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lingzi Zhang, Xin Zhou, and Zhiqi Shen. 2023.

</span>
<span class="ltx_bibblock">Multimodal Pre-training Framework for Sequential Recommendation via Contrastive Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">CoRR</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Qi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jieming Zhu, Zhaowei Wang, and Xiuqiang He. 2021.

</span>
<span class="ltx_bibblock">UNBERT: User-News Matching BERT for News Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Proceedings of IJCAI</em>. 3356–3362.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020.

</span>
<span class="ltx_bibblock">S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">CIKM</em>. 1893–1902.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023.

</span>
<span class="ltx_bibblock">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">CoRR</em> abs/2304.10592 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hongyuan Zhu, Ye Niu, Di Fu, and Hao Wang. 2021.

</span>
<span class="ltx_bibblock">MusicBERT: A Self-supervised Learning of Music Representation. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">ACM MM</em>. 3955–3963.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jieming Zhu, Quanyu Dai, Liangcai Su, Rong Ma, Jinyang Liu, Guohao Cai, Xi Xiao, and Rui Zhang. 2022.

</span>
<span class="ltx_bibblock">BARS: Towards Open Benchmarking for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">SIGIR</em>. 2912–2923.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat May 11 06:10:13 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
