<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.05795] Poisoning Attacks and Defenses in Federated Learning: A Survey</title><meta property="og:description" content="Federated learning (FL) enables the training of models among distributed clients without compromising the privacy of training datasets, while the invisibility of clients’ datasets and the training process poses a varie…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Poisoning Attacks and Defenses in Federated Learning: A Survey">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Poisoning Attacks and Defenses in Federated Learning: A Survey">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.05795">

<!--Generated on Fri Mar  1 07:40:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  poisoning attacks,  distributed machine learning
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Poisoning Attacks and Defenses in Federated Learning: A Survey</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Subhash Sagar, Chang-Tsun Li, Seng W. Loke, and Jinho Choi
</span><span class="ltx_author_notes">All the authors are with the School of Information Technology, Deakin University, Geelong,
Australia
<br class="ltx_break">E-mail: {subhash.sagar,changtsun.li,seng.loke,jinho.choi}@deakin.edu.au</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning (FL) enables the training of models among distributed clients without compromising the privacy of training datasets, while the invisibility of clients’ datasets and the training process poses a variety of security threats. This survey provides the taxonomy of poisoning attacks and experimental evaluation to discuss the need for robust FL.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, poisoning attacks, distributed machine learning

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent advances in distributed machine learning, in particular, federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> have paved the way for the next generation of data-driven intelligent applications. FL has emerged as a promising solution to a number of applications to solve data silos while protecting the privacy of data. Since its emergence, FL has been employed in a variety of applications including but not limited to healthcare, crowdsourcing systems, natural language processing (NLP), and the Internet of Things (IoT). The primary notion of FL is to generate a collaborative global training model without sharing the actual data distributed over several participating clients. In FL, the participating clients first train local models as well as the shared global model with local data and then send the local updates to the central server in order to update the global model. In this way, the privacy of the client’s data is protected from unauthorized access.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.2" class="ltx_p">Similar to machine learning, FL is susceptible to several adversarial attacks since both the data and the local training process are controlled by clients.
A few common attacks are inference attacks (targeting the privacy of clients) and poisoning attacks (targeting the training data or model) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In poisoning attacks, attackers aim to poison the local training data by injecting the poisoned instances in the training data (i.e., <em id="S1.p2.2.1" class="ltx_emph ltx_font_italic">data poisoning attacks (DPA)</em>), or directly manipulating the weights of the model updates (i.e., <em id="S1.p2.2.2" class="ltx_emph ltx_font_italic">model poisoning attacks (MPA)</em>). In essence, the main objective of poisoning attacks is to instigate the global model to predict the attacker-specific outputs on the poisoned inputs, and it is presumed that the main factor of poisoning attacks is the count of malicious clients and the poisoned data. Furthermore, Steinhardt <span id="S1.p2.2.3" class="ltx_text ltx_font_italic">et al. <cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.2.3.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p2.2.3.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> suggested that a <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="3\%" display="inline"><semantics id="S1.p2.1.m1.1a"><mrow id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mn id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">3</mn><mo id="S1.p2.1.m1.1.1.1" xref="S1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="latexml" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">3\%</annotation></semantics></math> ratio of malicious clients can reduce the accuracy of the main task by <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S1.p2.2.m2.1a"><mrow id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml"><mn id="S1.p2.2.m2.1.1.2" xref="S1.p2.2.m2.1.1.2.cmml">11</mn><mo id="S1.p2.2.m2.1.1.1" xref="S1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><apply id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1"><csymbol cd="latexml" id="S1.p2.2.m2.1.1.1.cmml" xref="S1.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S1.p2.2.m2.1.1.2.cmml" xref="S1.p2.2.m2.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">11\%</annotation></semantics></math>. Therefore, it is imperative to study the current state-of-the-art poisoning attacks and defense strategies to mitigate these attacks.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As of late, a plethora of research has been published in order to comprehensively discuss and understand the notion of FL and its use cases in the real world. Most recently, researchers have been exploring the security and privacy threats that limit the exploitation of FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. A comprehensive discussion of FL threats is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> in terms of a taxonomy of attacks and defense methods. The authors also conducted an experimental evaluation in order to draw a conclusion on how to select the suitable method in each category of adversarial attacks. Furthermore, a brief overview of threats to FL is discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and focuses on poisoning and inference attacks in order to comprehend the model behavior in presence of these attacks.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The generalization of state-of-the-art into various categories is the core of the surveys on security and privacy in FL that have emerged in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The discussed studies’ pros and cons with regard to evaluation criteria, including but not limited to the adaptability of attacks these studies can handle, their effectiveness in the presence of backdoor attacks, their use in real-world applications, and their impact on benign clients, are not taken into account by the current literature. In order to get over the aforementioned limitations, this survey presents the most recent state-of-the-art attacks and defense strategies suggested for FL. Additionally, this study lists the advantages and disadvantages of the attacks and defense strategies in relation to a variety of assessment criteria as well as via experimental evaluation. Finally, we have concluded the study by highlighting potential future research directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Preliminaries</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The notion of FL was first introduced by Google and is characterized as a distributed machine learning (ML) paradigm to collaboratively train a global ML model on datasets that are distributed across a number of clients (e.g., mobile devices) while protecting the privacy of clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In essence, there are two different types of parties in FL, i.e., a number of clients and a central server/cloud, wherein each client maintains a local model trained on the local dataset. In contrast, the central server maintains a global model, which is the aggregation of locally trained models.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In general, FL employs the stochastic gradient descent (SGD) to minimize a loss function and the models are updated iteratively (due to the use of SGD).
In each iteration, there are three stages as follows: (1) selection of clients; (2) local model updates; and (3) aggregation by the central server. In particular, firstly, the central server selects a number of clients and shares the current global model. Secondly, each client retrains the current global model on the local dataset and sends the latest local model back to the central server.
Finally, the central server aggregates all the local models in order to obtain the latest global model and such aggregation schemes include mean aggregation, byzantine-robust aggregation, etc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Poisoning Attacks</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The term ‘poisoning attack’ in FL refers to an attack that involves attackers intentionally tampering with the training data or model parameters to manipulate the model aggregation in order to disrupt the integrity and availability of the global learning model.
In FL, because the participants’ data and the training processes are invisible to the central server, poisoning attacks might be easy to carry out by some clients.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Poisoning attacks can be divided into two types, i.e., DPA and MPA, or can be divided into targeted and untargeted attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. We have considered the division in terms of data and poisoning attacks and a taxonomy of these attacks is illustrated in Figure <a href="#S2.F1.sf1" title="In Figure 1 ‣ 2.2.1 Data Poisoning Attacks ‣ 2.2 Poisoning Attacks ‣ 2 Preliminaries ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>. The location of poisoning attacks may vary and it can be on the client’s side, communication channel, or sometimes on the central server. Nevertheless, the common poisoning attacks usually take place on the client side (i.e., modifying the data, models, or both) as illustrated in Figure <a href="#S2.F1.sf2" title="In Figure 1 ‣ 2.2.1 Data Poisoning Attacks ‣ 2.2 Poisoning Attacks ‣ 2 Preliminaries ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a>.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Data Poisoning Attacks</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In DPA, it is presumed that the attackers have access to the training data of at least one client and are able to alter it. DPA can be further classified in terms of the characteristic of the poisoning, and we have categorized them into the following types of attacks.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p2.2" class="ltx_p"><em id="S2.SS2.SSS1.p2.2.1" class="ltx_emph ltx_font_italic">Label-Flipping Attacks:</em> In this type of attack, adversaries with access to the training data alter the labels of a portion of the data (e.g., flipping label <math id="S2.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S2.SS2.SSS1.p2.1.m1.1a"><mn id="S2.SS2.SSS1.p2.1.m1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.1.m1.1b"><cn type="integer" id="S2.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.1.m1.1c">4</annotation></semantics></math> to <math id="S2.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S2.SS2.SSS1.p2.2.m2.1a"><mn id="S2.SS2.SSS1.p2.2.m2.1.1" xref="S2.SS2.SSS1.p2.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.2.m2.1b"><cn type="integer" id="S2.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.2.m2.1c">1</annotation></semantics></math>, or vice versa) while preserving the remaining content in order to manipulate the FL models. Label flipping can be either targeted (i.e., flipping the targeted label) or untargeted (i.e., random flipping) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p"><em id="S2.SS2.SSS1.p3.1.1" class="ltx_emph ltx_font_italic">Poisoning Sample Attacks:</em> In this type of DPA, the attackers modify a portion of training data, i.e., by inserting modified patterns in data samples or adding unification noise. Recent studies suggest the use of generative adversarial nets to generate positioned patterns in order to maximize targeted attacks and evade defense approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p4.1" class="ltx_p"><em id="S2.SS2.SSS1.p4.1.1" class="ltx_emph ltx_font_italic">Backdoor Attacks:</em> The idea of backdoor attacks is to degrade the performance of a subtask while preserving the performance of the main task. These attacks inject triggers into the training data of one or more clients’ to poison the global model. Since these attacks do not affect the overall performance, it is difficult to mitigate. In essence, backdoor attacks do not pose threats to the FL’s main task. However, the integrity and the infrastructure of FL are vulnerable and can be considered a security threat as the prediction of test samples can be controlled by attackers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS1.p5.1" class="ltx_p"><em id="S2.SS2.SSS1.p5.1.1" class="ltx_emph ltx_font_italic">Untargeted Attacks:</em> The untargeted attacks aim at disrupting the performance of the FL model. One of the well-known scenarios is the <em id="S2.SS2.SSS1.p5.1.2" class="ltx_emph ltx_font_italic">byzantine attack</em>, wherein an adversary shares a randomly generated model trained over modified training data. The untargeted attacks can be further divided into the following two types, namely <em id="S2.SS2.SSS1.p5.1.3" class="ltx_emph ltx_font_italic">disruption untargeted</em> and <em id="S2.SS2.SSS1.p5.1.4" class="ltx_emph ltx_font_italic">exploitation untargeted</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. With disruption untargeted attacks, the attackers corrupt the FL model in order to disrupt the convergence of the training process. With exploitation untargeted attacks, the attackers utilize the poisoning attacks in order to exploit the FL framework for malicious purposes by mimicking as a benign client .</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/FL-PA-Taxonomy.png" id="S2.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">Taxonomy of Poisoning Attacks in Federated Learning</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/FL-Attacks-Poisoning.png" id="S2.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="286" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">Illustration of Poisoning Attacks in Federated Learning</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Taxonomy and illustration of poisoning attacks</span></figcaption>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Model Poisoning Attacks</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">MPA aims to manipulate the training processes of the local FL models by directly poisoning the model updates sent by the clients to the central server. Furthermore, DPA can be considered to be a subset of MPA as they eventually lead to MPA, and in general, MPA attempts to directly modify local model weights. A number of variants fall under the category of model poisoning and are discussed as follows.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p"><em id="S2.SS2.SSS2.p2.1.1" class="ltx_emph ltx_font_italic">Gradient Manipulation:</em> These types of attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> are based on generating random weights having a similar dimension as that of the original model weights. These random weights are then employed to manipulate the local model gradient and compromise the global model.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><em id="S2.SS2.SSS2.p3.1.1" class="ltx_emph ltx_font_italic">Optimization Methods:</em> Optimization methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> are utilized to maximize the performance of poisoning attacks, specifically, backdoor attacks, wherein the optimization helps in minimizing the difference between the original and the poisoned model from the last round in order to make it difficult for the attacks to be mitigated. Hence, the attackers force the main model to output the attacker-specified labels.</p>
</div>
<div id="S2.SS2.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.SSS2.p4.1" class="ltx_p"><em id="S2.SS2.SSS2.p4.1.1" class="ltx_emph ltx_font_italic">Untargeted Attacks:</em> The untargeted attacks are to lower the main task accuracy of the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Moreover, the attackers in these types of attacks quantify the original local model updates, and then alter their genuine local model updates so that the tainted global model updates significantly diverge from the original ones. In essence, these attacks aim at limiting the availability of the global model.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Poisoning Attacks and Defense Strategies</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section discusses existing state-of-the-art for poisoning attacks and defense strategies in FL.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Attacks Strategies</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In FL, poisoning attacks work by corrupting the data of one or more clients or the locally trained models in an effort to interfere with the global model. For DPA, it is assumed that the attacker has the access to the data of at least one client and the capacity to alter it, wherein the poisoned data is an amalgam of cleaned (original labels) and modified data. Furthermore, the MPA directly poison the local model updates sent by the participating clients to the global server by modifying the weights of the local models.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">With the assumption that the FL participants may change the raw data on their devices, the authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposed a DPA model in terms of label flipping attacks. Under this assumption, a complex neural network model is employed to investigate the FL application in the presence of label-flipping attacks. Furthermore, the efficacy of the attacks is shown with respect to varying percentages of malevolent participants using two well-known image datasets, <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">CIFAR-10</em> and <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Fashion-MNIST</em>. It can be observed that the attack is effective even with a small ratio of malicious participants and can also be targeted, i.e., having more impact on a subset of targeted data labels.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Xie et al. suggested a distributed backdoor attack, called DBA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which employed a composite global trigger to perform the attacks as opposed to the straightforward label-flipping attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> carried out by several attackers. The local models are trained on a poisoned dataset after each adversary picks a unique local trigger to poison the training dataset. The server then receives the updates from the attackers for model aggregation. The attackers employ the local triggers to create a global trigger at the inference step rather than using them directly. It is demonstrated that the global trigger has the highest attack success rate of any local trigger, even if it never appears in the training phase.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Furthermore, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> theoretically demonstrated that backdoor attacks are inevitable in the presence of adversarial samples in FL and intuitively proposed a new family of backdoor attacks, known as edge-case backdoor to manipulate the model on the input resides on the tail of the distribution. Therefore, this makes it difficult for the defense strategies to detect such attacks as the attacker’s access is restricted to developing an untrained feature map instead of a fully trained model.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, the vulnerability of FL towards a MPA is discussed. The MPA can be performed by using model replacement, wherein one or more malicious clients attempt to replace a benign model with a malicious one, leading the model to misclassify future inputs. Moreover, the study discusses the countermeasures and concludes that the attackers can easily bypass such measures. The main idea of FL is to take advantage of the diversity of the clients in terms of non-iid training data, including uncommon or low-quality data. However, using secure aggregation by the central server to filter out anomalous contributions runs counter to this idea.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.2" class="ltx_p">In contrast to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, Bhagoji <span id="S3.SS1.p6.2.1" class="ltx_text ltx_font_italic">et al. </span>proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to carry out the attack even when the global model is not yet converged. They specifically nurture the malicious updates for <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="lambda" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1a" xref="S3.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.4" xref="S3.SS1.p6.1.m1.1.1.4.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1b" xref="S3.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.5" xref="S3.SS1.p6.1.m1.1.1.5.cmml">b</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1c" xref="S3.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.6" xref="S3.SS1.p6.1.m1.1.1.6.cmml">d</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p6.1.m1.1.1.1d" xref="S3.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.p6.1.m1.1.1.7" xref="S3.SS1.p6.1.m1.1.1.7.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><times id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1"></times><ci id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2">𝑙</ci><ci id="S3.SS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3">𝑎</ci><ci id="S3.SS1.p6.1.m1.1.1.4.cmml" xref="S3.SS1.p6.1.m1.1.1.4">𝑚</ci><ci id="S3.SS1.p6.1.m1.1.1.5.cmml" xref="S3.SS1.p6.1.m1.1.1.5">𝑏</ci><ci id="S3.SS1.p6.1.m1.1.1.6.cmml" xref="S3.SS1.p6.1.m1.1.1.6">𝑑</ci><ci id="S3.SS1.p6.1.m1.1.1.7.cmml" xref="S3.SS1.p6.1.m1.1.1.7">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">lambda</annotation></semantics></math> times to counteract the benefits of benign clients. Two stealth metrics that the server might examine were suggested. The first is that the server may determine whether the attacker’s update aids in model training, i.e., enhances the performance of the overall model. The server may also examine if the submitted update differs from earlier updates, which is the second possibility. They then improved the loss function based on those two covert metrics to prevent anomaly identification in order to increase the attack’s robustness. Finally, the effectiveness of the model is evaluated with a number of assumptions (single-shot, repeated, and pixel-pattern attack). It was observed that the model can achieve <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mn id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">100</mn><mo id="S3.SS1.p6.2.m2.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><csymbol cd="latexml" id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">100\%</annotation></semantics></math> accuracy on an attacker’s task in just a single training round.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison of Attacks Strategies</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<td id="S3.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.1.1.1" class="ltx_p"><span id="S3.T1.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span>
</td>
<td id="S3.T1.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.2.1.1" class="ltx_p"><span id="S3.T1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Attacks Type</span></span>
</span>
</td>
<td id="S3.T1.4.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.3.1.1" class="ltx_p"><span id="S3.T1.4.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Training Set Type</span></span>
</span>
</td>
<td id="S3.T1.4.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.4.1.1" class="ltx_p"><span id="S3.T1.4.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Adaptive Attacks</span></span>
</span>
</td>
<td id="S3.T1.4.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.5.1.1" class="ltx_p"><span id="S3.T1.4.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Backdoor Attacks</span></span>
</span>
</td>
<td id="S3.T1.4.1.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.6.1.1" class="ltx_p"><span id="S3.T1.4.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Application</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.4.2.2" class="ltx_tr">
<td id="S3.T1.4.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.2.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
</span>
</td>
<td id="S3.T1.4.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.2.2.1.1" class="ltx_p">Data Poisoning</span>
</span>
</td>
<td id="S3.T1.4.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.2.3.1.1" class="ltx_p">iid</span>
</span>
</td>
<td id="S3.T1.4.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.2.4.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S3.T1.4.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.2.5.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S3.T1.4.2.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.2.6.1.1" class="ltx_p">Image Classification</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.3.3" class="ltx_tr">
<td id="S3.T1.4.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.3.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span>
</span>
</td>
<td id="S3.T1.4.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.3.2.1.1" class="ltx_p">Data Poisoning</span>
</span>
</td>
<td id="S3.T1.4.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.3.3.1.1" class="ltx_p">Non-iid</span>
</span>
</td>
<td id="S3.T1.4.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.3.4.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S3.T1.4.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.3.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.3.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.3.6.1.1" class="ltx_p">Image Classification</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></span>
</span>
</td>
<td id="S3.T1.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.2.1.1" class="ltx_p">Data Poisoning</span>
</span>
</td>
<td id="S3.T1.4.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.3.1.1" class="ltx_p">iid</span>
</span>
</td>
<td id="S3.T1.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.4.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.4.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.6.1.1" class="ltx_p">Image Classification, Text Prediction and Sentiment Analysis</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.5.5" class="ltx_tr">
<td id="S3.T1.4.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.5.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span>
</span>
</td>
<td id="S3.T1.4.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.5.2.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S3.T1.4.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.5.3.1.1" class="ltx_p">Non-iid</span>
</span>
</td>
<td id="S3.T1.4.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.5.4.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.5.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.5.5.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.5.6.1.1" class="ltx_p">Image Classification and Word Prediction</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.6.6" class="ltx_tr">
<td id="S3.T1.4.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.6.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span>
</span>
</td>
<td id="S3.T1.4.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.6.2.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S3.T1.4.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.6.3.1.1" class="ltx_p">iid</span>
</span>
</td>
<td id="S3.T1.4.6.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.6.4.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.6.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.6.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.6.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.6.6.6.1.1" class="ltx_p">Image Classification and Census Income Prediction</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.7.7" class="ltx_tr">
<td id="S3.T1.4.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:42.7pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.7.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite></span>
</span>
</td>
<td id="S3.T1.4.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.7.2.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S3.T1.4.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.7.3.1.1" class="ltx_p">Non-iid</span>
</span>
</td>
<td id="S3.T1.4.7.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.7.4.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.7.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.7.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S3.T1.4.7.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:99.6pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S3.T1.4.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.7.7.6.1.1" class="ltx_p">Image Classification and Breast Cancer Detection</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.8.8" class="ltx_tr">
<td id="S3.T1.4.8.8.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S3.T1.4.8.8.1.1" class="ltx_text ltx_font_bold">Attack Types</span></td>
<td id="S3.T1.4.8.8.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S3.T1.4.8.8.2.1" class="ltx_text ltx_font_bold">Training Set Types</span></td>
<td id="S3.T1.4.8.8.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S3.T1.4.8.8.3.1" class="ltx_text ltx_font_bold">Adaptive Attacks</span></td>
</tr>
<tr id="S3.T1.4.9.9" class="ltx_tr">
<td id="S3.T1.4.9.9.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Data Poisoning</td>
<td id="S3.T1.4.9.9.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">iid</td>
<td id="S3.T1.4.9.9.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Yes</td>
</tr>
<tr id="S3.T1.4.10.10" class="ltx_tr">
<td id="S3.T1.4.10.10.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Model Poisoning</td>
<td id="S3.T1.4.10.10.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Non-iid</td>
<td id="S3.T1.4.10.10.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">No</td>
</tr>
<tr id="S3.T1.4.11.11" class="ltx_tr">
<td id="S3.T1.4.11.11.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3"><span id="S3.T1.4.11.11.1.1" class="ltx_text ltx_font_bold">Backdoor Attacks</span></td>
<td id="S3.T1.4.11.11.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3"><span id="S3.T1.4.11.11.2.1" class="ltx_text ltx_font_bold">Target Application</span></td>
</tr>
<tr id="S3.T1.4.12.12" class="ltx_tr">
<td id="S3.T1.4.12.12.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Yes</td>
<td id="S3.T1.4.12.12.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Yes</td>
</tr>
<tr id="S3.T1.4.13.13" class="ltx_tr">
<td id="S3.T1.4.13.13.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">No</td>
<td id="S3.T1.4.13.13.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">No Specific Application</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Defense Approaches</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">A number of defense strategies are present in the literature, this study divides the state-of-the-art into three defense types, namely <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">anomaly detection</em>, <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">robust aggregation</em>, and <em id="S3.SS2.p1.1.3" class="ltx_emph ltx_font_italic">perturbation mechanism</em>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Anomaly Detection:</span> In general, anomaly detection is understood as the identification of rare events or observations significantly different from the notion of well-defined data or activities. A defense algorithm to mitigate the label-flipping attacks is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, wherein the aggregator employs an array to store the subset of parameters from the final layers of the deep neural network (DNN) architecture in each round. The list of stored parameters is then fed into principle component analysis to cluster into honest and malicious participants and accordingly limit the participation of malicious clients in model training. Similarly,
A two-phase defense mechanism, known as LoMar is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, wherein the kernel density approximation is employed in order to score the local model update over the neighbouring update. The measured score is utilized to cluster participants with similar characteristics and then employ an outlier detection technique to mitigate the poisoning attack.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Robust Aggregation:</span> Aggregation mechanisms are utilized to mitigate poisoning attacks during the training phase in contrast to anomaly detection, and a few robust aggregation mechanisms are Krum, mean, trimmed mean, etc. Furthermore, most of these methodologies assume the iid data, therefore, fail to mitigate against current poisoning attacks for non-iid scenarios.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">A feedback-based defense strategy, known as <em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">BaFFle</em>, is put forward in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to eliminate the backdoored model updates. <em id="S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">BaFFle</em> employs each participant to validate the global model for each round of FL by quantifying the validation function with their own private data to report whether the model is backdoored or not to the central server. The validation function compares the misclassification rate of a distinct class with the previous global model. Then, the central server utilizes the feedback via the validation function to decide whether to accept or reject the global model update based on the misclassification rate.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">The discussed defense strategies work fine in the presence of a small number of malicious participants. The performance of these strategies degrades with an increase in malicious participants. Zhang <span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_italic">et al. <cite class="ltx_cite ltx_citemacro_cite"><span id="S3.SS2.p5.1.1.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S3.SS2.p5.1.1.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> proposed the malicious client detection algorithm, named <em id="S3.SS2.p5.1.2" class="ltx_emph ltx_font_italic">FLDetector</em>, wherein the idea is to detect the malicious participants and eliminate them from the global model updates. In <em id="S3.SS2.p5.1.3" class="ltx_emph ltx_font_italic">FLDetector</em>, the central server predicts the participants’ model updates in each iteration based on historical updates and flags the participant as malicious if there is an inconsistency between the current update and the predicted one in multiple iterations. The performance evaluation illustrates promising results when compared with the current state-of-the-art. Nevertheless, the impact of eliminating the malicious participants on the main task accuracy is not discussed.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Perturbation Mechanism:</span> The perturbation mechanism in mathematics represents a method of solving a problem by comparing it with the known solution. One of the well-known perturbation mechanisms is differential privacy, which has been extensively employed in FL to improve the performance of outlier detection. A low-complexity defense approach is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> in order to mitigate the backdoored model updates via weight clipping and noise injection. However, this approach fails to mitigate the untargeted backdoored attacks that do not rely on model weight modification. In essence, the weight clipping method may negatively influence the change in the weights of benign participants’ model updates. Most recently, to overcome the limitation of current differential privacy-based strategies that deteriorate the performance of benign participants, a novel technique, called (<em id="S3.SS2.p6.1.2" class="ltx_emph ltx_font_italic">FLAME</em>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, is proposed. <em id="S3.SS2.p6.1.3" class="ltx_emph ltx_font_italic">FLAME</em> estimates the amount of noise to be injected to mitigate the targeted poisoning attacks with minimal impact on benign participants. This technique employs the clustering and weight-clipping methods to eliminate outliers in the participants’ updates and then the estimated noise is added to obtained parameters to mitigate possible attacks. The experimental evaluation demonstrates that <em id="S3.SS2.p6.1.4" class="ltx_emph ltx_font_italic">FLAME</em> is effective in defending against poisoning attacks when compared with the state-of-the-art. However, it requires extensive modification in the current FL framework.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/attack_accuracy.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Model Accuracy - DPA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/attack_success.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="438" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Attack Success Rate - DPA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/attack_accuracy_model.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="438" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S3.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Model Accuracy - MPA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S3.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/attack_success_model.png" id="S3.F2.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="447" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S3.F2.sf4.3.2" class="ltx_text" style="font-size:90%;">Attack Success Rate - MPA</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Poisoning Attacks - Model Accuracy and Attack Success Rate on CIFAR-10 w.r.t. varying malicious clients for DPA and varying iterations for MPA</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Analysis and Evaluation</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section presents the analysis and performance evaluation of poisoning attacks and the defense strategies.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Comparison and Evaluation of Attack Strategies</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Firstly, we compare attack strategies with the following fours characteristics (Table <a href="#S3.T1" title="TABLE I ‣ 3.1 Attacks Strategies ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>)</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">Training Set Types:</em> FL is devised to employ clients’ data while protecting the privacy, and there are two types of data in general, i.e., independent and identically distributed (iid) and non-independent and identically distributed (non-iid). Here, iid data samples are statistically identical and drawn from the same probability distribution, wherein each data sample is seen as an independent event, while non-iid data samples are not statistically identical and include complexities beyond iid, including but not limited to relationships, heterogeneity, dynamic data over time, sampling, etc. In general, most of the real-life datasets are non-iid. However, the current notion of analytics and machine learning methods often are based on the assumption that datasets are iid. Therefore, it is imperative to assess the proposed solutions on training set type in terms of their suitability for real-world applications.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">As can be seen in Table <a href="#S3.T1" title="TABLE I ‣ 3.1 Attacks Strategies ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, the majority of the attack strategies utilize iid datasets for DPA (except <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) in order to distinguish between the benign and malicious updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In contrast, the majority of MPA use datasets that are non-iid, making them beneficial in real-life FL environments.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><em id="S4.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Adaptive Attacks:</em> An adaptive attack represents the variation in the attack scenario of the attacker, e.g., each round, an attacker modifies its strategies to attack the data or model by learning the defense strategies of the FL system. A number of state-of-the-art attack approaches consider adaptive attacks
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> to disrupt the functionality of federated learning systems via poisoning attacks (i.e., data and model) from the intelligent malicious clients.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.p5.1" class="ltx_p"><em id="S4.SS1.p5.1.1" class="ltx_emph ltx_font_italic">Backdoor Attacks:</em> A backdoor attack aims to mislead the model to perform abnormally on data samples stamped with a backdoor trigger to misclassify specific target labels in terms of targeted backdoor but behave normally on all other samples. Moreover, the untargeted backdoor attacks aim to degrade the overall accuracy of the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The majority of the studies discussed in this research consider the backdoor attacks to mislead the backdoor model by injecting local triggers to poison the model (except <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>)</p>
</div>
<div id="S4.SS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS1.p6.1" class="ltx_p"><em id="S4.SS1.p6.1.1" class="ltx_emph ltx_font_italic">Application:</em> It is also imperative to identify the practicality of the proposed attacks model in terms of a specific application in order to devise the conclusion on the suitability of the current state-of-the-art. As can be seen from the table that most of the proposed attack strategies focus on poisoning attacks on image classification.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">The experimental evaluation in terms of overall model accuracy (MA) and attack success rate (ASR) is carried out for a few selected attacks models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> as depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for both DPA and MPA. MA measures the model accuracy on benign samples, whereas, ASR represents the probability (success rate) of attacks in misclassifying the target labels.</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<p id="S4.SS1.p8.1" class="ltx_p">We have trained the global model on CIFAR-10 for attack scenarios and MNIST for defense strategies with a total of 100 participants and considered the common simulation scenarios among all the compared models. For DPA, we train the global model with a varying number of malicious participants. For the MPA, the global model is trained with a varying number of iterations with a poisoning rate of <math id="S4.SS1.p8.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS1.p8.1.m1.1a"><mrow id="S4.SS1.p8.1.m1.1.1" xref="S4.SS1.p8.1.m1.1.1.cmml"><mn id="S4.SS1.p8.1.m1.1.1.2" xref="S4.SS1.p8.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS1.p8.1.m1.1.1.1" xref="S4.SS1.p8.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p8.1.m1.1b"><apply id="S4.SS1.p8.1.m1.1.1.cmml" xref="S4.SS1.p8.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p8.1.m1.1.1.1.cmml" xref="S4.SS1.p8.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p8.1.m1.1.1.2.cmml" xref="S4.SS1.p8.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p8.1.m1.1c">10\%</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p9" class="ltx_para">
<p id="S4.SS1.p9.1" class="ltx_p">The results for DPA are shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a and <a href="#S3.F2" title="Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b. It can be seen that <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> can achieve higher ASR (Figure <a href="#S3.F2.sf1" title="In Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>) than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and also maintains high MA (Figure <a href="#S3.F2.sf2" title="In Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>). The reason for lower ASR and lower MA for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is the use of complex CIFAR-10 datasets and the increase in the malicious ratio. Moreover, the results for MPA are depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>c and <a href="#S3.F2" title="Figure 2 ‣ 3.2 Defense Approaches ‣ 3 Poisoning Attacks and Defense Strategies ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>d. It can be seen that the ASR for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> decreases with an increase in the iterations as the benign participants dilute the impact of backdoor. However, the ASR and MA of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> remains stable with the iterations.</p>
</div>
<div id="S4.SS1.p10" class="ltx_para">
<p id="S4.SS1.p10.1" class="ltx_p">In general, the current studies are well-suited to poisoning attack scenarios with the fixed assumption. Nevertheless, the predefined conditions in the state-of-the-art attack strategies may not be suitable for real-world applications with dynamic characteristics.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Comparison of Defense Strategies</span></figcaption>
<table id="S4.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.4.1.1" class="ltx_tr">
<td id="S4.T2.4.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.1.1.1" class="ltx_p"><span id="S4.T2.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Literature</span></span>
</span>
</td>
<td id="S4.T2.4.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.2.1.1" class="ltx_p"><span id="S4.T2.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Defense Type</span></span>
</span>
</td>
<td id="S4.T2.4.1.1.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.3.1.1" class="ltx_p"><span id="S4.T2.4.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Attack Type</span></span>
</span>
</td>
<td id="S4.T2.4.1.1.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.4.1.1" class="ltx_p"><span id="S4.T2.4.1.1.4.1.1.1" class="ltx_text ltx_font_bold">Defense Target</span></span>
</span>
</td>
<td id="S4.T2.4.1.1.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.5.1.1" class="ltx_p"><span id="S4.T2.4.1.1.5.1.1.1" class="ltx_text ltx_font_bold">Training Time Defenses</span></span>
</span>
</td>
<td id="S4.T2.4.1.1.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.6.1.1" class="ltx_p"><span id="S4.T2.4.1.1.6.1.1.1" class="ltx_text ltx_font_bold">Secure Aggregation</span></span>
</span>
</td>
<td id="S4.T2.4.1.1.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.1.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.1.1.7.1.1" class="ltx_p"><span id="S4.T2.4.1.1.7.1.1.1" class="ltx_text ltx_font_bold">Effect on Benign Clients</span></span>
</span>
</td>
</tr>
<tr id="S4.T2.4.2.2" class="ltx_tr">
<td id="S4.T2.4.2.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.2.2.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.2.1.1" class="ltx_p">Anomaly Detection</span>
</span>
</td>
<td id="S4.T2.4.2.2.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.3.1.1" class="ltx_p">Data Poisoning</span>
</span>
</td>
<td id="S4.T2.4.2.2.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.4.1.1" class="ltx_p">Label Flipping</span>
</span>
</td>
<td id="S4.T2.4.2.2.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.2.2.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.6.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S4.T2.4.2.2.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.2.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.2.2.7.1.1" class="ltx_p">No</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.3.3" class="ltx_tr">
<td id="S4.T2.4.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.3.3.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.2.1.1" class="ltx_p">Anomaly Detection</span>
</span>
</td>
<td id="S4.T2.4.3.3.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.3.1.1" class="ltx_p">Data Poisoning</span>
</span>
</td>
<td id="S4.T2.4.3.3.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.4.1.1" class="ltx_p">Label Flipping</span>
</span>
</td>
<td id="S4.T2.4.3.3.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.3.3.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.6.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S4.T2.4.3.3.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.3.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.3.3.7.1.1" class="ltx_p">No</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.2.1.1" class="ltx_p">Robust Aggregation</span>
</span>
</td>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.3.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.4.1.1" class="ltx_p">Backdoor Attacks</span>
</span>
</td>
<td id="S4.T2.4.4.4.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.5.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S4.T2.4.4.4.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.6.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.4.4.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.4.4.7.1.1" class="ltx_p">No</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.5.5" class="ltx_tr">
<td id="S4.T2.4.5.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.5.5.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.2.1.1" class="ltx_p">Robust Aggregation</span>
</span>
</td>
<td id="S4.T2.4.5.5.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.3.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S4.T2.4.5.5.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.4.1.1" class="ltx_p">Untargeted Attacks</span>
</span>
</td>
<td id="S4.T2.4.5.5.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.5.5.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.6.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.5.5.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.5.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.5.5.7.1.1" class="ltx_p">Yes</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.6.6" class="ltx_tr">
<td id="S4.T2.4.6.6.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.6.6.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.2.1.1" class="ltx_p">Perturbation Mechanism</span>
</span>
</td>
<td id="S4.T2.4.6.6.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.3.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S4.T2.4.6.6.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.4.1.1" class="ltx_p">Backdoor Attacks</span>
</span>
</td>
<td id="S4.T2.4.6.6.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.6.6.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.6.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.6.6.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.6.6.7.1.1" class="ltx_p">Yes</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.7.7" class="ltx_tr">
<td id="S4.T2.4.7.7.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="width:34.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.1.1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></span>
</span>
</td>
<td id="S4.T2.4.7.7.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:93.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.2.1.1" class="ltx_p">Perturbation Mechanism</span>
</span>
</td>
<td id="S4.T2.4.7.7.3" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:64.0pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.3.1.1" class="ltx_p">Model Poisoning</span>
</span>
</td>
<td id="S4.T2.4.7.7.4" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:71.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.4.1.1" class="ltx_p">Backdoor Attacks</span>
</span>
</td>
<td id="S4.T2.4.7.7.5" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:54.1pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.5.1.1" class="ltx_p">Yes</span>
</span>
</td>
<td id="S4.T2.4.7.7.6" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:51.2pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.6.1.1" class="ltx_p">No</span>
</span>
</td>
<td id="S4.T2.4.7.7.7" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" style="width:56.9pt;padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S4.T2.4.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.4.7.7.7.1.1" class="ltx_p">No</span>
</span>
</td>
</tr>
<tr id="S4.T2.4.8.8" class="ltx_tr">
<td id="S4.T2.4.8.8.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3"><span id="S4.T2.4.8.8.1.1" class="ltx_text ltx_font_bold">Defense Types</span></td>
<td id="S4.T2.4.8.8.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S4.T2.4.8.8.2.1" class="ltx_text ltx_font_bold">Attack Type</span></td>
<td id="S4.T2.4.8.8.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S4.T2.4.8.8.3.1" class="ltx_text ltx_font_bold">Defense Target</span></td>
</tr>
<tr id="S4.T2.4.9.9" class="ltx_tr">
<td id="S4.T2.4.9.9.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Anomaly Detection</td>
<td id="S4.T2.4.9.9.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Data Poisoning</td>
<td id="S4.T2.4.9.9.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Label Flipping</td>
</tr>
<tr id="S4.T2.4.10.10" class="ltx_tr">
<td id="S4.T2.4.10.10.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Perturbation Mechanism</td>
<td id="S4.T2.4.10.10.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Model Poisoning</td>
<td id="S4.T2.4.10.10.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Backdoor Attacks</td>
</tr>
<tr id="S4.T2.4.11.11" class="ltx_tr">
<td id="S4.T2.4.11.11.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Robust Aggregation</td>
<td id="S4.T2.4.11.11.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">-</td>
<td id="S4.T2.4.11.11.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Untargeted Attacks</td>
</tr>
<tr id="S4.T2.4.12.12" class="ltx_tr">
<td id="S4.T2.4.12.12.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3"><span id="S4.T2.4.12.12.1.1" class="ltx_text ltx_font_bold">Training Type Defenses</span></td>
<td id="S4.T2.4.12.12.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S4.T2.4.12.12.2.1" class="ltx_text ltx_font_bold">Secure Aggregation</span></td>
<td id="S4.T2.4.12.12.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2"><span id="S4.T2.4.12.12.3.1" class="ltx_text ltx_font_bold">Influence on Benign Clients</span></td>
</tr>
<tr id="S4.T2.4.13.13" class="ltx_tr">
<td id="S4.T2.4.13.13.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Yes</td>
<td id="S4.T2.4.13.13.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Yes</td>
<td id="S4.T2.4.13.13.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Yes</td>
</tr>
<tr id="S4.T2.4.14.14" class="ltx_tr">
<td id="S4.T2.4.14.14.1" class="ltx_td ltx_align_center ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="3">Yes</td>
<td id="S4.T2.4.14.14.2" class="ltx_td ltx_align_center ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Np</td>
<td id="S4.T2.4.14.14.3" class="ltx_td ltx_align_center ltx_align_middle ltx_border_b ltx_border_l ltx_border_r" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">No</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Comparison and Evaluation of Defense Strategies</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Table <a href="#S4.T2" title="TABLE II ‣ 4.1 Comparison and Evaluation of Attack Strategies ‣ 4 Analysis and Evaluation ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> illustrates the comparison of defense strategies by employing a number of aspects discussed below.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">Defense Types:</em> A variety of defense strategies are suggested to lessen the effects of poisoning attacks and it is imperative to compare the effectiveness of each of these strategies since they each have merits and drawbacks. The defense strategies described in the literature have generally been grouped into three categories: <em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">anomaly detection</em>, <em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">perturbation mechanism</em>, and <em id="S4.SS2.p2.1.4" class="ltx_emph ltx_font_italic">robust aggregation</em>. In essence, the literature uses either the robust aggregation or perturbation method for the MPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, but the defensive strategies for DPA include anomaly detection as the defense type to mitigate these attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><em id="S4.SS2.p3.1.1" class="ltx_emph ltx_font_italic">Defense Target:</em> This parameter compares the existing literature in terms of types of targeted attacks mitigated by the current state-of-the-art (e.g., label-flipping attacks, backdoor attacks, untargeted attacks, etc). The label-flipping attack is the common DPA mitigated by the present state-of-the-art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>,
and in contrast, backdoor attacks and untargeted attacks are the defense target for the MPA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p"><em id="S4.SS2.p4.1.1" class="ltx_emph ltx_font_italic">Training Time Defense:</em> Most of the proposed solutions are effective in mitigating poisoning attacks during the training time period as the global model does not have access to client’s data, therefore, the submitted local model is evaluated in terms of its reliability. However, a few studies consider applying the defense strategies on the model submitted by the clients after training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In these studies, the defender prunes the dormant neurons after the training step via the submitted pruning sequence from the client.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.1" class="ltx_p"><em id="S4.SS2.p5.1.1" class="ltx_emph ltx_font_italic">Secure Aggregation:</em>
In order to prevent the inspection of confidential model updates, a number of FL works suggest utilizing secure multi-party computation (MPC). MPC is a subbranch of cryptography, wherein the goal is to create a method for multiple parties to jointly compute a function over their inputs
while preserving privacy. Thus, the comparison of current studies in terms of compatibility with secure aggregation is essential. Most of the FL systems employ MPC to secure submitted confidential models from the clients. However, a few defense strategies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> rely on detecting the local model updates and are not compatible with secure aggregation.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.1" class="ltx_p"><em id="S4.SS2.p6.1.1" class="ltx_emph ltx_font_italic">Effect on Benign Clients:</em> It is essential for the defense strategies to not only detect malicious clients but also have minimal effect on benign clients. A number of studies have a negative effect on benign clients. For example, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> utilize the suspicious score by comparing the predicted and updated model to detect malicious clients. This score can cause ill-effect on benign clients if the score is closer to the boundary of the suspicious score. The performance of the benign clients is impacted by the differential privacy mechanism in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> because the clipping factor will alter the benign updates.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.2" class="ltx_p">In Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Comparison and Evaluation of Defense Strategies ‣ 4 Analysis and Evaluation ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we have compared the four defense strategies studied in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> in terms of MA with respect to varying percentages of malicious participants the number of iterations for DPA and MPA, respectively. As depicted in Figure <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4.2 Comparison and Evaluation of Defense Strategies ‣ 4 Analysis and Evaluation ‣ Poisoning Attacks and Defenses in Federated Learning: A Survey" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, the MA for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> decreases as the number of malicious participants increases, and maintains the accuracy of around <math id="S4.SS2.p7.1.m1.1" class="ltx_Math" alttext="80\%" display="inline"><semantics id="S4.SS2.p7.1.m1.1a"><mrow id="S4.SS2.p7.1.m1.1.1" xref="S4.SS2.p7.1.m1.1.1.cmml"><mn id="S4.SS2.p7.1.m1.1.1.2" xref="S4.SS2.p7.1.m1.1.1.2.cmml">80</mn><mo id="S4.SS2.p7.1.m1.1.1.1" xref="S4.SS2.p7.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.1.m1.1b"><apply id="S4.SS2.p7.1.m1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p7.1.m1.1.1.1.cmml" xref="S4.SS2.p7.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p7.1.m1.1.1.2.cmml" xref="S4.SS2.p7.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.1.m1.1c">80\%</annotation></semantics></math>. Nevertheless, the accuracy of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> remains stable with an average MA of <math id="S4.SS2.p7.2.m2.1" class="ltx_Math" alttext="85\%" display="inline"><semantics id="S4.SS2.p7.2.m2.1a"><mrow id="S4.SS2.p7.2.m2.1.1" xref="S4.SS2.p7.2.m2.1.1.cmml"><mn id="S4.SS2.p7.2.m2.1.1.2" xref="S4.SS2.p7.2.m2.1.1.2.cmml">85</mn><mo id="S4.SS2.p7.2.m2.1.1.1" xref="S4.SS2.p7.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p7.2.m2.1b"><apply id="S4.SS2.p7.2.m2.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p7.2.m2.1.1.1.cmml" xref="S4.SS2.p7.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.p7.2.m2.1.1.2.cmml" xref="S4.SS2.p7.2.m2.1.1.2">85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p7.2.m2.1c">85\%</annotation></semantics></math>. Furthermore, the MA for defense strategies for MPA increases as the number of iterations increases for both <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Nonetheless, the convergence time in terms of defense mechanism for <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> is better than <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p">As a whole, the current research on defense strategies for poisoning attacks is well-defined for the specific scenarios. However, intelligent malicious clients can still learn the behaviour of these defense strategies and poison the FL systems.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/defense_accuracy_data.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="434" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">Defense Against DPA</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.05795/assets/defense_accuracy_model.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="434" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">Defense Against MPA</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Model Accuracy w.r.t. varying percentage of Malicious Clients and Number of Iterations</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Future Research Directions</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As the notion of the FL paradigm has been widely explored in recent years, adversarial attacks and defenses for FL are also investigated. Nevertheless, there are still numerous research challenges that need the attention of researchers. We have identified a number of such research challenges in this section.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Firstly, the mathematical formulation for different types of poisoning attacks is not lacking. For example, in data poisoning, the label-flipping attack depends on the number of malicious clients in the data and it is not deterministic (i.e., no mathematical exposition of this phenomenon). Furthermore, most of the attack strategies are inclined towards data with similar feature space, and different samples. However, the behaviour of these attacks will vary with data having different feature space (i.e., vertical FL) as the label information is usually hidden in this type of FL. In general, proposing the mathematical exposition of attacks and designing attack strategies suitable for vertical FL are potential research directions.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Secondly, defense strategies proposed in the literature somehow have vulnerabilities that can be transformed into another adversarial attack, hence, the study of these vulnerabilities and the solutions to mitigate such attacks is imperative and challenging. Furthermore, most defense strategies have trade-offs between attack prevention and the overall performance of the original task.
For example, a solution based on differential privacy tends to add noise in order to preserve privacy, which however impairs the performance of the global model. In addition, the client filtering-based defense solutions sometimes lead to filtering out more clients and thus, losing a large portion of data for the aggregation process. In general, it is imperative to consider an optimal defense strategy that can filter out all the attacks while maintaining the original task’s performance. The solution must not compromise the privacy of the clients while preventing adversarial attacks.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Thirdly, the importance of the types of training set cannot be understated since it is difficult to distinguish between malicious and benign clients with various data samples that are important for the learning process in the case of non-iid distribution of data. Employing an anomaly detection algorithm suitable for non-iid distribution or those that do not rely on data distribution
is the typical solution to these situations. Nevertheless, it is a problem that requires attention because the methods mentioned above still struggle to identify malicious clients in situations with highly skewed data distribution.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Finally, keeping track of adversaries is one of the key challenges since clients in the FL system are free to leave and rejoin the FL system at any moment during the training process. As a result, it is critical for the FL system to keep track of malicious clients that leave and then rejoin the system. However, existing research works do not offer any suggestions for a fix. One of the potential solutions to this issue is to employ either smart contracts or credibility evaluation mechanisms in multiple rounds to identify the reputation of clients in order to keep the adversaries traceable.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The emergence of FL is one of the challenging and interesting research directions in the field of machine learning because of in light of rigorous laws for protecting the privacy of participating clients. Nevertheless, the novel concept of FL leads to new challenges due to the invisibility of client-side training data in terms of adversarial attacks, and the poisoning attack is one of such attacks. Recent years have seen an increase in the literature on poisoning attacks and defense strategies to mitigate these attacks. This paper presents a comprehensive summary of current poisoning attacks and defense strategies for FL. Furthermore, we perform a comparative analysis of the state-of-the-art for both poisoning attack and defense in various aspects. Additionally, the results of experimental evaluations are presented for quantitative comparisons. Finally, we conclude that the study of FL threats is ongoing by pointing out the challenges and future research direction.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Jakub Konečný and Brendan McMahan and Daniel Ramage, “Federated
Optimization: Distributed Optimization Beyond the Datacenter,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CoRR</em>,
vol. abs/1511.03575, 2015. [Online]. Available:
http://arxiv.org/abs/1511.03575

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Jere, Malhar S. and Farnan, Tyler and Koushanfar, Farinaz, “A Taxonomy of
Attacks on Federated Learning,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Security &amp; Privacy</em>, vol. 19,
no. 2, pp. 20–28, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Steinhardt, P. W. Koh, and P. Liang, “Certified Defenses for Data
Poisoning Attacks,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 31st International
Conference on Neural Information Processing Systems</em>, 2017, p. 3520–3532.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
N. Rodríguez-Barroso, D. Jiménez-López, M. V. Luzón, F. Herrera, and
E. Martínez-Cámara, “Survey on federated Learning Threats: Concepts,
Taxonomy on Attacks and Defences, Experimental Study and Challenges,”
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Information Fusion</em>, vol. 90, pp. 148–173, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data Poisoning Attacks
Against Federated Learning Systems,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Computer Security – ESORICS
2020</em>.   Cham: Springer International
Publishing, 2020, pp. 480–501.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. Xie, K. Huang, P.-Y. Chen, and B. Li, “DBA: Distributed Backdoor Attacks
against Federated Learning,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning
Representations</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J.-y. Sohn,
K. Lee, and D. Papailiopoulos, “Attack of the Tails: Yes, You Really Can
Backdoor Federated Learning,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information
Processing Systems</em>, vol. 33.   Curran
Associates, Inc., 2020, pp. 16 070–16 084.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How To
Backdoor Federated Learning,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty Third
International Conference on Artificial Intelligence and Statistics</em>, ser.
Proceedings of Machine Learning Research, vol. 108.   PMLR, 26–28 Aug 2020, pp. 2938–2948.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Fang, X. Cao, J. Jia, and N. Z. Gong, “Local Model Poisoning Attacks to
Byzantine-Robust Federated Learning,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">29th USENIX Security
Symposium (USENIX Security 2020)</em>.   USA: USENIX Association, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Zhang, X. Cao, J. Jia, and N. Z. Gong, “FLDetector: Defending Federated
Learning Against Model Poisoning Attacks via Detecting Malicious Clients,”
in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining</em>, New York, NY, USA, 2022, p. 2545–2555.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing federated
learning through an adversarial lens,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>.   PMLR, 2019, pp.
634–643.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Li, Z. Qu, S. Zhao, B. Tang, Z. Lu, and Y. Liu, “LoMar: A Local Defense
Against Poisoning Attack on Federated Learning,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Dependable and Secure Computing</em>, no. 01, pp. 1–1, dec 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Andreina, G. A. Marson, H. Möllering, and G. Karame, “BaFFLe: Backdoor
Detection via Feedback-based Federated Learning,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 41st
International Conference on Distributed Computing Systems (ICDCS)</em>, 2021, pp.
852–863.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can You Really Backdoor
Federated Learning?” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.07963</em>, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
T. D. Nguyen, P. Rieger, H. Chen, H. Yalame, H. Möllering, H. Fereidooni,
S. Marchal, M. Miettinen, A. Mirhoseini, S. Zeitouni, F. Koushanfar, A.-R.
Sadeghi, and T. Schneider, “FLAME: Taming Backdoors in Federated
Learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">31st USENIX Security Symposium (USENIX Security 22)</em>,
Boston, MA, Aug. 2022, pp. 1415–1432.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.05794" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.05795" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.05795">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.05795" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.05796" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 07:40:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
