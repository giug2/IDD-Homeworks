<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.14966] What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?</title><meta property="og:description" content="In visual question answering (VQA), a machine must answer a question given an associated image. Recently, accessibility researchers have explored whether VQA can be deployed in a real-world setting where users with vis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.14966">

<!--Generated on Thu Mar 14 02:04:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">What’s Different between Visual Question Answering
<br class="ltx_break">for Machine “Understanding” Versus for Accessibility?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Trista Cao
<br class="ltx_break">University of Maryland 
<br class="ltx_break"><span id="id2.2.id1" class="ltx_text ltx_font_typewriter">ycao95@umd.edu</span> 
<br class="ltx_break">&amp;Kyle Seelman<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break">University of Maryland 
<br class="ltx_break"><span id="id3.3.id2" class="ltx_text ltx_font_typewriter">kseelman@umd.edu</span> 
<br class="ltx_break"><span id="id4.4.id3" class="ltx_ERROR undefined">\AND</span>Kyungjun Lee<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>
<br class="ltx_break">University of Maryland 
<br class="ltx_break"><span id="id5.5.id4" class="ltx_text ltx_font_typewriter">kyungjun@umd.edu</span> 
<br class="ltx_break">&amp;Hal Daumé III 
<br class="ltx_break">University of Maryland 
<br class="ltx_break">Microsoft Research 
<br class="ltx_break"><span id="id6.6.id5" class="ltx_text ltx_font_typewriter">me@hal3.name
<br class="ltx_break"></span>
</span><span class="ltx_author_notes"><sup id="id7.7.id1" class="ltx_sup"><span id="id7.7.id1.1" class="ltx_text ltx_font_italic">⋆</span></sup> Equal contribution</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">In visual question answering (VQA), a machine must answer a question given an associated image. Recently, accessibility researchers have explored whether VQA can be deployed in a real-world setting where users with visual impairments learn about their environment by capturing their visual surroundings and asking questions. However, most of the existing benchmarking datasets for VQA focus on machine “understanding” and it remains unclear how progress on those datasets corresponds to improvements in this real-world use case. We aim to answer this question by evaluating discrepancies between machine “understanding” datasets (VQA-v2) and accessibility datasets (VizWiz) by evaluating a variety of VQA models. Based on our findings, we discuss opportunities and challenges in VQA for accessibility and suggest directions for future work.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.2" class="ltx_p"><span id="p1.1.2.1" class="ltx_text ltx_font_bold">What’s Different between Visual Question Answering
<br class="ltx_break">for Machine “Understanding” Versus for Accessibility?</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.1" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.1.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.1.1.1" class="ltx_tr">
<span id="p1.1.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Yang Trista Cao<span id="p1.1.1.1.1.1.1.1.1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span> <sup id="p1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="p1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">⋆</span></sup> Equal contribution</span></span></span></span></span></span>
<span id="p1.1.1.1.1.2.1" class="ltx_tr">
<span id="p1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center">University of Maryland</span></span>
<span id="p1.1.1.1.1.3.2" class="ltx_tr">
<span id="p1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">ycao95@umd.edu</span></span></span>
</span>
</span></span>                      <span id="p1.1.1.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.2.1.1.1" class="ltx_tr">
<span id="p1.1.1.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Kyle Seelman<span id="footnotex3" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex3.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span></span></span></span>
<span id="p1.1.1.2.1.2.2" class="ltx_tr">
<span id="p1.1.1.2.1.2.2.1" class="ltx_td ltx_align_center">University of Maryland</span></span>
<span id="p1.1.1.2.1.3.3" class="ltx_tr">
<span id="p1.1.1.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.3.3.1.1" class="ltx_text ltx_font_typewriter">kseelman@umd.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.3" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.3.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.3.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.3.1.1.1.1" class="ltx_tr">
<span id="p1.1.3.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Kyungjun Lee<span id="footnotex4" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex4.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span></span></span></span>
<span id="p1.1.3.1.1.2.2" class="ltx_tr">
<span id="p1.1.3.1.1.2.2.1" class="ltx_td ltx_align_center">University of Maryland</span></span>
<span id="p1.1.3.1.1.3.3" class="ltx_tr">
<span id="p1.1.3.1.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.3.1.1.3.3.1.1" class="ltx_text ltx_font_typewriter">kyungjun@umd.edu</span></span></span>
</span>
</span></span>                      <span id="p1.1.3.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.3.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.3.2.1.1.1" class="ltx_tr">
<span id="p1.1.3.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Hal Daumé III</span></span></span>
<span id="p1.1.3.2.1.2.2" class="ltx_tr">
<span id="p1.1.3.2.1.2.2.1" class="ltx_td ltx_align_center">University of Maryland</span></span>
<span id="p1.1.3.2.1.3.3" class="ltx_tr">
<span id="p1.1.3.2.1.3.3.1" class="ltx_td ltx_align_center">Microsoft Research</span></span>
<span id="p1.1.3.2.1.4.4" class="ltx_tr">
<span id="p1.1.3.2.1.4.4.1" class="ltx_td ltx_align_center"><span id="p1.1.3.2.1.4.4.1.1" class="ltx_text ltx_font_typewriter">me@hal3.name</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2210.14966/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="631" height="330" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Given similar image content (left: food, right: cat), questions in the machine “understanding” dataset VQA-v2 and the accessibility dataset VizWiz are substantially different. The VizWiz examples show questions that are significantly more specific (with one question even explicitly stating that it’s already obvious that this is a can of food), more verbal, and significantly less artificial (as in the cat examples) than the VQA-v2 ones.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Much research has focused on evaluating and pushing the boundary of machine “understanding” – can machines achieve high scores on tasks thought to require human-like comprehension, including image tagging and captioning <cite class="ltx_cite ltx_citemacro_citep">(e.g., Lin et al., <a href="#bib.bib13" title="" class="ltx_ref">2014</a>)</cite>, and various forms of reasoning <cite class="ltx_cite ltx_citemacro_citep">(e.g., Wang et al., <a href="#bib.bib19" title="" class="ltx_ref">2018</a>; Sap et al., <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>.
In recent years, with the advancement of deep learning, we saw great improvements in machines’ capabilities in accomplishing these tasks, raising the possibility for deployment.
However, adapting machine systems in real-life is non-trivial as real-life situations and users can be significantly different from synthetic and crowdsourced dataset examples <cite class="ltx_cite ltx_citemacro_citep">(Shneiderman, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>.
In this paper we use the visual question answering (VQA) task as an example to call more attention to shifting from development on machine “understanding” to building machines that can make positive impacts to the society and people.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Visual question answering (VQA) is a task that requires a model to answer natural language questions based on images.
This idea dates back to at least to the 1960s in the form of answering questions about pictorial inputs <cite class="ltx_cite ltx_citemacro_citep">(Coles, <a href="#bib.bib7" title="" class="ltx_ref">1968</a>; Theune et al., <a href="#bib.bib18" title="" class="ltx_ref">2007</a>, i.a.)</cite>, and builds on “intelligence” tests like the total Turing test <cite class="ltx_cite ltx_citemacro_citep">(Harnad, <a href="#bib.bib9" title="" class="ltx_ref">1990</a>)</cite>.
Over the past few years, the task was re-popularized with new modeling techniques and datasets <cite class="ltx_cite ltx_citemacro_citep">(e.g. Malinowski and Fritz, <a href="#bib.bib14" title="" class="ltx_ref">2014</a>; Marino et al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>.
However, besides the purpose of testing a models’ multi-modal “understanding,” VQA systems could be potentially beneficial for visually impaired people in answering their questions about the visual world in real-time.
For simplicity, we call the former view <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">machine understanding VQA</span> (henceforth omitting the scare quotes) and the latter <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">accessibility VQA</span>.
The majority of research in VQA (<a href="#S2" title="2 Related Work ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 2</span></a>) focuses on the machine understanding view.
As a result, it is not clear whether VQA model architectures developed and evaluated on machine understanding datasets can be easily adapted to the accessibility setting, as the distribution of images, questions, and answers might be—and, as shown in <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, are—quite different.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we aim to investigate the gap between the machine understanding VQA and the accessibility VQA by uncovering the challenges of adapting machine understanding VQA model architectures on an accessibility VQA dataset.
Here, we focus on English VQA systems and datasets; for machine understanding VQA, we use the VQA-v2 dataset <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>, while for accessibility VQA, we use the VizWiz dataset <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> (<a href="#S3.SS1" title="3.1 Datasets ‣ 3 Experiment Setup ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 3.1</span></a>).
Through performance assessments of seven machine understanding VQA model architectures that span 2017–2021 (<a href="#S3.SS3" title="3.3 Models ‣ 3 Experiment Setup ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 3.3</span></a>), we find that model architecture advancements on machine understanding VQA also improve the performance on the accessibility task, but that the gap of the model performance between the two is still significant and is <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">increasing</span> (<a href="#S4.SS1" title="4.1 Model Performance Progress ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 4.1</span></a>).
This increasing gap in accuracy indicates that adapting model architectures that were developed for machine understanding to assist visually impaired people is challenging, and that model development in this area may indicate architectural overfitting.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We then further investigate what types of questions in the accessibility dataset remain hard for the state-of-the-art (SOTA) VQA model architecture (<a href="#S4.SS2" title="4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 4.2</span></a>).
We adopt the data challenge taxonomies from <cite class="ltx_cite ltx_citemacro_citet">Bhattacharya et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zeng et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> to perform both quantitative and qualitative error analysis based on these challenge classes.
We find some particularly challenging classes within the accessibility dataset for the VQA models as a direction for future work to improve on.
Additionally, we observe that many of the questions on which state-of-the-art models perform poorly are not due to the model not learning, but rather due to a need for higher quality annotations and evaluation metrics.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">To the best of our knowledge, this is the first work that attempts to quantify and understand the gap in performance VQA models have between the VQA-v2 dataset collected by sighted people and the VizWiz dataset that contains images and questions from people with visual impairments and answers from sighted people. <cite class="ltx_cite ltx_citemacro_citet">Brady et al. (<a href="#bib.bib6" title="" class="ltx_ref">2013</a>)</cite> conduct a thorough study on the types of questions people with visual impairments would like answered, and provide a taxonomy for the types of questions asked and the features of such questions. This work was a significant step in understanding the need in people with visual impairments for VQA systems. In combination with our own work, this gives a more complete picture of what kinds of questions not only contribute to better model performance, but actually help individuals with visual impairments. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Zeng et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> seek to understand the task of answering questions about images from people with visual impairments (i.e., VizWiz) and those from sighted people (i.e., VQA-v2).
The authors identified the common vision skills needed for both scenarios and quantified the difficulty of these skills for both humans and computers on both datasets.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Gurari et al. (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, who published a very first visual question answering (VQA) dataset, “VizWiz” containing images and questions from people with visual impairments, pointed out the artificial setting of other VQA datasets that include questions that are artificially created by sighted people.
The VizWiz challenge is based on real-world data and directs researchers working on VQA problems toward real-world VQA problems. This dataset was built on data collected with a crowdsourcing app, where users with visual impairments share an image and a question with a sighted crowdworker who answers the question for them <cite class="ltx_cite ltx_citemacro_cite">Bigham et al. (<a href="#bib.bib5" title="" class="ltx_ref">2010</a>)</cite>.
Other existing datasets, such as VQA <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite>, DAQUAR <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite>, and OK-VQA <cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>, are different in that their questions were not provided by those who took images. Instead, the images were first extracted from web searches, and then questions were later provided by sighted crowdworkers who viewed and imagined questions to ask about those images.
Here, we see that people with visual impairments can benefit the most from VQA technology but most of the existing VQA datasets do not involve people with visual impairments.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Some prior work has investigated VQA datasets further, focusing on assessing diversity in answers to visual questions. For instance, <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> looked at answers to visual questions created by blind people and sighted people and worked on anticipating the distribution of such answers. Predicting the distribution of answers asked, they helped crowdworkers create as many unique answers as possible for answer diversity.
<cite class="ltx_cite ltx_citemacro_citet">Bhattacharya et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> tackle the same issue by looking at images of VQA. They proposed a taxonomy of nine reasons that cause differences in answers and developed a model predicting potential reasons that can lead to differences in answers.
However, little work explores discrepancies between questions from actual users of VQA applications (i.e., users with visual impairments) and contributors who helped develop data for VQA applications.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Our work aims to understand this gap by assessing the discrepancies between the dataset containing artificially created data and the dataset containing real-world application data present across different VQA models. More specifically, we assess the performance of VQA models that were proposed in different times and delve into the old model and the state-of-the-art model with individual datapoints to identify patterns where the models perform poorly for the accessibility dataset.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment Setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To evaluate how existing VQA models’ performance on machine understanding dataset align with performances on the accessibility dataset, we select two VQA datasets and seven VQA models.
One of the datasets, VQA-v2, was proposed for machine understanding, whereas the other dataset, VizWiz, was collected to improve accessibility for visually-impaired people.
The seven VQA models, selected from the VQA-v2 leaderboard<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://paperswithcode.com/sota/visual-question-answering-on-vqa-v2-test-dev" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://paperswithcode.com/sota/visual-question-answering-on-vqa-v2-test-dev</a></span></span></span>, include MFB <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2017</a>)</cite>, MFH <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite>, BAN <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, BUTD <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>, MCAN <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>, Pythia <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>, and ALBEF <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>.
We assess all seven models on both of the datasets to investigate and understand the model progress across the machine understanding and accessibility datasets<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Code is available at <a target="_blank" href="https://github.com/kyleseelman/vqa_accessibility" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kyleseelman/vqa_accessibility</a></span></span></span>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As a representative of machine understanding VQA, we take the <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">VQA-v2</span> dataset <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib1" title="" class="ltx_ref">2017</a>)</cite>, which includes around 204,000 images from the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib13" title="" class="ltx_ref">2014</a>)</cite> with around one million questions. The images are collected through Flickr by amateur photographers. Thus the images are from sighted people rather than visually-impaired people.
In addition, questions in VQA-v2 are collected in a post-hoc manner — given a image, sighted crowdworkers are asked to create potential questions that could be asked for the image.
Finally, given the image-question pairs, a new set of annotators are asked to answer the questions based on the image information. For each image-question pair, ten annotations are collected as ground-truth.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">As a representative of accessibility VQA, we take the <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">VizWiz</span> dataset <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, which includes around 32,000 images and question pairs from people with visual impairments.
This dataset was built on data collected with a crowdsourcing-based app <cite class="ltx_cite ltx_citemacro_cite">Bigham et al. (<a href="#bib.bib5" title="" class="ltx_ref">2010</a>)</cite> where users with visual impairments ask questions by uploading an image with a recording of the spoken question.
The VizWiz dataset uses the image-question pairs from the data collected through the app and asks crowdworkers to annotate answers.
Similarly, ten ground-truth answers are provided for each image-question pair.
Note that in VizWiz each image-question pair is provided simultaneously by the same person, which is different from how the VQA-v2 dataset was curated.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Our evaluation also uses a smaller subset of VQA-v2’s training set, which we call <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">VQA-v2-sm</span>, limited in size to match that of VizWiz’s training set. This dataset is created to evaluate the effects of dataset size in VQA models’ performance.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluation Metric</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We evaluate the seven models on the VQA-v2 and the VizWiz datasets with the standard “accuracy” evaluation metric for VQA.
Since different annotators may provide different but valid answers, the metric does not penalize for the predicted answer not matching all the ground truth answers.
For each question, given the ten ground-truth from human annotators, we compute the model answer accuracy as in <a href="#S3.E1" title="1 ‣ 3.2 Evaluation Metric ‣ 3 Experiment Setup ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Eq missing<span class="ltx_text"></span> 1</span></a>.
If the model accurately predicts an answer that matches at least three ground-truth answers, it receives a maximal score of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="float" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">1.0</annotation></semantics></math>. Otherwise, the accuracy score is the number of ground-truth answers matched, divided by three:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\textrm{accuracy}=\min\left\{1,\frac{\textrm{\# matches}}{3}\right\}" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mtext id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2a.cmml">accuracy</mtext><mo id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">min</mi><mo id="S3.E1.m1.3.4.3.2a" xref="S3.E1.m1.3.4.3.1.cmml">⁡</mo><mrow id="S3.E1.m1.3.4.3.2.1" xref="S3.E1.m1.3.4.3.1.cmml"><mo id="S3.E1.m1.3.4.3.2.1.1" xref="S3.E1.m1.3.4.3.1.cmml">{</mo><mn id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">1</mn><mo id="S3.E1.m1.3.4.3.2.1.2" xref="S3.E1.m1.3.4.3.1.cmml">,</mo><mfrac id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mtext id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2a.cmml"># matches</mtext><mn id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml">3</mn></mfrac><mo id="S3.E1.m1.3.4.3.2.1.3" xref="S3.E1.m1.3.4.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><ci id="S3.E1.m1.3.4.2a.cmml" xref="S3.E1.m1.3.4.2"><mtext id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2">accuracy</mtext></ci><apply id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.2"><min id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"></min><cn type="integer" id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">1</cn><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><divide id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3"></divide><ci id="S3.E1.m1.3.3.2a.cmml" xref="S3.E1.m1.3.3.2"><mtext id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"># matches</mtext></ci><cn type="integer" id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\textrm{accuracy}=\min\left\{1,\frac{\textrm{\# matches}}{3}\right\}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2210.14966/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model accuracy on VQA-v2 (including a sm(aller) subsampled version), and VizWiz (including a fine-tuned variant). The models are ordered by the time they were proposed. Improvements on VQA-v2 <em id="S3.F2.2.1" class="ltx_emph ltx_font_italic">have</em> resulted in improvements on VizWiz, though the gap between the two remains significant.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Models</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">All of the following models approach the problem as a classification task by aggregating possible answers from the training and validation dataset as the answer space.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<dl id="S3.I1" class="ltx_description">
<dt id="S3.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">MFB &amp; MFH:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p">The multi-modal factorized bilinear &amp; multi-modal factorized high-order pooling models <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib22" title="" class="ltx_ref">2017</a>, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> are built upon the multi-modal factorized bilinear pooling that combines image features and text features as well as a co-attention module that jointly learns to generate attention maps from these multi-modal features. The MFB model is a simplified version of the MFH model.</p>
</div>
</dd>
<dt id="S3.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">BUTD:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p">The bottom-up and top-down attention model <cite class="ltx_cite ltx_citemacro_cite">Anderson et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> goes beyond top-down attention mechanism and proposes the addition of a bottom-ups attention that finds image regions, each with an associated feature vector, thus, creating a bottom-up and top-down approach that can calculate at the level of objects and other salient image regions.</p>
</div>
</dd>
<dt id="S3.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">BAN:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p">The bilinear attention network model <cite class="ltx_cite ltx_citemacro_cite">Kim et al. (<a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite> utilizes bilinear attention distributions to represent given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels.</p>
</div>
</dd>
<dt id="S3.I1.ix4" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix4.1.1.1" class="ltx_text ltx_font_bold">Pythia:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p">Pythia is an extension of the BUTD model, utilizing both data augmentation and ensembling to significantly improve VQA performance <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</dd>
<dt id="S3.I1.ix5" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix5.1.1.1" class="ltx_text ltx_font_bold">MCAN:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix5.p1" class="ltx_para">
<p id="S3.I1.ix5.p1.1" class="ltx_p">The modular co-attention network model <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> follows the co-attention approach of the previously mentioned models, but cascades modular co-attention layers at depth, to create an effective deep co-attention model where each MCA layer models the self-attention of questions and images.</p>
</div>
</dd>
<dt id="S3.I1.ix6" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S3.I1.ix6.1.1.1" class="ltx_text ltx_font_bold">ALBEF:</span></span></dt>
<dd class="ltx_item">
<div id="S3.I1.ix6.p1" class="ltx_para">
<p id="S3.I1.ix6.p1.1" class="ltx_p">The align before fusing model <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite> builds upon existing methods that employ a transformer-based multimodal encoder to jointly model visual tokens and word tokens, by aligning the image and text representations and fusing them through cross-model attention.</p>
</div>
</dd>
</dl>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.2" class="ltx_p">For all the models, the answer space of the VQA-v2 dataset is <math id="S3.SS3.p3.1.m1.2" class="ltx_Math" alttext="3,129" display="inline"><semantics id="S3.SS3.p3.1.m1.2a"><mrow id="S3.SS3.p3.1.m1.2.3.2" xref="S3.SS3.p3.1.m1.2.3.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">3</mn><mo id="S3.SS3.p3.1.m1.2.3.2.1" xref="S3.SS3.p3.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS3.p3.1.m1.2.2" xref="S3.SS3.p3.1.m1.2.2.cmml">129</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.2b"><list id="S3.SS3.p3.1.m1.2.3.1.cmml" xref="S3.SS3.p3.1.m1.2.3.2"><cn type="integer" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">3</cn><cn type="integer" id="S3.SS3.p3.1.m1.2.2.cmml" xref="S3.SS3.p3.1.m1.2.2">129</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.2c">3,129</annotation></semantics></math>, while the answer space of the VizWiz dataset is <math id="S3.SS3.p3.2.m2.2" class="ltx_Math" alttext="7,371" display="inline"><semantics id="S3.SS3.p3.2.m2.2a"><mrow id="S3.SS3.p3.2.m2.2.3.2" xref="S3.SS3.p3.2.m2.2.3.1.cmml"><mn id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">7</mn><mo id="S3.SS3.p3.2.m2.2.3.2.1" xref="S3.SS3.p3.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS3.p3.2.m2.2.2" xref="S3.SS3.p3.2.m2.2.2.cmml">371</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.2b"><list id="S3.SS3.p3.2.m2.2.3.1.cmml" xref="S3.SS3.p3.2.m2.2.3.2"><cn type="integer" id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">7</cn><cn type="integer" id="S3.SS3.p3.2.m2.2.2.cmml" xref="S3.SS3.p3.2.m2.2.2">371</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.2c">7,371</annotation></semantics></math>, which is provided by Pythia <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Implementation details.</h5>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS3.SSS0.Px1.p1.1" class="ltx_p">We use three different code bases for our evaluation: OpenVQA<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/MILVLG/openvqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/openvqa</a></span></span></span>, Pythia<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/allenai/pythia" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/allenai/pythia</a></span></span></span>, and ALBEF<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/salesforce/ALBEF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/salesforce/ALBEF</a>.</span></span></span>.
On the OpenVQA platform, four VQA models—MFB, BAN, BUTD, and MCAN—are already implemented.
Pythia supports both of the VQA-v2 and Vizwiz datasets, but OpenVQA and ALBEF only support the VQA-v2 dataset. Thus, we implement the support of the VizWiz dataset on OpenVQA (i.e., for MFB, BAN, BUTD, and MCAN) and ALBEF.
Their default hyperparameters are used to train models on VQA-v2 and VizWiz, respectively.
For OpenVQA and ALBEF on which we implement the VizWiz support, the default hyperparameters for VQA-v2 are used to train models on VizWiz as well.
We fix the default accuracy metric implemented in OpenVQA, which is silently incompatible with the VizWiz data format, consistently underscoring predictions.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Findings and Discussion</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2210.14966/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="164" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Model accuracy progress from MFB in 2017 (left point) to ALBEF in 2021 (right point) represented as lines measuring average model accuracy (left y-axis); these are subdivided by challenge classes from <cite class="ltx_cite ltx_citemacro_citet">Zeng et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite> (the orange lines in <span id="S4.F3.5.1" class="ltx_text ltx_font_italic">ObjRec</span>–<span id="S4.F3.6.2" class="ltx_text ltx_font_italic">Count</span>) and from <cite class="ltx_cite ltx_citemacro_citet">Bhattacharya et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite> (the blue lines in <span id="S4.F3.7.3" class="ltx_text ltx_font_italic">GranA</span>–<span id="S4.F3.8.4" class="ltx_text ltx_font_italic">InadA</span>) for the VizWiz dataset. The bars represent the percentage of validation data examples that belong to the challenge classes (right y-axis). </figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Our objective in this section is to investigate challenges of the VQA task on two different datasets. We assess the performance progress of VQA models and delve into errors. Then, we discuss research directions that future work could take.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Model Performance Progress</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">First, we examine whether the progress of VQA model architectures on the machine understanding dataset (VQA-v2) also apply to the accessibility dataset (VizWiz).
For VizWiz, we report testing results on both trained from scratch with VizWiz (<span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">VizWiz</span>) and trained on VQA-v2 and finetuned with VizWiz (<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">VizWiz-ft</span>).
As mentioned in Section <a href="#S3.SS1" title="3.1 Datasets ‣ 3 Experiment Setup ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we randomly sampled the same number of datapoints from the train set of VQA-v2 as that in VizWiz to form <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">VQA-v2-sm</span> to understand the effect of dataset size in the VQA performance.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.8" class="ltx_p">The results are shown in <a href="#S3.F2" title="Figure 2 ‣ 3.2 Evaluation Metric ‣ 3 Experiment Setup ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We do not include results of Pythia trained from scratch on VizWiz because their code expected to train VizWiz from a VQAv2.0 checkpoint, not from scratch. </span></span></span>
Overall, we observe that along with the advancement of model structures based on the VQA-v2 dataset, the model accuracy also improves on the VizWiz dataset.
We observe that, from 2018 through 2021, performance on VQA-v2 improved <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">10</mn><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">10\%</annotation></semantics></math> relatively (from <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="65.2\%" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">65.2</mn><mo id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">65.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">65.2\%</annotation></semantics></math> to <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="71.6\%" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mn id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">71.6</mn><mo id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">71.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">71.6\%</annotation></semantics></math> accuracy), resulting in a similar improvement of <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><mrow id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mn id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">11</mn><mo id="S4.SS1.p2.4.m4.1.1.1" xref="S4.SS1.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">11\%</annotation></semantics></math> (<math id="S4.SS1.p2.5.m5.1" class="ltx_Math" alttext="43.8\%" display="inline"><semantics id="S4.SS1.p2.5.m5.1a"><mrow id="S4.SS1.p2.5.m5.1.1" xref="S4.SS1.p2.5.m5.1.1.cmml"><mn id="S4.SS1.p2.5.m5.1.1.2" xref="S4.SS1.p2.5.m5.1.1.2.cmml">43.8</mn><mo id="S4.SS1.p2.5.m5.1.1.1" xref="S4.SS1.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.5.m5.1b"><apply id="S4.SS1.p2.5.m5.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.p2.5.m5.1.1.1.cmml" xref="S4.SS1.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p2.5.m5.1.1.2.cmml" xref="S4.SS1.p2.5.m5.1.1.2">43.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.5.m5.1c">43.8\%</annotation></semantics></math> to <math id="S4.SS1.p2.6.m6.1" class="ltx_Math" alttext="48.8\%" display="inline"><semantics id="S4.SS1.p2.6.m6.1a"><mrow id="S4.SS1.p2.6.m6.1.1" xref="S4.SS1.p2.6.m6.1.1.cmml"><mn id="S4.SS1.p2.6.m6.1.1.2" xref="S4.SS1.p2.6.m6.1.1.2.cmml">48.8</mn><mo id="S4.SS1.p2.6.m6.1.1.1" xref="S4.SS1.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.6.m6.1b"><apply id="S4.SS1.p2.6.m6.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1"><csymbol cd="latexml" id="S4.SS1.p2.6.m6.1.1.1.cmml" xref="S4.SS1.p2.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p2.6.m6.1.1.2.cmml" xref="S4.SS1.p2.6.m6.1.1.2">48.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.6.m6.1c">48.8\%</annotation></semantics></math>) on VizWiz without fine-tuning and 30% (<math id="S4.SS1.p2.7.m7.1" class="ltx_Math" alttext="39\%" display="inline"><semantics id="S4.SS1.p2.7.m7.1a"><mrow id="S4.SS1.p2.7.m7.1.1" xref="S4.SS1.p2.7.m7.1.1.cmml"><mn id="S4.SS1.p2.7.m7.1.1.2" xref="S4.SS1.p2.7.m7.1.1.2.cmml">39</mn><mo id="S4.SS1.p2.7.m7.1.1.1" xref="S4.SS1.p2.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.7.m7.1b"><apply id="S4.SS1.p2.7.m7.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1"><csymbol cd="latexml" id="S4.SS1.p2.7.m7.1.1.1.cmml" xref="S4.SS1.p2.7.m7.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p2.7.m7.1.1.2.cmml" xref="S4.SS1.p2.7.m7.1.1.2">39</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.7.m7.1c">39\%</annotation></semantics></math> to <math id="S4.SS1.p2.8.m8.1" class="ltx_Math" alttext="50.8\%" display="inline"><semantics id="S4.SS1.p2.8.m8.1a"><mrow id="S4.SS1.p2.8.m8.1.1" xref="S4.SS1.p2.8.m8.1.1.cmml"><mn id="S4.SS1.p2.8.m8.1.1.2" xref="S4.SS1.p2.8.m8.1.1.2.cmml">50.8</mn><mo id="S4.SS1.p2.8.m8.1.1.1" xref="S4.SS1.p2.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.8.m8.1b"><apply id="S4.SS1.p2.8.m8.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1"><csymbol cd="latexml" id="S4.SS1.p2.8.m8.1.1.1.cmml" xref="S4.SS1.p2.8.m8.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p2.8.m8.1.1.2.cmml" xref="S4.SS1.p2.8.m8.1.1.2">50.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.8.m8.1c">50.8\%</annotation></semantics></math>) on VizWiz with fine-tuning. The models fine-tuned from VQA-2 to VizWiz (i.e., VizWiz-ft) have similar performance with models trained on VizWiz from scratch. <cite class="ltx_cite ltx_citemacro_citet">Gurari et al. (<a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite> also reported a similar pattern but pointed out the gap between model performance and human performance.
These results show that improvements on VQA-v2 <span id="S4.SS1.p2.8.1" class="ltx_text ltx_font_italic">have</span> translated into improvements on VizWiz, whereas the performance gap between the two datasets are still significant.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.6" class="ltx_p">However, when controlling for dataset size, we see an relative improvement of <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="42\%" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mn id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">42</mn><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">42\%</annotation></semantics></math> (<math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="43.8\%" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><mn id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml">43.8</mn><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2">43.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">43.8\%</annotation></semantics></math> to <math id="S4.SS1.p3.3.m3.1" class="ltx_Math" alttext="62.4\%" display="inline"><semantics id="S4.SS1.p3.3.m3.1a"><mrow id="S4.SS1.p3.3.m3.1.1" xref="S4.SS1.p3.3.m3.1.1.cmml"><mn id="S4.SS1.p3.3.m3.1.1.2" xref="S4.SS1.p3.3.m3.1.1.2.cmml">62.4</mn><mo id="S4.SS1.p3.3.m3.1.1.1" xref="S4.SS1.p3.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.3.m3.1b"><apply id="S4.SS1.p3.3.m3.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1"><csymbol cd="latexml" id="S4.SS1.p3.3.m3.1.1.1.cmml" xref="S4.SS1.p3.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.3.m3.1.1.2.cmml" xref="S4.SS1.p3.3.m3.1.1.2">62.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.3.m3.1c">62.4\%</annotation></semantics></math>) on VQA-v2-sm, where the training data is capped at the size of VizWiz, a substantially larger improvement than the <math id="S4.SS1.p3.4.m4.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S4.SS1.p3.4.m4.1a"><mrow id="S4.SS1.p3.4.m4.1.1" xref="S4.SS1.p3.4.m4.1.1.cmml"><mn id="S4.SS1.p3.4.m4.1.1.2" xref="S4.SS1.p3.4.m4.1.1.2.cmml">11</mn><mo id="S4.SS1.p3.4.m4.1.1.1" xref="S4.SS1.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.4.m4.1b"><apply id="S4.SS1.p3.4.m4.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1"><csymbol cd="latexml" id="S4.SS1.p3.4.m4.1.1.1.cmml" xref="S4.SS1.p3.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS1.p3.4.m4.1.1.2.cmml" xref="S4.SS1.p3.4.m4.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.4.m4.1c">11\%</annotation></semantics></math> seen on VizWiz (the result on VizWiz with fine-tuning is not comparable here, because it is fine-tuned from the full VQA-v2 dataset).
This appears to demonstrate an “overfitting” effect, as both VQA-v2-sm and VizWiz start at almost exactly the same accuracy (<math id="S4.SS1.p3.5.m5.1" class="ltx_Math" alttext="43.8\%" display="inline"><semantics id="S4.SS1.p3.5.m5.1a"><mrow id="S4.SS1.p3.5.m5.1.1" xref="S4.SS1.p3.5.m5.1.1.cmml"><mn id="S4.SS1.p3.5.m5.1.1.2" xref="S4.SS1.p3.5.m5.1.1.2.cmml">43.8</mn><mo id="S4.SS1.p3.5.m5.1.1.1" xref="S4.SS1.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.5.m5.1b"><apply id="S4.SS1.p3.5.m5.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1"><csymbol cd="latexml" id="S4.SS1.p3.5.m5.1.1.1.cmml" xref="S4.SS1.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.5.m5.1.1.2.cmml" xref="S4.SS1.p3.5.m5.1.1.2">43.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.5.m5.1c">43.8\%</annotation></semantics></math> and <math id="S4.SS1.p3.6.m6.1" class="ltx_Math" alttext="43.2\%" display="inline"><semantics id="S4.SS1.p3.6.m6.1a"><mrow id="S4.SS1.p3.6.m6.1.1" xref="S4.SS1.p3.6.m6.1.1.cmml"><mn id="S4.SS1.p3.6.m6.1.1.2" xref="S4.SS1.p3.6.m6.1.1.2.cmml">43.2</mn><mo id="S4.SS1.p3.6.m6.1.1.1" xref="S4.SS1.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.6.m6.1b"><apply id="S4.SS1.p3.6.m6.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1"><csymbol cd="latexml" id="S4.SS1.p3.6.m6.1.1.1.cmml" xref="S4.SS1.p3.6.m6.1.1.1">percent</csymbol><cn type="float" id="S4.SS1.p3.6.m6.1.1.2.cmml" xref="S4.SS1.p3.6.m6.1.1.2">43.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.6.m6.1c">43.2\%</annotation></semantics></math>) but performance on VQA-v2-sm improves significantly more than on VizWiz.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Error Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We perform both quantitative and qualitative error analysis to better understand which types of data will be useful to improve accessibility VQA for future dataset collection and model improvement. In this section we discuss the overall patterns found for models evaluated on VizWiz and what type of questions specifically, these model fail on.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>VQA Challenge Datasets</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.3" class="ltx_p">In our first set of experiments, we aim to understand more precisely what that models have improved on between 2017 and 2021 that has led to an overall accuracy improvement on VizWiz-ft from <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="39.0\%" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mrow id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">39.0</mn><mo id="S4.SS2.SSS1.p1.1.m1.1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">39.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">39.0\%</annotation></semantics></math> (MFB) to <math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="50.8\%" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mrow id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS1.p1.2.m2.1.1.2" xref="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml">50.8</mn><mo id="S4.SS2.SSS1.p1.2.m2.1.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><apply id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1.2">50.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">50.8\%</annotation></semantics></math> (ALBEF).
To do this, we make use of two meta-data annotations of a subset of the VizWiz validation dataset (<math id="S4.SS2.SSS1.p1.3.m3.2" class="ltx_Math" alttext="3,143" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.2a"><mrow id="S4.SS2.SSS1.p1.3.m3.2.3.2" xref="S4.SS2.SSS1.p1.3.m3.2.3.1.cmml"><mn id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml">3</mn><mo id="S4.SS2.SSS1.p1.3.m3.2.3.2.1" xref="S4.SS2.SSS1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S4.SS2.SSS1.p1.3.m3.2.2" xref="S4.SS2.SSS1.p1.3.m3.2.2.cmml">143</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.2b"><list id="S4.SS2.SSS1.p1.3.m3.2.3.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.2.3.2"><cn type="integer" id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">3</cn><cn type="integer" id="S4.SS2.SSS1.p1.3.m3.2.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.2.2">143</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.2c">3,143</annotation></semantics></math> data examples): one labels each example with the <span id="S4.SS2.SSS1.p1.3.1" class="ltx_text ltx_font_italic">vision skills</span> required to answer that question <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>, the second labels each with aspects of the <span id="S4.SS2.SSS1.p1.3.2" class="ltx_text ltx_font_italic">image-question</span> combination that are challenging <cite class="ltx_cite ltx_citemacro_cite">Bhattacharya et al. (<a href="#bib.bib4" title="" class="ltx_ref">2019</a>)</cite>. Both of these papers investigate the challenges for <span id="S4.SS2.SSS1.p1.3.3" class="ltx_text ltx_font_italic">annotators</span>; here, we use these annotations to evaluate models.
<a href="#S4.T1" title="Table 1 ‣ 4.2.1 VQA Challenge Datasets ‣ 4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a> shows the taxonomies of VizWiz validation examples that are labeled with the challenge class according to majority vote over five annotations.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2210.14966/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="109" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The performance improvements from MFB to ALBEF on the VizWiz dataset with respect to the reduced number of data examples with <math id="S4.F4.5.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.F4.5.m1.1b"><mn id="S4.F4.5.m1.1.1" xref="S4.F4.5.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F4.5.m1.1c"><cn type="integer" id="S4.F4.5.m1.1.1.cmml" xref="S4.F4.5.m1.1.1">0</cn></annotation-xml></semantics></math> accuracy score. Red color represents the number of data examples ALBEF got <math id="S4.F4.6.m2.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.F4.6.m2.1b"><mn id="S4.F4.6.m2.1.1" xref="S4.F4.6.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F4.6.m2.1c"><cn type="integer" id="S4.F4.6.m2.1.1.cmml" xref="S4.F4.6.m2.1.1">0</cn></annotation-xml></semantics></math> score on while red plus blue color represents the number of data examples MFB got <math id="S4.F4.7.m3.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.F4.7.m3.1b"><mn id="S4.F4.7.m3.1.1" xref="S4.F4.7.m3.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.F4.7.m3.1c"><cn type="integer" id="S4.F4.7.m3.1.1.cmml" xref="S4.F4.7.m3.1.1">0</cn></annotation-xml></semantics></math> score on – blue color thus represent the number of data examples improved by ALBEF from MFB. Note that we combine the challenge classes that has less than <math id="S4.F4.8.m4.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.F4.8.m4.1b"><mn id="S4.F4.8.m4.1.1" xref="S4.F4.8.m4.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.F4.8.m4.1c"><cn type="integer" id="S4.F4.8.m4.1.1.cmml" xref="S4.F4.8.m4.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.8.m4.1d">50</annotation></semantics></math> data examples as “Other”. </figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<td id="S4.T1.1.2.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt"><span id="S4.T1.1.2.1.2.1" class="ltx_text ltx_font_bold">Label</span></td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.2.1.3.1.1" class="ltx_p" style="width:148.0pt;"><span id="S4.T1.1.2.1.3.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span>
</span>
</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T1.1.3.2.1.1" class="ltx_text">
<span id="S4.T1.1.3.2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:27.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:27.6pt;transform:translate(-10.36pt,-10.36pt) rotate(-90deg) ;">
<span id="S4.T1.1.3.2.1.1.1.1" class="ltx_p"><span id="S4.T1.1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Vision</span></span>
</span></span></span></td>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">ObjRec</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.3.2.3.1.1" class="ltx_p" style="width:148.0pt;">object recognition</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<td id="S4.T1.1.4.3.1" class="ltx_td ltx_nopad_l ltx_align_center">TextRec</td>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.4.3.2.1.1" class="ltx_p" style="width:148.0pt;">text recognition</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<td id="S4.T1.1.5.4.1" class="ltx_td ltx_nopad_l ltx_align_center">ColRec</td>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.5.4.2.1.1" class="ltx_p" style="width:148.0pt;">color recognition</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<td id="S4.T1.1.6.5.1" class="ltx_td ltx_nopad_l ltx_align_center">Count</td>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.6.5.2.1.1" class="ltx_p" style="width:148.0pt;">counting</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<td id="S4.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="9"><span id="S4.T1.1.7.6.1.1" class="ltx_text">
<span id="S4.T1.1.7.6.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.8pt;height:68.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:68.7pt;transform:translate(-29.94pt,-28.97pt) rotate(-90deg) ;">
<span id="S4.T1.1.7.6.1.1.1.1" class="ltx_p"><span id="S4.T1.1.7.6.1.1.1.1.1" class="ltx_text ltx_font_bold">Image-Question</span></span>
</span></span></span></td>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t">GranA</td>
<td id="S4.T1.1.7.6.3" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.7.6.3.1.1" class="ltx_p" style="width:148.0pt;">answers at different granularities</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center">AmbQ</td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.1.1.1.1" class="ltx_p" style="width:148.0pt;">ambiguous qs w/ <math id="S4.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="&gt;1" display="inline"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo id="S4.T1.1.1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.1.cmml">&gt;</mo><mn id="S4.T1.1.1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1"><gt id="S4.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.T1.1.1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S4.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">&gt;1</annotation></semantics></math> valid answer</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.8.7" class="ltx_tr">
<td id="S4.T1.1.8.7.1" class="ltx_td ltx_nopad_l ltx_align_center">SynonA</td>
<td id="S4.T1.1.8.7.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.8.7.2.1.1" class="ltx_p" style="width:148.0pt;">different wordings of same answer</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.9.8" class="ltx_tr">
<td id="S4.T1.1.9.8.1" class="ltx_td ltx_nopad_l ltx_align_center">MissA</td>
<td id="S4.T1.1.9.8.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.9.8.2.1.1" class="ltx_p" style="width:148.0pt;">answer not present given image</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.10.9" class="ltx_tr">
<td id="S4.T1.1.10.9.1" class="ltx_td ltx_nopad_l ltx_align_center">LoQual</td>
<td id="S4.T1.1.10.9.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.10.9.2.1.1" class="ltx_p" style="width:148.0pt;">low quality image</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.11.10" class="ltx_tr">
<td id="S4.T1.1.11.10.1" class="ltx_td ltx_nopad_l ltx_align_center">InvalQ</td>
<td id="S4.T1.1.11.10.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.11.10.2.1.1" class="ltx_p" style="width:148.0pt;">invalid question</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.12.11" class="ltx_tr">
<td id="S4.T1.1.12.11.1" class="ltx_td ltx_nopad_l ltx_align_center">HardQ</td>
<td id="S4.T1.1.12.11.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.12.11.2.1.1" class="ltx_p" style="width:148.0pt;">hard question requiring expertise</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.13.12" class="ltx_tr">
<td id="S4.T1.1.13.12.1" class="ltx_td ltx_nopad_l ltx_align_center">SubjQ</td>
<td id="S4.T1.1.13.12.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top">
<span id="S4.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.13.12.2.1.1" class="ltx_p" style="width:148.0pt;">subjective question</span>
</span>
</td>
</tr>
<tr id="S4.T1.1.14.13" class="ltx_tr">
<td id="S4.T1.1.14.13.1" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb">InadA</td>
<td id="S4.T1.1.14.13.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.1.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.1.14.13.2.1.1" class="ltx_p" style="width:148.0pt;">inadequate answers</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>VQA challenge taxonomies with labels. </figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Given this taxonomy, we assess the performance progress between MFB and ALBEF in the VizWiz-ft setting across each VQA challenge class.
The results are reported in <a href="#S4.F3" title="Figure 3 ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.
Compared to MFB, ALBEF improves on every class of challenges except HardQ—hard questions that may require domain expertise, special skills, or too much effort to answer—though HardQ is also one of the rarest categories. (It is somewhat surprising the high performance of the models on these “hard” questions.)
We observe that among the <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">vision skill</span> challenge classes, the models struggle the most on recognizing texts.
Among the <span id="S4.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_italic">image-question</span> challenges, models have low accuracy on almost all the challenge classes related to the answers — ground-truth answers with different granularities, wordings, and inadequate answers.
This indicates a potential problem in evaluating models on the VizWiz dataset, which is further explored in our qualitative analysis in <a href="#S4.SS2.SSS2" title="4.2.2 Where the Models Fail ‣ 4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 4.2.2</span></a>.
For the questions, models struggle the most with handling ambiguous or subjective questions, which we will discuss more in the next section.
Overall, the results point out the challenges that models have most difficulty on, which we hope can bring insights for future work to improve accessibility VQA systems.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Where the Models Fail</h4>

<figure id="S4.F5" class="ltx_figure"><img src="/html/2210.14966/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="373" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of low-performance ALBEF image/question pairs, that should be correct, together with the accuracy scores. (Left) ALBEF gives a more detailed answer of <span id="S4.F5.3.1" class="ltx_text ltx_font_italic">Iris</span>, but since most annotators put <span id="S4.F5.4.2" class="ltx_text ltx_font_italic">flowers</span>, performance score is low. (Middle) ALBEF correctly names the beer, but once again does not match the annotators, so MFB appears to perform better. (Right) ALBEF gives a number answer that is close to correct (and which is not much different from the set of ground truth answers), where MFB does not make an attempt. </figcaption>
</figure>
<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.2" class="ltx_p">To further understand the data examples that the models fine-tuned on VizWiz perform poorly on, we manually investigate the validation examples on which models achieve <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="0\%" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">0</mn><mo id="S4.SS2.SSS2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">0\%</annotation></semantics></math> accuracy: matching none of the ten human-provided answers.
We measure how many data examples that have <math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="0\%" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">0</mn><mo id="S4.SS2.SSS2.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">0\%</annotation></semantics></math> accuracy on MFB got improved by the ALBEF model for each challenge class, shown in <a href="#S4.F4" title="Figure 4 ‣ 4.2.1 VQA Challenge Datasets ‣ 4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.3" class="ltx_p">Model improvement is greatest on color recognition (<math id="S4.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="63\%" display="inline"><semantics id="S4.SS2.SSS2.p2.1.m1.1a"><mrow id="S4.SS2.SSS2.p2.1.m1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.p2.1.m1.1.1.2" xref="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml">63</mn><mo id="S4.SS2.SSS2.p2.1.m1.1.1.1" xref="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.1.m1.1b"><apply id="S4.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p2.1.m1.1.1.2">63</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.1.m1.1c">63\%</annotation></semantics></math>) and least on text recognition (<math id="S4.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="34\%" display="inline"><semantics id="S4.SS2.SSS2.p2.2.m2.1a"><mrow id="S4.SS2.SSS2.p2.2.m2.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.p2.2.m2.1.1.2" xref="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml">34</mn><mo id="S4.SS2.SSS2.p2.2.m2.1.1.1" xref="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.2.m2.1b"><apply id="S4.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p2.2.m2.1.1.2">34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.2.m2.1c">34\%</annotation></semantics></math>).
Meanwhile, object recognition, text recognition, color recognition, and ambiguous questions are the challenge classes which a current state-of-the-art model has the most difficulty.
When taking a closer look at the individual examples that ALBEF has <math id="S4.SS2.SSS2.p2.3.m3.1" class="ltx_Math" alttext="0\%" display="inline"><semantics id="S4.SS2.SSS2.p2.3.m3.1a"><mrow id="S4.SS2.SSS2.p2.3.m3.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.cmml"><mn id="S4.SS2.SSS2.p2.3.m3.1.1.2" xref="S4.SS2.SSS2.p2.3.m3.1.1.2.cmml">0</mn><mo id="S4.SS2.SSS2.p2.3.m3.1.1.1" xref="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p2.3.m3.1b"><apply id="S4.SS2.SSS2.p2.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.p2.3.m3.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p2.3.m3.1c">0\%</annotation></semantics></math> accuracy on, it turns out the issue is often with the <span id="S4.SS2.SSS2.p2.3.1" class="ltx_text ltx_font_italic">evaluation measure</span> and not with the ALBEF model itself. The most frequent issues are:</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Answerable Questions Marked Unanswerable.</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.3" class="ltx_p">The biggest difference (and what we deem an improvement) between ALBEF and MFB has to do with “unanswerable” questions. <math id="S4.SS2.SSS2.Px1.p1.1.m1.1" class="ltx_Math" alttext="27\%" display="inline"><semantics id="S4.SS2.SSS2.Px1.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.2.cmml">27</mn><mo id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.1.m1.1b"><apply id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.2">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.1.m1.1c">27\%</annotation></semantics></math> of the questions in the validation data are deemed “unanswerable” by at least three annotators—making “unanswerable” a prediction that would achieve perfect accuracy.
For 56% of the questions that were not of type <span id="S4.SS2.SSS2.Px1.p1.3.1" class="ltx_text ltx_font_italic">“unanswerable”</span>, MFB still answered <span id="S4.SS2.SSS2.Px1.p1.3.2" class="ltx_text ltx_font_italic">“unanswerable”</span>, while ALBEF did this only 30% of the time. This skew helps MFB on the evaluation metric, but ALBEF’s answers for many of these questions are at least as good—and therefore useful to a user—as saying “unanswerable.” For example, the <span id="S4.SS2.SSS2.Px1.p1.3.3" class="ltx_text ltx_font_italic">number</span> question type, MFB only answered with a number <math id="S4.SS2.SSS2.Px1.p1.2.m2.1" class="ltx_Math" alttext="2.2\%" display="inline"><semantics id="S4.SS2.SSS2.Px1.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.Px1.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1.2.cmml">2.2</mn><mo id="S4.SS2.SSS2.Px1.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.2.m2.1b"><apply id="S4.SS2.SSS2.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS2.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1.2">2.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.2.m2.1c">2.2\%</annotation></semantics></math> of the time, whereas ALBEF answered with a number <math id="S4.SS2.SSS2.Px1.p1.3.m3.1" class="ltx_Math" alttext="56\%" display="inline"><semantics id="S4.SS2.SSS2.Px1.p1.3.m3.1a"><mrow id="S4.SS2.SSS2.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.cmml"><mn id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2.cmml">56</mn><mo id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.1" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.3.m3.1b"><apply id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2">56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.3.m3.1c">56\%</annotation></semantics></math> of the time and, in those cases, the answers are often very close to the correct answer (see <a href="#S4.F5" title="Figure 5 ‣ 4.2.2 Where the Models Fail ‣ 4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>).</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Overly Generic Ground Truth.</h5>

<div id="S4.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p1.3" class="ltx_p">It is often the case that ALBEF provides a correct answer that is simply more specific than that provided by the ground truth annotation. For example, a common question in VizWiz is <span id="S4.SS2.SSS2.Px2.p1.3.1" class="ltx_text ltx_font_italic">“What is this?”</span>. When comparing ALBEF and MFB models, by accuracy alone, ALBEF outperforms MFB in <math id="S4.SS2.SSS2.Px2.p1.1.m1.1" class="ltx_Math" alttext="28.8\%" display="inline"><semantics id="S4.SS2.SSS2.Px2.p1.1.m1.1a"><mrow id="S4.SS2.SSS2.Px2.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.SSS2.Px2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.Px2.p1.1.m1.1.1.2.cmml">28.8</mn><mo id="S4.SS2.SSS2.Px2.p1.1.m1.1.1.1" xref="S4.SS2.SSS2.Px2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.Px2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px2.p1.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.Px2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS2.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.Px2.p1.1.m1.1.1.2">28.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.p1.1.m1.1c">28.8\%</annotation></semantics></math> of such cases, and MFB outperforms ALBEF in <math id="S4.SS2.SSS2.Px2.p1.2.m2.1" class="ltx_Math" alttext="12.6\%" display="inline"><semantics id="S4.SS2.SSS2.Px2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.Px2.p1.2.m2.1.1" xref="S4.SS2.SSS2.Px2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.Px2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.Px2.p1.2.m2.1.1.2.cmml">12.6</mn><mo id="S4.SS2.SSS2.Px2.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.Px2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.Px2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px2.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.Px2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.SSS2.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.Px2.p1.2.m2.1.1.2">12.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.p1.2.m2.1c">12.6\%</annotation></semantics></math>. However, in the majority of these examples, ALBEF gives a correct, but more detailed response than the ground truth, thus earning it <math id="S4.SS2.SSS2.Px2.p1.3.m3.1" class="ltx_Math" alttext="0\%" display="inline"><semantics id="S4.SS2.SSS2.Px2.p1.3.m3.1a"><mrow id="S4.SS2.SSS2.Px2.p1.3.m3.1.1" xref="S4.SS2.SSS2.Px2.p1.3.m3.1.1.cmml"><mn id="S4.SS2.SSS2.Px2.p1.3.m3.1.1.2" xref="S4.SS2.SSS2.Px2.p1.3.m3.1.1.2.cmml">0</mn><mo id="S4.SS2.SSS2.Px2.p1.3.m3.1.1.1" xref="S4.SS2.SSS2.Px2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.p1.3.m3.1b"><apply id="S4.SS2.SSS2.Px2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.Px2.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.Px2.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.Px2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.Px2.p1.3.m3.1.1.2">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.p1.3.m3.1c">0\%</annotation></semantics></math> accuracy (for example see <a href="#S4.F5" title="Figure 5 ‣ 4.2.2 Where the Models Fail ‣ 4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>). So while, based on the annotation, ALBEF is wrong, the model is actually correctly answering the question and performs worse than the MFB model only 2.6% of the time.
Furthermore, we found that both MFB and ALBEF models are both challenged by <span id="S4.SS2.SSS2.Px2.p1.3.2" class="ltx_text ltx_font_italic">yes/no</span> question types, but that these questions were often subjective or ambiguous.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Annotator Disagreement.</h5>

<div id="S4.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px3.p1.3" class="ltx_p">Questions such as <span id="S4.SS2.SSS2.Px3.p1.3.1" class="ltx_text ltx_font_italic">“Is this cat cute?”</span> or <span id="S4.SS2.SSS2.Px3.p1.3.2" class="ltx_text ltx_font_italic">“Are these bad for me?”</span> arguably make for poor questions when evaluating model performance: highly subjective yes/no questions often have annotations where at least three annotators state <span id="S4.SS2.SSS2.Px3.p1.3.3" class="ltx_text ltx_font_italic">“yes”</span>, and at least three state <span id="S4.SS2.SSS2.Px3.p1.3.4" class="ltx_text ltx_font_italic">“no”</span>. Therefore, per the evaluation metric, either answer achieves an accuracy of <math id="S4.SS2.SSS2.Px3.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.SSS2.Px3.p1.1.m1.1a"><mn id="S4.SS2.SSS2.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px3.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px3.p1.1.m1.1b"><cn type="integer" id="S4.SS2.SSS2.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px3.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px3.p1.1.m1.1c">1</annotation></semantics></math>. For example, for the question <span id="S4.SS2.SSS2.Px3.p1.3.5" class="ltx_text ltx_font_italic">“Do these socks match?”</span> ALBEF had an accuracy score of <math id="S4.SS2.SSS2.Px3.p1.2.m2.1" class="ltx_Math" alttext="60\%" display="inline"><semantics id="S4.SS2.SSS2.Px3.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.Px3.p1.2.m2.1.1" xref="S4.SS2.SSS2.Px3.p1.2.m2.1.1.cmml"><mn id="S4.SS2.SSS2.Px3.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.Px3.p1.2.m2.1.1.2.cmml">60</mn><mo id="S4.SS2.SSS2.Px3.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.Px3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px3.p1.2.m2.1b"><apply id="S4.SS2.SSS2.Px3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px3.p1.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.Px3.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.Px3.p1.2.m2.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px3.p1.2.m2.1c">60\%</annotation></semantics></math> for an answer of <span id="S4.SS2.SSS2.Px3.p1.3.6" class="ltx_text ltx_font_italic">no</span> and MFB had an accuracy score of <math id="S4.SS2.SSS2.Px3.p1.3.m3.1" class="ltx_Math" alttext="83\%" display="inline"><semantics id="S4.SS2.SSS2.Px3.p1.3.m3.1a"><mrow id="S4.SS2.SSS2.Px3.p1.3.m3.1.1" xref="S4.SS2.SSS2.Px3.p1.3.m3.1.1.cmml"><mn id="S4.SS2.SSS2.Px3.p1.3.m3.1.1.2" xref="S4.SS2.SSS2.Px3.p1.3.m3.1.1.2.cmml">83</mn><mo id="S4.SS2.SSS2.Px3.p1.3.m3.1.1.1" xref="S4.SS2.SSS2.Px3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px3.p1.3.m3.1b"><apply id="S4.SS2.SSS2.Px3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.Px3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS2.SSS2.Px3.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.Px3.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS2.SSS2.Px3.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.Px3.p1.3.m3.1.1.2">83</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px3.p1.3.m3.1c">83\%</annotation></semantics></math> for an answer of <span id="S4.SS2.SSS2.Px3.p1.3.7" class="ltx_text ltx_font_italic">yes</span>, even though either is arguably correct.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Limitations</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work aims to understand the degree to which progress on machine “understanding” VQA has, and has the ability to, improve performance on the task of accessibility VQA. Our findings should be interpreted with several limitations in mind. First, while we analyzed many models across several years of VQA research, our analysis is limited to two datasets. Moreover, as discussed in <a href="#S4.SS2.SSS2" title="4.2.2 Where the Models Fail ‣ 4.2 Error Analysis ‣ 4 Findings and Discussion ‣ What’s Different between Visual Question Answering for Machine “Understanding” Versus for Accessibility?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">§ 4.2.2</span></a>, the “ground truth” in these datasets, especially when combined with the standard evaluation metric, is not always reliable. Second, our analysis is limited to English, and may not generalize directly to other languages.
Finally, blind and low-vision users are not a monolithic group, and the photos taken and questions asked in the VizWiz dataset are representative only of those who used the mobile app, likely a small, unrepresentative subset of the population.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Directions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we have shown that, overall, performance improvements on machine “understanding” VQA <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">have</span> translated into performance improvements on the real-world task of accessibility VQA.
However, we have also shown evidence that there may be a significant overfitting effect, where significant model improvements on machine “understanding” VQA translate only into modest improvements in accessibility VQA.
This suggests that if the research community continues to only hill-climb on challenge datasets like VQA-v2, we run the risk of ceasing to make any process on a pressing human-centered application of this technology, and, in the worst case, could degrade performance.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">We have also shown that along with the overall model improvement, the accessibility VQA system have improved on almost all of the challenge classes though some challenges remain difficult.
In general, we observe the models struggle most on questions that require text recognition skill as well as ambiguous questions.
Future work thus may wish to pay more attention on these questions in both data collection and model design.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Finally, we have seen that we are likely reaching the limit of the usefulness of the standard VQA accuracy metric, and that more research is needed to develop automated evaluation protocols that are robust and accurately capture performance improvements. On top of this, VQA systems are reaching impressive levels of performance, suggesting that human evaluation of their performance in ecologically valid settings is becoming increasingly possible. As ecological validity would require conducting such an evaluation with blind or low-vision users, research is needed to ensure that such evaluation paradigms are conducted ethically and minimize potential harms to system users.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This material is based upon work partially supported by the National Science Foundation under Grant No. <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">2131508</span>. The authors are also grateful to all the reviewers who have provided helpful suggestions to improve this work, and to Hernisa Kacorri and Jordan Boyd-Graber who have provided pointers and suggestions.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2017)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence
Zitnick, Devi Parikh, and Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/s11263-016-0966-6" title="" class="ltx_ref ltx_href">Vqa: Visual
question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Int. J. Comput. Vision</em>, 123(1):4–31.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077–6086.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1505.00468" title="" class="ltx_ref ltx_href">VQA: visual question
answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1505.00468.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhattacharya et al. (2019)</span>
<span class="ltx_bibblock">
Nilavra Bhattacharya, Qing Li, and Danna Gurari. 2019.

</span>
<span class="ltx_bibblock">Why does a visual question have different answers?

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 4271–4280.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bigham et al. (2010)</span>
<span class="ltx_bibblock">
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller,
Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual
White, et al. 2010.

</span>
<span class="ltx_bibblock">Vizwiz: nearly real-time answers to visual questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23nd annual ACM symposium on User
interface software and technology</em>, pages 333–342.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brady et al. (2013)</span>
<span class="ltx_bibblock">
Erin Brady, Meredith Ringel Morris, Yu Zhong, Samuel White, and Jeffrey P.
Bigham. 2013.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/2470654.2481291" title="" class="ltx_ref ltx_href">Visual challenges in
the everyday lives of blind people</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems</em>, CHI ’13, page 2117–2126, New York, NY, USA. Association
for Computing Machinery.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coles (1968)</span>
<span class="ltx_bibblock">
L Stephen Coles. 1968.

</span>
<span class="ltx_bibblock">An on-line question-answering systems with natural language and
pictorial input.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1968 23rd ACM national conference</em>, pages
157–167.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2018)</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P. Bigham. 2018.

</span>
<span class="ltx_bibblock">Vizwiz grand challenge: Answering visual questions from blind people.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, pages 3608–3617.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harnad (1990)</span>
<span class="ltx_bibblock">
Stevan Harnad. 1990.

</span>
<span class="ltx_bibblock">The symbol grounding problem.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Physica D: Nonlinear Phenomena</em>, 42(1-3):335–346.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2018)</span>
<span class="ltx_bibblock">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh. 2018.

</span>
<span class="ltx_bibblock">Pythia v0. 1: the winning entry to the vqa challenge 2018.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.09956</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2018)</span>
<span class="ltx_bibblock">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018.

</span>
<span class="ltx_bibblock">Bilinear attention networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 31.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,
Caiming Xiong, and Steven Hoi. 2021.

</span>
<span class="ltx_bibblock">Align before fuse: Vision and language representation learning with
momentum distillation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>, pages 740–755.
Springer.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock">A multi-world approach to question answering about real-world scenes
based on uncertain input.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
27:1682–1690.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2020)</span>
<span class="ltx_bibblock">
Maarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-tutorials.7" title="" class="ltx_ref ltx_href">Commonsense
reasoning for natural language processing</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics: Tutorial Abstracts</em>, pages 27–33, Online.
Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shneiderman (2020)</span>
<span class="ltx_bibblock">
Ben Shneiderman. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/TTS.2020.2992669" title="" class="ltx_ref ltx_href">Design lessons from
ai’s two grand goals: Human emulation and useful applications</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Technology and Society</em>, 1(2):73–82.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Theune et al. (2007)</span>
<span class="ltx_bibblock">
Mariet Theune, Boris van Schooten, Rieks op den Akker, WAUTER Bosma, DENNIS
Hofs, Anton Nijholt, EMIEL Krahmer, Charlotte van Hooijdonk, and Erwin Marsi.
2007.

</span>
<span class="ltx_bibblock">Questions, pictures, answers: Introducing pictures in
question-answering systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ACTAS-1 of X Symposio Internacional de Comunicacion Social,
Santiago de Cuba</em>, pages 450–463.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
Bowman. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/W18-5446" title="" class="ltx_ref ltx_href">GLUE: A multi-task
benchmark and analysis platform for natural language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP</em>, pages 353–355,
Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Chun-Ju Yang, Kristen Grauman, and Danna Gurari. 2018.

</span>
<span class="ltx_bibblock">Visual question answer diversity.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Sixth AAAI Conference on Human Computation and
Crowdsourcing</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2019)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. 2019.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em>, pages 6281–6290.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2017)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. 2017.

</span>
<span class="ltx_bibblock">Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 1821–1830.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2018)</span>
<span class="ltx_bibblock">
Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao. 2018.

</span>
<span class="ltx_bibblock">Beyond bilinear: Generalized multimodal factorized high-order pooling
for visual question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>,
29(12):5947–5959.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2020)</span>
<span class="ltx_bibblock">
Xiaoyu Zeng, Yanan Wang, Tai-Yin Chiu, Nilavra Bhattacharya, and Danna Gurari.
2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3415220" title="" class="ltx_ref ltx_href">Vision skills needed to
answer visual questions</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Human-Computer Interaction</em>,
4(CSCW2):1–31.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.14965" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.14966" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.14966">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.14966" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.14967" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 02:04:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
