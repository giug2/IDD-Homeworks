<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1902.05660] Cycle-Consistency for Robust Visual Question Answering</title><meta property="og:description" content="Despite significant progress in Visual Question Answering over the years, robustness of today’s VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Cycle-Consistency for Robust Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Cycle-Consistency for Robust Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1902.05660">

<!--Generated on Fri Mar  1 17:30:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Cycle-Consistency for Robust Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meet Shah<sup id="id1.1.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id2.2.id1" class="ltx_sup">1</sup>Facebook AI Research, <sup id="id3.3.id2" class="ltx_sup">2</sup>Georgia Institute of Technology

<br class="ltx_break"><span id="id4.4.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{meetshah, xinleic, mrf}@fb.com, dparikh@gatech.edu</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinlei Chen<sup id="id5.1.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id6.2.id1" class="ltx_sup">1</sup>Facebook AI Research, <sup id="id7.3.id2" class="ltx_sup">2</sup>Georgia Institute of Technology

<br class="ltx_break"><span id="id8.4.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{meetshah, xinleic, mrf}@fb.com, dparikh@gatech.edu</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marcus Rohrbach<sup id="id9.1.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id10.2.id1" class="ltx_sup">1</sup>Facebook AI Research, <sup id="id11.3.id2" class="ltx_sup">2</sup>Georgia Institute of Technology

<br class="ltx_break"><span id="id12.4.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{meetshah, xinleic, mrf}@fb.com, dparikh@gatech.edu</span>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Devi Parikh<sup id="id13.1.id1" class="ltx_sup">1,2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup id="id14.2.id1" class="ltx_sup">1</sup>Facebook AI Research, <sup id="id15.3.id2" class="ltx_sup">2</sup>Georgia Institute of Technology

<br class="ltx_break"><span id="id16.4.id3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{meetshah, xinleic, mrf}@fb.com, dparikh@gatech.edu</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Despite significant progress in Visual Question Answering over the years, robustness of today’s VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions.
VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions spanning 40k images from the VQA v2.0 validation dataset.
As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question.
Without the use of additional annotations,
we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset.
In addition, our approach outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/1902.05660/assets/x1.png" id="S1.F1.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="243" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.4.1" class="ltx_text ltx_font_bold">Existing VQA models are brittle</span>. Shown above are examples from our new large-scale <span id="S1.F1.5.2" class="ltx_text ltx_font_bold">VQA-Rephrasings</span> dataset that enables systematic evaluation of robustness of VQA models to linguistic variations in the input question. Also shown are answers predicted by a state-of-the-art VQA model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>.
We see that the model predicts different answers for different reasonable rephrasings of the same question. We propose a novel model-agnostic framework that exploits cycle consistency in question answering and question generation to make VQA models more robust,
without using additional annotation.
Moreover, it outperforms state-of-the-art models on the standard VQA and Visual Question Generation tasks on the VQA v2.0 dataset.
</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) applications allow a human user to ask a machine questions about images – be it a user interacting with a visual chat-bot or a visually impaired user relying on an assistive device. As this technology steps out of the realm of curated datasets towards real-world settings, it is desirable that VQA models be robust to and consistent across reasonable variations in the input modalities. While there has been significant progress in VQA over the years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, today’s VQA models are, however, far from being robust.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA is a task that lies at the intersection of language and vision. Existing works have studied the robustness and sensitiveness of VQA models to meaningful semantic variations in images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, changing answer distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and adversarial attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> to images. However, to the best of our knowledge, no work has studied the robustness of VQA models to linguistic variations in the input question. This is important both from the perspective of VQA being a benchmark to test multi-modal AI capabilities (do our VQA models really “understand” the question when answering it?) and for applications (human users are likely to phrase the same query in a variety of different linguistic forms). However, today’s state-of-the-art VQA models are brittle to such linguistic variations as can be seen in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One approach to make VQA models more robust is to collect a dataset with diverse rephrasings of questions to train VQA models. This requires additional human annotation and thus is not always scalable in real-world settings.
Alternatively, an automatic approach that does not require additional human intervention but results in a VQA model that is more robust to linguistic variations observed in the natural language open-ended questions is desirable.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We propose a novel model-agnostic framework that relies on cycle consistency to learn robust VQA models without requiring additional annotation. Specifically, we train the model to not just answer a question, but also to generate diverse, semantically similar variations of questions conditioned on the answer. We enforce that the answer predicted for a generated question matches the ground truth answer to the original question. In other words, the model is being trained to predict the same (correct) answer for a question and its (generated) rephrasing.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Advantages of our proposed approach are two fold.
First, enforcing consistent correctness across diverse rephrasings allows models to generalize to unseen
semantically equivalent variations of questions at test time. The model achieves this by generating linguistically diverse rephrasings of questions on-the-fly and training with these variations.
Second, a model trained generatively to generate a valid question given a candidate answer and image has a stronger multi-modal understanding of vision and language. Questions tend to have less learnable biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. As a result, models that can jointly perform the task of question generation and question answering are less prone to taking “shortcuts” and exploiting linguistic priors in questions. Indeed, we find that models trained with our approach outperform existing state-of-the-art models on both VQA and Visual Question Generation (VQG) tasks on VQA v2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.2" class="ltx_p">We also observed that one reason for limited development of VQA models robust to linguistic variations in input questions is due to the lack of a benchmark to measure robustness.
A lack of such a benchmark makes it hard to quantitatively realize the inflated capabilities and limited multi-modal understanding of modern VQA models and consequently inhibits progress in pushing the state-of-the-art in multi-modal understanding aspects of computer vision.
To enable quantitative evaluation of robustness and consistency of VQA models across linguistic variations in input questions, we collect a large-scale dataset – <span id="S1.p6.2.1" class="ltx_text ltx_font_bold">VQA-Rephrasings</span> (Section <a href="#S4" title="4 VQA-Rephrasings Dataset ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>) based of the VQA v2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. VQA-Rephrasings contains 3 human-provided rephrasings for <math id="S1.p6.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p6.1.m1.1a"><mo id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><csymbol cd="latexml" id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">\sim</annotation></semantics></math>40k questions on <math id="S1.p6.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.p6.2.m2.1a"><mo id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><csymbol cd="latexml" id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">\sim</annotation></semantics></math>40k images from the validation split of the VQA v2.0 dataset. We also propose metrics to measure the robustness of VQA models across different question rephrasings. Further, we benchmark several state-of-the-art VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> on our proposed VQA-Rephrasings dataset to highlight the fragility of VQA models to question rephrasings. We observe a significant drop when VQA models are required to be consistent in addition to being correct (Section <a href="#S5" title="5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), which reinforces our belief that existing VQA models do not understand language ”enough”. We show that VQA models trained with our approach are significantly more robust across question rephrasings than their existing counterparts on the proposed VQA-Rephrasings dataset. 
<br class="ltx_break"></p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this paper, our contributions are the following:</p>
<ul id="S1.p7.2" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a model-agnostic cycle-consistent training scheme that enables VQA models to be more robust to linguistic variations observed in natural language open-ended questions.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.3" class="ltx_p">To evaluate the robustness of VQA models to linguistic variations, we introduce a large-scale <span id="S1.I1.i2.p1.3.1" class="ltx_text ltx_font_bold">VQA-Rephrasings</span> dataset and an associated consensus score. VQA-Rephrasings consists of 3 rephrasings for <math id="S1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.I1.i2.p1.1.m1.1a"><mo id="S1.I1.i2.p1.1.m1.1.1" xref="S1.I1.i2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.1.m1.1b"><csymbol cd="latexml" id="S1.I1.i2.p1.1.m1.1.1.cmml" xref="S1.I1.i2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.1.m1.1c">\sim</annotation></semantics></math>40k questions on <math id="S1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.I1.i2.p1.2.m2.1a"><mo id="S1.I1.i2.p1.2.m2.1.1" xref="S1.I1.i2.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.2.m2.1b"><csymbol cd="latexml" id="S1.I1.i2.p1.2.m2.1.1.cmml" xref="S1.I1.i2.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.2.m2.1c">\sim</annotation></semantics></math>40k images from the VQA v2.0 validation dataset, resulting in a total of <math id="S1.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S1.I1.i2.p1.3.m3.1a"><mo id="S1.I1.i2.p1.3.m3.1.1" xref="S1.I1.i2.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.p1.3.m3.1b"><csymbol cd="latexml" id="S1.I1.i2.p1.3.m3.1.1.cmml" xref="S1.I1.i2.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.p1.3.m3.1c">\sim</annotation></semantics></math>120k questions rephrasing by humans.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We show that models trained with our approach outperform state-of-the-art on the standard VQA and Visual Question Generation tasks on the VQA v2.0 dataset and are significantly more robust to linguistic variations on VQA-Rephrasings.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/1902.05660/assets/x2.png" id="S1.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="155" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) <span id="S1.F2.43.2" class="ltx_text ltx_font_bold">Abstract representation of the proposed cycle-consistent training scheme:</span> Given a triplet of image <math id="S1.F2.22.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S1.F2.22.m1.1b"><mi id="S1.F2.22.m1.1.1" xref="S1.F2.22.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S1.F2.22.m1.1c"><ci id="S1.F2.22.m1.1.1.cmml" xref="S1.F2.22.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.22.m1.1d">I</annotation></semantics></math>, question <math id="S1.F2.23.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S1.F2.23.m2.1b"><mi id="S1.F2.23.m2.1.1" xref="S1.F2.23.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S1.F2.23.m2.1c"><ci id="S1.F2.23.m2.1.1.cmml" xref="S1.F2.23.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.23.m2.1d">Q</annotation></semantics></math>, and ground truth answer <math id="S1.F2.24.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S1.F2.24.m3.1b"><mi id="S1.F2.24.m3.1.1" xref="S1.F2.24.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S1.F2.24.m3.1c"><ci id="S1.F2.24.m3.1.1.cmml" xref="S1.F2.24.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.24.m3.1d">A</annotation></semantics></math>, a VQA model is a transformation <math id="S1.F2.25.m4.2" class="ltx_Math" alttext="F:(Q,I)\mapsto A^{\prime}" display="inline"><semantics id="S1.F2.25.m4.2b"><mrow id="S1.F2.25.m4.2.3" xref="S1.F2.25.m4.2.3.cmml"><mi id="S1.F2.25.m4.2.3.2" xref="S1.F2.25.m4.2.3.2.cmml">F</mi><mo lspace="0.278em" rspace="0.278em" id="S1.F2.25.m4.2.3.1" xref="S1.F2.25.m4.2.3.1.cmml">:</mo><mrow id="S1.F2.25.m4.2.3.3" xref="S1.F2.25.m4.2.3.3.cmml"><mrow id="S1.F2.25.m4.2.3.3.2.2" xref="S1.F2.25.m4.2.3.3.2.1.cmml"><mo stretchy="false" id="S1.F2.25.m4.2.3.3.2.2.1" xref="S1.F2.25.m4.2.3.3.2.1.cmml">(</mo><mi id="S1.F2.25.m4.1.1" xref="S1.F2.25.m4.1.1.cmml">Q</mi><mo id="S1.F2.25.m4.2.3.3.2.2.2" xref="S1.F2.25.m4.2.3.3.2.1.cmml">,</mo><mi id="S1.F2.25.m4.2.2" xref="S1.F2.25.m4.2.2.cmml">I</mi><mo stretchy="false" id="S1.F2.25.m4.2.3.3.2.2.3" xref="S1.F2.25.m4.2.3.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S1.F2.25.m4.2.3.3.1" xref="S1.F2.25.m4.2.3.3.1.cmml">↦</mo><msup id="S1.F2.25.m4.2.3.3.3" xref="S1.F2.25.m4.2.3.3.3.cmml"><mi id="S1.F2.25.m4.2.3.3.3.2" xref="S1.F2.25.m4.2.3.3.3.2.cmml">A</mi><mo id="S1.F2.25.m4.2.3.3.3.3" xref="S1.F2.25.m4.2.3.3.3.3.cmml">′</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.25.m4.2c"><apply id="S1.F2.25.m4.2.3.cmml" xref="S1.F2.25.m4.2.3"><ci id="S1.F2.25.m4.2.3.1.cmml" xref="S1.F2.25.m4.2.3.1">:</ci><ci id="S1.F2.25.m4.2.3.2.cmml" xref="S1.F2.25.m4.2.3.2">𝐹</ci><apply id="S1.F2.25.m4.2.3.3.cmml" xref="S1.F2.25.m4.2.3.3"><csymbol cd="latexml" id="S1.F2.25.m4.2.3.3.1.cmml" xref="S1.F2.25.m4.2.3.3.1">maps-to</csymbol><interval closure="open" id="S1.F2.25.m4.2.3.3.2.1.cmml" xref="S1.F2.25.m4.2.3.3.2.2"><ci id="S1.F2.25.m4.1.1.cmml" xref="S1.F2.25.m4.1.1">𝑄</ci><ci id="S1.F2.25.m4.2.2.cmml" xref="S1.F2.25.m4.2.2">𝐼</ci></interval><apply id="S1.F2.25.m4.2.3.3.3.cmml" xref="S1.F2.25.m4.2.3.3.3"><csymbol cd="ambiguous" id="S1.F2.25.m4.2.3.3.3.1.cmml" xref="S1.F2.25.m4.2.3.3.3">superscript</csymbol><ci id="S1.F2.25.m4.2.3.3.3.2.cmml" xref="S1.F2.25.m4.2.3.3.3.2">𝐴</ci><ci id="S1.F2.25.m4.2.3.3.3.3.cmml" xref="S1.F2.25.m4.2.3.3.3.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.25.m4.2d">F:(Q,I)\mapsto A^{\prime}</annotation></semantics></math> used to predict the answer <math id="S1.F2.26.m5.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S1.F2.26.m5.1b"><msup id="S1.F2.26.m5.1.1" xref="S1.F2.26.m5.1.1.cmml"><mi id="S1.F2.26.m5.1.1.2" xref="S1.F2.26.m5.1.1.2.cmml">A</mi><mo id="S1.F2.26.m5.1.1.3" xref="S1.F2.26.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.26.m5.1c"><apply id="S1.F2.26.m5.1.1.cmml" xref="S1.F2.26.m5.1.1"><csymbol cd="ambiguous" id="S1.F2.26.m5.1.1.1.cmml" xref="S1.F2.26.m5.1.1">superscript</csymbol><ci id="S1.F2.26.m5.1.1.2.cmml" xref="S1.F2.26.m5.1.1.2">𝐴</ci><ci id="S1.F2.26.m5.1.1.3.cmml" xref="S1.F2.26.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.26.m5.1d">A^{\prime}</annotation></semantics></math>. Similarly, a VQG model <math id="S1.F2.27.m6.2" class="ltx_Math" alttext="G:(A^{\prime},I)\mapsto Q^{\prime}" display="inline"><semantics id="S1.F2.27.m6.2b"><mrow id="S1.F2.27.m6.2.2" xref="S1.F2.27.m6.2.2.cmml"><mi id="S1.F2.27.m6.2.2.3" xref="S1.F2.27.m6.2.2.3.cmml">G</mi><mo lspace="0.278em" rspace="0.278em" id="S1.F2.27.m6.2.2.2" xref="S1.F2.27.m6.2.2.2.cmml">:</mo><mrow id="S1.F2.27.m6.2.2.1" xref="S1.F2.27.m6.2.2.1.cmml"><mrow id="S1.F2.27.m6.2.2.1.1.1" xref="S1.F2.27.m6.2.2.1.1.2.cmml"><mo stretchy="false" id="S1.F2.27.m6.2.2.1.1.1.2" xref="S1.F2.27.m6.2.2.1.1.2.cmml">(</mo><msup id="S1.F2.27.m6.2.2.1.1.1.1" xref="S1.F2.27.m6.2.2.1.1.1.1.cmml"><mi id="S1.F2.27.m6.2.2.1.1.1.1.2" xref="S1.F2.27.m6.2.2.1.1.1.1.2.cmml">A</mi><mo id="S1.F2.27.m6.2.2.1.1.1.1.3" xref="S1.F2.27.m6.2.2.1.1.1.1.3.cmml">′</mo></msup><mo id="S1.F2.27.m6.2.2.1.1.1.3" xref="S1.F2.27.m6.2.2.1.1.2.cmml">,</mo><mi id="S1.F2.27.m6.1.1" xref="S1.F2.27.m6.1.1.cmml">I</mi><mo stretchy="false" id="S1.F2.27.m6.2.2.1.1.1.4" xref="S1.F2.27.m6.2.2.1.1.2.cmml">)</mo></mrow><mo stretchy="false" id="S1.F2.27.m6.2.2.1.2" xref="S1.F2.27.m6.2.2.1.2.cmml">↦</mo><msup id="S1.F2.27.m6.2.2.1.3" xref="S1.F2.27.m6.2.2.1.3.cmml"><mi id="S1.F2.27.m6.2.2.1.3.2" xref="S1.F2.27.m6.2.2.1.3.2.cmml">Q</mi><mo id="S1.F2.27.m6.2.2.1.3.3" xref="S1.F2.27.m6.2.2.1.3.3.cmml">′</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S1.F2.27.m6.2c"><apply id="S1.F2.27.m6.2.2.cmml" xref="S1.F2.27.m6.2.2"><ci id="S1.F2.27.m6.2.2.2.cmml" xref="S1.F2.27.m6.2.2.2">:</ci><ci id="S1.F2.27.m6.2.2.3.cmml" xref="S1.F2.27.m6.2.2.3">𝐺</ci><apply id="S1.F2.27.m6.2.2.1.cmml" xref="S1.F2.27.m6.2.2.1"><csymbol cd="latexml" id="S1.F2.27.m6.2.2.1.2.cmml" xref="S1.F2.27.m6.2.2.1.2">maps-to</csymbol><interval closure="open" id="S1.F2.27.m6.2.2.1.1.2.cmml" xref="S1.F2.27.m6.2.2.1.1.1"><apply id="S1.F2.27.m6.2.2.1.1.1.1.cmml" xref="S1.F2.27.m6.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S1.F2.27.m6.2.2.1.1.1.1.1.cmml" xref="S1.F2.27.m6.2.2.1.1.1.1">superscript</csymbol><ci id="S1.F2.27.m6.2.2.1.1.1.1.2.cmml" xref="S1.F2.27.m6.2.2.1.1.1.1.2">𝐴</ci><ci id="S1.F2.27.m6.2.2.1.1.1.1.3.cmml" xref="S1.F2.27.m6.2.2.1.1.1.1.3">′</ci></apply><ci id="S1.F2.27.m6.1.1.cmml" xref="S1.F2.27.m6.1.1">𝐼</ci></interval><apply id="S1.F2.27.m6.2.2.1.3.cmml" xref="S1.F2.27.m6.2.2.1.3"><csymbol cd="ambiguous" id="S1.F2.27.m6.2.2.1.3.1.cmml" xref="S1.F2.27.m6.2.2.1.3">superscript</csymbol><ci id="S1.F2.27.m6.2.2.1.3.2.cmml" xref="S1.F2.27.m6.2.2.1.3.2">𝑄</ci><ci id="S1.F2.27.m6.2.2.1.3.3.cmml" xref="S1.F2.27.m6.2.2.1.3.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.27.m6.2d">G:(A^{\prime},I)\mapsto Q^{\prime}</annotation></semantics></math> is used to generate a rephrasing <math id="S1.F2.28.m7.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S1.F2.28.m7.1b"><msup id="S1.F2.28.m7.1.1" xref="S1.F2.28.m7.1.1.cmml"><mi id="S1.F2.28.m7.1.1.2" xref="S1.F2.28.m7.1.1.2.cmml">Q</mi><mo id="S1.F2.28.m7.1.1.3" xref="S1.F2.28.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.28.m7.1c"><apply id="S1.F2.28.m7.1.1.cmml" xref="S1.F2.28.m7.1.1"><csymbol cd="ambiguous" id="S1.F2.28.m7.1.1.1.cmml" xref="S1.F2.28.m7.1.1">superscript</csymbol><ci id="S1.F2.28.m7.1.1.2.cmml" xref="S1.F2.28.m7.1.1.2">𝑄</ci><ci id="S1.F2.28.m7.1.1.3.cmml" xref="S1.F2.28.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.28.m7.1d">Q^{\prime}</annotation></semantics></math> of <math id="S1.F2.29.m8.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S1.F2.29.m8.1b"><mi id="S1.F2.29.m8.1.1" xref="S1.F2.29.m8.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S1.F2.29.m8.1c"><ci id="S1.F2.29.m8.1.1.cmml" xref="S1.F2.29.m8.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.29.m8.1d">Q</annotation></semantics></math>. The generated rephrasing <math id="S1.F2.30.m9.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S1.F2.30.m9.1b"><msup id="S1.F2.30.m9.1.1" xref="S1.F2.30.m9.1.1.cmml"><mi id="S1.F2.30.m9.1.1.2" xref="S1.F2.30.m9.1.1.2.cmml">Q</mi><mo id="S1.F2.30.m9.1.1.3" xref="S1.F2.30.m9.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.30.m9.1c"><apply id="S1.F2.30.m9.1.1.cmml" xref="S1.F2.30.m9.1.1"><csymbol cd="ambiguous" id="S1.F2.30.m9.1.1.1.cmml" xref="S1.F2.30.m9.1.1">superscript</csymbol><ci id="S1.F2.30.m9.1.1.2.cmml" xref="S1.F2.30.m9.1.1.2">𝑄</ci><ci id="S1.F2.30.m9.1.1.3.cmml" xref="S1.F2.30.m9.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.30.m9.1d">Q^{\prime}</annotation></semantics></math> is passed through <math id="S1.F2.31.m10.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S1.F2.31.m10.1b"><mi id="S1.F2.31.m10.1.1" xref="S1.F2.31.m10.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S1.F2.31.m10.1c"><ci id="S1.F2.31.m10.1.1.cmml" xref="S1.F2.31.m10.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.31.m10.1d">F</annotation></semantics></math> to obtain <math id="S1.F2.32.m11.1" class="ltx_Math" alttext="A^{\prime\prime}" display="inline"><semantics id="S1.F2.32.m11.1b"><msup id="S1.F2.32.m11.1.1" xref="S1.F2.32.m11.1.1.cmml"><mi id="S1.F2.32.m11.1.1.2" xref="S1.F2.32.m11.1.1.2.cmml">A</mi><mo id="S1.F2.32.m11.1.1.3" xref="S1.F2.32.m11.1.1.3.cmml">′′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.32.m11.1c"><apply id="S1.F2.32.m11.1.1.cmml" xref="S1.F2.32.m11.1.1"><csymbol cd="ambiguous" id="S1.F2.32.m11.1.1.1.cmml" xref="S1.F2.32.m11.1.1">superscript</csymbol><ci id="S1.F2.32.m11.1.1.2.cmml" xref="S1.F2.32.m11.1.1.2">𝐴</ci><ci id="S1.F2.32.m11.1.1.3.cmml" xref="S1.F2.32.m11.1.1.3">′′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.32.m11.1d">A^{\prime\prime}</annotation></semantics></math> and consistency is enforced between <math id="S1.F2.33.m12.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S1.F2.33.m12.1b"><mi id="S1.F2.33.m12.1.1" xref="S1.F2.33.m12.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S1.F2.33.m12.1c"><ci id="S1.F2.33.m12.1.1.cmml" xref="S1.F2.33.m12.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.33.m12.1d">Q</annotation></semantics></math> and <math id="S1.F2.34.m13.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S1.F2.34.m13.1b"><msup id="S1.F2.34.m13.1.1" xref="S1.F2.34.m13.1.1.cmml"><mi id="S1.F2.34.m13.1.1.2" xref="S1.F2.34.m13.1.1.2.cmml">Q</mi><mo id="S1.F2.34.m13.1.1.3" xref="S1.F2.34.m13.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.34.m13.1c"><apply id="S1.F2.34.m13.1.1.cmml" xref="S1.F2.34.m13.1.1"><csymbol cd="ambiguous" id="S1.F2.34.m13.1.1.1.cmml" xref="S1.F2.34.m13.1.1">superscript</csymbol><ci id="S1.F2.34.m13.1.1.2.cmml" xref="S1.F2.34.m13.1.1.2">𝑄</ci><ci id="S1.F2.34.m13.1.1.3.cmml" xref="S1.F2.34.m13.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.34.m13.1d">Q^{\prime}</annotation></semantics></math> and between <math id="S1.F2.35.m14.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S1.F2.35.m14.1b"><msup id="S1.F2.35.m14.1.1" xref="S1.F2.35.m14.1.1.cmml"><mi id="S1.F2.35.m14.1.1.2" xref="S1.F2.35.m14.1.1.2.cmml">A</mi><mo id="S1.F2.35.m14.1.1.3" xref="S1.F2.35.m14.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.35.m14.1c"><apply id="S1.F2.35.m14.1.1.cmml" xref="S1.F2.35.m14.1.1"><csymbol cd="ambiguous" id="S1.F2.35.m14.1.1.1.cmml" xref="S1.F2.35.m14.1.1">superscript</csymbol><ci id="S1.F2.35.m14.1.1.2.cmml" xref="S1.F2.35.m14.1.1.2">𝐴</ci><ci id="S1.F2.35.m14.1.1.3.cmml" xref="S1.F2.35.m14.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.35.m14.1d">A^{\prime}</annotation></semantics></math> and <math id="S1.F2.36.m15.1" class="ltx_Math" alttext="A^{\prime\prime}" display="inline"><semantics id="S1.F2.36.m15.1b"><msup id="S1.F2.36.m15.1.1" xref="S1.F2.36.m15.1.1.cmml"><mi id="S1.F2.36.m15.1.1.2" xref="S1.F2.36.m15.1.1.2.cmml">A</mi><mo id="S1.F2.36.m15.1.1.3" xref="S1.F2.36.m15.1.1.3.cmml">′′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.36.m15.1c"><apply id="S1.F2.36.m15.1.1.cmml" xref="S1.F2.36.m15.1.1"><csymbol cd="ambiguous" id="S1.F2.36.m15.1.1.1.cmml" xref="S1.F2.36.m15.1.1">superscript</csymbol><ci id="S1.F2.36.m15.1.1.2.cmml" xref="S1.F2.36.m15.1.1.2">𝐴</ci><ci id="S1.F2.36.m15.1.1.3.cmml" xref="S1.F2.36.m15.1.1.3">′′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.36.m15.1d">A^{\prime\prime}</annotation></semantics></math>. Image <math id="S1.F2.37.m16.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S1.F2.37.m16.1b"><mi id="S1.F2.37.m16.1.1" xref="S1.F2.37.m16.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S1.F2.37.m16.1c"><ci id="S1.F2.37.m16.1.1.cmml" xref="S1.F2.37.m16.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.37.m16.1d">I</annotation></semantics></math> is not shown for clarity.
(b) <span id="S1.F2.38.1" class="ltx_text ltx_font_bold">Detailed architecture of our visual question generation module <math id="S1.F2.38.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S1.F2.38.1.m1.1b"><mi id="S1.F2.38.1.m1.1.1" xref="S1.F2.38.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S1.F2.38.1.m1.1c"><ci id="S1.F2.38.1.m1.1.1.cmml" xref="S1.F2.38.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.38.1.m1.1d">G</annotation></semantics></math></span>. The predicted answer <math id="S1.F2.39.m17.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S1.F2.39.m17.1b"><msup id="S1.F2.39.m17.1.1" xref="S1.F2.39.m17.1.1.cmml"><mi id="S1.F2.39.m17.1.1.2" xref="S1.F2.39.m17.1.1.2.cmml">A</mi><mo id="S1.F2.39.m17.1.1.3" xref="S1.F2.39.m17.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.39.m17.1c"><apply id="S1.F2.39.m17.1.1.cmml" xref="S1.F2.39.m17.1.1"><csymbol cd="ambiguous" id="S1.F2.39.m17.1.1.1.cmml" xref="S1.F2.39.m17.1.1">superscript</csymbol><ci id="S1.F2.39.m17.1.1.2.cmml" xref="S1.F2.39.m17.1.1.2">𝐴</ci><ci id="S1.F2.39.m17.1.1.3.cmml" xref="S1.F2.39.m17.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.39.m17.1d">A^{\prime}</annotation></semantics></math> and image <math id="S1.F2.40.m18.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S1.F2.40.m18.1b"><mi id="S1.F2.40.m18.1.1" xref="S1.F2.40.m18.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S1.F2.40.m18.1c"><ci id="S1.F2.40.m18.1.1.cmml" xref="S1.F2.40.m18.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.40.m18.1d">I</annotation></semantics></math> are embedded to a lower dimension using task-specific encoders and the resulting feature maps are summed up with additive noise and fed to an LSTM to generate questions rephrasings <math id="S1.F2.41.m19.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S1.F2.41.m19.1b"><msup id="S1.F2.41.m19.1.1" xref="S1.F2.41.m19.1.1.cmml"><mi id="S1.F2.41.m19.1.1.2" xref="S1.F2.41.m19.1.1.2.cmml">Q</mi><mo id="S1.F2.41.m19.1.1.3" xref="S1.F2.41.m19.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F2.41.m19.1c"><apply id="S1.F2.41.m19.1.1.cmml" xref="S1.F2.41.m19.1.1"><csymbol cd="ambiguous" id="S1.F2.41.m19.1.1.1.cmml" xref="S1.F2.41.m19.1.1">superscript</csymbol><ci id="S1.F2.41.m19.1.1.2.cmml" xref="S1.F2.41.m19.1.1.2">𝑄</ci><ci id="S1.F2.41.m19.1.1.3.cmml" xref="S1.F2.41.m19.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.41.m19.1d">Q^{\prime}</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Visual Question Answering.</span>
There has been tremendous progress in building models for VQA using LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and convolutional networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. VQA models spanning different paradigms like attention networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, module networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, relational networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and multi-modal fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have been proposed. Our method is model-agnostic and is applicable with any existing VQA architecture.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">Robustness.</span>
Robustness of VQA models has been studied in several contexts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> studies the robustness of VQA models to changes in the answer distributions across training and test settings; <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> analyzes the extent of visual grounding in VQA models by studying robustness of VQA models to meaningful semantic changes in images; <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> shows that despite the use of an advanced attention mechanism, it is easy to fool a VQA model with very minor changes in the image. Our work, however, aims to complete the study in robustness by benchmarking and improving robustness of VQA models to linguistic and compositional variations in questions in the form of rephrasings. Robustness has also been studied in natural language processing (NLP) systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> in contexts of bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, domain-shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and syntactic variations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. To counter these issues in NLP systems, solutions like linguistically motivated data-augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have been proposed. We study this in the context of visual question answering which is a multi-modal task which grounds language into the visual world.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">(Visual) Question Generation.</span>
Question Generation (QG) as a task has been studied extensively by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> in NLP. Generating questions conditioned on an image was introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and a large-scale VQG dataset was collected by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to evaluate visually grounded question generation capabilities of models. More recently, there has been work on generating questions that are diverse <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Training models to ask informative questions about an image in an active learning fixed-budget setting was explored in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
While these techniques generate questions about an image in an answer-agnostic manner, techniques like <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> propose a variational LSTM based model trained with reinforcement learning to generate answer-specific questions for an image. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> generates answer-specific questions for specific question-types by modelling question generation as a dual task of question answering. Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, our method is not restricted to generating questions only for specific question types. Different from previous works, the goal of our VQG component is to automatically generate question rephrasings that make the VQA models more robust to linguistic variations. To the best of our knowledge, we are the first to demonstrate that the VQG module can be used to improve VQA accuracy in a cycle-consistent setting.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Cycle-Consistent Learning.</span>
Using cycle-consistency to regularize the training of models has been used extensively in object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, unpaired image-to-image translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and text-based question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Consistency enables learning of robust models by regularizing transformations that map one interconnected modality or domain to the other. While cycle consistency has been used vastly in the domains involving a single modality (text-only or image-only), it hasn’t been explored in the context of multi-modal tasks like VQA.
Cycle-consistency in VQA can be also thought of as an online data-augmentation technique where the model is trained on several generated rephrasings of the same question.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.17" class="ltx_p">We now introduce our cycle-consistent scheme to train robust VQA models. Given a triplet of image <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">I</annotation></semantics></math>, question <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">Q</annotation></semantics></math>, and ground truth answer <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">A</annotation></semantics></math>, a generic VQA model can be formulated as a transformation <math id="S3.p1.4.m4.2" class="ltx_Math" alttext="F:(Q,I)\mapsto A^{\prime}" display="inline"><semantics id="S3.p1.4.m4.2a"><mrow id="S3.p1.4.m4.2.3" xref="S3.p1.4.m4.2.3.cmml"><mi id="S3.p1.4.m4.2.3.2" xref="S3.p1.4.m4.2.3.2.cmml">F</mi><mo lspace="0.278em" rspace="0.278em" id="S3.p1.4.m4.2.3.1" xref="S3.p1.4.m4.2.3.1.cmml">:</mo><mrow id="S3.p1.4.m4.2.3.3" xref="S3.p1.4.m4.2.3.3.cmml"><mrow id="S3.p1.4.m4.2.3.3.2.2" xref="S3.p1.4.m4.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.p1.4.m4.2.3.3.2.2.1" xref="S3.p1.4.m4.2.3.3.2.1.cmml">(</mo><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">Q</mi><mo id="S3.p1.4.m4.2.3.3.2.2.2" xref="S3.p1.4.m4.2.3.3.2.1.cmml">,</mo><mi id="S3.p1.4.m4.2.2" xref="S3.p1.4.m4.2.2.cmml">I</mi><mo stretchy="false" id="S3.p1.4.m4.2.3.3.2.2.3" xref="S3.p1.4.m4.2.3.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S3.p1.4.m4.2.3.3.1" xref="S3.p1.4.m4.2.3.3.1.cmml">↦</mo><msup id="S3.p1.4.m4.2.3.3.3" xref="S3.p1.4.m4.2.3.3.3.cmml"><mi id="S3.p1.4.m4.2.3.3.3.2" xref="S3.p1.4.m4.2.3.3.3.2.cmml">A</mi><mo id="S3.p1.4.m4.2.3.3.3.3" xref="S3.p1.4.m4.2.3.3.3.3.cmml">′</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.2b"><apply id="S3.p1.4.m4.2.3.cmml" xref="S3.p1.4.m4.2.3"><ci id="S3.p1.4.m4.2.3.1.cmml" xref="S3.p1.4.m4.2.3.1">:</ci><ci id="S3.p1.4.m4.2.3.2.cmml" xref="S3.p1.4.m4.2.3.2">𝐹</ci><apply id="S3.p1.4.m4.2.3.3.cmml" xref="S3.p1.4.m4.2.3.3"><csymbol cd="latexml" id="S3.p1.4.m4.2.3.3.1.cmml" xref="S3.p1.4.m4.2.3.3.1">maps-to</csymbol><interval closure="open" id="S3.p1.4.m4.2.3.3.2.1.cmml" xref="S3.p1.4.m4.2.3.3.2.2"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝑄</ci><ci id="S3.p1.4.m4.2.2.cmml" xref="S3.p1.4.m4.2.2">𝐼</ci></interval><apply id="S3.p1.4.m4.2.3.3.3.cmml" xref="S3.p1.4.m4.2.3.3.3"><csymbol cd="ambiguous" id="S3.p1.4.m4.2.3.3.3.1.cmml" xref="S3.p1.4.m4.2.3.3.3">superscript</csymbol><ci id="S3.p1.4.m4.2.3.3.3.2.cmml" xref="S3.p1.4.m4.2.3.3.3.2">𝐴</ci><ci id="S3.p1.4.m4.2.3.3.3.3.cmml" xref="S3.p1.4.m4.2.3.3.3.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.2c">F:(Q,I)\mapsto A^{\prime}</annotation></semantics></math>, where <math id="S3.p1.5.m5.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S3.p1.5.m5.1a"><msup id="S3.p1.5.m5.1.1" xref="S3.p1.5.m5.1.1.cmml"><mi id="S3.p1.5.m5.1.1.2" xref="S3.p1.5.m5.1.1.2.cmml">A</mi><mo id="S3.p1.5.m5.1.1.3" xref="S3.p1.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.5.m5.1b"><apply id="S3.p1.5.m5.1.1.cmml" xref="S3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p1.5.m5.1.1.1.cmml" xref="S3.p1.5.m5.1.1">superscript</csymbol><ci id="S3.p1.5.m5.1.1.2.cmml" xref="S3.p1.5.m5.1.1.2">𝐴</ci><ci id="S3.p1.5.m5.1.1.3.cmml" xref="S3.p1.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.5.m5.1c">A^{\prime}</annotation></semantics></math> is the answer predicted by the model as in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(a). Similarly, a generic VQG model can be formulated as a transformation <math id="S3.p1.6.m6.2" class="ltx_Math" alttext="G:(A,I)\mapsto Q^{\prime}" display="inline"><semantics id="S3.p1.6.m6.2a"><mrow id="S3.p1.6.m6.2.3" xref="S3.p1.6.m6.2.3.cmml"><mi id="S3.p1.6.m6.2.3.2" xref="S3.p1.6.m6.2.3.2.cmml">G</mi><mo lspace="0.278em" rspace="0.278em" id="S3.p1.6.m6.2.3.1" xref="S3.p1.6.m6.2.3.1.cmml">:</mo><mrow id="S3.p1.6.m6.2.3.3" xref="S3.p1.6.m6.2.3.3.cmml"><mrow id="S3.p1.6.m6.2.3.3.2.2" xref="S3.p1.6.m6.2.3.3.2.1.cmml"><mo stretchy="false" id="S3.p1.6.m6.2.3.3.2.2.1" xref="S3.p1.6.m6.2.3.3.2.1.cmml">(</mo><mi id="S3.p1.6.m6.1.1" xref="S3.p1.6.m6.1.1.cmml">A</mi><mo id="S3.p1.6.m6.2.3.3.2.2.2" xref="S3.p1.6.m6.2.3.3.2.1.cmml">,</mo><mi id="S3.p1.6.m6.2.2" xref="S3.p1.6.m6.2.2.cmml">I</mi><mo stretchy="false" id="S3.p1.6.m6.2.3.3.2.2.3" xref="S3.p1.6.m6.2.3.3.2.1.cmml">)</mo></mrow><mo stretchy="false" id="S3.p1.6.m6.2.3.3.1" xref="S3.p1.6.m6.2.3.3.1.cmml">↦</mo><msup id="S3.p1.6.m6.2.3.3.3" xref="S3.p1.6.m6.2.3.3.3.cmml"><mi id="S3.p1.6.m6.2.3.3.3.2" xref="S3.p1.6.m6.2.3.3.3.2.cmml">Q</mi><mo id="S3.p1.6.m6.2.3.3.3.3" xref="S3.p1.6.m6.2.3.3.3.3.cmml">′</mo></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.6.m6.2b"><apply id="S3.p1.6.m6.2.3.cmml" xref="S3.p1.6.m6.2.3"><ci id="S3.p1.6.m6.2.3.1.cmml" xref="S3.p1.6.m6.2.3.1">:</ci><ci id="S3.p1.6.m6.2.3.2.cmml" xref="S3.p1.6.m6.2.3.2">𝐺</ci><apply id="S3.p1.6.m6.2.3.3.cmml" xref="S3.p1.6.m6.2.3.3"><csymbol cd="latexml" id="S3.p1.6.m6.2.3.3.1.cmml" xref="S3.p1.6.m6.2.3.3.1">maps-to</csymbol><interval closure="open" id="S3.p1.6.m6.2.3.3.2.1.cmml" xref="S3.p1.6.m6.2.3.3.2.2"><ci id="S3.p1.6.m6.1.1.cmml" xref="S3.p1.6.m6.1.1">𝐴</ci><ci id="S3.p1.6.m6.2.2.cmml" xref="S3.p1.6.m6.2.2">𝐼</ci></interval><apply id="S3.p1.6.m6.2.3.3.3.cmml" xref="S3.p1.6.m6.2.3.3.3"><csymbol cd="ambiguous" id="S3.p1.6.m6.2.3.3.3.1.cmml" xref="S3.p1.6.m6.2.3.3.3">superscript</csymbol><ci id="S3.p1.6.m6.2.3.3.3.2.cmml" xref="S3.p1.6.m6.2.3.3.3.2">𝑄</ci><ci id="S3.p1.6.m6.2.3.3.3.3.cmml" xref="S3.p1.6.m6.2.3.3.3.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.6.m6.2c">G:(A,I)\mapsto Q^{\prime}</annotation></semantics></math> as in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>(b).
For a given <math id="S3.p1.7.m7.3" class="ltx_Math" alttext="(I,Q,A)" display="inline"><semantics id="S3.p1.7.m7.3a"><mrow id="S3.p1.7.m7.3.4.2" xref="S3.p1.7.m7.3.4.1.cmml"><mo stretchy="false" id="S3.p1.7.m7.3.4.2.1" xref="S3.p1.7.m7.3.4.1.cmml">(</mo><mi id="S3.p1.7.m7.1.1" xref="S3.p1.7.m7.1.1.cmml">I</mi><mo id="S3.p1.7.m7.3.4.2.2" xref="S3.p1.7.m7.3.4.1.cmml">,</mo><mi id="S3.p1.7.m7.2.2" xref="S3.p1.7.m7.2.2.cmml">Q</mi><mo id="S3.p1.7.m7.3.4.2.3" xref="S3.p1.7.m7.3.4.1.cmml">,</mo><mi id="S3.p1.7.m7.3.3" xref="S3.p1.7.m7.3.3.cmml">A</mi><mo stretchy="false" id="S3.p1.7.m7.3.4.2.4" xref="S3.p1.7.m7.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.7.m7.3b"><vector id="S3.p1.7.m7.3.4.1.cmml" xref="S3.p1.7.m7.3.4.2"><ci id="S3.p1.7.m7.1.1.cmml" xref="S3.p1.7.m7.1.1">𝐼</ci><ci id="S3.p1.7.m7.2.2.cmml" xref="S3.p1.7.m7.2.2">𝑄</ci><ci id="S3.p1.7.m7.3.3.cmml" xref="S3.p1.7.m7.3.3">𝐴</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.7.m7.3c">(I,Q,A)</annotation></semantics></math> triplet, we first obtain an answer prediction <math id="S3.p1.8.m8.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S3.p1.8.m8.1a"><msup id="S3.p1.8.m8.1.1" xref="S3.p1.8.m8.1.1.cmml"><mi id="S3.p1.8.m8.1.1.2" xref="S3.p1.8.m8.1.1.2.cmml">A</mi><mo id="S3.p1.8.m8.1.1.3" xref="S3.p1.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.8.m8.1b"><apply id="S3.p1.8.m8.1.1.cmml" xref="S3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p1.8.m8.1.1.1.cmml" xref="S3.p1.8.m8.1.1">superscript</csymbol><ci id="S3.p1.8.m8.1.1.2.cmml" xref="S3.p1.8.m8.1.1.2">𝐴</ci><ci id="S3.p1.8.m8.1.1.3.cmml" xref="S3.p1.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.8.m8.1c">A^{\prime}</annotation></semantics></math> using the VQA model <math id="S3.p1.9.m9.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.p1.9.m9.1a"><mi id="S3.p1.9.m9.1.1" xref="S3.p1.9.m9.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.p1.9.m9.1b"><ci id="S3.p1.9.m9.1.1.cmml" xref="S3.p1.9.m9.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.9.m9.1c">F</annotation></semantics></math> for the original question <math id="S3.p1.10.m10.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.10.m10.1a"><mi id="S3.p1.10.m10.1.1" xref="S3.p1.10.m10.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.10.m10.1b"><ci id="S3.p1.10.m10.1.1.cmml" xref="S3.p1.10.m10.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.10.m10.1c">Q</annotation></semantics></math>.
We then use the predicted answer <math id="S3.p1.11.m11.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S3.p1.11.m11.1a"><msup id="S3.p1.11.m11.1.1" xref="S3.p1.11.m11.1.1.cmml"><mi id="S3.p1.11.m11.1.1.2" xref="S3.p1.11.m11.1.1.2.cmml">A</mi><mo id="S3.p1.11.m11.1.1.3" xref="S3.p1.11.m11.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.11.m11.1b"><apply id="S3.p1.11.m11.1.1.cmml" xref="S3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.p1.11.m11.1.1.1.cmml" xref="S3.p1.11.m11.1.1">superscript</csymbol><ci id="S3.p1.11.m11.1.1.2.cmml" xref="S3.p1.11.m11.1.1.2">𝐴</ci><ci id="S3.p1.11.m11.1.1.3.cmml" xref="S3.p1.11.m11.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.11.m11.1c">A^{\prime}</annotation></semantics></math> and the image <math id="S3.p1.12.m12.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.12.m12.1a"><mi id="S3.p1.12.m12.1.1" xref="S3.p1.12.m12.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.12.m12.1b"><ci id="S3.p1.12.m12.1.1.cmml" xref="S3.p1.12.m12.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.12.m12.1c">I</annotation></semantics></math> to generate a question <math id="S3.p1.13.m13.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.p1.13.m13.1a"><msup id="S3.p1.13.m13.1.1" xref="S3.p1.13.m13.1.1.cmml"><mi id="S3.p1.13.m13.1.1.2" xref="S3.p1.13.m13.1.1.2.cmml">Q</mi><mo id="S3.p1.13.m13.1.1.3" xref="S3.p1.13.m13.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.13.m13.1b"><apply id="S3.p1.13.m13.1.1.cmml" xref="S3.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.p1.13.m13.1.1.1.cmml" xref="S3.p1.13.m13.1.1">superscript</csymbol><ci id="S3.p1.13.m13.1.1.2.cmml" xref="S3.p1.13.m13.1.1.2">𝑄</ci><ci id="S3.p1.13.m13.1.1.3.cmml" xref="S3.p1.13.m13.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.13.m13.1c">Q^{\prime}</annotation></semantics></math> which is semantically similar to <math id="S3.p1.14.m14.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.14.m14.1a"><mi id="S3.p1.14.m14.1.1" xref="S3.p1.14.m14.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.14.m14.1b"><ci id="S3.p1.14.m14.1.1.cmml" xref="S3.p1.14.m14.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.14.m14.1c">Q</annotation></semantics></math> using the VQG model <math id="S3.p1.15.m15.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.p1.15.m15.1a"><mi id="S3.p1.15.m15.1.1" xref="S3.p1.15.m15.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.p1.15.m15.1b"><ci id="S3.p1.15.m15.1.1.cmml" xref="S3.p1.15.m15.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.15.m15.1c">G</annotation></semantics></math>.
Lastly, we obtain a answer prediction <math id="S3.p1.16.m16.1" class="ltx_Math" alttext="A^{\prime\prime}" display="inline"><semantics id="S3.p1.16.m16.1a"><msup id="S3.p1.16.m16.1.1" xref="S3.p1.16.m16.1.1.cmml"><mi id="S3.p1.16.m16.1.1.2" xref="S3.p1.16.m16.1.1.2.cmml">A</mi><mo id="S3.p1.16.m16.1.1.3" xref="S3.p1.16.m16.1.1.3.cmml">′′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.16.m16.1b"><apply id="S3.p1.16.m16.1.1.cmml" xref="S3.p1.16.m16.1.1"><csymbol cd="ambiguous" id="S3.p1.16.m16.1.1.1.cmml" xref="S3.p1.16.m16.1.1">superscript</csymbol><ci id="S3.p1.16.m16.1.1.2.cmml" xref="S3.p1.16.m16.1.1.2">𝐴</ci><ci id="S3.p1.16.m16.1.1.3.cmml" xref="S3.p1.16.m16.1.1.3">′′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.16.m16.1c">A^{\prime\prime}</annotation></semantics></math> for the generated question <math id="S3.p1.17.m17.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.p1.17.m17.1a"><msup id="S3.p1.17.m17.1.1" xref="S3.p1.17.m17.1.1.cmml"><mi id="S3.p1.17.m17.1.1.2" xref="S3.p1.17.m17.1.1.2.cmml">Q</mi><mo id="S3.p1.17.m17.1.1.3" xref="S3.p1.17.m17.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.17.m17.1b"><apply id="S3.p1.17.m17.1.1.cmml" xref="S3.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.p1.17.m17.1.1.1.cmml" xref="S3.p1.17.m17.1.1">superscript</csymbol><ci id="S3.p1.17.m17.1.1.2.cmml" xref="S3.p1.17.m17.1.1.2">𝑄</ci><ci id="S3.p1.17.m17.1.1.3.cmml" xref="S3.p1.17.m17.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.17.m17.1c">Q^{\prime}</annotation></semantics></math>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.3" class="ltx_p">Our design of consistency components is inspired by two beliefs.
Firstly, a model which can generate a semantically and syntactically correct question given a answer and an image, has a better understanding of the cross-modal connections among the image, the question and the answer, which make them a valid <math id="S3.p2.1.m1.3" class="ltx_Math" alttext="(I,Q,A)" display="inline"><semantics id="S3.p2.1.m1.3a"><mrow id="S3.p2.1.m1.3.4.2" xref="S3.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.p2.1.m1.3.4.2.1" xref="S3.p2.1.m1.3.4.1.cmml">(</mo><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">I</mi><mo id="S3.p2.1.m1.3.4.2.2" xref="S3.p2.1.m1.3.4.1.cmml">,</mo><mi id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">Q</mi><mo id="S3.p2.1.m1.3.4.2.3" xref="S3.p2.1.m1.3.4.1.cmml">,</mo><mi id="S3.p2.1.m1.3.3" xref="S3.p2.1.m1.3.3.cmml">A</mi><mo stretchy="false" id="S3.p2.1.m1.3.4.2.4" xref="S3.p2.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.3b"><vector id="S3.p2.1.m1.3.4.1.cmml" xref="S3.p2.1.m1.3.4.2"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝐼</ci><ci id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">𝑄</ci><ci id="S3.p2.1.m1.3.3.cmml" xref="S3.p2.1.m1.3.3">𝐴</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.3c">(I,Q,A)</annotation></semantics></math> triplet.
Secondly, assuming the generated question <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.p2.2.m2.1a"><msup id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">Q</mi><mo id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">superscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝑄</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">Q^{\prime}</annotation></semantics></math> is a valid rephrasing of the original question, a robust VQA model should answer this rephrasing with the same answer as the original question <math id="S3.p2.3.m3.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p2.3.m3.1a"><mi id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">Q</annotation></semantics></math>.
In practice, however, there are several challenges that inhibit enforcement of cycle-consistency in VQA. We discuss these challenges and describe the key components of our framework geared to tackle them in the following sections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Question Generation Module</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">Since VQA is a setting where there is high disparity in the information content of involved modalities (a question and answer pair is a very lossy compressed representation of the image), learning transformations that map one modality to another is non-trivial. In cycle-consistent models dealing with single-modalities, transformations need to be learned across different domains of the same modality (image or text) with roughly similar information contents. However in a multi-modality transformation like VQG, learning a transformation from a low information modality (such as answer) to high information modality (question) needs additional supervision.
We provide this additional supervision to the VQG model in the form of attention. To generate a rephrasing <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msup id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">Q</mi><mo id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑄</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">Q^{\prime}</annotation></semantics></math>, the VQG is guided to attend at regions of the image which were used by the VQA model to answer the original question <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">Q</annotation></semantics></math>. Unlike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, this enables our models to generate questions <span id="S3.SS1.p1.2.1" class="ltx_text" style="color:#000000;">more similar</span> to the original question from answers like “yes”, which could possibly have a large space of plausible questions.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We model the question generation module <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">G</annotation></semantics></math> in a fashion similar to a conditional image captioning model. The question generation module consists of two linear encoders that transform attended image features obtained from VQA model and the distribution over answer space to lower dimensional feature vectors. We sum these feature vectors with additive noise and pass them through an LSTM which is trained to reconstruct the original question and optimized by minimizing the negative log likelihood with teacher-forcing. Note that unlike <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> we do not pass the one-hot vector representing the answer obtained, or an embedding of the answer obtained to the question generation, but rather the predicted distribution over answers. This enables the question generation module to learn to map the model’s confidence over answers to the generated question.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.10" class="ltx_p"><span id="S3.SS1.p3.10.10" class="ltx_text" style="color:#000000;">Throughout the paper, <span id="S3.SS1.p3.10.10.1" class="ltx_text ltx_font_bold">Q-consistency</span> implies addition of a VQG module <math id="S3.SS1.p3.1.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p3.1.1.m1.1a"><mi mathcolor="#000000" id="S3.SS1.p3.1.1.m1.1.1" xref="S3.SS1.p3.1.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.1.m1.1b"><ci id="S3.SS1.p3.1.1.m1.1.1.cmml" xref="S3.SS1.p3.1.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.1.m1.1c">G</annotation></semantics></math> on top of the base VQA model <math id="S3.SS1.p3.2.2.m2.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS1.p3.2.2.m2.1a"><mi mathcolor="#000000" id="S3.SS1.p3.2.2.m2.1.1" xref="S3.SS1.p3.2.2.m2.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.2.m2.1b"><ci id="S3.SS1.p3.2.2.m2.1.1.cmml" xref="S3.SS1.p3.2.2.m2.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.2.m2.1c">F</annotation></semantics></math> to generate rephrasings <math id="S3.SS1.p3.3.3.m3.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.SS1.p3.3.3.m3.1a"><msup id="S3.SS1.p3.3.3.m3.1.1" xref="S3.SS1.p3.3.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p3.3.3.m3.1.1.2" xref="S3.SS1.p3.3.3.m3.1.1.2.cmml">Q</mi><mo mathcolor="#000000" id="S3.SS1.p3.3.3.m3.1.1.3" xref="S3.SS1.p3.3.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.3.m3.1b"><apply id="S3.SS1.p3.3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p3.3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.3.m3.1.1.2">𝑄</ci><ci id="S3.SS1.p3.3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.3.m3.1c">Q^{\prime}</annotation></semantics></math> from the image <math id="S3.SS1.p3.4.4.m4.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.p3.4.4.m4.1a"><mi mathcolor="#000000" id="S3.SS1.p3.4.4.m4.1.1" xref="S3.SS1.p3.4.4.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.4.m4.1b"><ci id="S3.SS1.p3.4.4.m4.1.1.cmml" xref="S3.SS1.p3.4.4.m4.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.4.m4.1c">I</annotation></semantics></math> and the predicted answer <math id="S3.SS1.p3.5.5.m5.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S3.SS1.p3.5.5.m5.1a"><msup id="S3.SS1.p3.5.5.m5.1.1" xref="S3.SS1.p3.5.5.m5.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p3.5.5.m5.1.1.2" xref="S3.SS1.p3.5.5.m5.1.1.2.cmml">A</mi><mo mathcolor="#000000" id="S3.SS1.p3.5.5.m5.1.1.3" xref="S3.SS1.p3.5.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.5.m5.1b"><apply id="S3.SS1.p3.5.5.m5.1.1.cmml" xref="S3.SS1.p3.5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.5.5.m5.1.1.1.cmml" xref="S3.SS1.p3.5.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p3.5.5.m5.1.1.2.cmml" xref="S3.SS1.p3.5.5.m5.1.1.2">𝐴</ci><ci id="S3.SS1.p3.5.5.m5.1.1.3.cmml" xref="S3.SS1.p3.5.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.5.m5.1c">A^{\prime}</annotation></semantics></math> with an associated Q-consistency loss <math id="S3.SS1.p3.6.6.m6.2" class="ltx_Math" alttext="\mathcal{L}_{G}(Q,Q^{\prime})" display="inline"><semantics id="S3.SS1.p3.6.6.m6.2a"><mrow id="S3.SS1.p3.6.6.m6.2.2" xref="S3.SS1.p3.6.6.m6.2.2.cmml"><msub id="S3.SS1.p3.6.6.m6.2.2.3" xref="S3.SS1.p3.6.6.m6.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" mathcolor="#000000" id="S3.SS1.p3.6.6.m6.2.2.3.2" xref="S3.SS1.p3.6.6.m6.2.2.3.2.cmml">ℒ</mi><mi mathcolor="#000000" id="S3.SS1.p3.6.6.m6.2.2.3.3" xref="S3.SS1.p3.6.6.m6.2.2.3.3.cmml">G</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.6.6.m6.2.2.2" xref="S3.SS1.p3.6.6.m6.2.2.2.cmml">​</mo><mrow id="S3.SS1.p3.6.6.m6.2.2.1.1" xref="S3.SS1.p3.6.6.m6.2.2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p3.6.6.m6.2.2.1.1.2" xref="S3.SS1.p3.6.6.m6.2.2.1.2.cmml">(</mo><mi mathcolor="#000000" id="S3.SS1.p3.6.6.m6.1.1" xref="S3.SS1.p3.6.6.m6.1.1.cmml">Q</mi><mo mathcolor="#000000" id="S3.SS1.p3.6.6.m6.2.2.1.1.3" xref="S3.SS1.p3.6.6.m6.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p3.6.6.m6.2.2.1.1.1" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p3.6.6.m6.2.2.1.1.1.2" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1.2.cmml">Q</mi><mo mathcolor="#000000" id="S3.SS1.p3.6.6.m6.2.2.1.1.1.3" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1.3.cmml">′</mo></msup><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p3.6.6.m6.2.2.1.1.4" xref="S3.SS1.p3.6.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.6.m6.2b"><apply id="S3.SS1.p3.6.6.m6.2.2.cmml" xref="S3.SS1.p3.6.6.m6.2.2"><times id="S3.SS1.p3.6.6.m6.2.2.2.cmml" xref="S3.SS1.p3.6.6.m6.2.2.2"></times><apply id="S3.SS1.p3.6.6.m6.2.2.3.cmml" xref="S3.SS1.p3.6.6.m6.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.6.6.m6.2.2.3.1.cmml" xref="S3.SS1.p3.6.6.m6.2.2.3">subscript</csymbol><ci id="S3.SS1.p3.6.6.m6.2.2.3.2.cmml" xref="S3.SS1.p3.6.6.m6.2.2.3.2">ℒ</ci><ci id="S3.SS1.p3.6.6.m6.2.2.3.3.cmml" xref="S3.SS1.p3.6.6.m6.2.2.3.3">𝐺</ci></apply><interval closure="open" id="S3.SS1.p3.6.6.m6.2.2.1.2.cmml" xref="S3.SS1.p3.6.6.m6.2.2.1.1"><ci id="S3.SS1.p3.6.6.m6.1.1.cmml" xref="S3.SS1.p3.6.6.m6.1.1">𝑄</ci><apply id="S3.SS1.p3.6.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.6.m6.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.6.6.m6.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1.2">𝑄</ci><ci id="S3.SS1.p3.6.6.m6.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.6.6.m6.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.6.m6.2c">\mathcal{L}_{G}(Q,Q^{\prime})</annotation></semantics></math>.
Similarly, <span id="S3.SS1.p3.10.10.2" class="ltx_text ltx_font_bold">A-consistency</span> implies passing all questions generated <math id="S3.SS1.p3.7.7.m7.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S3.SS1.p3.7.7.m7.1a"><msup id="S3.SS1.p3.7.7.m7.1.1" xref="S3.SS1.p3.7.7.m7.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p3.7.7.m7.1.1.2" xref="S3.SS1.p3.7.7.m7.1.1.2.cmml">Q</mi><mo mathcolor="#000000" id="S3.SS1.p3.7.7.m7.1.1.3" xref="S3.SS1.p3.7.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.7.7.m7.1b"><apply id="S3.SS1.p3.7.7.m7.1.1.cmml" xref="S3.SS1.p3.7.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.7.7.m7.1.1.1.cmml" xref="S3.SS1.p3.7.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.p3.7.7.m7.1.1.2.cmml" xref="S3.SS1.p3.7.7.m7.1.1.2">𝑄</ci><ci id="S3.SS1.p3.7.7.m7.1.1.3.cmml" xref="S3.SS1.p3.7.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.7.7.m7.1c">Q^{\prime}</annotation></semantics></math> by the VQG Model <math id="S3.SS1.p3.8.8.m8.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS1.p3.8.8.m8.1a"><mi mathcolor="#000000" id="S3.SS1.p3.8.8.m8.1.1" xref="S3.SS1.p3.8.8.m8.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.8.8.m8.1b"><ci id="S3.SS1.p3.8.8.m8.1.1.cmml" xref="S3.SS1.p3.8.8.m8.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.8.8.m8.1c">G</annotation></semantics></math> to the VQA model <math id="S3.SS1.p3.9.9.m9.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS1.p3.9.9.m9.1a"><mi mathcolor="#000000" id="S3.SS1.p3.9.9.m9.1.1" xref="S3.SS1.p3.9.9.m9.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.9.9.m9.1b"><ci id="S3.SS1.p3.9.9.m9.1.1.cmml" xref="S3.SS1.p3.9.9.m9.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.9.9.m9.1c">F</annotation></semantics></math> and an associated A-consistency loss <math id="S3.SS1.p3.10.10.m10.2" class="ltx_Math" alttext="\mathcal{L}_{cycle}(A,A^{\prime\prime})" display="inline"><semantics id="S3.SS1.p3.10.10.m10.2a"><mrow id="S3.SS1.p3.10.10.m10.2.2" xref="S3.SS1.p3.10.10.m10.2.2.cmml"><msub id="S3.SS1.p3.10.10.m10.2.2.3" xref="S3.SS1.p3.10.10.m10.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.3.2" xref="S3.SS1.p3.10.10.m10.2.2.3.2.cmml">ℒ</mi><mrow id="S3.SS1.p3.10.10.m10.2.2.3.3" xref="S3.SS1.p3.10.10.m10.2.2.3.3.cmml"><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.3.3.2" xref="S3.SS1.p3.10.10.m10.2.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.10.10.m10.2.2.3.3.1" xref="S3.SS1.p3.10.10.m10.2.2.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.3.3.3" xref="S3.SS1.p3.10.10.m10.2.2.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.10.10.m10.2.2.3.3.1a" xref="S3.SS1.p3.10.10.m10.2.2.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.3.3.4" xref="S3.SS1.p3.10.10.m10.2.2.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.10.10.m10.2.2.3.3.1b" xref="S3.SS1.p3.10.10.m10.2.2.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.3.3.5" xref="S3.SS1.p3.10.10.m10.2.2.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.10.10.m10.2.2.3.3.1c" xref="S3.SS1.p3.10.10.m10.2.2.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.3.3.6" xref="S3.SS1.p3.10.10.m10.2.2.3.3.6.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.10.10.m10.2.2.2" xref="S3.SS1.p3.10.10.m10.2.2.2.cmml">​</mo><mrow id="S3.SS1.p3.10.10.m10.2.2.1.1" xref="S3.SS1.p3.10.10.m10.2.2.1.2.cmml"><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p3.10.10.m10.2.2.1.1.2" xref="S3.SS1.p3.10.10.m10.2.2.1.2.cmml">(</mo><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.1.1" xref="S3.SS1.p3.10.10.m10.1.1.cmml">A</mi><mo mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.1.1.3" xref="S3.SS1.p3.10.10.m10.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p3.10.10.m10.2.2.1.1.1" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1.cmml"><mi mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.1.1.1.2" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1.2.cmml">A</mi><mo mathcolor="#000000" id="S3.SS1.p3.10.10.m10.2.2.1.1.1.3" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1.3.cmml">′′</mo></msup><mo mathcolor="#000000" stretchy="false" id="S3.SS1.p3.10.10.m10.2.2.1.1.4" xref="S3.SS1.p3.10.10.m10.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.10.10.m10.2b"><apply id="S3.SS1.p3.10.10.m10.2.2.cmml" xref="S3.SS1.p3.10.10.m10.2.2"><times id="S3.SS1.p3.10.10.m10.2.2.2.cmml" xref="S3.SS1.p3.10.10.m10.2.2.2"></times><apply id="S3.SS1.p3.10.10.m10.2.2.3.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.10.10.m10.2.2.3.1.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3">subscript</csymbol><ci id="S3.SS1.p3.10.10.m10.2.2.3.2.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.2">ℒ</ci><apply id="S3.SS1.p3.10.10.m10.2.2.3.3.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3"><times id="S3.SS1.p3.10.10.m10.2.2.3.3.1.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3.1"></times><ci id="S3.SS1.p3.10.10.m10.2.2.3.3.2.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3.2">𝑐</ci><ci id="S3.SS1.p3.10.10.m10.2.2.3.3.3.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3.3">𝑦</ci><ci id="S3.SS1.p3.10.10.m10.2.2.3.3.4.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3.4">𝑐</ci><ci id="S3.SS1.p3.10.10.m10.2.2.3.3.5.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3.5">𝑙</ci><ci id="S3.SS1.p3.10.10.m10.2.2.3.3.6.cmml" xref="S3.SS1.p3.10.10.m10.2.2.3.3.6">𝑒</ci></apply></apply><interval closure="open" id="S3.SS1.p3.10.10.m10.2.2.1.2.cmml" xref="S3.SS1.p3.10.10.m10.2.2.1.1"><ci id="S3.SS1.p3.10.10.m10.1.1.cmml" xref="S3.SS1.p3.10.10.m10.1.1">𝐴</ci><apply id="S3.SS1.p3.10.10.m10.2.2.1.1.1.cmml" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.10.10.m10.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.10.10.m10.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1.2">𝐴</ci><ci id="S3.SS1.p3.10.10.m10.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.10.10.m10.2.2.1.1.1.3">′′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.10.10.m10.2c">\mathcal{L}_{cycle}(A,A^{\prime\prime})</annotation></semantics></math>.
</span>
The overall loss can be written as:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.39" class="ltx_Math" alttext="\begin{split}\mathcal{L}_{total}=\mathcal{L}_{F}(A,A^{\prime})+\lambda_{G}\mathcal{L}_{G}(Q,Q^{\prime})\\
+\lambda_{C}\mathcal{L}_{cycle}(A,A^{\prime\prime})\end{split}" display="block"><semantics id="S3.E1.m1.39a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.39.39.6" xref="S3.E1.m1.36.36.3.cmml"><mtr id="S3.E1.m1.39.39.6a" xref="S3.E1.m1.36.36.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.39.39.6b" xref="S3.E1.m1.36.36.3.cmml"><mrow id="S3.E1.m1.38.38.5.35.24.24" xref="S3.E1.m1.36.36.3.cmml"><msub id="S3.E1.m1.38.38.5.35.24.24.25" xref="S3.E1.m1.36.36.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">ℒ</mi><mrow id="S3.E1.m1.2.2.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.E1.m1.2.2.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.2.2.1.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.2.2.1.3" xref="S3.E1.m1.2.2.2.2.2.2.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.2.1.1a" xref="S3.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.2.2.1.4" xref="S3.E1.m1.2.2.2.2.2.2.1.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.2.1.1b" xref="S3.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.2.2.1.5" xref="S3.E1.m1.2.2.2.2.2.2.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2.2.1.1c" xref="S3.E1.m1.2.2.2.2.2.2.1.1.cmml">​</mo><mi id="S3.E1.m1.2.2.2.2.2.2.1.6" xref="S3.E1.m1.2.2.2.2.2.2.1.6.cmml">l</mi></mrow></msub><mo id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.E1.m1.38.38.5.35.24.24.24" xref="S3.E1.m1.36.36.3.cmml"><mrow id="S3.E1.m1.37.37.4.34.23.23.23.1" xref="S3.E1.m1.36.36.3.cmml"><msub id="S3.E1.m1.37.37.4.34.23.23.23.1.3" xref="S3.E1.m1.36.36.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.4.cmml">ℒ</mi><mi id="S3.E1.m1.5.5.5.5.5.5.1" xref="S3.E1.m1.5.5.5.5.5.5.1.cmml">F</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.37.37.4.34.23.23.23.1.2" xref="S3.E1.m1.36.36.3.cmml">​</mo><mrow id="S3.E1.m1.37.37.4.34.23.23.23.1.1.1" xref="S3.E1.m1.36.36.3.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.36.36.3.cmml">(</mo><mi id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.7.7.7.7.7.7.cmml">A</mi><mo id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.36.36.3.cmml">,</mo><msup id="S3.E1.m1.37.37.4.34.23.23.23.1.1.1.1" xref="S3.E1.m1.36.36.3.cmml"><mi id="S3.E1.m1.9.9.9.9.9.9" xref="S3.E1.m1.9.9.9.9.9.9.cmml">A</mi><mo id="S3.E1.m1.10.10.10.10.10.10.1" xref="S3.E1.m1.10.10.10.10.10.10.1.cmml">′</mo></msup><mo stretchy="false" id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.36.36.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.12.12.12.12.12.12" xref="S3.E1.m1.12.12.12.12.12.12.cmml">+</mo><mrow id="S3.E1.m1.38.38.5.35.24.24.24.2" xref="S3.E1.m1.36.36.3.cmml"><msub id="S3.E1.m1.38.38.5.35.24.24.24.2.3" xref="S3.E1.m1.36.36.3.cmml"><mi id="S3.E1.m1.13.13.13.13.13.13" xref="S3.E1.m1.13.13.13.13.13.13.cmml">λ</mi><mi id="S3.E1.m1.14.14.14.14.14.14.1" xref="S3.E1.m1.14.14.14.14.14.14.1.cmml">G</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.38.38.5.35.24.24.24.2.2" xref="S3.E1.m1.36.36.3.cmml">​</mo><msub id="S3.E1.m1.38.38.5.35.24.24.24.2.4" xref="S3.E1.m1.36.36.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.15.15.15.15.15.15.cmml">ℒ</mi><mi id="S3.E1.m1.16.16.16.16.16.16.1" xref="S3.E1.m1.16.16.16.16.16.16.1.cmml">G</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.38.38.5.35.24.24.24.2.2a" xref="S3.E1.m1.36.36.3.cmml">​</mo><mrow id="S3.E1.m1.38.38.5.35.24.24.24.2.1.1" xref="S3.E1.m1.36.36.3.cmml"><mo stretchy="false" id="S3.E1.m1.17.17.17.17.17.17" xref="S3.E1.m1.36.36.3.cmml">(</mo><mi id="S3.E1.m1.18.18.18.18.18.18" xref="S3.E1.m1.18.18.18.18.18.18.cmml">Q</mi><mo id="S3.E1.m1.19.19.19.19.19.19" xref="S3.E1.m1.36.36.3.cmml">,</mo><msup id="S3.E1.m1.38.38.5.35.24.24.24.2.1.1.1" xref="S3.E1.m1.36.36.3.cmml"><mi id="S3.E1.m1.20.20.20.20.20.20" xref="S3.E1.m1.20.20.20.20.20.20.cmml">Q</mi><mo id="S3.E1.m1.21.21.21.21.21.21.1" xref="S3.E1.m1.21.21.21.21.21.21.1.cmml">′</mo></msup><mo stretchy="false" id="S3.E1.m1.22.22.22.22.22.22" xref="S3.E1.m1.36.36.3.cmml">)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.39.39.6c" xref="S3.E1.m1.36.36.3.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.39.39.6d" xref="S3.E1.m1.36.36.3.cmml"><mrow id="S3.E1.m1.39.39.6.36.12.12" xref="S3.E1.m1.36.36.3.cmml"><mo id="S3.E1.m1.39.39.6.36.12.12a" xref="S3.E1.m1.36.36.3.cmml">+</mo><mrow id="S3.E1.m1.39.39.6.36.12.12.12" xref="S3.E1.m1.36.36.3.cmml"><msub id="S3.E1.m1.39.39.6.36.12.12.12.3" xref="S3.E1.m1.36.36.3.cmml"><mi id="S3.E1.m1.24.24.24.2.2.2" xref="S3.E1.m1.24.24.24.2.2.2.cmml">λ</mi><mi id="S3.E1.m1.25.25.25.3.3.3.1" xref="S3.E1.m1.25.25.25.3.3.3.1.cmml">C</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.39.39.6.36.12.12.12.2" xref="S3.E1.m1.36.36.3.cmml">​</mo><msub id="S3.E1.m1.39.39.6.36.12.12.12.4" xref="S3.E1.m1.36.36.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.26.26.26.4.4.4" xref="S3.E1.m1.26.26.26.4.4.4.cmml">ℒ</mi><mrow id="S3.E1.m1.27.27.27.5.5.5.1" xref="S3.E1.m1.27.27.27.5.5.5.1.cmml"><mi id="S3.E1.m1.27.27.27.5.5.5.1.2" xref="S3.E1.m1.27.27.27.5.5.5.1.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.27.27.27.5.5.5.1.1" xref="S3.E1.m1.27.27.27.5.5.5.1.1.cmml">​</mo><mi id="S3.E1.m1.27.27.27.5.5.5.1.3" xref="S3.E1.m1.27.27.27.5.5.5.1.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.27.27.27.5.5.5.1.1a" xref="S3.E1.m1.27.27.27.5.5.5.1.1.cmml">​</mo><mi id="S3.E1.m1.27.27.27.5.5.5.1.4" xref="S3.E1.m1.27.27.27.5.5.5.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.27.27.27.5.5.5.1.1b" xref="S3.E1.m1.27.27.27.5.5.5.1.1.cmml">​</mo><mi id="S3.E1.m1.27.27.27.5.5.5.1.5" xref="S3.E1.m1.27.27.27.5.5.5.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.27.27.27.5.5.5.1.1c" xref="S3.E1.m1.27.27.27.5.5.5.1.1.cmml">​</mo><mi id="S3.E1.m1.27.27.27.5.5.5.1.6" xref="S3.E1.m1.27.27.27.5.5.5.1.6.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.39.39.6.36.12.12.12.2a" xref="S3.E1.m1.36.36.3.cmml">​</mo><mrow id="S3.E1.m1.39.39.6.36.12.12.12.1.1" xref="S3.E1.m1.36.36.3.cmml"><mo stretchy="false" id="S3.E1.m1.28.28.28.6.6.6" xref="S3.E1.m1.36.36.3.cmml">(</mo><mi id="S3.E1.m1.29.29.29.7.7.7" xref="S3.E1.m1.29.29.29.7.7.7.cmml">A</mi><mo id="S3.E1.m1.30.30.30.8.8.8" xref="S3.E1.m1.36.36.3.cmml">,</mo><msup id="S3.E1.m1.39.39.6.36.12.12.12.1.1.1" xref="S3.E1.m1.36.36.3.cmml"><mi id="S3.E1.m1.31.31.31.9.9.9" xref="S3.E1.m1.31.31.31.9.9.9.cmml">A</mi><mo id="S3.E1.m1.32.32.32.10.10.10.1" xref="S3.E1.m1.32.32.32.10.10.10.1.cmml">′′</mo></msup><mo stretchy="false" id="S3.E1.m1.33.33.33.11.11.11" xref="S3.E1.m1.36.36.3.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.39b"><apply id="S3.E1.m1.36.36.3.cmml" xref="S3.E1.m1.39.39.6"><eq id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></eq><apply id="S3.E1.m1.36.36.3.5.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.36.36.3.5.1.cmml" xref="S3.E1.m1.39.39.6">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">ℒ</ci><apply id="S3.E1.m1.2.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.1"></times><ci id="S3.E1.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.2">𝑡</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.3">𝑜</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.4.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.4">𝑡</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.5.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.5">𝑎</ci><ci id="S3.E1.m1.2.2.2.2.2.2.1.6.cmml" xref="S3.E1.m1.2.2.2.2.2.2.1.6">𝑙</ci></apply></apply><apply id="S3.E1.m1.36.36.3.3.cmml" xref="S3.E1.m1.39.39.6"><plus id="S3.E1.m1.12.12.12.12.12.12.cmml" xref="S3.E1.m1.12.12.12.12.12.12"></plus><apply id="S3.E1.m1.34.34.1.1.1.cmml" xref="S3.E1.m1.39.39.6"><times id="S3.E1.m1.34.34.1.1.1.2.cmml" xref="S3.E1.m1.39.39.6"></times><apply id="S3.E1.m1.34.34.1.1.1.3.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.34.34.1.1.1.3.1.cmml" xref="S3.E1.m1.39.39.6">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4.4">ℒ</ci><ci id="S3.E1.m1.5.5.5.5.5.5.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.1">𝐹</ci></apply><interval closure="open" id="S3.E1.m1.34.34.1.1.1.1.2.cmml" xref="S3.E1.m1.39.39.6"><ci id="S3.E1.m1.7.7.7.7.7.7.cmml" xref="S3.E1.m1.7.7.7.7.7.7">𝐴</ci><apply id="S3.E1.m1.34.34.1.1.1.1.1.1.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.34.34.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.39.39.6">superscript</csymbol><ci id="S3.E1.m1.9.9.9.9.9.9.cmml" xref="S3.E1.m1.9.9.9.9.9.9">𝐴</ci><ci id="S3.E1.m1.10.10.10.10.10.10.1.cmml" xref="S3.E1.m1.10.10.10.10.10.10.1">′</ci></apply></interval></apply><apply id="S3.E1.m1.35.35.2.2.2.cmml" xref="S3.E1.m1.39.39.6"><times id="S3.E1.m1.35.35.2.2.2.2.cmml" xref="S3.E1.m1.39.39.6"></times><apply id="S3.E1.m1.35.35.2.2.2.3.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.35.35.2.2.2.3.1.cmml" xref="S3.E1.m1.39.39.6">subscript</csymbol><ci id="S3.E1.m1.13.13.13.13.13.13.cmml" xref="S3.E1.m1.13.13.13.13.13.13">𝜆</ci><ci id="S3.E1.m1.14.14.14.14.14.14.1.cmml" xref="S3.E1.m1.14.14.14.14.14.14.1">𝐺</ci></apply><apply id="S3.E1.m1.35.35.2.2.2.4.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.35.35.2.2.2.4.1.cmml" xref="S3.E1.m1.39.39.6">subscript</csymbol><ci id="S3.E1.m1.15.15.15.15.15.15.cmml" xref="S3.E1.m1.15.15.15.15.15.15">ℒ</ci><ci id="S3.E1.m1.16.16.16.16.16.16.1.cmml" xref="S3.E1.m1.16.16.16.16.16.16.1">𝐺</ci></apply><interval closure="open" id="S3.E1.m1.35.35.2.2.2.1.2.cmml" xref="S3.E1.m1.39.39.6"><ci id="S3.E1.m1.18.18.18.18.18.18.cmml" xref="S3.E1.m1.18.18.18.18.18.18">𝑄</ci><apply id="S3.E1.m1.35.35.2.2.2.1.1.1.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.35.35.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.39.39.6">superscript</csymbol><ci id="S3.E1.m1.20.20.20.20.20.20.cmml" xref="S3.E1.m1.20.20.20.20.20.20">𝑄</ci><ci id="S3.E1.m1.21.21.21.21.21.21.1.cmml" xref="S3.E1.m1.21.21.21.21.21.21.1">′</ci></apply></interval></apply><apply id="S3.E1.m1.36.36.3.3.3.cmml" xref="S3.E1.m1.39.39.6"><times id="S3.E1.m1.36.36.3.3.3.2.cmml" xref="S3.E1.m1.39.39.6"></times><apply id="S3.E1.m1.36.36.3.3.3.3.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.36.36.3.3.3.3.1.cmml" xref="S3.E1.m1.39.39.6">subscript</csymbol><ci id="S3.E1.m1.24.24.24.2.2.2.cmml" xref="S3.E1.m1.24.24.24.2.2.2">𝜆</ci><ci id="S3.E1.m1.25.25.25.3.3.3.1.cmml" xref="S3.E1.m1.25.25.25.3.3.3.1">𝐶</ci></apply><apply id="S3.E1.m1.36.36.3.3.3.4.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.36.36.3.3.3.4.1.cmml" xref="S3.E1.m1.39.39.6">subscript</csymbol><ci id="S3.E1.m1.26.26.26.4.4.4.cmml" xref="S3.E1.m1.26.26.26.4.4.4">ℒ</ci><apply id="S3.E1.m1.27.27.27.5.5.5.1.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1"><times id="S3.E1.m1.27.27.27.5.5.5.1.1.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1.1"></times><ci id="S3.E1.m1.27.27.27.5.5.5.1.2.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1.2">𝑐</ci><ci id="S3.E1.m1.27.27.27.5.5.5.1.3.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1.3">𝑦</ci><ci id="S3.E1.m1.27.27.27.5.5.5.1.4.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1.4">𝑐</ci><ci id="S3.E1.m1.27.27.27.5.5.5.1.5.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1.5">𝑙</ci><ci id="S3.E1.m1.27.27.27.5.5.5.1.6.cmml" xref="S3.E1.m1.27.27.27.5.5.5.1.6">𝑒</ci></apply></apply><interval closure="open" id="S3.E1.m1.36.36.3.3.3.1.2.cmml" xref="S3.E1.m1.39.39.6"><ci id="S3.E1.m1.29.29.29.7.7.7.cmml" xref="S3.E1.m1.29.29.29.7.7.7">𝐴</ci><apply id="S3.E1.m1.36.36.3.3.3.1.1.1.cmml" xref="S3.E1.m1.39.39.6"><csymbol cd="ambiguous" id="S3.E1.m1.36.36.3.3.3.1.1.1.1.cmml" xref="S3.E1.m1.39.39.6">superscript</csymbol><ci id="S3.E1.m1.31.31.31.9.9.9.cmml" xref="S3.E1.m1.31.31.31.9.9.9">𝐴</ci><ci id="S3.E1.m1.32.32.32.10.10.10.1.cmml" xref="S3.E1.m1.32.32.32.10.10.10.1">′′</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.39c">\begin{split}\mathcal{L}_{total}=\mathcal{L}_{F}(A,A^{\prime})+\lambda_{G}\mathcal{L}_{G}(Q,Q^{\prime})\\
+\lambda_{C}\mathcal{L}_{cycle}(A,A^{\prime\prime})\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p3.15" class="ltx_p">where <math id="S3.SS1.p3.11.m1.2" class="ltx_Math" alttext="\mathcal{L}_{F}(A,A^{\prime})" display="inline"><semantics id="S3.SS1.p3.11.m1.2a"><mrow id="S3.SS1.p3.11.m1.2.2" xref="S3.SS1.p3.11.m1.2.2.cmml"><msub id="S3.SS1.p3.11.m1.2.2.3" xref="S3.SS1.p3.11.m1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.11.m1.2.2.3.2" xref="S3.SS1.p3.11.m1.2.2.3.2.cmml">ℒ</mi><mi id="S3.SS1.p3.11.m1.2.2.3.3" xref="S3.SS1.p3.11.m1.2.2.3.3.cmml">F</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.11.m1.2.2.2" xref="S3.SS1.p3.11.m1.2.2.2.cmml">​</mo><mrow id="S3.SS1.p3.11.m1.2.2.1.1" xref="S3.SS1.p3.11.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.11.m1.2.2.1.1.2" xref="S3.SS1.p3.11.m1.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p3.11.m1.1.1" xref="S3.SS1.p3.11.m1.1.1.cmml">A</mi><mo id="S3.SS1.p3.11.m1.2.2.1.1.3" xref="S3.SS1.p3.11.m1.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p3.11.m1.2.2.1.1.1" xref="S3.SS1.p3.11.m1.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.11.m1.2.2.1.1.1.2" xref="S3.SS1.p3.11.m1.2.2.1.1.1.2.cmml">A</mi><mo id="S3.SS1.p3.11.m1.2.2.1.1.1.3" xref="S3.SS1.p3.11.m1.2.2.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.SS1.p3.11.m1.2.2.1.1.4" xref="S3.SS1.p3.11.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.11.m1.2b"><apply id="S3.SS1.p3.11.m1.2.2.cmml" xref="S3.SS1.p3.11.m1.2.2"><times id="S3.SS1.p3.11.m1.2.2.2.cmml" xref="S3.SS1.p3.11.m1.2.2.2"></times><apply id="S3.SS1.p3.11.m1.2.2.3.cmml" xref="S3.SS1.p3.11.m1.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.11.m1.2.2.3.1.cmml" xref="S3.SS1.p3.11.m1.2.2.3">subscript</csymbol><ci id="S3.SS1.p3.11.m1.2.2.3.2.cmml" xref="S3.SS1.p3.11.m1.2.2.3.2">ℒ</ci><ci id="S3.SS1.p3.11.m1.2.2.3.3.cmml" xref="S3.SS1.p3.11.m1.2.2.3.3">𝐹</ci></apply><interval closure="open" id="S3.SS1.p3.11.m1.2.2.1.2.cmml" xref="S3.SS1.p3.11.m1.2.2.1.1"><ci id="S3.SS1.p3.11.m1.1.1.cmml" xref="S3.SS1.p3.11.m1.1.1">𝐴</ci><apply id="S3.SS1.p3.11.m1.2.2.1.1.1.cmml" xref="S3.SS1.p3.11.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.11.m1.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.11.m1.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.11.m1.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.11.m1.2.2.1.1.1.2">𝐴</ci><ci id="S3.SS1.p3.11.m1.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.11.m1.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.11.m1.2c">\mathcal{L}_{F}(A,A^{\prime})</annotation></semantics></math> and <math id="S3.SS1.p3.12.m2.2" class="ltx_Math" alttext="\mathcal{L}_{cycle}(A,A^{\prime\prime})" display="inline"><semantics id="S3.SS1.p3.12.m2.2a"><mrow id="S3.SS1.p3.12.m2.2.2" xref="S3.SS1.p3.12.m2.2.2.cmml"><msub id="S3.SS1.p3.12.m2.2.2.3" xref="S3.SS1.p3.12.m2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.12.m2.2.2.3.2" xref="S3.SS1.p3.12.m2.2.2.3.2.cmml">ℒ</mi><mrow id="S3.SS1.p3.12.m2.2.2.3.3" xref="S3.SS1.p3.12.m2.2.2.3.3.cmml"><mi id="S3.SS1.p3.12.m2.2.2.3.3.2" xref="S3.SS1.p3.12.m2.2.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.12.m2.2.2.3.3.1" xref="S3.SS1.p3.12.m2.2.2.3.3.1.cmml">​</mo><mi id="S3.SS1.p3.12.m2.2.2.3.3.3" xref="S3.SS1.p3.12.m2.2.2.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.12.m2.2.2.3.3.1a" xref="S3.SS1.p3.12.m2.2.2.3.3.1.cmml">​</mo><mi id="S3.SS1.p3.12.m2.2.2.3.3.4" xref="S3.SS1.p3.12.m2.2.2.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.12.m2.2.2.3.3.1b" xref="S3.SS1.p3.12.m2.2.2.3.3.1.cmml">​</mo><mi id="S3.SS1.p3.12.m2.2.2.3.3.5" xref="S3.SS1.p3.12.m2.2.2.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p3.12.m2.2.2.3.3.1c" xref="S3.SS1.p3.12.m2.2.2.3.3.1.cmml">​</mo><mi id="S3.SS1.p3.12.m2.2.2.3.3.6" xref="S3.SS1.p3.12.m2.2.2.3.3.6.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.12.m2.2.2.2" xref="S3.SS1.p3.12.m2.2.2.2.cmml">​</mo><mrow id="S3.SS1.p3.12.m2.2.2.1.1" xref="S3.SS1.p3.12.m2.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.12.m2.2.2.1.1.2" xref="S3.SS1.p3.12.m2.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p3.12.m2.1.1" xref="S3.SS1.p3.12.m2.1.1.cmml">A</mi><mo id="S3.SS1.p3.12.m2.2.2.1.1.3" xref="S3.SS1.p3.12.m2.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p3.12.m2.2.2.1.1.1" xref="S3.SS1.p3.12.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.12.m2.2.2.1.1.1.2" xref="S3.SS1.p3.12.m2.2.2.1.1.1.2.cmml">A</mi><mo id="S3.SS1.p3.12.m2.2.2.1.1.1.3" xref="S3.SS1.p3.12.m2.2.2.1.1.1.3.cmml">′′</mo></msup><mo stretchy="false" id="S3.SS1.p3.12.m2.2.2.1.1.4" xref="S3.SS1.p3.12.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.12.m2.2b"><apply id="S3.SS1.p3.12.m2.2.2.cmml" xref="S3.SS1.p3.12.m2.2.2"><times id="S3.SS1.p3.12.m2.2.2.2.cmml" xref="S3.SS1.p3.12.m2.2.2.2"></times><apply id="S3.SS1.p3.12.m2.2.2.3.cmml" xref="S3.SS1.p3.12.m2.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m2.2.2.3.1.cmml" xref="S3.SS1.p3.12.m2.2.2.3">subscript</csymbol><ci id="S3.SS1.p3.12.m2.2.2.3.2.cmml" xref="S3.SS1.p3.12.m2.2.2.3.2">ℒ</ci><apply id="S3.SS1.p3.12.m2.2.2.3.3.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3"><times id="S3.SS1.p3.12.m2.2.2.3.3.1.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3.1"></times><ci id="S3.SS1.p3.12.m2.2.2.3.3.2.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3.2">𝑐</ci><ci id="S3.SS1.p3.12.m2.2.2.3.3.3.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3.3">𝑦</ci><ci id="S3.SS1.p3.12.m2.2.2.3.3.4.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3.4">𝑐</ci><ci id="S3.SS1.p3.12.m2.2.2.3.3.5.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3.5">𝑙</ci><ci id="S3.SS1.p3.12.m2.2.2.3.3.6.cmml" xref="S3.SS1.p3.12.m2.2.2.3.3.6">𝑒</ci></apply></apply><interval closure="open" id="S3.SS1.p3.12.m2.2.2.1.2.cmml" xref="S3.SS1.p3.12.m2.2.2.1.1"><ci id="S3.SS1.p3.12.m2.1.1.cmml" xref="S3.SS1.p3.12.m2.1.1">𝐴</ci><apply id="S3.SS1.p3.12.m2.2.2.1.1.1.cmml" xref="S3.SS1.p3.12.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.12.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.12.m2.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.12.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.12.m2.2.2.1.1.1.2">𝐴</ci><ci id="S3.SS1.p3.12.m2.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.12.m2.2.2.1.1.1.3">′′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.12.m2.2c">\mathcal{L}_{cycle}(A,A^{\prime\prime})</annotation></semantics></math> <span id="S3.SS1.p3.15.1" class="ltx_text" style="color:#000000;">(<em id="S3.SS1.p3.15.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p3.15.1.2" class="ltx_text"></span> A-Consistency Loss)</span> are cross-entropy losses, <math id="S3.SS1.p3.13.m3.2" class="ltx_Math" alttext="\mathcal{L}_{G}(Q,Q^{\prime})" display="inline"><semantics id="S3.SS1.p3.13.m3.2a"><mrow id="S3.SS1.p3.13.m3.2.2" xref="S3.SS1.p3.13.m3.2.2.cmml"><msub id="S3.SS1.p3.13.m3.2.2.3" xref="S3.SS1.p3.13.m3.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.13.m3.2.2.3.2" xref="S3.SS1.p3.13.m3.2.2.3.2.cmml">ℒ</mi><mi id="S3.SS1.p3.13.m3.2.2.3.3" xref="S3.SS1.p3.13.m3.2.2.3.3.cmml">G</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p3.13.m3.2.2.2" xref="S3.SS1.p3.13.m3.2.2.2.cmml">​</mo><mrow id="S3.SS1.p3.13.m3.2.2.1.1" xref="S3.SS1.p3.13.m3.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS1.p3.13.m3.2.2.1.1.2" xref="S3.SS1.p3.13.m3.2.2.1.2.cmml">(</mo><mi id="S3.SS1.p3.13.m3.1.1" xref="S3.SS1.p3.13.m3.1.1.cmml">Q</mi><mo id="S3.SS1.p3.13.m3.2.2.1.1.3" xref="S3.SS1.p3.13.m3.2.2.1.2.cmml">,</mo><msup id="S3.SS1.p3.13.m3.2.2.1.1.1" xref="S3.SS1.p3.13.m3.2.2.1.1.1.cmml"><mi id="S3.SS1.p3.13.m3.2.2.1.1.1.2" xref="S3.SS1.p3.13.m3.2.2.1.1.1.2.cmml">Q</mi><mo id="S3.SS1.p3.13.m3.2.2.1.1.1.3" xref="S3.SS1.p3.13.m3.2.2.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.SS1.p3.13.m3.2.2.1.1.4" xref="S3.SS1.p3.13.m3.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.13.m3.2b"><apply id="S3.SS1.p3.13.m3.2.2.cmml" xref="S3.SS1.p3.13.m3.2.2"><times id="S3.SS1.p3.13.m3.2.2.2.cmml" xref="S3.SS1.p3.13.m3.2.2.2"></times><apply id="S3.SS1.p3.13.m3.2.2.3.cmml" xref="S3.SS1.p3.13.m3.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.p3.13.m3.2.2.3.1.cmml" xref="S3.SS1.p3.13.m3.2.2.3">subscript</csymbol><ci id="S3.SS1.p3.13.m3.2.2.3.2.cmml" xref="S3.SS1.p3.13.m3.2.2.3.2">ℒ</ci><ci id="S3.SS1.p3.13.m3.2.2.3.3.cmml" xref="S3.SS1.p3.13.m3.2.2.3.3">𝐺</ci></apply><interval closure="open" id="S3.SS1.p3.13.m3.2.2.1.2.cmml" xref="S3.SS1.p3.13.m3.2.2.1.1"><ci id="S3.SS1.p3.13.m3.1.1.cmml" xref="S3.SS1.p3.13.m3.1.1">𝑄</ci><apply id="S3.SS1.p3.13.m3.2.2.1.1.1.cmml" xref="S3.SS1.p3.13.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.13.m3.2.2.1.1.1.1.cmml" xref="S3.SS1.p3.13.m3.2.2.1.1.1">superscript</csymbol><ci id="S3.SS1.p3.13.m3.2.2.1.1.1.2.cmml" xref="S3.SS1.p3.13.m3.2.2.1.1.1.2">𝑄</ci><ci id="S3.SS1.p3.13.m3.2.2.1.1.1.3.cmml" xref="S3.SS1.p3.13.m3.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.13.m3.2c">\mathcal{L}_{G}(Q,Q^{\prime})</annotation></semantics></math> <span id="S3.SS1.p3.15.2" class="ltx_text" style="color:#000000;">(<em id="S3.SS1.p3.15.2.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S3.SS1.p3.15.2.2" class="ltx_text"></span> Q-Consistency Loss)</span> is sequence generation loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and <math id="S3.SS1.p3.14.m4.1" class="ltx_Math" alttext="\lambda_{G}" display="inline"><semantics id="S3.SS1.p3.14.m4.1a"><msub id="S3.SS1.p3.14.m4.1.1" xref="S3.SS1.p3.14.m4.1.1.cmml"><mi id="S3.SS1.p3.14.m4.1.1.2" xref="S3.SS1.p3.14.m4.1.1.2.cmml">λ</mi><mi id="S3.SS1.p3.14.m4.1.1.3" xref="S3.SS1.p3.14.m4.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.14.m4.1b"><apply id="S3.SS1.p3.14.m4.1.1.cmml" xref="S3.SS1.p3.14.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.14.m4.1.1.1.cmml" xref="S3.SS1.p3.14.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.14.m4.1.1.2.cmml" xref="S3.SS1.p3.14.m4.1.1.2">𝜆</ci><ci id="S3.SS1.p3.14.m4.1.1.3.cmml" xref="S3.SS1.p3.14.m4.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.14.m4.1c">\lambda_{G}</annotation></semantics></math>, <math id="S3.SS1.p3.15.m5.1" class="ltx_Math" alttext="\lambda_{C}" display="inline"><semantics id="S3.SS1.p3.15.m5.1a"><msub id="S3.SS1.p3.15.m5.1.1" xref="S3.SS1.p3.15.m5.1.1.cmml"><mi id="S3.SS1.p3.15.m5.1.1.2" xref="S3.SS1.p3.15.m5.1.1.2.cmml">λ</mi><mi id="S3.SS1.p3.15.m5.1.1.3" xref="S3.SS1.p3.15.m5.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.15.m5.1b"><apply id="S3.SS1.p3.15.m5.1.1.cmml" xref="S3.SS1.p3.15.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.15.m5.1.1.1.cmml" xref="S3.SS1.p3.15.m5.1.1">subscript</csymbol><ci id="S3.SS1.p3.15.m5.1.1.2.cmml" xref="S3.SS1.p3.15.m5.1.1.2">𝜆</ci><ci id="S3.SS1.p3.15.m5.1.1.3.cmml" xref="S3.SS1.p3.15.m5.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.15.m5.1c">\lambda_{C}</annotation></semantics></math> are tunable hyperparameters.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/1902.05660/assets/x3.png" id="S3.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="187" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span id="S3.F3.4.1" class="ltx_text ltx_font_bold"> (a) Qualitative examples from our VQA-Rephrasings dataset</span>.
The first question (shown in gray) in each block is the original question from VQA v2.0 validation set,
the questions that follow (shown in black) are rephrasings collected in VQA-Rephrasings.
<span id="S3.F3.5.2" class="ltx_text ltx_font_bold">(b) Qualitative examples of answer conditioned question generation (<span id="S3.F3.5.2.1" class="ltx_text ltx_framed ltx_framed_underline">input answer</span>) by our VQG module
</span>
</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Gating Mechanism</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">One of the assumptions of our proposed cycle-consistent training scheme is that the generated question is always semantically and syntactically correct. However, in practice this is not always true.
Previous attempts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> at naively generating questions conditioned on the answer and using them without filtering to augment the training data have been unsuccessful.
Like the visual question answering module, the visual question generation module is also not perfect. Therefore not all questions generated by the question generator are coherent and consistent with the image, the answer and the original question.
To overcome this issue, we propose a gating mechanism, which automatically filters undesirable questions generated by the VQG model before passing them to the VQA model <span id="S3.SS2.p1.2.1" class="ltx_text" style="color:#000000;">for A-consistency. The gating mechanism is only relevant when used in conjunction with A-consistency.</span>
We retain only those questions which either the VQA model <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">F</annotation></semantics></math> can answer correctly or have a cosine similarity with the original question encoding greater than a threshold <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="T_{sim}" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><msub id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">T</mi><mrow id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.3.1a" xref="S3.SS2.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S3.SS2.p1.2.m2.1.1.3.4" xref="S3.SS2.p1.2.m2.1.1.3.4.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑇</ci><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><times id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3.1"></times><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">𝑠</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">𝑖</ci><ci id="S3.SS2.p1.2.m2.1.1.3.4.cmml" xref="S3.SS2.p1.2.m2.1.1.3.4">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">T_{sim}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Late Activation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">One key component of designing cycle consistent models is to prevent mode collapse. Learning cycle-consistent models in complex settings like VQA needs a carefully chosen training scheme. Since cycle-consistent models have several interconnected sub-networks learning different transformations, it is important to ensure that each of these sub-networks are working in harmony.
For example, if the VQA model <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">F</annotation></semantics></math> and VQG model <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">G</annotation></semantics></math> are jointly trained and consistency is enforced in early stages of training, it is possible that both models can just “cheat” by both producing undesirable outputs.
We overcome this by activating cycle-consistency at later stages of training, to make sure both VQA and VQG models have been sufficiently trained to produce reasonable outputs. Specifically, we enable the loss associated with cycle-consistency after a fixed <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="A_{iter}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><msub id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">A</mi><mrow id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.1" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.1a" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.3.4" xref="S3.SS3.p1.3.m3.1.1.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.3.1b" xref="S3.SS3.p1.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p1.3.m3.1.1.3.5" xref="S3.SS3.p1.3.m3.1.1.3.5.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2">𝐴</ci><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><times id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3.1"></times><ci id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">𝑖</ci><ci id="S3.SS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3">𝑡</ci><ci id="S3.SS3.p1.3.m3.1.1.3.4.cmml" xref="S3.SS3.p1.3.m3.1.1.3.4">𝑒</ci><ci id="S3.SS3.p1.3.m3.1.1.3.5.cmml" xref="S3.SS3.p1.3.m3.1.1.3.5">𝑟</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">A_{iter}</annotation></semantics></math> iterations in the training process.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We find these design choices for question generation module, gating mechanism and late activation to be crucial for effectively training our model.
We demonstrate this empirically via ablation studies in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Consistency Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
As we want to increase the robustness of the VQA model to all generated variations, the weights between VQA models which answer the original question and the generated rephrasing are shared. Our formulation of cycle-consistency in VQA can be also thought of as an online data-augmentation technique where the model is trained on several generated rephrasings of the same question and hence is more robust to such anomalies during inference. We show that with clever training strategy, coupled with attention and carefully chosen model architectures for question generation, incorporating cycle consistency for VQA is possible and not only leads to models that are better performing, but also more robust and consistent. In addition, we show that this robustness also imparts VQA models the ability to better predict their own failures.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>VQA-Rephrasings Dataset</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we introduce the VQA-Rephrasings dataset, which is the first dataset that enables evaluation of VQA models for robustness and consistency to different rephrasings of questions with the same meaning.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We use the validation split of VQA v2.0  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> as our base dataset which contains a total of 214,354 questions spanning over 40,504 images. We randomly sample 40,504 questions (one question per image) from the base dataset to form a sampled subset. We collect 3 rephrasings of each question in the sampled subset using human annotators in two stages. In the first stage, humans were primed with the original question and the corresponding true answer and asked to rephrase the question such that answer to the rephrased question remains the same as the original answer. To ensure rephrasings from first stage are <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">syntactically</span> correct and <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">semantically</span> inline with the original question, we filter the collected responses in the next stage.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">In the second stage, humans were primed with the original question and it’s rephrasing and were asked to label the rephrasing invalid if:
(a) the plausible answer to the original question and it’s rephrasing is different (<em id="S4.p3.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.p3.1.2" class="ltx_text"></span> if the question and it’s rephrasing have different intents) or
(b) if the rephrasing is grammatically incorrect.
We collected 121,512 rephrasings from the original 40504 questions in the first stage. Of these, 1320 rephrasings were flagged as invalid in the second stage and were rephrased again in the first stage. Humans were shown examples of incorrect rephrasings in the first stage to minimize the number of invalid rephrasings.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The final dataset consists of 162,016 questions (including the original 40,504 questions) spanning 40,504 images with an average of <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.p4.1.m1.1a"><mo id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><csymbol cd="latexml" id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\sim</annotation></semantics></math>3 rephrasings per original question. A few qualitative examples from the collected dataset can be seen in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Question Generation Module ‣ 3 Approach ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(a). Additional details about the data collection, interfaces used and exhaustive dataset statistics can be found in Appendix <a href="#A1" title="Appendix A Dataset Details ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.7" class="ltx_p"><span id="S4.p5.7.1" class="ltx_text ltx_font_bold">Consensus Score.</span> Intuitively, for a VQA model to be consistent across various rephrasings of the same question, the answer to all rephrasings should be the same. We measure this by a Consensus Score <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="CS(k)" display="inline"><semantics id="S4.p5.1.m1.1a"><mrow id="S4.p5.1.m1.1.2" xref="S4.p5.1.m1.1.2.cmml"><mi id="S4.p5.1.m1.1.2.2" xref="S4.p5.1.m1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.2.1" xref="S4.p5.1.m1.1.2.1.cmml">​</mo><mi id="S4.p5.1.m1.1.2.3" xref="S4.p5.1.m1.1.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p5.1.m1.1.2.1a" xref="S4.p5.1.m1.1.2.1.cmml">​</mo><mrow id="S4.p5.1.m1.1.2.4.2" xref="S4.p5.1.m1.1.2.cmml"><mo stretchy="false" id="S4.p5.1.m1.1.2.4.2.1" xref="S4.p5.1.m1.1.2.cmml">(</mo><mi id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">k</mi><mo stretchy="false" id="S4.p5.1.m1.1.2.4.2.2" xref="S4.p5.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><apply id="S4.p5.1.m1.1.2.cmml" xref="S4.p5.1.m1.1.2"><times id="S4.p5.1.m1.1.2.1.cmml" xref="S4.p5.1.m1.1.2.1"></times><ci id="S4.p5.1.m1.1.2.2.cmml" xref="S4.p5.1.m1.1.2.2">𝐶</ci><ci id="S4.p5.1.m1.1.2.3.cmml" xref="S4.p5.1.m1.1.2.3">𝑆</ci><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">CS(k)</annotation></semantics></math>. For every group <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p5.2.m2.1a"><mi id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><ci id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">Q</annotation></semantics></math> consisting of <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p5.3.m3.1a"><mi id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><ci id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">n</annotation></semantics></math> rephrasings, we sample all subsets of size <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p5.4.m4.1a"><mi id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><ci id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">k</annotation></semantics></math>.
The consensus score <math id="S4.p5.5.m5.1" class="ltx_Math" alttext="CS(k)" display="inline"><semantics id="S4.p5.5.m5.1a"><mrow id="S4.p5.5.m5.1.2" xref="S4.p5.5.m5.1.2.cmml"><mi id="S4.p5.5.m5.1.2.2" xref="S4.p5.5.m5.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.p5.5.m5.1.2.1" xref="S4.p5.5.m5.1.2.1.cmml">​</mo><mi id="S4.p5.5.m5.1.2.3" xref="S4.p5.5.m5.1.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p5.5.m5.1.2.1a" xref="S4.p5.5.m5.1.2.1.cmml">​</mo><mrow id="S4.p5.5.m5.1.2.4.2" xref="S4.p5.5.m5.1.2.cmml"><mo stretchy="false" id="S4.p5.5.m5.1.2.4.2.1" xref="S4.p5.5.m5.1.2.cmml">(</mo><mi id="S4.p5.5.m5.1.1" xref="S4.p5.5.m5.1.1.cmml">k</mi><mo stretchy="false" id="S4.p5.5.m5.1.2.4.2.2" xref="S4.p5.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p5.5.m5.1b"><apply id="S4.p5.5.m5.1.2.cmml" xref="S4.p5.5.m5.1.2"><times id="S4.p5.5.m5.1.2.1.cmml" xref="S4.p5.5.m5.1.2.1"></times><ci id="S4.p5.5.m5.1.2.2.cmml" xref="S4.p5.5.m5.1.2.2">𝐶</ci><ci id="S4.p5.5.m5.1.2.3.cmml" xref="S4.p5.5.m5.1.2.3">𝑆</ci><ci id="S4.p5.5.m5.1.1.cmml" xref="S4.p5.5.m5.1.1">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.5.m5.1c">CS(k)</annotation></semantics></math> is defined as the ratio of the number of subsets where <em id="S4.p5.7.2" class="ltx_emph ltx_font_italic">all</em> the answers are correct and the total number of subsets of size <math id="S4.p5.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p5.6.m6.1a"><mi id="S4.p5.6.m6.1.1" xref="S4.p5.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p5.6.m6.1b"><ci id="S4.p5.6.m6.1.1.cmml" xref="S4.p5.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.6.m6.1c">k</annotation></semantics></math>. The answer to a question is considered correct if it has a non-zero VQA Accuracy <math id="S4.p5.7.m7.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S4.p5.7.m7.1a"><mi id="S4.p5.7.m7.1.1" xref="S4.p5.7.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S4.p5.7.m7.1b"><ci id="S4.p5.7.m7.1.1.cmml" xref="S4.p5.7.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.7.m7.1c">\theta</annotation></semantics></math> as defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
CS(k) is formally defined as:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.4" class="ltx_math_unparsed" alttext="CS(k)\quad=\sum_{Q^{\prime}\subset Q,|Q^{\prime}|=k}\frac{\mathcal{S}(Q^{\prime})}{\prescript{n\mkern-0.8mu}{}{C}_{k}}" display="block"><semantics id="S4.E2.m1.4a"><mrow id="S4.E2.m1.4b"><mi id="S4.E2.m1.4.5">C</mi><mi id="S4.E2.m1.4.6">S</mi><mrow id="S4.E2.m1.4.7"><mo stretchy="false" id="S4.E2.m1.4.7.1">(</mo><mi id="S4.E2.m1.4.4">k</mi><mo stretchy="false" id="S4.E2.m1.4.7.2">)</mo></mrow><mspace width="1em" id="S4.E2.m1.4.8"></mspace><mo rspace="0.111em" id="S4.E2.m1.4.9">=</mo><munder id="S4.E2.m1.4.10"><mo movablelimits="false" id="S4.E2.m1.4.10.2">∑</mo><mrow id="S4.E2.m1.2.2.2.2"><mrow id="S4.E2.m1.1.1.1.1.1"><msup id="S4.E2.m1.1.1.1.1.1.2"><mi id="S4.E2.m1.1.1.1.1.1.2.2">Q</mi><mo id="S4.E2.m1.1.1.1.1.1.2.3">′</mo></msup><mo id="S4.E2.m1.1.1.1.1.1.1">⊂</mo><mi id="S4.E2.m1.1.1.1.1.1.3">Q</mi></mrow><mo id="S4.E2.m1.2.2.2.2.3">,</mo><mrow id="S4.E2.m1.2.2.2.2.2"><mrow id="S4.E2.m1.2.2.2.2.2.1.1"><mo stretchy="false" id="S4.E2.m1.2.2.2.2.2.1.1.2">|</mo><msup id="S4.E2.m1.2.2.2.2.2.1.1.1"><mi id="S4.E2.m1.2.2.2.2.2.1.1.1.2">Q</mi><mo id="S4.E2.m1.2.2.2.2.2.1.1.1.3">′</mo></msup><mo stretchy="false" id="S4.E2.m1.2.2.2.2.2.1.1.3">|</mo></mrow><mo id="S4.E2.m1.2.2.2.2.2.2">=</mo><mi id="S4.E2.m1.2.2.2.2.2.3">k</mi></mrow></mrow></munder><mfrac id="S4.E2.m1.3.3"><mrow id="S4.E2.m1.3.3.1"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.3.3.1.3">𝒮</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.2">​</mo><mrow id="S4.E2.m1.3.3.1.1.1"><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.2">(</mo><msup id="S4.E2.m1.3.3.1.1.1.1"><mi id="S4.E2.m1.3.3.1.1.1.1.2">Q</mi><mo id="S4.E2.m1.3.3.1.1.1.1.3">′</mo></msup><mo stretchy="false" id="S4.E2.m1.3.3.1.1.1.3">)</mo></mrow></mrow><mmultiscripts id="S4.E2.m1.3.3.3"><mi id="S4.E2.m1.3.3.3.2.2">C</mi><mi id="S4.E2.m1.3.3.3.2.3">k</mi><mrow id="S4.E2.m1.3.3.3a"></mrow><mprescripts id="S4.E2.m1.3.3.3b"></mprescripts><mrow id="S4.E2.m1.3.3.3c"></mrow><mi id="S4.E2.m1.3.3.3.3">n</mi></mmultiscripts></mfrac></mrow><annotation encoding="application/x-tex" id="S4.E2.m1.4c">CS(k)\quad=\sum_{Q^{\prime}\subset Q,|Q^{\prime}|=k}\frac{\mathcal{S}(Q^{\prime})}{\prescript{n\mkern-0.8mu}{}{C}_{k}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p6" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.5" class="ltx_Math" alttext="\mathcal{S}(Q^{\prime})=\begin{cases}\quad 1&amp;\text{if }\enskip\forall q\in Q^{\prime}\enskip\theta(q)&gt;0,\\
\quad 0&amp;\text{otherwise}.\end{cases}" display="block"><semantics id="S4.E3.m1.5a"><mrow id="S4.E3.m1.5.5" xref="S4.E3.m1.5.5.cmml"><mrow id="S4.E3.m1.5.5.1" xref="S4.E3.m1.5.5.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.5.5.1.3" xref="S4.E3.m1.5.5.1.3.cmml">𝒮</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.5.5.1.2" xref="S4.E3.m1.5.5.1.2.cmml">​</mo><mrow id="S4.E3.m1.5.5.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.5.5.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.cmml">(</mo><msup id="S4.E3.m1.5.5.1.1.1.1" xref="S4.E3.m1.5.5.1.1.1.1.cmml"><mi id="S4.E3.m1.5.5.1.1.1.1.2" xref="S4.E3.m1.5.5.1.1.1.1.2.cmml">Q</mi><mo id="S4.E3.m1.5.5.1.1.1.1.3" xref="S4.E3.m1.5.5.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S4.E3.m1.5.5.1.1.1.3" xref="S4.E3.m1.5.5.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.5.5.2" xref="S4.E3.m1.5.5.2.cmml">=</mo><mrow id="S4.E3.m1.4.4" xref="S4.E3.m1.5.5.3.1.cmml"><mo id="S4.E3.m1.4.4.5" xref="S4.E3.m1.5.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E3.m1.4.4.4" xref="S4.E3.m1.5.5.3.1.cmml"><mtr id="S4.E3.m1.4.4.4a" xref="S4.E3.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.4.4.4b" xref="S4.E3.m1.5.5.3.1.cmml"><mn id="S4.E3.m1.1.1.1.1.1.1" xref="S4.E3.m1.1.1.1.1.1.1.cmml">1</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.4.4.4c" xref="S4.E3.m1.5.5.3.1.cmml"><mrow id="S4.E3.m1.2.2.2.2.2.1.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.cmml"><mrow id="S4.E3.m1.2.2.2.2.2.1.2.1" xref="S4.E3.m1.2.2.2.2.2.1.2.1.cmml"><mrow id="S4.E3.m1.2.2.2.2.2.1.2.1.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.cmml"><mtext id="S4.E3.m1.2.2.2.2.2.1.2.1.2.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.2a.cmml">if </mtext><mo lspace="0.667em" rspace="0em" id="S4.E3.m1.2.2.2.2.2.1.2.1.2.1" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.1.cmml">​</mo><mrow id="S4.E3.m1.2.2.2.2.2.1.2.1.2.3" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.cmml"><mo rspace="0.167em" id="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.1" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.1.cmml">∀</mo><mi id="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.2.cmml">q</mi></mrow></mrow><mo id="S4.E3.m1.2.2.2.2.2.1.2.1.3" xref="S4.E3.m1.2.2.2.2.2.1.2.1.3.cmml">∈</mo><mrow id="S4.E3.m1.2.2.2.2.2.1.2.1.4" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.cmml"><msup id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.cmml"><mi id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.2.cmml">Q</mi><mo id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.3" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.2.2.2.1.2.1.4.1" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.1.cmml">​</mo><mi id="S4.E3.m1.2.2.2.2.2.1.2.1.4.3" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.3.cmml">θ</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.2.2.2.1.2.1.4.1a" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.1.cmml">​</mo><mrow id="S4.E3.m1.2.2.2.2.2.1.2.1.4.4.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.1.2.1.4.4.2.1" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.cmml">(</mo><mi id="S4.E3.m1.2.2.2.2.2.1.1" xref="S4.E3.m1.2.2.2.2.2.1.1.cmml">q</mi><mo stretchy="false" id="S4.E3.m1.2.2.2.2.2.1.2.1.4.4.2.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.2.2.2.1.2.1.5" xref="S4.E3.m1.2.2.2.2.2.1.2.1.5.cmml">&gt;</mo><mn id="S4.E3.m1.2.2.2.2.2.1.2.1.6" xref="S4.E3.m1.2.2.2.2.2.1.2.1.6.cmml">0</mn></mrow><mo id="S4.E3.m1.2.2.2.2.2.1.2.2" xref="S4.E3.m1.2.2.2.2.2.1.2.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S4.E3.m1.4.4.4d" xref="S4.E3.m1.5.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.4.4.4e" xref="S4.E3.m1.5.5.3.1.cmml"><mn id="S4.E3.m1.3.3.3.3.1.1" xref="S4.E3.m1.3.3.3.3.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E3.m1.4.4.4f" xref="S4.E3.m1.5.5.3.1.cmml"><mrow id="S4.E3.m1.4.4.4.4.2.1.3" xref="S4.E3.m1.4.4.4.4.2.1.1a.cmml"><mtext id="S4.E3.m1.4.4.4.4.2.1.1" xref="S4.E3.m1.4.4.4.4.2.1.1.cmml">otherwise</mtext><mo lspace="0em" id="S4.E3.m1.4.4.4.4.2.1.3.1" xref="S4.E3.m1.4.4.4.4.2.1.1a.cmml">.</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.5b"><apply id="S4.E3.m1.5.5.cmml" xref="S4.E3.m1.5.5"><eq id="S4.E3.m1.5.5.2.cmml" xref="S4.E3.m1.5.5.2"></eq><apply id="S4.E3.m1.5.5.1.cmml" xref="S4.E3.m1.5.5.1"><times id="S4.E3.m1.5.5.1.2.cmml" xref="S4.E3.m1.5.5.1.2"></times><ci id="S4.E3.m1.5.5.1.3.cmml" xref="S4.E3.m1.5.5.1.3">𝒮</ci><apply id="S4.E3.m1.5.5.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.5.5.1.1.1.1.1.cmml" xref="S4.E3.m1.5.5.1.1.1">superscript</csymbol><ci id="S4.E3.m1.5.5.1.1.1.1.2.cmml" xref="S4.E3.m1.5.5.1.1.1.1.2">𝑄</ci><ci id="S4.E3.m1.5.5.1.1.1.1.3.cmml" xref="S4.E3.m1.5.5.1.1.1.1.3">′</ci></apply></apply><apply id="S4.E3.m1.5.5.3.1.cmml" xref="S4.E3.m1.4.4"><csymbol cd="latexml" id="S4.E3.m1.5.5.3.1.1.cmml" xref="S4.E3.m1.4.4.5">cases</csymbol><cn type="integer" id="S4.E3.m1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.1.1.1.1.1.1">1</cn><apply id="S4.E3.m1.2.2.2.2.2.1.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2"><and id="S4.E3.m1.2.2.2.2.2.1.2.1a.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2"></and><apply id="S4.E3.m1.2.2.2.2.2.1.2.1b.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2"><in id="S4.E3.m1.2.2.2.2.2.1.2.1.3.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.3"></in><apply id="S4.E3.m1.2.2.2.2.2.1.2.1.2.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2"><times id="S4.E3.m1.2.2.2.2.2.1.2.1.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.1"></times><ci id="S4.E3.m1.2.2.2.2.2.1.2.1.2.2a.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.2"><mtext id="S4.E3.m1.2.2.2.2.2.1.2.1.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.2">if </mtext></ci><apply id="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.3"><csymbol cd="latexml" id="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.1.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.1">for-all</csymbol><ci id="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.2.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.2.3.2">𝑞</ci></apply></apply><apply id="S4.E3.m1.2.2.2.2.2.1.2.1.4.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4"><times id="S4.E3.m1.2.2.2.2.2.1.2.1.4.1.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.1"></times><apply id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.1.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2">superscript</csymbol><ci id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.2.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.2">𝑄</ci><ci id="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.3.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.2.3">′</ci></apply><ci id="S4.E3.m1.2.2.2.2.2.1.2.1.4.3.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.4.3">𝜃</ci><ci id="S4.E3.m1.2.2.2.2.2.1.1.cmml" xref="S4.E3.m1.2.2.2.2.2.1.1">𝑞</ci></apply></apply><apply id="S4.E3.m1.2.2.2.2.2.1.2.1c.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2"><gt id="S4.E3.m1.2.2.2.2.2.1.2.1.5.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.5"></gt><share href="#S4.E3.m1.2.2.2.2.2.1.2.1.4.cmml" id="S4.E3.m1.2.2.2.2.2.1.2.1d.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2"></share><cn type="integer" id="S4.E3.m1.2.2.2.2.2.1.2.1.6.cmml" xref="S4.E3.m1.2.2.2.2.2.1.2.1.6">0</cn></apply></apply><cn type="integer" id="S4.E3.m1.3.3.3.3.1.1.cmml" xref="S4.E3.m1.3.3.3.3.1.1">0</cn><ci id="S4.E3.m1.4.4.4.4.2.1.1a.cmml" xref="S4.E3.m1.4.4.4.4.2.1.3"><mtext id="S4.E3.m1.4.4.4.4.2.1.1.cmml" xref="S4.E3.m1.4.4.4.4.2.1.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.5c">\mathcal{S}(Q^{\prime})=\begin{cases}\quad 1&amp;\text{if }\enskip\forall q\in Q^{\prime}\enskip\theta(q)&gt;0,\\
\quad 0&amp;\text{otherwise}.\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.10" class="ltx_p">Where <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\prescript{n\mkern-0.8mu}{}{C}_{k}" display="inline"><semantics id="S4.p7.1.m1.1a"><mmultiscripts id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi id="S4.p7.1.m1.1.1.2.2" xref="S4.p7.1.m1.1.1.2.2.cmml">C</mi><mi id="S4.p7.1.m1.1.1.2.3" xref="S4.p7.1.m1.1.1.2.3.cmml">k</mi><mrow id="S4.p7.1.m1.1.1a" xref="S4.p7.1.m1.1.1.cmml"></mrow><mprescripts id="S4.p7.1.m1.1.1b" xref="S4.p7.1.m1.1.1.cmml"></mprescripts><mrow id="S4.p7.1.m1.1.1c" xref="S4.p7.1.m1.1.1.cmml"></mrow><mi id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml">n</mi></mmultiscripts><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1">superscript</csymbol><apply id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.2.1.cmml" xref="S4.p7.1.m1.1.1">subscript</csymbol><ci id="S4.p7.1.m1.1.1.2.2.cmml" xref="S4.p7.1.m1.1.1.2.2">𝐶</ci><ci id="S4.p7.1.m1.1.1.2.3.cmml" xref="S4.p7.1.m1.1.1.2.3">𝑘</ci></apply><ci id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\prescript{n\mkern-0.8mu}{}{C}_{k}</annotation></semantics></math> is number of subsets of size <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p7.2.m2.1a"><mi id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><ci id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">k</annotation></semantics></math> sampled from a set of size <math id="S4.p7.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p7.3.m3.1a"><mi id="S4.p7.3.m3.1.1" xref="S4.p7.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p7.3.m3.1b"><ci id="S4.p7.3.m3.1.1.cmml" xref="S4.p7.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.3.m3.1c">n</annotation></semantics></math>.
As consensus score is a all-or-nothing score, to achieve a non-zero consensus score at <math id="S4.p7.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p7.4.m4.1a"><mi id="S4.p7.4.m4.1.1" xref="S4.p7.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p7.4.m4.1b"><ci id="S4.p7.4.m4.1.1.cmml" xref="S4.p7.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.4.m4.1c">k</annotation></semantics></math> for a group of questions <math id="S4.p7.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p7.5.m5.1a"><mi id="S4.p7.5.m5.1.1" xref="S4.p7.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p7.5.m5.1b"><ci id="S4.p7.5.m5.1.1.cmml" xref="S4.p7.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.5.m5.1c">Q</annotation></semantics></math>, the model has to answer at least <math id="S4.p7.6.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p7.6.m6.1a"><mi id="S4.p7.6.m6.1.1" xref="S4.p7.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p7.6.m6.1b"><ci id="S4.p7.6.m6.1.1.cmml" xref="S4.p7.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.6.m6.1c">k</annotation></semantics></math> questions correctly in a group of questions <math id="S4.p7.7.m7.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p7.7.m7.1a"><mi id="S4.p7.7.m7.1.1" xref="S4.p7.7.m7.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p7.7.m7.1b"><ci id="S4.p7.7.m7.1.1.cmml" xref="S4.p7.7.m7.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.7.m7.1c">Q</annotation></semantics></math>. When <math id="S4.p7.8.m8.1" class="ltx_Math" alttext="k=|Q|" display="inline"><semantics id="S4.p7.8.m8.1a"><mrow id="S4.p7.8.m8.1.2" xref="S4.p7.8.m8.1.2.cmml"><mi id="S4.p7.8.m8.1.2.2" xref="S4.p7.8.m8.1.2.2.cmml">k</mi><mo id="S4.p7.8.m8.1.2.1" xref="S4.p7.8.m8.1.2.1.cmml">=</mo><mrow id="S4.p7.8.m8.1.2.3.2" xref="S4.p7.8.m8.1.2.3.1.cmml"><mo stretchy="false" id="S4.p7.8.m8.1.2.3.2.1" xref="S4.p7.8.m8.1.2.3.1.1.cmml">|</mo><mi id="S4.p7.8.m8.1.1" xref="S4.p7.8.m8.1.1.cmml">Q</mi><mo stretchy="false" id="S4.p7.8.m8.1.2.3.2.2" xref="S4.p7.8.m8.1.2.3.1.1.cmml">|</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.8.m8.1b"><apply id="S4.p7.8.m8.1.2.cmml" xref="S4.p7.8.m8.1.2"><eq id="S4.p7.8.m8.1.2.1.cmml" xref="S4.p7.8.m8.1.2.1"></eq><ci id="S4.p7.8.m8.1.2.2.cmml" xref="S4.p7.8.m8.1.2.2">𝑘</ci><apply id="S4.p7.8.m8.1.2.3.1.cmml" xref="S4.p7.8.m8.1.2.3.2"><abs id="S4.p7.8.m8.1.2.3.1.1.cmml" xref="S4.p7.8.m8.1.2.3.2.1"></abs><ci id="S4.p7.8.m8.1.1.cmml" xref="S4.p7.8.m8.1.1">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.8.m8.1c">k=|Q|</annotation></semantics></math> (<em id="S4.p7.10.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.p7.10.2" class="ltx_text"></span> when <math id="S4.p7.9.m9.1" class="ltx_Math" alttext="k=4" display="inline"><semantics id="S4.p7.9.m9.1a"><mrow id="S4.p7.9.m9.1.1" xref="S4.p7.9.m9.1.1.cmml"><mi id="S4.p7.9.m9.1.1.2" xref="S4.p7.9.m9.1.1.2.cmml">k</mi><mo id="S4.p7.9.m9.1.1.1" xref="S4.p7.9.m9.1.1.1.cmml">=</mo><mn id="S4.p7.9.m9.1.1.3" xref="S4.p7.9.m9.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p7.9.m9.1b"><apply id="S4.p7.9.m9.1.1.cmml" xref="S4.p7.9.m9.1.1"><eq id="S4.p7.9.m9.1.1.1.cmml" xref="S4.p7.9.m9.1.1.1"></eq><ci id="S4.p7.9.m9.1.1.2.cmml" xref="S4.p7.9.m9.1.1.2">𝑘</ci><cn type="integer" id="S4.p7.9.m9.1.1.3.cmml" xref="S4.p7.9.m9.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.9.m9.1c">k=4</annotation></semantics></math> in VQA-Rephrasings), the model needs to answer all rephrasings of a question and the original question correctly in order to get a non-zero consensus score. It is evident that a model with higher average consensus score at high values of <math id="S4.p7.10.m10.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p7.10.m10.1a"><mi id="S4.p7.10.m10.1.1" xref="S4.p7.10.m10.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p7.10.m10.1b"><ci id="S4.p7.10.m10.1.1.cmml" xref="S4.p7.10.m10.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.10.m10.1c">k</annotation></semantics></math> is quantitatively more robust to linguistic variations in questions than a model with a lower score.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.5" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.5.1.1" class="ltx_tr">
<td id="S5.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S5.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;" colspan="4"><span id="S5.T1.5.1.1.2.1" class="ltx_text ltx_font_bold">CS(k)</span></td>
<td id="S5.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;" colspan="2"><span id="S5.T1.5.1.1.3.1" class="ltx_text ltx_font_bold">VQA Accuracy</span></td>
</tr>
<tr id="S5.T1.5.2.2" class="ltx_tr">
<td id="S5.T1.5.2.2.1" class="ltx_td" style="padding-left:3.5pt;padding-right:3.5pt;"></td>
<td id="S5.T1.5.2.2.2" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.2.2.2.1" class="ltx_text ltx_font_bold">k=1</span></td>
<td id="S5.T1.5.2.2.3" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.2.2.3.1" class="ltx_text ltx_font_bold">k=2</span></td>
<td id="S5.T1.5.2.2.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.2.2.4.1" class="ltx_text ltx_font_bold">k=3</span></td>
<td id="S5.T1.5.2.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.2.2.5.1" class="ltx_text ltx_font_bold">k=4</span></td>
<td id="S5.T1.5.2.2.6" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.2.2.6.1" class="ltx_text ltx_font_bold">ORI</span></td>
<td id="S5.T1.5.2.2.7" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.2.2.7.1" class="ltx_text ltx_font_bold">REP</span></td>
</tr>
<tr id="S5.T1.5.3.3" class="ltx_tr">
<td id="S5.T1.5.3.3.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S5.T1.5.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">56.68</td>
<td id="S5.T1.5.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">43.63</td>
<td id="S5.T1.5.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">38.94</td>
<td id="S5.T1.5.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">32.76</td>
<td id="S5.T1.5.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">59.08</td>
<td id="S5.T1.5.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">46.87</td>
</tr>
<tr id="S5.T1.5.4.4" class="ltx_tr">
<td id="S5.T1.5.4.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">BUTD  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S5.T1.5.4.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">60.55</td>
<td id="S5.T1.5.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">46.96</td>
<td id="S5.T1.5.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">40.54</td>
<td id="S5.T1.5.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">34.47</td>
<td id="S5.T1.5.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">61.51</td>
<td id="S5.T1.5.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">51.22</td>
</tr>
<tr id="S5.T1.5.5.5" class="ltx_tr">
<td id="S5.T1.5.5.5.1" class="ltx_td ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;">BUTD + CC</td>
<td id="S5.T1.5.5.5.2" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.5.5.2.1" class="ltx_text ltx_font_bold">61.66</span></td>
<td id="S5.T1.5.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.5.5.3.1" class="ltx_text ltx_font_bold">50.79</span></td>
<td id="S5.T1.5.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.5.5.4.1" class="ltx_text ltx_font_bold">44.68</span></td>
<td id="S5.T1.5.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.5.5.5.1" class="ltx_text ltx_font_bold">42.55</span></td>
<td id="S5.T1.5.5.5.6" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.5.5.6.1" class="ltx_text ltx_font_bold">62.44</span></td>
<td id="S5.T1.5.5.5.7" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.5.5.7.1" class="ltx_text ltx_font_bold">52.58</span></td>
</tr>
<tr id="S5.T1.5.6.6" class="ltx_tr">
<td id="S5.T1.5.6.6.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S5.T1.5.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">63.43</td>
<td id="S5.T1.5.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">52.03</td>
<td id="S5.T1.5.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">45.94</td>
<td id="S5.T1.5.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">39.49</td>
<td id="S5.T1.5.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">64.08</td>
<td id="S5.T1.5.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">54.20</td>
</tr>
<tr id="S5.T1.5.7.7" class="ltx_tr">
<td id="S5.T1.5.7.7.1" class="ltx_td ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;">Pythia + CC</td>
<td id="S5.T1.5.7.7.2" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.7.7.2.1" class="ltx_text ltx_font_bold">64.36</span></td>
<td id="S5.T1.5.7.7.3" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.7.7.3.1" class="ltx_text ltx_font_bold">55.45</span></td>
<td id="S5.T1.5.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.7.7.4.1" class="ltx_text ltx_font_bold">50.92</span></td>
<td id="S5.T1.5.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.7.7.5.1" class="ltx_text ltx_font_bold">44.30</span></td>
<td id="S5.T1.5.7.7.6" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.7.7.6.1" class="ltx_text ltx_font_bold">64.52</span></td>
<td id="S5.T1.5.7.7.7" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.7.7.7.1" class="ltx_text ltx_font_bold">55.65</span></td>
</tr>
<tr id="S5.T1.5.8.8" class="ltx_tr">
<td id="S5.T1.5.8.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</td>
<td id="S5.T1.5.8.8.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">64.88</td>
<td id="S5.T1.5.8.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">53.08</td>
<td id="S5.T1.5.8.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">47.45</td>
<td id="S5.T1.5.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">39.87</td>
<td id="S5.T1.5.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">64.97</td>
<td id="S5.T1.5.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">55.87</td>
</tr>
<tr id="S5.T1.5.9.9" class="ltx_tr">
<td id="S5.T1.5.9.9.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">BAN + CC</td>
<td id="S5.T1.5.9.9.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.9.9.2.1" class="ltx_text ltx_font_bold">65.77</span></td>
<td id="S5.T1.5.9.9.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.9.9.3.1" class="ltx_text ltx_font_bold">56.94</span></td>
<td id="S5.T1.5.9.9.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.9.9.4.1" class="ltx_text ltx_font_bold">51.76</span></td>
<td id="S5.T1.5.9.9.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.9.9.5.1" class="ltx_text ltx_font_bold">48.18</span></td>
<td id="S5.T1.5.9.9.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.9.9.6.1" class="ltx_text ltx_font_bold">65.87</span></td>
<td id="S5.T1.5.9.9.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T1.5.9.9.7.1" class="ltx_text ltx_font_bold">56.59</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S5.T1.8.1" class="ltx_text ltx_font_bold">Consensus performance on VQA-Rephrasings dataset</span>. CS(k) as defined in Eq. <a href="#S4.E2" title="In 4 VQA-Rephrasings Dataset ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is consensus score which is non-zero only if <em id="S5.T1.9.2" class="ltx_emph ltx_font_italic">at least</em> <math id="S5.T1.3.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T1.3.m1.1b"><mi id="S5.T1.3.m1.1.1" xref="S5.T1.3.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T1.3.m1.1c"><ci id="S5.T1.3.m1.1.1.cmml" xref="S5.T1.3.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.m1.1d">k</annotation></semantics></math> rephrasings are answered correctly, zero otherwise; averaged across all group of questions.
ORI represent a split of questions from VQA-Rephrasings which are original questions from VQA v2.0 and their corresponding rephrasings are represented by the split REP.
Models trained with our cycle-consistent (CC) framework consistently outperform their baseline counterparts at all values of <math id="S5.T1.4.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.T1.4.m2.1b"><mi id="S5.T1.4.m2.1.1" xref="S5.T1.4.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T1.4.m2.1c"><ci id="S5.T1.4.m2.1.1.cmml" xref="S5.T1.4.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.m2.1d">k</annotation></semantics></math>.
</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Consistency Performance</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We start by benchmarking a variety of existing VQA models on our proposed VQA-Rephrasings dataset.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">MUTAN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/Cadene/vqa.pytorch
</span></span></span> parametrizes bilinear interactions between visual and textual representations using a multi-modal low-rank decomposition. MUTAN uses skip-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> sentence embeddings to encode the question and Resnet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to encode images. MUTAN achieves 63.20% accuracy on VQA v2.0 test-dev. Among all models we analyze, MUTAN is the only model which uses sentence embeddings to encode questions and Resnet to encode images.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Bottom-Up Top-Down Attention (BUTD)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/hengyuan-hu/bottom-up-attention-vqa</span></span></span> incorporates bottom-up attention in VQA by extracting features associated with image regions proposed by Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> pretrained on Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. BUTD model won the VQA Challenge in 2017 and achieves 66.25% accuracy on VQA v2.0 test-dev.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Pythia</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/facebookresearch/pythia</span></span></span> extends the BUTD model by incorporating co-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> between question and image regions. Pythia uses features extracted from Detectron <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> pretrained on Visual Genome. An ensemble of Pythia models won the VQA Challenge in 2018 using additional training data from Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and using additional Resnet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> features. In this study, we use Pythia models which do not use Resnet features. Pythia without using Resnet features, achieves an accuracy of 68.43 % on VQA v2.0 test-dev.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.23" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.23.1.1" class="ltx_tr">
<th id="S5.T2.23.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt"><span id="S5.T2.23.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S5.T2.23.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S5.T2.23.1.1.2.1" class="ltx_text ltx_font_bold">val</span></td>
<td id="S5.T2.23.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.23.1.1.3.1" class="ltx_text ltx_font_bold">test-dev</span></td>
</tr>
<tr id="S5.T2.23.2.2" class="ltx_tr">
<th id="S5.T2.23.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S5.T2.23.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.04</td>
<td id="S5.T2.23.2.2.3" class="ltx_td ltx_align_center ltx_border_t">63.20</td>
</tr>
<tr id="S5.T2.23.3.3" class="ltx_tr">
<th id="S5.T2.23.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S5.T2.23.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.05</td>
<td id="S5.T2.23.3.3.3" class="ltx_td ltx_align_center ltx_border_t">66.25</td>
</tr>
<tr id="S5.T2.23.4.4" class="ltx_tr">
<th id="S5.T2.23.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">+ Q-consistency</th>
<td id="S5.T2.23.4.4.2" class="ltx_td ltx_align_center ltx_border_r">65.38</td>
<td id="S5.T2.23.4.4.3" class="ltx_td ltx_align_center">66.83</td>
</tr>
<tr id="S5.T2.23.5.5" class="ltx_tr">
<th id="S5.T2.23.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">   + A-consistency</th>
<td id="S5.T2.23.5.5.2" class="ltx_td ltx_align_center ltx_border_r">60.84</td>
<td id="S5.T2.23.5.5.3" class="ltx_td ltx_align_center">62.18</td>
</tr>
<tr id="S5.T2.23.6.6" class="ltx_tr">
<th id="S5.T2.23.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">    + Gating</th>
<td id="S5.T2.23.6.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.23.6.6.2.1" class="ltx_text ltx_font_bold">65.53</span></td>
<td id="S5.T2.23.6.6.3" class="ltx_td ltx_align_center"><span id="S5.T2.23.6.6.3.1" class="ltx_text ltx_font_bold">67.55</span></td>
</tr>
<tr id="S5.T2.23.7.7" class="ltx_tr">
<th id="S5.T2.23.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S5.T2.23.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">65.78</td>
<td id="S5.T2.23.7.7.3" class="ltx_td ltx_align_center ltx_border_t">68.43</td>
</tr>
<tr id="S5.T2.23.8.8" class="ltx_tr">
<th id="S5.T2.23.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">+ Q-consistency</th>
<td id="S5.T2.23.8.8.2" class="ltx_td ltx_align_center ltx_border_r">65.39</td>
<td id="S5.T2.23.8.8.3" class="ltx_td ltx_align_center">68.58</td>
</tr>
<tr id="S5.T2.23.9.9" class="ltx_tr">
<th id="S5.T2.23.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">   + A-consistency</th>
<td id="S5.T2.23.9.9.2" class="ltx_td ltx_align_center ltx_border_r">62.08</td>
<td id="S5.T2.23.9.9.3" class="ltx_td ltx_align_center">63.77</td>
</tr>
<tr id="S5.T2.23.10.10" class="ltx_tr">
<th id="S5.T2.23.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">    + Gating</th>
<td id="S5.T2.23.10.10.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T2.23.10.10.2.1" class="ltx_text ltx_font_bold">66.03</span></td>
<td id="S5.T2.23.10.10.3" class="ltx_td ltx_align_center"><span id="S5.T2.23.10.10.3.1" class="ltx_text ltx_font_bold">68.88</span></td>
</tr>
<tr id="S5.T2.23.11.11" class="ltx_tr">
<th id="S5.T2.23.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S5.T2.23.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">66.04</td>
<td id="S5.T2.23.11.11.3" class="ltx_td ltx_align_center ltx_border_t">69.64</td>
</tr>
<tr id="S5.T2.23.12.12" class="ltx_tr">
<th id="S5.T2.23.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">+ Q-consistency</th>
<td id="S5.T2.23.12.12.2" class="ltx_td ltx_align_center ltx_border_r">66.27</td>
<td id="S5.T2.23.12.12.3" class="ltx_td ltx_align_center">69.69</td>
</tr>
<tr id="S5.T2.23.13.13" class="ltx_tr">
<th id="S5.T2.23.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">   + A-consistency</th>
<td id="S5.T2.23.13.13.2" class="ltx_td ltx_align_center ltx_border_r">64.96</td>
<td id="S5.T2.23.13.13.3" class="ltx_td ltx_align_center">66.31</td>
</tr>
<tr id="S5.T2.23.14.14" class="ltx_tr">
<th id="S5.T2.23.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">    + Gating</th>
<td id="S5.T2.23.14.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T2.23.14.14.2.1" class="ltx_text ltx_font_bold">66.77</span></td>
<td id="S5.T2.23.14.14.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.23.14.14.3.1" class="ltx_text ltx_font_bold">69.87</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S5.T2.26.1" class="ltx_text ltx_font_bold">VQA Performance and ablation studies on VQA v2.0 validation and test-dev splits</span>. Each row in blocks represents a component of our cycle-consistent framework added to the previous row. First row in each block represents the baseline VQA model <math id="S5.T2.12.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.T2.12.m1.1b"><mi id="S5.T2.12.m1.1.1" xref="S5.T2.12.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.T2.12.m1.1c"><ci id="S5.T2.12.m1.1.1.cmml" xref="S5.T2.12.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.12.m1.1d">F</annotation></semantics></math>.
Q-consistency implies addition of a VQG module <math id="S5.T2.13.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S5.T2.13.m2.1b"><mi id="S5.T2.13.m2.1.1" xref="S5.T2.13.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S5.T2.13.m2.1c"><ci id="S5.T2.13.m2.1.1.cmml" xref="S5.T2.13.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.13.m2.1d">G</annotation></semantics></math> to generate rephrasings <math id="S5.T2.14.m3.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S5.T2.14.m3.1b"><msup id="S5.T2.14.m3.1.1" xref="S5.T2.14.m3.1.1.cmml"><mi id="S5.T2.14.m3.1.1.2" xref="S5.T2.14.m3.1.1.2.cmml">Q</mi><mo id="S5.T2.14.m3.1.1.3" xref="S5.T2.14.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.14.m3.1c"><apply id="S5.T2.14.m3.1.1.cmml" xref="S5.T2.14.m3.1.1"><csymbol cd="ambiguous" id="S5.T2.14.m3.1.1.1.cmml" xref="S5.T2.14.m3.1.1">superscript</csymbol><ci id="S5.T2.14.m3.1.1.2.cmml" xref="S5.T2.14.m3.1.1.2">𝑄</ci><ci id="S5.T2.14.m3.1.1.3.cmml" xref="S5.T2.14.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.14.m3.1d">Q^{\prime}</annotation></semantics></math> from the image <math id="S5.T2.15.m4.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.T2.15.m4.1b"><mi id="S5.T2.15.m4.1.1" xref="S5.T2.15.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.T2.15.m4.1c"><ci id="S5.T2.15.m4.1.1.cmml" xref="S5.T2.15.m4.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.15.m4.1d">I</annotation></semantics></math> and the predicted answer <math id="S5.T2.16.m5.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S5.T2.16.m5.1b"><msup id="S5.T2.16.m5.1.1" xref="S5.T2.16.m5.1.1.cmml"><mi id="S5.T2.16.m5.1.1.2" xref="S5.T2.16.m5.1.1.2.cmml">A</mi><mo id="S5.T2.16.m5.1.1.3" xref="S5.T2.16.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.16.m5.1c"><apply id="S5.T2.16.m5.1.1.cmml" xref="S5.T2.16.m5.1.1"><csymbol cd="ambiguous" id="S5.T2.16.m5.1.1.1.cmml" xref="S5.T2.16.m5.1.1">superscript</csymbol><ci id="S5.T2.16.m5.1.1.2.cmml" xref="S5.T2.16.m5.1.1.2">𝐴</ci><ci id="S5.T2.16.m5.1.1.3.cmml" xref="S5.T2.16.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.16.m5.1d">A^{\prime}</annotation></semantics></math> with an associated VQG loss <math id="S5.T2.17.m6.2" class="ltx_Math" alttext="\mathcal{L}_{vqg}(Q,Q^{\prime})" display="inline"><semantics id="S5.T2.17.m6.2b"><mrow id="S5.T2.17.m6.2.2" xref="S5.T2.17.m6.2.2.cmml"><msub id="S5.T2.17.m6.2.2.3" xref="S5.T2.17.m6.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T2.17.m6.2.2.3.2" xref="S5.T2.17.m6.2.2.3.2.cmml">ℒ</mi><mrow id="S5.T2.17.m6.2.2.3.3" xref="S5.T2.17.m6.2.2.3.3.cmml"><mi id="S5.T2.17.m6.2.2.3.3.2" xref="S5.T2.17.m6.2.2.3.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S5.T2.17.m6.2.2.3.3.1" xref="S5.T2.17.m6.2.2.3.3.1.cmml">​</mo><mi id="S5.T2.17.m6.2.2.3.3.3" xref="S5.T2.17.m6.2.2.3.3.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S5.T2.17.m6.2.2.3.3.1b" xref="S5.T2.17.m6.2.2.3.3.1.cmml">​</mo><mi id="S5.T2.17.m6.2.2.3.3.4" xref="S5.T2.17.m6.2.2.3.3.4.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.T2.17.m6.2.2.2" xref="S5.T2.17.m6.2.2.2.cmml">​</mo><mrow id="S5.T2.17.m6.2.2.1.1" xref="S5.T2.17.m6.2.2.1.2.cmml"><mo stretchy="false" id="S5.T2.17.m6.2.2.1.1.2" xref="S5.T2.17.m6.2.2.1.2.cmml">(</mo><mi id="S5.T2.17.m6.1.1" xref="S5.T2.17.m6.1.1.cmml">Q</mi><mo id="S5.T2.17.m6.2.2.1.1.3" xref="S5.T2.17.m6.2.2.1.2.cmml">,</mo><msup id="S5.T2.17.m6.2.2.1.1.1" xref="S5.T2.17.m6.2.2.1.1.1.cmml"><mi id="S5.T2.17.m6.2.2.1.1.1.2" xref="S5.T2.17.m6.2.2.1.1.1.2.cmml">Q</mi><mo id="S5.T2.17.m6.2.2.1.1.1.3" xref="S5.T2.17.m6.2.2.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S5.T2.17.m6.2.2.1.1.4" xref="S5.T2.17.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.17.m6.2c"><apply id="S5.T2.17.m6.2.2.cmml" xref="S5.T2.17.m6.2.2"><times id="S5.T2.17.m6.2.2.2.cmml" xref="S5.T2.17.m6.2.2.2"></times><apply id="S5.T2.17.m6.2.2.3.cmml" xref="S5.T2.17.m6.2.2.3"><csymbol cd="ambiguous" id="S5.T2.17.m6.2.2.3.1.cmml" xref="S5.T2.17.m6.2.2.3">subscript</csymbol><ci id="S5.T2.17.m6.2.2.3.2.cmml" xref="S5.T2.17.m6.2.2.3.2">ℒ</ci><apply id="S5.T2.17.m6.2.2.3.3.cmml" xref="S5.T2.17.m6.2.2.3.3"><times id="S5.T2.17.m6.2.2.3.3.1.cmml" xref="S5.T2.17.m6.2.2.3.3.1"></times><ci id="S5.T2.17.m6.2.2.3.3.2.cmml" xref="S5.T2.17.m6.2.2.3.3.2">𝑣</ci><ci id="S5.T2.17.m6.2.2.3.3.3.cmml" xref="S5.T2.17.m6.2.2.3.3.3">𝑞</ci><ci id="S5.T2.17.m6.2.2.3.3.4.cmml" xref="S5.T2.17.m6.2.2.3.3.4">𝑔</ci></apply></apply><interval closure="open" id="S5.T2.17.m6.2.2.1.2.cmml" xref="S5.T2.17.m6.2.2.1.1"><ci id="S5.T2.17.m6.1.1.cmml" xref="S5.T2.17.m6.1.1">𝑄</ci><apply id="S5.T2.17.m6.2.2.1.1.1.cmml" xref="S5.T2.17.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.T2.17.m6.2.2.1.1.1.1.cmml" xref="S5.T2.17.m6.2.2.1.1.1">superscript</csymbol><ci id="S5.T2.17.m6.2.2.1.1.1.2.cmml" xref="S5.T2.17.m6.2.2.1.1.1.2">𝑄</ci><ci id="S5.T2.17.m6.2.2.1.1.1.3.cmml" xref="S5.T2.17.m6.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.17.m6.2d">\mathcal{L}_{vqg}(Q,Q^{\prime})</annotation></semantics></math>.
A-consistency implies passing all the generated questions <math id="S5.T2.18.m7.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S5.T2.18.m7.1b"><msup id="S5.T2.18.m7.1.1" xref="S5.T2.18.m7.1.1.cmml"><mi id="S5.T2.18.m7.1.1.2" xref="S5.T2.18.m7.1.1.2.cmml">Q</mi><mo id="S5.T2.18.m7.1.1.3" xref="S5.T2.18.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.18.m7.1c"><apply id="S5.T2.18.m7.1.1.cmml" xref="S5.T2.18.m7.1.1"><csymbol cd="ambiguous" id="S5.T2.18.m7.1.1.1.cmml" xref="S5.T2.18.m7.1.1">superscript</csymbol><ci id="S5.T2.18.m7.1.1.2.cmml" xref="S5.T2.18.m7.1.1.2">𝑄</ci><ci id="S5.T2.18.m7.1.1.3.cmml" xref="S5.T2.18.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.18.m7.1d">Q^{\prime}</annotation></semantics></math> to the VQA model <math id="S5.T2.19.m8.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.T2.19.m8.1b"><mi id="S5.T2.19.m8.1.1" xref="S5.T2.19.m8.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.T2.19.m8.1c"><ci id="S5.T2.19.m8.1.1.cmml" xref="S5.T2.19.m8.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.19.m8.1d">F</annotation></semantics></math> and an associated loss <math id="S5.T2.20.m9.2" class="ltx_Math" alttext="\mathcal{L}_{cycle}(A,A^{\prime})" display="inline"><semantics id="S5.T2.20.m9.2b"><mrow id="S5.T2.20.m9.2.2" xref="S5.T2.20.m9.2.2.cmml"><msub id="S5.T2.20.m9.2.2.3" xref="S5.T2.20.m9.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.T2.20.m9.2.2.3.2" xref="S5.T2.20.m9.2.2.3.2.cmml">ℒ</mi><mrow id="S5.T2.20.m9.2.2.3.3" xref="S5.T2.20.m9.2.2.3.3.cmml"><mi id="S5.T2.20.m9.2.2.3.3.2" xref="S5.T2.20.m9.2.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T2.20.m9.2.2.3.3.1" xref="S5.T2.20.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.T2.20.m9.2.2.3.3.3" xref="S5.T2.20.m9.2.2.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S5.T2.20.m9.2.2.3.3.1b" xref="S5.T2.20.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.T2.20.m9.2.2.3.3.4" xref="S5.T2.20.m9.2.2.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T2.20.m9.2.2.3.3.1c" xref="S5.T2.20.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.T2.20.m9.2.2.3.3.5" xref="S5.T2.20.m9.2.2.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.T2.20.m9.2.2.3.3.1d" xref="S5.T2.20.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.T2.20.m9.2.2.3.3.6" xref="S5.T2.20.m9.2.2.3.3.6.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.T2.20.m9.2.2.2" xref="S5.T2.20.m9.2.2.2.cmml">​</mo><mrow id="S5.T2.20.m9.2.2.1.1" xref="S5.T2.20.m9.2.2.1.2.cmml"><mo stretchy="false" id="S5.T2.20.m9.2.2.1.1.2" xref="S5.T2.20.m9.2.2.1.2.cmml">(</mo><mi id="S5.T2.20.m9.1.1" xref="S5.T2.20.m9.1.1.cmml">A</mi><mo id="S5.T2.20.m9.2.2.1.1.3" xref="S5.T2.20.m9.2.2.1.2.cmml">,</mo><msup id="S5.T2.20.m9.2.2.1.1.1" xref="S5.T2.20.m9.2.2.1.1.1.cmml"><mi id="S5.T2.20.m9.2.2.1.1.1.2" xref="S5.T2.20.m9.2.2.1.1.1.2.cmml">A</mi><mo id="S5.T2.20.m9.2.2.1.1.1.3" xref="S5.T2.20.m9.2.2.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S5.T2.20.m9.2.2.1.1.4" xref="S5.T2.20.m9.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.20.m9.2c"><apply id="S5.T2.20.m9.2.2.cmml" xref="S5.T2.20.m9.2.2"><times id="S5.T2.20.m9.2.2.2.cmml" xref="S5.T2.20.m9.2.2.2"></times><apply id="S5.T2.20.m9.2.2.3.cmml" xref="S5.T2.20.m9.2.2.3"><csymbol cd="ambiguous" id="S5.T2.20.m9.2.2.3.1.cmml" xref="S5.T2.20.m9.2.2.3">subscript</csymbol><ci id="S5.T2.20.m9.2.2.3.2.cmml" xref="S5.T2.20.m9.2.2.3.2">ℒ</ci><apply id="S5.T2.20.m9.2.2.3.3.cmml" xref="S5.T2.20.m9.2.2.3.3"><times id="S5.T2.20.m9.2.2.3.3.1.cmml" xref="S5.T2.20.m9.2.2.3.3.1"></times><ci id="S5.T2.20.m9.2.2.3.3.2.cmml" xref="S5.T2.20.m9.2.2.3.3.2">𝑐</ci><ci id="S5.T2.20.m9.2.2.3.3.3.cmml" xref="S5.T2.20.m9.2.2.3.3.3">𝑦</ci><ci id="S5.T2.20.m9.2.2.3.3.4.cmml" xref="S5.T2.20.m9.2.2.3.3.4">𝑐</ci><ci id="S5.T2.20.m9.2.2.3.3.5.cmml" xref="S5.T2.20.m9.2.2.3.3.5">𝑙</ci><ci id="S5.T2.20.m9.2.2.3.3.6.cmml" xref="S5.T2.20.m9.2.2.3.3.6">𝑒</ci></apply></apply><interval closure="open" id="S5.T2.20.m9.2.2.1.2.cmml" xref="S5.T2.20.m9.2.2.1.1"><ci id="S5.T2.20.m9.1.1.cmml" xref="S5.T2.20.m9.1.1">𝐴</ci><apply id="S5.T2.20.m9.2.2.1.1.1.cmml" xref="S5.T2.20.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.T2.20.m9.2.2.1.1.1.1.cmml" xref="S5.T2.20.m9.2.2.1.1.1">superscript</csymbol><ci id="S5.T2.20.m9.2.2.1.1.1.2.cmml" xref="S5.T2.20.m9.2.2.1.1.1.2">𝐴</ci><ci id="S5.T2.20.m9.2.2.1.1.1.3.cmml" xref="S5.T2.20.m9.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.20.m9.2d">\mathcal{L}_{cycle}(A,A^{\prime})</annotation></semantics></math>.
Gating implies the use of gating mechanism to filter undesirable generated questions in <math id="S5.T2.21.m10.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S5.T2.21.m10.1b"><msup id="S5.T2.21.m10.1.1" xref="S5.T2.21.m10.1.1.cmml"><mi id="S5.T2.21.m10.1.1.2" xref="S5.T2.21.m10.1.1.2.cmml">Q</mi><mo id="S5.T2.21.m10.1.1.3" xref="S5.T2.21.m10.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T2.21.m10.1c"><apply id="S5.T2.21.m10.1.1.cmml" xref="S5.T2.21.m10.1.1"><csymbol cd="ambiguous" id="S5.T2.21.m10.1.1.1.cmml" xref="S5.T2.21.m10.1.1">superscript</csymbol><ci id="S5.T2.21.m10.1.1.2.cmml" xref="S5.T2.21.m10.1.1.2">𝑄</ci><ci id="S5.T2.21.m10.1.1.3.cmml" xref="S5.T2.21.m10.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.21.m10.1d">Q^{\prime}</annotation></semantics></math> and passing the remaining to VQA model <math id="S5.T2.22.m11.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.T2.22.m11.1b"><mi id="S5.T2.22.m11.1.1" xref="S5.T2.22.m11.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.T2.22.m11.1c"><ci id="S5.T2.22.m11.1.1.cmml" xref="S5.T2.22.m11.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.22.m11.1d">F</annotation></semantics></math>.
Models trained with our cycle-consistent <span id="S5.T2.27.2" class="ltx_text" style="color:#000000;">(last row in each block)</span> framework consistently outperform baselines.</figcaption>
</figure>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p"><span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_bold">Bilinear Attention Networks (BAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> <span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span id="footnote4.1.1.1" class="ltx_text ltx_font_medium">4</span></span><span id="footnote4.5" class="ltx_text ltx_font_medium">https://github.com/jnhwkim/ban-vqa</span></span></span></span></span> combines the idea of bilinear models and co-attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> between image regions and words in questions in a residual setting. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, it uses Faster-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> pretrained on Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to extract image features. In all our experiments, for a fair comparison, we use BAN models which do not use additional training data from Visual Genome. BAN achieves the current state-of-the-art single-model accuracy of 69.64 % on VQA v2.0 test-dev without using additional training data from Visual Genome.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/1902.05660/assets/x4.png" id="S5.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="215" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
<span id="S5.F4.3.1" class="ltx_text ltx_font_bold">Visualization of textual and image region attention across question variants:</span> The top row shows attention and predictions from a Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> model, the bottom row shows attention and predictions from the same Pythia model, but trained using our cycle-consistent approach. Our model attends to relevant image regions for all rephrasings and answers them correctly. The baseline Pythia counterpart, however, fails to attend over relevant image regions for some rephrasings.
</figcaption>
</figure>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">BLEU-1</span></th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.3.1" class="ltx_text ltx_font_bold">BLEU-2</span></th>
<th id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">BLEU-3</span></th>
<th id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">BLEU-4</span></th>
<th id="S5.T3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.6.1" class="ltx_text ltx_font_bold">ROUGE-L</span></th>
<th id="S5.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T3.1.1.1.7.1" class="ltx_text ltx_font_bold">METEOR</span></th>
<th id="S5.T3.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.1.1.1.8.1" class="ltx_text ltx_font_bold">CIDER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<td id="S5.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iQAN* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.582</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.467</td>
<td id="S5.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.385</td>
<td id="S5.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.320</td>
<td id="S5.T3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.617</td>
<td id="S5.T3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.276</td>
<td id="S5.T3.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">2.222</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<td id="S5.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">Pythia + CC*</td>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.2.2.1" class="ltx_text ltx_font_bold">0.708</span></td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">0.561</span></td>
<td id="S5.T3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.2.4.1" class="ltx_text ltx_font_bold">0.438</span></td>
<td id="S5.T3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.2.5.1" class="ltx_text ltx_font_bold">0.339</span></td>
<td id="S5.T3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.2.6.1" class="ltx_text ltx_font_bold">0.627</span></td>
<td id="S5.T3.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.1.3.2.7.1" class="ltx_text ltx_font_bold">0.284</span></td>
<td id="S5.T3.1.3.2.8" class="ltx_td ltx_align_center"><span id="S5.T3.1.3.2.8.1" class="ltx_text ltx_font_bold">2.301</span></td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<td id="S5.T3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">iVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.430</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.326</td>
<td id="S5.T3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.256</td>
<td id="S5.T3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.208</td>
<td id="S5.T3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.468</td>
<td id="S5.T3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.205</td>
<td id="S5.T3.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t">1.714</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<td id="S5.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Pythia + CC</td>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.1.5.4.2.1" class="ltx_text ltx_font_bold">0.486</span></td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.1.5.4.3.1" class="ltx_text ltx_font_bold">0.368</span></td>
<td id="S5.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.1.5.4.4.1" class="ltx_text ltx_font_bold">0.287</span></td>
<td id="S5.T3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.1.5.4.5.1" class="ltx_text ltx_font_bold">0.226</span></td>
<td id="S5.T3.1.5.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.1.5.4.6.1" class="ltx_text ltx_font_bold">0.556</span></td>
<td id="S5.T3.1.5.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T3.1.5.4.7.1" class="ltx_text ltx_font_bold">0.225</span></td>
<td id="S5.T3.1.5.4.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.1.5.4.8.1" class="ltx_text ltx_font_bold">1.843</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S5.T3.3.1" class="ltx_text ltx_font_bold">Question Generation Performance on VQA v2.0 validation set</span>, * signifies results on a constrained subset as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. CC represents models trained with our approach.</figcaption>
</figure>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.4" class="ltx_p"><span id="S5.SS1.p6.4.1" class="ltx_text ltx_font_bold">Implementation Details</span>
For all models trained with our cycle-consistent framework, we use the values <math id="S5.SS1.p6.1.m1.1" class="ltx_Math" alttext="T_{sim}{=}0.9" display="inline"><semantics id="S5.SS1.p6.1.m1.1a"><mrow id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml"><msub id="S5.SS1.p6.1.m1.1.1.2" xref="S5.SS1.p6.1.m1.1.1.2.cmml"><mi id="S5.SS1.p6.1.m1.1.1.2.2" xref="S5.SS1.p6.1.m1.1.1.2.2.cmml">T</mi><mrow id="S5.SS1.p6.1.m1.1.1.2.3" xref="S5.SS1.p6.1.m1.1.1.2.3.cmml"><mi id="S5.SS1.p6.1.m1.1.1.2.3.2" xref="S5.SS1.p6.1.m1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.2.3.1" xref="S5.SS1.p6.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.2.3.3" xref="S5.SS1.p6.1.m1.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.2.3.1a" xref="S5.SS1.p6.1.m1.1.1.2.3.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.2.3.4" xref="S5.SS1.p6.1.m1.1.1.2.3.4.cmml">m</mi></mrow></msub><mo id="S5.SS1.p6.1.m1.1.1.1" xref="S5.SS1.p6.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p6.1.m1.1.1.3" xref="S5.SS1.p6.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><apply id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1"><eq id="S5.SS1.p6.1.m1.1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1.1"></eq><apply id="S5.SS1.p6.1.m1.1.1.2.cmml" xref="S5.SS1.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p6.1.m1.1.1.2.1.cmml" xref="S5.SS1.p6.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS1.p6.1.m1.1.1.2.2.cmml" xref="S5.SS1.p6.1.m1.1.1.2.2">𝑇</ci><apply id="S5.SS1.p6.1.m1.1.1.2.3.cmml" xref="S5.SS1.p6.1.m1.1.1.2.3"><times id="S5.SS1.p6.1.m1.1.1.2.3.1.cmml" xref="S5.SS1.p6.1.m1.1.1.2.3.1"></times><ci id="S5.SS1.p6.1.m1.1.1.2.3.2.cmml" xref="S5.SS1.p6.1.m1.1.1.2.3.2">𝑠</ci><ci id="S5.SS1.p6.1.m1.1.1.2.3.3.cmml" xref="S5.SS1.p6.1.m1.1.1.2.3.3">𝑖</ci><ci id="S5.SS1.p6.1.m1.1.1.2.3.4.cmml" xref="S5.SS1.p6.1.m1.1.1.2.3.4">𝑚</ci></apply></apply><cn type="float" id="S5.SS1.p6.1.m1.1.1.3.cmml" xref="S5.SS1.p6.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">T_{sim}{=}0.9</annotation></semantics></math>, <math id="S5.SS1.p6.2.m2.1" class="ltx_Math" alttext="\lambda_{G}{=}1.0" display="inline"><semantics id="S5.SS1.p6.2.m2.1a"><mrow id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml"><msub id="S5.SS1.p6.2.m2.1.1.2" xref="S5.SS1.p6.2.m2.1.1.2.cmml"><mi id="S5.SS1.p6.2.m2.1.1.2.2" xref="S5.SS1.p6.2.m2.1.1.2.2.cmml">λ</mi><mi id="S5.SS1.p6.2.m2.1.1.2.3" xref="S5.SS1.p6.2.m2.1.1.2.3.cmml">G</mi></msub><mo id="S5.SS1.p6.2.m2.1.1.1" xref="S5.SS1.p6.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p6.2.m2.1.1.3" xref="S5.SS1.p6.2.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.1b"><apply id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1"><eq id="S5.SS1.p6.2.m2.1.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1.1"></eq><apply id="S5.SS1.p6.2.m2.1.1.2.cmml" xref="S5.SS1.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p6.2.m2.1.1.2.1.cmml" xref="S5.SS1.p6.2.m2.1.1.2">subscript</csymbol><ci id="S5.SS1.p6.2.m2.1.1.2.2.cmml" xref="S5.SS1.p6.2.m2.1.1.2.2">𝜆</ci><ci id="S5.SS1.p6.2.m2.1.1.2.3.cmml" xref="S5.SS1.p6.2.m2.1.1.2.3">𝐺</ci></apply><cn type="float" id="S5.SS1.p6.2.m2.1.1.3.cmml" xref="S5.SS1.p6.2.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.1c">\lambda_{G}{=}1.0</annotation></semantics></math>, <math id="S5.SS1.p6.3.m3.1" class="ltx_Math" alttext="\lambda_{C}{=}0.5" display="inline"><semantics id="S5.SS1.p6.3.m3.1a"><mrow id="S5.SS1.p6.3.m3.1.1" xref="S5.SS1.p6.3.m3.1.1.cmml"><msub id="S5.SS1.p6.3.m3.1.1.2" xref="S5.SS1.p6.3.m3.1.1.2.cmml"><mi id="S5.SS1.p6.3.m3.1.1.2.2" xref="S5.SS1.p6.3.m3.1.1.2.2.cmml">λ</mi><mi id="S5.SS1.p6.3.m3.1.1.2.3" xref="S5.SS1.p6.3.m3.1.1.2.3.cmml">C</mi></msub><mo id="S5.SS1.p6.3.m3.1.1.1" xref="S5.SS1.p6.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS1.p6.3.m3.1.1.3" xref="S5.SS1.p6.3.m3.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.3.m3.1b"><apply id="S5.SS1.p6.3.m3.1.1.cmml" xref="S5.SS1.p6.3.m3.1.1"><eq id="S5.SS1.p6.3.m3.1.1.1.cmml" xref="S5.SS1.p6.3.m3.1.1.1"></eq><apply id="S5.SS1.p6.3.m3.1.1.2.cmml" xref="S5.SS1.p6.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p6.3.m3.1.1.2.1.cmml" xref="S5.SS1.p6.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS1.p6.3.m3.1.1.2.2.cmml" xref="S5.SS1.p6.3.m3.1.1.2.2">𝜆</ci><ci id="S5.SS1.p6.3.m3.1.1.2.3.cmml" xref="S5.SS1.p6.3.m3.1.1.2.3">𝐶</ci></apply><cn type="float" id="S5.SS1.p6.3.m3.1.1.3.cmml" xref="S5.SS1.p6.3.m3.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.3.m3.1c">\lambda_{C}{=}0.5</annotation></semantics></math> and <math id="S5.SS1.p6.4.m4.1" class="ltx_Math" alttext="A_{iter}{=}5500" display="inline"><semantics id="S5.SS1.p6.4.m4.1a"><mrow id="S5.SS1.p6.4.m4.1.1" xref="S5.SS1.p6.4.m4.1.1.cmml"><msub id="S5.SS1.p6.4.m4.1.1.2" xref="S5.SS1.p6.4.m4.1.1.2.cmml"><mi id="S5.SS1.p6.4.m4.1.1.2.2" xref="S5.SS1.p6.4.m4.1.1.2.2.cmml">A</mi><mrow id="S5.SS1.p6.4.m4.1.1.2.3" xref="S5.SS1.p6.4.m4.1.1.2.3.cmml"><mi id="S5.SS1.p6.4.m4.1.1.2.3.2" xref="S5.SS1.p6.4.m4.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.4.m4.1.1.2.3.1" xref="S5.SS1.p6.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S5.SS1.p6.4.m4.1.1.2.3.3" xref="S5.SS1.p6.4.m4.1.1.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.4.m4.1.1.2.3.1a" xref="S5.SS1.p6.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S5.SS1.p6.4.m4.1.1.2.3.4" xref="S5.SS1.p6.4.m4.1.1.2.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.4.m4.1.1.2.3.1b" xref="S5.SS1.p6.4.m4.1.1.2.3.1.cmml">​</mo><mi id="S5.SS1.p6.4.m4.1.1.2.3.5" xref="S5.SS1.p6.4.m4.1.1.2.3.5.cmml">r</mi></mrow></msub><mo id="S5.SS1.p6.4.m4.1.1.1" xref="S5.SS1.p6.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS1.p6.4.m4.1.1.3" xref="S5.SS1.p6.4.m4.1.1.3.cmml">5500</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.4.m4.1b"><apply id="S5.SS1.p6.4.m4.1.1.cmml" xref="S5.SS1.p6.4.m4.1.1"><eq id="S5.SS1.p6.4.m4.1.1.1.cmml" xref="S5.SS1.p6.4.m4.1.1.1"></eq><apply id="S5.SS1.p6.4.m4.1.1.2.cmml" xref="S5.SS1.p6.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p6.4.m4.1.1.2.1.cmml" xref="S5.SS1.p6.4.m4.1.1.2">subscript</csymbol><ci id="S5.SS1.p6.4.m4.1.1.2.2.cmml" xref="S5.SS1.p6.4.m4.1.1.2.2">𝐴</ci><apply id="S5.SS1.p6.4.m4.1.1.2.3.cmml" xref="S5.SS1.p6.4.m4.1.1.2.3"><times id="S5.SS1.p6.4.m4.1.1.2.3.1.cmml" xref="S5.SS1.p6.4.m4.1.1.2.3.1"></times><ci id="S5.SS1.p6.4.m4.1.1.2.3.2.cmml" xref="S5.SS1.p6.4.m4.1.1.2.3.2">𝑖</ci><ci id="S5.SS1.p6.4.m4.1.1.2.3.3.cmml" xref="S5.SS1.p6.4.m4.1.1.2.3.3">𝑡</ci><ci id="S5.SS1.p6.4.m4.1.1.2.3.4.cmml" xref="S5.SS1.p6.4.m4.1.1.2.3.4">𝑒</ci><ci id="S5.SS1.p6.4.m4.1.1.2.3.5.cmml" xref="S5.SS1.p6.4.m4.1.1.2.3.5">𝑟</ci></apply></apply><cn type="integer" id="S5.SS1.p6.4.m4.1.1.3.cmml" xref="S5.SS1.p6.4.m4.1.1.3">5500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.4.m4.1c">A_{iter}{=}5500</annotation></semantics></math>.
When reporting results on the validation split and VQA-Rephrasings we train on the training split and when reporting results on the test split we train on both training and validation splits of VQA v2.0.
Note that we <em id="S5.SS1.p6.4.2" class="ltx_emph ltx_font_italic">never</em> explicitly train on the collected VQA-Rephrasings dataset and use it purely for evaluation purposes. We use publicly available implementations of each backbone VQA model.
The hidden size of the LSTM used in VQG module is 1024 and the linear encoders used to encode the answer and image in VQG have dimensions of 300 each.
Additional details about model-specific hyperparameters can be found in Appendix <a href="#A5" title="Appendix E Hyperparameters ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p id="S5.SS1.p7.4" class="ltx_p">We measure the robustness of each of these models on our proposed VQA-Rephrasings dataset using the consensus score (Eq. <a href="#S4.E2" title="In 4 VQA-Rephrasings Dataset ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Table <a href="#S5.T1" title="Table 1 ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the consensus scores at different values of <math id="S5.SS1.p7.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p7.1.m1.1a"><mi id="S5.SS1.p7.1.m1.1.1" xref="S5.SS1.p7.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.1b"><ci id="S5.SS1.p7.1.m1.1.1.cmml" xref="S5.SS1.p7.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.1c">k</annotation></semantics></math> for several VQA models. We see that all models suffer significantly when measured for consistency across rephrasings.
For <em id="S5.SS1.p7.4.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S5.SS1.p7.4.2" class="ltx_text"></span>, the performance of Pythia (winner of 2018 VQA challenge) is reduced to a consensus score of 39.49% at <math id="S5.SS1.p7.2.m2.1" class="ltx_Math" alttext="k=4" display="inline"><semantics id="S5.SS1.p7.2.m2.1a"><mrow id="S5.SS1.p7.2.m2.1.1" xref="S5.SS1.p7.2.m2.1.1.cmml"><mi id="S5.SS1.p7.2.m2.1.1.2" xref="S5.SS1.p7.2.m2.1.1.2.cmml">k</mi><mo id="S5.SS1.p7.2.m2.1.1.1" xref="S5.SS1.p7.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS1.p7.2.m2.1.1.3" xref="S5.SS1.p7.2.m2.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.2.m2.1b"><apply id="S5.SS1.p7.2.m2.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1"><eq id="S5.SS1.p7.2.m2.1.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1.1"></eq><ci id="S5.SS1.p7.2.m2.1.1.2.cmml" xref="S5.SS1.p7.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S5.SS1.p7.2.m2.1.1.3.cmml" xref="S5.SS1.p7.2.m2.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.2.m2.1c">k=4</annotation></semantics></math>.
Similar trends are observed for MUTAN, BAN and BUTD. The drop increases with increasing <math id="S5.SS1.p7.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p7.3.m3.1a"><mi id="S5.SS1.p7.3.m3.1.1" xref="S5.SS1.p7.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.3.m3.1b"><ci id="S5.SS1.p7.3.m3.1.1.cmml" xref="S5.SS1.p7.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.3.m3.1c">k</annotation></semantics></math>, the number of rephrasings used to measure consistency. Models like BUTD, BAN and Pythia which use word-level encodings of the question suffer significant drops. It is interesting to note that even MUTAN which uses skip-thought based sentence encoding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> suffers a drop when checked for consistency across rephrasings. We observe that BAN + CC model trained with our proposed cycle-consistent training framework consistently outperforms its counterpart BAN and all other models at all values of <math id="S5.SS1.p7.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS1.p7.4.m4.1a"><mi id="S5.SS1.p7.4.m4.1.1" xref="S5.SS1.p7.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.4.m4.1b"><ci id="S5.SS1.p7.4.m4.1.1.cmml" xref="S5.SS1.p7.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.4.m4.1c">k</annotation></semantics></math>.</p>
</div>
<div id="S5.SS1.p8" class="ltx_para">
<p id="S5.SS1.p8.1" class="ltx_p">Fig <a href="#S5.F4" title="Figure 4 ‣ 5.1 Consistency Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> qualitatively compares the textual and visual attention (over image regions) over 4 rephrasings of a question. The top row shows attention and predictions from a Pythia model, while the bottom row shows attention and predictions from the same Pythia model, but trained using our framework. Our model attends at relevant image regions for all rephrasings and answers all of them correctly. The Pythia counterpart, however, fails to attend over relevant image regions for some rephrasings and answers those rephrasings incorrectly. This qualitatively demonstrates the robustness of models trained with our framework.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Visual Question Answering Performance</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We now evaluate our approach and various ablations on the standard task of question answering on VQA v2.0 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
We compare the performance of several VQA models on the validation and test-dev splits of VQA v2.0. It consists of 443,757 training, 214,354 validation and 447,793 testing questions
spanning over 82,783, 40,504 and 81,434 images respectively. Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Consistency Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the VQA scores of different models on validation and test-dev splits. We show that BUTD, Pythia and BAN models trained with our cycle-consistent framework outperform their corresponding baselines.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.13" class="ltx_p">We show the impact of each component of our cycle-consistent framework by performing ablation studies on our models. We study the marginal effect of components like question consistency (Q-consistency), answer consistency (A-consistency) and gating mechanism by adding them step-by-step to the base VQA model <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">F</annotation></semantics></math>.
Q-consistency implies addition of a VQG module <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">G</annotation></semantics></math> to generate rephrasings <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><msup id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><mi id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">Q</mi><mo id="S5.SS2.p2.3.m3.1.1.3" xref="S5.SS2.p2.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">superscript</csymbol><ci id="S5.SS2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.2">𝑄</ci><ci id="S5.SS2.p2.3.m3.1.1.3.cmml" xref="S5.SS2.p2.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">Q^{\prime}</annotation></semantics></math> from the image <math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">I</annotation></semantics></math> and the predicted answer <math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><msup id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml"><mi id="S5.SS2.p2.5.m5.1.1.2" xref="S5.SS2.p2.5.m5.1.1.2.cmml">A</mi><mo id="S5.SS2.p2.5.m5.1.1.3" xref="S5.SS2.p2.5.m5.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><apply id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">superscript</csymbol><ci id="S5.SS2.p2.5.m5.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.2">𝐴</ci><ci id="S5.SS2.p2.5.m5.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">A^{\prime}</annotation></semantics></math> with an associated VQG loss <math id="S5.SS2.p2.6.m6.2" class="ltx_Math" alttext="\mathcal{L}_{vqg}(Q,Q^{\prime})" display="inline"><semantics id="S5.SS2.p2.6.m6.2a"><mrow id="S5.SS2.p2.6.m6.2.2" xref="S5.SS2.p2.6.m6.2.2.cmml"><msub id="S5.SS2.p2.6.m6.2.2.3" xref="S5.SS2.p2.6.m6.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p2.6.m6.2.2.3.2" xref="S5.SS2.p2.6.m6.2.2.3.2.cmml">ℒ</mi><mrow id="S5.SS2.p2.6.m6.2.2.3.3" xref="S5.SS2.p2.6.m6.2.2.3.3.cmml"><mi id="S5.SS2.p2.6.m6.2.2.3.3.2" xref="S5.SS2.p2.6.m6.2.2.3.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.6.m6.2.2.3.3.1" xref="S5.SS2.p2.6.m6.2.2.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.6.m6.2.2.3.3.3" xref="S5.SS2.p2.6.m6.2.2.3.3.3.cmml">q</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.6.m6.2.2.3.3.1a" xref="S5.SS2.p2.6.m6.2.2.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.6.m6.2.2.3.3.4" xref="S5.SS2.p2.6.m6.2.2.3.3.4.cmml">g</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.6.m6.2.2.2" xref="S5.SS2.p2.6.m6.2.2.2.cmml">​</mo><mrow id="S5.SS2.p2.6.m6.2.2.1.1" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.6.m6.2.2.1.1.2" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml">(</mo><mi id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">Q</mi><mo id="S5.SS2.p2.6.m6.2.2.1.1.3" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml">,</mo><msup id="S5.SS2.p2.6.m6.2.2.1.1.1" xref="S5.SS2.p2.6.m6.2.2.1.1.1.cmml"><mi id="S5.SS2.p2.6.m6.2.2.1.1.1.2" xref="S5.SS2.p2.6.m6.2.2.1.1.1.2.cmml">Q</mi><mo id="S5.SS2.p2.6.m6.2.2.1.1.1.3" xref="S5.SS2.p2.6.m6.2.2.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S5.SS2.p2.6.m6.2.2.1.1.4" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.2b"><apply id="S5.SS2.p2.6.m6.2.2.cmml" xref="S5.SS2.p2.6.m6.2.2"><times id="S5.SS2.p2.6.m6.2.2.2.cmml" xref="S5.SS2.p2.6.m6.2.2.2"></times><apply id="S5.SS2.p2.6.m6.2.2.3.cmml" xref="S5.SS2.p2.6.m6.2.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.2.2.3.1.cmml" xref="S5.SS2.p2.6.m6.2.2.3">subscript</csymbol><ci id="S5.SS2.p2.6.m6.2.2.3.2.cmml" xref="S5.SS2.p2.6.m6.2.2.3.2">ℒ</ci><apply id="S5.SS2.p2.6.m6.2.2.3.3.cmml" xref="S5.SS2.p2.6.m6.2.2.3.3"><times id="S5.SS2.p2.6.m6.2.2.3.3.1.cmml" xref="S5.SS2.p2.6.m6.2.2.3.3.1"></times><ci id="S5.SS2.p2.6.m6.2.2.3.3.2.cmml" xref="S5.SS2.p2.6.m6.2.2.3.3.2">𝑣</ci><ci id="S5.SS2.p2.6.m6.2.2.3.3.3.cmml" xref="S5.SS2.p2.6.m6.2.2.3.3.3">𝑞</ci><ci id="S5.SS2.p2.6.m6.2.2.3.3.4.cmml" xref="S5.SS2.p2.6.m6.2.2.3.3.4">𝑔</ci></apply></apply><interval closure="open" id="S5.SS2.p2.6.m6.2.2.1.2.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1"><ci id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1">𝑄</ci><apply id="S5.SS2.p2.6.m6.2.2.1.1.1.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.2.2.1.1.1.1.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1">superscript</csymbol><ci id="S5.SS2.p2.6.m6.2.2.1.1.1.2.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.2">𝑄</ci><ci id="S5.SS2.p2.6.m6.2.2.1.1.1.3.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.2c">\mathcal{L}_{vqg}(Q,Q^{\prime})</annotation></semantics></math>. As shown in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Consistency Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we see that addition of question consistency slightly improves performance of each VQA model. Inline with observations in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, this shows that indeed models which can generate questions from the answer have better multi-modal understanding and in turn are better at visual question answering.
A-consistency implies passing all the generated questions <math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><msup id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml"><mi id="S5.SS2.p2.7.m7.1.1.2" xref="S5.SS2.p2.7.m7.1.1.2.cmml">Q</mi><mo id="S5.SS2.p2.7.m7.1.1.3" xref="S5.SS2.p2.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><apply id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.7.m7.1.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">superscript</csymbol><ci id="S5.SS2.p2.7.m7.1.1.2.cmml" xref="S5.SS2.p2.7.m7.1.1.2">𝑄</ci><ci id="S5.SS2.p2.7.m7.1.1.3.cmml" xref="S5.SS2.p2.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">Q^{\prime}</annotation></semantics></math> to the VQA model <math id="S5.SS2.p2.8.m8.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS2.p2.8.m8.1a"><mi id="S5.SS2.p2.8.m8.1.1" xref="S5.SS2.p2.8.m8.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><ci id="S5.SS2.p2.8.m8.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">F</annotation></semantics></math> and an associated loss <math id="S5.SS2.p2.9.m9.2" class="ltx_Math" alttext="\mathcal{L}_{cycle}(A,A^{\prime})" display="inline"><semantics id="S5.SS2.p2.9.m9.2a"><mrow id="S5.SS2.p2.9.m9.2.2" xref="S5.SS2.p2.9.m9.2.2.cmml"><msub id="S5.SS2.p2.9.m9.2.2.3" xref="S5.SS2.p2.9.m9.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS2.p2.9.m9.2.2.3.2" xref="S5.SS2.p2.9.m9.2.2.3.2.cmml">ℒ</mi><mrow id="S5.SS2.p2.9.m9.2.2.3.3" xref="S5.SS2.p2.9.m9.2.2.3.3.cmml"><mi id="S5.SS2.p2.9.m9.2.2.3.3.2" xref="S5.SS2.p2.9.m9.2.2.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.9.m9.2.2.3.3.1" xref="S5.SS2.p2.9.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.9.m9.2.2.3.3.3" xref="S5.SS2.p2.9.m9.2.2.3.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.9.m9.2.2.3.3.1a" xref="S5.SS2.p2.9.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.9.m9.2.2.3.3.4" xref="S5.SS2.p2.9.m9.2.2.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.9.m9.2.2.3.3.1b" xref="S5.SS2.p2.9.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.9.m9.2.2.3.3.5" xref="S5.SS2.p2.9.m9.2.2.3.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.9.m9.2.2.3.3.1c" xref="S5.SS2.p2.9.m9.2.2.3.3.1.cmml">​</mo><mi id="S5.SS2.p2.9.m9.2.2.3.3.6" xref="S5.SS2.p2.9.m9.2.2.3.3.6.cmml">e</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.9.m9.2.2.2" xref="S5.SS2.p2.9.m9.2.2.2.cmml">​</mo><mrow id="S5.SS2.p2.9.m9.2.2.1.1" xref="S5.SS2.p2.9.m9.2.2.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.9.m9.2.2.1.1.2" xref="S5.SS2.p2.9.m9.2.2.1.2.cmml">(</mo><mi id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml">A</mi><mo id="S5.SS2.p2.9.m9.2.2.1.1.3" xref="S5.SS2.p2.9.m9.2.2.1.2.cmml">,</mo><msup id="S5.SS2.p2.9.m9.2.2.1.1.1" xref="S5.SS2.p2.9.m9.2.2.1.1.1.cmml"><mi id="S5.SS2.p2.9.m9.2.2.1.1.1.2" xref="S5.SS2.p2.9.m9.2.2.1.1.1.2.cmml">A</mi><mo id="S5.SS2.p2.9.m9.2.2.1.1.1.3" xref="S5.SS2.p2.9.m9.2.2.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S5.SS2.p2.9.m9.2.2.1.1.4" xref="S5.SS2.p2.9.m9.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.2b"><apply id="S5.SS2.p2.9.m9.2.2.cmml" xref="S5.SS2.p2.9.m9.2.2"><times id="S5.SS2.p2.9.m9.2.2.2.cmml" xref="S5.SS2.p2.9.m9.2.2.2"></times><apply id="S5.SS2.p2.9.m9.2.2.3.cmml" xref="S5.SS2.p2.9.m9.2.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.9.m9.2.2.3.1.cmml" xref="S5.SS2.p2.9.m9.2.2.3">subscript</csymbol><ci id="S5.SS2.p2.9.m9.2.2.3.2.cmml" xref="S5.SS2.p2.9.m9.2.2.3.2">ℒ</ci><apply id="S5.SS2.p2.9.m9.2.2.3.3.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3"><times id="S5.SS2.p2.9.m9.2.2.3.3.1.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3.1"></times><ci id="S5.SS2.p2.9.m9.2.2.3.3.2.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3.2">𝑐</ci><ci id="S5.SS2.p2.9.m9.2.2.3.3.3.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3.3">𝑦</ci><ci id="S5.SS2.p2.9.m9.2.2.3.3.4.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3.4">𝑐</ci><ci id="S5.SS2.p2.9.m9.2.2.3.3.5.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3.5">𝑙</ci><ci id="S5.SS2.p2.9.m9.2.2.3.3.6.cmml" xref="S5.SS2.p2.9.m9.2.2.3.3.6">𝑒</ci></apply></apply><interval closure="open" id="S5.SS2.p2.9.m9.2.2.1.2.cmml" xref="S5.SS2.p2.9.m9.2.2.1.1"><ci id="S5.SS2.p2.9.m9.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1">𝐴</ci><apply id="S5.SS2.p2.9.m9.2.2.1.1.1.cmml" xref="S5.SS2.p2.9.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.9.m9.2.2.1.1.1.1.cmml" xref="S5.SS2.p2.9.m9.2.2.1.1.1">superscript</csymbol><ci id="S5.SS2.p2.9.m9.2.2.1.1.1.2.cmml" xref="S5.SS2.p2.9.m9.2.2.1.1.1.2">𝐴</ci><ci id="S5.SS2.p2.9.m9.2.2.1.1.1.3.cmml" xref="S5.SS2.p2.9.m9.2.2.1.1.1.3">′</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.2c">\mathcal{L}_{cycle}(A,A^{\prime})</annotation></semantics></math>.
As seen in Table <a href="#S5.T2" title="Table 2 ‣ 5.1 Consistency Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we see that naively passing all the generated questions to the VQA model <math id="S5.SS2.p2.10.m10.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS2.p2.10.m10.1a"><mi id="S5.SS2.p2.10.m10.1.1" xref="S5.SS2.p2.10.m10.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.10.m10.1b"><ci id="S5.SS2.p2.10.m10.1.1.cmml" xref="S5.SS2.p2.10.m10.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.10.m10.1c">F</annotation></semantics></math> leads to significant reduction in performance than the base model <math id="S5.SS2.p2.11.m11.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS2.p2.11.m11.1a"><mi id="S5.SS2.p2.11.m11.1.1" xref="S5.SS2.p2.11.m11.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.11.m11.1b"><ci id="S5.SS2.p2.11.m11.1.1.cmml" xref="S5.SS2.p2.11.m11.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.11.m11.1c">F</annotation></semantics></math>. This goes in line with our earlier discussion that not all questions generated are <em id="S5.SS2.p2.13.1" class="ltx_emph ltx_font_italic">valid</em> rephrasings of the original question and hence enforcing consistency between the answers of two invalid pairs of questions naturally leads to degradation in performance.
Finally we show the effect of using our gating mechanism to filter undesirable generated questions in <math id="S5.SS2.p2.12.m12.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="S5.SS2.p2.12.m12.1a"><msup id="S5.SS2.p2.12.m12.1.1" xref="S5.SS2.p2.12.m12.1.1.cmml"><mi id="S5.SS2.p2.12.m12.1.1.2" xref="S5.SS2.p2.12.m12.1.1.2.cmml">Q</mi><mo id="S5.SS2.p2.12.m12.1.1.3" xref="S5.SS2.p2.12.m12.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.12.m12.1b"><apply id="S5.SS2.p2.12.m12.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.12.m12.1.1.1.cmml" xref="S5.SS2.p2.12.m12.1.1">superscript</csymbol><ci id="S5.SS2.p2.12.m12.1.1.2.cmml" xref="S5.SS2.p2.12.m12.1.1.2">𝑄</ci><ci id="S5.SS2.p2.12.m12.1.1.3.cmml" xref="S5.SS2.p2.12.m12.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.12.m12.1c">Q^{\prime}</annotation></semantics></math> and passing the remaining to VQA model <math id="S5.SS2.p2.13.m13.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS2.p2.13.m13.1a"><mi id="S5.SS2.p2.13.m13.1.1" xref="S5.SS2.p2.13.m13.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.13.m13.1b"><ci id="S5.SS2.p2.13.m13.1.1.cmml" xref="S5.SS2.p2.13.m13.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.13.m13.1c">F</annotation></semantics></math>. We see that all VQA models perform <span id="S5.SS2.p2.13.2" class="ltx_text" style="color:#000000;">consistently</span> better when using a gating than just using Q-consistency.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text" style="color:#000000;">We also experimented with Pythia model configurations where the VQG model uses unattended image features (unlike the default setting which uses image features with attention from the VQA model).
We found that with this configuration, our approach still shows improved performance over the baseline. However, the question generation quality is relatively poor, and the overall gain is smaller (3.58% in consistency <math id="S5.SS2.p3.1.1.m1.1" class="ltx_Math" alttext="CS(k=4)" display="inline"><semantics id="S5.SS2.p3.1.1.m1.1a"><mrow id="S5.SS2.p3.1.1.m1.1.1" xref="S5.SS2.p3.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S5.SS2.p3.1.1.m1.1.1.3" xref="S5.SS2.p3.1.1.m1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p3.1.1.m1.1.1.2" xref="S5.SS2.p3.1.1.m1.1.1.2.cmml">​</mo><mi mathcolor="#000000" id="S5.SS2.p3.1.1.m1.1.1.4" xref="S5.SS2.p3.1.1.m1.1.1.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p3.1.1.m1.1.1.2a" xref="S5.SS2.p3.1.1.m1.1.1.2.cmml">​</mo><mrow id="S5.SS2.p3.1.1.m1.1.1.1.1" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S5.SS2.p3.1.1.m1.1.1.1.1.2" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.p3.1.1.m1.1.1.1.1.1" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S5.SS2.p3.1.1.m1.1.1.1.1.1.2" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.2.cmml">k</mi><mo mathcolor="#000000" id="S5.SS2.p3.1.1.m1.1.1.1.1.1.1" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="S5.SS2.p3.1.1.m1.1.1.1.1.1.3" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.3.cmml">4</mn></mrow><mo mathcolor="#000000" stretchy="false" id="S5.SS2.p3.1.1.m1.1.1.1.1.3" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.1.m1.1b"><apply id="S5.SS2.p3.1.1.m1.1.1.cmml" xref="S5.SS2.p3.1.1.m1.1.1"><times id="S5.SS2.p3.1.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.1.m1.1.1.2"></times><ci id="S5.SS2.p3.1.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.1.m1.1.1.3">𝐶</ci><ci id="S5.SS2.p3.1.1.m1.1.1.4.cmml" xref="S5.SS2.p3.1.1.m1.1.1.4">𝑆</ci><apply id="S5.SS2.p3.1.1.m1.1.1.1.1.1.cmml" xref="S5.SS2.p3.1.1.m1.1.1.1.1"><eq id="S5.SS2.p3.1.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.1"></eq><ci id="S5.SS2.p3.1.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.2">𝑘</ci><cn type="integer" id="S5.SS2.p3.1.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.p3.1.1.m1.1.1.1.1.1.3">4</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.1.m1.1c">CS(k=4)</annotation></semantics></math> and 0.2% in VQA accuracy) compared to when using attention (8.08% and 0.5% respectively) – likely because attention helps in generating more-focused rephrasings</span></p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Visual Question Generation Performance</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Recall that our model also includes a VQG component which generates questions conditioned on an answer and image. Since the overall performance of our framework relies highly on the performance of question generation module, we evaluate our VQG component performance as well on commonly used image captioning metrics. We compare our VQG component to several answer-conditional VQG models on the VQA v2.0 dataset. We use standard image captioning metrics CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> as used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. We compare our approach to two recently proposed visual question generation approaches. <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_bold">iVQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> uses a variational LSTM model trained with reinforcement learning to generate answer-specific questions for an image. Syntactic correctness, diversity and intent of the generated question are used to allocate rewards.
<span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_bold">iQAN</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> generates answer-specific questions by modelling question generation as a dual task of question answering and sharing parameters between question answering and question generation modules. Since iQAN can only generate a specific type of questions, for a fair comparison, we compare to iQAN only on a subset of the dataset containing questions from these specific types. As shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.1 Consistency Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that our question generation module trained with cycle-consistency consistently outperforms iVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and iQAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> on all metrics. <span id="S5.SS3.p1.1.3" class="ltx_text" style="color:#000000;">A few qualitative examples of answer conditioned questions generated by our VQG model can be seen in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3.1 Question Generation Module ‣ 3 Approach ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>(b).</span> Additional examples can also be found in the Appendix <a href="#A4" title="Appendix D Question Generation ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.7.1.1" class="ltx_tr">
<th id="S5.T4.7.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S5.T4.7.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S5.T4.7.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T4.7.1.1.2.1" class="ltx_text ltx_font_bold">Precision</span></th>
<th id="S5.T4.7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><span id="S5.T4.7.1.1.3.1" class="ltx_text ltx_font_bold">Recall</span></th>
<th id="S5.T4.7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.7.1.1.4.1" class="ltx_text ltx_font_bold">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.7.2.1" class="ltx_tr">
<th id="S5.T4.7.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</th>
<td id="S5.T4.7.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.71</td>
<td id="S5.T4.7.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.78</td>
<td id="S5.T4.7.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.74</td>
</tr>
<tr id="S5.T4.7.3.2" class="ltx_tr">
<th id="S5.T4.7.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">   + FP</th>
<td id="S5.T4.7.3.2.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.3.2.2.1" class="ltx_text ltx_font_bold">0.74</span></td>
<td id="S5.T4.7.3.2.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.3.2.3.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S5.T4.7.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T4.7.3.2.4.1" class="ltx_text ltx_font_bold">0.79</span></td>
</tr>
<tr id="S5.T4.7.4.3" class="ltx_tr">
<th id="S5.T4.7.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">BUTD + CC</th>
<td id="S5.T4.7.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.73</td>
<td id="S5.T4.7.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.79</td>
<td id="S5.T4.7.4.3.4" class="ltx_td ltx_align_center ltx_border_t">0.76</td>
</tr>
<tr id="S5.T4.7.5.4" class="ltx_tr">
<th id="S5.T4.7.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">    + FP</th>
<td id="S5.T4.7.5.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.5.4.2.1" class="ltx_text ltx_font_bold">0.78</span></td>
<td id="S5.T4.7.5.4.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.5.4.3.1" class="ltx_text ltx_font_bold">0.83</span></td>
<td id="S5.T4.7.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T4.7.5.4.4.1" class="ltx_text ltx_font_bold">0.80</span></td>
</tr>
<tr id="S5.T4.7.6.5" class="ltx_tr">
<th id="S5.T4.7.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</th>
<td id="S5.T4.7.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.74</td>
<td id="S5.T4.7.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.79</td>
<td id="S5.T4.7.6.5.4" class="ltx_td ltx_align_center ltx_border_t">0.76</td>
</tr>
<tr id="S5.T4.7.7.6" class="ltx_tr">
<th id="S5.T4.7.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">   + FP</th>
<td id="S5.T4.7.7.6.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.7.6.2.1" class="ltx_text ltx_font_bold">0.76</span></td>
<td id="S5.T4.7.7.6.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.7.7.6.3.1" class="ltx_text ltx_font_bold">0.88</span></td>
<td id="S5.T4.7.7.6.4" class="ltx_td ltx_align_center"><span id="S5.T4.7.7.6.4.1" class="ltx_text ltx_font_bold">0.82</span></td>
</tr>
<tr id="S5.T4.7.8.7" class="ltx_tr">
<th id="S5.T4.7.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Pythia + CC</th>
<td id="S5.T4.7.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.77</td>
<td id="S5.T4.7.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.81</td>
<td id="S5.T4.7.8.7.4" class="ltx_td ltx_align_center ltx_border_t">0.77</td>
</tr>
<tr id="S5.T4.7.9.8" class="ltx_tr">
<th id="S5.T4.7.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r">    + FP</th>
<td id="S5.T4.7.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T4.7.9.8.2.1" class="ltx_text ltx_font_bold">0.82</span></td>
<td id="S5.T4.7.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T4.7.9.8.3.1" class="ltx_text ltx_font_bold">0.84</span></td>
<td id="S5.T4.7.9.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.7.9.8.4.1" class="ltx_text ltx_font_bold">0.83</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
<span id="S5.T4.9.1" class="ltx_text ltx_font_bold">Failure prediction performance on VQA v2.0 validation dataset</span>.
Each row in blocks represents a component added to the previous row. CC represents models trained with our cycle-consistent framework and FP represents models with an additional binary classification Failure Prediction submodule to predict if the predicted answer <math id="S5.T4.4.m1.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S5.T4.4.m1.1b"><msup id="S5.T4.4.m1.1.1" xref="S5.T4.4.m1.1.1.cmml"><mi id="S5.T4.4.m1.1.1.2" xref="S5.T4.4.m1.1.1.2.cmml">A</mi><mo id="S5.T4.4.m1.1.1.3" xref="S5.T4.4.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.T4.4.m1.1c"><apply id="S5.T4.4.m1.1.1.cmml" xref="S5.T4.4.m1.1.1"><csymbol cd="ambiguous" id="S5.T4.4.m1.1.1.1.cmml" xref="S5.T4.4.m1.1.1">superscript</csymbol><ci id="S5.T4.4.m1.1.1.2.cmml" xref="S5.T4.4.m1.1.1.2">𝐴</ci><ci id="S5.T4.4.m1.1.1.3.cmml" xref="S5.T4.4.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.4.m1.1d">A^{\prime}</annotation></semantics></math> is correct given a question and image pair (<math id="S5.T4.5.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.T4.5.m2.1b"><mi id="S5.T4.5.m2.1.1" xref="S5.T4.5.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.T4.5.m2.1c"><ci id="S5.T4.5.m2.1.1.cmml" xref="S5.T4.5.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.5.m2.1d">Q</annotation></semantics></math>, <math id="S5.T4.6.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.T4.6.m3.1b"><mi id="S5.T4.6.m3.1.1" xref="S5.T4.6.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.T4.6.m3.1c"><ci id="S5.T4.6.m3.1.1.cmml" xref="S5.T4.6.m3.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.6.m3.1d">I</annotation></semantics></math>). For models trained without the FP module, scores are obtained by thresholding the answer confidences.
</figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Failure Prediction Performance</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.5" class="ltx_p">In previous results, we show that by training models to generate and answer questions while being consistent across both tasks leads to improvement in performance and robustness. Another way of testing robustness of these models is to see if models can predict their own failures.
A robust model is less confident about an incorrect answer and vice versa.
Motivated by this, we seek to verify if models trained with our cycle-consistent framework can identify their own failures <em id="S5.SS4.p1.5.6" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS4.p1.5.7" class="ltx_text"></span> correctly identify if they’re wrong about a prediction.
<span id="S5.SS4.p1.5.5" class="ltx_text" style="color:#000000;">To this end, we use two failure predictions schemes. First, we naively threshold the confidence of the predicted answer. All answers above a particular threshold are marked as correctly answered and vice versa. Second, we design a failure prediction binary classification module (FP), which predicts for a given image <math id="S5.SS4.p1.1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.SS4.p1.1.1.m1.1a"><mi mathcolor="#000000" id="S5.SS4.p1.1.1.m1.1.1" xref="S5.SS4.p1.1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.1.m1.1b"><ci id="S5.SS4.p1.1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.1.m1.1c">I</annotation></semantics></math>, question <math id="S5.SS4.p1.2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.SS4.p1.2.2.m2.1a"><mi mathcolor="#000000" id="S5.SS4.p1.2.2.m2.1.1" xref="S5.SS4.p1.2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.2.m2.1b"><ci id="S5.SS4.p1.2.2.m2.1.1.cmml" xref="S5.SS4.p1.2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.2.m2.1c">Q</annotation></semantics></math> and answer <math id="S5.SS4.p1.3.3.m3.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="S5.SS4.p1.3.3.m3.1a"><msup id="S5.SS4.p1.3.3.m3.1.1" xref="S5.SS4.p1.3.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.3.3.m3.1.1.2" xref="S5.SS4.p1.3.3.m3.1.1.2.cmml">A</mi><mo mathcolor="#000000" id="S5.SS4.p1.3.3.m3.1.1.3" xref="S5.SS4.p1.3.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.3.m3.1b"><apply id="S5.SS4.p1.3.3.m3.1.1.cmml" xref="S5.SS4.p1.3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.3.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.3.m3.1.1">superscript</csymbol><ci id="S5.SS4.p1.3.3.m3.1.1.2.cmml" xref="S5.SS4.p1.3.3.m3.1.1.2">𝐴</ci><ci id="S5.SS4.p1.3.3.m3.1.1.3.cmml" xref="S5.SS4.p1.3.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.3.m3.1c">A^{\prime}</annotation></semantics></math> (predicted by the base VQA model <math id="S5.SS4.p1.4.4.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S5.SS4.p1.4.4.m4.1a"><mi mathcolor="#000000" id="S5.SS4.p1.4.4.m4.1.1" xref="S5.SS4.p1.4.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.4.m4.1b"><ci id="S5.SS4.p1.4.4.m4.1.1.cmml" xref="S5.SS4.p1.4.4.m4.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.4.m4.1c">F</annotation></semantics></math>), whether the predicted answer is correct for the given <math id="S5.SS4.p1.5.5.m5.2" class="ltx_Math" alttext="(I,Q)" display="inline"><semantics id="S5.SS4.p1.5.5.m5.2a"><mrow id="S5.SS4.p1.5.5.m5.2.3.2" xref="S5.SS4.p1.5.5.m5.2.3.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S5.SS4.p1.5.5.m5.2.3.2.1" xref="S5.SS4.p1.5.5.m5.2.3.1.cmml">(</mo><mi mathcolor="#000000" id="S5.SS4.p1.5.5.m5.1.1" xref="S5.SS4.p1.5.5.m5.1.1.cmml">I</mi><mo mathcolor="#000000" id="S5.SS4.p1.5.5.m5.2.3.2.2" xref="S5.SS4.p1.5.5.m5.2.3.1.cmml">,</mo><mi mathcolor="#000000" id="S5.SS4.p1.5.5.m5.2.2" xref="S5.SS4.p1.5.5.m5.2.2.cmml">Q</mi><mo mathcolor="#000000" stretchy="false" id="S5.SS4.p1.5.5.m5.2.3.2.3" xref="S5.SS4.p1.5.5.m5.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.5.m5.2b"><interval closure="open" id="S5.SS4.p1.5.5.m5.2.3.1.cmml" xref="S5.SS4.p1.5.5.m5.2.3.2"><ci id="S5.SS4.p1.5.5.m5.1.1.cmml" xref="S5.SS4.p1.5.5.m5.1.1">𝐼</ci><ci id="S5.SS4.p1.5.5.m5.2.2.cmml" xref="S5.SS4.p1.5.5.m5.2.2">𝑄</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.5.m5.2c">(I,Q)</annotation></semantics></math> pair. The FP module uses image and answer encoders similar to those used in the question generation module (Section <a href="#S3.SS1" title="3.1 Question Generation Module ‣ 3 Approach ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and makes use of the question representation from the base VQA model as the question encoding. These encodings are concatenated and passed to a linear layer for binary classification. The FP module is trained keeping the parameters of the base VQA model frozen.</span>
In Table <a href="#S5.T4" title="Table 4 ‣ 5.3 Visual Question Generation Performance ‣ 5 Experiments ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show the failure prediction performance of the baseline VQA models and models trained with our proposed framework.
<span id="S5.SS4.p1.5.8" class="ltx_text" style="color:#000000;">It shows that the cycle consistency framework, even <em id="S5.SS4.p1.5.8.1" class="ltx_emph ltx_font_italic">without</em> an explicit failure predictor module, makes the models more calibrated – more capable of detecting their own failures.
In both settings: (a) when using naive confidence thresholding (not marked as “+ FP” in the Table) and (b) using a specifically designed submodule to detect failures (marked as “+ FP”), models trained with our cycle-consistent training framework are better than their corresponding baselines.
We see similar improvments in detecting failures for both BUTD and Pythia models, which shows that our cycle-consistency framework is model agnostic.</span>
This also shows that not only does cycle-consistent training make models robust to linguistic variations, but also allows them to be aware of their failures.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we propose a novel model-agnostic training strategy to incorporate cycle consistency in VQA models to make them robust to linguistic variations and self-aware of their failures.
We also collect a large-scale dataset, VQA-Rephrasings and propose a consensus metric to measure robustness of VQA models to linguistic variations of a question.
We show that models trained with our training strategy are robust to linguistic variations, and achieve state-of-the-art performance in VQA and VQG on VQA v2.0 dataset.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Analyzing the behavior of visual question answering models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.07356</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Don’t just assume; look and answer: Overcoming priors for visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2018 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
H. Ali, Y. Chali, and S. A. Hasan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Automation of question generation from sentences.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of QG2010: The Third Workshop on Question
Generation</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, pages 58–67, 2010.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
J. Andreas, M. Rohrbach, T. Darrell, and D. Klein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Learning to compose neural networks for question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1601.01705</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
H. Ben-Younes, R. Cadene, M. Cord, and N. Thome.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Mutan: Multimodal tucker fusion for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
M. Denkowski and A. Lavie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Meteor universal: Language specific translation evaluation for any
target language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the EACL 2014 Workshop on Statistical Machine
Translation</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
A. Ettinger, S. Rao, H. Daumé III, and E. M. Bender.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Towards linguistically generalizable nlp systems: A workshop and
shared task.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.01505</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
R. Girshick, I. Radosavovic, G. Gkioxari, P. Dollár, and K. He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Detectron.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/facebookresearch/detectron" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/facebookresearch/detectron</a><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 6325–6334. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Dual learning for machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages
820–828, 2016.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
J. R. Hobbs, D. E. Appelt, J. Bear, and M. Tyson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Robust processing of real-world natural-language texts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the third conference on Applied natural
language processing</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 186–192. Association for Computational
Linguistics, 1992.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
S. Hochreiter and J. Schmidhuber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Long short-term memory.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural computation</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 9(8):1735–1780, 1997.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
R. Hu, J. Andreas, M. Rohrbach, T. Darrell, and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Learning to reason: End-to-end module networks for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE International Conference on Computer Vision
(ICCV)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 804–813. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
M. Iyyer, J. Wieting, K. Gimpel, and L. Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Adversarial example generation with syntactically controlled
paraphrase networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.06059</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
U. Jain, Z. Zhang, and A. Schwing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Creativity: Generating diverse questions using variational
autoencoders.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages 5415–5424. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, J. Hoffman, L. Fei-Fei,
C. Lawrence Zitnick, and R. Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Inferring and executing programs for visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, pages 2989–2998, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
K. Kafle, M. Yousefhussien, and C. Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Data augmentation for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 10th International Conference on Natural
Language Generation</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 198–202, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
S. Kalady, A. Elikkottil, and R. Das.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Natural language question generation using syntax and keywords.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of QG2010: The Third Workshop on Question
Generation</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
J.-H. Kim, J. Jun, and B.-T. Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Bilinear Attention Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1805.07932</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and
S. Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Skip-thought vectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages
3294–3302, 2015.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 123(1):32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Gradient-based learning applied to document recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 86(11):2278–2324, 1998.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Y. Li, T. Cohn, and T. Baldwin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Robust training under linguistic adversity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 15th Conference of the European Chapter of
the Association for Computational Linguistics: Volume 2, Short Papers</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">,
volume 2, pages 21–27, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Y. Li, N. Duan, B. Zhou, X. Chu, W. Ouyang, X. Wang, and M. Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Visual question generation as dual task of visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
C.-Y. Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Rouge: A package for automatic evaluation of summaries.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Text Summarization Branches Out</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2004.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
F. Liu, T. Xiang, T. M. Hospedales, W. Yang, and C. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">ivqa: Inverse visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 8611–8619, 2018.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
J. Lu, J. Yang, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Hierarchical question-image co-attention for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances In Neural Information Processing Systems</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages
289–297, 2016.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
B. McCann, N. S. Keskar, C. Xiong, and R. Socher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">The natural language decathlon: Multitask learning as question
answering, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
I. Misra, R. Girshick, R. Fergus, M. Hebert, A. Gupta, and L. van der Maaten.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Learning by asking questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages 11–20, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
I. M. Mora and S. P. de la Puente.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Towards automatic generation of question answer pairs from images.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
N. Mostafazadeh, I. Misra, J. Devlin, M. Mitchell, X. He, and L. Vanderwende.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Generating natural questions about an image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1603.06059</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Bleu: a method for automatic evaluation of machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 40th annual meeting on association for
computational linguistics</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 311–318. Association for Computational
Linguistics, 2002.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Film: Visual reasoning with a general conditioning layer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Thirty-Second AAAI Conference on Artificial Intelligence</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
S. Ren, K. He, R. Girshick, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages
91–99, 2015.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
I. V. Serban, A. García-Durán, C. Gulcehre, S. Ahn, S. Chandar,
A. Courville, and Y. Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Generating factoid questions with recurrent neural networks: The 30m
factoid question-answer corpus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1603.06807</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
M. Spranger, J. Suchan, and M. Bhatt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Robust natural language processing-combining reasoning, cognitive
semantics and construction grammar for spatial language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1607.05968</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
M. Stede.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">The search for robustness in natural language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artificial Intelligence Review</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 6(4):383–414, 1992.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
N. Sundaram, T. Brox, and K. Keutzer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Dense point trajectories by gpu-accelerated large displacement
optical flow.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 438–451.
Springer, 2010.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
D. Tang, N. Duan, Z. Yan, Z. Zhang, Y. Sun, S. Liu, Y. Lv, and M. Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Learning to collaborate for question answering and asking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers)</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, volume 1, pages 1564–1574, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
R. Vedantam, C. Lawrence Zitnick, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Cider: Consensus-based image description evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 4566–4575, 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Z. Wang, A. S. Lan, W. Nie, A. E. Waters, P. J. Grimaldi, and R. G. Baraniuk.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Qg-net: A data-driven question generation model for educational
content.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Fifth Annual ACM Conference on Learning at
Scale</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 7:1–7:10, 2018.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
X. Xu, X. Chen, C. Liu, A. Rohrbach, T. Darrell, and D. Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Fooling vision and language models despite localization and attention
mechanism.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 4951–4961, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Z. Yang, X. He, J. Gao, L. Deng, and A. Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 21–29, 2016.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Yu Jiang*, Vivek Natarajan*, Xinlei Chen*, M. Rohrbach, D. Batra, and
D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Pythia v0.1: the winning entry to the vqa challenge 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.09956</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and D. Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Yin and yang: Balancing and answering binary visual questions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, pages 5014–5022, 2016.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Unpaired image-to-image translation using cycle-consistent
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision (ICCV), 2017 IEEE International Conference
on</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">The appendix is organized as follows:</p>
<ul id="p1.2" class="ltx_itemize">
<li id="A0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i1.p1" class="ltx_para">
<p id="A0.I1.i1.p1.1" class="ltx_p">Section <a href="#A1" title="Appendix A Dataset Details ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> covers information about the dataset collection pipeline, user interface and provides some dataset statistics.</p>
</div>
</li>
<li id="A0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i2.p1" class="ltx_para">
<p id="A0.I1.i2.p1.1" class="ltx_p">Section <a href="#A2" title="Appendix B Attention Analysis ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> shows qualitative examples of how attention over image regions varies for VQA models when different rephrasings of the same question are used as input.</p>
</div>
</li>
<li id="A0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i3.p1" class="ltx_para">
<p id="A0.I1.i3.p1.1" class="ltx_p">Section <a href="#A3" title="Appendix C Attention Consistency ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> describes an attention based consistency strategy that we experimented with, but did not improve performance (and so was not a part of our final model presented in the paper).</p>
</div>
</li>
<li id="A0.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i4.p1" class="ltx_para">
<p id="A0.I1.i4.p1.1" class="ltx_p">Section <a href="#A4" title="Appendix D Question Generation ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">D</span></a> shows qualitative examples of answer conditioned questions generated by our VQG module.</p>
</div>
</li>
<li id="A0.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i5.p1" class="ltx_para">
<p id="A0.I1.i5.p1.1" class="ltx_p">Section <a href="#A5" title="Appendix E Hyperparameters ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> lists the hyperparameters used for each base VQA model.</p>
</div>
</li>
</ul>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Details</h2>

<figure id="A1.F5" class="ltx_figure"><img src="/html/1902.05660/assets/x5.png" id="A1.F5.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="655" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="A1.F5.3.1" class="ltx_text ltx_font_bold">Dataset Statistics.</span>
(a) Shows the number of words (in percentage) belonging to different Parts-of-Speech tags. The distributions follow similar trends in VQA-Rephrasings and VQA v2.0.
(b) Shows the number of questions (in percentage) with varying lengths. The average length of questions in VQA-Rephrasings is 7.15 which is slightly higher than the average length in VQA v2.0, which is 6.32.
</figcaption>
</figure>
<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text ltx_font_bold">Statistics.</span>
Fig <a href="#A1.F5" title="Figure 5 ‣ Appendix A Dataset Details ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(a) shows the number of words (in percentage) belonging to different Parts-of-Speech tags. The distributions follow almost similar trends in VQA-Rephrasings and VQA v2.0. This shows that the rephrasings are not obtained by merely adding more adjectives or adverbs in the original question.
Fig <a href="#A1.F5" title="Figure 5 ‣ Appendix A Dataset Details ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>(b) shows the number of questions (in percentage) with varying lengths. The average length of questions in VQA-Rephrasings is 7.15 which is slightly higher than the average length in VQA v2.0, which is 6.32.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_font_bold">Interface.</span>
<span id="A1.p2.1.2" class="ltx_text" style="color:#000000;">We used a simplistic web interface to</span> collect rephrasings from human annotators. The interface provided three examples of invalid rephrasings and their corresponding explanations to help human annotators understand the task better. We A/B tested with 50 questions using all 4 combinations of:</p>
<ul id="A1.p2.2" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">Showing both valid and invalid rephrasing examples and explanations.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">Showing only valid and no invalid rephrasing examples and explanations.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Showing none of valid and invalid rephrasing examples and explanations.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">Showing no valid and only invalid rephrasing examples and explanations.</p>
</div>
</li>
</ul>
<p id="A1.p2.3" class="ltx_p">We found (via manual inspection) that the last setup provided higher quality data, and used that as our final interface <span id="A1.p2.3.1" class="ltx_text" style="color:#000000;">choice</span>.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p"><span id="A1.p3.1.1" class="ltx_text ltx_font_bold">Examples.</span>
Fig <a href="#A5.F6" title="Figure 6 ‣ Appendix E Hyperparameters ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows several qualitative examples from the VQA-Rephrasings dataset. We see that the rephrasings maintain the intent of the original question while varying linguistically.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Attention Analysis</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Fig <a href="#A5.F7" title="Figure 7 ‣ Appendix E Hyperparameters ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> qualitatively compares the textual and visual attention (over image regions) for rephrasings of a question.
Each row compares predicted answers and attention from a baseline Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> model and the same Pythia model trained with our framework (Pythia + CC), using two question rephrasings.
First and third row shows the outputs of a Pythia model (baseline) and second and forth row shows the output of a Pythia model (baseline + CC) trained with our framework.
We see that in most examples, the attention over image regions doesn’t vary across rephrasings for models trained with our framework (and the model answers the questions correctly). However for the baseline model, one can see that minor linguistic changes in the question can result in completely different answers (Row 2, Columns 1 and 3). This qualitatively demonstrates the robustness of models trained with our framework.
Since the baseline Pythia model doesn’t include a counting module, it doesn’t perform well on questions requiring counting. As a result we see that both the baseline and its cycle-consistent counterpart perform poorly on counting questions (Row 5, Columns 1 through 4).</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Attention Consistency</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">Intuitively, it seems like training the VQA model to attend over the same image regions for different rephrasings of a question should improve the robustness of the model. We tried to enforce this in our cycle-consistent framework using an additional attention consistency loss.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.17" class="ltx_p">Recall that for a given image <math id="A3.p2.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="A3.p2.1.m1.1a"><mi id="A3.p2.1.m1.1.1" xref="A3.p2.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="A3.p2.1.m1.1b"><ci id="A3.p2.1.m1.1.1.cmml" xref="A3.p2.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.1.m1.1c">I</annotation></semantics></math>, question <math id="A3.p2.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A3.p2.2.m2.1a"><mi id="A3.p2.2.m2.1.1" xref="A3.p2.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A3.p2.2.m2.1b"><ci id="A3.p2.2.m2.1.1.cmml" xref="A3.p2.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.2.m2.1c">Q</annotation></semantics></math> and answer <math id="A3.p2.3.m3.1" class="ltx_Math" alttext="A" display="inline"><semantics id="A3.p2.3.m3.1a"><mi id="A3.p2.3.m3.1.1" xref="A3.p2.3.m3.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="A3.p2.3.m3.1b"><ci id="A3.p2.3.m3.1.1.cmml" xref="A3.p2.3.m3.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.3.m3.1c">A</annotation></semantics></math>, our model consists of a VQA model <math id="A3.p2.4.m4.1" class="ltx_Math" alttext="F" display="inline"><semantics id="A3.p2.4.m4.1a"><mi id="A3.p2.4.m4.1.1" xref="A3.p2.4.m4.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="A3.p2.4.m4.1b"><ci id="A3.p2.4.m4.1.1.cmml" xref="A3.p2.4.m4.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.4.m4.1c">F</annotation></semantics></math> which takes (<math id="A3.p2.5.m5.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="A3.p2.5.m5.1a"><mi id="A3.p2.5.m5.1.1" xref="A3.p2.5.m5.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="A3.p2.5.m5.1b"><ci id="A3.p2.5.m5.1.1.cmml" xref="A3.p2.5.m5.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.5.m5.1c">Q</annotation></semantics></math>, <math id="A3.p2.6.m6.1" class="ltx_Math" alttext="I" display="inline"><semantics id="A3.p2.6.m6.1a"><mi id="A3.p2.6.m6.1.1" xref="A3.p2.6.m6.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="A3.p2.6.m6.1b"><ci id="A3.p2.6.m6.1.1.cmml" xref="A3.p2.6.m6.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.6.m6.1c">I</annotation></semantics></math>) as an input and uses the question to attend over image regions with attention <math id="A3.p2.7.m7.1" class="ltx_Math" alttext="\gamma_{Q}" display="inline"><semantics id="A3.p2.7.m7.1a"><msub id="A3.p2.7.m7.1.1" xref="A3.p2.7.m7.1.1.cmml"><mi id="A3.p2.7.m7.1.1.2" xref="A3.p2.7.m7.1.1.2.cmml">γ</mi><mi id="A3.p2.7.m7.1.1.3" xref="A3.p2.7.m7.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p2.7.m7.1b"><apply id="A3.p2.7.m7.1.1.cmml" xref="A3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="A3.p2.7.m7.1.1.1.cmml" xref="A3.p2.7.m7.1.1">subscript</csymbol><ci id="A3.p2.7.m7.1.1.2.cmml" xref="A3.p2.7.m7.1.1.2">𝛾</ci><ci id="A3.p2.7.m7.1.1.3.cmml" xref="A3.p2.7.m7.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.7.m7.1c">\gamma_{Q}</annotation></semantics></math> and predicts an answer <math id="A3.p2.8.m8.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="A3.p2.8.m8.1a"><msup id="A3.p2.8.m8.1.1" xref="A3.p2.8.m8.1.1.cmml"><mi id="A3.p2.8.m8.1.1.2" xref="A3.p2.8.m8.1.1.2.cmml">A</mi><mo id="A3.p2.8.m8.1.1.3" xref="A3.p2.8.m8.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="A3.p2.8.m8.1b"><apply id="A3.p2.8.m8.1.1.cmml" xref="A3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="A3.p2.8.m8.1.1.1.cmml" xref="A3.p2.8.m8.1.1">superscript</csymbol><ci id="A3.p2.8.m8.1.1.2.cmml" xref="A3.p2.8.m8.1.1.2">𝐴</ci><ci id="A3.p2.8.m8.1.1.3.cmml" xref="A3.p2.8.m8.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.8.m8.1c">A^{\prime}</annotation></semantics></math>. We also have a VQG model <math id="A3.p2.9.m9.1" class="ltx_Math" alttext="G" display="inline"><semantics id="A3.p2.9.m9.1a"><mi id="A3.p2.9.m9.1.1" xref="A3.p2.9.m9.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="A3.p2.9.m9.1b"><ci id="A3.p2.9.m9.1.1.cmml" xref="A3.p2.9.m9.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.9.m9.1c">G</annotation></semantics></math> which uses the predicted answer <math id="A3.p2.10.m10.1" class="ltx_Math" alttext="A^{\prime}" display="inline"><semantics id="A3.p2.10.m10.1a"><msup id="A3.p2.10.m10.1.1" xref="A3.p2.10.m10.1.1.cmml"><mi id="A3.p2.10.m10.1.1.2" xref="A3.p2.10.m10.1.1.2.cmml">A</mi><mo id="A3.p2.10.m10.1.1.3" xref="A3.p2.10.m10.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="A3.p2.10.m10.1b"><apply id="A3.p2.10.m10.1.1.cmml" xref="A3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="A3.p2.10.m10.1.1.1.cmml" xref="A3.p2.10.m10.1.1">superscript</csymbol><ci id="A3.p2.10.m10.1.1.2.cmml" xref="A3.p2.10.m10.1.1.2">𝐴</ci><ci id="A3.p2.10.m10.1.1.3.cmml" xref="A3.p2.10.m10.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.10.m10.1c">A^{\prime}</annotation></semantics></math> and image <math id="A3.p2.11.m11.1" class="ltx_Math" alttext="I" display="inline"><semantics id="A3.p2.11.m11.1a"><mi id="A3.p2.11.m11.1.1" xref="A3.p2.11.m11.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="A3.p2.11.m11.1b"><ci id="A3.p2.11.m11.1.1.cmml" xref="A3.p2.11.m11.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.11.m11.1c">I</annotation></semantics></math> to generate a question <math id="A3.p2.12.m12.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="A3.p2.12.m12.1a"><msup id="A3.p2.12.m12.1.1" xref="A3.p2.12.m12.1.1.cmml"><mi id="A3.p2.12.m12.1.1.2" xref="A3.p2.12.m12.1.1.2.cmml">Q</mi><mo id="A3.p2.12.m12.1.1.3" xref="A3.p2.12.m12.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="A3.p2.12.m12.1b"><apply id="A3.p2.12.m12.1.1.cmml" xref="A3.p2.12.m12.1.1"><csymbol cd="ambiguous" id="A3.p2.12.m12.1.1.1.cmml" xref="A3.p2.12.m12.1.1">superscript</csymbol><ci id="A3.p2.12.m12.1.1.2.cmml" xref="A3.p2.12.m12.1.1.2">𝑄</ci><ci id="A3.p2.12.m12.1.1.3.cmml" xref="A3.p2.12.m12.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.12.m12.1c">Q^{\prime}</annotation></semantics></math>. Intuitively, the VQA model should attend over the same image regions when answering <math id="A3.p2.13.m13.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="A3.p2.13.m13.1a"><msup id="A3.p2.13.m13.1.1" xref="A3.p2.13.m13.1.1.cmml"><mi id="A3.p2.13.m13.1.1.2" xref="A3.p2.13.m13.1.1.2.cmml">Q</mi><mo id="A3.p2.13.m13.1.1.3" xref="A3.p2.13.m13.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="A3.p2.13.m13.1b"><apply id="A3.p2.13.m13.1.1.cmml" xref="A3.p2.13.m13.1.1"><csymbol cd="ambiguous" id="A3.p2.13.m13.1.1.1.cmml" xref="A3.p2.13.m13.1.1">superscript</csymbol><ci id="A3.p2.13.m13.1.1.2.cmml" xref="A3.p2.13.m13.1.1.2">𝑄</ci><ci id="A3.p2.13.m13.1.1.3.cmml" xref="A3.p2.13.m13.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.13.m13.1c">Q^{\prime}</annotation></semantics></math>. In other words, the attention over image regions <math id="A3.p2.14.m14.1" class="ltx_Math" alttext="\gamma_{Q^{\prime}}" display="inline"><semantics id="A3.p2.14.m14.1a"><msub id="A3.p2.14.m14.1.1" xref="A3.p2.14.m14.1.1.cmml"><mi id="A3.p2.14.m14.1.1.2" xref="A3.p2.14.m14.1.1.2.cmml">γ</mi><msup id="A3.p2.14.m14.1.1.3" xref="A3.p2.14.m14.1.1.3.cmml"><mi id="A3.p2.14.m14.1.1.3.2" xref="A3.p2.14.m14.1.1.3.2.cmml">Q</mi><mo id="A3.p2.14.m14.1.1.3.3" xref="A3.p2.14.m14.1.1.3.3.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="A3.p2.14.m14.1b"><apply id="A3.p2.14.m14.1.1.cmml" xref="A3.p2.14.m14.1.1"><csymbol cd="ambiguous" id="A3.p2.14.m14.1.1.1.cmml" xref="A3.p2.14.m14.1.1">subscript</csymbol><ci id="A3.p2.14.m14.1.1.2.cmml" xref="A3.p2.14.m14.1.1.2">𝛾</ci><apply id="A3.p2.14.m14.1.1.3.cmml" xref="A3.p2.14.m14.1.1.3"><csymbol cd="ambiguous" id="A3.p2.14.m14.1.1.3.1.cmml" xref="A3.p2.14.m14.1.1.3">superscript</csymbol><ci id="A3.p2.14.m14.1.1.3.2.cmml" xref="A3.p2.14.m14.1.1.3.2">𝑄</ci><ci id="A3.p2.14.m14.1.1.3.3.cmml" xref="A3.p2.14.m14.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.14.m14.1c">\gamma_{Q^{\prime}}</annotation></semantics></math> used by the VQA model to answer <math id="A3.p2.15.m15.1" class="ltx_Math" alttext="Q^{\prime}" display="inline"><semantics id="A3.p2.15.m15.1a"><msup id="A3.p2.15.m15.1.1" xref="A3.p2.15.m15.1.1.cmml"><mi id="A3.p2.15.m15.1.1.2" xref="A3.p2.15.m15.1.1.2.cmml">Q</mi><mo id="A3.p2.15.m15.1.1.3" xref="A3.p2.15.m15.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="A3.p2.15.m15.1b"><apply id="A3.p2.15.m15.1.1.cmml" xref="A3.p2.15.m15.1.1"><csymbol cd="ambiguous" id="A3.p2.15.m15.1.1.1.cmml" xref="A3.p2.15.m15.1.1">superscript</csymbol><ci id="A3.p2.15.m15.1.1.2.cmml" xref="A3.p2.15.m15.1.1.2">𝑄</ci><ci id="A3.p2.15.m15.1.1.3.cmml" xref="A3.p2.15.m15.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.15.m15.1c">Q^{\prime}</annotation></semantics></math> should be close to the <math id="A3.p2.16.m16.1" class="ltx_Math" alttext="\gamma_{Q}" display="inline"><semantics id="A3.p2.16.m16.1a"><msub id="A3.p2.16.m16.1.1" xref="A3.p2.16.m16.1.1.cmml"><mi id="A3.p2.16.m16.1.1.2" xref="A3.p2.16.m16.1.1.2.cmml">γ</mi><mi id="A3.p2.16.m16.1.1.3" xref="A3.p2.16.m16.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p2.16.m16.1b"><apply id="A3.p2.16.m16.1.1.cmml" xref="A3.p2.16.m16.1.1"><csymbol cd="ambiguous" id="A3.p2.16.m16.1.1.1.cmml" xref="A3.p2.16.m16.1.1">subscript</csymbol><ci id="A3.p2.16.m16.1.1.2.cmml" xref="A3.p2.16.m16.1.1.2">𝛾</ci><ci id="A3.p2.16.m16.1.1.3.cmml" xref="A3.p2.16.m16.1.1.3">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.16.m16.1c">\gamma_{Q}</annotation></semantics></math>. We added an additional attention consistency loss to the total loss which reduces the <math id="A3.p2.17.m17.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="A3.p2.17.m17.1a"><msub id="A3.p2.17.m17.1.1" xref="A3.p2.17.m17.1.1.cmml"><mi id="A3.p2.17.m17.1.1.2" xref="A3.p2.17.m17.1.1.2.cmml">L</mi><mn id="A3.p2.17.m17.1.1.3" xref="A3.p2.17.m17.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A3.p2.17.m17.1b"><apply id="A3.p2.17.m17.1.1.cmml" xref="A3.p2.17.m17.1.1"><csymbol cd="ambiguous" id="A3.p2.17.m17.1.1.1.cmml" xref="A3.p2.17.m17.1.1">subscript</csymbol><ci id="A3.p2.17.m17.1.1.2.cmml" xref="A3.p2.17.m17.1.1.2">𝐿</ci><cn type="integer" id="A3.p2.17.m17.1.1.3.cmml" xref="A3.p2.17.m17.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.17.m17.1c">L_{2}</annotation></semantics></math> norm between these two attentions.</p>
</div>
<div id="A3.p3" class="ltx_para">
<p id="A3.p3.1" class="ltx_p">However, we found that this leads to reduction in model performance. Specifically, this reduces the performance of a cycle consistent Pythia model by 1.34% VQA accuracy when evaluated on the VQA v2.0 validation split (training on train split only).</p>
</div>
<div id="A3.p4" class="ltx_para">
<p id="A3.p4.4" class="ltx_p">We suspect one reason why enforcing attention consistency across rephrasings reduces performance is perhaps because minimizing a large number of diverse losses ( cross entropy losses <math id="A3.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{F}" display="inline"><semantics id="A3.p4.1.m1.1a"><msub id="A3.p4.1.m1.1.1" xref="A3.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p4.1.m1.1.1.2" xref="A3.p4.1.m1.1.1.2.cmml">ℒ</mi><mi id="A3.p4.1.m1.1.1.3" xref="A3.p4.1.m1.1.1.3.cmml">F</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p4.1.m1.1b"><apply id="A3.p4.1.m1.1.1.cmml" xref="A3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="A3.p4.1.m1.1.1.1.cmml" xref="A3.p4.1.m1.1.1">subscript</csymbol><ci id="A3.p4.1.m1.1.1.2.cmml" xref="A3.p4.1.m1.1.1.2">ℒ</ci><ci id="A3.p4.1.m1.1.1.3.cmml" xref="A3.p4.1.m1.1.1.3">𝐹</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.1.m1.1c">\mathcal{L}_{F}</annotation></semantics></math> and <math id="A3.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{cycle}" display="inline"><semantics id="A3.p4.2.m2.1a"><msub id="A3.p4.2.m2.1.1" xref="A3.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p4.2.m2.1.1.2" xref="A3.p4.2.m2.1.1.2.cmml">ℒ</mi><mrow id="A3.p4.2.m2.1.1.3" xref="A3.p4.2.m2.1.1.3.cmml"><mi id="A3.p4.2.m2.1.1.3.2" xref="A3.p4.2.m2.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="A3.p4.2.m2.1.1.3.1" xref="A3.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p4.2.m2.1.1.3.3" xref="A3.p4.2.m2.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="A3.p4.2.m2.1.1.3.1a" xref="A3.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p4.2.m2.1.1.3.4" xref="A3.p4.2.m2.1.1.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="A3.p4.2.m2.1.1.3.1b" xref="A3.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p4.2.m2.1.1.3.5" xref="A3.p4.2.m2.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="A3.p4.2.m2.1.1.3.1c" xref="A3.p4.2.m2.1.1.3.1.cmml">​</mo><mi id="A3.p4.2.m2.1.1.3.6" xref="A3.p4.2.m2.1.1.3.6.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.p4.2.m2.1b"><apply id="A3.p4.2.m2.1.1.cmml" xref="A3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="A3.p4.2.m2.1.1.1.cmml" xref="A3.p4.2.m2.1.1">subscript</csymbol><ci id="A3.p4.2.m2.1.1.2.cmml" xref="A3.p4.2.m2.1.1.2">ℒ</ci><apply id="A3.p4.2.m2.1.1.3.cmml" xref="A3.p4.2.m2.1.1.3"><times id="A3.p4.2.m2.1.1.3.1.cmml" xref="A3.p4.2.m2.1.1.3.1"></times><ci id="A3.p4.2.m2.1.1.3.2.cmml" xref="A3.p4.2.m2.1.1.3.2">𝑐</ci><ci id="A3.p4.2.m2.1.1.3.3.cmml" xref="A3.p4.2.m2.1.1.3.3">𝑦</ci><ci id="A3.p4.2.m2.1.1.3.4.cmml" xref="A3.p4.2.m2.1.1.3.4">𝑐</ci><ci id="A3.p4.2.m2.1.1.3.5.cmml" xref="A3.p4.2.m2.1.1.3.5">𝑙</ci><ci id="A3.p4.2.m2.1.1.3.6.cmml" xref="A3.p4.2.m2.1.1.3.6">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.2.m2.1c">\mathcal{L}_{cycle}</annotation></semantics></math> for VQA, sequence generation loss <math id="A3.p4.3.m3.1" class="ltx_Math" alttext="\mathcal{L}_{G}" display="inline"><semantics id="A3.p4.3.m3.1a"><msub id="A3.p4.3.m3.1.1" xref="A3.p4.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p4.3.m3.1.1.2" xref="A3.p4.3.m3.1.1.2.cmml">ℒ</mi><mi id="A3.p4.3.m3.1.1.3" xref="A3.p4.3.m3.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="A3.p4.3.m3.1b"><apply id="A3.p4.3.m3.1.1.cmml" xref="A3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="A3.p4.3.m3.1.1.1.cmml" xref="A3.p4.3.m3.1.1">subscript</csymbol><ci id="A3.p4.3.m3.1.1.2.cmml" xref="A3.p4.3.m3.1.1.2">ℒ</ci><ci id="A3.p4.3.m3.1.1.3.cmml" xref="A3.p4.3.m3.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.3.m3.1c">\mathcal{L}_{G}</annotation></semantics></math> for VQG and mean squared loss <math id="A3.p4.4.m4.1" class="ltx_Math" alttext="\mathcal{L}_{attention}" display="inline"><semantics id="A3.p4.4.m4.1a"><msub id="A3.p4.4.m4.1.1" xref="A3.p4.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A3.p4.4.m4.1.1.2" xref="A3.p4.4.m4.1.1.2.cmml">ℒ</mi><mrow id="A3.p4.4.m4.1.1.3" xref="A3.p4.4.m4.1.1.3.cmml"><mi id="A3.p4.4.m4.1.1.3.2" xref="A3.p4.4.m4.1.1.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.3" xref="A3.p4.4.m4.1.1.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1a" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.4" xref="A3.p4.4.m4.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1b" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.5" xref="A3.p4.4.m4.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1c" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.6" xref="A3.p4.4.m4.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1d" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.7" xref="A3.p4.4.m4.1.1.3.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1e" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.8" xref="A3.p4.4.m4.1.1.3.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1f" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.9" xref="A3.p4.4.m4.1.1.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="A3.p4.4.m4.1.1.3.1g" xref="A3.p4.4.m4.1.1.3.1.cmml">​</mo><mi id="A3.p4.4.m4.1.1.3.10" xref="A3.p4.4.m4.1.1.3.10.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.p4.4.m4.1b"><apply id="A3.p4.4.m4.1.1.cmml" xref="A3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="A3.p4.4.m4.1.1.1.cmml" xref="A3.p4.4.m4.1.1">subscript</csymbol><ci id="A3.p4.4.m4.1.1.2.cmml" xref="A3.p4.4.m4.1.1.2">ℒ</ci><apply id="A3.p4.4.m4.1.1.3.cmml" xref="A3.p4.4.m4.1.1.3"><times id="A3.p4.4.m4.1.1.3.1.cmml" xref="A3.p4.4.m4.1.1.3.1"></times><ci id="A3.p4.4.m4.1.1.3.2.cmml" xref="A3.p4.4.m4.1.1.3.2">𝑎</ci><ci id="A3.p4.4.m4.1.1.3.3.cmml" xref="A3.p4.4.m4.1.1.3.3">𝑡</ci><ci id="A3.p4.4.m4.1.1.3.4.cmml" xref="A3.p4.4.m4.1.1.3.4">𝑡</ci><ci id="A3.p4.4.m4.1.1.3.5.cmml" xref="A3.p4.4.m4.1.1.3.5">𝑒</ci><ci id="A3.p4.4.m4.1.1.3.6.cmml" xref="A3.p4.4.m4.1.1.3.6">𝑛</ci><ci id="A3.p4.4.m4.1.1.3.7.cmml" xref="A3.p4.4.m4.1.1.3.7">𝑡</ci><ci id="A3.p4.4.m4.1.1.3.8.cmml" xref="A3.p4.4.m4.1.1.3.8">𝑖</ci><ci id="A3.p4.4.m4.1.1.3.9.cmml" xref="A3.p4.4.m4.1.1.3.9">𝑜</ci><ci id="A3.p4.4.m4.1.1.3.10.cmml" xref="A3.p4.4.m4.1.1.3.10">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p4.4.m4.1c">\mathcal{L}_{attention}</annotation></semantics></math> for attention consistency) is a hard problem to optimize.
Concretely identifying why enforcing attention consistency across question rephrasings hurts performance is currently under investigation and is part of future work. We find naively matching attentions across question rephrasings is not effective in current settings and therefore do not include this in the final model.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Question Generation</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">Fig <a href="#A5.F8" title="Figure 8 ‣ Appendix E Hyperparameters ‣ Cycle-Consistency for Robust Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows qualitative examples of answer conditioned questions generated by our VQG model.
Our VQG model is able to correctly generate answer conditioned questions for a wide range of answers ranging from numbers, to colors and even yes/no.</p>
</div>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Hyperparameters</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.7" class="ltx_p">We use the default hyperparameters as described in publicly available implementations of MUTAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and BAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
When using these models as base VQA models to train cycle consistent variants of them, we use the same parameters for the VQA model. For the the VQG model we use <math id="A5.p1.1.m1.1" class="ltx_Math" alttext="T_{sim}{=}0.9" display="inline"><semantics id="A5.p1.1.m1.1a"><mrow id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml"><msub id="A5.p1.1.m1.1.1.2" xref="A5.p1.1.m1.1.1.2.cmml"><mi id="A5.p1.1.m1.1.1.2.2" xref="A5.p1.1.m1.1.1.2.2.cmml">T</mi><mrow id="A5.p1.1.m1.1.1.2.3" xref="A5.p1.1.m1.1.1.2.3.cmml"><mi id="A5.p1.1.m1.1.1.2.3.2" xref="A5.p1.1.m1.1.1.2.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="A5.p1.1.m1.1.1.2.3.1" xref="A5.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A5.p1.1.m1.1.1.2.3.3" xref="A5.p1.1.m1.1.1.2.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="A5.p1.1.m1.1.1.2.3.1a" xref="A5.p1.1.m1.1.1.2.3.1.cmml">​</mo><mi id="A5.p1.1.m1.1.1.2.3.4" xref="A5.p1.1.m1.1.1.2.3.4.cmml">m</mi></mrow></msub><mo id="A5.p1.1.m1.1.1.1" xref="A5.p1.1.m1.1.1.1.cmml">=</mo><mn id="A5.p1.1.m1.1.1.3" xref="A5.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><apply id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1"><eq id="A5.p1.1.m1.1.1.1.cmml" xref="A5.p1.1.m1.1.1.1"></eq><apply id="A5.p1.1.m1.1.1.2.cmml" xref="A5.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A5.p1.1.m1.1.1.2.1.cmml" xref="A5.p1.1.m1.1.1.2">subscript</csymbol><ci id="A5.p1.1.m1.1.1.2.2.cmml" xref="A5.p1.1.m1.1.1.2.2">𝑇</ci><apply id="A5.p1.1.m1.1.1.2.3.cmml" xref="A5.p1.1.m1.1.1.2.3"><times id="A5.p1.1.m1.1.1.2.3.1.cmml" xref="A5.p1.1.m1.1.1.2.3.1"></times><ci id="A5.p1.1.m1.1.1.2.3.2.cmml" xref="A5.p1.1.m1.1.1.2.3.2">𝑠</ci><ci id="A5.p1.1.m1.1.1.2.3.3.cmml" xref="A5.p1.1.m1.1.1.2.3.3">𝑖</ci><ci id="A5.p1.1.m1.1.1.2.3.4.cmml" xref="A5.p1.1.m1.1.1.2.3.4">𝑚</ci></apply></apply><cn type="float" id="A5.p1.1.m1.1.1.3.cmml" xref="A5.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">T_{sim}{=}0.9</annotation></semantics></math>, <math id="A5.p1.2.m2.1" class="ltx_Math" alttext="\lambda_{G}{=}1.0" display="inline"><semantics id="A5.p1.2.m2.1a"><mrow id="A5.p1.2.m2.1.1" xref="A5.p1.2.m2.1.1.cmml"><msub id="A5.p1.2.m2.1.1.2" xref="A5.p1.2.m2.1.1.2.cmml"><mi id="A5.p1.2.m2.1.1.2.2" xref="A5.p1.2.m2.1.1.2.2.cmml">λ</mi><mi id="A5.p1.2.m2.1.1.2.3" xref="A5.p1.2.m2.1.1.2.3.cmml">G</mi></msub><mo id="A5.p1.2.m2.1.1.1" xref="A5.p1.2.m2.1.1.1.cmml">=</mo><mn id="A5.p1.2.m2.1.1.3" xref="A5.p1.2.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.2.m2.1b"><apply id="A5.p1.2.m2.1.1.cmml" xref="A5.p1.2.m2.1.1"><eq id="A5.p1.2.m2.1.1.1.cmml" xref="A5.p1.2.m2.1.1.1"></eq><apply id="A5.p1.2.m2.1.1.2.cmml" xref="A5.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A5.p1.2.m2.1.1.2.1.cmml" xref="A5.p1.2.m2.1.1.2">subscript</csymbol><ci id="A5.p1.2.m2.1.1.2.2.cmml" xref="A5.p1.2.m2.1.1.2.2">𝜆</ci><ci id="A5.p1.2.m2.1.1.2.3.cmml" xref="A5.p1.2.m2.1.1.2.3">𝐺</ci></apply><cn type="float" id="A5.p1.2.m2.1.1.3.cmml" xref="A5.p1.2.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.2.m2.1c">\lambda_{G}{=}1.0</annotation></semantics></math>, <math id="A5.p1.3.m3.1" class="ltx_Math" alttext="\lambda_{C}{=}0.5" display="inline"><semantics id="A5.p1.3.m3.1a"><mrow id="A5.p1.3.m3.1.1" xref="A5.p1.3.m3.1.1.cmml"><msub id="A5.p1.3.m3.1.1.2" xref="A5.p1.3.m3.1.1.2.cmml"><mi id="A5.p1.3.m3.1.1.2.2" xref="A5.p1.3.m3.1.1.2.2.cmml">λ</mi><mi id="A5.p1.3.m3.1.1.2.3" xref="A5.p1.3.m3.1.1.2.3.cmml">C</mi></msub><mo id="A5.p1.3.m3.1.1.1" xref="A5.p1.3.m3.1.1.1.cmml">=</mo><mn id="A5.p1.3.m3.1.1.3" xref="A5.p1.3.m3.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.3.m3.1b"><apply id="A5.p1.3.m3.1.1.cmml" xref="A5.p1.3.m3.1.1"><eq id="A5.p1.3.m3.1.1.1.cmml" xref="A5.p1.3.m3.1.1.1"></eq><apply id="A5.p1.3.m3.1.1.2.cmml" xref="A5.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="A5.p1.3.m3.1.1.2.1.cmml" xref="A5.p1.3.m3.1.1.2">subscript</csymbol><ci id="A5.p1.3.m3.1.1.2.2.cmml" xref="A5.p1.3.m3.1.1.2.2">𝜆</ci><ci id="A5.p1.3.m3.1.1.2.3.cmml" xref="A5.p1.3.m3.1.1.2.3">𝐶</ci></apply><cn type="float" id="A5.p1.3.m3.1.1.3.cmml" xref="A5.p1.3.m3.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.3.m3.1c">\lambda_{C}{=}0.5</annotation></semantics></math> and <math id="A5.p1.4.m4.1" class="ltx_Math" alttext="A_{iter}{=}5500" display="inline"><semantics id="A5.p1.4.m4.1a"><mrow id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml"><msub id="A5.p1.4.m4.1.1.2" xref="A5.p1.4.m4.1.1.2.cmml"><mi id="A5.p1.4.m4.1.1.2.2" xref="A5.p1.4.m4.1.1.2.2.cmml">A</mi><mrow id="A5.p1.4.m4.1.1.2.3" xref="A5.p1.4.m4.1.1.2.3.cmml"><mi id="A5.p1.4.m4.1.1.2.3.2" xref="A5.p1.4.m4.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A5.p1.4.m4.1.1.2.3.1" xref="A5.p1.4.m4.1.1.2.3.1.cmml">​</mo><mi id="A5.p1.4.m4.1.1.2.3.3" xref="A5.p1.4.m4.1.1.2.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="A5.p1.4.m4.1.1.2.3.1a" xref="A5.p1.4.m4.1.1.2.3.1.cmml">​</mo><mi id="A5.p1.4.m4.1.1.2.3.4" xref="A5.p1.4.m4.1.1.2.3.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="A5.p1.4.m4.1.1.2.3.1b" xref="A5.p1.4.m4.1.1.2.3.1.cmml">​</mo><mi id="A5.p1.4.m4.1.1.2.3.5" xref="A5.p1.4.m4.1.1.2.3.5.cmml">r</mi></mrow></msub><mo id="A5.p1.4.m4.1.1.1" xref="A5.p1.4.m4.1.1.1.cmml">=</mo><mn id="A5.p1.4.m4.1.1.3" xref="A5.p1.4.m4.1.1.3.cmml">5500</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><apply id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1"><eq id="A5.p1.4.m4.1.1.1.cmml" xref="A5.p1.4.m4.1.1.1"></eq><apply id="A5.p1.4.m4.1.1.2.cmml" xref="A5.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="A5.p1.4.m4.1.1.2.1.cmml" xref="A5.p1.4.m4.1.1.2">subscript</csymbol><ci id="A5.p1.4.m4.1.1.2.2.cmml" xref="A5.p1.4.m4.1.1.2.2">𝐴</ci><apply id="A5.p1.4.m4.1.1.2.3.cmml" xref="A5.p1.4.m4.1.1.2.3"><times id="A5.p1.4.m4.1.1.2.3.1.cmml" xref="A5.p1.4.m4.1.1.2.3.1"></times><ci id="A5.p1.4.m4.1.1.2.3.2.cmml" xref="A5.p1.4.m4.1.1.2.3.2">𝑖</ci><ci id="A5.p1.4.m4.1.1.2.3.3.cmml" xref="A5.p1.4.m4.1.1.2.3.3">𝑡</ci><ci id="A5.p1.4.m4.1.1.2.3.4.cmml" xref="A5.p1.4.m4.1.1.2.3.4">𝑒</ci><ci id="A5.p1.4.m4.1.1.2.3.5.cmml" xref="A5.p1.4.m4.1.1.2.3.5">𝑟</ci></apply></apply><cn type="integer" id="A5.p1.4.m4.1.1.3.cmml" xref="A5.p1.4.m4.1.1.3">5500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">A_{iter}{=}5500</annotation></semantics></math>.
While some models use adaptive learning rates for their base VQA models, the VQG model is always trained with a fixed learning rate of <math id="A5.p1.5.m5.1" class="ltx_Math" alttext="0.0005" display="inline"><semantics id="A5.p1.5.m5.1a"><mn id="A5.p1.5.m5.1.1" xref="A5.p1.5.m5.1.1.cmml">0.0005</mn><annotation-xml encoding="MathML-Content" id="A5.p1.5.m5.1b"><cn type="float" id="A5.p1.5.m5.1.1.cmml" xref="A5.p1.5.m5.1.1">0.0005</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.5.m5.1c">0.0005</annotation></semantics></math>. In case of BAN and Pythia, we also clip the gradients whose <math id="A5.p1.6.m6.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="A5.p1.6.m6.1a"><msub id="A5.p1.6.m6.1.1" xref="A5.p1.6.m6.1.1.cmml"><mi id="A5.p1.6.m6.1.1.2" xref="A5.p1.6.m6.1.1.2.cmml">L</mi><mn id="A5.p1.6.m6.1.1.3" xref="A5.p1.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A5.p1.6.m6.1b"><apply id="A5.p1.6.m6.1.1.cmml" xref="A5.p1.6.m6.1.1"><csymbol cd="ambiguous" id="A5.p1.6.m6.1.1.1.cmml" xref="A5.p1.6.m6.1.1">subscript</csymbol><ci id="A5.p1.6.m6.1.1.2.cmml" xref="A5.p1.6.m6.1.1.2">𝐿</ci><cn type="integer" id="A5.p1.6.m6.1.1.3.cmml" xref="A5.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.6.m6.1c">L_{2}</annotation></semantics></math> norm is greater than <math id="A5.p1.7.m7.1" class="ltx_Math" alttext="0.25" display="inline"><semantics id="A5.p1.7.m7.1a"><mn id="A5.p1.7.m7.1.1" xref="A5.p1.7.m7.1.1.cmml">0.25</mn><annotation-xml encoding="MathML-Content" id="A5.p1.7.m7.1b"><cn type="float" id="A5.p1.7.m7.1.1.cmml" xref="A5.p1.7.m7.1.1">0.25</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.7.m7.1c">0.25</annotation></semantics></math>.</p>
</div>
<figure id="A5.F6" class="ltx_figure"><img src="/html/1902.05660/assets/x6.png" id="A5.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="528" height="631" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
<span id="A5.F6.3.1" class="ltx_text ltx_font_bold">Examples from our VQA-Rephrasings dataset</span>.
The first question (shown in gray) in each block is the original question from VQA v2.0 validation set,
the questions that follow (shown in black) are rephrasings collected in VQA-Rephrasings.
</figcaption>
</figure>
<figure id="A5.F7" class="ltx_figure"><img src="/html/1902.05660/assets/x7.png" id="A5.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="528" height="562" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
Visualization of textual and image region attention for different question variants:
Each row compares answers predicted and attention for two question rephrasings using a baseline Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> model and the same Pythia model trained with our framework (Pythia + CC).
Higher opaqueness in highlighted regions represents higher attention.
First and third rows show the output of a Pythia model (baseline) and second and forth rows show the output of a Pythia model (baseline + CC) trained with our framework.
As one can see, in most examples, the attention over image regions doesn’t vary much for models trained with our framework.
However for the baseline model, one can see that by very minor linguistic changes in the question it is possible to predict completely different answers (Row 2, Columns 1 and 3).
These examples qualitatively demonstrate the robustness of models trained with our framework.
</figcaption>
</figure>
<figure id="A5.F8" class="ltx_figure"><img src="/html/1902.05660/assets/x8.png" id="A5.F8.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="528" height="453" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Qualitative examples of answer conditioned question generation by our VQG module.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1902.05659" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1902.05660" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1902.05660">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1902.05660" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1902.05662" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 17:30:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
