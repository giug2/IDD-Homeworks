<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.08899] Document Visual Question Answering Challenge 2020</title><meta property="og:description" content="This paper presents results of Document Visual Question Answering Challenge organized as part of “Text and Documents in the Deep Learning Era” workshop, in CVPR 2020. The challenge introduces a new problem - Visual Que…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Document Visual Question Answering Challenge 2020">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Document Visual Question Answering Challenge 2020">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.08899">

<!--Generated on Wed Mar  6 18:25:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="visual question answering document understanding">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>CVIT, IIIT Hyderabad, India </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Computer Vision Center, UAB, Spain </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>University of Massachusetts Amherst, USA
<br class="ltx_break"></span></span></span>
<h1 class="ltx_title ltx_title_document">Document Visual Question Answering Challenge 2020</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minesh Mathew 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rubèn Tito
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dimosthenis Karatzas
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">R. Manmatha
</span><span class="ltx_author_notes">33</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">C.V. Jawahar
</span><span class="ltx_author_notes">11</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.4" class="ltx_p">This paper presents results of Document Visual Question Answering Challenge organized as part of “Text and Documents in the Deep Learning Era” workshop, in CVPR 2020. The challenge introduces a new problem - Visual Question Answering on document images. The challenge comprised two tasks.
The first task concerns with asking questions on a single document image.
On the other hand, the second task is set as a retrieval task where the question is posed over a collection of images.
For the task 1 a new dataset is introduced comprising <math id="id1.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="id1.1.m1.2a"><mrow id="id1.1.m1.2.3.2" xref="id1.1.m1.2.3.1.cmml"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">50</mn><mo id="id1.1.m1.2.3.2.1" xref="id1.1.m1.2.3.1.cmml">,</mo><mn id="id1.1.m1.2.2" xref="id1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.2b"><list id="id1.1.m1.2.3.1.cmml" xref="id1.1.m1.2.3.2"><cn type="integer" id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">50</cn><cn type="integer" id="id1.1.m1.2.2.cmml" xref="id1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.2c">50,000</annotation></semantics></math> questions-answer(s) pairs defined over <math id="id2.2.m2.2" class="ltx_Math" alttext="12,767" display="inline"><semantics id="id2.2.m2.2a"><mrow id="id2.2.m2.2.3.2" xref="id2.2.m2.2.3.1.cmml"><mn id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">12</mn><mo id="id2.2.m2.2.3.2.1" xref="id2.2.m2.2.3.1.cmml">,</mo><mn id="id2.2.m2.2.2" xref="id2.2.m2.2.2.cmml">767</mn></mrow><annotation-xml encoding="MathML-Content" id="id2.2.m2.2b"><list id="id2.2.m2.2.3.1.cmml" xref="id2.2.m2.2.3.2"><cn type="integer" id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">12</cn><cn type="integer" id="id2.2.m2.2.2.cmml" xref="id2.2.m2.2.2">767</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.2c">12,767</annotation></semantics></math> document images. For task 2 another dataset has been created comprising <math id="id3.3.m3.1" class="ltx_Math" alttext="20" display="inline"><semantics id="id3.3.m3.1a"><mn id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><cn type="integer" id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">20</annotation></semantics></math> questions over <math id="id4.4.m4.2" class="ltx_Math" alttext="14,362" display="inline"><semantics id="id4.4.m4.2a"><mrow id="id4.4.m4.2.3.2" xref="id4.4.m4.2.3.1.cmml"><mn id="id4.4.m4.1.1" xref="id4.4.m4.1.1.cmml">14</mn><mo id="id4.4.m4.2.3.2.1" xref="id4.4.m4.2.3.1.cmml">,</mo><mn id="id4.4.m4.2.2" xref="id4.4.m4.2.2.cmml">362</mn></mrow><annotation-xml encoding="MathML-Content" id="id4.4.m4.2b"><list id="id4.4.m4.2.3.1.cmml" xref="id4.4.m4.2.3.2"><cn type="integer" id="id4.4.m4.1.1.cmml" xref="id4.4.m4.1.1">14</cn><cn type="integer" id="id4.4.m4.2.2.cmml" xref="id4.4.m4.2.2">362</cn></list></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.2c">14,362</annotation></semantics></math> document images which share the same document template.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>visual question answering document understanding
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) has attracted an intense research effort over the past few years, as one of the most important tasks at the frontier between vision and language. Notably, at the same time as reading systems research considered that the field of scene text understanding was mature enough to build scene text based VQA systems on, VQA researchers realised that the capacity of reading is actually important for any VQA agent. As a result, ST-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and TextVQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> were introduced in parallel in 2019
In this year’s Visual Question Answering workshop at CVPR 2020, three out of the five VQA challenges actually revolve around text <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
The time seemed right for the introduction of a large scale scanned Document VQA task.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this short paper we introduce the DocVQA dataset and the challenge organized as part of the “Text and Documents in the Deep Learning Era” workshop at CVPR 2020. The paper offers a quick description of the dataset, the challenge and results of the submitted methods to date. The challenge is open for continuous submission at the Robust Reading Competition (RRC) portal <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://rrc.cvc.uab.es/?ch=17" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://rrc.cvc.uab.es/?ch=17</a></span></span></span>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:195.1pt;"><img src="/html/2008.08899/assets/images/ljvl0226_1.png" id="S1.F1.sf1.g1" class="ltx_graphics ltx_img_portrait" width="598" height="776" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
<br class="ltx_break ltx_break">
<table id="S1.F1.sf1.1" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.sf1.1.1.1" class="ltx_tr">
<td id="S1.F1.sf1.1.1.1.1" class="ltx_td ltx_align_left">Who is the message for ?</td>
</tr>
<tr id="S1.F1.sf1.1.2.2" class="ltx_tr">
<td id="S1.F1.sf1.1.2.2.1" class="ltx_td ltx_align_left">What is the date of the message ?</td>
</tr>
</tbody>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:195.1pt;"><img src="/html/2008.08899/assets/images/Task2-Example0_zoom3.jpg" id="S1.F1.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="601" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
<br class="ltx_break ltx_break">
<table id="S1.F1.sf2.1" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.sf2.1.1.1" class="ltx_tr">
<td id="S1.F1.sf2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S1.F1.sf2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.F1.sf2.1.1.1.1.1.1" class="ltx_p" style="width:156.5pt;">In which legislative counties did Gary L. Schoessler run for County Commissioner?</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">(Left)</span> A sample document image and questions defined on it from the dataset for task 1. <span id="S1.F1.4.2" class="ltx_text ltx_font_bold">(Right)</span> A sample document image from dataset for task 2 and a sample question posed on the whole task 2 document collection.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Challenge</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The challenge ran between March and May 2020.
Ranking of submitted methods presented in this report reflect state of submissions at the closure of the official challenge period.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The challenge comprised two separate tasks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Task 1 - VQA on Document Images</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Task 1 of the challenge is similar to the typical VQA setting , i.e answer a question asked on an image; here a document image. Answer to the question is always text present in the image, or in other words it is an extractive QA task. The participants are required to submit their results on the test split of the dataset which comprise of 5188 questions defined on 1287 document images. For evaluation of the submissions, we use the same metric as ST-VQA challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which is Average Normalized Levenshtein Similarity (ANLS).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Task 2 - VQA on Document Image Collection</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In task 2 the question is posed over a collection of documents instead of a single image. Hence, the task requires one to retrieve the evidence as well output the answer.
For this first edition, we focused on the first part to rank the methods, and left the second as an optional response, which is nevertheless evaluated.
Performance of the methods for the retrieval part is scored by the Mean Average Precision (MAP), which is the standard metric in retrieval scenarios. It is important to note that we force positive evidences that have been equally scored by the methods, to be at the end of the ranking among those documents. This is to ensure that the ranking is consistent and do not depend on the default order, or the way the score is evaluated. Finally, in spite of the fact that answers score is not used to rank the methods, precision and recall are used to show the performance of the methods in this task.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Datasets</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">Dataset for task 1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> comprises <math id="S3.p1.1.m1.2" class="ltx_Math" alttext="50,000" display="inline"><semantics id="S3.p1.1.m1.2a"><mrow id="S3.p1.1.m1.2.3.2" xref="S3.p1.1.m1.2.3.1.cmml"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">50</mn><mo id="S3.p1.1.m1.2.3.2.1" xref="S3.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.2b"><list id="S3.p1.1.m1.2.3.1.cmml" xref="S3.p1.1.m1.2.3.2"><cn type="integer" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">50</cn><cn type="integer" id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.2c">50,000</annotation></semantics></math> questions over <math id="S3.p1.2.m2.2" class="ltx_Math" alttext="12,767" display="inline"><semantics id="S3.p1.2.m2.2a"><mrow id="S3.p1.2.m2.2.3.2" xref="S3.p1.2.m2.2.3.1.cmml"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">12</mn><mo id="S3.p1.2.m2.2.3.2.1" xref="S3.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.p1.2.m2.2.2" xref="S3.p1.2.m2.2.2.cmml">767</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.2b"><list id="S3.p1.2.m2.2.3.1.cmml" xref="S3.p1.2.m2.2.3.2"><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">12</cn><cn type="integer" id="S3.p1.2.m2.2.2.cmml" xref="S3.p1.2.m2.2.2">767</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.2c">12,767</annotation></semantics></math> document images. The images are pages extracted from 6071 scanned documents sourced from the industry documents library <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://www.industrydocuments.ucsf.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.industrydocuments.ucsf.edu/</a></span></span></span>. We manually selected the documents from the library to ensure document variety.
Each question-answer(s) pair in the dataset is also qualified with a question type among 9 types that denote the type of analysis required to arrive at the answer. The 9 question types are figure, ‘form’, ‘table/list’, ‘layout’, ‘running text’, ‘photograph’, ‘handwritten’, ‘yes/no’ and ‘other’. We also provide commercial OCR transcriptions of all the documents.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.2" class="ltx_p">Dataset for task 2 consists of <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="14,362" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">14</mn><mo id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">362</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><list id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><cn type="integer" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">14</cn><cn type="integer" id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">362</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">14,362</annotation></semantics></math> document images sharing the same document template — US Candidate Registration form.
Among these images there are documents filled in handwriting like <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Document Visual Question Answering Challenge 2020" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> (right), while others are typewritten.
The images were sourced from the Open Data portal of the Public Disclosure Commission (PDC) and over this collection a set of <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.p2.2.m2.1a"><mn id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><cn type="integer" id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">20</annotation></semantics></math> different questions is posed.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Task 1:</span> We received submissions from 9 different teams for task 1.
The final ranking is shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Results ‣ Document Visual Question Answering Challenge 2020" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Performance for the 9 submissions for different question types is shown in Figure <a href="#S4.F2" title="Figure 2 ‣ Table 1 ‣ 4 Results ‣ Document Visual Question Answering Challenge 2020" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T1.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_bottom" style="width:151.8pt;">
<table id="S4.T1.fig1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.fig1.1.1.1" class="ltx_tr">
<th id="S4.T1.fig1.1.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row">
<span id="S4.T1.fig1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.1.1.1.1.1" class="ltx_p">Method</span>
</span>
</th>
<th id="S4.T1.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">ANLS</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.fig1.1.2.1" class="ltx_tr">
<th id="S4.T1.fig1.1.2.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_tt">
<span id="S4.T1.fig1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.2.1.1.1.1" class="ltx_p">PingAn-OneConnect-Gammalab-DQA</span>
</span>
</th>
<td id="S4.T1.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">0.85</td>
</tr>
<tr id="S4.T1.fig1.1.3.2" class="ltx_tr">
<th id="S4.T1.fig1.1.3.2.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.3.2.1.1.1" class="ltx_p">Structural LM- v2</span>
</span>
</th>
<td id="S4.T1.fig1.1.3.2.2" class="ltx_td ltx_align_center">0.75</td>
</tr>
<tr id="S4.T1.fig1.1.4.3" class="ltx_tr">
<th id="S4.T1.fig1.1.4.3.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.4.3.1.1.1" class="ltx_p">QA_Base_MRC_1</span>
</span>
</th>
<td id="S4.T1.fig1.1.4.3.2" class="ltx_td ltx_align_center">0.74</td>
</tr>
<tr id="S4.T1.fig1.1.5.4" class="ltx_tr">
<th id="S4.T1.fig1.1.5.4.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.5.4.1.1.1" class="ltx_p">HyperDQA_V4</span>
</span>
</th>
<td id="S4.T1.fig1.1.5.4.2" class="ltx_td ltx_align_center">0.69</td>
</tr>
<tr id="S4.T1.fig1.1.6.5" class="ltx_tr">
<th id="S4.T1.fig1.1.6.5.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.6.5.1.1.1" class="ltx_p">bert fulldata fintuned</span>
</span>
</th>
<td id="S4.T1.fig1.1.6.5.2" class="ltx_td ltx_align_center">0.59</td>
</tr>
<tr id="S4.T1.fig1.1.7.6" class="ltx_tr">
<th id="S4.T1.fig1.1.7.6.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.7.6.1.1.1" class="ltx_p">Plain BERT QA</span>
</span>
</th>
<td id="S4.T1.fig1.1.7.6.2" class="ltx_td ltx_align_center">0.35</td>
</tr>
<tr id="S4.T1.fig1.1.8.7" class="ltx_tr">
<th id="S4.T1.fig1.1.8.7.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.8.7.1.1.1" class="ltx_p">HDNet</span>
</span>
</th>
<td id="S4.T1.fig1.1.8.7.2" class="ltx_td ltx_align_center">0.34</td>
</tr>
<tr id="S4.T1.fig1.1.9.8" class="ltx_tr">
<th id="S4.T1.fig1.1.9.8.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row">
<span id="S4.T1.fig1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.9.8.1.1.1" class="ltx_p">CLOVA OCR</span>
</span>
</th>
<td id="S4.T1.fig1.1.9.8.2" class="ltx_td ltx_align_center">0.33</td>
</tr>
<tr id="S4.T1.fig1.1.10.9" class="ltx_tr">
<th id="S4.T1.fig1.1.10.9.1" class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb">
<span id="S4.T1.fig1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.fig1.1.10.9.1.1.1" class="ltx_p">docVQAQV_V0.1</span>
</span>
</th>
<td id="S4.T1.fig1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb">0.30</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 1: </span><span id="S4.T1.fig1.3.1" class="ltx_text" style="font-size:80%;">Final ranking for task 1</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_bottom" style="width:260.2pt;"><img src="/html/2008.08899/assets/x1.png" id="S4.T1.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S4.F2.2.1" class="ltx_text" style="font-size:80%;">Performance of the submitted methods for different question types in task 1.</span></figcaption>
</figure>
</div>
</div>
</figure>
<figure id="S4.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T2.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<table id="S4.T2.fig1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.fig1.1.1.1" class="ltx_tr">
<td id="S4.T2.fig1.1.1.1.1" class="ltx_td"></td>
<th id="S4.T2.fig1.1.1.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="S4.T2.fig1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Retrieval</th>
<th id="S4.T2.fig1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2">Answers</th>
</tr>
<tr id="S4.T2.fig1.1.2.2" class="ltx_tr">
<td id="S4.T2.fig1.1.2.2.1" class="ltx_td ltx_align_left">Method</td>
<td id="S4.T2.fig1.1.2.2.2" class="ltx_td"></td>
<td id="S4.T2.fig1.1.2.2.3" class="ltx_td ltx_align_left">MAP</td>
<td id="S4.T2.fig1.1.2.2.4" class="ltx_td ltx_align_center">Precision</td>
<td id="S4.T2.fig1.1.2.2.5" class="ltx_td ltx_align_left">Recall</td>
</tr>
<tr id="S4.T2.fig1.1.3.3" class="ltx_tr">
<th id="S4.T2.fig1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">DQA</th>
<th id="S4.T2.fig1.1.3.3.2" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="S4.T2.fig1.1.3.3.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">0.8090</th>
<th id="S4.T2.fig1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">-</th>
<th id="S4.T2.fig1.1.3.3.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">-</th>
</tr>
<tr id="S4.T2.fig1.1.4.4" class="ltx_tr">
<td id="S4.T2.fig1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_bb">DOCR</td>
<td id="S4.T2.fig1.1.4.4.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T2.fig1.1.4.4.3" class="ltx_td ltx_align_left ltx_border_bb">0.7915</td>
<td id="S4.T2.fig1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="S4.T2.fig1.1.4.4.5" class="ltx_td ltx_align_left ltx_border_bb">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span><span id="S4.T2.fig1.3.1" class="ltx_text" style="font-size:80%;">Final ranking for task 2.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3" class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" style="width:216.8pt;"><img src="/html/2008.08899/assets/images/Task2-QAP.png" id="S4.T2.1.g1" class="ltx_graphics ltx_figure_panel ltx_img_landscape" width="598" height="416" alt="Refer to caption">
<br class="ltx_break ltx_break">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.2.1" class="ltx_text" style="font-size:80%;">Average precision of the submitted methods for each question in the test set.</span></figcaption>
</figure>
</div>
</div>
</figure>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Task 2:</span> We received submissions from two different teams. The winning method is DQA from PingAn team, followed by a small margin by DOCR from iFLYTEK team (see <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Document Visual Question Answering Challenge 2020" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>). None of the submitted methods provided the answers to the questions.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgements</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work has been supported by Amazon through an AWS Machine Learning Research Award</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:80%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:80%;">
Biten, A.F., Tito, R., Mafla, A., Gómez, L., Rusiñol, M., Mathew,
M., Jawahar, C.V., Valveny, E., Karatzas, D.: ICDAR 2019 competition on
scene text visual question answering. CoRR </span><span id="bib.bib1.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;">abs/1907.00490</span><span id="bib.bib1.3.3" class="ltx_text" style="font-size:80%;"> (2019)
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:80%;">
Biten, A.F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar,
C., Karatzas, D.: Scene text visual question answering. In: ICCV (2019)
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:80%;">
Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J.,
Bigham, J.P.: Vizwiz grand challenge: Answering visual questions from blind
people. In: CVPR. pp. 3608–3617 (2018)
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:80%;">
Jayasundara, V., Jayasekara, S., Jayasekara, H., Rajasegaran, J., Seneviratne,
S., Rodrigo, R.: Textcaps: Handwritten character recognition with very small
datasets. In: WACV. pp. 254–262. IEEE (2019)
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:80%;">
Mathew, M., Karatzas, D., Jawahar, C.: Docvqa: A dataset for vqa on document
images. In: WACV (2021)
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:80%;">
Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,
Rohrbach, M.: Towards vqa models that can read. In: CVPR. pp. 8317–8326
(2019)
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.08898" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.08899" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.08899">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.08899" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.08900" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 18:25:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
