<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>APEX: Attention on Personality based Emotion ReXgnition Framework</title>
<!--Generated on Tue Sep 10 00:03:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Affective Computing,  Machine Learning,  Emotion Recognition
" lang="en" name="keywords"/>
<base href="/html/2409.06118v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S1" title="In APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S2" title="In APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S2.SS1" title="In II Method â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S2.SS2" title="In II Method â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">APEX framework</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S3" title="In APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experiments and Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S4" title="In APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S5" title="In APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_bold" id="id1.id1">APEX</span>: <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="id2.id2">A</span>ttention on <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="id3.id3">P</span>ersonality based <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="id4.id4">E</span>motion Re<span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="id5.id5">X</span>gnition Framework
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ruijie Fang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.1.id1">ECE Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.2.id2">University of California, Davis 
<br class="ltx_break"/></span>CA, USA
<br class="ltx_break"/>rjfang@ucdavis.edu
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruoyu Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.1.id1">ECE Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id9.2.id2">University of California, Davis
<br class="ltx_break"/></span>CA, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elahe Hosseini
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id10.1.id1">ECE Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id11.2.id2">University of California, Davis
<br class="ltx_break"/></span>CA, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chongzhou Fang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id12.1.id1">ECE Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id13.2.id2">University of California, Davis
<br class="ltx_break"/></span>CA, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahdi Eslaminehr
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id14.1.id1">Quandary Peak Research </span>
<br class="ltx_break"/>CA, USA 
<br class="ltx_break"/>mahdi@quandarypeak.com
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Setareh Rafatirad
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id15.1.id1">CS Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id16.2.id2">University of California, Davis
<br class="ltx_break"/></span>CA, USA
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Houman Homayoun
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id17.1.id1">ECE Department</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id18.2.id2">University of California, Davis
<br class="ltx_break"/></span>CA, USA
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id19.id1">Automated emotion recognition has applications in various fields, such as human-machine interaction, healthcare, security, education, and emotion-aware recommendation/feedback systems. Developing methods to analyze human emotions accurately is essential to enable such diverse applications. Multiple studies have been conducted to explore the possibility of using physiological signals and machine-learning techniques to evaluate human emotions. Furthermore, internal factors such as personality have been considered and involved in emotion recognition. However, integrating personality that is user specific within traditional machine-learning methods that use user-agnostic large data sets has become a critical problem. This study proposes the APEX: attention on personality-based emotion recognition framework, in which multiple weak classifiers are trained on physiological signals of each participantâ€™s data, and the classification results are reweighed based on the personality correlations between corresponding subjects and test subjects. Experiments have been conducted on the ASCERTAIN dataset, and the results show that the proposed framework outperforms existing studies.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Affective Computing, Machine Learning, Emotion Recognition

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Emotion is a mental state caused by neurophysiological changes associated with thoughts, feelings, and behavioral responses, which play an essential role in human life <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib1" title="">1</a>]</cite>. From the perspective of evolution, emotion is a product of natural selection as it helps humans survive. For example, fear causes humans to avoid harm, joy makes us repeat what works, sadness nudges us to ask for help, and the emotion of disgust makes us spit up accidentally eaten foreign objects. In modern society, emotion is essential as it directly influences peopleâ€™s activity and productivity. Positive emotions can promote coordination and organization, which is conducive to improving productivity. In contrast, negative emotions can make people feel bored, depressed, and dull and negatively affect peopleâ€™s creative thinking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Emotion has three distinct components: a subjective experience, external behaviors, and physiological arousal. Subjective experience is an individualâ€™s self-feeling of different emotional states <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib6" title="">6</a>]</cite>. External behaviors of emotions, often called expressions, is the quantified form of actions of various parts of the body when an emotional state occurs, including facial expressions, gesture expressions, and intonation expressions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib8" title="">8</a>]</cite>. Physiological arousal refers to the physiological response to emotion, including a series of responses in the autonomic nervous system (ANS). Multiple studies have been conducted to automatically recognize emotion based on external behaviors or physiological signals. These methods exploit the connections between emotions, external behaviors, and physiological arousal to automatically recognize emotion and apply it in different applications e.g. human-machine interface, neuro-marketing, and healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Automated emotion recognition can be categorized into behavior-based and physiological signal-based approaches. The behavior-based approach captures various body activities, including facial expressions, body posture, gestures, and voice tone. The physiological signal-based approach collects physiological signals, including Electroencephalography (EEG), Electrocardiography (ECG), Galvanic Skin Response (GSR), Heart Rate Variability (HRV), Respiration Rate (RR), Skin Temperature, and Electromyogram (EMG). In both methods, the collected data is processed using a machine learning pipeline that performs data pre-processing, feature extraction, training, and validating to make predictions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib11" title="">11</a>]</cite>. The predicted targets are binary classification tasks derived from 2D or 3D models of emotion. These classifiers split emotion into different dimensions, e.g., arousal, valence, and dominance (if using 3 dimensions) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib12" title="">12</a>]</cite>. In the past decade, multiple studies have explored the feasibility of such automated emotion recognition approaches, and several datasets have been published, including ASCERTAIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib13" title="">13</a>]</cite>, MAHNOB-HCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib14" title="">14</a>]</cite>, etc.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="319" id="S1.F1.g1" src="extracted/5844054/figs/framework.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The proposed APEX framework.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recent research has proposed using additional contextual and psychological factors, including personality, in automated emotion recognition. The relationship between personality and effect is first proposed in Eysenckâ€™s personality model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib15" title="">15</a>]</cite> and further validated. By using functional magnetic resonance imaging (fMRI), the blood oxygenation level dependent (BOLD) signal was examined to indicate arousal and valence level which further proved the correlation between personality and emotion responses. Different databases have been released for personality-involved automated emotion recognition, including ASCERTAIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib13" title="">13</a>]</cite>. These data sets used emotional videos as stimuli and captured physiological signals while subjects were watching videos. Also, the big-five traits for personality were used to evaluate subjectsâ€™ personalities. Subramanian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib13" title="">13</a>]</cite> deployed traditional machine learning algorithms including support vector machine (SVM) and naive Bayes (NB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib16" title="">16</a>]</cite>, to classify emotions and obtained averaged f1-score of 0.7 for Valence and 0.655 for Arousal. Shao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib17" title="">17</a>]</cite> proposed a hypergraph neural network where both a modality and a personality derive hypergraphs, and the vertices in the hypergraphs are subject-video pairs. The hypergraphs output to a fully-connected layer to yield the final prediction. Results on ASCERTAIN dataset showed 80.57% accuracy in classifying high/low valence and 73.67% accuracy in classifying high/low arousal. Tian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib18" title="">18</a>]</cite> first cluster subjects of ASCERTAIN dataset in different groups based on their personality using K-means and applied deep neural network to do classification. However, the former studies have treated personality too lightly or arbitrarily. This study proposes a novel framework that embeds the personality attention mechanism from natural language processing to ensemble learning, specifically, bagging. A personality score is calculated by the inner product of two 5D personality traits. It is used as the reweighing factor of different weak classifiers to yield a more personalized and adapted prediction for the specific subject.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The main contributions of this paper can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a scoring system to calculate the similarity between the personalities of two subjects.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A novel framework that embeds the personality score to ensemble learning is proposed.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conducted experiments on the ASCERTAIN dataset to verify the feasibility of the proposed framework. Results showed that our proposed method overperforms existing studies.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The remainder of this paper is organized as follows. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S2" title="II Method â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">II</span></a> introduces the dataset used in the study and provides a detailed description of the proposed approaches. In section <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S3" title="III Experiments and Results â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">III</span></a>, we present the experiments conducted and the evaluation results. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S4" title="IV Discussion â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">IV</span></a> discusses this studyâ€™s merits, potential, weaknesses, and future directions. Lastly, Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S5" title="V Conclusion â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">V</span></a> concludes this study and summarizes the key learnings.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Method</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The pipeline of the proposed framework is presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S1.F1" title="Figure 1 â€£ I Introduction â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">1</span></a>. It contains two parts: the bagging part and the personality attention part.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Dataset</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We used ASCERTAIN dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib13" title="">13</a>]</cite> to validate the feasibility of this study. ASCERTAIN dataset is a multimodal dataset for emotion recognition that uses physiological sensors. Fifty-eight subjects were involved in the study and were tested for their big-five personality traits. Subjects were instructed to report their emotional self-ratings of â€Arousal,â€ â€Valence,â€ â€Engagement,â€ â€Liking,â€ and â€Familiarityâ€ after watching 36 videos that evoke emotion, such as â€The Shining.â€ Physiological signals were recorded while the subjects watched the videos. The recorded signals included EEG, ECG, GSR, and facial activity. In this study, we adopted the 2D emotion model, which has two dimensions â€Arousalâ€ and â€Valence.â€ Thus, we designate two classification tasks: arousal and valence. In addition, we adopted two physiological signals, ECG and GSR, because these two signals reveal critical information and can be deployed on wearable devices.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">APEX framework</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S1.F1" title="Figure 1 â€£ I Introduction â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">1</span></a> shows the APEX framework, which consists of Bagging and Personalized attention. The critical assumption of this framework is that personality and emotional response are correlated; hence participants with similar personalities evoke similar emotional responses. Thus, when doing emotion recognition for a particular participant, we want the training data with a similar personality to contribute more to the classification and reduce the weights of data with less personality similarity.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.10">To do so, we treat each participant as an independent training sub-set. Assume the total number of participants in the training set is <math alttext="\mathbf{N}" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\mathbf{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">bold_N</annotation></semantics></math>, then <math alttext="\mathbf{N}" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">\mathbf{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.1d">bold_N</annotation></semantics></math> training sub-sets are used. Each training sub-set <math alttext="\mathbf{S_{i}}" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m3.1"><semantics id="S2.SS2.p2.3.m3.1a"><msub id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml"><mi id="S2.SS2.p2.3.m3.1.1.2" xref="S2.SS2.p2.3.m3.1.1.2.cmml">ğ’</mi><mi id="S2.SS2.p2.3.m3.1.1.3" xref="S2.SS2.p2.3.m3.1.1.3.cmml">ğ¢</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><apply id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.3.m3.1.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p2.3.m3.1.1.2.cmml" xref="S2.SS2.p2.3.m3.1.1.2">ğ’</ci><ci id="S2.SS2.p2.3.m3.1.1.3.cmml" xref="S2.SS2.p2.3.m3.1.1.3">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">\mathbf{S_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.m3.1d">bold_S start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT</annotation></semantics></math> is used to train a weak classifier and in total, there are <math alttext="\mathbf{N}" class="ltx_Math" display="inline" id="S2.SS2.p2.4.m4.1"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">\mathbf{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.4.m4.1d">bold_N</annotation></semantics></math> weak classifiers, where <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p2.5.m5.1"><semantics id="S2.SS2.p2.5.m5.1a"><mi id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><ci id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.5.m5.1d">italic_i</annotation></semantics></math> is the index for a training sub-set. We use decision trees as the weak classifier in this study, but this choice is highly flexible. After all the weak classifiers are trained, they can be used to infer a new subjectâ€™s data <math alttext="\mathbf{S_{x}}" class="ltx_Math" display="inline" id="S2.SS2.p2.6.m6.1"><semantics id="S2.SS2.p2.6.m6.1a"><msub id="S2.SS2.p2.6.m6.1.1" xref="S2.SS2.p2.6.m6.1.1.cmml"><mi id="S2.SS2.p2.6.m6.1.1.2" xref="S2.SS2.p2.6.m6.1.1.2.cmml">ğ’</mi><mi id="S2.SS2.p2.6.m6.1.1.3" xref="S2.SS2.p2.6.m6.1.1.3.cmml">ğ±</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m6.1b"><apply id="S2.SS2.p2.6.m6.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.6.m6.1.1.1.cmml" xref="S2.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p2.6.m6.1.1.2.cmml" xref="S2.SS2.p2.6.m6.1.1.2">ğ’</ci><ci id="S2.SS2.p2.6.m6.1.1.3.cmml" xref="S2.SS2.p2.6.m6.1.1.3">ğ±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m6.1c">\mathbf{S_{x}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.6.m6.1d">bold_S start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT</annotation></semantics></math>. This will yield <math alttext="\mathbf{N}" class="ltx_Math" display="inline" id="S2.SS2.p2.7.m7.1"><semantics id="S2.SS2.p2.7.m7.1a"><mi id="S2.SS2.p2.7.m7.1.1" xref="S2.SS2.p2.7.m7.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m7.1b"><ci id="S2.SS2.p2.7.m7.1.1.cmml" xref="S2.SS2.p2.7.m7.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m7.1c">\mathbf{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.7.m7.1d">bold_N</annotation></semantics></math> classification results <math alttext="\mathbf{I_{i}}" class="ltx_Math" display="inline" id="S2.SS2.p2.8.m8.1"><semantics id="S2.SS2.p2.8.m8.1a"><msub id="S2.SS2.p2.8.m8.1.1" xref="S2.SS2.p2.8.m8.1.1.cmml"><mi id="S2.SS2.p2.8.m8.1.1.2" xref="S2.SS2.p2.8.m8.1.1.2.cmml">ğˆ</mi><mi id="S2.SS2.p2.8.m8.1.1.3" xref="S2.SS2.p2.8.m8.1.1.3.cmml">ğ¢</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m8.1b"><apply id="S2.SS2.p2.8.m8.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.8.m8.1.1.1.cmml" xref="S2.SS2.p2.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.p2.8.m8.1.1.2.cmml" xref="S2.SS2.p2.8.m8.1.1.2">ğˆ</ci><ci id="S2.SS2.p2.8.m8.1.1.3.cmml" xref="S2.SS2.p2.8.m8.1.1.3">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m8.1c">\mathbf{I_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.8.m8.1d">bold_I start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT</annotation></semantics></math> in probability form, i.e., a possibility number ranging in <math alttext="[0,1]" class="ltx_Math" display="inline" id="S2.SS2.p2.9.m9.2"><semantics id="S2.SS2.p2.9.m9.2a"><mrow id="S2.SS2.p2.9.m9.2.3.2" xref="S2.SS2.p2.9.m9.2.3.1.cmml"><mo id="S2.SS2.p2.9.m9.2.3.2.1" stretchy="false" xref="S2.SS2.p2.9.m9.2.3.1.cmml">[</mo><mn id="S2.SS2.p2.9.m9.1.1" xref="S2.SS2.p2.9.m9.1.1.cmml">0</mn><mo id="S2.SS2.p2.9.m9.2.3.2.2" xref="S2.SS2.p2.9.m9.2.3.1.cmml">,</mo><mn id="S2.SS2.p2.9.m9.2.2" xref="S2.SS2.p2.9.m9.2.2.cmml">1</mn><mo id="S2.SS2.p2.9.m9.2.3.2.3" stretchy="false" xref="S2.SS2.p2.9.m9.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m9.2b"><interval closure="closed" id="S2.SS2.p2.9.m9.2.3.1.cmml" xref="S2.SS2.p2.9.m9.2.3.2"><cn id="S2.SS2.p2.9.m9.1.1.cmml" type="integer" xref="S2.SS2.p2.9.m9.1.1">0</cn><cn id="S2.SS2.p2.9.m9.2.2.cmml" type="integer" xref="S2.SS2.p2.9.m9.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.9.m9.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.9.m9.2d">[ 0 , 1 ]</annotation></semantics></math>, where <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p2.10.m10.1"><semantics id="S2.SS2.p2.10.m10.1a"><mi id="S2.SS2.p2.10.m10.1.1" xref="S2.SS2.p2.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m10.1b"><ci id="S2.SS2.p2.10.m10.1.1.cmml" xref="S2.SS2.p2.10.m10.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.10.m10.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.10.m10.1d">italic_i</annotation></semantics></math> is the index of the weak classifier.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.6">On the other hand, each training sub-set has five-dimensional personality traits, represented as a five-dimensional data vector <math alttext="p_{i}" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><msub id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml"><mi id="S2.SS2.p3.1.m1.1.1.2" xref="S2.SS2.p3.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS2.p3.1.m1.1.1.3" xref="S2.SS2.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.1.m1.1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p3.1.m1.1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.1.2">ğ‘</ci><ci id="S2.SS2.p3.1.m1.1.1.3.cmml" xref="S2.SS2.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the test subject also has personality traits <math alttext="p_{x}" class="ltx_Math" display="inline" id="S2.SS2.p3.2.m2.1"><semantics id="S2.SS2.p3.2.m2.1a"><msub id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">p</mi><mi id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">ğ‘</ci><ci id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">p_{x}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT</annotation></semantics></math>. To calculate the score, we use the inner product of between each training sub-set <math alttext="p_{i}" class="ltx_Math" display="inline" id="S2.SS2.p3.3.m3.1"><semantics id="S2.SS2.p3.3.m3.1a"><msub id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">p</mi><mi id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">ğ‘</ci><ci id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the testing subject <math alttext="p_{x}" class="ltx_Math" display="inline" id="S2.SS2.p3.4.m4.1"><semantics id="S2.SS2.p3.4.m4.1a"><msub id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml"><mi id="S2.SS2.p3.4.m4.1.1.2" xref="S2.SS2.p3.4.m4.1.1.2.cmml">p</mi><mi id="S2.SS2.p3.4.m4.1.1.3" xref="S2.SS2.p3.4.m4.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><apply id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.4.m4.1.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p3.4.m4.1.1.2.cmml" xref="S2.SS2.p3.4.m4.1.1.2">ğ‘</ci><ci id="S2.SS2.p3.4.m4.1.1.3.cmml" xref="S2.SS2.p3.4.m4.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">p_{x}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.4.m4.1d">italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT</annotation></semantics></math> to yield <math alttext="\mathbf{N}" class="ltx_Math" display="inline" id="S2.SS2.p3.5.m5.1"><semantics id="S2.SS2.p3.5.m5.1a"><mi id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><ci id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">\mathbf{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.5.m5.1d">bold_N</annotation></semantics></math> products <math alttext="product_{i}" class="ltx_Math" display="inline" id="S2.SS2.p3.6.m6.1"><semantics id="S2.SS2.p3.6.m6.1a"><mrow id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml"><mi id="S2.SS2.p3.6.m6.1.1.2" xref="S2.SS2.p3.6.m6.1.1.2.cmml">p</mi><mo id="S2.SS2.p3.6.m6.1.1.1" xref="S2.SS2.p3.6.m6.1.1.1.cmml">â¢</mo><mi id="S2.SS2.p3.6.m6.1.1.3" xref="S2.SS2.p3.6.m6.1.1.3.cmml">r</mi><mo id="S2.SS2.p3.6.m6.1.1.1a" xref="S2.SS2.p3.6.m6.1.1.1.cmml">â¢</mo><mi id="S2.SS2.p3.6.m6.1.1.4" xref="S2.SS2.p3.6.m6.1.1.4.cmml">o</mi><mo id="S2.SS2.p3.6.m6.1.1.1b" xref="S2.SS2.p3.6.m6.1.1.1.cmml">â¢</mo><mi id="S2.SS2.p3.6.m6.1.1.5" xref="S2.SS2.p3.6.m6.1.1.5.cmml">d</mi><mo id="S2.SS2.p3.6.m6.1.1.1c" xref="S2.SS2.p3.6.m6.1.1.1.cmml">â¢</mo><mi id="S2.SS2.p3.6.m6.1.1.6" xref="S2.SS2.p3.6.m6.1.1.6.cmml">u</mi><mo id="S2.SS2.p3.6.m6.1.1.1d" xref="S2.SS2.p3.6.m6.1.1.1.cmml">â¢</mo><mi id="S2.SS2.p3.6.m6.1.1.7" xref="S2.SS2.p3.6.m6.1.1.7.cmml">c</mi><mo id="S2.SS2.p3.6.m6.1.1.1e" xref="S2.SS2.p3.6.m6.1.1.1.cmml">â¢</mo><msub id="S2.SS2.p3.6.m6.1.1.8" xref="S2.SS2.p3.6.m6.1.1.8.cmml"><mi id="S2.SS2.p3.6.m6.1.1.8.2" xref="S2.SS2.p3.6.m6.1.1.8.2.cmml">t</mi><mi id="S2.SS2.p3.6.m6.1.1.8.3" xref="S2.SS2.p3.6.m6.1.1.8.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><apply id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1"><times id="S2.SS2.p3.6.m6.1.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1.1"></times><ci id="S2.SS2.p3.6.m6.1.1.2.cmml" xref="S2.SS2.p3.6.m6.1.1.2">ğ‘</ci><ci id="S2.SS2.p3.6.m6.1.1.3.cmml" xref="S2.SS2.p3.6.m6.1.1.3">ğ‘Ÿ</ci><ci id="S2.SS2.p3.6.m6.1.1.4.cmml" xref="S2.SS2.p3.6.m6.1.1.4">ğ‘œ</ci><ci id="S2.SS2.p3.6.m6.1.1.5.cmml" xref="S2.SS2.p3.6.m6.1.1.5">ğ‘‘</ci><ci id="S2.SS2.p3.6.m6.1.1.6.cmml" xref="S2.SS2.p3.6.m6.1.1.6">ğ‘¢</ci><ci id="S2.SS2.p3.6.m6.1.1.7.cmml" xref="S2.SS2.p3.6.m6.1.1.7">ğ‘</ci><apply id="S2.SS2.p3.6.m6.1.1.8.cmml" xref="S2.SS2.p3.6.m6.1.1.8"><csymbol cd="ambiguous" id="S2.SS2.p3.6.m6.1.1.8.1.cmml" xref="S2.SS2.p3.6.m6.1.1.8">subscript</csymbol><ci id="S2.SS2.p3.6.m6.1.1.8.2.cmml" xref="S2.SS2.p3.6.m6.1.1.8.2">ğ‘¡</ci><ci id="S2.SS2.p3.6.m6.1.1.8.3.cmml" xref="S2.SS2.p3.6.m6.1.1.8.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">product_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.6.m6.1d">italic_p italic_r italic_o italic_d italic_u italic_c italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="{product}_{i}=\langle\mathbf{p}_{i}\;,\;\mathbf{p}_{x}\rangle" class="ltx_Math" display="block" id="S2.E1.m1.2"><semantics id="S2.E1.m1.2a"><mrow id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.2.2.4" xref="S2.E1.m1.2.2.4.cmml"><mi id="S2.E1.m1.2.2.4.2" xref="S2.E1.m1.2.2.4.2.cmml">p</mi><mo id="S2.E1.m1.2.2.4.1" xref="S2.E1.m1.2.2.4.1.cmml">â¢</mo><mi id="S2.E1.m1.2.2.4.3" xref="S2.E1.m1.2.2.4.3.cmml">r</mi><mo id="S2.E1.m1.2.2.4.1a" xref="S2.E1.m1.2.2.4.1.cmml">â¢</mo><mi id="S2.E1.m1.2.2.4.4" xref="S2.E1.m1.2.2.4.4.cmml">o</mi><mo id="S2.E1.m1.2.2.4.1b" xref="S2.E1.m1.2.2.4.1.cmml">â¢</mo><mi id="S2.E1.m1.2.2.4.5" xref="S2.E1.m1.2.2.4.5.cmml">d</mi><mo id="S2.E1.m1.2.2.4.1c" xref="S2.E1.m1.2.2.4.1.cmml">â¢</mo><mi id="S2.E1.m1.2.2.4.6" xref="S2.E1.m1.2.2.4.6.cmml">u</mi><mo id="S2.E1.m1.2.2.4.1d" xref="S2.E1.m1.2.2.4.1.cmml">â¢</mo><mi id="S2.E1.m1.2.2.4.7" xref="S2.E1.m1.2.2.4.7.cmml">c</mi><mo id="S2.E1.m1.2.2.4.1e" xref="S2.E1.m1.2.2.4.1.cmml">â¢</mo><msub id="S2.E1.m1.2.2.4.8" xref="S2.E1.m1.2.2.4.8.cmml"><mi id="S2.E1.m1.2.2.4.8.2" xref="S2.E1.m1.2.2.4.8.2.cmml">t</mi><mi id="S2.E1.m1.2.2.4.8.3" xref="S2.E1.m1.2.2.4.8.3.cmml">i</mi></msub></mrow><mo id="S2.E1.m1.2.2.3" xref="S2.E1.m1.2.2.3.cmml">=</mo><mrow id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.3.cmml"><mo id="S2.E1.m1.2.2.2.2.3" stretchy="false" xref="S2.E1.m1.2.2.2.3.cmml">âŸ¨</mo><msub id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.2.cmml">ğ©</mi><mi id="S2.E1.m1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.2.2.2.2.4" rspace="0.447em" xref="S2.E1.m1.2.2.2.3.cmml">,</mo><msub id="S2.E1.m1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.cmml"><mi id="S2.E1.m1.2.2.2.2.2.2" xref="S2.E1.m1.2.2.2.2.2.2.cmml">ğ©</mi><mi id="S2.E1.m1.2.2.2.2.2.3" xref="S2.E1.m1.2.2.2.2.2.3.cmml">x</mi></msub><mo id="S2.E1.m1.2.2.2.2.5" stretchy="false" xref="S2.E1.m1.2.2.2.3.cmml">âŸ©</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.2b"><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><eq id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2.3"></eq><apply id="S2.E1.m1.2.2.4.cmml" xref="S2.E1.m1.2.2.4"><times id="S2.E1.m1.2.2.4.1.cmml" xref="S2.E1.m1.2.2.4.1"></times><ci id="S2.E1.m1.2.2.4.2.cmml" xref="S2.E1.m1.2.2.4.2">ğ‘</ci><ci id="S2.E1.m1.2.2.4.3.cmml" xref="S2.E1.m1.2.2.4.3">ğ‘Ÿ</ci><ci id="S2.E1.m1.2.2.4.4.cmml" xref="S2.E1.m1.2.2.4.4">ğ‘œ</ci><ci id="S2.E1.m1.2.2.4.5.cmml" xref="S2.E1.m1.2.2.4.5">ğ‘‘</ci><ci id="S2.E1.m1.2.2.4.6.cmml" xref="S2.E1.m1.2.2.4.6">ğ‘¢</ci><ci id="S2.E1.m1.2.2.4.7.cmml" xref="S2.E1.m1.2.2.4.7">ğ‘</ci><apply id="S2.E1.m1.2.2.4.8.cmml" xref="S2.E1.m1.2.2.4.8"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.4.8.1.cmml" xref="S2.E1.m1.2.2.4.8">subscript</csymbol><ci id="S2.E1.m1.2.2.4.8.2.cmml" xref="S2.E1.m1.2.2.4.8.2">ğ‘¡</ci><ci id="S2.E1.m1.2.2.4.8.3.cmml" xref="S2.E1.m1.2.2.4.8.3">ğ‘–</ci></apply></apply><list id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2"><apply id="S2.E1.m1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.2">ğ©</ci><ci id="S2.E1.m1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.E1.m1.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2.2.2">ğ©</ci><ci id="S2.E1.m1.2.2.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2.2.3">ğ‘¥</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.2c">{product}_{i}=\langle\mathbf{p}_{i}\;,\;\mathbf{p}_{x}\rangle</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.2d">italic_p italic_r italic_o italic_d italic_u italic_c italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = âŸ¨ bold_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT âŸ©</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.3">All products are pushed to normalization to re-scale to <math alttext="[0,1]" class="ltx_Math" display="inline" id="S2.SS2.p4.1.m1.2"><semantics id="S2.SS2.p4.1.m1.2a"><mrow id="S2.SS2.p4.1.m1.2.3.2" xref="S2.SS2.p4.1.m1.2.3.1.cmml"><mo id="S2.SS2.p4.1.m1.2.3.2.1" stretchy="false" xref="S2.SS2.p4.1.m1.2.3.1.cmml">[</mo><mn id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml">0</mn><mo id="S2.SS2.p4.1.m1.2.3.2.2" xref="S2.SS2.p4.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS2.p4.1.m1.2.2" xref="S2.SS2.p4.1.m1.2.2.cmml">1</mn><mo id="S2.SS2.p4.1.m1.2.3.2.3" stretchy="false" xref="S2.SS2.p4.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.2b"><interval closure="closed" id="S2.SS2.p4.1.m1.2.3.1.cmml" xref="S2.SS2.p4.1.m1.2.3.2"><cn id="S2.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S2.SS2.p4.1.m1.1.1">0</cn><cn id="S2.SS2.p4.1.m1.2.2.cmml" type="integer" xref="S2.SS2.p4.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>. Then, they were transformed to probability scores <math alttext="\mathbf{score}_{i}" class="ltx_Math" display="inline" id="S2.SS2.p4.2.m2.1"><semantics id="S2.SS2.p4.2.m2.1a"><msub id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml"><mi id="S2.SS2.p4.2.m2.1.1.2" xref="S2.SS2.p4.2.m2.1.1.2.cmml">ğ¬ğœğ¨ğ«ğ</mi><mi id="S2.SS2.p4.2.m2.1.1.3" xref="S2.SS2.p4.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><apply id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.2.m2.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p4.2.m2.1.1.2.cmml" xref="S2.SS2.p4.2.m2.1.1.2">ğ¬ğœğ¨ğ«ğ</ci><ci id="S2.SS2.p4.2.m2.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">\mathbf{score}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.2.m2.1d">bold_score start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> by being imported to a softmax function. After softmax function, the sum of <math alttext="\mathbf{score}_{i}" class="ltx_Math" display="inline" id="S2.SS2.p4.3.m3.1"><semantics id="S2.SS2.p4.3.m3.1a"><msub id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml"><mi id="S2.SS2.p4.3.m3.1.1.2" xref="S2.SS2.p4.3.m3.1.1.2.cmml">ğ¬ğœğ¨ğ«ğ</mi><mi id="S2.SS2.p4.3.m3.1.1.3" xref="S2.SS2.p4.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.1b"><apply id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.3.m3.1.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p4.3.m3.1.1.2.cmml" xref="S2.SS2.p4.3.m3.1.1.2">ğ¬ğœğ¨ğ«ğ</ci><ci id="S2.SS2.p4.3.m3.1.1.3.cmml" xref="S2.SS2.p4.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.1c">\mathbf{score}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.3.m3.1d">bold_score start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is 1. The scores are calculated as:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle score_{i}=\mathrm{softmax}(product_{i})" class="ltx_Math" display="inline" id="S2.E2X.2.1.1.m1.1"><semantics id="S2.E2X.2.1.1.m1.1a"><mrow id="S2.E2X.2.1.1.m1.1.1" xref="S2.E2X.2.1.1.m1.1.1.cmml"><mrow id="S2.E2X.2.1.1.m1.1.1.3" xref="S2.E2X.2.1.1.m1.1.1.3.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.3.2" xref="S2.E2X.2.1.1.m1.1.1.3.2.cmml">s</mi><mo id="S2.E2X.2.1.1.m1.1.1.3.1" xref="S2.E2X.2.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.3.3" xref="S2.E2X.2.1.1.m1.1.1.3.3.cmml">c</mi><mo id="S2.E2X.2.1.1.m1.1.1.3.1a" xref="S2.E2X.2.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.3.4" xref="S2.E2X.2.1.1.m1.1.1.3.4.cmml">o</mi><mo id="S2.E2X.2.1.1.m1.1.1.3.1b" xref="S2.E2X.2.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.3.5" xref="S2.E2X.2.1.1.m1.1.1.3.5.cmml">r</mi><mo id="S2.E2X.2.1.1.m1.1.1.3.1c" xref="S2.E2X.2.1.1.m1.1.1.3.1.cmml">â¢</mo><msub id="S2.E2X.2.1.1.m1.1.1.3.6" xref="S2.E2X.2.1.1.m1.1.1.3.6.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.3.6.2" xref="S2.E2X.2.1.1.m1.1.1.3.6.2.cmml">e</mi><mi id="S2.E2X.2.1.1.m1.1.1.3.6.3" xref="S2.E2X.2.1.1.m1.1.1.3.6.3.cmml">i</mi></msub></mrow><mo id="S2.E2X.2.1.1.m1.1.1.2" xref="S2.E2X.2.1.1.m1.1.1.2.cmml">=</mo><mrow id="S2.E2X.2.1.1.m1.1.1.1" xref="S2.E2X.2.1.1.m1.1.1.1.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.1.3" xref="S2.E2X.2.1.1.m1.1.1.1.3.cmml">softmax</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.2" xref="S2.E2X.2.1.1.m1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2X.2.1.1.m1.1.1.1.1.1" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.cmml"><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2X.2.1.1.m1.1.1.1.1.1.1" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.2" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.2.cmml">p</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.3" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.3.cmml">r</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1a" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.4" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.4.cmml">o</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1b" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.5" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.5.cmml">d</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1c" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.6" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.6.cmml">u</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1d" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.7" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.7.cmml">c</mi><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1e" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml">â¢</mo><msub id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.2" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.2.cmml">t</mi><mi id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.3" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.3.cmml">i</mi></msub></mrow><mo id="S2.E2X.2.1.1.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2X.2.1.1.m1.1b"><apply id="S2.E2X.2.1.1.m1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1"><eq id="S2.E2X.2.1.1.m1.1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.2"></eq><apply id="S2.E2X.2.1.1.m1.1.1.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.3"><times id="S2.E2X.2.1.1.m1.1.1.3.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.1"></times><ci id="S2.E2X.2.1.1.m1.1.1.3.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.2">ğ‘ </ci><ci id="S2.E2X.2.1.1.m1.1.1.3.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.3">ğ‘</ci><ci id="S2.E2X.2.1.1.m1.1.1.3.4.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.4">ğ‘œ</ci><ci id="S2.E2X.2.1.1.m1.1.1.3.5.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.5">ğ‘Ÿ</ci><apply id="S2.E2X.2.1.1.m1.1.1.3.6.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.6"><csymbol cd="ambiguous" id="S2.E2X.2.1.1.m1.1.1.3.6.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.6">subscript</csymbol><ci id="S2.E2X.2.1.1.m1.1.1.3.6.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.6.2">ğ‘’</ci><ci id="S2.E2X.2.1.1.m1.1.1.3.6.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.3.6.3">ğ‘–</ci></apply></apply><apply id="S2.E2X.2.1.1.m1.1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1"><times id="S2.E2X.2.1.1.m1.1.1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.2"></times><ci id="S2.E2X.2.1.1.m1.1.1.1.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.3">softmax</ci><apply id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1"><times id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.1"></times><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.3">ğ‘Ÿ</ci><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.4.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.4">ğ‘œ</ci><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.5.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.5">ğ‘‘</ci><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.6.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.6">ğ‘¢</ci><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.7.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.7">ğ‘</ci><apply id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8"><csymbol cd="ambiguous" id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8">subscript</csymbol><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.2">ğ‘¡</ci><ci id="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.1.1.1.8.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2X.2.1.1.m1.1c">\displaystyle score_{i}=\mathrm{softmax}(product_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.E2X.2.1.1.m1.1d">italic_s italic_c italic_o italic_r italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_softmax ( italic_p italic_r italic_o italic_d italic_u italic_c italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E2Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle=\frac{\exp(product_{i})}{\sum_{j=1}^{m}\exp(product_{j})}\in%
\mathbb{R}." class="ltx_Math" display="inline" id="S2.E2Xa.2.1.1.m1.5"><semantics id="S2.E2Xa.2.1.1.m1.5a"><mrow id="S2.E2Xa.2.1.1.m1.5.5.1" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.cmml"><mrow id="S2.E2Xa.2.1.1.m1.5.5.1.1" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.cmml"><mi id="S2.E2Xa.2.1.1.m1.5.5.1.1.2" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.2.cmml"></mi><mo id="S2.E2Xa.2.1.1.m1.5.5.1.1.3" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.3.cmml">=</mo><mstyle displaystyle="true" id="S2.E2Xa.2.1.1.m1.4.4" xref="S2.E2Xa.2.1.1.m1.4.4.cmml"><mfrac id="S2.E2Xa.2.1.1.m1.4.4a" xref="S2.E2Xa.2.1.1.m1.4.4.cmml"><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.1.1.1.1" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.cmml">exp</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2a" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">â¡</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml"><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.2" stretchy="false" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.cmml">p</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.cmml">r</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1a" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.4" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.4.cmml">o</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1b" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.5" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.5.cmml">d</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1c" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.6" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.6.cmml">u</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1d" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.7" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.7.cmml">c</mi><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1e" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml">â¢</mo><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.2.cmml">t</mi><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.3.cmml">i</mi></msub></mrow><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.3" stretchy="false" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S2.E2Xa.2.1.1.m1.4.4.4" xref="S2.E2Xa.2.1.1.m1.4.4.4.cmml"><msubsup id="S2.E2Xa.2.1.1.m1.4.4.4.3" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.cmml"><mo id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.2" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.2.cmml">âˆ‘</mo><mrow id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.2" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.2.cmml">j</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.1" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.1.cmml">=</mo><mn id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.3" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.3.cmml">1</mn></mrow><mi id="S2.E2Xa.2.1.1.m1.4.4.4.3.3" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.3.cmml">m</mi></msubsup><mrow id="S2.E2Xa.2.1.1.m1.4.4.4.2.1" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.3.3.3.1" xref="S2.E2Xa.2.1.1.m1.3.3.3.1.cmml">exp</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1a" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.2.cmml">â¡</mo><mrow id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.2.cmml"><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.2" stretchy="false" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.2.cmml">(</mo><mrow id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.cmml"><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.2" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.2.cmml">p</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.3" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.3.cmml">r</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1a" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.4" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.4.cmml">o</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1b" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.5" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.5.cmml">d</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1c" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.6" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.6.cmml">u</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1d" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml">â¢</mo><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.7" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.7.cmml">c</mi><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1e" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml">â¢</mo><msub id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.cmml"><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.2" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.2.cmml">t</mi><mi id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.3" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.3.cmml">j</mi></msub></mrow><mo id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.3" stretchy="false" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle><mo id="S2.E2Xa.2.1.1.m1.5.5.1.1.4" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.4.cmml">âˆˆ</mo><mi id="S2.E2Xa.2.1.1.m1.5.5.1.1.5" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.5.cmml">â„</mi></mrow><mo id="S2.E2Xa.2.1.1.m1.5.5.1.2" lspace="0em" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2Xa.2.1.1.m1.5b"><apply id="S2.E2Xa.2.1.1.m1.5.5.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1"><and id="S2.E2Xa.2.1.1.m1.5.5.1.1a.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1"></and><apply id="S2.E2Xa.2.1.1.m1.5.5.1.1b.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1"><eq id="S2.E2Xa.2.1.1.m1.5.5.1.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.3"></eq><csymbol cd="latexml" id="S2.E2Xa.2.1.1.m1.5.5.1.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.2">absent</csymbol><apply id="S2.E2Xa.2.1.1.m1.4.4.cmml" xref="S2.E2Xa.2.1.1.m1.4.4"><divide id="S2.E2Xa.2.1.1.m1.4.4.5.cmml" xref="S2.E2Xa.2.1.1.m1.4.4"></divide><apply id="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2"><exp id="S2.E2Xa.2.1.1.m1.1.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1"></exp><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1"><times id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.1"></times><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.2">ğ‘</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.3">ğ‘Ÿ</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.4.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.4">ğ‘œ</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.5.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.5">ğ‘‘</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.6.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.6">ğ‘¢</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.7.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.7">ğ‘</ci><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.2">ğ‘¡</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.1.1.8.3">ğ‘–</ci></apply></apply></apply><apply id="S2.E2Xa.2.1.1.m1.4.4.4.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4"><apply id="S2.E2Xa.2.1.1.m1.4.4.4.3.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.4.4.4.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3">superscript</csymbol><apply id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3">subscript</csymbol><sum id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.2"></sum><apply id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3"><eq id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.1"></eq><ci id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.2">ğ‘—</ci><cn id="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.3.cmml" type="integer" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.2.3.3">1</cn></apply></apply><ci id="S2.E2Xa.2.1.1.m1.4.4.4.3.3.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.3.3">ğ‘š</ci></apply><apply id="S2.E2Xa.2.1.1.m1.4.4.4.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1"><exp id="S2.E2Xa.2.1.1.m1.3.3.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.3.3.3.1"></exp><apply id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1"><times id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.1"></times><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.2">ğ‘</ci><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.3">ğ‘Ÿ</ci><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.4.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.4">ğ‘œ</ci><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.5.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.5">ğ‘‘</ci><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.6.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.6">ğ‘¢</ci><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.7.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.7">ğ‘</ci><apply id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.1.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.2.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.2">ğ‘¡</ci><ci id="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.3.cmml" xref="S2.E2Xa.2.1.1.m1.4.4.4.2.1.1.1.8.3">ğ‘—</ci></apply></apply></apply></apply></apply></apply><apply id="S2.E2Xa.2.1.1.m1.5.5.1.1c.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1"><in id="S2.E2Xa.2.1.1.m1.5.5.1.1.4.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.4"></in><share href="https://arxiv.org/html/2409.06118v1#S2.E2Xa.2.1.1.m1.4.4.cmml" id="S2.E2Xa.2.1.1.m1.5.5.1.1d.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1"></share><ci id="S2.E2Xa.2.1.1.m1.5.5.1.1.5.cmml" xref="S2.E2Xa.2.1.1.m1.5.5.1.1.5">â„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2Xa.2.1.1.m1.5c">\displaystyle=\frac{\exp(product_{i})}{\sum_{j=1}^{m}\exp(product_{j})}\in%
\mathbb{R}.</annotation><annotation encoding="application/x-llamapun" id="S2.E2Xa.2.1.1.m1.5d">= divide start_ARG roman_exp ( italic_p italic_r italic_o italic_d italic_u italic_c italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT roman_exp ( italic_p italic_r italic_o italic_d italic_u italic_c italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) end_ARG âˆˆ blackboard_R .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison of APEX and Existing Studies Performance</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.1">
<tr class="ltx_tr" id="S2.T1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1">Study</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1">Task</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.4.1">Accuracy</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.1">AUC</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.2.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.2.1.1">Santamaria et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib19" title="">19</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.2.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.2.2.1">SVM/NB (no personality)</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.1.2.3">Arousal</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.2.4">65.5%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.1.2.5">0.70</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.3.1">Valence</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.2">70.3%</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.3.3">0.73</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.4.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.4.1.1">Shao et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib17" title="">17</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.4.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.4.2.1">Hypergraph Neural Network</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.4.3">Arousal</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.4.4">70.8%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.4.5">0.77</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5">
<td class="ltx_td ltx_align_left" id="S2.T1.1.5.1">Valence</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.2">72.9%</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.5.3">0.79</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.6.1" rowspan="2"><span class="ltx_text" id="S2.T1.1.6.1.1">Tian et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib18" title="">18</a>]</cite></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.6.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.6.2.1">K-mean and Deep Neural Networks</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.6.3">Arousal</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.6.4">71.1%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.6.5">0.73</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7">
<td class="ltx_td ltx_align_left" id="S2.T1.1.7.1">Valence</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.2">70.6%</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.7.3">0.73</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.1.8.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.8.1.1">APEX Framework</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.1.8.2" rowspan="2"><span class="ltx_text" id="S2.T1.1.8.2.1">Attention-based Bagging</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.8.3">Arousal</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.8.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.8.4.1">77.1%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.8.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.8.5.1">0.83</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9">
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T1.1.9.1">Valence</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.9.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.9.2.1">76.9%</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S2.T1.1.9.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.9.3.1">0.81</span></td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.2">To embed the personality attention to bagging model, the inferences are multiplied with scores one by one and summed up to yield the final prediction:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="FinalPrediction=\mathrm{BinaryStep}(\sum_{i=1}^{N}score_{i}\times I_{i})" class="ltx_Math" display="block" id="S2.E3.m1.1"><semantics id="S2.E3.m1.1a"><mrow id="S2.E3.m1.1.1" xref="S2.E3.m1.1.1.cmml"><mrow id="S2.E3.m1.1.1.3" xref="S2.E3.m1.1.1.3.cmml"><mi id="S2.E3.m1.1.1.3.2" xref="S2.E3.m1.1.1.3.2.cmml">F</mi><mo id="S2.E3.m1.1.1.3.1" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.3" xref="S2.E3.m1.1.1.3.3.cmml">i</mi><mo id="S2.E3.m1.1.1.3.1a" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.4" xref="S2.E3.m1.1.1.3.4.cmml">n</mi><mo id="S2.E3.m1.1.1.3.1b" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.5" xref="S2.E3.m1.1.1.3.5.cmml">a</mi><mo id="S2.E3.m1.1.1.3.1c" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.6" xref="S2.E3.m1.1.1.3.6.cmml">l</mi><mo id="S2.E3.m1.1.1.3.1d" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.7" xref="S2.E3.m1.1.1.3.7.cmml">P</mi><mo id="S2.E3.m1.1.1.3.1e" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.8" xref="S2.E3.m1.1.1.3.8.cmml">r</mi><mo id="S2.E3.m1.1.1.3.1f" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.9" xref="S2.E3.m1.1.1.3.9.cmml">e</mi><mo id="S2.E3.m1.1.1.3.1g" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.10" xref="S2.E3.m1.1.1.3.10.cmml">d</mi><mo id="S2.E3.m1.1.1.3.1h" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.11" xref="S2.E3.m1.1.1.3.11.cmml">i</mi><mo id="S2.E3.m1.1.1.3.1i" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.12" xref="S2.E3.m1.1.1.3.12.cmml">c</mi><mo id="S2.E3.m1.1.1.3.1j" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.13" xref="S2.E3.m1.1.1.3.13.cmml">t</mi><mo id="S2.E3.m1.1.1.3.1k" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.14" xref="S2.E3.m1.1.1.3.14.cmml">i</mi><mo id="S2.E3.m1.1.1.3.1l" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.15" xref="S2.E3.m1.1.1.3.15.cmml">o</mi><mo id="S2.E3.m1.1.1.3.1m" xref="S2.E3.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.3.16" xref="S2.E3.m1.1.1.3.16.cmml">n</mi></mrow><mo id="S2.E3.m1.1.1.2" xref="S2.E3.m1.1.1.2.cmml">=</mo><mrow id="S2.E3.m1.1.1.1" xref="S2.E3.m1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.3" xref="S2.E3.m1.1.1.1.3.cmml">BinaryStep</mi><mo id="S2.E3.m1.1.1.1.2" xref="S2.E3.m1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><munderover id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml"><mo id="S2.E3.m1.1.1.1.1.1.1.1.2.2" lspace="0em" movablelimits="false" xref="S2.E3.m1.1.1.1.1.1.1.1.2.2.cmml">âˆ‘</mo><mrow id="S2.E3.m1.1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1.2.3.1" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.1.cmml">=</mo><mn id="S2.E3.m1.1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow><mi id="S2.E3.m1.1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.1.3.cmml">N</mi></munderover><mrow id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml"><mrow id="S2.E3.m1.1.1.1.1.1.1.2.2" xref="S2.E3.m1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.2.2" xref="S2.E3.m1.1.1.1.1.1.1.2.2.2.cmml">s</mi><mo id="S2.E3.m1.1.1.1.1.1.1.2.2.1" xref="S2.E3.m1.1.1.1.1.1.1.2.2.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.2.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.2.3.cmml">c</mi><mo id="S2.E3.m1.1.1.1.1.1.1.2.2.1a" xref="S2.E3.m1.1.1.1.1.1.1.2.2.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.2.2.4" xref="S2.E3.m1.1.1.1.1.1.1.2.2.4.cmml">o</mi><mo id="S2.E3.m1.1.1.1.1.1.1.2.2.1b" xref="S2.E3.m1.1.1.1.1.1.1.2.2.1.cmml">â¢</mo><mi id="S2.E3.m1.1.1.1.1.1.1.2.2.5" xref="S2.E3.m1.1.1.1.1.1.1.2.2.5.cmml">r</mi><mo id="S2.E3.m1.1.1.1.1.1.1.2.2.1c" xref="S2.E3.m1.1.1.1.1.1.1.2.2.1.cmml">â¢</mo><msub id="S2.E3.m1.1.1.1.1.1.1.2.2.6" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.2.6.2" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6.2.cmml">e</mi><mi id="S2.E3.m1.1.1.1.1.1.1.2.2.6.3" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6.3.cmml">i</mi></msub></mrow><mo id="S2.E3.m1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E3.m1.1.1.1.1.1.1.2.1.cmml">Ã—</mo><msub id="S2.E3.m1.1.1.1.1.1.1.2.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.2" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml">I</mi><mi id="S2.E3.m1.1.1.1.1.1.1.2.3.3" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow></mrow><mo id="S2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.1b"><apply id="S2.E3.m1.1.1.cmml" xref="S2.E3.m1.1.1"><eq id="S2.E3.m1.1.1.2.cmml" xref="S2.E3.m1.1.1.2"></eq><apply id="S2.E3.m1.1.1.3.cmml" xref="S2.E3.m1.1.1.3"><times id="S2.E3.m1.1.1.3.1.cmml" xref="S2.E3.m1.1.1.3.1"></times><ci id="S2.E3.m1.1.1.3.2.cmml" xref="S2.E3.m1.1.1.3.2">ğ¹</ci><ci id="S2.E3.m1.1.1.3.3.cmml" xref="S2.E3.m1.1.1.3.3">ğ‘–</ci><ci id="S2.E3.m1.1.1.3.4.cmml" xref="S2.E3.m1.1.1.3.4">ğ‘›</ci><ci id="S2.E3.m1.1.1.3.5.cmml" xref="S2.E3.m1.1.1.3.5">ğ‘</ci><ci id="S2.E3.m1.1.1.3.6.cmml" xref="S2.E3.m1.1.1.3.6">ğ‘™</ci><ci id="S2.E3.m1.1.1.3.7.cmml" xref="S2.E3.m1.1.1.3.7">ğ‘ƒ</ci><ci id="S2.E3.m1.1.1.3.8.cmml" xref="S2.E3.m1.1.1.3.8">ğ‘Ÿ</ci><ci id="S2.E3.m1.1.1.3.9.cmml" xref="S2.E3.m1.1.1.3.9">ğ‘’</ci><ci id="S2.E3.m1.1.1.3.10.cmml" xref="S2.E3.m1.1.1.3.10">ğ‘‘</ci><ci id="S2.E3.m1.1.1.3.11.cmml" xref="S2.E3.m1.1.1.3.11">ğ‘–</ci><ci id="S2.E3.m1.1.1.3.12.cmml" xref="S2.E3.m1.1.1.3.12">ğ‘</ci><ci id="S2.E3.m1.1.1.3.13.cmml" xref="S2.E3.m1.1.1.3.13">ğ‘¡</ci><ci id="S2.E3.m1.1.1.3.14.cmml" xref="S2.E3.m1.1.1.3.14">ğ‘–</ci><ci id="S2.E3.m1.1.1.3.15.cmml" xref="S2.E3.m1.1.1.3.15">ğ‘œ</ci><ci id="S2.E3.m1.1.1.3.16.cmml" xref="S2.E3.m1.1.1.3.16">ğ‘›</ci></apply><apply id="S2.E3.m1.1.1.1.cmml" xref="S2.E3.m1.1.1.1"><times id="S2.E3.m1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.2"></times><ci id="S2.E3.m1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.3">BinaryStep</ci><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1"><apply id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S2.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1">subscript</csymbol><sum id="S2.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2.2"></sum><apply id="S2.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3"><eq id="S2.E3.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.1"></eq><ci id="S2.E3.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.2">ğ‘–</ci><cn id="S2.E3.m1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S2.E3.m1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S2.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2"><times id="S2.E3.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.1"></times><apply id="S2.E3.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2"><times id="S2.E3.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.1"></times><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.2">ğ‘ </ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.3">ğ‘</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.4.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.4">ğ‘œ</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.5.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.5">ğ‘Ÿ</ci><apply id="S2.E3.m1.1.1.1.1.1.1.2.2.6.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.2.6.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.6.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6.2">ğ‘’</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.2.6.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.2.6.3">ğ‘–</ci></apply></apply><apply id="S2.E3.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.2">ğ¼</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.1c">FinalPrediction=\mathrm{BinaryStep}(\sum_{i=1}^{N}score_{i}\times I_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.1d">italic_F italic_i italic_n italic_a italic_l italic_P italic_r italic_e italic_d italic_i italic_c italic_t italic_i italic_o italic_n = roman_BinaryStep ( âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_s italic_c italic_o italic_r italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT Ã— italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p5.1">Note that the output inferences are probability values indicating the possibility of the predicted label, and it is a fraction ranging in <math alttext="[0,1]" class="ltx_Math" display="inline" id="S2.SS2.p5.1.m1.2"><semantics id="S2.SS2.p5.1.m1.2a"><mrow id="S2.SS2.p5.1.m1.2.3.2" xref="S2.SS2.p5.1.m1.2.3.1.cmml"><mo id="S2.SS2.p5.1.m1.2.3.2.1" stretchy="false" xref="S2.SS2.p5.1.m1.2.3.1.cmml">[</mo><mn id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml">0</mn><mo id="S2.SS2.p5.1.m1.2.3.2.2" xref="S2.SS2.p5.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS2.p5.1.m1.2.2" xref="S2.SS2.p5.1.m1.2.2.cmml">1</mn><mo id="S2.SS2.p5.1.m1.2.3.2.3" stretchy="false" xref="S2.SS2.p5.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.2b"><interval closure="closed" id="S2.SS2.p5.1.m1.2.3.1.cmml" xref="S2.SS2.p5.1.m1.2.3.2"><cn id="S2.SS2.p5.1.m1.1.1.cmml" type="integer" xref="S2.SS2.p5.1.m1.1.1">0</cn><cn id="S2.SS2.p5.1.m1.2.2.cmml" type="integer" xref="S2.SS2.p5.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>. Thus, in order to transform the probability value into a binary classes, a binary step activation function is deployed here, where the function is described as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{BinaryStep}(x)=\begin{cases}0,&amp;\text{if }x&lt;0.5\\
1,&amp;\text{if }x\geq 0.5\end{cases}" class="ltx_Math" display="block" id="S2.E4.m1.5"><semantics id="S2.E4.m1.5a"><mrow id="S2.E4.m1.5.6" xref="S2.E4.m1.5.6.cmml"><mrow id="S2.E4.m1.5.6.2" xref="S2.E4.m1.5.6.2.cmml"><mi id="S2.E4.m1.5.6.2.2" xref="S2.E4.m1.5.6.2.2.cmml">BinaryStep</mi><mo id="S2.E4.m1.5.6.2.1" xref="S2.E4.m1.5.6.2.1.cmml">â¢</mo><mrow id="S2.E4.m1.5.6.2.3.2" xref="S2.E4.m1.5.6.2.cmml"><mo id="S2.E4.m1.5.6.2.3.2.1" stretchy="false" xref="S2.E4.m1.5.6.2.cmml">(</mo><mi id="S2.E4.m1.5.5" xref="S2.E4.m1.5.5.cmml">x</mi><mo id="S2.E4.m1.5.6.2.3.2.2" stretchy="false" xref="S2.E4.m1.5.6.2.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.5.6.1" xref="S2.E4.m1.5.6.1.cmml">=</mo><mrow id="S2.E4.m1.4.4" xref="S2.E4.m1.5.6.3.1.cmml"><mo id="S2.E4.m1.4.4.5" xref="S2.E4.m1.5.6.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E4.m1.4.4.4" rowspacing="0pt" xref="S2.E4.m1.5.6.3.1.cmml"><mtr id="S2.E4.m1.4.4.4a" xref="S2.E4.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E4.m1.4.4.4b" xref="S2.E4.m1.5.6.3.1.cmml"><mrow id="S2.E4.m1.1.1.1.1.1.1.3" xref="S2.E4.m1.5.6.3.1.cmml"><mn id="S2.E4.m1.1.1.1.1.1.1.1" xref="S2.E4.m1.1.1.1.1.1.1.1.cmml">0</mn><mo id="S2.E4.m1.1.1.1.1.1.1.3.1" xref="S2.E4.m1.5.6.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E4.m1.4.4.4c" xref="S2.E4.m1.5.6.3.1.cmml"><mrow id="S2.E4.m1.2.2.2.2.2.1" xref="S2.E4.m1.2.2.2.2.2.1.cmml"><mrow id="S2.E4.m1.2.2.2.2.2.1.2" xref="S2.E4.m1.2.2.2.2.2.1.2.cmml"><mtext id="S2.E4.m1.2.2.2.2.2.1.2.2" xref="S2.E4.m1.2.2.2.2.2.1.2.2a.cmml">ifÂ </mtext><mo id="S2.E4.m1.2.2.2.2.2.1.2.1" xref="S2.E4.m1.2.2.2.2.2.1.2.1.cmml">â¢</mo><mi id="S2.E4.m1.2.2.2.2.2.1.2.3" xref="S2.E4.m1.2.2.2.2.2.1.2.3.cmml">x</mi></mrow><mo id="S2.E4.m1.2.2.2.2.2.1.1" xref="S2.E4.m1.2.2.2.2.2.1.1.cmml">&lt;</mo><mn id="S2.E4.m1.2.2.2.2.2.1.3" xref="S2.E4.m1.2.2.2.2.2.1.3.cmml">0.5</mn></mrow></mtd></mtr><mtr id="S2.E4.m1.4.4.4d" xref="S2.E4.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E4.m1.4.4.4e" xref="S2.E4.m1.5.6.3.1.cmml"><mrow id="S2.E4.m1.3.3.3.3.1.1.3" xref="S2.E4.m1.5.6.3.1.cmml"><mn id="S2.E4.m1.3.3.3.3.1.1.1" xref="S2.E4.m1.3.3.3.3.1.1.1.cmml">1</mn><mo id="S2.E4.m1.3.3.3.3.1.1.3.1" xref="S2.E4.m1.5.6.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E4.m1.4.4.4f" xref="S2.E4.m1.5.6.3.1.cmml"><mrow id="S2.E4.m1.4.4.4.4.2.1" xref="S2.E4.m1.4.4.4.4.2.1.cmml"><mrow id="S2.E4.m1.4.4.4.4.2.1.2" xref="S2.E4.m1.4.4.4.4.2.1.2.cmml"><mtext id="S2.E4.m1.4.4.4.4.2.1.2.2" xref="S2.E4.m1.4.4.4.4.2.1.2.2a.cmml">ifÂ </mtext><mo id="S2.E4.m1.4.4.4.4.2.1.2.1" xref="S2.E4.m1.4.4.4.4.2.1.2.1.cmml">â¢</mo><mi id="S2.E4.m1.4.4.4.4.2.1.2.3" xref="S2.E4.m1.4.4.4.4.2.1.2.3.cmml">x</mi></mrow><mo id="S2.E4.m1.4.4.4.4.2.1.1" xref="S2.E4.m1.4.4.4.4.2.1.1.cmml">â‰¥</mo><mn id="S2.E4.m1.4.4.4.4.2.1.3" xref="S2.E4.m1.4.4.4.4.2.1.3.cmml">0.5</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.5b"><apply id="S2.E4.m1.5.6.cmml" xref="S2.E4.m1.5.6"><eq id="S2.E4.m1.5.6.1.cmml" xref="S2.E4.m1.5.6.1"></eq><apply id="S2.E4.m1.5.6.2.cmml" xref="S2.E4.m1.5.6.2"><times id="S2.E4.m1.5.6.2.1.cmml" xref="S2.E4.m1.5.6.2.1"></times><ci id="S2.E4.m1.5.6.2.2.cmml" xref="S2.E4.m1.5.6.2.2">BinaryStep</ci><ci id="S2.E4.m1.5.5.cmml" xref="S2.E4.m1.5.5">ğ‘¥</ci></apply><apply id="S2.E4.m1.5.6.3.1.cmml" xref="S2.E4.m1.4.4"><csymbol cd="latexml" id="S2.E4.m1.5.6.3.1.1.cmml" xref="S2.E4.m1.4.4.5">cases</csymbol><cn id="S2.E4.m1.1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E4.m1.1.1.1.1.1.1.1">0</cn><apply id="S2.E4.m1.2.2.2.2.2.1.cmml" xref="S2.E4.m1.2.2.2.2.2.1"><lt id="S2.E4.m1.2.2.2.2.2.1.1.cmml" xref="S2.E4.m1.2.2.2.2.2.1.1"></lt><apply id="S2.E4.m1.2.2.2.2.2.1.2.cmml" xref="S2.E4.m1.2.2.2.2.2.1.2"><times id="S2.E4.m1.2.2.2.2.2.1.2.1.cmml" xref="S2.E4.m1.2.2.2.2.2.1.2.1"></times><ci id="S2.E4.m1.2.2.2.2.2.1.2.2a.cmml" xref="S2.E4.m1.2.2.2.2.2.1.2.2"><mtext id="S2.E4.m1.2.2.2.2.2.1.2.2.cmml" xref="S2.E4.m1.2.2.2.2.2.1.2.2">ifÂ </mtext></ci><ci id="S2.E4.m1.2.2.2.2.2.1.2.3.cmml" xref="S2.E4.m1.2.2.2.2.2.1.2.3">ğ‘¥</ci></apply><cn id="S2.E4.m1.2.2.2.2.2.1.3.cmml" type="float" xref="S2.E4.m1.2.2.2.2.2.1.3">0.5</cn></apply><cn id="S2.E4.m1.3.3.3.3.1.1.1.cmml" type="integer" xref="S2.E4.m1.3.3.3.3.1.1.1">1</cn><apply id="S2.E4.m1.4.4.4.4.2.1.cmml" xref="S2.E4.m1.4.4.4.4.2.1"><geq id="S2.E4.m1.4.4.4.4.2.1.1.cmml" xref="S2.E4.m1.4.4.4.4.2.1.1"></geq><apply id="S2.E4.m1.4.4.4.4.2.1.2.cmml" xref="S2.E4.m1.4.4.4.4.2.1.2"><times id="S2.E4.m1.4.4.4.4.2.1.2.1.cmml" xref="S2.E4.m1.4.4.4.4.2.1.2.1"></times><ci id="S2.E4.m1.4.4.4.4.2.1.2.2a.cmml" xref="S2.E4.m1.4.4.4.4.2.1.2.2"><mtext id="S2.E4.m1.4.4.4.4.2.1.2.2.cmml" xref="S2.E4.m1.4.4.4.4.2.1.2.2">ifÂ </mtext></ci><ci id="S2.E4.m1.4.4.4.4.2.1.2.3.cmml" xref="S2.E4.m1.4.4.4.4.2.1.2.3">ğ‘¥</ci></apply><cn id="S2.E4.m1.4.4.4.4.2.1.3.cmml" type="float" xref="S2.E4.m1.4.4.4.4.2.1.3">0.5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.5c">\mathrm{BinaryStep}(x)=\begin{cases}0,&amp;\text{if }x&lt;0.5\\
1,&amp;\text{if }x\geq 0.5\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.5d">roman_BinaryStep ( italic_x ) = { start_ROW start_CELL 0 , end_CELL start_CELL if italic_x &lt; 0.5 end_CELL end_ROW start_ROW start_CELL 1 , end_CELL start_CELL if italic_x â‰¥ 0.5 end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p5.3">In the end, the final prediction is a class which embodies well-weighed decisions from all training sub-sets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experiments and Results</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To validate the feasibility of the proposed framework, we conducted experiments on the ASCERTAIN dataset. We used ECG and GSR data in this study. In the pre-processing stage, a low-pass filter with a 0.2 Hz cut-off frequency was used to obtain the low-pass GSR signal. Next, a band-pass filter with 0.67 to 40 Hz cut-off frequencies was applied to the ECG signals. Then, a five seconds time window with a five seconds window shift is used to extract features. In total, 42 features are extracted. Feature selection is applied, including variance threshold, SelectKbest, and tree-based selecter, to optimize the feature space. After feature selection, the ten most important features are kept, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S3.T2" title="TABLE II â€£ III Experiments and Results â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.2">Then, the data were normalized to the scale of <math alttext="[0,1]" class="ltx_Math" display="inline" id="S3.p2.1.m1.2"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mo id="S3.p2.1.m1.2.3.2.1" stretchy="false" xref="S3.p2.1.m1.2.3.1.cmml">[</mo><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">0</mn><mo id="S3.p2.1.m1.2.3.2.2" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">1</mn><mo id="S3.p2.1.m1.2.3.2.3" stretchy="false" xref="S3.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><interval closure="closed" id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><cn id="S3.p2.1.m1.1.1.cmml" type="integer" xref="S3.p2.1.m1.1.1">0</cn><cn id="S3.p2.1.m1.2.2.cmml" type="integer" xref="S3.p2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.2d">[ 0 , 1 ]</annotation></semantics></math>. This is because we hypothesize that different subjects have different vital signs baselines and have different physiological responses to emotional videos. In order to lessen the bias among subjects, normalization was applied. Afterward, data were imported into the APEX framework. Each subjectâ€™s feature set becomes a training subset <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.p2.2.m2.1"><semantics id="S3.p2.2.m2.1a"><msub id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">S</mi><mi id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ğ‘†</ci><ci id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.2.m2.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> to train a weak classifier.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We used Decision Tree as the weak classifier as it overperformed other classifiers in our preliminary experiments. Decision tree, support vector machine (SVM), k-NN, linear discriminative analysis have been tested and decision tree overperformed the second best classifiers, SVM by 0.9% in accuracy. We consider this is due to the advantages of the decision tree algorithm itself, as it can be considered a collection of if-then rules, which are highly interpretable, and fast in prediction. In addition, for a decision tree, there is no need to consider whether the features are interdependent. However, when using the decision tree algorithm alone, it is prone to overfitting. Through various methods, the complexity of the decision tree is suppressed, the fitting ability of a single decision tree is reduced, and multiple decision trees are integrated by the ensemble method, which can solve the problem of overfitting. Thus, the ensemble learning method and the decision tree learning algorithm can complement each other and are a perfect pair.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Feature List</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1">
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1">Category</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1">Feature</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.1">Description</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.2.1" rowspan="6"><span class="ltx_text" id="S3.T2.1.2.1.1">HRV</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.2.2">MAV</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.2.3">Mean Absolute Value</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.3.1">Range</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.3.2">Range</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.4.1">SDNN</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.4.2">Standard deviation of NN intervals</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.5.1">RMSSD</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.5.2">
<span class="ltx_text" id="S3.T2.1.5.2.1"></span><span class="ltx_text" id="S3.T2.1.5.2.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.5.2.2.1">
<span class="ltx_tr" id="S3.T2.1.5.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.5.2.2.1.1.1">Root mean square of successive</span></span>
<span class="ltx_tr" id="S3.T2.1.5.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.5.2.2.1.2.1">RR interval differences</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.5.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.6.1">pNN50</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.6.2">
<span class="ltx_text" id="S3.T2.1.6.2.1"></span><span class="ltx_text" id="S3.T2.1.6.2.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.1.6.2.2.1">
<span class="ltx_tr" id="S3.T2.1.6.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.6.2.2.1.1.1">Percentage of successive RR intervals that</span></span>
<span class="ltx_tr" id="S3.T2.1.6.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S3.T2.1.6.2.2.1.2.1">differ by more than 50 ms</span></span>
</span></span><span class="ltx_text" id="S3.T2.1.6.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.7.1">TINN</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.7.2">Baseline width of the RR interval histogram</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S3.T2.1.8.1" rowspan="4"><span class="ltx_text" id="S3.T2.1.8.1.1">GSR</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.8.2">MAV</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.8.3">Mean Absolute Value</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.9.1">P2P</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.9.2">Peak to Peak Amplitude</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.10.1">VAR</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.10.2">Variance</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S3.T2.1.11.1">MeanFreq</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S3.T2.1.11.2">Mean Frequency</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">There are 58 participants in the ASCERTAIN dataset. However, due to the quality of the physiological signals, ten subjects with relatively low data quality were removed from the study. Thus, 48 weak classifiers were trained. For testing, we set each subject as the testing set at a time, excluded this particular test subject from the weak classifiers, and used the other 47 weak classifiers to infer the test subject. This process was repeated 48 times for all subjects in the dataset. We reported the averaged systemâ€™s performance from all subjects.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">To assess the effectiveness of our novel framework, we conducted a comprehensive evaluation by replicating three seminal studies that tackled emotion recognition, both with and without the incorporation of personality traits. In contrast to these established works, our proposed framework was implemented and compared against their methodologies. In the study by Santamaria et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib19" title="">19</a>]</cite>, a convolutional neural network was employed to classify arousal and valence using the AMIGOS dataset. Their deep learning model demonstrated substantial efficacy in emotion recognition through physiological signals in the absence of personality integration. Shao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib17" title="">17</a>]</cite> pursued a distinctive approach by employing a hyperedge algorithm to establish connections between personality attributes, physiological signals, and emotions. Their work highlighted the potential synergy between these facets, showcasing the intricate interplay between personality and emotional states. In a similar vein, Tian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#bib.bib18" title="">18</a>]</cite> leveraged K-means clustering to categorize participants based on their personality traits. Subsequently, deep neural networks were trained for each personality subgroup. Our framework reproduced and extended their signal processing, feature extraction, and model deployment procedures on the ASCERTAIN dataset, utilizing the same participant cohort. The outcomes of our endeavors are meticulously presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S2.T1" title="TABLE I â€£ II-B APEX framework â€£ II Method â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">I</span></a>, underscoring the advancements brought about by our proposed framework.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="529" id="S3.F2.g1" src="extracted/5844054/figs/ROC.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The ROC curve of APEX framework on Arousal task from ASCERTAIN dataset.</figcaption>
</figure>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">The results showed our proposed framework overperformed existing studies. Our proposed framework achieved 77.1% and 76.9% accuracy on arousal and valence tasks, respectively, which overperformed 6% and 4% compared with the best results from baseline studies. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.06118v1#S3.F2" title="Figure 2 â€£ III Experiments and Results â€£ APEX: Attention on Personality based Emotion ReXgnition Framework"><span class="ltx_text ltx_ref_tag">2</span></a> shows the receiver operating characteristic curve of APEX working on the Arousal task from ASCERTAIN dataset. As we have 48 different classifier sets and individual testing sets, 48 ROC curves were produced. We averaged the ROC curves by the y-axis. All ROC curves lay within the gray zone and the boundaries represent the best and worst test cases of the area under curves equal to 0.78 and 0.89, respectively.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This study proposes a novel framework that integrates personality score into ensemble learning so that when inferencing, the decision pay more attention to subjects with similar personality. The APEX framework has a core of re-weight, it not only consider the personality similarity to adjust and pay more attention to more corelatted training samples while also keep decisions from physiological signals as dominant. We consider this is the main reason APEX is more accurate when compared with state-of-the-art approaches involving personality in emotion recognition where personality is treated arbitrarily (e.g., cluster subjects into personality groups) or considered slightly (e.g., a modality in parallel with other physiological signal modalities). This idea enables the APEX framework to progress more than the baseline (without personality) on a higher hierarchy.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In the personality attention part, several products are produced after the inner product of two personality vectors. We add a normalization process before sending them to softmax. This is because a personality vector is 5-dimensional with values ranging from <math alttext="[1,7]" class="ltx_Math" display="inline" id="S4.p2.1.m1.2"><semantics id="S4.p2.1.m1.2a"><mrow id="S4.p2.1.m1.2.3.2" xref="S4.p2.1.m1.2.3.1.cmml"><mo id="S4.p2.1.m1.2.3.2.1" stretchy="false" xref="S4.p2.1.m1.2.3.1.cmml">[</mo><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">1</mn><mo id="S4.p2.1.m1.2.3.2.2" xref="S4.p2.1.m1.2.3.1.cmml">,</mo><mn id="S4.p2.1.m1.2.2" xref="S4.p2.1.m1.2.2.cmml">7</mn><mo id="S4.p2.1.m1.2.3.2.3" stretchy="false" xref="S4.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.2b"><interval closure="closed" id="S4.p2.1.m1.2.3.1.cmml" xref="S4.p2.1.m1.2.3.2"><cn id="S4.p2.1.m1.1.1.cmml" type="integer" xref="S4.p2.1.m1.1.1">1</cn><cn id="S4.p2.1.m1.2.2.cmml" type="integer" xref="S4.p2.1.m1.2.2">7</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.2c">[1,7]</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.2d">[ 1 , 7 ]</annotation></semantics></math> and the average personality trait value in the ASCERTAIN dataset is 4.7. Thus, the output inner products usually range in hundred scales which is quite a large number when importing to softmax. In such case, the output probability will be concentrated to the highest value, i.e., 0.99 for the highest subject and 0.01 for all rest subjectsâ€™ summation. Thus, the decision will almost all depend on that specific subject with the 0.99 score. This is not the ideal case, so a normalization process is added to prevent the such phenomenon.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">In the proposed study, decision tree is selected as the weak classifier in the framework from four classifiers. However, other models can also be embedded such as linear classifiers, shallow neoral networks, etc. They may have different performances on different problems consider the type of data, the complexity of relationships and the potential challenges and computation time requirments.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">To extend this study, other factors that may influence emotion recognition other than personality can be taken into consideration, for example, demographic information. Exploring more factors and finding out the influential group of factors can lead emotion recognition to a more personalized and accurate stage. In addition, other algorithms can be used. This could include modifying the training process of neural networks by letting fewer score subjects only contribute to the first few epochs.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This study proposes the APEX framework that combines attention mechanism and ensemble learning to perform personalized emotion recognition. In the proposed framework, personality scores are calculated and used to re-weigh the outputs from weak classifiers. We conducted experiments on 48 subjects from ASCERTAIN dataset, and the results showed that the proposed framework performed better than the current state-of-the-art studies. We obtained classification accuracies for valence and arousal were 76.9% and 77.1%, respectively.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Andrius Dzedzickis, ArtÅ«ras Kaklauskas, and Vytautas Bucinskas.

</span>
<span class="ltx_bibblock">Human emotion recognition: Review of sensors and methods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Sensors</span>, 20(3):592, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
WÂ Michael Vanderlind, Yael Millgram, ArielleÂ R Baskin-Sommers, MargaretÂ S
Clark, and Jutta Joormann.

</span>
<span class="ltx_bibblock">Understanding positive emotion deficits in depression: From emotion
preferences to emotion regulation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Clinical Psychology Review</span>, 76:101826, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ruijie Fang, Ruoyu Zhang, Elahe Hosseini, Mahdi Orooji, Houman Homayoun,
SayedÂ Mohammad Hosseini, Mahya Faghih, Soheil Rafatirad, and Setareh
Rafatirad.

</span>
<span class="ltx_bibblock">Atlas: An adaptive transfer learning based pain assessment system: A
real life unsupervised pain assessment solution.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">2022 44th Annual International Conference of the IEEE
Engineering in Medicine &amp; Biology Society (EMBC)</span>, pages 1331â€“1337. IEEE,
2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jianhua Zhang, Zhong Yin, Peng Chen, and Stefano Nichele.

</span>
<span class="ltx_bibblock">Emotion recognition using multi-modal data and machine learning
techniques: A tutorial and review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Information Fusion</span>, 59:103â€“126, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ruijie Fang, Ruoyu Zhang, Elahe Hosseini, AnnaÂ M Parenteau, Sally Hang, Setareh
Rafatirad, CameliaÂ E Hostinar, Mahdi Orooji, and Houman Homayoun.

</span>
<span class="ltx_bibblock">Towards generalized ml model in automated physiological arousal
computing: A transfer learning-based domain generalization approach.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">2022 IEEE International Conference on Bioinformatics and
Biomedicine (BIBM)</span>, pages 2577â€“2584. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Elahe Hosseini, Ruijie Fang, Ruoyu Zhang, Setareh Rafatirad, and Houman
Homayoun.

</span>
<span class="ltx_bibblock">Emotion and stress recognition utilizing galvanic skin response and
wearable technology: A real-time approach for mental health care.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">2023 IEEE International Conference on Bioinformatics and
Biomedicine (BIBM)</span>, pages 1125â€“1131. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Ruijie Fang, Ruoyu Zhang, SayedÂ M Hosseini, Mahya Faghih, Soheil Rafatirad,
Setareh Rafatirad, and Houman Homayoun.

</span>
<span class="ltx_bibblock">Pain level modeling of intensive care unit patients with machine
learning methods: An effective congeneric clustering-based approach.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">2022 4th International Conference on Intelligent Medicine and
Image Processing</span>, pages 89â€“95, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ruoyu Zhang, Ruijie Fang, Zhichao Zhang, Elahe Hosseini, Mahdi Orooji, Houman
Homayoun, and Gozde Goncu-Berk.

</span>
<span class="ltx_bibblock">Short: Real-time bladder monitoring by bio-impedance analysis to aid
urinary incontinence.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 8th ACM/IEEE International Conference on
Connected Health: Applications, Systems and Engineering Technologies</span>, pages
138â€“142, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Lin Shu, Jinyan Xie, Mingyue Yang, Ziyi Li, Zhenqi Li, Dan Liao, Xiangmin Xu,
and Xinyi Yang.

</span>
<span class="ltx_bibblock">A review of emotion recognition using physiological signals.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Sensors</span>, 18(7):2074, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ruoyu Zhang, Ruijie Fang, Chongzhou Fang, Houman Homayoun, and Gozde
GoncuÂ Berk.

</span>
<span class="ltx_bibblock">Privee: A wearable for real-time bladder monitoring system.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Adjunct Proceedings of the 2023 ACM International Joint
Conference on Pervasive and Ubiquitous Computing &amp; the 2023 ACM
International Symposium on Wearable Computing</span>, pages 291â€“295, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ruijie Fang, Ruoyu Zhang, Elahe Hosseini, AnnaÂ M Parenteau, Sally Hang, Setareh
Rafatirad, CameliaÂ E Hostinar, Mahdi Orooji, and Houman Homayoun.

</span>
<span class="ltx_bibblock">Prevent over-fitting and redundancy in physiological signal analyses
for stress detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">2022 IEEE International Conference on Bioinformatics and
Biomedicine (BIBM)</span>, pages 2585â€“2588. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ruijie Fang, Ruoyu Zhang, Elahe Hosseini, Chongzhou Fang, Setareh Rafatirad,
and Houman Homayoun.

</span>
<span class="ltx_bibblock">Introducing an open-source python toolkit for machine learning
research in physiological signal based affective computing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">2023 IEEE International Conference on Bioinformatics and
Biomedicine (BIBM)</span>, pages 1890â€“1894. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ramanathan Subramanian, Julia Wache, MojtabaÂ Khomami Abadi, RaduÂ L Vieriu,
Stefan Winkler, and Nicu Sebe.

</span>
<span class="ltx_bibblock">Ascertain: Emotion and personality recognition using commercial
sensors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Affective Computing</span>, 9(2):147â€“160, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Mohammad Soleymani, Jeroen Lichtenauer, Thierry Pun, and Maja Pantic.

</span>
<span class="ltx_bibblock">A multimodal database for affect recognition and implicit tagging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE transactions on affective computing</span>, 3(1):42â€“55, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
HansÂ J Eysenck.

</span>
<span class="ltx_bibblock">Biological dimensions of personality.

</span>
<span class="ltx_bibblock">1990.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
FarukÂ Enes OÄŸuz, Ahmet Alkan, and Thorsten SchÃ¶ler.

</span>
<span class="ltx_bibblock">Emotion detection from ecg signals with different learning algorithms
and automated feature engineering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Signal, Image and Video Processing</span>, pages 1â€“9, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jingzhi Shao, Junjie Zhu, Yuxuan Wei, Yifan Feng, and Xibin Zhao.

</span>
<span class="ltx_bibblock">Emotion recognition by edge-weighted hypergraph neural network.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">2019 IEEE International Conference on Image Processing
(ICIP)</span>, pages 2144â€“2148. IEEE, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zhihang Tian, Dongmin Huang, Sijin Zhou, Zhidan Zhao, and Dazhi Jiang.

</span>
<span class="ltx_bibblock">Personality first in emotion: a deep neural network based on
electroencephalogram channel attention for cross-subject emotion recognition.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Royal Society open science</span>, 8(8):201976, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Luz Santamaria-Granados, Mario Munoz-Organero, Gustavo Ramirez-Gonzalez, Enas
Abdulhay, and NJIA Arunkumar.

</span>
<span class="ltx_bibblock">Using deep convolutional neural network for emotion detection on a
physiological signals dataset (amigos).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE Access</span>, 7:57â€“67, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 00:03:17 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
