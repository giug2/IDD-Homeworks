<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models</title>
<!--Generated on Wed Oct  9 18:07:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
High-level synthesis,  hardware design,  large language models,  few-shot learning,  program optimization
" lang="en" name="keywords"/>
<base href="/html/2410.07356v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S1" title="In Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2" title="In Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.SS1" title="In II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Document Embedding</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.SS2" title="In II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Document Retrieval</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.SS3" title="In II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Prompt engineering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.SS4" title="In II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Generator</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.SS5" title="In II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-E</span> </span><span class="ltx_text ltx_font_italic">Benchmarks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S3" title="In Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Experiment</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S3.SS1" title="In III Experiment ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Experiment Design</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S3.SS2" title="In III Experiment ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Result Analysis</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S4" title="In Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S5" title="In Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Related Work and Future Work</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Optimizing High-Level Synthesis Designs with
Retrieval-Augmented Large Language Models
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haocheng Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.1.id1">Dept. of EECS</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id3.2.id2">University of California, Irvine
<br class="ltx_break"/></span>Irvine, USA 
<br class="ltx_break"/>haochx5@uci.edu
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haotian Hu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">School of the Gifted Young</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.2.id2">University of Science and Technology of China
<br class="ltx_break"/></span>Hefei, China 
<br class="ltx_break"/>haotianhu@mail.ustc.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sitao Huang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.1.id1">Dept. of EECS</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.2.id2">University of California, Irvine
<br class="ltx_break"/></span>Irvine, USA 
<br class="ltx_break"/>sitaoh@uci.edu
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">High-level synthesis (HLS) allows hardware designers to create hardware designs with high-level programming languages like C/C++/OpenCL, which greatly improves hardware design productivity. However, existing HLS flows require programmers’ hardware design expertise and rely on programmers’ manual code transformations and directive annotations to guide compiler optimizations. Optimizing HLS designs requires non-trivial HLS expertise and tedious iterative process in HLS code optimization. Automating HLS code optimizations has become a burning need.
Recently, large language models (LLMs) trained on massive code and programming tasks have demonstrated remarkable proficiency in comprehending code, showing the ability to handle domain-specific programming queries directly without labor-intensive fine-tuning.
In this work, we propose a novel retrieval-augmented LLM-based approach to effectively optimize high-level synthesis (HLS) programs.
Our proposed method leverages few-shot learning, enabling large language models to adopt domain-specific knowledge through natural language prompts.
We propose a unique framework, Retrieve Augmented Large Language Model Aided Design (RALAD), designed to enhance LLMs’ performance in HLS code optimization tasks. RALAD employs advanced embedding techniques and top-<em class="ltx_emph ltx_font_italic" id="id1.1.1">k</em> search algorithms to dynamically source relevant knowledge from extensive databases, thereby providing contextually appropriate responses to complex programming queries. Our implementation of RALAD on two specialized domains, utilizing comparatively smaller language models, achieves an impressive 80% success rate in compilation tasks and outperforms general LLMs by 3.7 – 19<math alttext="\times" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">×</annotation></semantics></math> in latency improvement.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
High-level synthesis, hardware design, large language models, few-shot learning, program optimization

</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_publicationid" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">publicationid: </span>pubid: <span class="ltx_text ltx_inline-block" id="id1.1a" style="width:433.6pt;"> 979-8-3503-7608-1/24$31.00 ©2024 IEEE  </span> <span class="ltx_text ltx_inline-block" id="id1.2" style="width:433.6pt;"> </span></span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The last decades have witnessed the emergence of domain-specific accelerators in various application domains, which greatly improves the computational efficiency of computer systems. In the field of domain-specific accelerator design, high-level synthesis (HLS) has become a popular hardware design method as it allows designers to write C/C++/OpenCL code to generate hardware designs with the help of HLS compilers. Even though HLS greatly improves hardware design efficiency compared to RTL-level design flows, existing HLS compilers heavily rely on programmers’ manual code transformation and compiler directive insertion to guide HLS compiler optimizations, making the HLS optimization time-consuming and sub-optimal. Optimizing HLS effectively and automatically has been a big challenge. In this work, we propose a novel generative AI-based solution to this HLS optimization challenge, <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">RALAD</span>, a hybrid approach that combines large language models (LLMs) based code generation with HLS domain-specific optimization knowledge base, enabled by retrieval-augmented generation (RAG) technology.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S1.F1.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(left) Original HLS C code; (middle) Code-Llama-13B + RAG generated HLS C code; (right) GPT-3.5 + RAG generated HLS C code. </figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Large language models (LLMs) have demonstrated exceptional performance across numerous domains and hold immense potential in fields like coding, medicine, and law, closely approaching human-level expertise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib1" title="">1</a>]</cite>. However, they still suffer when answering questions that require knowledge of local conditions, specific varieties, or up-to-date data<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib2" title="">2</a>]</cite>. One way to address this issue is to fine-tune the pre-trained LLMs on specific datasets to adapt the domain knowledge. However, this approach usually requires retraining on partial or full layers of the pre-trained LLMs on a specific dataset, which is compute-intensive and time-consuming. Moreover, fine-tuning will be required each time when the LLMs need to be deployed in a different domain. Another way to address the domain knowledge issue is through <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">retrieval-augmented generation (RAG)</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib3" title="">3</a>]</cite>. RAG allows LLMs to retrieve information from an external source, augment their input with relevant context, and generate more informed, accurate prompts to guide the LLMs’ responses to domain-specific queries.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Code optimization, for instance, can potentially benefit from RAG by fetching the relevant information from previous similar code optimization examples and using them to guide new code optimizations or perform debugging. At high level, our proposed flow works as follows. The flow takes users’ input of a piece of code with the optimization instructions. Those original prompts will be embedded by one of the pre-trained embedding models; then, a query index search will try to match the prompts embedding with the relevant document (code) embedding in the customized code base; carefully crafted prompts will be generated by integrating the instructions, original prompts, and retrieved code; finally, the pre-trained LLMs will process the crafted prompts to optimize the code. This RAG mechanism for code-generation systems does not require training or fine-tuning of the existing LLMs.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Methodology</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.F2" title="Figure 2 ‣ II-A Document Embedding ‣ II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows the basic structure of our RALAD code optimization framework which includes source code splitting, embedding, related code search, prompt reconstruction, and code generation. We will introduce each part in the following subsections.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Document Embedding</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Prior works have demonstrated the promising performance boost that RAG can bring to text generation. Patrick et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib3" title="">3</a>]</cite> explored the application of RAG on knowledge-intensive tasks using Wikipedia as their primary data source. Fu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib4" title="">4</a>]</cite> leverage the most relevant code demonstration as references for processing the query code (QC). Inspired by prior works, we show that retrieving similar text or code from programming books or guides (in natural language), specifically those focusing on the HLS domain could be beneficial for our intended tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Given the highly limited amount of source code datasets that come with HLS pragma annotations, we use “Parallel Programming for FPGAs” (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">pp4f</em>) book <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib5" title="">5</a>]</cite> as our foundational dataset due to its potential to offer instructive examples and relevant documentation for learning. To preserve the integrity of pragma directives embedded within the source code, each source file was segmented into chunks to ensure the comprehensive retention of pragma-related information, which is critical for understanding and applying HLS optimizations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Following the segmentation of source code into chunks, we explored various embedding techniques including, OpenAI embedding, sentence embedding, and Code T5 embedding. Embeddings are essentially vectors generated by models to encapsulate significant data within contexts to enhance our framework’s comprehension of the source code’s semantics and structure while reducing the search complexity.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">More specifically, we divided the entire <em class="ltx_emph ltx_font_italic" id="S2.SS1.p4.1.1">pp4f</em> book into 699 chunks, each with a chunk size of 1000 characters and with an overlap size of 200 characters between consecutive segments. This overlapping strategy was employed to minimize the risk of dividing the wrong part of the codes into different chunks to ensure the integrity of the code examples within the individual chunks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Similarly, the queries from the users were also transformed into vector representation, to ensure a direct comparison and retrieval process based on the semantic similarity between the query code and the source code.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="262" id="S2.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The Overview of Our Proposed RALAD HLS Code Optimization Framework</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Document Retrieval</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Identifying the top-K results for our query can be efficiently achieved using specialized libraries, such as FAISS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib6" title="">6</a>]</cite>, or through a direct comparison of distances between the encoded query and the encoded code chunks, selecting the top K most similar. Zhou et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib7" title="">7</a>]</cite> declared that retrieving the top 4 relevant documents is generally sufficient for most applications as increasing the retrieved documents leads to larger memory consumption without proportional benefits.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Prompt engineering</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Noor et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib8" title="">8</a>]</cite> investigated how to build few-shot learning prompts for code-processing tasks. Inspired by them, we have developed a version of prompt design based on their structure.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">For HLS Optimization tasks, we construct prompts by integrating three key components: (a) Natural language instructions (NLI) (b) Retrieved Documents (RD) (c) Query with to-be-optimized code (QC). To demonstrate, we use the example of optimizing Matrix Multiplication in High-Level Synthesis, combining the above elements to form a final prompt, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.F3" title="Figure 3 ‣ II-D Generator ‣ II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">To refine the prompts, we introduce additional features. Taking optimizing an FIR Filter as an example, illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.F4" title="Figure 4 ‣ II-D Generator ‣ II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we focus on the code’s main body, ignoring the initialization part of the function. This decision is driven by the observation that during document retrieval, variable names can obscure critical information, leading to model outputs that are irrelevant to the query.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">In addition, we apply the expert intervention to manually insert “<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p4.1.1">add pragma lines here</span>” annotations in our query at positions where pragma lines could be expected, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.F5" title="Figure 5 ‣ II-D Generator ‣ II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>. This step aims to improve the model’s capability to generate HLS synthesizable codes by providing explicit hints about optimization locations. The effectiveness of this technique is further evaluated in the experiment sections.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Percentage of Synthesizable Cases for Models with Various Settings (the higher the better)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Valid Percentage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Zero Shot</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">RAG</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">RAG + Annotations</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1">Code Llama-7B model</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.3">30%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4">50%</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.3.2.1">Code Llama-13B model</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.3.2.2">10%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.3.2.3">60%</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.1.3.2.4">80%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Generator</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">For the generator component of our framework, we explored various LLMs including GPT-3.5 and other fine-tuned open-sourced LLMs for code tasks, like CodeLlama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib9" title="">9</a>]</cite> and T5 Code <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib10" title="">10</a>]</cite>. We selected CodeLlama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib9" title="">9</a>]</cite>, a variant of Llama 2 specifically fine-tuned for code-related tasks. CodeLlama was developed by extending the training of Llama 2 with additional code-specific datasets, thereby enriching its understanding of programming languages and code optimization tasks. This choice was motivated by the necessity for our model to acquire a foundational comprehension of the code we aim to optimize.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">Although the T5 code model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib10" title="">10</a>]</cite>, known for its encoder-decoder architecture, was also considered, it failed to produce valuable outputs for our specific use case. Even when supplemented with RAG, T5 model’s performance did not improve significantly. This outcome led us to conclude that effective code optimization requires more than just pattern recognition; T5 model’s lack of intrinsic understanding of C code proved to be a significant barrier, preventing it from generating useful contributions to HLS code optimization tasks.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="427" id="S2.F3.g1" src="x3.png" width="315"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Structure of Input Prompts</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="171" id="S2.F4.g1" src="x4.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We ignore the initialization part of the query code, only extract the main loops as input, as shown in the yellow part of the graph</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S2.F5.g1" src="x5.png" width="374"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Manual Annotations: We add hints to the prompt manually by human experts to enhance the performance of the generator</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS5.5.1.1">II-E</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS5.6.2">Benchmarks</span>
</h3>
<div class="ltx_para" id="S2.SS5.p1">
<p class="ltx_p" id="S2.SS5.p1.1">In our study, we benchmarked the capabilities of GPT-3.5 and the open-sourced LLMs, CodeLlama-7b and CodeLlama-13b, specifically in the context of HLS pragma insertion. Aligning with the methodologies outlined in previous research, we focused on a fundamental task in HLS optimization: Matrix Multiplication. To assess the performance of these models, we employed the Pass @k metric (measuring the successful compilations in k attempts on the same example) with or without RAG. This approach allowed us to systematically compare the effectiveness of each model before and after the application of RAG, as well as against the performances of alternative models, providing a comprehensive overview of their respective capabilities in optimizing code for HLS tasks.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Latency Evaluation of Output Design With RAG</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S2.T2.16.16">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.16.16.17.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T2.16.16.17.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.17.1.1.1">MatMul</span></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.17.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.17.1.2.1">DSP</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.17.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.17.1.3.1">FF</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.17.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.17.1.4.1">LUT</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.17.1.5"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.17.1.5.1">Latency</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.16.16.17.1.6"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.17.1.6.1">Speed up</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.2.2.2.3">Original HLS code</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.2.2.2.4">6</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.2.2.2.5">1223</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.2.2.2.6">1752</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.1.1.1.1">30.08 <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.m1.1a"><mi id="S2.T2.1.1.1.1.m1.1.1" xref="S2.T2.1.1.1.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.2.2.2.2">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.2.2.2.2.m1.1"><semantics id="S2.T2.2.2.2.2.m1.1a"><mo id="S2.T2.2.2.2.2.m1.1.1" xref="S2.T2.2.2.2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.2.2.m1.1b"><times id="S2.T2.2.2.2.2.m1.1.1.cmml" xref="S2.T2.2.2.2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.2.2.m1.1d">×</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.4.4.4.3">GPT-3.5 Raw</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.4.4.4.4">3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.4.4.4.5">616</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.4.4.4.6">512</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.3.3.3.1">60.07 <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.3.3.3.1.m1.1"><semantics id="S2.T2.3.3.3.1.m1.1a"><mi id="S2.T2.3.3.3.1.m1.1.1" xref="S2.T2.3.3.3.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.1.m1.1b"><ci id="S2.T2.3.3.3.1.m1.1.1.cmml" xref="S2.T2.3.3.3.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.3.3.3.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.4.4.4.2">0.51<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.4.4.4.2.m1.1"><semantics id="S2.T2.4.4.4.2.m1.1a"><mo id="S2.T2.4.4.4.2.m1.1.1" xref="S2.T2.4.4.4.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.4.4.4.2.m1.1b"><times id="S2.T2.4.4.4.2.m1.1.1.cmml" xref="S2.T2.4.4.4.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.4.4.4.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.4.4.4.2.m1.1d">×</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.16.16.18.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.16.16.18.2.1">C-Llama-13B Raw*</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.16.16.18.2.2">–</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.16.16.18.2.3">–</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.16.16.18.2.4">–</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.16.16.18.2.5">–</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.16.16.18.2.6">–</td>
</tr>
<tr class="ltx_tr" id="S2.T2.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.6.6.6.3">C-Llama-13B + RAG</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.6.6.6.4">60</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.6.6.6.5">6034</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.6.6.6.6">2489</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.5.5.5.1">3.09 <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.5.5.5.1.m1.1"><semantics id="S2.T2.5.5.5.1.m1.1a"><mi id="S2.T2.5.5.5.1.m1.1.1" xref="S2.T2.5.5.5.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.5.1.m1.1b"><ci id="S2.T2.5.5.5.1.m1.1.1.cmml" xref="S2.T2.5.5.5.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.5.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.5.5.5.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.6.6.6.2">9.74<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.6.6.6.2.m1.1"><semantics id="S2.T2.6.6.6.2.m1.1a"><mo id="S2.T2.6.6.6.2.m1.1.1" xref="S2.T2.6.6.6.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.6.6.6.2.m1.1b"><times id="S2.T2.6.6.6.2.m1.1.1.cmml" xref="S2.T2.6.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.6.6.6.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.6.6.6.2.m1.1d">×</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.8.8.8.3">GPT-3.5 + RAG</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.8.8.8.4">120</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.8.8.8.5">52434</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.8.8.8.6">46433</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.7.7.7.1">
<span class="ltx_text ltx_font_bold" id="S2.T2.7.7.7.1.1">1.88</span> <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.7.7.7.1.m1.1"><semantics id="S2.T2.7.7.7.1.m1.1a"><mi id="S2.T2.7.7.7.1.m1.1.1" xref="S2.T2.7.7.7.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.7.1.m1.1b"><ci id="S2.T2.7.7.7.1.m1.1.1.cmml" xref="S2.T2.7.7.7.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.7.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.7.7.7.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.8.8.8.2"><span class="ltx_text ltx_font_bold" id="S2.T2.8.8.8.2.1">16.00<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.8.8.8.2.1.m1.1"><semantics id="S2.T2.8.8.8.2.1.m1.1a"><mo id="S2.T2.8.8.8.2.1.m1.1.1" xref="S2.T2.8.8.8.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.8.2.1.m1.1b"><times id="S2.T2.8.8.8.2.1.m1.1.1.cmml" xref="S2.T2.8.8.8.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.8.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.8.8.8.2.1.m1.1d">×</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.16.16.19.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S2.T2.16.16.19.3.1"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.19.3.1.1">SpMM</span></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.19.3.2"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.19.3.2.1">DSP</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.19.3.3"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.19.3.3.1">FF</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.19.3.4"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.19.3.4.1">LUT</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S2.T2.16.16.19.3.5"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.19.3.5.1">Latency</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S2.T2.16.16.19.3.6"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.19.3.6.1">Speed up</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.10.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.10.10.10.3">Original code</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.10.10.10.4">3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.10.10.10.5">583</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.10.10.10.6">390</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.9.9.9.1">2.41 <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.9.9.9.1.m1.1"><semantics id="S2.T2.9.9.9.1.m1.1a"><mi id="S2.T2.9.9.9.1.m1.1.1" xref="S2.T2.9.9.9.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.9.9.9.1.m1.1b"><ci id="S2.T2.9.9.9.1.m1.1.1.cmml" xref="S2.T2.9.9.9.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.9.9.9.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.9.9.9.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.10.10.10.2">1.00<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.10.10.10.2.m1.1"><semantics id="S2.T2.10.10.10.2.m1.1a"><mo id="S2.T2.10.10.10.2.m1.1.1" xref="S2.T2.10.10.10.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.10.10.10.2.m1.1b"><times id="S2.T2.10.10.10.2.m1.1.1.cmml" xref="S2.T2.10.10.10.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.10.10.10.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.10.10.10.2.m1.1d">×</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.12.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.12.12.12.3">GPT-3.5 Raw</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.12.12.12.4">3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.12.12.12.5">506</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.12.12.12.6">351</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.11.11.11.1">8.71 <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.11.11.11.1.m1.1"><semantics id="S2.T2.11.11.11.1.m1.1a"><mi id="S2.T2.11.11.11.1.m1.1.1" xref="S2.T2.11.11.11.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.11.11.11.1.m1.1b"><ci id="S2.T2.11.11.11.1.m1.1.1.cmml" xref="S2.T2.11.11.11.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.11.11.11.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.11.11.11.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.12.12.12.2">0.28<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.12.12.12.2.m1.1"><semantics id="S2.T2.12.12.12.2.m1.1a"><mo id="S2.T2.12.12.12.2.m1.1.1" xref="S2.T2.12.12.12.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.12.12.12.2.m1.1b"><times id="S2.T2.12.12.12.2.m1.1.1.cmml" xref="S2.T2.12.12.12.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.12.12.12.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.12.12.12.2.m1.1d">×</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.14.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S2.T2.14.14.14.3">C-Llama-13B + RAG</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.14.14.14.4">6</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.14.14.14.5">1550</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.14.14.14.6">1722</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S2.T2.13.13.13.1">2.31 <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.13.13.13.1.m1.1"><semantics id="S2.T2.13.13.13.1.m1.1a"><mi id="S2.T2.13.13.13.1.m1.1.1" xref="S2.T2.13.13.13.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.13.13.13.1.m1.1b"><ci id="S2.T2.13.13.13.1.m1.1.1.cmml" xref="S2.T2.13.13.13.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.13.13.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.13.13.13.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.14.14.14.2">1.04<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.14.14.14.2.m1.1"><semantics id="S2.T2.14.14.14.2.m1.1a"><mo id="S2.T2.14.14.14.2.m1.1.1" xref="S2.T2.14.14.14.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.14.14.14.2.m1.1b"><times id="S2.T2.14.14.14.2.m1.1.1.cmml" xref="S2.T2.14.14.14.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.14.14.14.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.14.14.14.2.m1.1d">×</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.16.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.16.16.16.3">GPT-3.5 + RAG</th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.16.16.16.4">6</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.16.16.16.5">1401</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.16.16.16.6">1133</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t" id="S2.T2.15.15.15.1">
<span class="ltx_text ltx_font_bold" id="S2.T2.15.15.15.1.1">2.11</span> <math alttext="\mu" class="ltx_Math" display="inline" id="S2.T2.15.15.15.1.m1.1"><semantics id="S2.T2.15.15.15.1.m1.1a"><mi id="S2.T2.15.15.15.1.m1.1.1" xref="S2.T2.15.15.15.1.m1.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.T2.15.15.15.1.m1.1b"><ci id="S2.T2.15.15.15.1.m1.1.1.cmml" xref="S2.T2.15.15.15.1.m1.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.15.15.15.1.m1.1c">\mu</annotation><annotation encoding="application/x-llamapun" id="S2.T2.15.15.15.1.m1.1d">italic_μ</annotation></semantics></math>s</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t" id="S2.T2.16.16.16.2"><span class="ltx_text ltx_font_bold" id="S2.T2.16.16.16.2.1">1.14<math alttext="\times" class="ltx_Math" display="inline" id="S2.T2.16.16.16.2.1.m1.1"><semantics id="S2.T2.16.16.16.2.1.m1.1a"><mo id="S2.T2.16.16.16.2.1.m1.1.1" xref="S2.T2.16.16.16.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.16.16.16.2.1.m1.1b"><times id="S2.T2.16.16.16.2.1.m1.1.1.cmml" xref="S2.T2.16.16.16.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.16.16.16.2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.T2.16.16.16.2.1.m1.1d">×</annotation></semantics></math></span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul class="ltx_itemize ltx_centering ltx_figure_panel" id="S2.I1">
<li class="ltx_item" id="S2.I1.1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="S2.I1.1.p1">
<p class="ltx_p" id="S2.I1.1.p1.1"><span class="ltx_text" id="S2.I1.1.p1.1.1" style="font-size:80%;">* Code Llama-13B Raw did not generate synthesizable code; 
<br class="ltx_break"/>  “C-Llama” stands for Code Llama</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Experiment</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we detail our experimental approach and evaluate the efficacy of RAG in optimizing pragma directives. Furthermore, we explore specific instances where our methodology surpasses the performance of zero-shot models.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Experiment Design</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our process began with encoding “Parallel Programming for FPGAs” by Ryan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib5" title="">5</a>]</cite> into vector representations. Subsequently, we utilized the FAISS library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib6" title="">6</a>]</cite> to construct an efficient index for these vectors. Guided by insights from Ma et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib11" title="">11</a>]</cite>, who demonstrated that the top 4 retrieved documents yield optimal results for Large Language Models, we adapt to this recommendation by selecting the four most similar pieces of code to craft our prompts.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">In our experiment, we measure the Average Case Real-Time Latency of the outputs generated by our method using RAG and zero-shot approaches, employing the Vivado HLS tool for evaluation.
We specifically target a series of examples in HLS to gauge the performance of our technique. Various methodologies have been proposed for evaluating model output performance. For instance, Fu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib4" title="">4</a>]</cite> utilize the Pass @k metric, which quantifies the number of successful compilations within k attempts for the same example in addition to the latency. This dual approach allows us to comprehensively understand the effectiveness of our method in optimizing code for HLS tasks. We also report the resource utilization from the Vivado HLS tool.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Result Analysis</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As shown in TABLE <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.T1" title="TABLE I ‣ II-C Prompt engineering ‣ II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_tag">I</span></a>, we present a comparison of outcomes before and after RAG across 10 attempts. The data reveal a significant increase in the successful compilation pass rate upon integrating retrieved documents. Our method improves the pass@10 rate of both the CodeLlama-7b and CodeLlama-13b models by 40% and 50%, respectively, in the FIR filter baseline example. Additionally, the incorporation of supplementary hints by human experts into the query further increases the pass@10 rate of our model’s output, proving the benefits of our refined methodology.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.4">We benchmarked our outputs using standard examples specific to the HLS domain to evaluate their effectiveness. As shown in TABLE <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#S2.T2" title="TABLE II ‣ II-E Benchmarks ‣ II Methodology ‣ Optimizing High-Level Synthesis Designs with Retrieval-Augmented Large Language Models"><span class="ltx_text ltx_ref_tag">II</span></a>, our results indicate that the CodeLlama-13b model with RAG achieved 9.737<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mo id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><times id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">×</annotation></semantics></math> speed up in the Matrix Multiplication task. In comparison, GPT-3.5 with RAG reached 16<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mo id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><times id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">×</annotation></semantics></math> speedup. However, the GPT-3.5 demands significantly more resources, including double the Digital Signal Processing (DSP) usage, 8<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><times id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">×</annotation></semantics></math> increase in Flip-Flops (FF), and an 18<math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mo id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><times id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">×</annotation></semantics></math> rise in Look-Up Tables (LUT). This substantial resource consumption raises critical questions regarding the validity of GPT-3.5’s optimization when trading resources for a comparatively modest performance improvement. In the Sparse Matrix Multiplication task, both Code Llama-13b and GPT-3.5 with RAG performed closely, utilizing a similar amount of resources.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We proposed a unique framework, Retrieve Augmented Large Language Model Aided Design (RALAD), engineered to enhance LLMs’ performance in HLS tasks. Our implementation of RALAD on two specialized domains, utilizing comparatively smaller language models, has yielded promising results.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Related Work and Future Work</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our framework, RALAD, represents an initial exploration of the design space of code optimization in High-Level Synthesis with large language models. There are some previous work targeting LLM-aided code generation tasks, especially for language with extensive pre-training resources like Python<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Generator Selection</span>
In our work, we only tested the limited LLMs for the generator. The performance difference between CodeLlama and T5 Code led us to conjecture that a model must first acquire some foundational understanding of coding principles to effectively address domain-specific challenges in HLS. To overcome this, some fine-tuning with the HLS dataset could be beneficial. Moreover, there are some other code-gen LLMs like GitHub Copilot<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib14" title="">14</a>]</cite> and Codex<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib15" title="">15</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Source Retrieving</span>
Documents and datasets with higher quality in the target domain are needed to retrieve more precise and efficient materials. Sometimes, getting those correct documents could be hard. We may also sometimes retrieve irrelevant or inaccurate documents from the datasets and perform knowledge correction via different types of documents retrieved before generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib16" title="">16</a>]</cite>.
To deal with this, more processes are required to manipulate our prompt.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Other Possibilities</span>
Chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.07356v1#bib.bib17" title="">17</a>]</cite> will iteratively increase the performance of the output, proving especially beneficial when multiple optimization layers are employed. Performance-conditioned generation, on the other hand, requires different implementations of identical examples to further distinguish the performance difference across the same or similar optimization stacks, thus enabling the most substantial performance improvements.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian.

</span>
<span class="ltx_bibblock">A comprehensive overview of large language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevão Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp, Bruno Silva, Swati Sharma, Vijay Aski, and Ranveer Chandra.

</span>
<span class="ltx_bibblock">Rag vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Yonggan Fu, Yongan Zhang, Zhongzhi Yu, Sixu Li, Zhifan Ye, Chaojian Li, Cheng Wan, and Yingyan Lin.

</span>
<span class="ltx_bibblock">Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Ryan Kastner, Janarbek Matai, and Stephen Neuendorffer.

</span>
<span class="ltx_bibblock">Parallel programming for fpgas, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou.

</span>
<span class="ltx_bibblock">The faiss library, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Shuyan Zhou, Uri Alon, Frank F. Xu, Zhiruo Wang, Zhengbao Jiang, and Graham Neubig.

</span>
<span class="ltx_bibblock">Docprompting: Generating code by retrieving the docs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">International Conference on Learning Representations (ICLR)</span>, Kigali, Rwanda, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Noor Nashid, Mifta Sintaha, and Ali Mesbah.

</span>
<span class="ltx_bibblock">Retrieval-based prompt selection for code-related few-shot learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</span>, pages 2450–2462, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Code Llama.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/codellama" title="">https://huggingface.co/codellama</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi.

</span>
<span class="ltx_bibblock">Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin.

</span>
<span class="ltx_bibblock">A replication study of dense passage retriever, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez.

</span>
<span class="ltx_bibblock">Improving code generation by training with natural language feedback, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.

</span>
<span class="ltx_bibblock">Codegen: An open large language model for code with multi-turn program synthesis, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Github Codepilot.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/features/copilot" title="">https://github.com/features/copilot</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Codex.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/openai-codex" title="">https://openai.com/blog/openai-codex</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-04-08.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.

</span>
<span class="ltx_bibblock">Corrective retrieval augmented generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh.

</span>
<span class="ltx_bibblock">Learning performance-improving code edits, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  9 18:07:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
