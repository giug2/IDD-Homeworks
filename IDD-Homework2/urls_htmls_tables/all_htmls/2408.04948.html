<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</title>
<!--Generated on Fri Aug  9 09:06:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.04948v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S1" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S1.SS1" title="In 1. Introduction ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Prior Work and Our Contribution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS1" title="In 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>VectorRAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS2" title="In 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Knowledge Graph Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS3" title="In 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>GraphRAG</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS3.SSS1" title="In 2.3. GraphRAG ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>HybridRAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS4" title="In 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS4.SSS1" title="In 2.4. Evaluation Metrics ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Faithfulness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS4.SSS2" title="In 2.4. Evaluation Metrics ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Answer Relevance:-</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS4.SSS3" title="In 2.4. Evaluation Metrics ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.3 </span>Context Precision</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.SS4.SSS4" title="In 2.4. Evaluation Metrics ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.4 </span>Context Recall</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S3" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS1" title="In 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Knowledge Graph Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS2" title="In 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>VectorRAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS3" title="In 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>GraphRAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS4" title="In 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>HybridRAG</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S5" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S6" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S7" title="In HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgement</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bhaskarjit Sarmah
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bhaskarjit.sarmah@blackrock.com">bhaskarjit.sarmah@blackrock.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">BlackRock, Inc.</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Gurugram</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">India</span>
</span></span></span>
<span class="ltx_author_before">,¬†</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benika Hall
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bhall@nvidia.com">bhall@nvidia.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Santa Clara, CA</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span></span></span>
<span class="ltx_author_before">,¬†</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rohan Rao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rohrao@nvidia.com">rohrao@nvidia.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Santa Clara, CA</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">USA</span>
</span></span></span>
<span class="ltx_author_before">,¬†</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sunil Patel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:supatel@nvidia.com">supatel@nvidia.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">NVIDIA</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Santa Clara, CA</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">USA</span>
</span></span></span>
<span class="ltx_author_before">,¬†</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefano Pasquali
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:stefano.pasquali@blackrock.com">stefano.pasquali@blackrock.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">BlackRock, Inc.</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">New York, NY</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">USA</span>
</span></span></span>
<span class="ltx_author_before">¬†and¬†</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dhagash Mehta
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:dhagash.mehta@blackrock.com">dhagash.mehta@blackrock.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">BlackRock, Inc.</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">New York, NY</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">USA</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called <span class="ltx_text ltx_font_italic" id="id19.id1.1">HybridRAG</span>, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">For the financial analyst, it is crucial to extract and analyze information from unstructured data sources like news articles, earnings reports, and other financial documents to have at least some chance to be on the better side of potential information asymmetry. These sources hold valuable insights that can impact investment decisions, market predictions, and overall sentiment. However, traditional data analysis methods struggle to effectively extract and utilize this information due to its unstructured nature. Large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib4" title="">4</a>]</cite> have emerged as powerful tools for financial services and investment management. Their ability to process and understand vast amounts of textual data makes them invaluable for tasks such as sentiment analysis, market trend predictions, and automated report generation. Specifically, extracting information from annual reports and other financial documents can greatly enhance the efficiency and accuracy of financial analysts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib5" title="">5</a>]</cite>. A robust information extraction system can help analysts quickly gather relevant data, identify market trends, and make informed decisions, leading to better investment strategies and risk management <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib6" title="">6</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Although LLMs have substantial potential in financial applications, there are notable challenges in using pre-trained models to extract information from financial documents outside their training data while also reducing hallucination <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib8" title="">8</a>]</cite>. Financial documents typically contain domain-specific language, multiple data formats, and unique contextual relationships that general purpose-trained LLMs do not handle well. In addition, extracting consistent and coherent information from multiple financial documents can be challenging due to variations in terminology, format, and context across different textual sources. The specialized terminology and complex data formats in financial documents make it difficult for models to extract meaningful insights, in turn, causing inaccurate predictions, overlooked insights, and unreliable analysis, which ultimately hinder the ability to make well-informed decisions.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Current approaches to mitigate these issues include various Retrieval-Augmented Generation (RAG) techniques <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib9" title="">9</a>]</cite>, which aim to improve the performance of LLMs by incorporating relevant retrieval techniques. VectorRAG (the traditional RAG techniques that are based on vector databases) focuses on improving Natural Language Processing (NLP) tasks by retrieving relevant textual information to support the generation tasks. VectorRAG excels in situations where context from related textual documents is crucial for generating meaningful and coherent responses <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib11" title="">11</a>]</cite>. RAG-based methods ensure the LLMs generate relevant and coherent responses that are aligned with the original input query. However, for financial documents, these approaches have significant challenges as a standalone solution. For instance, traditional RAG systems often use paragraph-level chunking techniques, which assume the text in those documents are uniform in length. This approach neglects the hierarchical nature of financial statements and can result in the loss of critical contextual information for an accurate analysis<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib13" title="">13</a>]</cite>. Due to the complexities in analyzing financial documents, the quality of the LLM retrieved-context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges demonstrate the need for more sophisticated methods that can effectively integrate and process the detailed and domain-specific information found in financial documents, ensuring more reliable and accurate results for informed decision-making.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Knowledge graphs (KGs) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib14" title="">14</a>]</cite> may provide a different point of view to looking at the financial documents where the documents are viewed as a collection of triplets of entities and their relationships as depicted in the text of the documents. KGs have become a pivotal technology in data management and analysis, providing a structured way to represent knowledge through entities and relationships and have been widely adopted in various domains, including search engines, recommendation systems, and biomedical research <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib17" title="">17</a>]</cite>. The primary advantage of KGs lies in their ability to offer a structured representation, which facilitates efficient querying and reasoning. However, building and maintaining KGs and integrating data from different sources, such as documents, news articles, and other external sources, into a coherent knowledge graph poses significant challenges.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The financial services industry has recognized the potential of KGs in enhancing data integration of heterogeneous data sources, risk management, and predictive analytics <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib18" title="">18</a>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">findkg2022</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib21" title="">21</a>]</cite>. Financial KGs integrate various financial data sources such as market data, financial reports, and news articles, creating a comprehensive view of financial entities and their relationships. This unified view improves the accuracy and comprehensiveness of financial analysis, facilitates risk management by identifying hidden relationships, and supports advanced predictive analytics for more accurate market predictions and investment decisions. However, handling large volumes of financial data and continuously updating the knowledge graph to reflect the dynamic nature of financial markets can be challenging and resource-intensive.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">GraphRAG (Graph-based Retrieval-Augmented Generation) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib27" title="">27</a>]</cite> is a novel approach that leverages knowledge graphs (KGs) to enhance the performance of NLP tasks such as Q&amp;A systems. By integrating KGs with RAG techniques, GraphRAG enables more accurate and context-aware generation of responses based on the structured information extracted from financial documents. But GraphRAG generally underperforms in abstractive Q&amp;A tasks or when there is not explicit entity mentioned in the question.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In the present work, we propose a combination of VectorRAG and GraphRAG, called HybridRAG, to retrieve the relevant information from external documents for a given query to the LLM that brings advantages of both the RAGs together to provide demonstrably more accurate answers to the queries.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Prior Work and Our Contribution</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">VectorRAG has been extensively investigated in the recent years and focuses on enhancing NLP tasks by retrieving relevant textual information to support generation processes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib26" title="">26</a>]</cite>. However, the effectiveness of the retrieval mechanism across multiple documents and longer contexts poses a significant challenge in extracting relevant responses. GraphRAG combines the capabilities of KGs with RAG to improve traditional NLP tasks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib25" title="">25</a>]</cite>. Within our implementations of both VectorRAG GraphRAG techniques, we explicitly add information on the metadata of the documents that is also shown to improve the performance of VectorRAG <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">To the best of our knowledge the present work is the first work that proposes a RAG approach that is hybrid of both VectorRAG and GraphRAG and demonstrates its potential of more effective analysis and utilization of financial documents by leveraging the combined strengths of VectorRAG and GraphRAG. We also utilize a novel ground-truth Q&amp;A dataset extracted from publicly available financial call transcripts of the companies included in the Nifty-50 index which is an Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nseindia.com/products/content/equities/indices/nifty_50.htm" title="">https://www.nseindia.com/products/content/equities/indices/nifty_50.htm</a></span></span></span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Methodology</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this Section, we provide details of the proposed methodology by first discussing details of VectorRAG, then methodologies of constructing KGs from given documents and our proposed methodology of GraphRAG and finally the HybridGraph technique.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>VectorRAG</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The traditional RAG <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib9" title="">9</a>]</cite> process begins with a query that is related to the information possessed within external document(s) that are not a part of the training dataset for the LLM. This query is used to search an external repository, such as a vector database or indexed corpus, to fetch relevant documents or passages containing useful information. These retrieved documents are then fed back into the LLM as additional context. Hence, in turn, for the given query, the language model generates a response based not only on its internal training data but also by incorporating the retrieved external information. This integration ensures that the generated content is grounded in more recent and verifiable data, improving the accuracy and contextual relevance of the responses. By combining the retrieval of external information with the generative capabilities of large language models, RAG enhances the overall quality and reliability of the generated text.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In traditional VectorRAG, the given external documents are divided into multiple chunks because of the limitation of context size of the language model. Those chunks are converted into embeddings using an embeddings model and then stored into a vector database. After that, the retrieval component performs a similarity search within the vector database to identify and rank the chunks most relevant to the query. The top-ranked chunks are retrieved and aggregated to provide context for the generative model.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Then, the generative model takes this retrieved context along with the original query and synthesizes a response. Thus, it merges the real-time information from the retrieved chunks with its pre-existing knowledge, ensuring that the response is both contextually relevant and detailed.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">The schematic diagram in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.F1" title="Figure 1 ‚Ä£ 2.1. VectorRAG ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">1</span></a> provides details on the part of RAG that generates vector database from given external documents in the traditional VectorRAG methodology where we also include explicit reference of metadata <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib8" title="">8</a>]</cite>. Section <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS2" title="4.2. VectorRAG ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">4.2</span></a> will provide implementation details for our experiments.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="336" id="S2.F1.g1" src="x1.png" width="628"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">A schematic diagram describing the vector database creation of a RAG application.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Knowledge Graph Construction</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">A KG is a structured representation of real-world entities, their attributes, and their relations, usually stored in a graph database or a triplet store, i.e., a KG consists of nodes that represent entities and edges that represent relations, as well as labels and attributes for both. A graph triplet is a basic unit of information in a KG, consisting of a subject, a predicate, and an object.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In most methodologies to build a KG from given documents, three main steps are involved: knowledge extraction, knowledge improvement, and knowledge adaptation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib28" title="">28</a>]</cite>. Here, we do not use knowledge adaptation and treat the KGs as static graphs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p3.1.1">Knowledge Extraction:-</span> This step aims to extract structured information from unstructured or semi-structured data, such as text, databases, and existing ontologies. The main tasks in this step are entity recognition, relationship extraction, and co-reference resolution.
Entity recognition and relationship extraction techniques use typical NLP tasks to identify entities and their relationships from textual sources <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib29" title="">29</a>]</cite>. Coreference resolution identifies and connects different references of the same entity, keeping coherence within the knowledge graph. For example, if the text refers to a company as both ‚Äùthe company‚Äù and ‚Äùit‚Äù, coreference resolution can link these mentions to the same entity node in the graph.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1">Knowledge Improvement:-</span> This step aims to enhance the quality and completeness of the KG by removing redundancies and addressing gaps in the extracted information. The primary tasks in this step are KG completion and fusion. KG completion technique infers missing entities and relationships within the graph using methods such as link prediction and entity resolution. Link prediction predicts the existence and type of a relation between two entities based on the graph structure and features, while entity resolution matches and merges different representations of the same entity from different sources.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Knowledge fusion combines information from multiple sources to create a coherent and unified KG. This involves resolving conflicts and redundancies among the sources, such as contradictory or duplicate facts, and aggregating or reconciling the information based on rules, probabilities, or semantic similarity.</p>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">We utilized a robust methodology for creating KG triplets from unstructured text data, specifically focusing on corporate documents such as earnings call transcripts, adapted from Ref.¬†<cite class="ltx_cite ltx_citemacro_citep">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2023findkg</span>, <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib18" title="">18</a>]</cite>. This process involves several interconnected stages, each designed to extract, refine, and structure information effectively.</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<p class="ltx_p" id="S2.SS2.p7.1">We implement a two-tiered LLM chain for content refinement and information extraction. The first tier employs an LLM to generate an abstract representation of each document chunk. This refinement process is crucial as it distills the essential information while preserving the original meaning and key relationships between concepts that serves as a more focused input for subsequent processing, enhancing the overall efficiency and accuracy of our triplet extraction pipeline. The second tier of our LLM chain is dedicated to entity extraction and relationship identification.</p>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p" id="S2.SS2.p8.1">Both the steps are executed using carefully performed prompt engineering on a pre-trained LLM. A detailed discussion on implementation of the methodology will be provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS1" title="4.1. Knowledge Graph Construction ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">4.1</span></a></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>GraphRAG</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">KG based RAG <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib22" title="">22</a>]</cite>, or GraphRAG, also begins with a query based on the user‚Äôs input same as VectorRAG. The main difference between VectorRAG and GraphRAG lies in the retrieval part. The query here is now used to search the KG to retrieve relevant nodes (entities) and edges (relationships) related to the query. A subgraph, which consists of these relevant nodes and edges, is extracted from the full KG to provide context. This subgraph is then integrated with the language model‚Äôs internal knowledge, by encoding the graph structure into embeddings that the model can interpret. The language model uses this combined context to generate responses that are informed by both the structured information from the KG and its pre-trained knowledge. Crucially, when responding to user queries about a particular company, we leveraged the metadata information to selectively filter and retrieve only those document segments pertinent to the queried company <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib8" title="">8</a>]</cite>. This integration helps ensure that the generated outputs are accurate, contextually relevant, and grounded in verifiable information.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">A schematic diagram of the retrieval methodology of GraphRAG is given in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S2.F2" title="Figure 2 ‚Ä£ 2.3. GraphRAG ‚Ä£ 2. Methodology ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">2</span></a>. Here we first write a prompt to clean the data and then write another prompt in the second stage to create knowledge triplets along with metadata. It will be covered in more detail in section <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS1" title="4.1. Knowledge Graph Construction ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">4.1</span></a></p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="171" id="S2.F2.g1" src="extracted/5771482/kg.png" width="314"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">A schematic diagram describing knowledge graph creation process of GraphRAG.</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1. </span>HybridRAG</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">For the HybridRAG technique, we propose to integrate the aforementioned two distinct RAG techniques: VectorRAG and GraphRAG. This integration involves a systematic combination of contextual information retrieved from both the traditional vector-based retrieval mechanism and the KG-based retrieval system, the latter of which was constructed specifically for this study.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">The amalgamation of these two contexts allows us to leverage the strengths of both approaches. The VectorRAG component provides a broad, similarity-based retrieval of relevant information, while the GraphRAG element contributes structured, relationship-rich contextual data. This combined context is then utilized as input for a LLM to generate the final responses. Details on the implementation of the HybridRAG will be provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS4" title="4.4. HybridRAG ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">To assess the efficacy of this integrated approach, we conducted a comparative analysis among the three approaches in a controlled experimental set up: VectorRAG, GraphRAG and HybridRAG. The responses generated using the combined VectorRAG and GraphRAG contexts were juxtaposed against those produced individually by VectorRAG and GraphRAG. This comparative evaluation aimed to discern potential improvements in response quality, accuracy, and comprehensiveness that might arise from the synergistic integration of these two RAG methodologies.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">To objectively evaluate different RAG approaches (VectorRAG and GraphRAG in their case), Ref.¬†<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib22" title="">22</a>]</cite> utilized metrics such as comprehensiveness (i.e., the amount of details the answer provides to cover all aspects and
details of the question?); diversity (i.e., the richness of the answer in providing different perspectives and insights
on the question); empowerment (i.e., the helpfulness of the answer to the reader understand and make informed judgements about the topic); and, directness (i.e., clearness of the answer in addressing the question). here, the LLM was provided tuples of question, target metric, and a pair of answers, and was asked to assess which answer was better according to the metric and why.</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">These metrics though compare the final generated answers, do not necessarily directly evaluate the retrieval and generation parts separately. Instead, here we implement a comprehensive set of evaluation metrics which are designed to capture different aspects of a given RAG system‚Äôs output quality, focusing on faithfulness, answer relevance, and context relevance <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib30" title="">30</a>]</cite>. Each metric provides unique insights into the system‚Äôs capabilities and limitations.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1. </span>Faithfulness</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.1">Faithfulness is a crucial metric that measures the extent to which the generated answer can be inferred from the provided context. Our implementation of the faithfulness metric involves a two-step process:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS1.p2">
<p class="ltx_p" id="S2.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS1.p2.1.1">Statement Extraction:-</span> We use an LLM to decompose the generated answer into a set of concise statements. This step is crucial for breaking down complex answers into more manageable and verifiable units. The prompt used for this step is:</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p3">
<p class="ltx_p" id="S2.SS4.SSS1.p3.1">‚ÄùGiven a question and answer, create one or more statements from each sentence in the given answer.
question: [question]
answer: [answer]‚Äù.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS4.SSS1.p4">
<p class="ltx_p" id="S2.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.SSS1.p4.1.1">Statement Verification:-</span> For each extracted statement, we employ the LLM to determine if it can be inferred from the given context. This verification process uses the following prompt:</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p5">
<p class="ltx_p" id="S2.SS4.SSS1.p5.1">‚ÄùConsider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.
statement: [statement 1]
‚Ä¶
statement: [statement n]‚Äù.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p6">
<p class="ltx_p" id="S2.SS4.SSS1.p6.4">The faithfulness score (<math alttext="F" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p6.1.m1.1"><semantics id="S2.SS4.SSS1.p6.1.m1.1a"><mi id="S2.SS4.SSS1.p6.1.m1.1.1" xref="S2.SS4.SSS1.p6.1.m1.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p6.1.m1.1b"><ci id="S2.SS4.SSS1.p6.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p6.1.m1.1.1">ùêπ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p6.1.m1.1c">F</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p6.1.m1.1d">italic_F</annotation></semantics></math>) is <math alttext="F=|V|/|S|" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p6.2.m2.2"><semantics id="S2.SS4.SSS1.p6.2.m2.2a"><mrow id="S2.SS4.SSS1.p6.2.m2.2.3" xref="S2.SS4.SSS1.p6.2.m2.2.3.cmml"><mi id="S2.SS4.SSS1.p6.2.m2.2.3.2" xref="S2.SS4.SSS1.p6.2.m2.2.3.2.cmml">F</mi><mo id="S2.SS4.SSS1.p6.2.m2.2.3.1" xref="S2.SS4.SSS1.p6.2.m2.2.3.1.cmml">=</mo><mrow id="S2.SS4.SSS1.p6.2.m2.2.3.3" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.cmml"><mrow id="S2.SS4.SSS1.p6.2.m2.2.3.3.2.2" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.2.1.cmml"><mo id="S2.SS4.SSS1.p6.2.m2.2.3.3.2.2.1" stretchy="false" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.2.1.1.cmml">|</mo><mi id="S2.SS4.SSS1.p6.2.m2.1.1" xref="S2.SS4.SSS1.p6.2.m2.1.1.cmml">V</mi><mo id="S2.SS4.SSS1.p6.2.m2.2.3.3.2.2.2" stretchy="false" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.2.1.1.cmml">|</mo></mrow><mo id="S2.SS4.SSS1.p6.2.m2.2.3.3.1" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.1.cmml">/</mo><mrow id="S2.SS4.SSS1.p6.2.m2.2.3.3.3.2" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.3.1.cmml"><mo id="S2.SS4.SSS1.p6.2.m2.2.3.3.3.2.1" stretchy="false" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.3.1.1.cmml">|</mo><mi id="S2.SS4.SSS1.p6.2.m2.2.2" xref="S2.SS4.SSS1.p6.2.m2.2.2.cmml">S</mi><mo id="S2.SS4.SSS1.p6.2.m2.2.3.3.3.2.2" stretchy="false" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.3.1.1.cmml">|</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p6.2.m2.2b"><apply id="S2.SS4.SSS1.p6.2.m2.2.3.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3"><eq id="S2.SS4.SSS1.p6.2.m2.2.3.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.1"></eq><ci id="S2.SS4.SSS1.p6.2.m2.2.3.2.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.2">ùêπ</ci><apply id="S2.SS4.SSS1.p6.2.m2.2.3.3.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.3"><divide id="S2.SS4.SSS1.p6.2.m2.2.3.3.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.1"></divide><apply id="S2.SS4.SSS1.p6.2.m2.2.3.3.2.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.2.2"><abs id="S2.SS4.SSS1.p6.2.m2.2.3.3.2.1.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.2.2.1"></abs><ci id="S2.SS4.SSS1.p6.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.1.1">ùëâ</ci></apply><apply id="S2.SS4.SSS1.p6.2.m2.2.3.3.3.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.3.2"><abs id="S2.SS4.SSS1.p6.2.m2.2.3.3.3.1.1.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.3.3.3.2.1"></abs><ci id="S2.SS4.SSS1.p6.2.m2.2.2.cmml" xref="S2.SS4.SSS1.p6.2.m2.2.2">ùëÜ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p6.2.m2.2c">F=|V|/|S|</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p6.2.m2.2d">italic_F = | italic_V | / | italic_S |</annotation></semantics></math>, where <math alttext="|V|" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p6.3.m3.1"><semantics id="S2.SS4.SSS1.p6.3.m3.1a"><mrow id="S2.SS4.SSS1.p6.3.m3.1.2.2" xref="S2.SS4.SSS1.p6.3.m3.1.2.1.cmml"><mo id="S2.SS4.SSS1.p6.3.m3.1.2.2.1" stretchy="false" xref="S2.SS4.SSS1.p6.3.m3.1.2.1.1.cmml">|</mo><mi id="S2.SS4.SSS1.p6.3.m3.1.1" xref="S2.SS4.SSS1.p6.3.m3.1.1.cmml">V</mi><mo id="S2.SS4.SSS1.p6.3.m3.1.2.2.2" stretchy="false" xref="S2.SS4.SSS1.p6.3.m3.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p6.3.m3.1b"><apply id="S2.SS4.SSS1.p6.3.m3.1.2.1.cmml" xref="S2.SS4.SSS1.p6.3.m3.1.2.2"><abs id="S2.SS4.SSS1.p6.3.m3.1.2.1.1.cmml" xref="S2.SS4.SSS1.p6.3.m3.1.2.2.1"></abs><ci id="S2.SS4.SSS1.p6.3.m3.1.1.cmml" xref="S2.SS4.SSS1.p6.3.m3.1.1">ùëâ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p6.3.m3.1c">|V|</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p6.3.m3.1d">| italic_V |</annotation></semantics></math> is the number of supported statements and <math alttext="|S|" class="ltx_Math" display="inline" id="S2.SS4.SSS1.p6.4.m4.1"><semantics id="S2.SS4.SSS1.p6.4.m4.1a"><mrow id="S2.SS4.SSS1.p6.4.m4.1.2.2" xref="S2.SS4.SSS1.p6.4.m4.1.2.1.cmml"><mo id="S2.SS4.SSS1.p6.4.m4.1.2.2.1" stretchy="false" xref="S2.SS4.SSS1.p6.4.m4.1.2.1.1.cmml">|</mo><mi id="S2.SS4.SSS1.p6.4.m4.1.1" xref="S2.SS4.SSS1.p6.4.m4.1.1.cmml">S</mi><mo id="S2.SS4.SSS1.p6.4.m4.1.2.2.2" stretchy="false" xref="S2.SS4.SSS1.p6.4.m4.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p6.4.m4.1b"><apply id="S2.SS4.SSS1.p6.4.m4.1.2.1.cmml" xref="S2.SS4.SSS1.p6.4.m4.1.2.2"><abs id="S2.SS4.SSS1.p6.4.m4.1.2.1.1.cmml" xref="S2.SS4.SSS1.p6.4.m4.1.2.2.1"></abs><ci id="S2.SS4.SSS1.p6.4.m4.1.1.cmml" xref="S2.SS4.SSS1.p6.4.m4.1.1">ùëÜ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p6.4.m4.1c">|S|</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS1.p6.4.m4.1d">| italic_S |</annotation></semantics></math> is the total number of statements.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2. </span>Answer Relevance:-</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">The answer relevance metric assesses how well the generated answer addresses the original question, irrespective of factual accuracy. This metric helps identify cases of incomplete answers or responses containing irrelevant information. Our implementation involves the following steps:</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1">Question Generation: We prompt the LLM to generate n potential questions based on the given answer:</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p3">
<p class="ltx_p" id="S2.SS4.SSS2.p3.1">‚ÄùGenerate a question for the given answer.
answer: [answer]‚Äù.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p4">
<p class="ltx_p" id="S2.SS4.SSS2.p4.1">Then, we obtain embeddings for all generated questions and the original question using OpenAI‚Äôs text-embedding-ada-002 model<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/guides/embeddings/embedding-models" title="">https://platform.openai.com/docs/guides/embeddings/embedding-models</a></span></span></span>.
We then calculate the cosine similarity between each generated question‚Äôs embedding and the original question‚Äôs embedding.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p5">
<p class="ltx_p" id="S2.SS4.SSS2.p5.5">Finally, the answer relevance score (AR) is computed as the average similarity across all generated questions: <math alttext="AR=\frac{1}{n}\sum(sim(q,q_{i}))" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p5.1.m1.2"><semantics id="S2.SS4.SSS2.p5.1.m1.2a"><mrow id="S2.SS4.SSS2.p5.1.m1.2.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.cmml"><mrow id="S2.SS4.SSS2.p5.1.m1.2.2.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.cmml"><mi id="S2.SS4.SSS2.p5.1.m1.2.2.3.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.2.cmml">A</mi><mo id="S2.SS4.SSS2.p5.1.m1.2.2.3.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.1.cmml">‚Å¢</mo><mi id="S2.SS4.SSS2.p5.1.m1.2.2.3.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.3.cmml">R</mi></mrow><mo id="S2.SS4.SSS2.p5.1.m1.2.2.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.2.cmml">=</mo><mrow id="S2.SS4.SSS2.p5.1.m1.2.2.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.cmml"><mfrac id="S2.SS4.SSS2.p5.1.m1.2.2.1.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3.cmml"><mn id="S2.SS4.SSS2.p5.1.m1.2.2.1.3.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3.2.cmml">1</mn><mi id="S2.SS4.SSS2.p5.1.m1.2.2.1.3.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3.3.cmml">n</mi></mfrac><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.2.cmml">‚Å¢</mo><mrow id="S2.SS4.SSS2.p5.1.m1.2.2.1.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.cmml"><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.2" rspace="0em" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.2.cmml">‚àë</mo><mrow id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.cmml"><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.2" stretchy="false" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.cmml"><mi id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.3.cmml">s</mi><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.4" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.4.cmml">i</mi><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2a" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2.cmml">‚Å¢</mo><mi id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.5" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.5.cmml">m</mi><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2b" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2.cmml">‚Å¢</mo><mrow id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.2.cmml"><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mi id="S2.SS4.SSS2.p5.1.m1.1.1" xref="S2.SS4.SSS2.p5.1.m1.1.1.cmml">q</mi><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.2.cmml">,</mo><msub id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.2" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml">q</mi><mi id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.3" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.4" stretchy="false" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.3" stretchy="false" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p5.1.m1.2b"><apply id="S2.SS4.SSS2.p5.1.m1.2.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2"><eq id="S2.SS4.SSS2.p5.1.m1.2.2.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.2"></eq><apply id="S2.SS4.SSS2.p5.1.m1.2.2.3.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.3"><times id="S2.SS4.SSS2.p5.1.m1.2.2.3.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.1"></times><ci id="S2.SS4.SSS2.p5.1.m1.2.2.3.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.2">ùê¥</ci><ci id="S2.SS4.SSS2.p5.1.m1.2.2.3.3.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.3.3">ùëÖ</ci></apply><apply id="S2.SS4.SSS2.p5.1.m1.2.2.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1"><times id="S2.SS4.SSS2.p5.1.m1.2.2.1.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.2"></times><apply id="S2.SS4.SSS2.p5.1.m1.2.2.1.3.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3"><divide id="S2.SS4.SSS2.p5.1.m1.2.2.1.3.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3"></divide><cn id="S2.SS4.SSS2.p5.1.m1.2.2.1.3.2.cmml" type="integer" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3.2">1</cn><ci id="S2.SS4.SSS2.p5.1.m1.2.2.1.3.3.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.3.3">ùëõ</ci></apply><apply id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1"><sum id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.2"></sum><apply id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1"><times id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.2"></times><ci id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.3">ùë†</ci><ci id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.4.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.4">ùëñ</ci><ci id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.5.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.5">ùëö</ci><interval closure="open" id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1"><ci id="S2.SS4.SSS2.p5.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.1.1">ùëû</ci><apply id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.2">ùëû</ci><ci id="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS4.SSS2.p5.1.m1.2.2.1.1.1.1.1.1.1.1.3">ùëñ</ci></apply></interval></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p5.1.m1.2c">AR=\frac{1}{n}\sum(sim(q,q_{i}))</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p5.1.m1.2d">italic_A italic_R = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ‚àë ( italic_s italic_i italic_m ( italic_q , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )</annotation></semantics></math>, where <math alttext="sim(q,q_{i})" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p5.2.m2.2"><semantics id="S2.SS4.SSS2.p5.2.m2.2a"><mrow id="S2.SS4.SSS2.p5.2.m2.2.2" xref="S2.SS4.SSS2.p5.2.m2.2.2.cmml"><mi id="S2.SS4.SSS2.p5.2.m2.2.2.3" xref="S2.SS4.SSS2.p5.2.m2.2.2.3.cmml">s</mi><mo id="S2.SS4.SSS2.p5.2.m2.2.2.2" xref="S2.SS4.SSS2.p5.2.m2.2.2.2.cmml">‚Å¢</mo><mi id="S2.SS4.SSS2.p5.2.m2.2.2.4" xref="S2.SS4.SSS2.p5.2.m2.2.2.4.cmml">i</mi><mo id="S2.SS4.SSS2.p5.2.m2.2.2.2a" xref="S2.SS4.SSS2.p5.2.m2.2.2.2.cmml">‚Å¢</mo><mi id="S2.SS4.SSS2.p5.2.m2.2.2.5" xref="S2.SS4.SSS2.p5.2.m2.2.2.5.cmml">m</mi><mo id="S2.SS4.SSS2.p5.2.m2.2.2.2b" xref="S2.SS4.SSS2.p5.2.m2.2.2.2.cmml">‚Å¢</mo><mrow id="S2.SS4.SSS2.p5.2.m2.2.2.1.1" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.2.cmml"><mo id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.2" stretchy="false" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.2.cmml">(</mo><mi id="S2.SS4.SSS2.p5.2.m2.1.1" xref="S2.SS4.SSS2.p5.2.m2.1.1.cmml">q</mi><mo id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.3" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.2.cmml">,</mo><msub id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.2" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.2.cmml">q</mi><mi id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.3" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.4" stretchy="false" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p5.2.m2.2b"><apply id="S2.SS4.SSS2.p5.2.m2.2.2.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2"><times id="S2.SS4.SSS2.p5.2.m2.2.2.2.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.2"></times><ci id="S2.SS4.SSS2.p5.2.m2.2.2.3.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.3">ùë†</ci><ci id="S2.SS4.SSS2.p5.2.m2.2.2.4.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.4">ùëñ</ci><ci id="S2.SS4.SSS2.p5.2.m2.2.2.5.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.5">ùëö</ci><interval closure="open" id="S2.SS4.SSS2.p5.2.m2.2.2.1.2.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1"><ci id="S2.SS4.SSS2.p5.2.m2.1.1.cmml" xref="S2.SS4.SSS2.p5.2.m2.1.1">ùëû</ci><apply id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.2">ùëû</ci><ci id="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.3.cmml" xref="S2.SS4.SSS2.p5.2.m2.2.2.1.1.1.3">ùëñ</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p5.2.m2.2c">sim(q,q_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p5.2.m2.2d">italic_s italic_i italic_m ( italic_q , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> is the cosine similarity between the embedding of the original question <math alttext="q" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p5.3.m3.1"><semantics id="S2.SS4.SSS2.p5.3.m3.1a"><mi id="S2.SS4.SSS2.p5.3.m3.1.1" xref="S2.SS4.SSS2.p5.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p5.3.m3.1b"><ci id="S2.SS4.SSS2.p5.3.m3.1.1.cmml" xref="S2.SS4.SSS2.p5.3.m3.1.1">ùëû</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p5.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p5.3.m3.1d">italic_q</annotation></semantics></math> and the embeddings of each of the <math alttext="n" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p5.4.m4.1"><semantics id="S2.SS4.SSS2.p5.4.m4.1a"><mi id="S2.SS4.SSS2.p5.4.m4.1.1" xref="S2.SS4.SSS2.p5.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p5.4.m4.1b"><ci id="S2.SS4.SSS2.p5.4.m4.1.1.cmml" xref="S2.SS4.SSS2.p5.4.m4.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p5.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p5.4.m4.1d">italic_n</annotation></semantics></math> generated questions <math alttext="q_{i}" class="ltx_Math" display="inline" id="S2.SS4.SSS2.p5.5.m5.1"><semantics id="S2.SS4.SSS2.p5.5.m5.1a"><msub id="S2.SS4.SSS2.p5.5.m5.1.1" xref="S2.SS4.SSS2.p5.5.m5.1.1.cmml"><mi id="S2.SS4.SSS2.p5.5.m5.1.1.2" xref="S2.SS4.SSS2.p5.5.m5.1.1.2.cmml">q</mi><mi id="S2.SS4.SSS2.p5.5.m5.1.1.3" xref="S2.SS4.SSS2.p5.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p5.5.m5.1b"><apply id="S2.SS4.SSS2.p5.5.m5.1.1.cmml" xref="S2.SS4.SSS2.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS4.SSS2.p5.5.m5.1.1.1.cmml" xref="S2.SS4.SSS2.p5.5.m5.1.1">subscript</csymbol><ci id="S2.SS4.SSS2.p5.5.m5.1.1.2.cmml" xref="S2.SS4.SSS2.p5.5.m5.1.1.2">ùëû</ci><ci id="S2.SS4.SSS2.p5.5.m5.1.1.3.cmml" xref="S2.SS4.SSS2.p5.5.m5.1.1.3">ùëñ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p5.5.m5.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.SSS2.p5.5.m5.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.3. </span>Context Precision</h4>
<div class="ltx_para" id="S2.SS4.SSS3.p1">
<p class="ltx_p" id="S2.SS4.SSS3.p1.1">It is a metric used to evaluate the relevance of retrieved context chunks in relation to a specified ground truth for a given question<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html" title="">https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html</a></span></span></span>. It calculates the proportion of relevant items that appear in the top ranks of the context. The formula for context precision at K is the sum of the products of precision at each rank k and a binary relevance indicator v_k, divided by the total number of relevant items in the top K results. Precision at each rank k is determined by the ratio of true positives at k to the sum of true positives and false positives at k. This metric helps in assessing how well the context supports the ground truth, aiming for higher scores which indicate better precision.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.4. </span>Context Recall</h4>
<div class="ltx_para" id="S2.SS4.SSS4.p1">
<p class="ltx_p" id="S2.SS4.SSS4.p1.1">It is a metric used to evaluate how well the retrieved context aligns with the ground truth answer, which is considered the definitive correct response<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html" title="">https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html</a></span></span></span>. It is quantified by comparing each sentence in the ground truth answer to see if it can be traced back to the retrieved context. The formula for context recall is the ratio of the number of ground truth sentences that can be attributed to the context to the total number of sentences in the ground truth. Higher values, ranging from 0 to 1, indicate better alignment and thus better context recall. This metric is crucial for assessing the effectiveness of information retrieval systems in providing relevant context.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Data Description</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Although there do exist some public financial datasets, none of them were suitable for the present experiments: e.g., FinQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib31" title="">31</a>]</cite>, TAT-QA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib32" title="">32</a>]</cite>, FIQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib33" title="">33</a>]</cite>, FinanceBench <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib34" title="">34</a>]</cite>, etc. datasets are limited to specific usecases such as benchmarking LLMs‚Äô abilities to perform complex numerical reasoning or sentiment analysis. On the other hand, FinTextQA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib35" title="">35</a>]</cite> dataset was not publicly available at the time of writing the present work. In addition, in most of these datasets, access to the actual documents from which the ground-truth Q&amp;As were created is not available, making it impossible to use them for our RAG techniques evaluation purposes. Hence, we resorted to a dataset of our own though through publicly available documents but such that we finally have access to both the actual financial documents and the ground-truth Q&amp;As. Datasets like FinanceBench<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/PatronusAI/financebench-test" title="">https://huggingface.co/datasets/PatronusAI/financebench-test</a></span></span></span> provides question-context-answer triplets but they are not useful as here we are comparing VectorRAG, GraphRAG and HybridRAG and they do not provide the context generated from a KG. A recent paper <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib22" title="">22</a>]</cite> has not made the KG and triplets constructed by their algorithm public to the best of our knowledge either.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In short, there is no publicly available benchmark dataset to compare VectorRAG and GraphRAG either for financial or general domains to the best of our knowledge. Hence, we had to rely on our own dataset constructed as explained below.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">We used transcripts from earnings calls of Nifty 50 constituents for
our analysis. The NIFTY 50 is popular index in the Indian stock market that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange (NSE). The dataset of the earning call documents of Nifty 50 companies is widely recognized in the investment realm and is esteemed as an authoritative and extensive collection
of earnings call transcripts. In our investigation, we focus on data
spanning the quarter ending in June, 2023 i.e. the earnings reports
for Q1 of the financial year 2024 (A financial year in India starts on the 1st April and ends in 31st March, so the quarter from 1st April to 30th June is the first quarter of 2024 for the Indian market).</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Our dataset encompasses 50
transcripts for this quarter, spanning over 50 companies within
Nifty 50 universe from diverse range of sectors including Infrastructure,
Healthcare, Consumer Durables, Banking, Automobile, Financial
Services, Energy - Oil &amp; Gas, Telecommunication, Consumer Goods,
Pharmaceuticals, Energy - Coal, Materials, Information Technology,
Construction, Diversified, Metals, Energy - Power and Chemicals
providing a substantial and diverse foundation for our study.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">We start the data collection process focused on acquiring earnings reports from company websites within the Nifty 50 universe by developing and deploying a custom web scraping tool to navigate through the websites of each company within the Nifty 50 index, systematically retrieving the pertinent earnings reports for Q1 of the financial year 2024. By utilizing this web scraping approach, we aimed to compile a comprehensive dataset encompassing the earnings reports of the constituent companies.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S3.T1" title="Table 1 ‚Ä£ 3. Data Description ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes basic statistics of the documents we will be experimenting with in the remainder of this work.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.2.1.1.1">Number of companies/documents</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.2.1.1.2">50</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.2.2.2.1">Average number of pages</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.2.2.2">27</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S3.T1.2.3.3.1">Average number of questions</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.2.3.3.2">16</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.4.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S3.T1.2.4.4.1">Average number of tokens</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S3.T1.2.4.4.2">60,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.3.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S3.T1.4.2" style="font-size:90%;">Summary Statistics for the call transcript documents used in the present work.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1">These call transcripts documents consist of questions and answers between financial analysts and the company representatives for the respective companies, hence, there already exist certain Q&amp;A pairs within these documents along with additional text. We examined the earnings reports within the Nifty50 universe, systematically curated a comprehensive array of randomly selected 400 questions posed during the earnings calls from all the documents, and gathered the exact responses corresponding to these questions. These questions constitute the specific queries articulated by financial analysts to the management during these calls.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Implementation Details</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this Section, we provide details of implementation of the proposed methodology.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Knowledge Graph Construction</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The initial phase of our approach centers on document preprocessing. We utilize the PyPDFLoader<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/" title="">https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/</a></span></span></span> to import PDF documents, which are subsequently segmented into manageable chunks using the RecursiveCharacterTextSplitter. This chunking strategy employs a size of 2024 characters with an overlap of 204 characters, ensuring comprehensive coverage while maintaining context across segment boundaries.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Following the preprocessing stage, we implement the two-tiered language model chain for content refinement and information extraction. It is not possible to include the exact prompt here due to the limited space, but a baseline prompt can be found in Ref.¬†<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:433.6pt;height:157.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(9.7pt,-3.5pt) scale(1.04672900225573,1.04672900225573) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.1.1.1">
<span class="ltx_p" id="S4.T2.2.1.1.1.1.1.1" style="width:151.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.1.1.1.1">Entity Type</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.1.2.1">
<span class="ltx_p" id="S4.T2.2.1.1.1.2.1.1" style="width:238.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.1.1.1.2.1.1.1">Examples</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.2.1.1.1">
<span class="ltx_p" id="S4.T2.2.1.2.1.1.1.1" style="width:151.8pt;">Companies and Corporations</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.2.1.2.1">
<span class="ltx_p" id="S4.T2.2.1.2.1.2.1.1" style="width:238.5pt;">Official names, abbreviations, informal references</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.3.2.1.1">
<span class="ltx_p" id="S4.T2.2.1.3.2.1.1.1" style="width:151.8pt;">Financial Metrics and Indicators</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.3.2.2.1">
<span class="ltx_p" id="S4.T2.2.1.3.2.2.1.1" style="width:238.5pt;">Revenue, profit margins, EBITDA</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.4.3.1.1">
<span class="ltx_p" id="S4.T2.2.1.4.3.1.1.1" style="width:151.8pt;">Corporate Executives and Key Personnel</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.4.3.2.1">
<span class="ltx_p" id="S4.T2.2.1.4.3.2.1.1" style="width:238.5pt;">CEOs, CFOs, board members</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.5.4.1.1">
<span class="ltx_p" id="S4.T2.2.1.5.4.1.1.1" style="width:151.8pt;">Products and Services</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.5.4.2.1">
<span class="ltx_p" id="S4.T2.2.1.5.4.2.1.1" style="width:238.5pt;">Tangible products and intangible services</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.6.5.1.1">
<span class="ltx_p" id="S4.T2.2.1.6.5.1.1.1" style="width:151.8pt;">Geographical Locations</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.6.5.2.1">
<span class="ltx_p" id="S4.T2.2.1.6.5.2.1.1" style="width:238.5pt;">Headquarters, operational regions, markets</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.7.6.1.1">
<span class="ltx_p" id="S4.T2.2.1.7.6.1.1.1" style="width:151.8pt;">Corporate Events</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S4.T2.2.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.7.6.2.1">
<span class="ltx_p" id="S4.T2.2.1.7.6.2.1.1" style="width:238.5pt;">Mergers, acquisitions, product launches, earnings calls</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.2.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.8.7.1.1">
<span class="ltx_p" id="S4.T2.2.1.8.7.1.1.1" style="width:151.8pt;">Legal and Regulatory Information</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.2.1.8.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.8.7.2.1">
<span class="ltx_p" id="S4.T2.2.1.8.7.2.1.1" style="width:238.5pt;">Legal cases, regulatory compliance</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>. </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Entities extracted from earnings call transcripts</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.T2" title="Table 2 ‚Ä£ 4.1. Knowledge Graph Construction ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes details on entities extracted from the earning calls transcripts using our prompt based method. Concurrently, LLM identifies relationships between these entities using a curated set of verbs, capturing the nuanced interactions within the corporate narrative. A key improvement in our methodology is the enhanced prompt engineering to generate the structured output format for knowledge triplets. Each triplet is represented as a nested list [‚Äôh‚Äô, ‚Äôtype‚Äô, ‚Äôr‚Äô, ‚Äôo‚Äô, ‚Äôtype‚Äô, ‚Äômetadata‚Äô], where ‚Äôh‚Äô and ‚Äôo‚Äô denote the head and object entities respectively, ‚Äôtype‚Äô specifies the entity category, ‚Äôr‚Äô represents the relationship, and ‚Äômetadata‚Äô encapsulates additional contextual information. This format allows for a rich, multidimensional representation of information, facilitating more nuanced downstream analysis.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Our process incorporates several advanced features to enhance the quality and utility of the extracted triplets. Entity disambiguation techniques are employed to consolidate different references to the same entity, improving consistency across the KG. We also prioritize conciseness in entity representation, aiming for descriptions of less than four words where possible, which aids in maintaining a clean and navigable graph structure.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">The extraction pipeline is applied iteratively to each document chunk, with results aggregated to form a comprehensive knowledge graph representation of the entire document, allowing for scalable processing of large documents while maintaining local context within each chunk. We have added explicit instruction on obtaining metadata following Ref.¬†<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib8" title="">8</a>]</cite> for both VectorRAG and GraphRAG.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">Finally, we implement a data persistence strategy, converting the extracted triplets from their initial string format to Python data structures and storing them in a pickle file. This facilitates easy retrieval and further manipulation of the knowledge graph data in subsequent analysis stages.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Our methodology represents a significant advancement in automated knowledge extraction from corporate documents. By combining advanced NLP techniques with a structured approach to information representation, we create a rich, queryable knowledge base that captures the complex relationships and key information present in corporate narratives. This approach opens up new possibilities for financial analysis and automated reasoning in the business domain that will be explored further in the future.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>VectorRAG</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our methodology builds upon the concept of RAG <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib9" title="">9</a>]</cite> which allows for the creation of a system that can provide context-aware, accurate responses to queries about company financial information, leveraging both the power of large language models and the efficiency of semantic search.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">At the core of our system is a Pinecone vector database<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pinecone.io/" title="">https://www.pinecone.io/</a></span></span></span>, which serves as the foundation for our information retrieval process. We employ OpenAI‚Äôs text-embedding-ada-002 model to transform textual data from earnings call transcripts into high-dimensional vector representations. This vectorization process enables semantic similarity searches, significantly enhancing the relevance and accuracy of retrieved information. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.T3" title="Table 3 ‚Ä£ 4.2. VectorRAG ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">3</span></a> provides summary of the configuration of the set up in use for our experiments.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.1.1.1.1">LLM</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.2.1.1.2">GPT-3.5-Turbo</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.2.1.1.1">LLM Temperature</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.2.1.2">0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.3.2.1.1">Embedding Model</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.3.2.2">text-embedding-ada-002</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.4.3.1.1">Framework</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.4.3.2">LangChain</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.5.4.1.1">Vector Database</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.5.4.2">Pinecone</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.6.5.1.1">Chunk Size</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.6.5.2">1024</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.7.6.1.1">Chunk Overlap</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.7.6.2">0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.8.7.1.1">Maximum Output Tokens</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.8.7.2">1024</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.9.8.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.9.8.1.1">Chunks for Similarity Algorithm</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.2.9.8.2">20</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T3.2.10.9.1.1">Number of Context Retrieved</span></th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.2.10.9.2">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>. </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">VectorRAG Configuration</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The Q&amp;A pipeline is constructed using the LangChain framework<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain" title="">https://docs.smith.langchain.com/old/cookbook/hub-examples/retrieval-qa-chain</a></span></span></span>. The begins with a context retrieval step, where we query the Pinecone vector store to obtain the most relevant document chunks for a given question. This retrieval process is fine-tuned with specific filters for quarters, years, and company names, ensuring that the retrieved information is both relevant and current.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Following retrieval, we implement a context formatting step that consolidates the retrieved document chunks into a coherent context string. This formatted context serves as the informational basis for the language model‚Äôs response generation. We have developed a sophisticated prompt template, that instructs the language model to function as an expert Q&amp;A system, emphasizing the importance of utilizing only the provided context information and avoiding direct references to the context in the generated responses.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">For the core language processing task, we integrate OpenAI‚Äôs GPT-3.5-turbo model which processes the formatted context and query to generate natural language responses that are informative, coherent, and contextually appropriate.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">To evaluate the performance of our system, we developed a comprehensive framework that includes the preparation of a custom dataset of question-answer pairs specific to each company‚Äôs earnings call. Our system processes each question in this dataset, generating answers based on the retrieved context. The evaluation results, including the original question, generated answer, retrieved contexts, and ground truth, are compiled into structured formats (CSV and JSON) to facilitate further analysis. The outputs generated by our system are stored in both CSV and JSON formats, enabling easy integration with various analysis tools and dashboards. This approach facilitates both quantitative performance metrics and qualitative assessment of the system‚Äôs responses, providing a comprehensive view of its effectiveness.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">By parameterizing company names, quarters, and years, we can easily adapt the system to different datasets and time periods. This design choice allows for seamless integration of new data and expansion to cover multiple companies and earnings calls.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>GraphRAG</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">For GraphRAG, we developed an Q&amp;A system specifically designed for corporate earnings call transcripts. Our implementation of GraphRAG leverages several key components and techniques:</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.1.1.1.1">LLM</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.2.1.1.2">GPT-3.5-Turbo</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.1.1.1">LLM Temperature</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.2.1.2">0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.3.2.1.1">Framework</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.3.2.2">LangChain</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.4.3.1.1">KG Manipulation</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.4.3.2">Networkx</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.5.4.1.1">Chunk Size</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.5.4.2">1024</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.6.5.1.1">Chunk Overlap</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.6.5.2">0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.7.6.1.1">Number of Triplets</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.7.6.2">13950</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.8.7.1.1">Number of nodes</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.8.7.2">11405</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.9.8.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.9.8.1.1">Number of edges</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T4.2.9.8.2">13883</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.2.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.10.9.1.1">DFS Depth</span></th>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.2.10.9.2">1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.3.1.1" style="font-size:90%;">Table 4</span>. </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">GraphRAG Configuration</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Knowledge Graph Construction:-</span>
We begin by constructing a KG from a set of knowledge triplets extracted from corporate documents using the prompt engineering based methodology as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.SS1" title="4.1. Knowledge Graph Construction ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">4.1</span></a>. These triplets, stored in a pickle file, represent structured information in the form of subject-predicate-object relationships. We use the NetworkxEntityGraph class from the LangChain library to create and manage this graph structure. Each triple is added to the graph, which encapsulates the head entity, relation, tail entity, and additional metadata.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">We implement the Q&amp;A functionality using the GraphQAChain class from LangChain. This chain combines the KG with an LLM (in our case, OpenAI‚Äôs GPT-3.5-turbo) to generate responses. The GraphQAChain traverses the KG to find relevant information and uses the language model to formulate coherent answers based on the retrieved context. A summary of configuration of our LLM models and other libraries used for GraphRAG is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S4.T4" title="Table 4 ‚Ä£ 4.3. GraphRAG ‚Ä£ 4. Implementation Details ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">In KG as the information is stored in the form of entities and relationships and there can be multiple relations emanating from one single entity, in this experiment, to extract relevant information from the KG, we employ a depth-first search strategy constrained by a depth of one from the specified entity.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">To prepare for assessing the performance of our GraphRAG system, we follow the below steps:
<span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">Dataset Preparation:-</span> We use a pre-generated CSV file containing question-answer pairs specific to the earnings call transcript.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">Iterative Processing:-</span> For each question in the dataset, we run the GraphQAChain to generate an answer.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p7.1.1">Result Compilation:-</span> We compile the results, including the original questions, generated answers, retrieved contexts, and ground truth answers, into a structured format.</p>
</div>
<div class="ltx_para" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.1">Finally, the evaluation results are saved in both CSV and JSON formats for further analysis and comparison. We then fed these outputs into our RAG evaluation pipeline. For each Q&amp;A pair in our dataset, we compute all three metrics: faithfulness, answer relevance, context precision and context recall.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>HybridRAG</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">For the proposed HybridRAG technique, upon obtaining all the contextual information from VectorRAG and GraphRAG, we concatenate these contexts to form a unified context utilizing both techniques. This combined context is then fed into the answer generator model to produce a response. The context used for response generation is relatively larger, which affects the precision of the generated response. The context from VectorRAG is appended first, followed by the context from GraphRAG. Consequently, the precision of the generated answer depends on the source context. If the answer is generated from the GraphRAG context, it will have lower precision, as the GraphRAG context is added last in the sequence of contexts provided to the answer generator model, and vice versa.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We evaluate both the retrieval and generation parts of RAG for the three different RAG pipelines. Evaluating the RAG outputs is also an active area of research there is no standard tool which is universally accepted as of yet, though we use a currently popular framework RAGAS<cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#bib.bib30" title="">30</a>]</cite> to evaluate the three RAG pipelines in the present work where we have modified them a bit to make more precise comparisons.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The results of our comparative analysis reveal notable differences in performance among VectorRAG, GraphRAG, and HybridRAG approaches as summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.04948v1#S5.T5" title="Table 5 ‚Ä£ 5. Results ‚Ä£ HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction"><span class="ltx_text ltx_ref_tag">5</span></a>. In terms of Faithfulness, GraphRAG and HybridRAG demonstrated superior performance, both achieving a score of 0.96, while VectorRAG trailed slightly with a score of 0.94. Answer relevancy scores varied across the methods, with HybridRAG outperforming the others at 0.96, followed by VectorRAG at 0.91, and GraphRAG at 0.89. Context precision was highest for GraphRAG at 0.96, significantly surpassing VectorRAG (0.84) and HybridRAG (0.79). However, in context recall, both VectorRAG and HybridRAG achieved perfect scores of 1, while GraphRAG lagged behind at 0.85.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Overall, these results suggest that GraphRAG offers improvements over VectorRAG, particularly in faithfulness and context precision. Furthermore, HybridRAG emerges as the most balanced and effective approach, outperforming both VectorRAG and GraphRAG in key metrics such as faithfulness and answer relevancy, while maintaining high context recall.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The relatively lower context precision observed for HybridRAG (0.79) can be attributed to its unique approach of combining contexts from both VectorRAG and GraphRAG methods. While this integration allows for more comprehensive information retrieval, it also introduces additional content that may not align precisely with the ground truth, thus affecting the context precision metric. Despite this trade-off, HybridRAG‚Äôs superior performance in faithfulness, answer relevancy, and context recall underscores its effectiveness. When considering the overall evaluation metrics, HybridRAG emerges as the most promising approach, balancing high-quality answers with comprehensive context retrieval.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Overall GraphRAG performs better in extractive questions compared to VectorRAG. And VectorRAG does better in abstractive questions where information is not explicitly mentioned in the raw data. And also GraphRAG sometimes fails to answer questions correctly whenever there is no entity explicitly mentioned in the question. So HybridRAG does a good job overall, as whenever VectorRAG fails to fetch correct context in extractive questions it falls back to GraphRAG to generate the answer. And whenever GraphRAG fails to fetch correct context in abstractive questions it falls back to VectorRAG to generate the answer.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.2.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.2.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.2.1.1.2.1">VectorRAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T5.2.1.1.3.1">GraphRAG</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T5.2.1.1.4.1">HybridRAG</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.2.2.2.1">F</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.2.2.2">0.94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.2.2.3">0.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.2.2.4">0.96</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.2.3.3.1">AR</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.3.3.2">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.3.3.3">0.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.3.3.4">0.96</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.2.4.4.1">CP</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.4.4.2">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.4.4.3">0.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.2.4.4.4">0.79</td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T5.2.5.5.1">CR</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.2.5.5.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.2.5.5.3">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.2.5.5.4">1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.3.1.1" style="font-size:90%;">Table 5</span>. </span><span class="ltx_text" id="S5.T5.4.2" style="font-size:90%;">Performance Metrics for Different RAG Pipelines. Here, F, AR, CP and CR refer to Faithfulness, Answer Relevence, Context Precision and Context Recall.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion and Future Directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Among the current approaches to mitigate issues regarding information extraction from external documents that were not part of training data for the LLM, Retrieval Augmented Generation (RAG) techniques have emerged as the most popular ones that aim to improve the performance of LLMs by incorporating relevant retrieval mechanisms. RAG methods enhance the LLMs‚Äô capabilities by retrieving pertinent documents or text to provide additional context during the generation process. However, these approaches encounter significant limitations when applied to the specialized and intricate domain of financial documents. Furthermore, the quality of the retrieved context from a vast and heterogeneous corpus can be inconsistent, leading to inaccuracies and incomplete analyses. These challenges highlight the need for more sophisticated methods that can effectively integrate and process the detailed and domain-specific information found in financial documents, ensuring more reliable and accurate outputs for informed decision-making.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In the present work, we have introduced a novel approach that significantly advances the field of information extraction from financial documents through the development of a hybrid RAG system. This system, called HybridRAG, which integrates the strengths of both Knowledge Graphs (KGs) and advanced language models, represents a leap forward in our ability to extract and interpret complex information from unstructured financial texts. The hybrid RAG system, by combining traditional vector-based RAG and KG-based RAG, has shown superior performance in terms of retrieval accuracy and answer generation, marking a pivotal step towards more effective financial analysis tools.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Through a comparative analysis using objecive evaluation metrics, we have highlighted the distinct performance advantages of the HybridRAG approach over its vector-based and KG-based counterparts. The HybridRAG system excels in faithfulness, answer relevancy, and context recall, demonstrating the benefits of integrating contexts from both VectorRAG and GraphRAG methods, despite potential trade-offs in context precision.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">The implications of this research extend beyond the immediate realm of financial analysis. By developing a system capable of understanding and responding to nuanced queries about complex financial documents, we pave the way for more sophisticated AI-assisted financial decision-making tools that could potentially democratize access to financial insights, allowing a broader range of stakeholders to engage with and understand financial information.</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Future directions for this research include expanding the system to handle multi-modal inputs, incorporating numerical data analysis capabilities, and developing more sophisticated evaluation metrics that capture the nuances of financial language and the accuracy of numerical information in the responses. Additionally, exploring the integration of this system with real-time financial data streams could further enhance its utility in dynamic financial environments.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Acknowledgement</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The views expressed here are those of the authors alone and not of BlackRock, Inc or NVIDIA. We are grateful to Emma Lind for her invaluable support for this collaboration.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space.

</span>
<span class="ltx_bibblock">2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan¬†N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yi¬†Yang, Mark Christopher¬†Siy Uy, and Allen Huang.

</span>
<span class="ltx_bibblock">Finbert: A pretrained language model for financial communications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2006.08097</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.

</span>
<span class="ltx_bibblock">Bloomberggpt: A large language model for finance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2303.17564</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John¬†M Mulvey, H¬†Vincent Poor, Qingsong Wen, and Stefan Zohren.

</span>
<span class="ltx_bibblock">A survey of large language models for financial applications: Progress, prospects and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2406.11903</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, and Tianming Liu.

</span>
<span class="ltx_bibblock">Revolutionizing finance with llms: An overview of applications and insights, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et¬†al.

</span>
<span class="ltx_bibblock">Domain specialization as the key to make large language models disruptive: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2305.18703</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu.

</span>
<span class="ltx_bibblock">Towards reducing hallucination in extracting information from financial reports using large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Proceedings of the Third International Conference on AI-ML Systems</span>, pages 1‚Äì5, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et¬†al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing Systems</span>, 33:9459‚Äì9474, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2007.01282</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Realm: Retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2002.08909</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Antonio¬†Jose Jimeno¬†Yepes et¬†al.

</span>
<span class="ltx_bibblock">Financial report chunking for effective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2402.05131</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
SuperAcc.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation on financial statements.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">SuperAcc Insights</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S¬†Yu Philip.

</span>
<span class="ltx_bibblock">A survey on knowledge graphs: Representation, acquisition, and applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">IEEE transactions on neural networks and learning systems</span>, 33(2):494‚Äì514, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Heiko Paulheim.

</span>
<span class="ltx_bibblock">Knowledge graphs: State of the art and future directions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Semantic Web Journal</span>, 10(4):1‚Äì20, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia D‚ÄôAmato, Gerard¬†de Melo, Claudio Guti√©rrez, and Maria ‚Ä¶¬†Maleshkova.

</span>
<span class="ltx_bibblock">Knowledge graphs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2003.02320</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lisa Ehrlinger and Wolfram W√∂√ü.

</span>
<span class="ltx_bibblock">Towards a definition of knowledge graphs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">SEMANTiCS (Posters, Demos, SuCCESS)</span>, pages 1‚Äì4, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Xiaohui¬†Victor Li and Francesco¬†Sanna Passino.

</span>
<span class="ltx_bibblock">Findkg: Dynamic knowledge graphs with large language models for detecting global trends in financial markets.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2407.10909</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shourya De, Anima Aggarwal, and Anna¬†Cinzia Squicciarini.

</span>
<span class="ltx_bibblock">Financial knowledge graphs: A novel approach to empower data-driven financial applications.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">IEEE International Conference on Big Data (Big Data)</span>, pages 1311‚Äì1320, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Marina Petrova and Birgit Reinwald.

</span>
<span class="ltx_bibblock">Knowledge graphs in finance: Applications and opportunities.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Journal of Financial Data Science</span>, 2(2):10‚Äì19, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wei Liu, Jun Zhang, and Li¬†Pan.

</span>
<span class="ltx_bibblock">Utilizing knowledge graphs for financial data integration and analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Data Science Journal</span>, 18(1):1‚Äì15, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Darren Edge, Ha¬†Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson.

</span>
<span class="ltx_bibblock">From local to global: A graph rag approach to query-focused summarization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2404.16130</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Xuchen Yao, Yanan Sun, Zhen Huang, and Dong Li.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2101.07554</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Weizhi Zhao, Haoyu Chen, Kai Liu, and Jun Zhao.

</span>
<span class="ltx_bibblock">Graph-based retrieval-augmented generation for open-domain question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, volume¬†36, pages 3098‚Äì3106, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Bill¬†Yuchen Lin, Yuanhe Liu, Ming Shen, and Xiang Ren.

</span>
<span class="ltx_bibblock">Kgpt: Knowledge-grounded pre-training for data-to-text generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</span>, pages 710‚Äì724, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi¬†Dai, Jiawei Sun, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2312.10997</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Tyler Procko.

</span>
<span class="ltx_bibblock">Graph retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Available at SSRN</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu.

</span>
<span class="ltx_bibblock">A comprehensive survey on automatic knowledge graph construction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">ACM Computing Surveys</span>, 56(4):1‚Äì62, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ishani Mondal, Yufang Hou, and Charles Jochim.

</span>
<span class="ltx_bibblock">End-to-end nlp knowledge graph construction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2106.01167</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert.

</span>
<span class="ltx_bibblock">Ragas: Automated evaluation of retrieval augmented generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2309.15217</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et¬†al.

</span>
<span class="ltx_bibblock">Finqa: A dataset of numerical reasoning over financial data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2109.00122</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Macedo Maia, Siegfried Handschuh, Andr√© Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur.

</span>
<span class="ltx_bibblock">Www‚Äô18 open challenge: financial opinion mining and question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Companion proceedings of the the web conference 2018</span>, pages 1941‚Äì1942, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2105.07624</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen.

</span>
<span class="ltx_bibblock">Financebench: A new benchmark for financial question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2311.11944</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang.

</span>
<span class="ltx_bibblock">Fintextqa: A dataset for long-form financial question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2405.09980</span>, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug  9 09:06:21 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
