<!DOCTYPE html>

<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets</title>
<!--Generated on Tue Apr  9 08:17:26 2024 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv_0.7.4.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.06107v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S1" title="1. Introduction ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S2" title="2. Related Work ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3" title="3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Preliminary</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1" title="3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>MMT with Search Engine Based Image Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px1" title="Text Encoder ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Text Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px2" title="Image Retrieval ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Image Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px3" title="Text-Aware Attentive Visual Encoder ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Text-Aware Attentive Visual Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px4" title="Translation Decoder ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Translation Decoder</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS2" title="3.2. MMT with Visual Noise Filtering ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>MMT with Visual Noise Filtering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS2.SSS0.Px1" title="Noise Image Filter ‣ 3.2. MMT with Visual Noise Filtering ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Noise Image Filter</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS2.SSS0.Px2" title="Noise Region Filter ‣ 3.2. MMT with Visual Noise Filtering ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Noise Region Filter</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3" title="3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Supplementary Text Enhanced NMT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3.SSS0.Px1" title="Supplementary Text Retrieval ‣ 3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Supplementary Text Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3.SSS0.Px2" title="Text-Aware Attentive Supplementary Text Encoder ‣ 3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title">Text-Aware Attentive Supplementary Text Encoder</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4" title="4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.SS1" title="4.1. Dataset ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.SS2" title="4.2. Model Implementation ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Model Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.SS3" title="4.3. Training Parameters ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Training Parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.SS4" title="4.4. Baselines ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5" title="5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5.SS1" title="5.1. Translation Performances across Varied Datasets ‣ 5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Translation Performances across Varied Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5.SS2" title="5.2. Influence of the Correlation between Text and Images ‣ 5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Influence of the Correlation between Text and Images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5.SS3" title="5.3. Exploring the Necessity of Visual Modality ‣ 5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Exploring the Necessity of Visual Modality</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S6" title="6. Conclusions ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1" title="Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Qualitative Examples</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content"><div class="section" id="target-section"><div id="license-tr">License: CC Zero</div><div id="watermark-tr">arXiv:2404.06107v1 [cs.CL] 09 Apr 2024</div></div>
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Exploring the Necessity of Visual Modality 
<br class="ltx_break"/>in Multimodal Machine Translation using Authentic Datasets</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Recent research in the field of multimodal machine translation (MMT) has indicated that the visual modality is either dispensable or offers only marginal advantages.
However, most of these conclusions are drawn from the analysis of experimental results based on a limited set of bilingual sentence-image pairs, such as Multi30k.
In these kinds of datasets, the content of one bilingual parallel sentence pair must be well represented by a manually annotated image, which is different from the real-world translation scenario.
In this work, we adhere to the universal multimodal machine translation framework proposed by <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite>.
This approach allows us to delve into the impact of the visual modality on translation efficacy by leveraging real-world translation datasets.
Through a comprehensive exploration via probing tasks, we find that the visual modality proves advantageous for the majority of authentic translation datasets.
Notably, the translation performance primarily hinges on the alignment and coherence between textual and visual contents.
Furthermore, our results suggest that visual information serves a supplementary role in multimodal translation and can be substituted.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id1.id1.1">Keywords: </span>MMT, image retrieval, visual noise filtering, supplementary text retrieval</p>
</div>
<span class="ltx_ERROR undefined" id="id1">\NAT@set@cites</span>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text" id="p1.1.1"></span></p>
</div>
<div class="ltx_para ltx_align_center" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text ltx_font_bold" id="p2.1.1" style="font-size:144%;">Exploring the Necessity of Visual Modality</span></p>
<p class="ltx_p" id="p2.2"><span class="ltx_text ltx_font_bold" id="p2.2.1" style="font-size:144%;">in Multimodal Machine Translation using Authentic Datasets</span></p>
</div>
<div class="ltx_para ltx_align_center" id="p3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_top" id="p3.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="p3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="p3.6.6.6"><span class="ltx_text ltx_font_bold" id="p3.6.6.6.6" style="font-size:120%;">Zi Long<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.1.1.1.1.m1.1"><semantics id="p3.1.1.1.1.m1.1a"><msup id="p3.1.1.1.1.m1.1.1" xref="p3.1.1.1.1.m1.1.1.cmml"><mi id="p3.1.1.1.1.m1.1.1a" xref="p3.1.1.1.1.m1.1.1.cmml"></mi><mn id="p3.1.1.1.1.m1.1.1.1" mathvariant="normal" xref="p3.1.1.1.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.1.1.1.1.m1.1b"><apply id="p3.1.1.1.1.m1.1.1.cmml" xref="p3.1.1.1.1.m1.1.1"><cn id="p3.1.1.1.1.m1.1.1.1.cmml" type="integer" xref="p3.1.1.1.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.1.1.1.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Zhenhao Tang<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.2.2.2.2.m2.1"><semantics id="p3.2.2.2.2.m2.1a"><msup id="p3.2.2.2.2.m2.1.1" xref="p3.2.2.2.2.m2.1.1.cmml"><mi id="p3.2.2.2.2.m2.1.1a" xref="p3.2.2.2.2.m2.1.1.cmml"></mi><mn id="p3.2.2.2.2.m2.1.1.1" mathvariant="normal" xref="p3.2.2.2.2.m2.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.2.2.2.2.m2.1b"><apply id="p3.2.2.2.2.m2.1.1.cmml" xref="p3.2.2.2.2.m2.1.1"><cn id="p3.2.2.2.2.m2.1.1.1.cmml" type="integer" xref="p3.2.2.2.2.m2.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.2.2.2.2.m2.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.2.2.2.2.m2.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Xianghua Fu<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.3.3.3.3.m3.1"><semantics id="p3.3.3.3.3.m3.1a"><msup id="p3.3.3.3.3.m3.1.1" xref="p3.3.3.3.3.m3.1.1.cmml"><mi id="p3.3.3.3.3.m3.1.1a" xref="p3.3.3.3.3.m3.1.1.cmml"></mi><mn id="p3.3.3.3.3.m3.1.1.1" mathvariant="normal" xref="p3.3.3.3.3.m3.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.3.3.3.3.m3.1b"><apply id="p3.3.3.3.3.m3.1.1.cmml" xref="p3.3.3.3.3.m3.1.1"><cn id="p3.3.3.3.3.m3.1.1.1.cmml" type="integer" xref="p3.3.3.3.3.m3.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.3.3.3.3.m3.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.3.3.3.3.m3.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Jian Chen<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.4.4.4.4.m4.1"><semantics id="p3.4.4.4.4.m4.1a"><msup id="p3.4.4.4.4.m4.1.1" xref="p3.4.4.4.4.m4.1.1.cmml"><mi id="p3.4.4.4.4.m4.1.1a" xref="p3.4.4.4.4.m4.1.1.cmml"></mi><mn id="p3.4.4.4.4.m4.1.1.1" mathvariant="normal" xref="p3.4.4.4.4.m4.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.4.4.4.4.m4.1b"><apply id="p3.4.4.4.4.m4.1.1.cmml" xref="p3.4.4.4.4.m4.1.1"><cn id="p3.4.4.4.4.m4.1.1.1.cmml" type="integer" xref="p3.4.4.4.4.m4.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.4.4.4.4.m4.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.4.4.4.4.m4.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Shilong Hou<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.5.5.5.5.m5.1"><semantics id="p3.5.5.5.5.m5.1a"><msup id="p3.5.5.5.5.m5.1.1" xref="p3.5.5.5.5.m5.1.1.cmml"><mi id="p3.5.5.5.5.m5.1.1a" xref="p3.5.5.5.5.m5.1.1.cmml"></mi><mn id="p3.5.5.5.5.m5.1.1.1" mathvariant="normal" xref="p3.5.5.5.5.m5.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.5.5.5.5.m5.1b"><apply id="p3.5.5.5.5.m5.1.1.cmml" xref="p3.5.5.5.5.m5.1.1"><cn id="p3.5.5.5.5.m5.1.1.1.cmml" type="integer" xref="p3.5.5.5.5.m5.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.5.5.5.5.m5.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.5.5.5.5.m5.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>, Jinze Lyu<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.6.6.6.6.m6.1"><semantics id="p3.6.6.6.6.m6.1a"><msup id="p3.6.6.6.6.m6.1.1" xref="p3.6.6.6.6.m6.1.1.cmml"><mi id="p3.6.6.6.6.m6.1.1a" xref="p3.6.6.6.6.m6.1.1.cmml"></mi><mn id="p3.6.6.6.6.m6.1.1.1" mathvariant="normal" xref="p3.6.6.6.6.m6.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.6.6.6.6.m6.1b"><apply id="p3.6.6.6.6.m6.1.1.cmml" xref="p3.6.6.6.6.m6.1.1"><cn id="p3.6.6.6.6.m6.1.1.1.cmml" type="integer" xref="p3.6.6.6.6.m6.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.6.6.6.6.m6.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.6.6.6.6.m6.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="p3.7.7">
<td class="ltx_td ltx_align_center" id="p3.7.7.1">
<math alttext="{}^{1}" class="ltx_Math" display="inline" id="p3.7.7.1.m1.1"><semantics id="p3.7.7.1.m1.1a"><msup id="p3.7.7.1.m1.1.1" xref="p3.7.7.1.m1.1.1.cmml"><mi id="p3.7.7.1.m1.1.1a" xref="p3.7.7.1.m1.1.1.cmml"></mi><mn id="p3.7.7.1.m1.1.1.1" xref="p3.7.7.1.m1.1.1.1.cmml">1</mn></msup><annotation-xml encoding="MathML-Content" id="p3.7.7.1.m1.1b"><apply id="p3.7.7.1.m1.1.1.cmml" xref="p3.7.7.1.m1.1.1"><cn id="p3.7.7.1.m1.1.1.1.cmml" type="integer" xref="p3.7.7.1.m1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.7.7.1.m1.1c">{}^{1}</annotation><annotation encoding="application/x-llamapun" id="p3.7.7.1.m1.1d">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China</td>
</tr>
<tr class="ltx_tr" id="p3.8.8">
<td class="ltx_td ltx_align_center" id="p3.8.8.1">
<math alttext="{}^{2}" class="ltx_Math" display="inline" id="p3.8.8.1.m1.1"><semantics id="p3.8.8.1.m1.1a"><msup id="p3.8.8.1.m1.1.1" xref="p3.8.8.1.m1.1.1.cmml"><mi id="p3.8.8.1.m1.1.1a" xref="p3.8.8.1.m1.1.1.cmml"></mi><mn id="p3.8.8.1.m1.1.1.1" xref="p3.8.8.1.m1.1.1.1.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="p3.8.8.1.m1.1b"><apply id="p3.8.8.1.m1.1.1.cmml" xref="p3.8.8.1.m1.1.1"><cn id="p3.8.8.1.m1.1.1.1.cmml" type="integer" xref="p3.8.8.1.m1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="p3.8.8.1.m1.1c">{}^{2}</annotation><annotation encoding="application/x-llamapun" id="p3.8.8.1.m1.1d">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math>College of Application and Technology, Shenzhen University, Shenzhen, China</td>
</tr>
</tbody>
</table>
<br class="ltx_break"/>
<p class="ltx_p" id="p3.9"><span class="ltx_text ltx_font_italic" id="p3.9.1">Abstract content</span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.   Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the development of neural machine translation (NMT), the role of visual information in machine translation has attracted researchers’ attention <cite class="ltx_cite ltx_citemacro_citep">(Specia et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib45" title="">2016</a>; Elliott et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib22" title="">2017</a>; Barrault et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib7" title="">2018</a>)</cite>.
Different from those text-only NMT <cite class="ltx_cite ltx_citemacro_citep">(Bahdanau et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib5" title="">2014a</a>; Gehring et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib25" title="">2016</a>)</cite>, a bilingual parallel corpora with manual image annotations are used to train an MMT model by an end-to-end framework, and therefore visual information can assist NMT model to achieve better translation performance <cite class="ltx_cite ltx_citemacro_citep">(Calixto and Liu, <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib10" title="">2017</a>; Calixto et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib11" title="">2017</a>; Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Concurrently, researchers have also undertaken a diverse range of experiments in an effort to validate the specific role of visual information in NMT.
For example, <cite class="ltx_cite ltx_citemacro_citet">Grönroos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib27" title="">2018a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Lala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib35" title="">2018</a>)</cite> observed that the robustness of MMT systems remains unaffected when the input image lacks direct relevance to the accompanying text. Notably, the absence of visual features, as highlighted by <cite class="ltx_cite ltx_citemacro_citet">Elliott (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib21" title="">2018</a>)</cite>, also does not yield detrimental effects. <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib54" title="">2021</a>)</cite> underscores that the utilization of the visual modality serves as a regularization mechanism during training rather than serving as a true complement to the textual modality.
Oppositely, <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib9" title="">2019</a>)</cite> delve into the correlation between visual features and text. Their investigation reveals that incorporating the input image aids translation, particularly when certain input words are masked. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib36" title="">2022</a>)</cite> design more detailed probing tasks and found that stronger vision features strengthen MMT systems.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Note that most of the previous conclusions are drawn from the analysis of experimental results based on a restricted selection of manually annotated bilingual sentence-image pairs, known as the Multi30k dataset <cite class="ltx_cite ltx_citemacro_citep">(Elliott et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib24" title="">2016</a>)</cite>.
Within the Multi30k dataset, as depicted in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">1</span></a>, the sentences primarily comprise common and straightforward vocabulary, with each bilingual parallel sentence pair being effectively depicted by a single image.
Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S1.T1" title="Table 1 ‣ 1. Introduction ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">1</span></a> also presents an illustration of a bilingual sentence-image pair extracted from a genuine news report from the United Nations News<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://news.un.org/en/" title="">https://news.un.org/en/</a>
</span></span></span>, alongside examples of sentence pairs derived from other other authentic translation datasets. Evidently, a substantial disparity exists between the Multi30k dataset and the authentic translation data.
Hence, the evidence and findings derived from Multi30k may potentially exhibit inadequate generalizability and offer limited utility when attempting to analyze the role of the visual modality in MMT within real-world translation scenarios.
In these scenarios, sentences often incorporate rare and uncommon words and are only partially depicted by accompanying images.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S1.T1.2.3.1.1">Data source</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.2.3.1.2">Sentences</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.2.3.1.3">Image</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="S1.T1.1.1.2">Multi30k</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.3.1">
<tr class="ltx_tr" id="S1.T1.1.1.3.1.1">
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.1.1.1">​​​EN:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.1.1.3.1.1.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.1.1.3.1.1.2.1">A dog is running in the snow.</p>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.3.1.2">
<td class="ltx_td ltx_align_center" id="S1.T1.1.1.3.1.2.1">​​​DE:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.1.1.3.1.2.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.1.1.3.1.2.2.1">Ein Hund rennt im Schnee.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1">​​​

<table class="ltx_tabular ltx_align_middle" id="S1.T1.1.1.1.1">
<tr class="ltx_tr" id="S1.T1.1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.1.1.1.1.1.1">​​​
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="122" id="S1.T1.1.1.1.1.1.1.g1" src="extracted/5525381/figures/ex1.png" width="141"/>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S1.T1.2.2.2">UN News</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.2.2.3">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.2.2.3.1">
<tr class="ltx_tr" id="S1.T1.2.2.3.1.1">
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.3.1.1.1">​​​EN:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.2.2.3.1.1.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.2.2.3.1.1.2.1">Rescue workers look for survivors in a building in Samada, Syria destroyed by the February 6 earthquake.</p>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2.3.1.2">
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.3.1.2.1">​​​DE:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.2.2.3.1.2.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.2.2.3.1.2.2.1">Rettungskräfte suchen nach Überlebenden in einem Gebäude in Samada, Syrien, das durch das Erdbeben vom 6. Februar zerstört wurde.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.2.2.1">​​​

<table class="ltx_tabular ltx_align_middle" id="S1.T1.2.2.1.1">
<tr class="ltx_tr" id="S1.T1.2.2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.2.1.1.1.1">​​​
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="66" id="S1.T1.2.2.1.1.1.1.g1" src="extracted/5525381/figures/ex2.png" width="144"/>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S1.T1.2.4.1.1">Bible</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.2.4.1.2">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.2.4.1.2.1">
<tr class="ltx_tr" id="S1.T1.2.4.1.2.1.1">
<td class="ltx_td ltx_align_center" id="S1.T1.2.4.1.2.1.1.1">​​​EN:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.2.4.1.2.1.1.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.2.4.1.2.1.1.2.1">I saw, and behold, there was no man,and all the birds of the sky had fled.</p>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.4.1.2.1.2">
<td class="ltx_td ltx_align_center" id="S1.T1.2.4.1.2.1.2.1">​​​DE:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.2.4.1.2.1.2.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.2.4.1.2.1.2.2.1">Ich sah, und siehe, da war kein Mensch, und alle Vögel unter dem Himmel waren weggeflogen.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.2.4.1.3">no image</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S1.T1.2.5.2.1">MultiUN</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.2.5.2.2">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.2.5.2.2.1">
<tr class="ltx_tr" id="S1.T1.2.5.2.2.1.1">
<td class="ltx_td ltx_align_center" id="S1.T1.2.5.2.2.1.1.1">​​​EN:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.2.5.2.2.1.1.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.2.5.2.2.1.1.2.1">Development assistance cannot by itself prevent or end conflict.</p>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.5.2.2.1.2">
<td class="ltx_td ltx_align_center" id="S1.T1.2.5.2.2.1.2.1">​​​DE:</td>
<td class="ltx_td ltx_align_justify" id="S1.T1.2.5.2.2.1.2.2" style="width:170.7pt;">
<p class="ltx_p ltx_align_top" id="S1.T1.2.5.2.2.1.2.2.1">Entwicklungshilfe allein kann Konflikte weder verhüten noch beenden.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S1.T1.2.5.2.3">no image</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison between Multi30k Dataset and Authentic Datasets</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In a recent study, <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> introduced a universal multimodal neural machine translation model that integrates open-vocabulary image retrieval techniques.
In this work, inspired by  <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite>,
we formulate a set of comprehensive probing tasks aimed at assessing the extent to which the visual modality enhances MMT within real-world translation scenarios.
In addition to commonly used Multi30k, we conduct an extensive set of experiments across four authentic text-only translation datasets.
We further evaluated two visual noise filtering approaches based on the correlation between textual and visual content.
Furthermore, we investigate the necessity of visual modality in the current multimodal translation process by substituting visual data with closely equivalent textual content.
To summarize, our findings are:
</p>
<dl class="ltx_description" id="S1.I1">
<dt class="ltx_item" id="S1.I1.ix1"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix1.1.1.1">(1)</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">Visual modality is mostly beneficial for translation, but its effectiveness wanes as text vocabulary becomes less image-friendly.</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.ix2"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix2.1.1.1">(2)</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">The MMT performance depends on the consistency between textual and visual contents, and utilizing filters based on the textual-visual correlation can enhance the performance.</p>
</div>
</dd>
<dt class="ltx_item" id="S1.I1.ix3"><span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I1.ix3.1.1.1">(3)</span></span></dt>
<dd class="ltx_item">
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">Visual information plays a supplementary role in the multimodal translation process and can be substituted by the incorporation of additional textual information.</p>
</div>
</dd>
</dl>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.   Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The integration of extra knowledge to build fine-grained representations is a crucial aspect in language modeling <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib37" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib38" title="">b</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib58" title="">2020</a>)</cite>. Incorporating the visual modality into language modeling has the potential to enhance the machine’s understanding of the real world from a more comprehensive perspective.
Inspired by the studies on the image description generation task <cite class="ltx_cite ltx_citemacro_citep">(Elliott et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib23" title="">2015</a>; Venugopalan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib52" title="">2015</a>; Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib55" title="">2015</a>)</cite>, MMT models have gradually become a hot topic in machine translation research.
In some cases, visual features are directly used as supplementary information to the text presentation. For example, <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib31" title="">2016</a>)</cite> take global visual features and local visual features as additional information for sentences. <cite class="ltx_cite ltx_citemacro_citet">Calixto and Liu (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib10" title="">2017</a>)</cite> initializes the encoder hidden states or decoder hidden states through global visual features.
<cite class="ltx_cite ltx_citemacro_citet">Calixto et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib11" title="">2017</a>)</cite> use an independent attention mechanism to capture visual representations. <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib8" title="">2016</a>)</cite> incorporate spatial visual features into the MMT model via an independent attention mechanism. On this basis, <cite class="ltx_cite ltx_citemacro_citet">Delbrouck and Dupont (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib18" title="">2017b</a>)</cite> employs compact bilinear pooling to fuse two modalities. <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib39" title="">2020</a>)</cite> attempt to introduce the capsule network into MMT, they use the timestep-specific source-side context vector to guide the routing procedure. <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>)</cite> introduce image-text mutual interactions to refine their semantic representations.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="444" id="S2.F1.g1" src="x1.png" width="627"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Frameworks of three probing methods</figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Researchers have also come to recognize the potential redundancy of the visual modality. Inconsequential images exhibit minimal impact on translation quality, and the absence of images does not yield a significant drop in BLEU scores, as noted by <cite class="ltx_cite ltx_citemacro_citet">Elliott (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib21" title="">2018</a>)</cite>. Encouraging findings emerged in the study by <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib9" title="">2019</a>)</cite>. They highlighted the continuing utility of the visual modality in scenarios where linguistic context is limited but noted its diminished sensitivity when exposed to complete sentences.
In a more recent investigation, <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib54" title="">2021</a>)</cite> attributed the observed BLEU improvement in MMT tasks to training regularization. They underscored the importance of constructing appropriate probing tasks with inadequate textual input. It’s important to highlight that the proposed probing task represents an enhanced iteration building upon prior research <cite class="ltx_cite ltx_citemacro_citep">(Caglayan et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib9" title="">2019</a>; Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib54" title="">2021</a>)</cite>.
 <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib36" title="">2022</a>)</cite> made a systematic study on whether stronger vision features are helpful.
All the preceding research has been conducted exclusively on the Multi30k dataset, which has limitations in scale and considerably differs from real-world translation scenarios.
In this study, we employ the framework introduced by  <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> to systematically examine the influence of visual information across various authentic translation datasets, extending our analysis beyond the limitations of the small and specialized Multi30k dataset.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.   Preliminary</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We start with a description of three probing methods employed in this work, which encompass the approach introduced by  <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> and two additional methods derived from it. Figure  <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">1</span></a> shows frameworks of these three methods.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.1.   MMT with Search Engine Based Image Retrieval</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">As depicted in the top section of Figure  <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">1</span></a>,  <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> introduced a search engine-based image retrieval technique and a text-aware attention image encoder. This innovation enables the handling of authentic text-only translation data within MMT systems.
We implement this approach across multiple authentic translation datasets to examine the influence of visual information across datasets with varying styles.
To ensure the comprehensiveness of this paper, this section will provide a brief overview of the approach proposed by Tang et al. (2022).</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Text Encoder</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.4">In this work, we employ a commonly utilized bi-directional LSTM as the RNN text encoder.
For a given sentence denoted as <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">italic_X</annotation></semantics></math>, the output of the text encoder is represented as <math alttext="C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.2.m2.4"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.4a"><mrow id="S3.SS1.SSS0.Px1.p1.2.m2.4.4" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.5" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.5.cmml">C</mi><mo id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.4" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml">(</mo><msub id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2.cmml">𝒉</mi><mn id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.5" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2.cmml">𝒉</mi><mn id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.6" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.7" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.2.cmml">𝒉</mi><mi id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.3.cmml">N</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.4b"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4"><eq id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.4.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.4"></eq><ci id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.5.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.5">𝐶</ci><vector id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3"><apply id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.2">𝒉</ci><cn id="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.2">𝒉</ci><cn id="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">…</ci><apply id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.2">𝒉</ci><ci id="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.4.4.3.3.3.3">𝑁</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.4c">C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.2.m2.4d">italic_C = ( bold_italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_h start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.3.m3.1d">italic_N</annotation></semantics></math> denotes the length of the sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.4.m4.1d">italic_X</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Image Retrieval</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.5">To emphasize the core components of the sentence and mitigate the impact of noise, including stopwords and infrequent words,  <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> utilized the TF-IDF method  <cite class="ltx_cite ltx_citemacro_citep">(Witten et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib53" title="">2005</a>)</cite> to generate search queries for image search engines.
Subsequently, the generated search queries are utilized in image search engines to retrieve the first available image associated with each query.
For each given sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">italic_X</annotation></semantics></math>, <math alttext="M" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.2.m2.1d">italic_M</annotation></semantics></math> search queries denoted as (<math alttext="q_{1},q_{2},\ldots,q_{M}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.3.m3.4"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.4a"><mrow id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.4.cmml"><msub id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2.cmml">q</mi><mn id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.4" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.cmml">q</mi><mn id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.5" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.6" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.2.cmml">q</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.4b"><list id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.4.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3"><apply id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.2">𝑞</ci><cn id="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.3.m3.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.2">𝑞</ci><cn id="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.3.m3.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">…</ci><apply id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.2">𝑞</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.4.4.3.3.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.4c">q_{1},q_{2},\ldots,q_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.3.m3.4d">italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_q start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math>) are generated, and subsequently <math alttext="M" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.4.m4.1d">italic_M</annotation></semantics></math> images represented as (<math alttext="A_{1},A_{2},\ldots,A_{M}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.5.m5.4"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.4a"><mrow id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml"><msub id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.2.cmml">A</mi><mn id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.4" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.cmml">A</mi><mn id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.5" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.6" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.2" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.2.cmml">A</mi><mi id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.4b"><list id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.4.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3"><apply id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.2">𝐴</ci><cn id="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.5.m5.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.2">𝐴</ci><cn id="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p1.5.m5.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1">…</ci><apply id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.2">𝐴</ci><ci id="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.4.4.3.3.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.4c">A_{1},A_{2},\ldots,A_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.5.m5.4d">italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_A start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math>) are retrieved from search engines.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Text-Aware Attentive Visual Encoder</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.14">Each image <math alttext="A_{m}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">A</mi><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2">𝐴</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">A_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.1.m1.1d">italic_A start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> (<math alttext="m=1,\dots,M" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2.3"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.3a"><mrow id="S3.SS1.SSS0.Px3.p1.2.m2.3.4" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.2" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.2.cmml">m</mi><mo id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.2" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.2.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.2.m2.2.2" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.2.m2.2.2.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.2.2" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.2.m2.3.3" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.3.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.3b"><apply id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4"><eq id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.1"></eq><ci id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.2.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.2">𝑚</ci><list id="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.4.3.2"><cn id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">1</cn><ci id="S3.SS1.SSS0.Px3.p1.2.m2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.2.2">…</ci><ci id="S3.SS1.SSS0.Px3.p1.2.m2.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.3.3">𝑀</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.3c">m=1,\dots,M</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.2.m2.3d">italic_m = 1 , … , italic_M</annotation></semantics></math>) is transformed into a <math alttext="196\times 1024" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px3.p1.3.m3.1a"><mrow id="S3.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml">196</mn><mo id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1"><times id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.1"></times><cn id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2">196</cn><cn id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.1c">196\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.3.m3.1d">196 × 1024</annotation></semantics></math> dimensional feature vector using ResNet-50 <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib30" title="">2016</a>)</cite> .
A simple but effective scaled dot-product attention in visual encoder is subsequently employed in the visual encoder to derive a resultant visual representation. Here, we utilize the average pooling <math alttext="C_{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px3.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2.cmml">C</mi><msup id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3a" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml"></mi><mo id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.1" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.1.cmml">′</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.2">𝐶</ci><apply id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3"><ci id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.4.m4.1c">C_{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.4.m4.1d">italic_C start_POSTSUBSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> of the text representation <math alttext="C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.5.m5.4"><semantics id="S3.SS1.SSS0.Px3.p1.5.m5.4a"><mrow id="S3.SS1.SSS0.Px3.p1.5.m5.4.4" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.5" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.5.cmml">C</mi><mo id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.4" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml">(</mo><msub id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.2.cmml">𝒉</mi><mn id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.5" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.2.cmml">𝒉</mi><mn id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.6" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.7" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.2.cmml">𝒉</mi><mi id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.3.cmml">N</mi></msub><mo id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.5.m5.4b"><apply id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4"><eq id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.4"></eq><ci id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.5.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.5">𝐶</ci><vector id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3"><apply id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.2">𝒉</ci><cn id="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.5.m5.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.2">𝒉</ci><cn id="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.5.m5.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1">…</ci><apply id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.2">𝒉</ci><ci id="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.4.4.3.3.3.3">𝑁</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.5.m5.4c">C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.5.m5.4d">italic_C = ( bold_italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_h start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math> as the query, while the visual feature vectors <math alttext="A_{1},A_{2},\ldots,A_{M}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.6.m6.4"><semantics id="S3.SS1.SSS0.Px3.p1.6.m6.4a"><mrow id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.4.cmml"><msub id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.2.cmml">A</mi><mn id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.4" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.2.cmml">A</mi><mn id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.5" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.6.m6.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.6.m6.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.6" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.2.cmml">A</mi><mi id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.6.m6.4b"><list id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3"><apply id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.2">𝐴</ci><cn id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.2">𝐴</ci><cn id="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.6.m6.3.3.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.1.1">…</ci><apply id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.2">𝐴</ci><ci id="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.4.4.3.3.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.6.m6.4c">A_{1},A_{2},\ldots,A_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.6.m6.4d">italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_A start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math> serve as the keys and values in this attention mechanism.
The resultant visual representation <math alttext="A" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.7.m7.1"><semantics id="S3.SS1.SSS0.Px3.p1.7.m7.1a"><mi id="S3.SS1.SSS0.Px3.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.7.m7.1b"><ci id="S3.SS1.SSS0.Px3.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.7.m7.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.7.m7.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.7.m7.1d">italic_A</annotation></semantics></math> is also expressed as a <math alttext="196\times 1024" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.8.m8.1"><semantics id="S3.SS1.SSS0.Px3.p1.8.m8.1a"><mrow id="S3.SS1.SSS0.Px3.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.2" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.2.cmml">196</mn><mo id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.3" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.8.m8.1b"><apply id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1"><times id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.1"></times><cn id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.2.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.2">196</cn><cn id="S3.SS1.SSS0.Px3.p1.8.m8.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.8.m8.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.8.m8.1c">196\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.8.m8.1d">196 × 1024</annotation></semantics></math> dimensional feature vector, which can be regarded as a matrix <math alttext="A=(\boldsymbol{a}_{1},\boldsymbol{a}_{2},\ldots,\boldsymbol{a}_{L})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.9.m9.4"><semantics id="S3.SS1.SSS0.Px3.p1.9.m9.4a"><mrow id="S3.SS1.SSS0.Px3.p1.9.m9.4.4" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.5" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.5.cmml">A</mi><mo id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.4" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml">(</mo><msub id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.2.cmml">𝒂</mi><mn id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.5" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.2.cmml">𝒂</mi><mn id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.6" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.9.m9.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.9.m9.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.7" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.2.cmml">𝒂</mi><mi id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.3.cmml">L</mi></msub><mo id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.9.m9.4b"><apply id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4"><eq id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.4"></eq><ci id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.5.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.5">𝐴</ci><vector id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3"><apply id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.2">𝒂</ci><cn id="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.9.m9.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.2">𝒂</ci><cn id="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.9.m9.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px3.p1.9.m9.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.1.1">…</ci><apply id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.2">𝒂</ci><ci id="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.9.m9.4.4.3.3.3.3">𝐿</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.9.m9.4c">A=(\boldsymbol{a}_{1},\boldsymbol{a}_{2},\ldots,\boldsymbol{a}_{L})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.9.m9.4d">italic_A = ( bold_italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_a start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="L=196" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.10.m10.1"><semantics id="S3.SS1.SSS0.Px3.p1.10.m10.1a"><mrow id="S3.SS1.SSS0.Px3.p1.10.m10.1.1" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.2" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.2.cmml">L</mi><mo id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.1" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.3" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.3.cmml">196</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.10.m10.1b"><apply id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1"><eq id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.1"></eq><ci id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.2">𝐿</ci><cn id="S3.SS1.SSS0.Px3.p1.10.m10.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.10.m10.1.1.3">196</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.10.m10.1c">L=196</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.10.m10.1d">italic_L = 196</annotation></semantics></math> and each <math alttext="\boldsymbol{a}_{l}\in R^{1024}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.11.m11.1"><semantics id="S3.SS1.SSS0.Px3.p1.11.m11.1a"><mrow id="S3.SS1.SSS0.Px3.p1.11.m11.1.1" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.cmml"><msub id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.2" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.2.cmml">𝒂</mi><mi id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.3" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.1" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.2" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.2.cmml">R</mi><mn id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.3" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.3.cmml">1024</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.11.m11.1b"><apply id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1"><in id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.1"></in><apply id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.2">𝒂</ci><ci id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.2.3">𝑙</ci></apply><apply id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.2">𝑅</ci><cn id="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.11.m11.1.1.3.3">1024</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.11.m11.1c">\boldsymbol{a}_{l}\in R^{1024}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.11.m11.1d">bold_italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ italic_R start_POSTSUPERSCRIPT 1024 end_POSTSUPERSCRIPT</annotation></semantics></math> (<math alttext="l=1,\dots,L" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.12.m12.3"><semantics id="S3.SS1.SSS0.Px3.p1.12.m12.3a"><mrow id="S3.SS1.SSS0.Px3.p1.12.m12.3.4" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.2" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.2.cmml">l</mi><mo id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.1" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.2" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.12.m12.1.1" xref="S3.SS1.SSS0.Px3.p1.12.m12.1.1.cmml">1</mn><mo id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.2.1" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.12.m12.2.2" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.12.m12.2.2.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.2.2" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.12.m12.3.3" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.3.cmml">L</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.12.m12.3b"><apply id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4"><eq id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.1.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.1"></eq><ci id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.2.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.2">𝑙</ci><list id="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.4.3.2"><cn id="S3.SS1.SSS0.Px3.p1.12.m12.1.1.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.12.m12.1.1">1</cn><ci id="S3.SS1.SSS0.Px3.p1.12.m12.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m12.2.2">…</ci><ci id="S3.SS1.SSS0.Px3.p1.12.m12.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.12.m12.3.3">𝐿</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.12.m12.3c">l=1,\dots,L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.12.m12.3d">italic_l = 1 , … , italic_L</annotation></semantics></math>).
Visual representation <math alttext="A=(\boldsymbol{a}_{1},\boldsymbol{a}_{2},\ldots,\boldsymbol{a}_{L})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.13.m13.4"><semantics id="S3.SS1.SSS0.Px3.p1.13.m13.4a"><mrow id="S3.SS1.SSS0.Px3.p1.13.m13.4.4" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.5" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.5.cmml">A</mi><mo id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.4" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml">(</mo><msub id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.2.cmml">𝒂</mi><mn id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.5" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.2.cmml">𝒂</mi><mn id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.6" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.13.m13.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.13.m13.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.7" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.2.cmml">𝒂</mi><mi id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.3.cmml">L</mi></msub><mo id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.13.m13.4b"><apply id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4"><eq id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.4"></eq><ci id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.5.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.5">𝐴</ci><vector id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3"><apply id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.2">𝒂</ci><cn id="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.13.m13.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.2">𝒂</ci><cn id="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.13.m13.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px3.p1.13.m13.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.1.1">…</ci><apply id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.2">𝒂</ci><ci id="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.13.m13.4.4.3.3.3.3">𝐿</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.13.m13.4c">A=(\boldsymbol{a}_{1},\boldsymbol{a}_{2},\ldots,\boldsymbol{a}_{L})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.13.m13.4d">italic_A = ( bold_italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_a start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT )</annotation></semantics></math> and text representation <math alttext="C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.14.m14.4"><semantics id="S3.SS1.SSS0.Px3.p1.14.m14.4a"><mrow id="S3.SS1.SSS0.Px3.p1.14.m14.4.4" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.5" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.5.cmml">C</mi><mo id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.4" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml">(</mo><msub id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.2.cmml">𝒉</mi><mn id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.5" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.2.cmml">𝒉</mi><mn id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.6" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.14.m14.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.14.m14.1.1.cmml">…</mi><mo id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.7" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.2.cmml">𝒉</mi><mi id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.3.cmml">N</mi></msub><mo id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.14.m14.4b"><apply id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4"><eq id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.4"></eq><ci id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.5.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.5">𝐶</ci><vector id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3"><apply id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.2">𝒉</ci><cn id="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.14.m14.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.2">𝒉</ci><cn id="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px3.p1.14.m14.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px3.p1.14.m14.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.1.1">…</ci><apply id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.2">𝒉</ci><ci id="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px3.p1.14.m14.4.4.3.3.3.3">𝑁</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.14.m14.4c">C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.14.m14.4d">italic_C = ( bold_italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_h start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math> are then used as the inputs of translation decoder.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Translation Decoder</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.7">For the decoder, we adopt the approach introduced by  <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>)</cite>, implementing both a bidirectional attention network and a co-attention network to effectively capture the underlying semantic interactions between textual and visual elements.
Based on the results of the preliminary experiment, it was evident that transformer-based models did not confer a performance advantage on datasets like Global Voices and other smaller ones. Consequently, we followed the approach of <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> and selected LSTM as our foundational model.
The bidirectional attention network enhances the representations of both text and image.
These enhanced representations are subsequently input into the co-attention network to obtain the time-dependent context vector <math alttext="c_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px4.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.2">𝑐</ci><ci id="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.1.m1.1c">c_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.1.m1.1d">italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and the visual vector <math alttext="v_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px4.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml">v</mi><mi id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.2">𝑣</ci><ci id="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.2.m2.1c">v_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.2.m2.1d">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
Within the co-attention network, we calculate the probability distribution for the next target word <math alttext="y_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px4.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml">y</mi><mi id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.2">𝑦</ci><ci id="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.3.m3.1c">y_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.3.m3.1d">italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> using the previous hidden state <math alttext="s_{t-1}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px4.p1.4.m4.1a"><msub id="S3.SS1.SSS0.Px4.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2.cmml">s</mi><mrow id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.2" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.2.cmml">t</mi><mo id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.1" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.1.cmml">−</mo><mn id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.3" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.2">𝑠</ci><apply id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3"><minus id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.1"></minus><ci id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.2">𝑡</ci><cn id="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS0.Px4.p1.4.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.4.m4.1c">s_{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.4.m4.1d">italic_s start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, the previously generated target word <math alttext="y_{t-1}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.5.m5.1"><semantics id="S3.SS1.SSS0.Px4.p1.5.m5.1a"><msub id="S3.SS1.SSS0.Px4.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2.cmml">y</mi><mrow id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.2" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.2.cmml">t</mi><mo id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.1" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.1.cmml">−</mo><mn id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.3" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.2">𝑦</ci><apply id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3"><minus id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.1"></minus><ci id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.2">𝑡</ci><cn id="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS0.Px4.p1.5.m5.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.5.m5.1c">y_{t-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.5.m5.1d">italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT</annotation></semantics></math>, the time-dependent context vector <math alttext="c_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.6.m6.1"><semantics id="S3.SS1.SSS0.Px4.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px4.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2.cmml">c</mi><mi id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.2">𝑐</ci><ci id="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.6.m6.1c">c_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.6.m6.1d">italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, and the time-dependent visual vector <math alttext="v_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px4.p1.7.m7.1"><semantics id="S3.SS1.SSS0.Px4.p1.7.m7.1a"><msub id="S3.SS1.SSS0.Px4.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2.cmml">v</mi><mi id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px4.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.2">𝑣</ci><ci id="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px4.p1.7.m7.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px4.p1.7.m7.1c">v_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px4.p1.7.m7.1d">italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.2.   MMT with Visual Noise Filtering</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Considering that the noise images obtained from search engines could have a substantial impact on the performance of the MMT system, we further evaluated two visual noise filtering approaches based on the correlation between textual and visual content, as depicted in the central part of Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">1</span></a>.
One approach utilizes the pretrained CLIP model to filter out noise images, while the other employs a region-level image-text attentive filter module to filter out noisy image regions.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Noise Image Filter</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.3">In the CLIP-based noise image filtering approach, we begin by retrieving <math alttext="M^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><msup id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">M</mi><msup id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3a" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml"></mi><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">𝑀</ci><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3"><ci id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">M^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.1.m1.1d">italic_M start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> (<math alttext="M^{{}^{\prime}}&gt;M" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><msup id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">M</mi><msup id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml"><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3a" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml"></mi><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.1.cmml">′</mo></msup></msup><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">&gt;</mo><mi id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><gt id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1"></gt><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">𝑀</ci><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3"><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.1">′</ci></apply></apply><ci id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">M^{{}^{\prime}}&gt;M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.2.m2.1d">italic_M start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT &gt; italic_M</annotation></semantics></math>) images from search engines for each input sentence.
Following this, we calculate the correlation between the input text and the retrieved images using a pretrained CLIP model  <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib42" title="">2021</a>)</cite>.
Subsequently, we select only the top-<math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px1.p1.3.m3.1d">italic_M</annotation></semantics></math> images with the highest correlation to the input source text as the output of the image retrieval process.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Noise Region Filter</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.8">In the noise image region filtering approach, we begin by extracting convolutional feature maps from the top-<math alttext="O" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.1c">O</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.1.m1.1d">italic_O</annotation></semantics></math> most confident regions denoted as (<math alttext="r_{1},\dots,r_{O}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.2.m2.3"><semantics id="S3.SS2.SSS0.Px2.p1.2.m2.3a"><mrow id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml">r</mi><mn id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.3.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml">…</mi><mo id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.4" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.3.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.2" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.2.cmml">r</mi><mi id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.3" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.3.cmml">O</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.2.m2.3b"><list id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2"><apply id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.2">𝑟</ci><cn id="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.2.m2.2.2.1.1.3">1</cn></apply><ci id="S3.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.1.1">…</ci><apply id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.2">𝑟</ci><ci id="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.2.m2.3.3.2.2.3">𝑂</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.2.m2.3c">r_{1},\dots,r_{O}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.2.m2.3d">italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math>) in each collected image.
This is achieved using a pretrained Faster R-CNN model <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib44" title="">2015</a>)</cite>, aiding in the initial filtration of visual information that may be challenging to distinguish as distinct regions in the images.
The image region of each collected image is then represented as a <math alttext="1024" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.3.m3.1a"><mn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.3.m3.1b"><cn id="S3.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.3.m3.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.3.m3.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.3.m3.1d">1024</annotation></semantics></math> dimensional feature vector using ResNet-50. For all the retrieved <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS2.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS2.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS2.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.4.m4.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.4.m4.1d">italic_M</annotation></semantics></math> images, we extract a total of <math alttext="M\times O" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS2.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml">O</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1"><times id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.1"></times><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.2">𝑀</ci><ci id="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.5.m5.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.5.m5.1c">M\times O</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.5.m5.1d">italic_M × italic_O</annotation></semantics></math> regions (<math alttext="r_{1},\dots,r_{M\times O}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.6.m6.3"><semantics id="S3.SS2.SSS0.Px2.p1.6.m6.3a"><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.3.cmml"><msub id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.2.cmml">r</mi><mn id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.3.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.6.m6.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml">…</mi><mo id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.4" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.3.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.2.cmml">r</mi><mrow id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.2.cmml">M</mi><mo id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.3.cmml">O</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.6.m6.3b"><list id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2"><apply id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.2">𝑟</ci><cn id="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.6.m6.2.2.1.1.3">1</cn></apply><ci id="S3.SS2.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.1.1">…</ci><apply id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.2">𝑟</ci><apply id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3"><times id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.1"></times><ci id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.2">𝑀</ci><ci id="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.6.m6.3.3.2.2.3.3">𝑂</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.6.m6.3c">r_{1},\dots,r_{M\times O}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.6.m6.3d">italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_M × italic_O end_POSTSUBSCRIPT</annotation></semantics></math>), resulting in <math alttext="M\times O" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS2.SSS0.Px2.p1.7.m7.1a"><mrow id="S3.SS2.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml">O</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1"><times id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.1"></times><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.2">𝑀</ci><ci id="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.7.m7.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.7.m7.1c">M\times O</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.7.m7.1d">italic_M × italic_O</annotation></semantics></math> feature vectors (<math alttext="\boldsymbol{a}_{1},\dots,\boldsymbol{a}_{M\times O},\boldsymbol{a}_{o}\in R^{1%
024}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.8.m8.4"><semantics id="S3.SS2.SSS0.Px2.p1.8.m8.4a"><mrow id="S3.SS2.SSS0.Px2.p1.8.m8.4.4" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.cmml"><mrow id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.4.cmml"><msub id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.2.cmml">𝒂</mi><mn id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.4" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.4.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.8.m8.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml">…</mi><mo id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.5" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.2.cmml">𝒂</mi><mrow id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.2.cmml">M</mi><mo id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.3.cmml">O</mi></mrow></msub><mo id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.6" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.2.cmml">𝒂</mi><mi id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.3.cmml">o</mi></msub></mrow><mo id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.4" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.4.cmml">∈</mo><msup id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.cmml"><mi id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.2" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.2.cmml">R</mi><mn id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.3" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.3.cmml">1024</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.8.m8.4b"><apply id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4"><in id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.4.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.4"></in><list id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.4.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3"><apply id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.2">𝒂</ci><cn id="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.8.m8.2.2.1.1.1.3">1</cn></apply><ci id="S3.SS2.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.1.1">…</ci><apply id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.2">𝒂</ci><apply id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3"><times id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.1"></times><ci id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.2">𝑀</ci><ci id="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.3.3.2.2.2.3.3">𝑂</ci></apply></apply><apply id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.2">𝒂</ci><ci id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.3.3.3.3">𝑜</ci></apply></list><apply id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.1.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.2.cmml" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.2">𝑅</ci><cn id="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.8.m8.4.4.5.3">1024</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.8.m8.4c">\boldsymbol{a}_{1},\dots,\boldsymbol{a}_{M\times O},\boldsymbol{a}_{o}\in R^{1%
024}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.8.m8.4d">bold_italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_italic_a start_POSTSUBSCRIPT italic_M × italic_O end_POSTSUBSCRIPT , bold_italic_a start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ∈ italic_R start_POSTSUPERSCRIPT 1024 end_POSTSUPERSCRIPT</annotation></semantics></math>).
Subsequently, we compute the correlation score between each image region and the input text using the following equation:
</p>
<table class="ltx_equationgroup ltx_eqn_eqnarray ltx_eqn_table" id="A1.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle S(\boldsymbol{a}_{o},C^{{}^{\prime}})" class="ltx_Math" display="inline" id="S3.Ex1.m1.2"><semantics id="S3.Ex1.m1.2a"><mrow id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.4" xref="S3.Ex1.m1.2.2.4.cmml">S</mi><mo id="S3.Ex1.m1.2.2.3" xref="S3.Ex1.m1.2.2.3.cmml">⁢</mo><mrow id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.3.cmml"><mo id="S3.Ex1.m1.2.2.2.2.3" stretchy="false" xref="S3.Ex1.m1.2.2.2.3.cmml">(</mo><msub id="S3.Ex1.m1.1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.1.1.1.1.1.2" xref="S3.Ex1.m1.1.1.1.1.1.2.cmml">𝒂</mi><mi id="S3.Ex1.m1.1.1.1.1.1.3" xref="S3.Ex1.m1.1.1.1.1.1.3.cmml">o</mi></msub><mo id="S3.Ex1.m1.2.2.2.2.4" xref="S3.Ex1.m1.2.2.2.3.cmml">,</mo><msup id="S3.Ex1.m1.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.2.2.cmml">C</mi><msup id="S3.Ex1.m1.2.2.2.2.2.3" xref="S3.Ex1.m1.2.2.2.2.2.3.cmml"><mi id="S3.Ex1.m1.2.2.2.2.2.3a" xref="S3.Ex1.m1.2.2.2.2.2.3.cmml"></mi><mo id="S3.Ex1.m1.2.2.2.2.2.3.1" xref="S3.Ex1.m1.2.2.2.2.2.3.1.cmml">′</mo></msup></msup><mo id="S3.Ex1.m1.2.2.2.2.5" stretchy="false" xref="S3.Ex1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.2b"><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><times id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.3"></times><ci id="S3.Ex1.m1.2.2.4.cmml" xref="S3.Ex1.m1.2.2.4">𝑆</ci><interval closure="open" id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2"><apply id="S3.Ex1.m1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.1.1.1.2">𝒂</ci><ci id="S3.Ex1.m1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.1.1.1.1.1.3">𝑜</ci></apply><apply id="S3.Ex1.m1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.2.2.2.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2">superscript</csymbol><ci id="S3.Ex1.m1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2.2.2">𝐶</ci><apply id="S3.Ex1.m1.2.2.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.2.2.3"><ci id="S3.Ex1.m1.2.2.2.2.2.3.1.cmml" xref="S3.Ex1.m1.2.2.2.2.2.3.1">′</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.2c">\displaystyle S(\boldsymbol{a}_{o},C^{{}^{\prime}})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.2d">italic_S ( bold_italic_a start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_C start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell"><math alttext="\displaystyle=" class="ltx_Math" display="inline" id="S3.Ex1.m2.1"><semantics id="S3.Ex1.m2.1a"><mo id="S3.Ex1.m2.1.1" xref="S3.Ex1.m2.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.1b"><eq id="S3.Ex1.m2.1.1.cmml" xref="S3.Ex1.m2.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.1c">\displaystyle=</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m2.1d">=</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle V_{a}{\rm tanh}(W_{a}\boldsymbol{a}_{o}+U_{a}C^{{}^{\prime}})" class="ltx_Math" display="inline" id="S3.Ex1.m3.1"><semantics id="S3.Ex1.m3.1a"><mrow id="S3.Ex1.m3.1.1" xref="S3.Ex1.m3.1.1.cmml"><msub id="S3.Ex1.m3.1.1.3" xref="S3.Ex1.m3.1.1.3.cmml"><mi id="S3.Ex1.m3.1.1.3.2" xref="S3.Ex1.m3.1.1.3.2.cmml">V</mi><mi id="S3.Ex1.m3.1.1.3.3" xref="S3.Ex1.m3.1.1.3.3.cmml">a</mi></msub><mo id="S3.Ex1.m3.1.1.2" xref="S3.Ex1.m3.1.1.2.cmml">⁢</mo><mi id="S3.Ex1.m3.1.1.4" xref="S3.Ex1.m3.1.1.4.cmml">tanh</mi><mo id="S3.Ex1.m3.1.1.2a" xref="S3.Ex1.m3.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m3.1.1.1.1" xref="S3.Ex1.m3.1.1.1.1.1.cmml"><mo id="S3.Ex1.m3.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m3.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m3.1.1.1.1.1" xref="S3.Ex1.m3.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m3.1.1.1.1.1.2" xref="S3.Ex1.m3.1.1.1.1.1.2.cmml"><msub id="S3.Ex1.m3.1.1.1.1.1.2.2" xref="S3.Ex1.m3.1.1.1.1.1.2.2.cmml"><mi id="S3.Ex1.m3.1.1.1.1.1.2.2.2" xref="S3.Ex1.m3.1.1.1.1.1.2.2.2.cmml">W</mi><mi id="S3.Ex1.m3.1.1.1.1.1.2.2.3" xref="S3.Ex1.m3.1.1.1.1.1.2.2.3.cmml">a</mi></msub><mo id="S3.Ex1.m3.1.1.1.1.1.2.1" xref="S3.Ex1.m3.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S3.Ex1.m3.1.1.1.1.1.2.3" xref="S3.Ex1.m3.1.1.1.1.1.2.3.cmml"><mi id="S3.Ex1.m3.1.1.1.1.1.2.3.2" xref="S3.Ex1.m3.1.1.1.1.1.2.3.2.cmml">𝒂</mi><mi id="S3.Ex1.m3.1.1.1.1.1.2.3.3" xref="S3.Ex1.m3.1.1.1.1.1.2.3.3.cmml">o</mi></msub></mrow><mo id="S3.Ex1.m3.1.1.1.1.1.1" xref="S3.Ex1.m3.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.Ex1.m3.1.1.1.1.1.3" xref="S3.Ex1.m3.1.1.1.1.1.3.cmml"><msub id="S3.Ex1.m3.1.1.1.1.1.3.2" xref="S3.Ex1.m3.1.1.1.1.1.3.2.cmml"><mi id="S3.Ex1.m3.1.1.1.1.1.3.2.2" xref="S3.Ex1.m3.1.1.1.1.1.3.2.2.cmml">U</mi><mi id="S3.Ex1.m3.1.1.1.1.1.3.2.3" xref="S3.Ex1.m3.1.1.1.1.1.3.2.3.cmml">a</mi></msub><mo id="S3.Ex1.m3.1.1.1.1.1.3.1" xref="S3.Ex1.m3.1.1.1.1.1.3.1.cmml">⁢</mo><msup id="S3.Ex1.m3.1.1.1.1.1.3.3" xref="S3.Ex1.m3.1.1.1.1.1.3.3.cmml"><mi id="S3.Ex1.m3.1.1.1.1.1.3.3.2" xref="S3.Ex1.m3.1.1.1.1.1.3.3.2.cmml">C</mi><msup id="S3.Ex1.m3.1.1.1.1.1.3.3.3" xref="S3.Ex1.m3.1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex1.m3.1.1.1.1.1.3.3.3a" xref="S3.Ex1.m3.1.1.1.1.1.3.3.3.cmml"></mi><mo id="S3.Ex1.m3.1.1.1.1.1.3.3.3.1" xref="S3.Ex1.m3.1.1.1.1.1.3.3.3.1.cmml">′</mo></msup></msup></mrow></mrow><mo id="S3.Ex1.m3.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m3.1b"><apply id="S3.Ex1.m3.1.1.cmml" xref="S3.Ex1.m3.1.1"><times id="S3.Ex1.m3.1.1.2.cmml" xref="S3.Ex1.m3.1.1.2"></times><apply id="S3.Ex1.m3.1.1.3.cmml" xref="S3.Ex1.m3.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m3.1.1.3.1.cmml" xref="S3.Ex1.m3.1.1.3">subscript</csymbol><ci id="S3.Ex1.m3.1.1.3.2.cmml" xref="S3.Ex1.m3.1.1.3.2">𝑉</ci><ci id="S3.Ex1.m3.1.1.3.3.cmml" xref="S3.Ex1.m3.1.1.3.3">𝑎</ci></apply><ci id="S3.Ex1.m3.1.1.4.cmml" xref="S3.Ex1.m3.1.1.4">tanh</ci><apply id="S3.Ex1.m3.1.1.1.1.1.cmml" xref="S3.Ex1.m3.1.1.1.1"><plus id="S3.Ex1.m3.1.1.1.1.1.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.1"></plus><apply id="S3.Ex1.m3.1.1.1.1.1.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2"><times id="S3.Ex1.m3.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.1"></times><apply id="S3.Ex1.m3.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m3.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.Ex1.m3.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.2.2">𝑊</ci><ci id="S3.Ex1.m3.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.2.3">𝑎</ci></apply><apply id="S3.Ex1.m3.1.1.1.1.1.2.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m3.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.Ex1.m3.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.3.2">𝒂</ci><ci id="S3.Ex1.m3.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.2.3.3">𝑜</ci></apply></apply><apply id="S3.Ex1.m3.1.1.1.1.1.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3"><times id="S3.Ex1.m3.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.1"></times><apply id="S3.Ex1.m3.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.Ex1.m3.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.Ex1.m3.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.2.2">𝑈</ci><ci id="S3.Ex1.m3.1.1.1.1.1.3.2.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.2.3">𝑎</ci></apply><apply id="S3.Ex1.m3.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m3.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.Ex1.m3.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.3.2">𝐶</ci><apply id="S3.Ex1.m3.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.3.3"><ci id="S3.Ex1.m3.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex1.m3.1.1.1.1.1.3.3.3.1">′</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m3.1c">\displaystyle V_{a}{\rm tanh}(W_{a}\boldsymbol{a}_{o}+U_{a}C^{{}^{\prime}})</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m3.1d">italic_V start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT roman_tanh ( italic_W start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT bold_italic_a start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_U start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_C start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.14">Here, <math alttext="C^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.9.m1.1"><semantics id="S3.SS2.SSS0.Px2.p1.9.m1.1a"><msup id="S3.SS2.SSS0.Px2.p1.9.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.2.cmml">C</mi><msup id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3a" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.cmml"></mi><mo id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.1" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.9.m1.1b"><apply id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1">superscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.2">𝐶</ci><apply id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3"><ci id="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.9.m1.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.9.m1.1c">C^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.9.m1.1d">italic_C start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> represents the average pooling of the text representation <math alttext="C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.10.m2.4"><semantics id="S3.SS2.SSS0.Px2.p1.10.m2.4a"><mrow id="S3.SS2.SSS0.Px2.p1.10.m2.4.4" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.5" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.5.cmml">C</mi><mo id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.4" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.4.cmml">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml"><mo id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.4" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml">(</mo><msub id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.2.cmml">𝒉</mi><mn id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.5" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.2" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.2.cmml">𝒉</mi><mn id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.3" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.6" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml">,</mo><mi id="S3.SS2.SSS0.Px2.p1.10.m2.1.1" mathvariant="normal" xref="S3.SS2.SSS0.Px2.p1.10.m2.1.1.cmml">…</mi><mo id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.7" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml">,</mo><msub id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.cmml"><mi id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.2" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.2.cmml">𝒉</mi><mi id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.3" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.3.cmml">N</mi></msub><mo id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.8" stretchy="false" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.10.m2.4b"><apply id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4"><eq id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.4.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.4"></eq><ci id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.5.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.5">𝐶</ci><vector id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.4.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3"><apply id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.2">𝒉</ci><cn id="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.10.m2.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.2">𝒉</ci><cn id="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS2.SSS0.Px2.p1.10.m2.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.SSS0.Px2.p1.10.m2.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.1.1">…</ci><apply id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.1.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.2.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.2">𝒉</ci><ci id="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.3.cmml" xref="S3.SS2.SSS0.Px2.p1.10.m2.4.4.3.3.3.3">𝑁</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.10.m2.4c">C=(\boldsymbol{h}_{1},\boldsymbol{h}_{2},\ldots,\boldsymbol{h}_{N})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.10.m2.4d">italic_C = ( bold_italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_h start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math>.
We retain only the visual information from the top-<math alttext="O" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.11.m3.1"><semantics id="S3.SS2.SSS0.Px2.p1.11.m3.1a"><mi id="S3.SS2.SSS0.Px2.p1.11.m3.1.1" xref="S3.SS2.SSS0.Px2.p1.11.m3.1.1.cmml">O</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.11.m3.1b"><ci id="S3.SS2.SSS0.Px2.p1.11.m3.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.11.m3.1.1">𝑂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.11.m3.1c">O</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.11.m3.1d">italic_O</annotation></semantics></math> most relevant regions out of the initially extracted <math alttext="M\times O" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.12.m4.1"><semantics id="S3.SS2.SSS0.Px2.p1.12.m4.1a"><mrow id="S3.SS2.SSS0.Px2.p1.12.m4.1.1" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.2" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.1.cmml">×</mo><mi id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.3" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.3.cmml">O</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.12.m4.1b"><apply id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1"><times id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.1"></times><ci id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.2">𝑀</ci><ci id="S3.SS2.SSS0.Px2.p1.12.m4.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.12.m4.1.1.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.12.m4.1c">M\times O</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.12.m4.1d">italic_M × italic_O</annotation></semantics></math> regions.
This preserved visual information serves as the visual representation for the given input sentence, denoted as
<math alttext="A=\{\boldsymbol{a}_{o}|{S}(\boldsymbol{a}_{o},C^{{}^{\prime}})" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS0.Px2.p1.13.m5.1"><semantics id="S3.SS2.SSS0.Px2.p1.13.m5.1a"><mrow id="S3.SS2.SSS0.Px2.p1.13.m5.1b"><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.1">A</mi><mo id="S3.SS2.SSS0.Px2.p1.13.m5.1.2">=</mo><mrow id="S3.SS2.SSS0.Px2.p1.13.m5.1.3"><mo id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.1" stretchy="false">{</mo><msub id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.2"><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.2.2">𝒂</mi><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.2.3">o</mi></msub><mo fence="false" id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.3" rspace="0.167em" stretchy="false">|</mo><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.4">S</mi><mrow id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5"><mo id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.1" stretchy="false">(</mo><msub id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.2"><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.2.2">𝒂</mi><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.2.3">o</mi></msub><mo id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.3">,</mo><msup id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.4"><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.4.2">C</mi><msup id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.4.3"><mi id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.4.3a"></mi><mo id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.4.3.1">′</mo></msup></msup><mo id="S3.SS2.SSS0.Px2.p1.13.m5.1.3.5.5" stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.13.m5.1c">A=\{\boldsymbol{a}_{o}|{S}(\boldsymbol{a}_{o},C^{{}^{\prime}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.13.m5.1d">italic_A = { bold_italic_a start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT | italic_S ( bold_italic_a start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , italic_C start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT )</annotation></semantics></math> ranks in the top-<math alttext="O,1\leq o\leq M\times O\}" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS0.Px2.p1.14.m6.2"><semantics id="S3.SS2.SSS0.Px2.p1.14.m6.2a"><mrow id="S3.SS2.SSS0.Px2.p1.14.m6.2b"><mi id="S3.SS2.SSS0.Px2.p1.14.m6.1.1">O</mi><mo id="S3.SS2.SSS0.Px2.p1.14.m6.2.3">,</mo><mn id="S3.SS2.SSS0.Px2.p1.14.m6.2.2">1</mn><mo id="S3.SS2.SSS0.Px2.p1.14.m6.2.4">≤</mo><mi id="S3.SS2.SSS0.Px2.p1.14.m6.2.5">o</mi><mo id="S3.SS2.SSS0.Px2.p1.14.m6.2.6">≤</mo><mi id="S3.SS2.SSS0.Px2.p1.14.m6.2.7">M</mi><mo id="S3.SS2.SSS0.Px2.p1.14.m6.2.8" lspace="0.222em" rspace="0.222em">×</mo><mi id="S3.SS2.SSS0.Px2.p1.14.m6.2.9">O</mi><mo id="S3.SS2.SSS0.Px2.p1.14.m6.2.10" stretchy="false">}</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.14.m6.2c">O,1\leq o\leq M\times O\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p1.14.m6.2d">italic_O , 1 ≤ italic_o ≤ italic_M × italic_O }</annotation></semantics></math>
, and it is subsequently fed into the translation decoder module. Less relevant regions are discarded during this process.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">3.3.   Supplementary Text Enhanced NMT</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">As discussed by  <cite class="ltx_cite ltx_citemacro_citet">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib9" title="">2019</a>)</cite>, multimodal translation models typically view visual information as a complementary component to textual information.
However, we raise the question of whether this complementary role can also be achieved by incorporating additional textual information, potentially obviating the need for images in the process.
Hence, our investigation aims to assess the necessity of visual information in the existing multimodal translation process by substituting visual data with nearly equivalent textual information.
As illustrated in the lower section of Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S2.F1" title="Figure 1 ‣ 2. Related Work ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">1</span></a>, we replace the image retrieval module with a supplementary text retrieval module and substitute the text-aware attentive visual encoder with a similar text-aware attentive supplementary text encoder.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Supplementary Text Retrieval</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.6">Similar to the process of retrieving images from search engines, we collected supplementary textual data from search engines.
For every input source sentence <math alttext="X" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">italic_X</annotation></semantics></math>, we follow the same approach as outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px2" title="Image Retrieval ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a> to generate <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.2.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.2.m2.1d">italic_M</annotation></semantics></math> search queries (<math alttext="q_{1},\dots,q_{M}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.3.m3.3"><semantics id="S3.SS3.SSS0.Px1.p1.3.m3.3a"><mrow id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.3.cmml"><msub id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.2" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.2.cmml">q</mi><mn id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.3" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.3" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.3.cmml">,</mo><mi id="S3.SS3.SSS0.Px1.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.4" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.3.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.2" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.2.cmml">q</mi><mi id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.3" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.3.m3.3b"><list id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2"><apply id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.2">𝑞</ci><cn id="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.3.m3.2.2.1.1.3">1</cn></apply><ci id="S3.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.1.1">…</ci><apply id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.1.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.2">𝑞</ci><ci id="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.3.m3.3.3.2.2.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.3.m3.3c">q_{1},\dots,q_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.3.m3.3d">italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_q start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math>).
Subsequently, we collect <math alttext="M" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.4.m4.1c">M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.4.m4.1d">italic_M</annotation></semantics></math> sentences (<math alttext="T_{1},\dots,T_{M}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.5.m5.3"><semantics id="S3.SS3.SSS0.Px1.p1.5.m5.3a"><mrow id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.3.cmml"><msub id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.cmml"><mi id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.2" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.2.cmml">T</mi><mn id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.3" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.3" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.3.cmml">,</mo><mi id="S3.SS3.SSS0.Px1.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml">…</mi><mo id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.4" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.3.cmml">,</mo><msub id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.cmml"><mi id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.2" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.2.cmml">T</mi><mi id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.3" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.3.cmml">M</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.5.m5.3b"><list id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2"><apply id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.2">𝑇</ci><cn id="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.5.m5.2.2.1.1.3">1</cn></apply><ci id="S3.SS3.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.1.1">…</ci><apply id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.1.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.2">𝑇</ci><ci id="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.5.m5.3.3.2.2.3">𝑀</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.5.m5.3c">T_{1},\dots,T_{M}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.5.m5.3d">italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_T start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT</annotation></semantics></math>) that contains all the terms present in the respective search queries (<math alttext="q_{i}\subseteq T_{i},1\leq i\leq M" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.6.m6.2"><semantics id="S3.SS3.SSS0.Px1.p1.6.m6.2a"><mrow id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.3.cmml"><mrow id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.cmml"><msub id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.cmml"><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.2.cmml">q</mi><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.1" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.1.cmml">⊆</mo><msub id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.2.cmml">T</mi><mi id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.3a.cmml">,</mo><mrow id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.cmml"><mn id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.2" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.2.cmml">1</mn><mo id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.3" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.3.cmml">≤</mo><mi id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.4" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.4.cmml">i</mi><mo id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.5" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.5.cmml">≤</mo><mi id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.6" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.6.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.6.m6.2b"><apply id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.3a.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.3">formulae-sequence</csymbol><apply id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1"><subset id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.1"></subset><apply id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.2">𝑞</ci><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3">subscript</csymbol><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.2">𝑇</ci><ci id="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.1.1.1.1.3.3">𝑖</ci></apply></apply><apply id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2"><and id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2a.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2"></and><apply id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2b.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2"><leq id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.3.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.3"></leq><cn id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.2.cmml" type="integer" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.2">1</cn><ci id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.4.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.4">𝑖</ci></apply><apply id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2c.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2"><leq id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.5.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.5"></leq><share href="#S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.4.cmml" id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2d.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2"></share><ci id="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.6.cmml" xref="S3.SS3.SSS0.Px1.p1.6.m6.2.2.2.2.6">𝑀</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.6.m6.2c">q_{i}\subseteq T_{i},1\leq i\leq M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.6.m6.2d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⊆ italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , 1 ≤ italic_i ≤ italic_M</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Text-Aware Attentive Supplementary Text Encoder</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.7">Each gathered supplementary text <math alttext="T_{m}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px2.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml">m</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.1.m1.1.1.3">𝑚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.1.m1.1c">T_{m}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT</annotation></semantics></math> (<math alttext="m=1,\dots,M" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.2.m2.3"><semantics id="S3.SS3.SSS0.Px2.p1.2.m2.3a"><mrow id="S3.SS3.SSS0.Px2.p1.2.m2.3.4" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.cmml"><mi id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.2" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.2.cmml">m</mi><mo id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.1.cmml">=</mo><mrow id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.2" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.1.cmml"><mn id="S3.SS3.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.2.1" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.SSS0.Px2.p1.2.m2.2.2" mathvariant="normal" xref="S3.SS3.SSS0.Px2.p1.2.m2.2.2.cmml">…</mi><mo id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.2.2" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.SS3.SSS0.Px2.p1.2.m2.3.3" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.3.cmml">M</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.2.m2.3b"><apply id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4"><eq id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.1"></eq><ci id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.2.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.2">𝑚</ci><list id="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.4.3.2"><cn id="S3.SS3.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS3.SSS0.Px2.p1.2.m2.1.1">1</cn><ci id="S3.SS3.SSS0.Px2.p1.2.m2.2.2.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.2.2">…</ci><ci id="S3.SS3.SSS0.Px2.p1.2.m2.3.3.cmml" xref="S3.SS3.SSS0.Px2.p1.2.m2.3.3">𝑀</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.2.m2.3c">m=1,\dots,M</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.2.m2.3d">italic_m = 1 , … , italic_M</annotation></semantics></math>) is transformed into a <math alttext="N\times 1024" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS3.SSS0.Px2.p1.3.m3.1a"><mrow id="S3.SS3.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml">N</mi><mo id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1"><times id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.1"></times><ci id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.2">𝑁</ci><cn id="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS3.SSS0.Px2.p1.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.3.m3.1c">N\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.3.m3.1d">italic_N × 1024</annotation></semantics></math> dimensional feature vector using BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib19" title="">2018</a>)</cite>, where <math alttext="N" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS3.SSS0.Px2.p1.4.m4.1a"><mi id="S3.SS3.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.4.m4.1b"><ci id="S3.SS3.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.4.m4.1d">italic_N</annotation></semantics></math> denotes the length of the gathered text data.
To ensure consistency, these textual feature vectors are subsequently padded to match the dimensions of <math alttext="L\times 1024" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS3.SSS0.Px2.p1.5.m5.1a"><mrow id="S3.SS3.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml">L</mi><mo id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.5.m5.1b"><apply id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1"><times id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.1"></times><ci id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.2">𝐿</ci><cn id="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.SS3.SSS0.Px2.p1.5.m5.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.5.m5.1c">L\times 1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.5.m5.1d">italic_L × 1024</annotation></semantics></math> (<math alttext="L=196" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.6.m6.1"><semantics id="S3.SS3.SSS0.Px2.p1.6.m6.1a"><mrow id="S3.SS3.SSS0.Px2.p1.6.m6.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml">L</mi><mo id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml">196</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.6.m6.1b"><apply id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1"><eq id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.1"></eq><ci id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.2">𝐿</ci><cn id="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3.cmml" type="integer" xref="S3.SS3.SSS0.Px2.p1.6.m6.1.1.3">196</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.6.m6.1c">L=196</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.6.m6.1d">italic_L = 196</annotation></semantics></math>), aligning them with the visual feature vectors.
These feature vectors are then integrated into the scaled dot-product attention module as keys and values, with the average pooling <math alttext="C^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px2.p1.7.m7.1"><semantics id="S3.SS3.SSS0.Px2.p1.7.m7.1a"><msup id="S3.SS3.SSS0.Px2.p1.7.m7.1.1" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.cmml"><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.cmml">C</mi><msup id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.cmml"><mi id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3a" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.cmml"></mi><mo id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.1" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.1.cmml">′</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px2.p1.7.m7.1b"><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.2">𝐶</ci><apply id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3"><ci id="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.1.cmml" xref="S3.SS3.SSS0.Px2.p1.7.m7.1.1.3.1">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px2.p1.7.m7.1c">C^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px2.p1.7.m7.1d">italic_C start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> representing the input text serving as the query.
The resultant supplementary text representation is then passed to the translation decoder.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.   Experiment Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.1.   Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conducted experiments on five commonly used machine translation datasets, including multimodal machine translation dataset Multi30k <cite class="ltx_cite ltx_citemacro_citep">(Elliott et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib24" title="">2016</a>)</cite> English-to-German, Global Voices <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann, <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib49" title="">2012</a>)</cite> English-to-German , and WMT’ 16 (100k) English-to-German (Newstest2016 as the test set)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>
To ensure a focused evaluation of the retrieved visual information’s effectiveness, we intentionally sought to minimize the impact of data size. Consequently, we opted to construct our training set by randomly sampling 100,000 sentence pairs from the total pool of 4.5 million sentence pairs. This sampling approach aligns our dataset size more closely with that of other datasets for a fairer assessment.
</span></span></span>
, Bible <cite class="ltx_cite ltx_citemacro_citep">(Christodouloupoulos and Steedman, <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib14" title="">2015</a>)</cite> English-to-German, and MultiUN <cite class="ltx_cite ltx_citemacro_citep">(Eisele and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib20" title="">2010</a>)</cite> English-to-German.
The statistics for each dataset are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.T2" title="Table 2 ‣ 4.1. Dataset ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T2.1.1.1.1">dataset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2">​​​training set</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3">​​​dev set</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.4">​​​test set</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="S4.T2.1.2.1.1">Multi30k</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.2.1.2">29,000</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.2.1.3">1,014</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.2.1.4">1,000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T2.1.3.2.1">Global Voices</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.2">69,227</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.3">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.4">2,000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T2.1.4.3.1">WMT’16 (100k)</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3.2">100,000</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3.3">2,000</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3.4">3,000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T2.1.5.4.1">Bible</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4.2">56,734</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4.3">1,953</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4.4">1,821</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S4.T2.1.6.5.1">MultiUN</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.6.5.2">56,235</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.6.5.3">4,000</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.6.5.4">4,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Statistics of datasets</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_t" colspan="2" id="S4.T3.1.1.1.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1.2">BLEU Score</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id="S4.T3.1.2.2.1">Text-only</td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt" id="S4.T3.1.2.2.2">Bi-LSTM <cite class="ltx_cite ltx_citemacro_citep">(Calixto et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib11" title="">2017</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.2.2.3">33.70</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T3.1.3.3.1">NMT</td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S4.T3.1.3.3.2">Transformer <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib57" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.3.3.3">36.86</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_tt" id="S4.T3.1.4.4.1"></td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt" id="S4.T3.1.4.4.2"><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib57" title="">2019</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.4.4.3">36.86</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T3.1.5.5.1">MMT with</td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S4.T3.1.5.5.2"><cite class="ltx_cite ltx_citemacro_citet">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib59" title="">2021</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.5.5.3">38.40</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T3.1.6.6.1">Original Images</td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S4.T3.1.6.6.2"><cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.6.6.3">39.20</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.7">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T3.1.7.7.1"></td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S4.T3.1.7.7.2">
<cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1" title="3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.7.7.3">38.14</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt" id="S4.T3.1.8.8.1">MMT with</td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_tt" id="S4.T3.1.8.8.2"><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib57" title="">2019</a>)</cite></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.1.8.8.3">36.94</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r" id="S4.T3.1.9.9.1">Retrieved Images</td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S4.T3.1.9.9.2">
<cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite> (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1" title="3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.9.9.3">38.43</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.10.10">
<td class="ltx_td ltx_border_l ltx_border_r" id="S4.T3.1.10.10.1"></td>
<td class="ltx_td ltx_align_left ltx_border_rr ltx_border_t" id="S4.T3.1.10.10.2">MMT with Visual Noise Filtering (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS2" title="3.2. MMT with Visual Noise Filtering ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.2</span></a>)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.10.10.3">38.51</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr ltx_border_tt" colspan="2" id="S4.T3.1.11.11.1">NMT with Retrieved Supplementary Text (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3.SSS0.Px2" title="Text-Aware Attentive Supplementary Text Encoder ‣ 3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.3</span></a>)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S4.T3.1.11.11.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.11.11.2.1">39.13</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Results on Multi30K</figcaption>
</figure>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T4.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S4.T4.1.1.1.2">Dataset</th>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr" id="S4.T4.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.2">Multi30k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.3">Global Voices</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.4">WMT‘16 (100k)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.5">Bible</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.2.2.6">MultiUN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="S4.T4.1.3.1.1">Text-only NMT</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.3.1.2">33.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.3.1.3">9.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.3.1.4">7.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.3.1.5">35.23</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T4.1.3.1.6">39.49</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T4.1.4.2.1">MMT with Random Images</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.2.2">37.65</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.2.3">9.29</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.2.4">8.11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.2.5">35.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.2.6">39.48</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S4.T4.1.5.3.1">MMT with Blank Images</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.3.2">37.79</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.3.3">9.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.3.4">8.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.3.5">35.39</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.5.3.6">39.52</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S4.T4.1.6.4.1">MMT with Retrieved Images</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.2.1">38.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.6.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.3.1">9.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.6.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.4.1">8.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.6.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.5.1">35.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.6.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.1.6.4.6.1">39.53</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Translation performance across diverse datasets under varied image conditions (BLEU score)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.2.   Model Implementation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For image retrieval, we used the Microsoft Bing<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://global.bing.com/images" title="">https://global.bing.com/images</a>
</span></span></span> as the image search engine.
In contrast, for supplementary text retrieval, we gathered sample sentences that included all the terms found in the respective search queries by referencing the Microsoft Bing Dictionary<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.bing.com/dict" title="">https://www.bing.com/dict</a>
</span></span></span>.
As described in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px2" title="Image Retrieval ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3.SSS0.Px1" title="Supplementary Text Retrieval ‣ 3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we set <math alttext="M" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_M</annotation></semantics></math> to 5.
This choice signifies that we formulated 5 search queries and procured 5 images or supplementary text instances<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>
When an insufficient number of sample sentences can be collected, we resort to large pretrained models like ChatGPT to generate sentences that meet the search query.
</span></span></span> for every source language sentence.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Regarding the text encoder, we used a bi-directional RNN with GRU to extract text features. Specifically, we used a 256 dimensional single-layer forward RNN and a 256 dimensional single-layer backward RNN.
For the translation decoder, we adhered to the approach proposed by  <cite class="ltx_cite ltx_citemacro_citet">Su et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>)</cite> and utilized a modified cGRU with hidden states of 256 dimensions.
Furthermore, we configured the embedding sizes for both source and target words to be 128.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.4">As described in Section  <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px3" title="Text-Aware Attentive Visual Encoder ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the visual encoder we employed leveraged the <math alttext="\mathrm{res4f}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">res4f</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">res4f</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathrm{res4f}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">res4f</annotation></semantics></math> layer of a pretrained ResNet-50<cite class="ltx_cite ltx_citemacro_citep">(He et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib30" title="">2016</a>)</cite> model to extract visual features of dimensions <math alttext="196\times 1024" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">196</mn><mo id="S4.SS2.p3.2.m2.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.2.m2.1.1.3" xref="S4.SS2.p3.2.m2.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><times id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1"></times><cn id="S4.SS2.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS2.p3.2.m2.1.1.2">196</cn><cn id="S4.SS2.p3.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p3.2.m2.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">196\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">196 × 1024</annotation></semantics></math>.
Furthermore, as described in Section  <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3.SSS0.Px2" title="Text-Aware Attentive Supplementary Text Encoder ‣ 3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.3</span></a>, the supplementary text encoder utilized a BERT model pretrained on the BooksCorpus<cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib60" title="">2015</a>)</cite> and English Wikipedia<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/English_Wikipedia" title="">https://en.wikipedia.org/wiki/English_Wikipedia</a>
</span></span></span>.
This model was employed to extract textual features of dimensions <math alttext="N\times 1024" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><mrow id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml"><mi id="S4.SS2.p3.3.m3.1.1.2" xref="S4.SS2.p3.3.m3.1.1.2.cmml">N</mi><mo id="S4.SS2.p3.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS2.p3.3.m3.1.1.1.cmml">×</mo><mn id="S4.SS2.p3.3.m3.1.1.3" xref="S4.SS2.p3.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1"><times id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1.1"></times><ci id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2">𝑁</ci><cn id="S4.SS2.p3.3.m3.1.1.3.cmml" type="integer" xref="S4.SS2.p3.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">N\times 1024</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">italic_N × 1024</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mi id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><ci id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">italic_N</annotation></semantics></math> represents the length of the retrieved supplementary text.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.2">Regarding the noise image filter, we set <math alttext="M^{{}^{\prime}}=10" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1.1"><semantics id="S4.SS2.p4.1.m1.1a"><mrow id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><msup id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2.2" xref="S4.SS2.p4.1.m1.1.1.2.2.cmml">M</mi><msup id="S4.SS2.p4.1.m1.1.1.2.3" xref="S4.SS2.p4.1.m1.1.1.2.3.cmml"><mi id="S4.SS2.p4.1.m1.1.1.2.3a" xref="S4.SS2.p4.1.m1.1.1.2.3.cmml"></mi><mo id="S4.SS2.p4.1.m1.1.1.2.3.1" xref="S4.SS2.p4.1.m1.1.1.2.3.1.cmml">′</mo></msup></msup><mo id="S4.SS2.p4.1.m1.1.1.1" xref="S4.SS2.p4.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><eq id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1.1"></eq><apply id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.2.1.cmml" xref="S4.SS2.p4.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.2.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2.2">𝑀</ci><apply id="S4.SS2.p4.1.m1.1.1.2.3.cmml" xref="S4.SS2.p4.1.m1.1.1.2.3"><ci id="S4.SS2.p4.1.m1.1.1.2.3.1.cmml" xref="S4.SS2.p4.1.m1.1.1.2.3.1">′</ci></apply></apply><cn id="S4.SS2.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS2.p4.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">M^{{}^{\prime}}=10</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.1.m1.1d">italic_M start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT ′ end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT = 10</annotation></semantics></math> and used a CLIP model <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib42" title="">2021</a>)</cite> pretrained on the YFCC100M dataset <cite class="ltx_cite ltx_citemacro_citep">(Thomee et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib48" title="">2016</a>)</cite> to filter out noisy images.
For the noise region fitler, we configured it with <math alttext="O=128" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2.1"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mi id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">O</mi><mo id="S4.SS2.p4.2.m2.1.1.1" xref="S4.SS2.p4.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><eq id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1.1"></eq><ci id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">𝑂</ci><cn id="S4.SS2.p4.2.m2.1.1.3.cmml" type="integer" xref="S4.SS2.p4.2.m2.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">O=128</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p4.2.m2.1d">italic_O = 128</annotation></semantics></math>. Here, we utilized a pretrained Faster R-CNN model <cite class="ltx_cite ltx_citemacro_citep">(Ren et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib44" title="">2015</a>)</cite> that had been trained on the Open Images dataset <cite class="ltx_cite ltx_citemacro_citep">(Kuznetsova et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib34" title="">2020</a>)</cite>. This model was employed to identify and filter noisy regions in images effectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.3.   Training Parameters</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">We initiated the word embeddings and other associated model parameters randomly, following a uniform distribution with a range of <math alttext="-0.1" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mo id="S4.SS3.p1.1.m1.1.1a" xref="S4.SS3.p1.1.m1.1.1.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><minus id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></minus><cn id="S4.SS3.p1.1.m1.1.1.2.cmml" type="float" xref="S4.SS3.p1.1.m1.1.1.2">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">-0.1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">- 0.1</annotation></semantics></math> to <math alttext="0.1" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn id="S4.SS3.p1.2.m2.1.1.cmml" type="float" xref="S4.SS3.p1.2.m2.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">0.1</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">0.1</annotation></semantics></math>.
During training, we employed the Adam optimizer with a mini-batch size of 32 and set the learning rate to 0.001. Additionally, a dropout strategy with a rate of 0.3 was applied to further enhance the models.
The training process continued for up to 15 epochs, with early stopping activated if the BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib41" title="">2002</a>)</cite> score on the development set did not exhibit improvement for 3 consecutive epochs. The model with the highest BLEU score on the dev set was selected for evaluation on the test set.
To minimize the impact of random seeds on experimental results and ensure result stability, we conducted the experiment 5 times with fixed random seeds and reported the macro-average of BLEU scores as the final result.
</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">4.4.   Baselines</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In the case of the Multi30k dataset, we conducted a quantitative comparison of the probing methods with several recent MMT models <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib57" title="">2019</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib59" title="">2021</a>; Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite>.
However, the main focus of this research is to evaluate the necessity of visual information within real-world translation scenarios.
Four out of the five datasets utilized in our evaluation experiments are authentic text-only translation datasets without any visual annotation.
Consequently, for each dataset, we exclusively employed the text-only Bi-LSTM <cite class="ltx_cite ltx_citemacro_citep">(Calixto et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib11" title="">2017</a>)</cite> as a baseline.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The baseline model and the models detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3" title="3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3</span></a> were all trained using the same training set and identical training parameters.
For all these models, we present the 4-gram BLEU score <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib41" title="">2002</a>)</cite> as the primary evaluation metric.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.   Results and Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.T3" title="Table 3 ‣ 4.1. Dataset ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3</span></a> presents the experimental results of the Multi30k dataset.
Compared to various baseline models, all three probing methods mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3" title="3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3</span></a> have achieved promising results. Notably, the MMT model with visual noise filtering (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS2" title="3.2. MMT with Visual Noise Filtering ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.2</span></a>) achieved a BLEU score of 38.51, while the NMT model with retrieved supplementary text (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS3.SSS0.Px2" title="Text-Aware Attentive Supplementary Text Encoder ‣ 3.3. Supplementary Text Enhanced NMT ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.3</span></a>) achieved an impressive BLEU score of 39.13.
In comparison to text-only NMT models <cite class="ltx_cite ltx_citemacro_citep">(Calixto et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib11" title="">2017</a>; Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib51" title="">2017</a>)</cite>, the NMT model with retrieved supplementary text significantly outperforms them, showcasing a substantial increase in BLEU score.
When compared to existing MMT methods that utilize original images <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib57" title="">2019</a>; Zhao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib59" title="">2021</a>; Su et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib46" title="">2021</a>)</cite>, the NMT model with retrieved supplementary text obtains a comparable BLEU score.
Furthermore, in contrast to the MMT methods with retrieved images <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib57" title="">2019</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite>, the NMT model with retrieved supplementary text demonstrates performance gains of approximately 2.2 and 0.7 BLEU points, respectively.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Further experimental results and analysis will be presented in the following sections.
</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t" id="S5.T5.1.1.1.1" style="width:99.6pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.2">Multi30k</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.3">Global Voices</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.4">WMT’16 (100k)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.5">Bible</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.1.1.1.6">MultiUN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T5.1.2.1.1" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="S5.T5.1.2.1.1.1">Number of sentences with half or more non-entity keywords</p>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.1.2">27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.1.3">94</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.1.4">796</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.1.5">398</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.2.1.6">818</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S5.T5.1.3.2.1" style="width:99.6pt;">
<p class="ltx_p ltx_align_top" id="S5.T5.1.3.2.1.1">Number of sentences with half of more noise images</p>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.3.2.2">61</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.3.2.3">228</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.3.2.4">685</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.3.2.5">761</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.1.3.2.6">663</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Summary of manual analysis of image retrieval outcomes for each dataset</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.1.   Translation Performances across Varied Datasets</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We firstly quantitatively compared text-only NMT <cite class="ltx_cite ltx_citemacro_citep">(Calixto et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib11" title="">2017</a>)</cite> with MMT utilizing retrieved images (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1" title="3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a>) across five diverse datasets mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.SS1" title="4.1. Dataset ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
As demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.T4" title="Table 4 ‣ 4.1. Dataset ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">4</span></a>, MMT achieved significantly higher BLEU scores on Multi30k, higher BLEU scores on Global Voices and WMT’16 (100k), and slightly higher BLEU scores on Bible and MultiUN.
It is intriguing to note that the improvement in translation performance is substantial on Multi30k, with an increase of approximately 4.7, whereas the gain on MultiUN is relatively modest, at approximately 0.04.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We speculate that the variations in results among the aforementioned translation datasets, such as Multi30k and other datasets, may be attributed to the differing qualities of images collected through the search engine.
To evaluate the influence of the quality of collected images, we train the MMT model with randomly retrieved unrelated images, blank images, and retrieved images from image search engines, respectively.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">The evaluation results are shown in table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S4.T4" title="Table 4 ‣ 4.1. Dataset ‣ 4. Experiment Setup ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">4</span></a>.
It is obvious that MMT models with retrieved images achieves the highest BLEU score on all Multi30k and other four datasets, demonstrating the effectiveness of visual information from retrieved images.
Compared with the model with random images and blank images, the performance gain of collected images is approximately 0.7 &amp; 0.6 BLEU score on Multi30k, and 0.5 &amp; 0.3 BLUE score on Global Voices.
However, on WMT’16 (100k), Bible, and MultiUN datasets, models with retrieved images achieve almost the same BLEU score as the model with blank images.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">One of the possible reason is that sentences from those three datasets contains fewer entity words that can be represented by images, and therefore, the search engine based image retrieval method collects numbers of noise images.
Sentences from WMT’16 (100k), Bible, and MultiUN datasets describe abstract concepts and complex events, while sentences from
Multi30k and Global Voices describe real objects and people, which is more reliable for image retrieval.
<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>
Examples of retrieved images from various datasets are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1.T8" title="Table 8 ‣ Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">8</span></a>.
</span></span></span></p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">To validate the hypotheses, we manually analyzed the image retrieval outcomes of each dataset.
In detail, we initially conducted a random sampling of 1,000 sentences and employed the image retrieval methods outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1.SSS0.Px2" title="Image Retrieval ‣ 3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a> to gather keywords and images for each sentence.
Regarding the extracted keywords, we conducted manual assessments to identify whether each keyword qualifies as an entity word.
Regarding the collected images, we carried out manual evaluations to determine if an image could offer pertinent visual information for the search query, and those that could not.
Images in the latter category were categorized as noise images.
Lastly, we tallied the quantity of sentences containing at least half of non-entity keywords and the quantity of sentences harboring at least half of noise images among the collected images.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">As presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5.T5" title="Table 5 ‣ 5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">5</span></a>, for the Multi30k dataset, out of 1000 sentences, only 27 sentences contained half or more non-entity keywords, and 61 sentences gathered half or more noise images from search engines.
However, in the WMT’16 (100k) dataset, there are 796 sentences with half or more non-entity keywords and 685 sentences with half or more noise images, accounting for more than half of the sampled sentences. Consequently, our method shows poor performance on the WMT’16 (100k) dataset. The Bible dataset and MultiUN dataset exhibit a similar situation.
For the Global Voices dataset, there are 94 sentences with half or more non-entity keywords and 228 sentences with half or more noise images. This falls between the Multi30K and WMT’16 (100k) datasets.
It is interesting to note that the Multi30k dataset, which has the smallest proportions of non-entity keywords and noise images, achieves the most significant gain in translation performance. On the other hand, datasets with the largest proportions of non-entity keywords and noise images show the smallest gain in translation performance.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S5.T6.3.4.1.1">Method</th>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5" id="S5.T6.3.4.1.2">Dataset</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.5.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_rr" id="S5.T6.3.5.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.5.2.2">Multi30k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.5.2.3">Global Voices</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.5.2.4">WMT’16 (100k)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.5.2.5">Bible</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.3.5.2.6">MultiUN</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T6.3.6.3.1">MMT with retrieved images</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.3.6.3.2">38.43</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.3.6.3.3">9.81</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.3.6.3.4">8.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.3.6.3.5">35.42</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.3.6.3.6">39.53</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" id="S5.T6.3.7.4.1"><cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite></th>
<td class="ltx_td ltx_border_r" id="S5.T6.3.7.4.2"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.3.7.4.3"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.3.7.4.4"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.3.7.4.5"></td>
<td class="ltx_td ltx_border_r" id="S5.T6.3.7.4.6"></td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T6.1.1.1">  <math alttext="+" class="ltx_Math" display="inline" id="S5.T6.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.m1.1.1" xref="S5.T6.1.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.m1.1b"><plus id="S5.T6.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.m1.1d">+</annotation></semantics></math> noise image filter</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.1.1.2">38.50</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.1.1.3">10.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.1.1.4">8.89</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.1.1.5">36.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T6.1.1.6">39.91</td>
</tr>
<tr class="ltx_tr" id="S5.T6.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S5.T6.2.2.1">  <math alttext="+" class="ltx_Math" display="inline" id="S5.T6.2.2.1.m1.1"><semantics id="S5.T6.2.2.1.m1.1a"><mo id="S5.T6.2.2.1.m1.1.1" xref="S5.T6.2.2.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.2.1.m1.1b"><plus id="S5.T6.2.2.1.m1.1.1.cmml" xref="S5.T6.2.2.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.2.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.2.1.m1.1d">+</annotation></semantics></math> noise region filter</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.2">38.46</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.3">9.95</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.4">8.78</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.5">35.84</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.2.2.6">39.72</td>
</tr>
<tr class="ltx_tr" id="S5.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S5.T6.3.3.1">  <math alttext="+" class="ltx_Math" display="inline" id="S5.T6.3.3.1.m1.1"><semantics id="S5.T6.3.3.1.m1.1a"><mo id="S5.T6.3.3.1.m1.1.1" xref="S5.T6.3.3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T6.3.3.1.m1.1b"><plus id="S5.T6.3.3.1.m1.1.1.cmml" xref="S5.T6.3.3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.3.3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.T6.3.3.1.m1.1d">+</annotation></semantics></math> noise image &amp; region filter</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.2.1">38.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.3.1">10.23</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.3.3.4"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.4.1">8.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.3.3.5"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.5.1">36.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T6.3.3.6"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.6.1">39.95</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Results of image and region filtering method across diverse datasets (BLEU score)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.2.   Influence of the Correlation between Text and Images</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5.T6" title="Table 6 ‣ 5.1. Translation Performances across Varied Datasets ‣ 5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">6</span></a> shows the evaluation results of applying two filtering approaches described in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS2" title="3.2. MMT with Visual Noise Filtering ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.2</span></a> in MMT.
It is obvious that MMT models with both noise image filter and noise region filter achieves the highest BLEU score across all datasets, including Multi30k and the other four,underscoring the effectiveness of these two filtering approaches.
<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>
A correct example generated by MMT with visual noise filtering is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1.T9" title="Table 9 ‣ Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">9</span></a>.
</span></span></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Notably, it is intriguing to note that the noise filtering techniques exhibited more substantial enhancements in translation performance for the WMT’16 (100k), Bible, and MultiUN datasets, in contrast to the improvements observed in the Multi30k and Global Voices datasets.
This further underscores the significant impact of the correspondence between image and text content on the translation performance the alignment and coherence between image and text content on the translation performance of the MMT system. It also elucidates why noise filtering methods yield marginal improvements on the Multi30K dataset.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">In conclusion, the translation performance of the multimodal model primarily hinges on the consistency between textual and visual content. In other words, the more alignment exists between textual and visual content, the greater enhancement in translation performance with multimodal translation compared to text-only translation.
Hence, we arrive at a conclusion that aligns closely with <cite class="ltx_cite ltx_citemacro_cite">Caglayan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib9" title="">2019</a>)</cite>, which suggest that multimodal translation models predominantly treat visual information as a complement to textual information.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_font_bold ltx_title_subsection" style="font-size:110%;">5.3.   Exploring the Necessity of Visual Modality</h3>
<figure class="ltx_table" id="S5.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T7.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" id="S5.T7.3.4.1.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.4.1.2">​​​BLEU score</td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" id="S5.T7.3.5.2.1">text-only NMT</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.3.5.2.2">33.70</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T7.1.1.1">  <math alttext="+" class="ltx_Math" display="inline" id="S5.T7.1.1.1.m1.1"><semantics id="S5.T7.1.1.1.m1.1a"><mo id="S5.T7.1.1.1.m1.1.1" xref="S5.T7.1.1.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T7.1.1.1.m1.1b"><plus id="S5.T7.1.1.1.m1.1.1.cmml" xref="S5.T7.1.1.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.1.1.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.T7.1.1.1.m1.1d">+</annotation></semantics></math> visual information</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T7.1.1.2">38.43</td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.6.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T7.3.6.3.1">      (MMT with retrieved images)</td>
<td class="ltx_td ltx_border_r" id="S5.T7.3.6.3.2"></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.7.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T7.3.7.4.1">      <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#bib.bib47" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_border_r" id="S5.T7.3.7.4.2"></td>
</tr>
<tr class="ltx_tr" id="S5.T7.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" id="S5.T7.2.2.1">  <math alttext="+" class="ltx_Math" display="inline" id="S5.T7.2.2.1.m1.1"><semantics id="S5.T7.2.2.1.m1.1a"><mo id="S5.T7.2.2.1.m1.1.1" xref="S5.T7.2.2.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T7.2.2.1.m1.1b"><plus id="S5.T7.2.2.1.m1.1.1.cmml" xref="S5.T7.2.2.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.2.2.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.T7.2.2.1.m1.1d">+</annotation></semantics></math> textual information</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T7.2.2.2.1">39.13</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.3.3">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="S5.T7.3.3.1">  <math alttext="+" class="ltx_Math" display="inline" id="S5.T7.3.3.1.m1.1"><semantics id="S5.T7.3.3.1.m1.1a"><mo id="S5.T7.3.3.1.m1.1.1" xref="S5.T7.3.3.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S5.T7.3.3.1.m1.1b"><plus id="S5.T7.3.3.1.m1.1.1.cmml" xref="S5.T7.3.3.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S5.T7.3.3.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S5.T7.3.3.1.m1.1d">+</annotation></semantics></math> visual &amp; textual information</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T7.3.3.2">38.55</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results on Multi30k using visual information or textual information enhanced NMT</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conducted a quantitative comparison between MMT with retrieved images (Section <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S3.SS1" title="3.1. MMT with Search Engine Based Image Retrieval ‣ 3. Preliminary ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">3.1</span></a>) and NMT with retrieved supplementary texts on the Multi30k dataset.
Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#S5.T7" title="Table 7 ‣ 5.3. Exploring the Necessity of Visual Modality ‣ 5. Results and Analysis ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">7</span></a> shows the experimental results. In comparison to MMT model employing images for translation enhancement, the approach integrating supplementary textual data for translation enhancement demonstrated a significantly higher BLEU score of 39.13. Remarkably, the combined utilization of both images and supplementary texts for translation enhancement yielded a BLEU score of 38.55, positioning itself between image-enhanced NMT and text-enhanced NMT.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">This demonstrates that both additional visual and supplementary textual information play an entirely equivalent supplementary role in the translation process. Moreover, in most cases, the utilization of supplementary textual information assists the translation process more effectively.
<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>
A correct example comparing NMT with retrieved supplementary texts to MMT with retrieved images is presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1.T10" title="Table 10 ‣ Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">10</span></a>.
</span></span></span></p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Therefore, we speculate that multimodal translation models trained on a large volume of data might face challenges in outperforming text-only translation models trained on comparable data volumes. This is because as the volume of data used in multimodal model training increases, the potential impact of visual information could diminish. We will verify this in future work.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.   Conclusions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper,
we conduct an in-depth exploration into the role of visual information within the multimodal translation process on Multi30k and other four authentic translation datasets.
Our findings emphasize that the substantial correlation between visual and textual content significantly impacts the efficacy of multimodal translation, while employing filtering mechanisms based on the textual-visual correlation can enhance translation performance.
Additionally, experimental results reveal that visual information plays a supplementary role in the multimodal translation process. This supplementary function of visual information can be substituted by the incorporation of supplementary textual information.
As one of our future work, we plan to assess the impact of the visual modality on more extensive translation datasets, including the complete WMT’16 dataset.
We speculate that as multimodal translation models are trained using larger datasets, the impact of visual information is likely to diminish.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx1.1.1" style="font-size:120%;">Acknowledgements</span></h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This research is supported by the Research Promotion Project of Key Construction Discipline in Guangdong Province (2022ZDJS112).</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section"><span class="ltx_text ltx_font_bold ltx_align_center" id="Sx2.1.1" style="font-size:120%;">References</span></h2>
<span class="ltx_ERROR undefined" id="Sx2.2">\c@NAT@ctr</span>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aho and Ullman (1972)</span>
<span class="ltx_bibblock">
Alfred V. Aho and Jeffrey D. Ullman. 1972.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">The Theory of Parsing, Translation and Compiling</em>, volume 1.

</span>
<span class="ltx_bibblock">Prentice-Hall, Englewood Cliffs, NJ.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">American Psychological Association (1983)</span>
<span class="ltx_bibblock">
American Psychological Association. 1983.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Publications Manual</em>.

</span>
<span class="ltx_bibblock">American Psychological Association, Washington, DC.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ando and Zhang (2005)</span>
<span class="ltx_bibblock">
Rie Kubota Ando and Tong Zhang. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.jmlr.org/papers/volume6/ando05a/ando05a.pdf" title="">A framework for learning predictive structures from multiple tasks and unlabeled data</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Journal of Machine Learning Research</em>, 6:1817–1853.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andrew and Gao (2007)</span>
<span class="ltx_bibblock">
Galen Andrew and Jianfeng Gao. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://dl.acm.org/doi/abs/10.1145/1273496.1273501" title="">Scalable training of <math alttext="L_{1}" class="ltx_Math" display="inline" id="bib.bib4.1.1.m1.1"><semantics id="bib.bib4.1.1.m1.1a"><msub id="bib.bib4.1.1.m1.1.1" xref="bib.bib4.1.1.m1.1.1.cmml"><mi id="bib.bib4.1.1.m1.1.1.2" xref="bib.bib4.1.1.m1.1.1.2.cmml">L</mi><mn id="bib.bib4.1.1.m1.1.1.3" xref="bib.bib4.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="bib.bib4.1.1.m1.1b"><apply id="bib.bib4.1.1.m1.1.1.cmml" xref="bib.bib4.1.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib4.1.1.m1.1.1.1.cmml" xref="bib.bib4.1.1.m1.1.1">subscript</csymbol><ci id="bib.bib4.1.1.m1.1.1.2.cmml" xref="bib.bib4.1.1.m1.1.1.2">𝐿</ci><cn id="bib.bib4.1.1.m1.1.1.3.cmml" type="integer" xref="bib.bib4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.1.1.m1.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="bib.bib4.1.1.m1.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-regularized log-linear models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.1">Proceedings of the 24th International Conference on Machine Learning</em>, pages 33–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2014a)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014a.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and translate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1409.0473</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahdanau et al. (2014b)</span>
<span class="ltx_bibblock">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014b.

</span>
<span class="ltx_bibblock">Neural machine translation by jointly learning to align and translate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:1409.0473</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault et al. (2018)</span>
<span class="ltx_bibblock">
Loïc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018.

</span>
<span class="ltx_bibblock">Findings of the third shared task on multimodal machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, volume 2, pages 308–327.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al. (2016)</span>
<span class="ltx_bibblock">
Ozan Caglayan, Loïc Barrault, and Fethi Bougares. 2016.

</span>
<span class="ltx_bibblock">Multimodal attention for neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1609.03976</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caglayan et al. (2019)</span>
<span class="ltx_bibblock">
Ozan Caglayan, Pranava Madhyastha, Lucia Specia, and Loïc Barrault. 2019.

</span>
<span class="ltx_bibblock">Probing the need for visual context in multimodal machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2019 Conference of the North</em>, pages 4159–4170. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto and Liu (2017)</span>
<span class="ltx_bibblock">
Iacer Calixto and Qun Liu. 2017.

</span>
<span class="ltx_bibblock">Incorporating global visual features into attention-based neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, pages 992–1003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Calixto et al. (2017)</span>
<span class="ltx_bibblock">
Iacer Calixto, Qun Liu, and Nick Campbell. 2017.

</span>
<span class="ltx_bibblock">Doubly-attentive decoder for multi-modal neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1913–1924.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandra et al. (1981)</span>
<span class="ltx_bibblock">
Ashok K. Chandra, Dexter C. Kozen, and Larry J. Stockmeyer. 1981.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/322234.322243" title="">Alternation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of the Association for Computing Machinery</em>, 28(1):114–133.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2014)</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">On the properties of neural machine translation: Encoder-decoder approaches.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1409.1259</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christodouloupoulos and Steedman (2015)</span>
<span class="ltx_bibblock">
Christos Christodouloupoulos and Mark Steedman. 2015.

</span>
<span class="ltx_bibblock">A massively parallel corpus: The Bible in 100 languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Language Resources and Evaluation</em>, 49(2):375–395.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2011)</span>
<span class="ltx_bibblock">
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011.

</span>
<span class="ltx_bibblock">Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</em>, pages 176–181.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cooley and Tukey (1965)</span>
<span class="ltx_bibblock">
James W. Cooley and John W. Tukey. 1965.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf" title="">An algorithm for the machine calculation of complex Fourier series</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Mathematics of Computation</em>, 19(90):297–301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delbrouck and Dupont (2017a)</span>
<span class="ltx_bibblock">
Jean-Benoit Delbrouck and Stéphane Dupont. 2017a.

</span>
<span class="ltx_bibblock">An empirical study on the effectiveness of images in multimodal neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1707.00995</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Delbrouck and Dupont (2017b)</span>
<span class="ltx_bibblock">
Jean-Benoit Delbrouck and Stephane Dupont. 2017b.

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for multimodal neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1703.08084</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eisele and Chen (2010)</span>
<span class="ltx_bibblock">
Andreas Eisele and Yu Chen. 2010.

</span>
<span class="ltx_bibblock">MultiUN: A multilingual corpus from united nation documents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott (2018)</span>
<span class="ltx_bibblock">
Desmond Elliott. 2018.

</span>
<span class="ltx_bibblock">Adversarial evaluation of multimodal machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2974–2978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2017)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Loïc Barrault, Fethi Bougares, and Lucia Specia. 2017.

</span>
<span class="ltx_bibblock">Findings of the second shared task on multimodal machine translation and multilingual image description.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 215–233.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2015)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, and Eva Hasler. 2015.

</span>
<span class="ltx_bibblock">Multilingual image description with neural sequence models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1510.04709</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott et al. (2016)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. 2016.

</span>
<span class="ltx_bibblock">Multi30k: Multilingual english-german image descriptions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1605.00459</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehring et al. (2016)</span>
<span class="ltx_bibblock">
Jonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. 2016.

</span>
<span class="ltx_bibblock">A convolutional encoder model for neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:1611.02344</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves (2012)</span>
<span class="ltx_bibblock">
Alex Graves. 2012.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Supervised sequence labelling with recurrent neural networks</em>, pages 37–45.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grönroos et al. (2018a)</span>
<span class="ltx_bibblock">
Stig-Arne Grönroos, Benoit Huet, Mikko Kurimo, Jorma Laaksonen, Bernard Merialdo, Phu Pham, Mats Sjöberg, Umut Sulubacak, Jörg Tiedemann, Raphael Troncy, and Raúl Vázquez. 2018a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6439" title="">The MeMAD submission to the WMT18 multimodal translation task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 603–611, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grönroos et al. (2018b)</span>
<span class="ltx_bibblock">
Stig-Arne Grönroos, Benoit Huet, Mikko Kurimo, Jorma Laaksonen, Bernard Merialdo, Phu Pham, Mats Sjöberg, Umut Sulubacak, Jörg Tiedemann, Raphael Troncy, et al. 2018b.

</span>
<span class="ltx_bibblock">The memad submission to the wmt18 multimodal translation task.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:1808.10802</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gusfield (1997)</span>
<span class="ltx_bibblock">
Dan Gusfield. 1997.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.cambridge.org/core/books/algorithms-on-strings-trees-and-sequences/F0B095049C7E6EF5356F0A26686C20D3" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1.1">Algorithms on Strings, Trees and Sequences</em></a>.

</span>
<span class="ltx_bibblock">Cambridge University Press, Cambridge, UK.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2016)</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 770–778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2016)</span>
<span class="ltx_bibblock">
Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean Oh, and Chris Dyer. 2016.

</span>
<span class="ltx_bibblock">Attention-based multimodal neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers</em>, pages 639–645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ive et al. (2019)</span>
<span class="ltx_bibblock">
Julia Ive, Pranava Madhyastha, and Lucia Specia. 2019.

</span>
<span class="ltx_bibblock">Distilling translations with visual awareness.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:1906.07701</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein et al. (2017)</span>
<span class="ltx_bibblock">
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017.

</span>
<span class="ltx_bibblock">Opennmt: Open-source toolkit for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of ACL 2017, System Demonstrations</em>, pages 67–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuznetsova et al. (2020)</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. 2020.

</span>
<span class="ltx_bibblock">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">International Journal of Computer Vision</em>, 128(7):1956–1981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lala et al. (2018)</span>
<span class="ltx_bibblock">
Chiraag Lala, Pranava Swaroop Madhyastha, Carolina Scarton, and Lucia Specia. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6442" title="">Sheffield submissions for WMT18 multimodal translation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 624–631, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2022)</span>
<span class="ltx_bibblock">
Bei Li, Chuanhao Lv, Zefan Zhou, Tao Zhou, Tong Xiao, Anxiang Ma, and JingBo Zhu. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.438" title="">On vision features in multimodal machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 6327–6337, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020a)</span>
<span class="ltx_bibblock">
Zuchao Li, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Zhuosheng Zhang, and Hai Zhao. 2020a.

</span>
<span class="ltx_bibblock">Explicit sentence compression for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, pages 8311–8318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020b)</span>
<span class="ltx_bibblock">
Zuchao Li, Rui Wang, Kehai Chen, Masso Utiyama, Eiichiro Sumita, Zhuosheng Zhang, and Hai Zhao. 2020b.

</span>
<span class="ltx_bibblock">Data-dependent gaussian prior objective for language generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Eighth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2020)</span>
<span class="ltx_bibblock">
Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, Zhengyuan Yang, Yubin Ge, Jie Zhou, and Jiebo Luo. 2020.

</span>
<span class="ltx_bibblock">Dynamic context-guided capsule network for multimodal machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 28th ACM International Conference on Multimedia</em>, pages 1320–1329.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">European conference on computer vision</em>, pages 740–755. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">International conference on machine learning</em>, pages 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasooli and Tetreault (2015)</span>
<span class="ltx_bibblock">
Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1503.06733" title="">Yara parser: A fast and accurate dependency parser</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Computing Research Repository</em>, arXiv:1503.06733.

</span>
<span class="ltx_bibblock">Version 2.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Advances in neural information processing systems</em>, 28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia et al. (2016)</span>
<span class="ltx_bibblock">
Lucia Specia, Stella Frank, Khalil Sima’An, and Desmond Elliott. 2016.

</span>
<span class="ltx_bibblock">A shared task on multimodal machine translation and crosslingual image description.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers</em>, pages 543–553.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al. (2021)</span>
<span class="ltx_bibblock">
Jinsong Su, Jinchang Chen, Hui Jiang, Chulun Zhou, Huan Lin, Yubin Ge, Qingqiang Wu, and Yongxuan Lai. 2021.

</span>
<span class="ltx_bibblock">Multi-modal neural machine translation with deep semantic interactions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Information Sciences</em>, 554:47–60.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
ZhenHao Tang, XiaoBing Zhang, Zi Long, and XiangHua Fu. 2022.

</span>
<span class="ltx_bibblock">Multimodal neural machine translation with search engine based image retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 9th Workshop on Asian Translation</em>, pages 89–98.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomee et al. (2016)</span>
<span class="ltx_bibblock">
Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. 2016.

</span>
<span class="ltx_bibblock">Yfcc100m: The new data in multimedia research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Communications of the ACM</em>, 59(2):64–73.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2012)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2012.

</span>
<span class="ltx_bibblock">Parallel data, tools and interfaces in opus.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12)</em>, Istanbul, Turkey. European Language Resources Association (ELRA).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turney (2000)</span>
<span class="ltx_bibblock">
Peter D Turney. 2000.

</span>
<span class="ltx_bibblock">Learning algorithms for keyphrase extraction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Information retrieval</em>, 2(4):303–336.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venugopalan et al. (2015)</span>
<span class="ltx_bibblock">
Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Donahue, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2015.

</span>
<span class="ltx_bibblock">Sequence to sequence-video to text.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 4534–4542.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Witten et al. (2005)</span>
<span class="ltx_bibblock">
Ian H Witten, Gordon W Paynter, Eibe Frank, Carl Gutwin, and Craig G Nevill-Manning. 2005.

</span>
<span class="ltx_bibblock">Kea: Practical automated keyphrase extraction.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Design and Usability of Digital Libraries: Case Studies in the Asia Pacific</em>, pages 129–152. IGI global.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Zhiyong Wu, Lingpeng Kong, Wei Bi, Xiang Li, and Ben Kao. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.480" title="">Good for misconceived reasons: An empirical revisiting on the need for visual context in multimodal machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 6153–6166, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2015)</span>
<span class="ltx_bibblock">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. 2015.

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual attention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">International conference on machine learning</em>, pages 2048–2057. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2017)</span>
<span class="ltx_bibblock">
Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura. 2017.

</span>
<span class="ltx_bibblock">Nict-naist system for wmt17 multimodal translation task.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 477–482.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2019)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao. 2019.

</span>
<span class="ltx_bibblock">Neural machine translation with universal visual representation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, and Xiang Zhou. 2020.

</span>
<span class="ltx_bibblock">Semantics-aware bert for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, pages 9628–9635.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2021)</span>
<span class="ltx_bibblock">
Yuting Zhao, Mamoru Komachi, Tomoyuki Kajiwara, and Chenhui Chu. 2021.

</span>
<span class="ltx_bibblock">Word-region alignment-guided multimodal neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 30:244–259.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2015)</span>
<span class="ltx_bibblock">
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015.

</span>
<span class="ltx_bibblock">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the IEEE international conference on computer vision</em>, pages 19–27.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Qualitative Examples</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In this appendix, we provide examples of retrieved images (Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1.T8" title="Table 8 ‣ Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">8</span></a>), as well as translation examples for MMT with visual noise filtering (Table <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1.T9" title="Table 9 ‣ Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">9</span></a>) and NMT with retrieved supplementary texts (Table  <a class="ltx_ref" href="https://arxiv.org/html/2404.06107v1#A1.T10" title="Table 10 ‣ Appendix A Qualitative Examples ‣ Exploring the Necessity of Visual Modality in Multimodal Machine Translation using Authentic Datasets"><span class="ltx_text ltx_ref_tag">10</span></a>).</p>
</div>
<figure class="ltx_table" id="A1.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T8.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.5.6.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="A1.T8.5.6.1.1">Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.5.6.1.2">English Sentence</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T8.5.6.1.3">One of five retrieved images</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" id="A1.T8.1.1.2">Multi30k</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="A1.T8.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.1.1.3.1">
<tr class="ltx_tr" id="A1.T8.1.1.3.1.1">
<td class="ltx_td ltx_align_justify" id="A1.T8.1.1.3.1.1.1" style="width:128.0pt;">
<p class="ltx_p ltx_align_top" id="A1.T8.1.1.3.1.1.1.1">The person in the striped shirt is mountain climbing.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T8.1.1.1">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.1.1.1.1">
<tr class="ltx_tr" id="A1.T8.1.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.1.1.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="115" id="A1.T8.1.1.1.1.1.1.g1" src="extracted/5525381/figures/f1.png" width="226"/></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="A1.T8.2.2.2">
<span class="ltx_rule" style="width:0.0pt;height:54.1pt;position:relative; bottom:-2.0pt;background:black;display:inline-block;"></span>
Global Voices</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T8.2.2.3">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.2.2.3.1">
<tr class="ltx_tr" id="A1.T8.2.2.3.1.1">
<td class="ltx_td ltx_align_justify" id="A1.T8.2.2.3.1.1.1" style="width:128.0pt;">
<p class="ltx_p ltx_align_top" id="A1.T8.2.2.3.1.1.1.1">Now the city is under a siege from the security forces.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.2.2.1">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.2.2.1.1">
<tr class="ltx_tr" id="A1.T8.2.2.1.1.1">
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="148" id="A1.T8.2.2.1.1.1.1.g1" src="extracted/5525381/figures/f2.png" width="227"/></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="A1.T8.3.3.2">
<span class="ltx_rule" style="width:0.0pt;height:54.1pt;position:relative; bottom:-2.0pt;background:black;display:inline-block;"></span>
WMT’16 (100k)</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T8.3.3.3">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.3.3.3.1">
<tr class="ltx_tr" id="A1.T8.3.3.3.1.1">
<td class="ltx_td ltx_align_justify" id="A1.T8.3.3.3.1.1.1" style="width:128.0pt;">
<p class="ltx_p ltx_align_top" id="A1.T8.3.3.3.1.1.1.1">In the future, integration will be a topic for the whole of society even more than it is today.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.1">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.3.3.1.1">
<tr class="ltx_tr" id="A1.T8.3.3.1.1.1">
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="150" id="A1.T8.3.3.1.1.1.1.g1" src="extracted/5525381/figures/f3.png" width="228"/></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="A1.T8.4.4.2">Bible</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T8.4.4.3">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.4.4.3.1">
<tr class="ltx_tr" id="A1.T8.4.4.3.1.1">
<td class="ltx_td ltx_align_justify" id="A1.T8.4.4.3.1.1.1" style="width:128.0pt;">
<p class="ltx_p ltx_align_top" id="A1.T8.4.4.3.1.1.1.1">You are Yahweh, even you alone. You have made heaven. the heaven of heavens, with all their army, the earth and all things that are on it, the seas and all that is in them and you preserve them all.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.4.4.1">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.4.4.1.1">
<tr class="ltx_tr" id="A1.T8.4.4.1.1.1">
<td class="ltx_td ltx_align_center" id="A1.T8.4.4.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="116" id="A1.T8.4.4.1.1.1.1.g1" src="extracted/5525381/figures/f4.png" width="231"/></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T8.5.5.2">MultiUN</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A1.T8.5.5.3">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.5.5.3.1">
<tr class="ltx_tr" id="A1.T8.5.5.3.1.1">
<td class="ltx_td ltx_align_justify" id="A1.T8.5.5.3.1.1.1" style="width:128.0pt;">
<p class="ltx_p ltx_align_top" id="A1.T8.5.5.3.1.1.1.1">Development assistance cannot by itself prevent or end conflict.</p>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T8.5.5.1">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.5.5.1.1">
<tr class="ltx_tr" id="A1.T8.5.5.1.1.1">
<td class="ltx_td ltx_align_center" id="A1.T8.5.5.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="101" id="A1.T8.5.5.1.1.1.1.g1" src="extracted/5525381/figures/f5.png" width="104"/></td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Examples of retrieved image from different datasets. For the sentence from Multi30k dataset, our method efficiently retrieves an image that accurately represents the sentence’s content “A man is rock climbing”. For the sentence from Global Voice dataset, the retrieved image exhibits a degree of alignment with the source sentences, encompassing elements like “city”,“siege” and“forces”. However, for the sentence from WMT’16 (100k), Bible and MultiUN datasets, it becomes evident that the retrieved images offer limited relevant visual information and thus provide little assistance for translation.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T9.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.10.11.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T9.10.11.1.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.11.1.1.1">Source (En)</p>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T9.10.11.1.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.11.1.2.1">But he answered and said, "Every plant which my heavenly Father didn’t plant will be uprooted.</p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.10.12.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T9.10.12.1.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.12.1.1.1">Target (De)</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A1.T9.10.12.1.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.12.1.2.1">Aber er antwortete und sprach: Alle Pflanzen, die mein himmlischer Vater nicht pflanzte, die werden ausgereutet.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.5.5">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="A1.T9.5.5.6" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.5.5.6.1">Retrieved images</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A1.T9.5.5.5" style="width:284.5pt;">
<table class="ltx_tabular ltx_align_top" id="A1.T9.5.5.5.5.5.5">
<tr class="ltx_tr" id="A1.T9.3.3.3.3.3.3.3">
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.1.1.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="112" id="A1.T9.1.1.1.1.1.1.1.1.g1" src="extracted/5525381/figures/prefilter_1.png" width="113"/></td>
<td class="ltx_td ltx_align_center" id="A1.T9.2.2.2.2.2.2.2.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="111" id="A1.T9.2.2.2.2.2.2.2.2.g1" src="extracted/5525381/figures/prefilter_2.png" width="110"/></td>
<td class="ltx_td ltx_align_center" id="A1.T9.3.3.3.3.3.3.3.3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="117" id="A1.T9.3.3.3.3.3.3.3.3.g1" src="extracted/5525381/figures/prefilter_3.png" width="116"/></td>
</tr>
<tr class="ltx_tr" id="A1.T9.5.5.5.5.5.5.5">
<td class="ltx_td ltx_align_center" id="A1.T9.4.4.4.4.4.4.4.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="110" id="A1.T9.4.4.4.4.4.4.4.1.g1" src="extracted/5525381/figures/prefilter_4.png" width="140"/></td>
<td class="ltx_td ltx_align_center" id="A1.T9.5.5.5.5.5.5.5.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="103" id="A1.T9.5.5.5.5.5.5.5.2.g1" src="extracted/5525381/figures/prefilter_5.png" width="167"/></td>
<td class="ltx_td" id="A1.T9.5.5.5.5.5.5.5.3"></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.10.13.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T9.10.13.2.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.13.2.1.1">MMT with retrieved images</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A1.T9.10.13.2.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.13.2.2.1">Er antwortete aber und sprach: Alle Pflanzen, die mein himmlischer Vater nicht verderbte Quelle.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.10.10">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="A1.T9.10.10.6" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.10.6.1">Retrieved images with noise image filter</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A1.T9.10.10.5" style="width:284.5pt;">
<table class="ltx_tabular ltx_align_top" id="A1.T9.10.10.5.5.5.5">
<tr class="ltx_tr" id="A1.T9.8.8.3.3.3.3.3">
<td class="ltx_td ltx_align_center" id="A1.T9.6.6.1.1.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="103" id="A1.T9.6.6.1.1.1.1.1.1.g1" src="extracted/5525381/figures/aftfilter_3.png" width="103"/></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.7.2.2.2.2.2.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="102" id="A1.T9.7.7.2.2.2.2.2.2.g1" src="extracted/5525381/figures/aftfilter_4.png" width="102"/></td>
<td class="ltx_td ltx_align_center" id="A1.T9.8.8.3.3.3.3.3.3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="103" id="A1.T9.8.8.3.3.3.3.3.3.g1" src="extracted/5525381/figures/prefilter_5.png" width="167"/></td>
</tr>
<tr class="ltx_tr" id="A1.T9.10.10.5.5.5.5.5">
<td class="ltx_td ltx_align_center" id="A1.T9.9.9.4.4.4.4.4.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="112" id="A1.T9.9.9.4.4.4.4.4.1.g1" src="extracted/5525381/figures/prefilter_1.png" width="113"/></td>
<td class="ltx_td ltx_align_center" id="A1.T9.10.10.5.5.5.5.5.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="111" id="A1.T9.10.10.5.5.5.5.5.2.g1" src="extracted/5525381/figures/prefilter_2.png" width="110"/></td>
<td class="ltx_td" id="A1.T9.10.10.5.5.5.5.5.3"></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.10.14.3">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T9.10.14.3.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.14.3.1.1">MMT with noise image filter</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A1.T9.10.14.3.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.14.3.2.1">Er antwortete aber und sprach: Alle Pflanzen, die mein himmlischer Vater nicht pflanzte.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.10.15.4">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="A1.T9.10.15.4.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.15.4.1.1">MMT with both noise image and region filter</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="A1.T9.10.15.4.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T9.10.15.4.2.1">Er antwortete aber und sprach: Alle Pflanzen, die mein himmlischer Vater nicht pflanzte, wird entwurzelt werden.</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>A correct example generated by MMT with visual noise filtering. Due to its unique characteristics, the Bible dataset contains numerous entity words but is challenging to obtain images that effectively represent the textual content. However, visual noise filtering based on visual-text correlation can partially alleviate this situation. In this example, the filtered visual information has enabled the translation of “uprooted” to be correct.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T10.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T10.5.6.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T10.5.6.1.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.6.1.1.1">Source (En)</p>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T10.5.6.1.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.6.1.2.1">Group of Asian boys wait for meat to cook over barbecue.</p>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T10.5.7.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T10.5.7.1.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.7.1.1.1">Target (De)</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A1.T10.5.7.1.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.7.1.2.1">Eine Gruppe asiatischer Jungen wartet am Grill darauf, dass Fleisch gar wird.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.8.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="A1.T10.5.8.2.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.8.2.1.1">Text-only NMT</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A1.T10.5.8.2.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.8.2.2.1">Eine asiatische Jungen warten auf dem Fleisch, um den Grill zu kochen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.5">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="A1.T10.5.5.6" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.5.6.1">Retrieved images</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A1.T10.5.5.5" style="width:284.5pt;">
<table class="ltx_tabular ltx_align_top" id="A1.T10.5.5.5.5.5.5">
<tr class="ltx_tr" id="A1.T10.3.3.3.3.3.3.3">
<td class="ltx_td ltx_align_center" id="A1.T10.1.1.1.1.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="94" id="A1.T10.1.1.1.1.1.1.1.1.g1" src="extracted/5525381/figures/text_1.png" width="141"/></td>
<td class="ltx_td ltx_align_center" id="A1.T10.2.2.2.2.2.2.2.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="92" id="A1.T10.2.2.2.2.2.2.2.2.g1" src="extracted/5525381/figures/text_2.png" width="138"/></td>
<td class="ltx_td ltx_align_center" id="A1.T10.3.3.3.3.3.3.3.3"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="93" id="A1.T10.3.3.3.3.3.3.3.3.g1" src="extracted/5525381/figures/text_3.png" width="140"/></td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.5.5.5.5.5.5">
<td class="ltx_td ltx_align_center" id="A1.T10.4.4.4.4.4.4.4.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="94" id="A1.T10.4.4.4.4.4.4.4.1.g1" src="extracted/5525381/figures/text_4.png" width="140"/></td>
<td class="ltx_td ltx_align_center" id="A1.T10.5.5.5.5.5.5.5.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="99" id="A1.T10.5.5.5.5.5.5.5.2.g1" src="extracted/5525381/figures/text_5.png" width="126"/></td>
<td class="ltx_td" id="A1.T10.5.5.5.5.5.5.5.3"></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.9.3">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="A1.T10.5.9.3.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.9.3.1.1">MMT with retrieved images</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" id="A1.T10.5.9.3.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.9.3.2.1">Eine Gruppe von asiatischen Jungen wartet darauf, um Fleisch zu grillen.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.10.4">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt" id="A1.T10.5.10.4.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.10.4.1.1">Retrieved supplementary texts</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_tt" id="A1.T10.5.10.4.2" style="width:284.5pt;">
<table class="ltx_tabular ltx_align_top" id="A1.T10.5.10.4.2.1">
<tr class="ltx_tr" id="A1.T10.5.10.4.2.1.1">
<td class="ltx_td ltx_align_right" id="A1.T10.5.10.4.2.1.1.1">​​​(1)</td>
<td class="ltx_td ltx_align_justify" id="A1.T10.5.10.4.2.1.1.2" style="width:264.6pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.10.4.2.1.1.2.1">Delivery is hardly limited to pizza at this point; everything from sushi to barbecue seems available as a to-go order.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.10.4.2.1.2">
<td class="ltx_td ltx_align_right" id="A1.T10.5.10.4.2.1.2.1">​​​(2)</td>
<td class="ltx_td ltx_align_justify" id="A1.T10.5.10.4.2.1.2.2" style="width:264.6pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.10.4.2.1.2.2.1">While the savory aroma of barbecue filled the air, friends and family gathered around the grill, eagerly sharing stories and laughter as they waited for the delicious meal to be ready.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.10.4.2.1.3">
<td class="ltx_td ltx_align_right" id="A1.T10.5.10.4.2.1.3.1">​​​(3)</td>
<td class="ltx_td ltx_align_justify" id="A1.T10.5.10.4.2.1.3.2" style="width:264.6pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.10.4.2.1.3.2.1">As the sun dipped below the horizon, our group of friends decided to have a barbecue in the backyard, lighting up the grill and eagerly waiting for the charcoal to heat up so that we could start cooking our favorite dishes.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.10.4.2.1.4">
<td class="ltx_td ltx_align_right" id="A1.T10.5.10.4.2.1.4.1">​​​(4)</td>
<td class="ltx_td ltx_align_justify" id="A1.T10.5.10.4.2.1.4.2" style="width:264.6pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.10.4.2.1.4.2.1">At the lively outdoor barbecue gathering, a diverse group of friends, including a talented Asian chef, couldn’t wait to cook up a mouthwatering feast.</p>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.10.4.2.1.5">
<td class="ltx_td ltx_align_right" id="A1.T10.5.10.4.2.1.5.1">​​​(5)</td>
<td class="ltx_td ltx_align_justify" id="A1.T10.5.10.4.2.1.5.2" style="width:264.6pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.10.4.2.1.5.2.1">While the enthusiastic Asian group gathered around the barbecue, they took turns to cook their favorite dishes, making everyone else eagerly wait in anticipation of the delicious meal.</p>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="A1.T10.5.11.5">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t" id="A1.T10.5.11.5.1" style="width:113.8pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.11.5.1.1">MMT with retrieved supplementary texts</p>
</th>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" id="A1.T10.5.11.5.2" style="width:284.5pt;">
<p class="ltx_p ltx_align_top" id="A1.T10.5.11.5.2.1">Eine Gruppe von asiatischen Jungen wartet darauf, dass Fleisch über Grill zukochen.</p>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>A correct example generated by NMT with retrieved supplementary texts. In this example, in contrast to text-only NMT without any supplementary information, visual information and supplementary text information play an equivalent role, correctly translating “Group” to “Gruppe”. Benefiting from the rich information in the supplementary text, the NMT with retrieved supplementary text achieves more accurate translations compared to MMT with retrieved images.</figcaption>
</figure>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"></p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr  9 08:17:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span style="font-size:70%;position:relative; bottom:2.2pt;">A</span>T<span style="position:relative; bottom:-0.4ex;">E</span></span><span class="ltx_font_smallcaps">xml</span><img alt="[LOGO]" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
