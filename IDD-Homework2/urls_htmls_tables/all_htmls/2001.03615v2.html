<!DOCTYPE html><html lang="en-US">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2001.03615] In Defense of Grid Features for Visual Question Answering</title><meta property="og:description" content="Popularized as ‘bottom-up’ attention [2], bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like vis…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="In Defense of Grid Features for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="In Defense of Grid Features for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2001.03615">

<!--Generated on Sun Mar 17 06:50:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en-US">
<h1 class="ltx_title ltx_title_document">In Defense of Grid Features for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Huaizu Jiang<sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup><span id="footnote1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">*</span></span></span></span> , Ishan Misra<sup id="id9.9.id2" class="ltx_sup">2</sup>, Marcus Rohrbach<sup id="id10.10.id3" class="ltx_sup">2</sup>, Erik Learned-Miller<sup id="id11.11.id4" class="ltx_sup">1</sup>, and Xinlei Chen<sup id="id12.12.id5" class="ltx_sup">2</sup> 
<br class="ltx_break"><sup id="id13.13.id6" class="ltx_sup">1</sup>UMass Amherst, <sup id="id14.14.id7" class="ltx_sup">2</sup>Facebook AI Research (FAIR) 
<br class="ltx_break"><span id="id15.15.id8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{hzjiang,elm}@cs.umass.edu</span>, <span id="id16.16.id9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{imisra,mrf,xinleic}@fb.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p"><span id="id17.id1.1" class="ltx_text">Popularized as ‘bottom-up’ attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, bounding box (or region) based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering (VQA). However, it is not clear whether the advantages of regions (<em id="id17.id1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id17.id1.1.2" class="ltx_text"></span> better localization) are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well – running more than an order of magnitude faster with the same accuracy (<em id="id17.id1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="id17.id1.1.4" class="ltx_text"></span> if pre-trained in a similar fashion). Through extensive experiments, we verify that this observation holds true across different VQA models (reporting a state-of-the-art accuracy on VQA 2.0 <span id="id17.id1.1.5" class="ltx_text ltx_font_typewriter">test-std</span>, <span id="id17.id1.1.6" class="ltx_text ltx_font_bold">72.71</span>), datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available.</span></p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>This work was done when Huaizu Jiang was an intern at FAIR.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">After the introduction of deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> to multi-modal vision and language research, perhaps one of the most significant developments was the discovery of ‘bottom-up’ attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Unlike normal attention that uses ‘top-down’ linguistic inputs to focus on specific parts of the visual input, bottom-up attention uses pre-trained object detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to identify salient regions based <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">solely</em> on the visual input itself. As a result, images are represented by a collection of bounding box or <span id="S1.p1.1.2" class="ltx_text ltx_font_bold">region<span id="footnote1a" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1a.1.1.1" class="ltx_text ltx_font_medium">1</span></span><span id="footnote1a.5" class="ltx_text ltx_font_medium">We use the terms ‘region’ and ‘bounding box’ interchangeably.</span></span></span></span></span>-based features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>–in contrast to vanilla <span id="S1.p1.1.3" class="ltx_text ltx_font_bold">grid</span> convolutional feature maps from ConvNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>–for follow-up tasks. These region features have since then gained wide popularity and dominated vision and language leader boards <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> for major tasks like visual question answering (VQA).</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">So what makes these region features successful? Naturally, one would assume a major reason is better <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">localization</em> of individual objects, as the regions are direct bounding box outputs from detectors. Another plausible answer is that a number of regions can easily capture both the coarse-level information and fine-grained details in the image – even if they overlap. However, do these potential advantages actually demonstrate that region features are superior to grids?</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2001.03615/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We revisit <span id="S1.F1.4.1" class="ltx_text ltx_font_bold">grid</span>-based convolutional features for VQA, and find they can <em id="S1.F1.5.2" class="ltx_emph ltx_font_italic">match</em> the accuracy of the dominant <span id="S1.F1.6.3" class="ltx_text ltx_font_bold">region</span>-based features from bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, provided that one closely follow the pre-training process on Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. As computing grid features skips the expensive region-related steps (shown in colors), it leads to significant speed-ups (all modules run on GPU; timed in the same environment).</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Surprisingly, we discovered that grid features extracted from <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">exactly</em> the same layer of the pre-trained detector can perform competitively against their region-based counterparts for VQA. Moreover, with simple modifications during training, the same grid features can be made even more effective and that they consistently achieve comparable and sometimes better VQA accuracy than region features. In fact, our ablative analysis suggests that the key factors which contributed to the high accuracy of existing bottom-up attention features are: 1) the large-scale object and attribute annotations collected in the Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> dataset used for pre-training; and 2) the high spatial resolution of the input images used for computing features. As for the feature <em id="S1.p3.1.2" class="ltx_emph ltx_font_italic">format</em> itself – region or grid – it only affects accuracy <em id="S1.p3.1.3" class="ltx_emph ltx_font_italic">minimally</em>. Through a comprehensive set of experiments, we verified that our observations generalize across different network backbones, different VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, different VQA benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and even to other relevant tasks (<em id="S1.p3.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p3.1.5" class="ltx_text"></span> image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our findings have important consequences for the design of future multi-modal vision and language models. The immediate benefit of switching to grids is inference speed, as we can now skip <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">all</em> of the region-related steps in the existing VQA pipeline (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). For example, using a ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> backbone, we find the overall running time drops from 0.89s to 0.02s per image – <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">40+</span> times faster with slightly better accuracy! In fact, extracting region features is so time-consuming that most state-of-the-art models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> are directly trained and evaluated on <em id="S1.p4.1.3" class="ltx_emph ltx_font_italic">cached</em> visual features. This practice not only imposes unnecessary constraints on model designs, but also limits potential applications of existing vision and language systems.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Empowered by grid features, we therefore take an initial step to train VQA models <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">end-to-end</em> from pixels directly to answers. Note that end-to-end training with region features is challenging, since fine-tuning region locations likely requires additional grounding annotations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> that are computationally expensive and difficult to acquire. In contrast, grid features can be readily optimized for the final objective (<em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p5.1.3" class="ltx_text"></span> to answer questions correctly) without extra grounding. The grid-feature pipeline also allows us to explore more effective designs for VQA (<em id="S1.p5.1.4" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S1.p5.1.5" class="ltx_text"></span> pyramid pooling module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>) and enables networks pre-trained with <span id="S1.p5.1.6" class="ltx_text ltx_font_bold">zero</span> region-level annotations to greatly reduce the gap in accuracy with VG models (trained on bounding boxes) – indicating strong VQA models can be achieved without <em id="S1.p5.1.7" class="ltx_emph ltx_font_italic">any</em> explicit notion of regions. These results further strengthen our defense of grid features for VQA. We hope our discovery can open up new opportunities for vision and language research in general.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual features for vision and language tasks.</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Features have played a key role in the advancement of vision and language tasks. For example, deep learning features led to remarkable improvements in image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. While a complete review of visual features used for vision and language tasks is beyond the scope of this paper, we note that the accuracies of modern VQA models are dependent on the underlying visual features used, including VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> grid features, which were later dominated by bottom-up attention region features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. Today, most state-of-the-art VQA models focus on fusing schemes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> and are built with region features as-is <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>; whereas our work revisits grid features, and shows that they can be equally effective and lead to remarkable speed-ups – often greater than an order of magnitude!</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p"><span id="S2.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_bold">Pre-training for VQA.</span> Most VQA methods use two separately pre-trained models: vision models trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>; and word embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for linguistic features. As these separately trained features may not be optimal for joint vision and language understanding, a recent hot topic is to develop jointly pre-trained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for vision and language tasks. A common scheme for such methods is to view regions and words as ‘tokens’ for their respective domain, and pre-train a variant of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> for ‘masked’ token prediction. Complementary to that direction, our work delves specifically into the ‘format’ of visual tokens and can be potentially combined with such methods for mutual benefits (<em id="S2.SS0.SSS0.Px1.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS0.SSS0.Px1.p2.1.3" class="ltx_text"></span> trade-off between speed and accuracy).</p>
</div>
<div id="S2.SS0.SSS0.Px1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS0.SSS0.Px1.p3.1" class="ltx_p"><span id="S2.SS0.SSS0.Px1.p3.1.1" class="ltx_text ltx_font_bold">Regions <em id="S2.SS0.SSS0.Px1.p3.1.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S2.SS0.SSS0.Px1.p3.1.1.2" class="ltx_text"></span> grids.</span> The debate between region features and grid features carries some inherent connections to object detection: the dominance of the R-CNN based detection models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> demonstrates that a region (the ‘R’ in R-CNN) based refinement stage is beneficial for object detection. On the other hand, one-stage detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> approach the detection task <em id="S2.SS0.SSS0.Px1.p3.1.2" class="ltx_emph ltx_font_italic">without</em> the need for explicit region-level computation and show that grid features can be competitive for object detection. In our work, we also use grid features – <em id="S2.SS0.SSS0.Px1.p3.1.3" class="ltx_emph ltx_font_italic">no</em> regions for the VQA task. To minimize changes from bottom-up attention paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, we pre-train the features with Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. However, during inference, we discard the region-related steps from the detector and use <em id="S2.SS0.SSS0.Px1.p3.1.4" class="ltx_emph ltx_font_italic">only</em> the grid convolutional features. This in fact gives us a <em id="S2.SS0.SSS0.Px1.p3.1.5" class="ltx_emph ltx_font_italic">stronger</em> defense for grids, as we show that VQA can operate on a ‘single’ feature map, instead of feature maps of ‘multiple’ scales that one-stage detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> thrive on.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2001.03615/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="188" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.9.1" class="ltx_text ltx_font_bold">From regions to grids.</span> <span id="S2.F2.10.2" class="ltx_text ltx_font_bold">Left</span>: We convert the original region feature extractor used by bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> back to the ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> grid feature extractor for the <em id="S2.F2.11.3" class="ltx_emph ltx_font_italic">same</em> layer (see Sec. <a href="#S3.SS2" title="3.2 Grid Features from the Same Layer ‣ 3 From Regions to Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, weights in  blue are transferred), and find it works surprisingly well for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. <span id="S2.F2.12.4" class="ltx_text ltx_font_bold">Right</span>: We build a detector based on 1<math id="S2.F2.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.F2.2.m1.1b"><mo id="S2.F2.2.m1.1.1" xref="S2.F2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.F2.2.m1.1c"><times id="S2.F2.2.m1.1.1.cmml" xref="S2.F2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.m1.1d">\times</annotation></semantics></math>1 <span id="S2.F2.13.5" class="ltx_text ltx_font_typewriter">RoIPool</span> while keeping the output architecture <em id="S2.F2.14.6" class="ltx_emph ltx_font_italic">fixed</em> for grid features (see Sec. <a href="#S3.SS3" title="3.3 1×1 RoIPool for Improved Grid Features ‣ 3 From Regions to Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>), and the resulting grid features consistently perform at-par with region features.</figcaption>
</figure>
<div id="S2.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p4.1" class="ltx_p">It is also worth noting that while region features are effective on benchmarks like VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and COCO captions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, for benchmarks that diagnose a model’s reasoning abilities when answering visual questions (<em id="S2.SS0.SSS0.Px1.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S2.SS0.SSS0.Px1.p4.1.2" class="ltx_text"></span> CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>), simple methods based on grids <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> have shown strong performance. We hope that our discovery that grid features also work well for the general VQA task can bridge the gap between these two lines of work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>From Regions to Grids</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we explain our approach to obtaining grid features that are just as effective as region features, with the constraint that they have been pre-trained with the <em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">same</em> task. In Sec. <a href="#S7" title="7 Towards End-to-end VQA ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we show that the ‘same pre-training’ constraint can be lifted and grid features can still close the gap to regions with end-to-end training on down-stream tasks. We first briefly review the region features from bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Bottom-Up Attention with Regions</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The bottom-up attention method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> uses a Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> detection model. The detector is trained on a cleaned version of Visual Genome <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, with thousands of object categories and hundreds of attributes with bounding box (region) annotations.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In order to obtain bottom-up attention features for tasks like VQA, two region-related steps are needed:</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Region selection.</span> As Faster R-CNN is a two-stage detector, region selection happens twice in the pipeline. The first is through a region proposal network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which deforms and selects prominent candidate ‘anchors’ as Regions of Interest (RoIs). Another selection is done as post-processing to aggregate top <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">N</annotation></semantics></math> boxes in a <em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">per-class</em> manner. In both steps, non-maximal suppression (NMS) is used, which keeps the region with the highest classification score and removes other near-duplicates in a local neighborhood.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Region feature computation.</span> Given regions from the first stage (up to thousands), <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_typewriter">RoIPool</span> operations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> are used to extract the initial region-level features. Additional network layers then compute the output representation of regions <em id="S3.SS1.p4.1.3" class="ltx_emph ltx_font_italic">separately</em>. Finally, region features that survive both rounds of selection are stacked together as the bottom-up features to represent an image.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p">It is important to note that due to the complexity of the VG dataset (<em id="S3.SS1.p5.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S3.SS1.p5.1.2" class="ltx_text"></span> thousands of classes) and the specific Faster R-CNN detector used <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> (described next), both steps are computationally intensive. In contrast, directly using grid features can skip or accelerate these steps and offer potentially significant speed-ups.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Grid Features from the Same Layer</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The simplest way to convert region features to grids is to see if one can directly compute outputs of the same network layer, but in a <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">shared</em>, fully convolutional manner. To this end, we take a closer look at the specific Faster R-CNN architecture used by the original bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p">The Faster R-CNN is a variant of the <em id="S3.SS2.p2.6.1" class="ltx_emph ltx_font_italic">c4</em> model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> with an extra branch for attribute classification. It divides the weights from a ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> into two separate sets: given an input image, it first computes feature maps using the lower blocks of ResNet up to <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="C_{4}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">C_{4}</annotation></semantics></math>. This feature map is shared among all regions. Then, separately, per-region feature computations are performed by applying the <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">C_{5}</annotation></semantics></math> block on the 14<math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mo id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><times id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\times</annotation></semantics></math>14 <span id="S3.SS2.p2.6.2" class="ltx_text ltx_font_typewriter">RoIPool</span>-ed features. The output of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">C_{5}</annotation></semantics></math> is then <em id="S3.SS2.p2.6.3" class="ltx_emph ltx_font_italic">AvgPool</em>-ed to a final vector for each region as the bottom-up features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Since all the final region features are from <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">C_{5}</annotation></semantics></math>, it is easy to convert the detector <em id="S3.SS2.p2.6.4" class="ltx_emph ltx_font_italic">back</em> to the ResNet classifier and take the same <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">C</mi><mn id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">C_{5}</annotation></semantics></math> layer as our output grid features. Fig. <a href="#S2.F2" title="Figure 2 ‣ Visual features for vision and language tasks. ‣ 2 Related Work ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (left) illustrates our conversion process.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">As our experiments will show, directly using the converted <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msub id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">C</mi><mn id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">C_{5}</annotation></semantics></math> output already works surprisingly well. Any performance drop from doing so may be because Faster R-CNN is highly optimized for <em id="S3.SS2.p3.1.1" class="ltx_emph ltx_font_italic">region</em>-based object detection, and likely not so much for grids. Therefore, we next see if some minimal adjustments to the model can be made to improve grid features.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>1<math id="S3.SS3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.1.m1.1b"><mo id="S3.SS3.1.m1.1.1" xref="S3.SS3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.1.m1.1c"><times id="S3.SS3.1.m1.1.1.cmml" xref="S3.SS3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.1.m1.1d">\times</annotation></semantics></math>1 <span id="S3.SS3.2.1" class="ltx_text ltx_font_typewriter">RoIPool</span> for Improved Grid Features</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Our idea is to simply use 1<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mo id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><times id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\times</annotation></semantics></math>1 <span id="S3.SS3.p1.2.1" class="ltx_text ltx_font_typewriter">RoIPool</span>. This means representing each region with a single <em id="S3.SS3.p1.2.2" class="ltx_emph ltx_font_italic">vector</em>, rather than a three-dimensional tensor in Faster R-CNN. At first glance, it may seem counter-intuitive, as the two additional spatial dimensions (height and width) are useful to characterize different parts of objects in 2D – indeed, we find this modification negatively affects object detection performance on VG. But importantly, using 1<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mo id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><times id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\times</annotation></semantics></math>1 <span id="S3.SS3.p1.2.3" class="ltx_text ltx_font_typewriter">RoIPool</span> regions also means each vector on the grid feature map is <em id="S3.SS3.p1.2.4" class="ltx_emph ltx_font_italic">forced</em> to cover all the information for a spatial region <em id="S3.SS3.p1.2.5" class="ltx_emph ltx_font_italic">alone</em>, which can potentially result in stronger grid features.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">However, directly applying 1<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><times id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\times</annotation></semantics></math>1 <span id="S3.SS3.p2.3.1" class="ltx_text ltx_font_typewriter">RoIPool</span> on the original model is problematic, likely because <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">C_{5}</annotation></semantics></math> consists of several ImageNet pre-trained convolutional layers that work best with inputs of particular spatial dimensions. To resolve this, we follow recent developments in object detection and use the entire ResNet up to <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><msub id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">C</mi><mn id="S3.SS3.p2.3.m3.1.1.3" xref="S3.SS3.p2.3.m3.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p2.3.m3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">C_{5}</annotation></semantics></math> as the backbone for shared feature computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>; and for region-level computation place two 1024D fully-connected (<span id="S3.SS3.p2.3.2" class="ltx_text ltx_font_typewriter">FC</span>) layers on the top, which by <em id="S3.SS3.p2.3.3" class="ltx_emph ltx_font_italic">default</em> accept vectors as inputs.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.3" class="ltx_p">To reduce the effect of low resolutions when training the detector with features pooled from <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">C</mi><mn id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">C_{5}</annotation></semantics></math> (<math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><msub id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mi id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">C</mi><mn id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">C_{5}</annotation></semantics></math> has stride 32, whereas <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="C_{4}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><msub id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml">C</mi><mn id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml">4</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2">𝐶</ci><cn type="integer" id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">C_{4}</annotation></semantics></math> has 16), the stride-2 layers are replaced with stride-1 layers, and the remaining layers are dilated with a factor of 2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. For grid feature extraction, we remove this dilation and convert it back to the normal ResNet.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">Fig. <a href="#S2.F2" title="Figure 2 ‣ Visual features for vision and language tasks. ‣ 2 Related Work ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (right) summarizes the changes we made to improved grids. Note that compared to the original model (left), we only made necessary modifications to the region related components during training. Since all such computations are removed during feature extraction, our grid feature extractor is kept <em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">untouched</em> during inference.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.7" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.7.8" class="ltx_tr">
<td id="S3.T1.7.8.1" class="ltx_td" style="padding:0.8pt 5.0pt;"></td>
<td id="S3.T1.7.8.2" class="ltx_td ltx_border_r" style="padding:0.8pt 5.0pt;"></td>
<td id="S3.T1.7.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;" colspan="3"><span id="S3.T1.7.8.3.1" class="ltx_text" style="font-size:80%;">VG detection pre-train</span></td>
<td id="S3.T1.7.8.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;" colspan="2"><span id="S3.T1.7.8.4.1" class="ltx_text" style="font-size:80%;">VQA</span></td>
</tr>
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S3.T1.1.1.2.1" class="ltx_text" style="font-size:80%;">#</span></td>
<td id="S3.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">feature</span></td>
<td id="S3.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.1.1.4.1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">RoIPool</span></td>
<td id="S3.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.1.1.5.1" class="ltx_text" style="font-size:80%;">region layers</span></td>
<td id="S3.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.1.1.6.1" class="ltx_text" style="font-size:80%;">AP</span></td>
<td id="S3.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.1.1.7.1" class="ltx_text" style="font-size:80%;">accuracy</span></td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><math id="S3.T1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S3.T1.1.1.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\Delta</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S3.T1.3.3.3.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S3.T1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;" rowspan="2"><span id="S3.T1.3.3.4.1" class="ltx_text" style="font-size:80%;"><span id="S3.T1.3.3.4.1.1" class="ltx_text ltx_font_smallcaps">R</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span></td>
<td id="S3.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;">
<span id="S3.T1.2.2.1.1" class="ltx_text" style="font-size:80%;">14</span><math id="S3.T1.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.2.2.1.m1.1a"><mo mathsize="80%" id="S3.T1.2.2.1.m1.1.1" xref="S3.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.m1.1b"><times id="S3.T1.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.m1.1c">\times</annotation></semantics></math><span id="S3.T1.2.2.1.2" class="ltx_text" style="font-size:80%;">14</span>
</td>
<td id="S3.T1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;">
<math id="S3.T1.3.3.2.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.T1.3.3.2.m1.1a"><msub id="S3.T1.3.3.2.m1.1.1" xref="S3.T1.3.3.2.m1.1.1.cmml"><mi mathsize="80%" id="S3.T1.3.3.2.m1.1.1.2" xref="S3.T1.3.3.2.m1.1.1.2.cmml">C</mi><mn mathsize="80%" id="S3.T1.3.3.2.m1.1.1.3" xref="S3.T1.3.3.2.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.2.m1.1b"><apply id="S3.T1.3.3.2.m1.1.1.cmml" xref="S3.T1.3.3.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.2.m1.1.1.1.cmml" xref="S3.T1.3.3.2.m1.1.1">subscript</csymbol><ci id="S3.T1.3.3.2.m1.1.1.2.cmml" xref="S3.T1.3.3.2.m1.1.1.2">𝐶</ci><cn type="integer" id="S3.T1.3.3.2.m1.1.1.3.cmml" xref="S3.T1.3.3.2.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.2.m1.1c">C_{5}</annotation></semantics></math><span id="S3.T1.3.3.2.1" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.T1.3.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S3.T1.3.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S3.T1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.3.3.5.1" class="ltx_text" style="font-size:80%;">4.07</span></td>
<td id="S3.T1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.3.3.6.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">64.29</span></td>
<td id="S3.T1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.3.3.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T1.4.4" class="ltx_tr">
<td id="S3.T1.4.4.2" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S3.T1.4.4.2.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S3.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="S3.T1.4.4.1.1" class="ltx_text" style="font-size:80%;">1</span><math id="S3.T1.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.4.4.1.m1.1a"><mo mathsize="80%" id="S3.T1.4.4.1.m1.1.1" xref="S3.T1.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.1.m1.1b"><times id="S3.T1.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.1.m1.1c">\times</annotation></semantics></math><span id="S3.T1.4.4.1.2" class="ltx_text" style="font-size:80%;">1</span>
</td>
<td id="S3.T1.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="S3.T1.4.4.3.1" class="ltx_text" style="font-size:80%;">2-</span><span id="S3.T1.4.4.3.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">FC</span>
</td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S3.T1.4.4.4.1" class="ltx_text" style="font-size:80%;">2.90</span></td>
<td id="S3.T1.4.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S3.T1.4.4.5.1" class="ltx_text" style="font-size:80%;">63.94</span></td>
<td id="S3.T1.4.4.6" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><em id="S3.T1.4.4.6.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">-0.35</em></td>
</tr>
<tr id="S3.T1.6.6" class="ltx_tr">
<td id="S3.T1.6.6.3" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S3.T1.6.6.3.1" class="ltx_text" style="font-size:80%;">3</span></td>
<td id="S3.T1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;" rowspan="3"><span id="S3.T1.6.6.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">G</span></td>
<td id="S3.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;">
<span id="S3.T1.5.5.1.1" class="ltx_text" style="font-size:80%;">14</span><math id="S3.T1.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.5.5.1.m1.1a"><mo mathsize="80%" id="S3.T1.5.5.1.m1.1.1" xref="S3.T1.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.1.m1.1b"><times id="S3.T1.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.1.m1.1c">\times</annotation></semantics></math><span id="S3.T1.5.5.1.2" class="ltx_text" style="font-size:80%;">14</span>
</td>
<td id="S3.T1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><math id="S3.T1.6.6.2.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.T1.6.6.2.m1.1a"><msub id="S3.T1.6.6.2.m1.1.1" xref="S3.T1.6.6.2.m1.1.1.cmml"><mi mathsize="80%" id="S3.T1.6.6.2.m1.1.1.2" xref="S3.T1.6.6.2.m1.1.1.2.cmml">C</mi><mn mathsize="80%" id="S3.T1.6.6.2.m1.1.1.3" xref="S3.T1.6.6.2.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.2.m1.1b"><apply id="S3.T1.6.6.2.m1.1.1.cmml" xref="S3.T1.6.6.2.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.6.6.2.m1.1.1.1.cmml" xref="S3.T1.6.6.2.m1.1.1">subscript</csymbol><ci id="S3.T1.6.6.2.m1.1.1.2.cmml" xref="S3.T1.6.6.2.m1.1.1.2">𝐶</ci><cn type="integer" id="S3.T1.6.6.2.m1.1.1.3.cmml" xref="S3.T1.6.6.2.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.2.m1.1c">C_{5}</annotation></semantics></math></td>
<td id="S3.T1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.6.6.5.1" class="ltx_text" style="font-size:80%;">4.07</span></td>
<td id="S3.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.6.6.6.1" class="ltx_text" style="font-size:80%;">63.64</span></td>
<td id="S3.T1.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><em id="S3.T1.6.6.7.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">-0.65</em></td>
</tr>
<tr id="S3.T1.7.7" class="ltx_tr">
<td id="S3.T1.7.7.2" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S3.T1.7.7.2.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S3.T1.7.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="S3.T1.7.7.1.1" class="ltx_text" style="font-size:80%;">1</span><math id="S3.T1.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.7.7.1.m1.1a"><mo mathsize="80%" id="S3.T1.7.7.1.m1.1.1" xref="S3.T1.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><times id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">\times</annotation></semantics></math><span id="S3.T1.7.7.1.2" class="ltx_text" style="font-size:80%;">1</span>
</td>
<td id="S3.T1.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="S3.T1.7.7.3.1" class="ltx_text" style="font-size:80%;">2-</span><span id="S3.T1.7.7.3.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">FC</span>
</td>
<td id="S3.T1.7.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S3.T1.7.7.4.1" class="ltx_text" style="font-size:80%;">2.90</span></td>
<td id="S3.T1.7.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S3.T1.7.7.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">64.37</span></td>
<td id="S3.T1.7.7.6" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><em id="S3.T1.7.7.6.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">0.08</em></td>
</tr>
<tr id="S3.T1.7.9" class="ltx_tr">
<td id="S3.T1.7.9.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S3.T1.7.9.1.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="S3.T1.7.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;" colspan="3"><span id="S3.T1.7.9.2.1" class="ltx_text" style="font-size:80%;color:#808080;">ImageNet pre-train</span></td>
<td id="S3.T1.7.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.7.9.3.1" class="ltx_text" style="font-size:80%;color:#808080;">60.76</span></td>
<td id="S3.T1.7.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S3.T1.7.9.4.1" class="ltx_text ltx_font_italic" style="font-size:80%;color:#808080;">-3.53</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.34.1" class="ltx_text ltx_font_bold">Main comparison</span>. ‘<span id="S3.T1.35.2" class="ltx_text ltx_font_smallcaps">R</span>’ stands for region features as in bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. ‘<span id="S3.T1.36.3" class="ltx_text ltx_font_smallcaps">G</span>’ stands for grid features. All results reported on VQA 2.0 <span id="S3.T1.37.4" class="ltx_text ltx_font_typewriter">vqa-eval</span>. We show that: <span id="S3.T1.38.5" class="ltx_text ltx_font_bold">1)</span> by simply extracting grid features from the <em id="S3.T1.39.6" class="ltx_emph ltx_font_italic">same</em> layer <math id="S3.T1.10.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S3.T1.10.m1.1b"><msub id="S3.T1.10.m1.1.1" xref="S3.T1.10.m1.1.1.cmml"><mi id="S3.T1.10.m1.1.1.2" xref="S3.T1.10.m1.1.1.2.cmml">C</mi><mn id="S3.T1.10.m1.1.1.3" xref="S3.T1.10.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S3.T1.10.m1.1c"><apply id="S3.T1.10.m1.1.1.cmml" xref="S3.T1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.10.m1.1.1.1.cmml" xref="S3.T1.10.m1.1.1">subscript</csymbol><ci id="S3.T1.10.m1.1.1.2.cmml" xref="S3.T1.10.m1.1.1.2">𝐶</ci><cn type="integer" id="S3.T1.10.m1.1.1.3.cmml" xref="S3.T1.10.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.m1.1d">C_{5}</annotation></semantics></math> of the same model, the VQA accuracy is already much closer to bottom-up attention than ImageNet pre-trained ones (row 1,3 &amp; 5); <span id="S3.T1.40.7" class="ltx_text ltx_font_bold">2)</span> 1<math id="S3.T1.11.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.11.m2.1b"><mo id="S3.T1.11.m2.1.1" xref="S3.T1.11.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.11.m2.1c"><times id="S3.T1.11.m2.1.1.cmml" xref="S3.T1.11.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.m2.1d">\times</annotation></semantics></math>1 <span id="S3.T1.41.8" class="ltx_text ltx_font_typewriter">RoIPool</span> based detector pre-training improves the grid features accuracy while the region features get worse (row 1,2 &amp; 4). Last column is the gap compared to the original bottom-up features (underlined).</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Main Comparison: Regions <em id="S4.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S4.2.2" class="ltx_text"></span> Grids</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">From this section on, we report our experimental results comparing regions with grids. We choose VQA (2.0) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> as our main task of interest, since it is currently a major benchmark for evaluating joint vision and language understanding and has clear metrics for evaluation.
For all our comparisons, we denote methods using region features with the tag ‘<span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">R</span>’, and methods using grid features with ‘<span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">G</span>’. In this section, we focus on reporting our main findings from converting regions to grids as described in Sec. <a href="#S3" title="3 From Regions to Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We begin by briefly describing our experimental setups (more details in the supplementary material). Note that our goal here is to make the conclusion <em id="S4.p1.1.3" class="ltx_emph ltx_font_italic">meaningful</em> by controlled comparisons, and not necessarily to optimize for absolute performance.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Faster R-CNN.</span> For analysis, we use Faster R-CNN with a ResNet-50 backbone pre-trained on ImageNet by default<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/facebookresearch/maskrcnn-benchmark" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/maskrcnn-benchmark</a></span></span></span>. Closely following bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the detector is then trained on the VG dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> with region-level annotations for 1600 object categories and 400 attribute classes. For attributes, an additional branch is added with loss weight 0.5. The model is trained with ‘1x’ schedule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Notably, input images are resized to have a maximum shorter side of 600 pixels (longest 1000) when keeping aspect ratio fixed. For region features, we set <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">N</annotation></semantics></math>=100.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">VQA split.</span> Unless otherwise specified, we use the default <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">train</span> set for training. To assist our analysis, we create a local validation set, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">vqa-dev</span>, out of the standard <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">val</span> set to select the best model during training for evaluation. It contains randomly sampled 8.4K images and their corresponding questions, with 66K pairs in total. The rest of the original <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_typewriter">val</span> set (named <span id="S4.SS1.p2.1.6" class="ltx_text ltx_font_typewriter">vqa-eval</span>) is reserved for testing, on which we report results.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">VQA model.</span> We use the co-attention model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> implemented in Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. This model fuses visual features (either region or grid) with textual representations of questions, and outputs the final answer.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our main results are summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 1×1 RoIPool for Improved Grid Features ‣ 3 From Regions to Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We make two observations:
First, compared with the widely used bottom-up region features (row 1), directly extracting outputs from <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">C</mi><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">C_{5}</annotation></semantics></math> with the same model (row 3) works <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">surprisingly</em> well (64.29 <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">vs</em>.<span id="S4.SS2.p1.1.3" class="ltx_text"></span> 63.64 accuracy). In contrast, the standard ResNet-50 model pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> shows much worse performance – 60.76 accuracy, a gap of more than 3% with the bottom-up features.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2001.03615/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.4.1" class="ltx_text ltx_font_bold">VQA accuracy <em id="S4.F3.4.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S4.F3.4.1.2" class="ltx_text"></span> number of features <math id="S4.F3.4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.F3.4.1.m1.1b"><mi id="S4.F3.4.1.m1.1.1" xref="S4.F3.4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.F3.4.1.m1.1c"><ci id="S4.F3.4.1.m1.1.1.cmml" xref="S4.F3.4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.4.1.m1.1d">N</annotation></semantics></math></span> as input to the VQA model. We report the average accuracy and standard deviation across 5 independent runs on the VQA 2.0 <span id="S4.F3.8.2" class="ltx_text ltx_font_typewriter">vqa-eval</span> set. We observe that the VQA accuracy of region features saturates around 200 regions. In contrast, the grid features benefit from a larger <math id="S4.F3.5.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.F3.5.m1.1b"><mi id="S4.F3.5.m1.1.1" xref="S4.F3.5.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.F3.5.m1.1c"><ci id="S4.F3.5.m1.1.1.cmml" xref="S4.F3.5.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.5.m1.1d">N</annotation></semantics></math> (translates from a larger input size) and in this case stays better than regions even when <math id="S4.F3.6.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.F3.6.m2.1b"><mi id="S4.F3.6.m2.1.1" xref="S4.F3.6.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.F3.6.m2.1c"><ci id="S4.F3.6.m2.1.1.cmml" xref="S4.F3.6.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m2.1d">N</annotation></semantics></math> is the same (608).
</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">Second, while our 1<math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><times id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\times</annotation></semantics></math>1 <span id="S4.SS2.p2.3.1" class="ltx_text ltx_font_typewriter">RoIPool</span>-based variant hurts the object detection performance (average precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on VG drops from 4.07 to 2.90), it helps VQA – boosting the accuracy by 0.73% (row 3 &amp; 4) and as a result slightly <em id="S4.SS2.p2.3.2" class="ltx_emph ltx_font_italic">outperforms</em> the original region-based features. On the other hand, our RoI-based variant does not help the region features method and drops the accuracy of region features to 63.94. This indicates the original model used by bottom-up attention favors regions; while our design works better for grids. Thus, we use the setting of the 1<sup id="S4.SS2.p2.3.3" class="ltx_sup"><span id="S4.SS2.p2.3.3.1" class="ltx_text ltx_font_italic">st</span></sup> row (best for regions) to represent ‘<span id="S4.SS2.p2.3.4" class="ltx_text ltx_font_smallcaps">R</span>’, and the 4<sup id="S4.SS2.p2.3.5" class="ltx_sup"><span id="S4.SS2.p2.3.5.1" class="ltx_text ltx_font_italic">th</span></sup> row (best for grids) to represent ‘<span id="S4.SS2.p2.3.6" class="ltx_text ltx_font_smallcaps">G</span>’, to perform a more in-depth study and fair comparison between the two through the rest of the paper.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Number of Regions</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.4" class="ltx_p">Apart from architectural differences in training, another factor that can affect VQA accuracy is the number of feature vectors <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">N</annotation></semantics></math> used to represent images.
Our region model from Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> has a default setting that uses the top 100 boxes to represent region features, increasing it from the original 36 boxes in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to improve the accuracy. On the other hand, since grid features are convolutional feature maps for a pre-set layer, the number of features is determined by the input size to the network. As our largest input size is 600<math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mo id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><times id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">\times</annotation></semantics></math>1000, a 32-stride feature map (<math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><msub id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mi id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">C</mi><mn id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">𝐶</ci><cn type="integer" id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">C_{5}</annotation></semantics></math>) results in 608 grid features – much larger than the number of region features. To understand how these different numbers of region features affect the accuracy, we ran experiments with varying number of features <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mi id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><ci id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">N</annotation></semantics></math> and show the results in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Main Results ‣ 4 Main Comparison: Regions vs. Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">As for the region features, we observe an improvement in accuracy as the number of regions increases from 30 to 200, beyond which the accuracy saturates. Interestingly, our grid features are better even when compared to the highest number of regions<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Since NMS is used in selecting regions, the maximum number <math id="footnote3.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="footnote3.m1.1b"><mi id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><ci id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">N</annotation></semantics></math> varies across images. Therefore we 1) cannot directly set it to the same number as grids and 2) report maximum <math id="footnote3.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="footnote3.m2.1b"><mi id="footnote3.m2.1.1" xref="footnote3.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="footnote3.m2.1c"><ci id="footnote3.m2.1.1.cmml" xref="footnote3.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m2.1d">N</annotation></semantics></math> instead (zero paddings are used for images with fewer regions).</span></span></span>. Thus, the higher number of feature vectors used in our grid method compared to the baseline region method, is not the reason for its improved VQA accuracy.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.2" class="ltx_td ltx_border_r" style="padding:0.8pt 2.0pt;"></td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;" rowspan="2"><span id="S4.T2.1.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S4.T2.1.1.1.1.2" class="ltx_text"></span> <span id="S4.T2.1.1.1.1.1" class="ltx_text">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;"># features</span></span>
<span id="S4.T2.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S4.T2.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">(<math id="S4.T2.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.T2.1.1.1.1.1.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.1.1.m1.1c">N</annotation></semantics></math>)</span></span>
</span></span> <span id="S4.T2.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;" rowspan="2"><span id="S4.T2.1.1.3.1" class="ltx_text" style="font-size:80%;"><span id="S4.T2.1.1.3.1.1" class="ltx_text"></span> <span id="S4.T2.1.1.3.1.2" class="ltx_text">
<span id="S4.T2.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.1.3.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.1.3.1.2.1.1.1.1" class="ltx_text ltx_font_typewriter">test-dev</span></span></span>
<span id="S4.T2.1.1.3.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">accuracy</span></span>
</span></span> <span id="S4.T2.1.1.3.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center" style="padding:0.8pt 2.0pt;" colspan="5"><span id="S4.T2.1.1.4.1" class="ltx_text" style="font-size:80%;">inference time breakdown (ms)</span></td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_border_r" style="padding:0.8pt 2.0pt;"></td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;">
<span id="S4.T2.1.2.2.1" class="ltx_text"></span><span id="S4.T2.1.2.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T2.1.2.2.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T2.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.2.2.3.1.1" class="ltx_tr">
<span id="S4.T2.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">shared</span></span>
<span id="S4.T2.1.2.2.3.1.2" class="ltx_tr">
<span id="S4.T2.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">conv.</span></span>
</span></span><span id="S4.T2.1.2.2.4" class="ltx_text"></span><span id="S4.T2.1.2.2.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;">
<span id="S4.T2.1.2.3.1" class="ltx_text"></span><span id="S4.T2.1.2.3.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T2.1.2.3.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T2.1.2.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.2.3.3.1.1" class="ltx_tr">
<span id="S4.T2.1.2.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">region</span></span>
<span id="S4.T2.1.2.3.3.1.2" class="ltx_tr">
<span id="S4.T2.1.2.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">feat. comp.</span></span>
</span></span><span id="S4.T2.1.2.3.4" class="ltx_text"></span><span id="S4.T2.1.2.3.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T2.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;">
<span id="S4.T2.1.2.4.1" class="ltx_text"></span><span id="S4.T2.1.2.4.2" class="ltx_text" style="font-size:80%;"> </span><span id="S4.T2.1.2.4.3" class="ltx_text" style="font-size:80%;">
<span id="S4.T2.1.2.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.2.4.3.1.1" class="ltx_tr">
<span id="S4.T2.1.2.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">region</span></span>
<span id="S4.T2.1.2.4.3.1.2" class="ltx_tr">
<span id="S4.T2.1.2.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 2.0pt;">selection</span></span>
</span></span><span id="S4.T2.1.2.4.4" class="ltx_text"></span><span id="S4.T2.1.2.4.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T2.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.2.5.1" class="ltx_text" style="font-size:80%;">VQA</span></td>
<td id="S4.T2.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.2.6.1" class="ltx_text" style="font-size:80%;">total</span></td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;" rowspan="2"><span id="S4.T2.1.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">R</span></td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.2.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.3.1" class="ltx_text" style="font-size:80%;">66.13</span></td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.4.1" class="ltx_text" style="font-size:80%;">9</span></td>
<td id="S4.T2.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.5.1" class="ltx_text" style="font-size:80%;">326</span></td>
<td id="S4.T2.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.6.1" class="ltx_text" style="font-size:80%;">548</span></td>
<td id="S4.T2.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.7.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S4.T2.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.3.8.1" class="ltx_text" style="font-size:80%;">889</span></td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.1.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.2.1" class="ltx_text" style="font-size:80%;">66.22</span></td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.3.1" class="ltx_text" style="font-size:80%;">9</span></td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.4.1" class="ltx_text" style="font-size:80%;">322</span></td>
<td id="S4.T2.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.5.1" class="ltx_text" style="font-size:80%;">544</span></td>
<td id="S4.T2.1.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.6.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="S4.T2.1.4.7" class="ltx_td ltx_align_center" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.4.7.1" class="ltx_text" style="font-size:80%;">882</span></td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">G</span></td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.2.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.3.1" class="ltx_text" style="font-size:80%;">66.27</span></td>
<td id="S4.T2.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.4.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S4.T2.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.1.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.7.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="S4.T2.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 2.0pt;"><span id="S4.T2.1.5.8.1" class="ltx_text" style="font-size:80%;">18</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.7.1" class="ltx_text ltx_font_bold">Region <em id="S4.T2.7.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S4.T2.7.1.2" class="ltx_text"></span> grid features</span> on the VQA 2.0 <span id="S4.T2.8.2" class="ltx_text ltx_font_typewriter">test-dev</span> with accuracy and inference time breakdown measured in milliseconds per image. Our grid features achieve comparable VQA accuracy to region features while being much faster without region feature computation and region selection.</figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Test Accuracy and Inference Time</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We now report results on the VQA 2.0 <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_typewriter">test-dev</span> set to quantify the difference in performance between region and grid features. Note that different from previous setups, we use <span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_typewriter">trainval</span>+<span id="S4.SS4.p1.1.4" class="ltx_text ltx_font_typewriter">vqa-eval</span> for training. We report the VQA accuracy and the inference time breakdown in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Number of Regions ‣ 4 Main Comparison: Regions vs. Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Unlike our grid features which directly use convolutional feature maps, region features involve additional operations of region selection and region feature computation. These additional operations take 98.3% of the total inference time for a region-based model. As a result, the VQA model that takes our grid features as input runs <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_bold">48<math id="S4.SS4.p1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{\times}" display="inline"><semantics id="S4.SS4.p1.1.1.m1.1a"><mo id="S4.SS4.p1.1.1.m1.1.1" xref="S4.SS4.p1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.1.m1.1b"><times id="S4.SS4.p1.1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.1.m1.1c">\mathbf{\times}</annotation></semantics></math></span> faster than its counterpart using bottom-up region features.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<table id="S4.F4.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.F4.16.17" class="ltx_tr">
<td id="S4.F4.16.17.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.17.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.17.1.1.1" class="ltx_text ltx_font_medium">: Which devices do you see?</span></span></td>
<td id="S4.F4.16.17.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.17.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.17.2.1.1" class="ltx_text ltx_font_medium">: Has the pizza been eaten?</span></span></td>
<td id="S4.F4.16.17.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.17.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.17.3.1.1" class="ltx_text ltx_font_medium">: What color are the curtains?</span></span></td>
<td id="S4.F4.16.17.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.17.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.17.4.1.1" class="ltx_text ltx_font_medium">: What is the cat laying on?</span></span></td>
</tr>
<tr id="S4.F4.16.18" class="ltx_tr">
<td id="S4.F4.16.18.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.18.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.18.1.1.1" class="ltx_text ltx_font_medium">: phones</span></span></td>
<td id="S4.F4.16.18.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.18.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.18.2.1.1" class="ltx_text ltx_font_medium">: no</span></span></td>
<td id="S4.F4.16.18.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.18.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.18.3.1.1" class="ltx_text ltx_font_medium">: red and white</span></span></td>
<td id="S4.F4.16.18.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.18.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.18.4.1.1" class="ltx_text ltx_font_medium">: suitcase</span></span></td>
</tr>
<tr id="S4.F4.16.19" class="ltx_tr">
<td id="S4.F4.16.19.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.1.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.1.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): phones <span id="S4.F4.16.19.1.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.19.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.2.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.2.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): phones <span id="S4.F4.16.19.2.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.3.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.3.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): no <span id="S4.F4.16.19.3.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.19.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.4.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.4.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): yes <span id="S4.F4.16.19.4.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
<td id="S4.F4.16.19.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.5.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.5.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): red <span id="S4.F4.16.19.5.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
<td id="S4.F4.16.19.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.6.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.6.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): red and white <span id="S4.F4.16.19.6.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.19.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.7.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.7.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): shoes <span id="S4.F4.16.19.7.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
<td id="S4.F4.16.19.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.19.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.19.8.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.19.8.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): shoe <span id="S4.F4.16.19.8.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
</tr>
<tr id="S4.F4.8.8" class="ltx_tr">
<td id="S4.F4.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000133034_133034007_td_rois.jpg" id="S4.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000133034_133034007_td_grids.jpg" id="S4.F4.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000016733_16733001_td_rois.jpg" id="S4.F4.3.3.3.g1" class="ltx_graphics ltx_img_landscape" width="78" height="46" alt="Refer to caption"></td>
<td id="S4.F4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000016733_16733001_td_grids.jpg" id="S4.F4.4.4.4.g1" class="ltx_graphics ltx_img_landscape" width="78" height="46" alt="Refer to caption"></td>
<td id="S4.F4.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000398661_398661003_td_rois.jpg" id="S4.F4.5.5.5.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000398661_398661003_td_grids.jpg" id="S4.F4.6.6.6.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000535080_535080000_td_rois.jpg" id="S4.F4.7.7.7.g1" class="ltx_graphics ltx_img_landscape" width="61" height="46" alt="Refer to caption"></td>
<td id="S4.F4.8.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000535080_535080000_td_grids.jpg" id="S4.F4.8.8.8.g1" class="ltx_graphics ltx_img_landscape" width="61" height="46" alt="Refer to caption"></td>
</tr>
<tr id="S4.F4.16.20" class="ltx_tr">
<td id="S4.F4.16.20.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.20.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.20.1.1.1" class="ltx_text ltx_font_medium">: Is the plate white?</span></span></td>
<td id="S4.F4.16.20.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.20.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.20.2.1.1" class="ltx_text ltx_font_medium">: What breed of dog is this?</span></span></td>
<td id="S4.F4.16.20.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.20.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.20.3.1.1" class="ltx_text ltx_font_medium">: What is the person doing?</span></span></td>
<td id="S4.F4.16.20.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.20.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Q<span id="S4.F4.16.20.4.1.1" class="ltx_text ltx_font_medium">: How many boats do you see?</span></span></td>
</tr>
<tr id="S4.F4.16.21" class="ltx_tr">
<td id="S4.F4.16.21.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.21.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.21.1.1.1" class="ltx_text ltx_font_medium">: yes</span></span></td>
<td id="S4.F4.16.21.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.21.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.21.2.1.1" class="ltx_text ltx_font_medium">: pug</span></span></td>
<td id="S4.F4.16.21.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.21.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.21.3.1.1" class="ltx_text ltx_font_medium">: cutting</span></span></td>
<td id="S4.F4.16.21.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2"><span id="S4.F4.16.21.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GT-A<span id="S4.F4.16.21.4.1.1" class="ltx_text ltx_font_medium">: 7</span></span></td>
</tr>
<tr id="S4.F4.16.22" class="ltx_tr">
<td id="S4.F4.16.22.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.1.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.1.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): yes <span id="S4.F4.16.22.1.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.22.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.2.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.2.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): yes <span id="S4.F4.16.22.2.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.22.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.3.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.3.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): pug <span id="S4.F4.16.22.3.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.22.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.4.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.4.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): bulldog <span id="S4.F4.16.22.4.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
<td id="S4.F4.16.22.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.5.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.5.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): texting <span id="S4.F4.16.22.5.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
<td id="S4.F4.16.22.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.6.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.6.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): cutting <span id="S4.F4.16.22.6.1.1.2" class="ltx_text" style="color:#00FF00;">✓</span></span></span></td>
<td id="S4.F4.16.22.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.7.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.7.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>): 5 <span id="S4.F4.16.22.7.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
<td id="S4.F4.16.22.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><span id="S4.F4.16.22.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">A<span id="S4.F4.16.22.8.1.1" class="ltx_text ltx_font_medium">(<span id="S4.F4.16.22.8.1.1.1" class="ltx_text ltx_font_smallcaps">G</span>): 4 <span id="S4.F4.16.22.8.1.1.2" class="ltx_text" style="color:#FF0000;">✗</span></span></span></td>
</tr>
<tr id="S4.F4.16.16" class="ltx_tr">
<td id="S4.F4.9.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000134688_134688001_td_rois.jpg" id="S4.F4.9.9.1.g1" class="ltx_graphics ltx_img_landscape" width="61" height="46" alt="Refer to caption"></td>
<td id="S4.F4.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000134688_134688001_td_grids.jpg" id="S4.F4.10.10.2.g1" class="ltx_graphics ltx_img_landscape" width="61" height="46" alt="Refer to caption"></td>
<td id="S4.F4.11.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000521133_521133000_td_rois.jpg" id="S4.F4.11.11.3.g1" class="ltx_graphics ltx_img_landscape" width="62" height="46" alt="Refer to caption"></td>
<td id="S4.F4.12.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000521133_521133000_td_grids.jpg" id="S4.F4.12.12.4.g1" class="ltx_graphics ltx_img_landscape" width="62" height="46" alt="Refer to caption"></td>
<td id="S4.F4.13.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000478812_478812014_td_rois.jpg" id="S4.F4.13.13.5.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.14.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000478812_478812014_td_grids.jpg" id="S4.F4.14.14.6.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.15.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000202652_202652002_td_rois.jpg" id="S4.F4.15.15.7.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
<td id="S4.F4.16.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;"><img src="/html/2001.03615/assets/figs/top_down_attention/COCO_val2014_000000202652_202652002_td_grids.jpg" id="S4.F4.16.16.8.g1" class="ltx_graphics ltx_img_landscape" width="69" height="46" alt="Refer to caption"></td>
</tr>
<tr id="S4.F4.16.23" class="ltx_tr">
<td id="S4.F4.16.23.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">(a)</td>
<td id="S4.F4.16.23.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">(b)</td>
<td id="S4.F4.16.23.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">(c)</td>
<td id="S4.F4.16.23.4" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.0pt;padding-right:0.0pt;" colspan="2">(d)</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S4.F4.19.1" class="ltx_text ltx_font_bold">Visualizations of attention maps overlaid on images</span> produced by VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Source images taken from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to compare against bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> on VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. We show questions (Q), ground-truth answers (GT-A), and side-by-side predictions (attention maps, answers) of region (R) and grid (G) features. <span id="S4.F4.20.2" class="ltx_text ltx_font_bold">From left to right</span>: (a) both region and grid features give correct answers, (b) region features give correct answers but grid features fail, (c) region features fail but grid features give correct answers, and (d) both region and grid features fail. Best viewed in color.</figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Qualitative Comparison</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We visualize attention maps over input images from the top-down attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, together with answers from both regions and grids in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.4 Test Accuracy and Inference Time ‣ 4 Main Comparison: Regions vs. Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Source images are taken from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> on which VQA 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> benchmark is built. To obtain the attention map, we propagate the attention value of each region or grid to its corresponding pixels, and then average the attention value for each pixel (normalizing them individually to [0, 1]).
As can be seen, both types of features are able to capture relevant concepts in input images (<em id="S4.SS5.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS5.p1.1.2" class="ltx_text"></span>, snowfield in the top left). Naturally, attention maps of region features tend to cover object-like regions, while for grid features the attention does not necessarily cover the full area the supporting concept (<em id="S4.SS5.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS5.p1.1.4" class="ltx_text"></span>, the snowfield), which can be used to answer the question. However, both features are able to answer visual questions well, suggesting that localization is important, but accurate object detection of individual objects is not crucial for VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">We show failure cases of region and grid features in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.4 Test Accuracy and Inference Time ‣ 4 Main Comparison: Regions vs. Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (b)(c)(d). In most examples, the models attend to the supporting concepts but still give wrong answers. In the cases where both region and grid features fail, specifically designed modules may be needed (<em id="S4.SS5.p2.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S4.SS5.p2.1.2" class="ltx_text"></span>, counting module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> in the bottom right example) to answer the question correctly.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.2.3" class="ltx_tr">
<td id="S4.T3.2.3.1" class="ltx_td ltx_border_r" style="padding:0.8pt 2.5pt;"></td>
<td id="S4.T3.2.3.2" class="ltx_td ltx_border_r" style="padding:0.8pt 2.5pt;"></td>
<td id="S4.T3.2.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.5pt;"><span id="S4.T3.2.3.3.1" class="ltx_text" style="font-size:80%;">accuracy</span></td>
<td id="S4.T3.2.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.5pt;"><span id="S4.T3.2.3.4.1" class="ltx_text" style="font-size:80%;">pre-training task</span></td>
<td id="S4.T3.2.3.5" class="ltx_td ltx_align_center" style="padding:0.8pt 2.5pt;"><span id="S4.T3.2.3.5.1" class="ltx_text" style="font-size:80%;">input size</span></td>
</tr>
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.5pt;" rowspan="2"><span id="S4.T3.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">G</span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.5pt;"><em id="S4.T3.1.1.3.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">prev.</em></td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.5pt;"><span id="S4.T3.1.1.4.1" class="ltx_text" style="font-size:80%;">60.76</span></td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 2.5pt;">
<span id="S4.T3.1.1.5.1" class="ltx_text" style="font-size:80%;">ImageNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.1.1.5.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S4.T3.1.1.5.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.T3.1.1.5.4" class="ltx_text" style="font-size:80%;"> classification</span>
</td>
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 2.5pt;">
<span id="S4.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">448</span><math id="S4.T3.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.1.1.1.m1.1a"><mo mathsize="80%" id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><times id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\times</annotation></semantics></math><span id="S4.T3.1.1.1.2" class="ltx_text" style="font-size:80%;">448</span>
</td>
</tr>
<tr id="S4.T3.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.5pt;"><em id="S4.T3.2.2.2.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">ours</em></td>
<td id="S4.T3.2.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.5pt;"><span id="S4.T3.2.2.3.1" class="ltx_text" style="font-size:80%;">64.37</span></td>
<td id="S4.T3.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 2.5pt;">
<span id="S4.T3.2.2.4.1" class="ltx_text" style="font-size:80%;">VG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T3.2.2.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S4.T3.2.2.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.T3.2.2.4.4" class="ltx_text" style="font-size:80%;"> object+attribute detection</span>
</td>
<td id="S4.T3.2.2.1" class="ltx_td ltx_align_center" style="padding:0.8pt 2.5pt;">
<span id="S4.T3.2.2.1.1" class="ltx_text" style="font-size:80%;">600</span><math id="S4.T3.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T3.2.2.1.m1.1a"><mo mathsize="80%" id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><times id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">\times</annotation></semantics></math><span id="S4.T3.2.2.1.2" class="ltx_text" style="font-size:80%;">1000</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison between the conventional <span id="S4.T3.9.1" class="ltx_text ltx_font_bold">ImageNet pre-trained and our proposed grid features</span> on the VQA 2.0 <span id="S4.T3.10.2" class="ltx_text ltx_font_typewriter">vqa-eval</span> set. Besides VQA accuracy, we list two major differences between the two: 1) pre-training task and 2) input image size.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Why do Our Grid Features Work?</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">As we mentioned in Sec. <a href="#S2" title="2 Related Work ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, grid features are not new – in fact, they were widely used in vision and language tasks before the introduction of bottom-up attention features. Compared to the previous attempts at grid features, why do <em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">our</em> grid features work well? In Table <a href="#S4.T3" title="Table 3 ‣ 4.5 Qualitative Comparison ‣ 4 Main Comparison: Regions vs. Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we show the performance of grid-based methods (ResNet-50 <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="C_{5}" display="inline"><semantics id="S5.p1.1.m1.1a"><msub id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml">C</mi><mn id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml">5</mn></msub><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2">𝐶</ci><cn type="integer" id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">C_{5}</annotation></semantics></math> features) for different settings and find that there are two major factors: 1) input image size; 2) pre-training task. We study both these factors next and report results on the <span id="S5.p1.1.2" class="ltx_text ltx_font_typewriter">vqa-eval</span> set.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Factor 1: Input Image Size</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">The standard image size used during feature extraction for ImageNet pre-trained models is 448<math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><times id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\times</annotation></semantics></math>448 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> discarding the aspect ratio; whereas for VG detection in bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the default size is 600<math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mo id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><times id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\times</annotation></semantics></math>1000 while keeping the aspect ratio intact. Therefore, we experimented with different combinations and reported results for all of them in Table <a href="#S5.T4" title="Table 4 ‣ 5.1 Factor 1: Input Image Size ‣ 5 Why do Our Grid Features Work? ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We note that for grid features, a larger input size means more features for the VQA model.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.3" class="ltx_p">From the table, we find that grid features benefit from larger images as input, indicating this factor is indeed important. However, input size has a different effect for models pre-trained on ImageNet <em id="S5.SS1.p2.3.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S5.SS1.p2.3.2" class="ltx_text"></span> VG. For ImageNet models which are pre-trained on smaller images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, the performance saturates around 600<math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><times id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">\times</annotation></semantics></math>1000. Interestingly, the performance of VG models improves with the input size and continues to increase even at 800<math id="S5.SS1.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.2.m2.1a"><mo id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><times id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">\times</annotation></semantics></math>1333. We still use 600<math id="S5.SS1.p2.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS1.p2.3.m3.1a"><mo id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><times id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">\times</annotation></semantics></math>1000 for the rest of the paper.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T4.1.1" class="ltx_tr">
<td id="S5.T4.1.1.2" class="ltx_td ltx_border_r" style="padding:0.8pt 5.0pt;"></td>
<td id="S5.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;" rowspan="2"><span id="S5.T4.1.1.3.1" class="ltx_text" style="font-size:80%;">dataset</span></td>
<td id="S5.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;" colspan="2"><span id="S5.T4.1.1.4.1" class="ltx_text" style="font-size:80%;">input size</span></td>
<td id="S5.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;" rowspan="2"><span id="S5.T4.1.1.1.1" class="ltx_text" style="font-size:80%;"><span id="S5.T4.1.1.1.1.2" class="ltx_text"></span> <span id="S5.T4.1.1.1.1.1" class="ltx_text">
<span id="S5.T4.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S5.T4.1.1.1.1.1.1.2" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 5.0pt;"># features</span></span>
<span id="S5.T4.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T4.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 5.0pt;"><math id="S5.T4.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.T4.1.1.1.1.1.1.1.1.m1.1a"><mi id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.1.1.m1.1c">N</annotation></semantics></math></span></span>
</span></span> <span id="S5.T4.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="S5.T4.1.1.5" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;" rowspan="2"><span id="S5.T4.1.1.5.1" class="ltx_text" style="font-size:80%;">accuracy</span></td>
</tr>
<tr id="S5.T4.1.2" class="ltx_tr">
<td id="S5.T4.1.2.1" class="ltx_td ltx_border_r" style="padding:0.8pt 5.0pt;"></td>
<td id="S5.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.2.2.1" class="ltx_text" style="font-size:80%;">shorter side</span></td>
<td id="S5.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.2.3.1" class="ltx_text" style="font-size:80%;">longer side</span></td>
</tr>
<tr id="S5.T4.1.3" class="ltx_tr">
<td id="S5.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;" rowspan="8"><span id="S5.T4.1.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">G</span></td>
<td id="S5.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;" rowspan="4"><span id="S5.T4.1.3.2.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T4.1.3.2.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.0pt;height:33.7pt;vertical-align:-14.9pt;"><span class="ltx_transformed_inner" style="width:33.8pt;transform:translate(-13.38pt,2.33pt) rotate(-90deg) ;">
<span id="S5.T4.1.3.2.1.1.1" class="ltx_p">ImageNet</span>
</span></span></span></td>
<td id="S5.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.3.3.1" class="ltx_text" style="font-size:80%;">448</span></td>
<td id="S5.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.3.4.1" class="ltx_text" style="font-size:80%;">448</span></td>
<td id="S5.T4.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.3.5.1" class="ltx_text" style="font-size:80%;">196</span></td>
<td id="S5.T4.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.3.6.1" class="ltx_text" style="font-size:80%;">60.76</span></td>
</tr>
<tr id="S5.T4.1.4" class="ltx_tr">
<td id="S5.T4.1.4.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.4.1.1" class="ltx_text" style="font-size:80%;">448</span></td>
<td id="S5.T4.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.4.2.1" class="ltx_text" style="font-size:80%;">746</span></td>
<td id="S5.T4.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.4.3.1" class="ltx_text" style="font-size:80%;">336</span></td>
<td id="S5.T4.1.4.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.4.4.1" class="ltx_text" style="font-size:80%;">61.21</span></td>
</tr>
<tr id="S5.T4.1.5" class="ltx_tr">
<td id="S5.T4.1.5.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.5.1.1" class="ltx_text" style="font-size:80%;">600</span></td>
<td id="S5.T4.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.5.2.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S5.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.5.3.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="S5.T4.1.5.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.5.4.1" class="ltx_text" style="font-size:80%;">61.52</span></td>
</tr>
<tr id="S5.T4.1.6" class="ltx_tr">
<td id="S5.T4.1.6.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.6.1.1" class="ltx_text" style="font-size:80%;">800</span></td>
<td id="S5.T4.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.6.2.1" class="ltx_text" style="font-size:80%;">1333</span></td>
<td id="S5.T4.1.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.6.3.1" class="ltx_text" style="font-size:80%;">1050</span></td>
<td id="S5.T4.1.6.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.6.4.1" class="ltx_text" style="font-size:80%;">61.52</span></td>
</tr>
<tr id="S5.T4.1.7" class="ltx_tr">
<td id="S5.T4.1.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;" rowspan="4"><span id="S5.T4.1.7.1.1" class="ltx_text" style="font-size:80%;">
<span id="S5.T4.1.7.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:12.1pt;vertical-align:-3.3pt;"><span class="ltx_transformed_inner" style="width:12.1pt;transform:translate(-3.29pt,0pt) rotate(-90deg) ;">
<span id="S5.T4.1.7.1.1.1.1" class="ltx_p">VG</span>
</span></span></span></td>
<td id="S5.T4.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.7.2.1" class="ltx_text" style="font-size:80%;">448</span></td>
<td id="S5.T4.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.7.3.1" class="ltx_text" style="font-size:80%;">448</span></td>
<td id="S5.T4.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.7.4.1" class="ltx_text" style="font-size:80%;">196</span></td>
<td id="S5.T4.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.7.5.1" class="ltx_text" style="font-size:80%;">63.24</span></td>
</tr>
<tr id="S5.T4.1.8" class="ltx_tr">
<td id="S5.T4.1.8.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.8.1.1" class="ltx_text" style="font-size:80%;">448</span></td>
<td id="S5.T4.1.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.8.2.1" class="ltx_text" style="font-size:80%;">746</span></td>
<td id="S5.T4.1.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.8.3.1" class="ltx_text" style="font-size:80%;">336</span></td>
<td id="S5.T4.1.8.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.8.4.1" class="ltx_text" style="font-size:80%;">63.81</span></td>
</tr>
<tr id="S5.T4.1.9" class="ltx_tr">
<td id="S5.T4.1.9.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.9.1.1" class="ltx_text" style="font-size:80%;">600</span></td>
<td id="S5.T4.1.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.9.2.1" class="ltx_text" style="font-size:80%;">1000</span></td>
<td id="S5.T4.1.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.9.3.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="S5.T4.1.9.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.9.4.1" class="ltx_text" style="font-size:80%;">64.37</span></td>
</tr>
<tr id="S5.T4.1.10" class="ltx_tr">
<td id="S5.T4.1.10.1" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.10.1.1" class="ltx_text" style="font-size:80%;">800</span></td>
<td id="S5.T4.1.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.10.2.1" class="ltx_text" style="font-size:80%;">1333</span></td>
<td id="S5.T4.1.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.10.3.1" class="ltx_text" style="font-size:80%;">1050</span></td>
<td id="S5.T4.1.10.4" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="S5.T4.1.10.4.1" class="ltx_text" style="font-size:80%;">64.61</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="S5.T4.10.1" class="ltx_text ltx_font_bold">Impact of input image size</span> on the VQA 2.0 <span id="S5.T4.11.2" class="ltx_text ltx_font_typewriter">vqa-eval</span> set. Grid features benefit from larger input image sizes. For an ImageNet pre-trained model, the accuracy saturates around 600<math id="S5.T4.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T4.3.m1.1b"><mo id="S5.T4.3.m1.1.1" xref="S5.T4.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.m1.1c"><times id="S5.T4.3.m1.1.1.cmml" xref="S5.T4.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.m1.1d">\times</annotation></semantics></math>1000 but the VG model makes a better use of larger input image sizes.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Factor 2: Pre-Training Task</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">We now study the difference in VQA accuracy due to the pre-training task in the ImageNet (classification) and VG (detection)<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Strictly speaking, VG also uses ImageNet classification for pre-training, because the detector is fine-tuned from a standard ImageNet pre-trained model.</span></span></span>. To understand these differences better, we introduce an additional pre-trained model in each setting. For classification, we include a model trained on YFCC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, which has 92M images with image tags. For detection, we include a standard model from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> which only has object annotations (no attributes). All models use a ResNet-50 backbone for fair comparison.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The results are shown in Table <a href="#S5.T5" title="Table 5 ‣ 5.2 Factor 2: Pre-Training Task ‣ 5 Why do Our Grid Features Work? ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. In the image classification pre-trained setting, the YFCC model (trained on weak image level tags), performs better than the ImageNet model, possibly because it is trained on two orders of magnitude more data. For detection based pre-training, the VG model (trained with objects and attributes) gives better results than the COCO model. The larger number of categories in VG compared to COCO (1600 <em id="S5.SS2.p2.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S5.SS2.p2.1.2" class="ltx_text"></span> 80) or the additional attribute annotations it has are two possible reasons for the improved performance. We study the impact of attributes next.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T5.3.1" class="ltx_tr">
<td id="S5.T5.3.1.1" class="ltx_td ltx_border_r" style="padding:1.2pt 4.0pt;"></td>
<td id="S5.T5.3.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;" colspan="4"><span id="S5.T5.3.1.2.1" class="ltx_text" style="font-size:80%;">pre-train task</span></td>
<td id="S5.T5.3.1.3" class="ltx_td ltx_align_center" style="padding:1.2pt 4.0pt;" rowspan="2"><span id="S5.T5.3.1.3.1" class="ltx_text" style="font-size:80%;">accuracy</span></td>
</tr>
<tr id="S5.T5.3.2" class="ltx_tr">
<td id="S5.T5.3.2.1" class="ltx_td ltx_border_r" style="padding:1.2pt 4.0pt;"></td>
<td id="S5.T5.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.2.2.1" class="ltx_text" style="font-size:80%;">setup</span></td>
<td id="S5.T5.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.2.3.1" class="ltx_text" style="font-size:80%;">dataset</span></td>
<td id="S5.T5.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.2.4.1" class="ltx_text" style="font-size:80%;">annotation</span></td>
<td id="S5.T5.3.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.2.5.1" class="ltx_text" style="font-size:80%;">#images</span></td>
</tr>
<tr id="S5.T5.3.3" class="ltx_tr">
<td id="S5.T5.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;" rowspan="4"><span id="S5.T5.3.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">G</span></td>
<td id="S5.T5.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.3.2.1" class="ltx_text" style="font-size:80%;">cls</span></td>
<td id="S5.T5.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;">
<span id="S5.T5.3.3.3.1" class="ltx_text" style="font-size:80%;">ImageNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.3.3.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="S5.T5.3.3.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T5.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.3.4.1" class="ltx_text" style="font-size:80%;">image label</span></td>
<td id="S5.T5.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.3.5.1" class="ltx_text" style="font-size:80%;">1.3M</span></td>
<td id="S5.T5.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.3.6.1" class="ltx_text" style="font-size:80%;">61.52</span></td>
</tr>
<tr id="S5.T5.3.4" class="ltx_tr">
<td id="S5.T5.3.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.4.1.1" class="ltx_text" style="font-size:80%;">cls</span></td>
<td id="S5.T5.3.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;">
<span id="S5.T5.3.4.2.1" class="ltx_text" style="font-size:80%;">YFCC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.3.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S5.T5.3.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T5.3.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.4.3.1" class="ltx_text" style="font-size:80%;">image tag</span></td>
<td id="S5.T5.3.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.4.4.1" class="ltx_text" style="font-size:80%;">92M</span></td>
<td id="S5.T5.3.4.5" class="ltx_td ltx_align_center" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.4.5.1" class="ltx_text" style="font-size:80%;">62.72</span></td>
</tr>
<tr id="S5.T5.3.5" class="ltx_tr">
<td id="S5.T5.3.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.5.1.1" class="ltx_text" style="font-size:80%;">det</span></td>
<td id="S5.T5.3.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;">
<span id="S5.T5.3.5.2.1" class="ltx_text" style="font-size:80%;">COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.3.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S5.T5.3.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T5.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.5.3.1" class="ltx_text" style="font-size:80%;">object box</span></td>
<td id="S5.T5.3.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.5.4.1" class="ltx_text" style="font-size:80%;">118K</span></td>
<td id="S5.T5.3.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.5.5.1" class="ltx_text" style="font-size:80%;">62.46</span></td>
</tr>
<tr id="S5.T5.3.6" class="ltx_tr">
<td id="S5.T5.3.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.6.1.1" class="ltx_text" style="font-size:80%;">det</span></td>
<td id="S5.T5.3.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;">
<span id="S5.T5.3.6.2.1" class="ltx_text" style="font-size:80%;">VG </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.T5.3.6.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S5.T5.3.6.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S5.T5.3.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.6.3.1" class="ltx_text" style="font-size:80%;">object+attribute</span></td>
<td id="S5.T5.3.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.6.4.1" class="ltx_text" style="font-size:80%;">103K</span></td>
<td id="S5.T5.3.6.5" class="ltx_td ltx_align_center" style="padding:1.2pt 4.0pt;"><span id="S5.T5.3.6.5.1" class="ltx_text" style="font-size:80%;">64.37</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="S5.T5.10.1" class="ltx_text ltx_font_bold">Choice of pre-training task.</span> We explore the impact of the type of pre-training task on the final performance while keeping the input size fixed at 600<math id="S5.T5.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.T5.2.m1.1b"><mo id="S5.T5.2.m1.1.1" xref="S5.T5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.m1.1c"><times id="S5.T5.2.m1.1.1.cmml" xref="S5.T5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.m1.1d">\times</annotation></semantics></math>1000. Results reported on <span id="S5.T5.11.2" class="ltx_text ltx_font_typewriter">vqa-eval</span>. We broadly characterize the pre-training tasks into two types - object detection (‘det’) and image classification (‘cls’).</figcaption>
</figure>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Attributes.</h4>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">Fig. <a href="#S5.F5" title="Figure 5 ‣ Attributes. ‣ 5.2 Factor 2: Pre-Training Task ‣ 5 Why do Our Grid Features Work? ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the impact of the attribute loss weight on VQA accuracy. Setting the attribute loss weight to zero during pre-training on VG, results in a drop in VQA performance. In fact, the VQA accuracy in this case matches the accuracy from a pre-trained COCO model suggesting that attributes in the pre-training task are a major reason for the better performance of VG models. We also note that the grid features consistently outperform the region features for all values of the attribute loss weight.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2001.03615/assets/x4.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="244" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Analysis on <span id="S5.F5.3.1" class="ltx_text ltx_font_bold">attribute loss weights</span> when pre-training grid features on Visual Genome (VG). All results on VQA 2.0 <span id="S5.F5.4.2" class="ltx_text ltx_font_typewriter">vqa-eval</span> set. </figcaption>
</figure>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Generalization of Grid Features</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We now study whether our findings about grid features are more broadly applicable to other tasks and models. In this section, we study generalization across: 1) different backbones; 2) different VQA models; 3) different VQA tasks; 4) other tasks. For all the studies, we set the attribute loss weight to 0.2, and compare both the accuracy and speed. For regions we use top <math id="S6.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S6.p1.1.m1.1a"><mi id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><ci id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">N</annotation></semantics></math>=100 ones. Detailed hyper-parameters are in the supplementary material.</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different backbone.</h4>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px1.p1.1" class="ltx_p">We train Faster R-CNN models with ResNeXt-101-32x8d <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> backbone on VG and use the same Pythia setting from Section <a href="#S4.SS5" title="4.5 Qualitative Comparison ‣ 4 Main Comparison: Regions vs. Grids ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>. Results on VQA 2.0 <span id="S6.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_typewriter">test-dev</span> split are reported in Table <a href="#S6.T6.sf1" title="In Table 6 ‣ Different VQA task. ‣ 6 Generalization of Grid Features ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6a</span></a>. We find that our grid features are competitive to the region features even on this more powerful backbone model. Speed-wise, grid features still run substantially faster (23.8<math id="S6.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.1.m1.1a"><mo id="S6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.1.m1.1b"><times id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.1.m1.1c">\times</annotation></semantics></math>) than region ones.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different VQA model.</h4>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">We further test our features obtained from the previous ResNeXt-101 backbone with the state-of-the-art VQA model, MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> (2019 VQA Challenge winner). We use the open-sourced implementation<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/MILVLG/mcan-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/MILVLG/mcan-vqa</a></span></span></span> to train the <em id="S6.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">large</em> version of the model. The results on VQA 2.0 <span id="S6.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_typewriter">test-dev</span> set are in Table <a href="#S6.T6.sf2" title="In Table 6 ‣ Different VQA task. ‣ 6 Generalization of Grid Features ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6b</span></a>, where our own region features perform better than the results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> due to stronger backbone. On top of that, our grid features work even <em id="S6.SS0.SSS0.Px2.p1.1.3" class="ltx_emph ltx_font_italic">better</em> than regions, leading to significant improvement over results reported in MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> (+1.66). This final model reports a state-of-the-art <span id="S6.SS0.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_typewriter">test-std</span> result of <span id="S6.SS0.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_bold">72.71</span> (single-model performance) for future reference.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Different VQA task.</h4>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px3.p1.1" class="ltx_p">We use the VizWiz VQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, which is a real world dataset of pictures taken with cellphones by visually-impaired users. It is more challenging due to poor image quality, conversation-style questions, and unanswerable questions, <em id="S6.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>. Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> model is used (2018 challenge winner). Results on the <span id="S6.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_typewriter">test-dev</span> set of VizWiz are reported in Table <a href="#S6.T6.sf3" title="In Table 6 ‣ Different VQA task. ‣ 6 Generalization of Grid Features ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6c</span></a>, where our grid features achieve comparable results to the regions. It is worth pointing out that our grid features run much faster (23<math id="S6.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS0.SSS0.Px3.p1.1.m1.1a"><mo id="S6.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.1.m1.1b"><times id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.1.m1.1c">\times</annotation></semantics></math>), which provides great potential to be deployed in practice, <em id="S6.SS0.SSS0.Px3.p1.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.SS0.SSS0.Px3.p1.1.4" class="ltx_text"></span>, on cell phones, to better assist the visually-impaired.</p>
</div>
<figure id="S6.T6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.T6.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="S6.T6.sf1.1" class="ltx_p"><span id="S6.T6.sf1.1.1" class="ltx_text ltx_inline-block" style="width:91.1pt;">
<span id="S6.T6.sf1.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="S6.T6.sf1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf1.1.1.1.1.1" class="ltx_tr">
<span id="S6.T6.sf1.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding:0.8pt 3.0pt;"></span>
<span id="S6.T6.sf1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">accuracy</span>
<span id="S6.T6.sf1.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf1.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S6.T6.sf1.1.1.1.1.1.3.2" class="ltx_text">
<span id="S6.T6.sf1.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf1.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S6.T6.sf1.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">time</span></span>
<span id="S6.T6.sf1.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S6.T6.sf1.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">(ms)</span></span>
</span></span><span id="S6.T6.sf1.1.1.1.1.1.3.3" class="ltx_text"></span></span></span>
<span id="S6.T6.sf1.1.1.1.1.2" class="ltx_tr">
<span id="S6.T6.sf1.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>
<span id="S6.T6.sf1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">68.31</span>
<span id="S6.T6.sf1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">-</span></span>
<span id="S6.T6.sf1.1.1.1.1.3" class="ltx_tr">
<span id="S6.T6.sf1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf1.1.1.1.1.3.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="S6.T6.sf1.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">68.21</span>
<span id="S6.T6.sf1.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">929</span></span>
<span id="S6.T6.sf1.1.1.1.1.4" class="ltx_tr">
<span id="S6.T6.sf1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf1.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="S6.T6.sf1.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">67.76</span>
<span id="S6.T6.sf1.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">39</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.T6.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="S6.T6.sf2.1" class="ltx_p"><span id="S6.T6.sf2.1.1" class="ltx_text ltx_inline-block" style="width:91.1pt;">
<span id="S6.T6.sf2.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="S6.T6.sf2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf2.1.1.1.1.1" class="ltx_tr">
<span id="S6.T6.sf2.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding:0.8pt 3.0pt;"></span>
<span id="S6.T6.sf2.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">accuracy</span>
<span id="S6.T6.sf2.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf2.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S6.T6.sf2.1.1.1.1.1.3.2" class="ltx_text">
<span id="S6.T6.sf2.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf2.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S6.T6.sf2.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">time</span></span>
<span id="S6.T6.sf2.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S6.T6.sf2.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">(ms)</span></span>
</span></span><span id="S6.T6.sf2.1.1.1.1.1.3.3" class="ltx_text"></span></span></span>
<span id="S6.T6.sf2.1.1.1.1.2" class="ltx_tr">
<span id="S6.T6.sf2.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></span>
<span id="S6.T6.sf2.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">70.93</span>
<span id="S6.T6.sf2.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">-</span></span>
<span id="S6.T6.sf2.1.1.1.1.3" class="ltx_tr">
<span id="S6.T6.sf2.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf2.1.1.1.1.3.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="S6.T6.sf2.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">72.01</span>
<span id="S6.T6.sf2.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">963</span></span>
<span id="S6.T6.sf2.1.1.1.1.4" class="ltx_tr">
<span id="S6.T6.sf2.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf2.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="S6.T6.sf2.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">72.59</span>
<span id="S6.T6.sf2.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">72</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.T6.sf3" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="S6.T6.sf3.1" class="ltx_p"><span id="S6.T6.sf3.1.1" class="ltx_text ltx_inline-block" style="width:91.1pt;">
<span id="S6.T6.sf3.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="S6.T6.sf3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf3.1.1.1.1.1" class="ltx_tr">
<span id="S6.T6.sf3.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding:0.8pt 3.0pt;"></span>
<span id="S6.T6.sf3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">accuracy</span>
<span id="S6.T6.sf3.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf3.1.1.1.1.1.3.1" class="ltx_text"></span> <span id="S6.T6.sf3.1.1.1.1.1.3.2" class="ltx_text">
<span id="S6.T6.sf3.1.1.1.1.1.3.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf3.1.1.1.1.1.3.2.1.1" class="ltx_tr">
<span id="S6.T6.sf3.1.1.1.1.1.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">time</span></span>
<span id="S6.T6.sf3.1.1.1.1.1.3.2.1.2" class="ltx_tr">
<span id="S6.T6.sf3.1.1.1.1.1.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">(ms)</span></span>
</span></span><span id="S6.T6.sf3.1.1.1.1.1.3.3" class="ltx_text"></span></span></span>
<span id="S6.T6.sf3.1.1.1.1.2" class="ltx_tr">
<span id="S6.T6.sf3.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>
<span id="S6.T6.sf3.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">54.22</span>
<span id="S6.T6.sf3.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">-</span></span>
<span id="S6.T6.sf3.1.1.1.1.3" class="ltx_tr">
<span id="S6.T6.sf3.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf3.1.1.1.1.3.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="S6.T6.sf3.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">54.28</span>
<span id="S6.T6.sf3.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">874</span></span>
<span id="S6.T6.sf3.1.1.1.1.4" class="ltx_tr">
<span id="S6.T6.sf3.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf3.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="S6.T6.sf3.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">54.17</span>
<span id="S6.T6.sf3.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">38</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(c) </span>
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.T6.sf4" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="S6.T6.sf4.1" class="ltx_p"><span id="S6.T6.sf4.1.1" class="ltx_text ltx_inline-block" style="width:147.4pt;">
<span id="S6.T6.sf4.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="S6.T6.sf4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf4.1.1.1.1.1" class="ltx_tr">
<span id="S6.T6.sf4.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding:0.8pt 3.0pt;"></span>
<span id="S6.T6.sf4.1.1.1.1.1.2" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">B4</span>
<span id="S6.T6.sf4.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">M</span>
<span id="S6.T6.sf4.1.1.1.1.1.4" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">C</span>
<span id="S6.T6.sf4.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">S</span>
<span id="S6.T6.sf4.1.1.1.1.1.6" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf4.1.1.1.1.1.6.1" class="ltx_text"></span> <span id="S6.T6.sf4.1.1.1.1.1.6.2" class="ltx_text">
<span id="S6.T6.sf4.1.1.1.1.1.6.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T6.sf4.1.1.1.1.1.6.2.1.1" class="ltx_tr">
<span id="S6.T6.sf4.1.1.1.1.1.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">time</span></span>
<span id="S6.T6.sf4.1.1.1.1.1.6.2.1.2" class="ltx_tr">
<span id="S6.T6.sf4.1.1.1.1.1.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">(ms)</span></span>
</span></span><span id="S6.T6.sf4.1.1.1.1.1.6.3" class="ltx_text"></span></span></span>
<span id="S6.T6.sf4.1.1.1.1.2" class="ltx_tr">
<span id="S6.T6.sf4.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">BUTD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>
<span id="S6.T6.sf4.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">36.2</span>
<span id="S6.T6.sf4.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">27.0</span>
<span id="S6.T6.sf4.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">113.5</span>
<span id="S6.T6.sf4.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">20.3</span>
<span id="S6.T6.sf4.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">-</span></span>
<span id="S6.T6.sf4.1.1.1.1.3" class="ltx_tr">
<span id="S6.T6.sf4.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf4.1.1.1.1.3.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="S6.T6.sf4.1.1.1.1.3.2" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">36.2</span>
<span id="S6.T6.sf4.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">27.7</span>
<span id="S6.T6.sf4.1.1.1.1.3.4" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">113.9</span>
<span id="S6.T6.sf4.1.1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">20.8</span>
<span id="S6.T6.sf4.1.1.1.1.3.6" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">1101</span></span>
<span id="S6.T6.sf4.1.1.1.1.4" class="ltx_tr">
<span id="S6.T6.sf4.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;"><span id="S6.T6.sf4.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="S6.T6.sf4.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">36.4</span>
<span id="S6.T6.sf4.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">27.4</span>
<span id="S6.T6.sf4.1.1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">113.8</span>
<span id="S6.T6.sf4.1.1.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">20.7</span>
<span id="S6.T6.sf4.1.1.1.1.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">240</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(d) </span>
</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="S6.T6.6.1" class="ltx_text ltx_font_bold">Generalizations of grid features</span>. From left to right: (a) Different <span id="S6.T6.7.2" class="ltx_text ltx_font_bold">backbone.</span> We use a ResNeXt-101-32x8d instead of a ResNet-50 as the backbone. (b) Different <span id="S6.T6.8.3" class="ltx_text ltx_font_bold">VQA model</span>. We use MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> implementation which is the state-of-the-art VQA model. (c) Accuracy on <span id="S6.T6.9.4" class="ltx_text ltx_font_bold">VizWiz</span> using the same VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. (d) <span id="S6.T6.10.5" class="ltx_text ltx_font_bold">Image captioning</span> on COCO Karpathy test split. Abbreviations: BLEU4 (B4), METEOR (M), CIDEr (C), and SPICE (S). Our grid features generalize well by achieving results at-par with bottom-up region features while being significantly faster.</figcaption>
</figure>
</section>
<section id="S6.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Image captioning.</h4>

<div id="S6.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S6.SS0.SSS0.Px4.p1.1" class="ltx_p">We train the bottom-up attention model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> implemented in Pythia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> taking our features as input for image captioning on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. No CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is used for fair comparison. Quantitative results on the test set of Karpathy split <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> are reported in Table <a href="#S6.T6.sf4" title="In Table 6 ‣ Different VQA task. ‣ 6 Generalization of Grid Features ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6d</span></a>. We use standard evaluation metrics including BLEU4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, CIDEr, and SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Similar to the VQA task, our grid features achieve comparable results to bottom-up region ones for image captioning while being significantly faster.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<table id="S6.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T7.1.1" class="ltx_tr">
<td id="S6.T7.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;" colspan="2"><span id="S6.T7.1.1.2.1" class="ltx_text" style="font-size:80%;">pre-train task</span></td>
<td id="S6.T7.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="2"><span id="S6.T7.1.1.3.1" class="ltx_text" style="font-size:80%;">e2e</span></td>
<td id="S6.T7.1.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="2"><span id="S6.T7.1.1.4.1" class="ltx_text" style="font-size:80%;"><span id="S6.T7.1.1.4.1.1" class="ltx_text"></span> <span id="S6.T7.1.1.4.1.2" class="ltx_text">
<span id="S6.T7.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.1.1.4.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;">PPM</span></span>
<span id="S6.T7.1.1.4.1.2.1.2" class="ltx_tr">
<span id="S6.T7.1.1.4.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite></span></span>
</span></span> <span id="S6.T7.1.1.4.1.3" class="ltx_text"></span></span></td>
<td id="S6.T7.1.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="2"><span id="S6.T7.1.1.5.1" class="ltx_text" style="font-size:80%;">accuracy</span></td>
<td id="S6.T7.1.1.1" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="2"><span id="S6.T7.1.1.1.1" class="ltx_text" style="font-size:80%;"><math id="S6.T7.1.1.1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="S6.T7.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S6.T7.1.1.1.1.m1.1.1" xref="S6.T7.1.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S6.T7.1.1.1.1.m1.1b"><ci id="S6.T7.1.1.1.1.m1.1.1.cmml" xref="S6.T7.1.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.1.1.1.1.m1.1c">\Delta</annotation></semantics></math></span></td>
</tr>
<tr id="S6.T7.1.2" class="ltx_tr">
<td id="S6.T7.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.2.1.1" class="ltx_text" style="font-size:80%;">dataset</span></td>
<td id="S6.T7.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;">
<span id="S6.T7.1.2.2.1" class="ltx_text"></span><span id="S6.T7.1.2.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="S6.T7.1.2.2.3" class="ltx_text" style="font-size:80%;">
<span id="S6.T7.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.1.2.2.3.1.1" class="ltx_tr">
<span id="S6.T7.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;">region</span></span>
<span id="S6.T7.1.2.2.3.1.2" class="ltx_tr">
<span id="S6.T7.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;">annotations?</span></span>
</span></span><span id="S6.T7.1.2.2.4" class="ltx_text"></span><span id="S6.T7.1.2.2.5" class="ltx_text" style="font-size:80%;"></span>
</td>
</tr>
<tr id="S6.T7.1.3" class="ltx_tr">
<td id="S6.T7.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="3"><span id="S6.T7.1.3.1.1" class="ltx_text" style="font-size:80%;"><span id="S6.T7.1.3.1.1.1" class="ltx_text"></span> <span id="S6.T7.1.3.1.1.2" class="ltx_text">
<span id="S6.T7.1.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.1.3.1.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;">VG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></span></span>
</span></span> <span id="S6.T7.1.3.1.1.3" class="ltx_text"></span></span></td>
<td id="S6.T7.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="3"><span id="S6.T7.1.3.2.1" class="ltx_text" style="font-size:120%;color:#666666;">✓</span></td>
<td id="S6.T7.1.3.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.3.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.3.5.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">66.27</span></td>
<td id="S6.T7.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.3.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S6.T7.1.4" class="ltx_tr">
<td id="S6.T7.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.4.1.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.4.2" class="ltx_td ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.4.3.1" class="ltx_text" style="font-size:80%;">66.47</span></td>
<td id="S6.T7.1.4.4" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><em id="S6.T7.1.4.4.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">0.20</em></td>
</tr>
<tr id="S6.T7.1.5" class="ltx_tr">
<td id="S6.T7.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.5.1.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.5.2.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.5.3.1" class="ltx_text" style="font-size:80%;">66.74</span></td>
<td id="S6.T7.1.5.4" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><em id="S6.T7.1.5.4.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">0.47</em></td>
</tr>
<tr id="S6.T7.1.6" class="ltx_tr">
<td id="S6.T7.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="3"><span id="S6.T7.1.6.1.1" class="ltx_text" style="font-size:80%;"><span id="S6.T7.1.6.1.1.1" class="ltx_text"></span> <span id="S6.T7.1.6.1.1.2" class="ltx_text">
<span id="S6.T7.1.6.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.1.6.1.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.6.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;">ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite></span></span>
</span></span> <span id="S6.T7.1.6.1.1.3" class="ltx_text"></span></span></td>
<td id="S6.T7.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="3"><span id="S6.T7.1.6.2.1" class="ltx_text" style="font-size:120%;color:#666666;">✗</span></td>
<td id="S6.T7.1.6.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.6.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.6.5.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">63.21</span></td>
<td id="S6.T7.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.6.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S6.T7.1.7" class="ltx_tr">
<td id="S6.T7.1.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.7.1.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.7.2" class="ltx_td ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.7.3.1" class="ltx_text" style="font-size:80%;">64.98</span></td>
<td id="S6.T7.1.7.4" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><em id="S6.T7.1.7.4.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">1.77</em></td>
</tr>
<tr id="S6.T7.1.8" class="ltx_tr">
<td id="S6.T7.1.8.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.8.1.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.8.2.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.8.3.1" class="ltx_text" style="font-size:80%;">65.97</span></td>
<td id="S6.T7.1.8.4" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><em id="S6.T7.1.8.4.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">2.76</em></td>
</tr>
<tr id="S6.T7.1.9" class="ltx_tr">
<td id="S6.T7.1.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="3"><span id="S6.T7.1.9.1.1" class="ltx_text" style="font-size:80%;"><span id="S6.T7.1.9.1.1.1" class="ltx_text"></span> <span id="S6.T7.1.9.1.1.2" class="ltx_text">
<span id="S6.T7.1.9.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T7.1.9.1.1.2.1.1" class="ltx_tr">
<span id="S6.T7.1.9.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;">YFCC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite></span></span>
</span></span> <span id="S6.T7.1.9.1.1.3" class="ltx_text"></span></span></td>
<td id="S6.T7.1.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;" rowspan="3"><span id="S6.T7.1.9.2.1" class="ltx_text" style="font-size:120%;color:#666666;">✗</span></td>
<td id="S6.T7.1.9.3" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.9.4" class="ltx_td ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.9.5.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:80%;">65.04</span></td>
<td id="S6.T7.1.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.9.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S6.T7.1.10" class="ltx_tr">
<td id="S6.T7.1.10.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.10.1.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.10.2" class="ltx_td ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"></td>
<td id="S6.T7.1.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.10.3.1" class="ltx_text" style="font-size:80%;">65.35</span></td>
<td id="S6.T7.1.10.4" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><em id="S6.T7.1.10.4.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">0.31</em></td>
</tr>
<tr id="S6.T7.1.11" class="ltx_tr">
<td id="S6.T7.1.11.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.11.1.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.11.2.1" class="ltx_text" style="font-size:80%;color:#666666;">✓</span></td>
<td id="S6.T7.1.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.4pt;padding-bottom:0.4pt;"><span id="S6.T7.1.11.3.1" class="ltx_text" style="font-size:80%;">66.61</span></td>
<td id="S6.T7.1.11.4" class="ltx_td ltx_align_center" style="padding-top:0.4pt;padding-bottom:0.4pt;"><em id="S6.T7.1.11.4.1" class="ltx_emph ltx_font_italic" style="font-size:80%;">1.57</em></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Results of <span id="S6.T7.11.1" class="ltx_text ltx_font_bold">end-to-end trained VQA</span> models with grid features on the VQA 2.0 <span id="S6.T7.12.2" class="ltx_text ltx_font_typewriter">test-dev</span> set. End-to-end learning boosts accuracy for all models and more for ones trained on ImageNet and YFCC. Adding PPM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> further improves accuracy.</figcaption>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Towards End-to-end VQA</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Although pre-training on VG, ImageNet, or YFCC provides useful feature representations for VQA, there are still potential domain shifts between the pre-training tasks and the downstream tasks. For example, YFCC contains a lot of outdoor images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, which are not present in the VQA dataset. Instead of using pre-computed fixed feature representations, <em id="S7.p1.1.1" class="ltx_emph ltx_font_italic">end-to-end</em> training, where the initial feature representations will be fine-tuned, provides a natural solution to reducing such domain gaps. Empowered by the dramatic simplification of grid features for the VQA pipeline, we take an initial step towards this goal.</p>
</div>
<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Training details.</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">We adopt the 22K learning rate schedule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to train both the ResNet-50 model and the Pythia VQA model jointly, with errors from the answering accuracy <em id="S7.SS0.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">directly</em> back-propagated to the grid convolutional feature maps. We fix the first two residual blocks and fine-tune the rest of the model. Since the visual representations are computed online (not stored on disk), it allows us to perform data augmentation including color jitter and affine transformation over the input images to reduce chance of over-fitting. For more details see supplementary material.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results.</h4>

<div id="S7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p1.1" class="ltx_p">We experiment with three models pre-trained on VG, ImageNet, and YFCC. Note that while VG uses <em id="S7.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">region</em>-level annotations, both ImageNet and YFCC only use <em id="S7.SS0.SSS0.Px2.p1.1.2" class="ltx_emph ltx_font_italic">image</em>-level ones (human labels or noisy image tags). As can be seen from Table <a href="#S6.T7" title="Table 7 ‣ Image captioning. ‣ 6 Generalization of Grid Features ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, end-to-end training (denoted as ‘e2e’) can boost accuracy for all three pre-trained models, with the biggest improvements for ImageNet models.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Flexible network design.</h4>

<div id="S7.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px3.p1.1" class="ltx_p">As we now have the ability to train our models end-to-end in a simple manner, it allows us to introduce more flexible architectural designs for vision and language tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Specifically, on top of the grid features from the ResNet-50 model, we add a Pyramid Pooling Module (PPM, a component widely used for semantic segmentation; details in supplementary material) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> to aggregate visual information from grid features of different spatial resolutions. After adding this module to different pre-trained models (Table <a href="#S6.T7" title="Table 7 ‣ Image captioning. ‣ 6 Generalization of Grid Features ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, ‘PPM’), the VQA accuracy can be further improved. Remarkably, for ImageNet and YFCC pre-trained models, a combination of end-to-end training and PPM results in close or even <em id="S7.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">better</em> performance than a VG pre-trained model using pre-computed region features. This result is particularly desirable as it indicates good VQA accuracy can be achieved even with <em id="S7.SS0.SSS0.Px3.p1.1.2" class="ltx_emph ltx_font_italic">zero</em> use of explicit region (bounding box) annotations.</p>
</div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this paper, we revisit grid features as an alternative to the widely used bottom-up region features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> for vision and language tasks. We show they can in fact achieve on-par results in terms of accuracy over different VQA tasks and models and even on captioning.
As a result of skipping the computationally expensive region-related bottlenecks in the pipeline, we see remarkable speed-ups – often more than an order of magnitude – to the existing systems that rely on regions. Our experiments show that rather than the ‘format’ of features (region <em id="S8.p1.1.1" class="ltx_emph ltx_font_italic">vs</em>.<span id="S8.p1.1.2" class="ltx_text"></span> grids), the semantic content that features represent is more critical for their effectiveness. Such effective representation, per our experiment, can be achieved either by pre-training on object and attribute datasets such as VG, or more importantly, by <em id="S8.p1.1.3" class="ltx_emph ltx_font_italic">end-to-end</em> training of grid features directly for the end-task. Note that while easy with grid-features, end-to-end training is not trivial with regions.
Even with limited exploration in this direction, we already find that given a more flexible design space, grid features pre-trained without <em id="S8.p1.1.4" class="ltx_emph ltx_font_italic">any</em> region-level annotations can in fact achieve strong performance on VQA. While we are aware that for tasks like referring expressions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> where the output itself is a region, modeling region is likely unavoidable, we hope our grid features can potentially offer new perspectives for vision and language research in general.</p>
</div>
<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph" style="font-size:80%;">Acknowledgements.</h4>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="S8.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:80%;">We would like to thank Larry Zitnick, Duy-Kien Nguyen, Devi Parikh, and Amanpreet Singh for helpful discussions. ELM acknowledges support from
AFRL and DARPA (#FA8750- 18-2-0126). The U.S. government is authorized
to reproduce and distribute reprints for government purposes notwithstanding any
copyright notation thereon. The views and conclusions contained herein
are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the AFRL and DARPA or the U.S. government.</span></p>
</div>
</section>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of Hyperparameters</h2>

<figure id="A1.T8" class="ltx_table">
<table id="A1.T8.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A1.T8.1.1" class="ltx_tr">
<td id="A1.T8.1.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.1.1" class="ltx_text" style="font-size:80%;">model</span></td>
<td id="A1.T8.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.2.1" class="ltx_text" style="font-size:80%;">dataset</span></td>
<td id="A1.T8.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.3.1" class="ltx_text" style="font-size:80%;">optimizer</span></td>
<td id="A1.T8.1.1.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.4.1" class="ltx_text" style="font-size:80%;"># iterations</span></td>
<td id="A1.T8.1.1.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.1.5.1" class="ltx_text"></span><span id="A1.T8.1.1.5.2" class="ltx_text" style="font-size:80%;"> </span><span id="A1.T8.1.1.5.3" class="ltx_text" style="font-size:80%;">
<span id="A1.T8.1.1.5.3.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T8.1.1.5.3.1.1" class="ltx_tr">
<span id="A1.T8.1.1.5.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 5.0pt;">batch</span></span>
<span id="A1.T8.1.1.5.3.1.2" class="ltx_tr">
<span id="A1.T8.1.1.5.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 5.0pt;">size</span></span>
</span></span><span id="A1.T8.1.1.5.4" class="ltx_text"></span><span id="A1.T8.1.1.5.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="A1.T8.1.1.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.6.1" class="ltx_text" style="font-size:80%;">initial lr</span></td>
<td id="A1.T8.1.1.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.7.1" class="ltx_text" style="font-size:80%;">lr decay</span></td>
<td id="A1.T8.1.1.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.1.8.1" class="ltx_text" style="font-size:80%;">lr schedule</span></td>
<td id="A1.T8.1.1.9" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.1.9.1" class="ltx_text"></span><span id="A1.T8.1.1.9.2" class="ltx_text" style="font-size:80%;"> </span><span id="A1.T8.1.1.9.3" class="ltx_text" style="font-size:80%;">
<span id="A1.T8.1.1.9.3.1" class="ltx_tabular ltx_align_middle">
<span id="A1.T8.1.1.9.3.1.1" class="ltx_tr">
<span id="A1.T8.1.1.9.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 5.0pt;">gradient</span></span>
<span id="A1.T8.1.1.9.3.1.2" class="ltx_tr">
<span id="A1.T8.1.1.9.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 5.0pt;">clip</span></span>
</span></span><span id="A1.T8.1.1.9.4" class="ltx_text"></span><span id="A1.T8.1.1.9.5" class="ltx_text" style="font-size:80%;"></span>
</td>
</tr>
<tr id="A1.T8.1.2" class="ltx_tr">
<td id="A1.T8.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.1.1" class="ltx_text" style="font-size:80%;">Faster R-CNN</span></td>
<td id="A1.T8.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.2.1" class="ltx_text" style="font-size:80%;">VG/COCO</span></td>
<td id="A1.T8.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.3.1" class="ltx_text" style="font-size:80%;">SGD</span></td>
<td id="A1.T8.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.4.1" class="ltx_text" style="font-size:80%;">90K</span></td>
<td id="A1.T8.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.5.1" class="ltx_text" style="font-size:80%;">16</span></td>
<td id="A1.T8.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.6.1" class="ltx_text" style="font-size:80%;">0.02</span></td>
<td id="A1.T8.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.7.1" class="ltx_text" style="font-size:80%;">0.1</span></td>
<td id="A1.T8.1.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.8.1" class="ltx_text" style="font-size:80%;">[60K, 80K]</span></td>
<td id="A1.T8.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.2.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="A1.T8.1.3" class="ltx_tr">
<td id="A1.T8.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.3.1.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A1.T8.1.3.1.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="A1.T8.1.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.3.2.1" class="ltx_text" style="font-size:80%;">VQA 2.0, </span><span id="A1.T8.1.3.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">train</span>
</td>
<td id="A1.T8.1.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.3.3.1" class="ltx_text" style="font-size:80%;">Adamax </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.3.3.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="A1.T8.1.3.3.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="A1.T8.1.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.3.4.1" class="ltx_text" style="font-size:80%;">12K</span></td>
<td id="A1.T8.1.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.3.5.1" class="ltx_text" style="font-size:80%;">512</span></td>
<td id="A1.T8.1.3.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.3.6.1" class="ltx_text" style="font-size:80%;">0.01</span></td>
<td id="A1.T8.1.3.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.3.7.1" class="ltx_text" style="font-size:80%;">0.1</span></td>
<td id="A1.T8.1.3.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.3.8.1" class="ltx_text" style="font-size:80%;">[5K, 7K, 9K, 11K]</span></td>
<td id="A1.T8.1.3.9" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.3.9.1" class="ltx_text" style="font-size:80%;">0.25</span></td>
</tr>
<tr id="A1.T8.1.4" class="ltx_tr">
<td id="A1.T8.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.4.1.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A1.T8.1.4.1.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="A1.T8.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.4.2.1" class="ltx_text" style="font-size:80%;">VQA 2.0, </span><span id="A1.T8.1.4.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">train</span><span id="A1.T8.1.4.2.3" class="ltx_text" style="font-size:80%;"> + </span><span id="A1.T8.1.4.2.4" class="ltx_text ltx_font_typewriter" style="font-size:80%;">vqa-eval</span>
</td>
<td id="A1.T8.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.3.1" class="ltx_text" style="font-size:80%;">Adamax</span></td>
<td id="A1.T8.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.4.1" class="ltx_text" style="font-size:80%;">22K</span></td>
<td id="A1.T8.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.5.1" class="ltx_text" style="font-size:80%;">512</span></td>
<td id="A1.T8.1.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.6.1" class="ltx_text" style="font-size:80%;">0.01</span></td>
<td id="A1.T8.1.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.7.1" class="ltx_text" style="font-size:80%;">0.1</span></td>
<td id="A1.T8.1.4.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.8.1" class="ltx_text" style="font-size:80%;">[15K, 18K, 20K, 21K]</span></td>
<td id="A1.T8.1.4.9" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.4.9.1" class="ltx_text" style="font-size:80%;">0.25</span></td>
</tr>
<tr id="A1.T8.1.5" class="ltx_tr">
<td id="A1.T8.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.5.1.1" class="ltx_text" style="font-size:80%;">MCAN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.5.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="A1.T8.1.5.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="A1.T8.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.5.2.1" class="ltx_text" style="font-size:80%;">VQA 2.0 </span><span id="A1.T8.1.5.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">trainval</span><span id="A1.T8.1.5.2.3" class="ltx_text" style="font-size:80%;"> + VG</span>
</td>
<td id="A1.T8.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.5.3.1" class="ltx_text" style="font-size:80%;">Adam</span></td>
<td id="A1.T8.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.5.4.1" class="ltx_text" style="font-size:80%;">234K</span><span id="footnote6" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">6</span></span></span></span>
</td>
<td id="A1.T8.1.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.5.5.1" class="ltx_text" style="font-size:80%;">64</span></td>
<td id="A1.T8.1.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.5.6.1" class="ltx_text" style="font-size:80%;">5e-5</span></td>
<td id="A1.T8.1.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.5.7.1" class="ltx_text" style="font-size:80%;">0.02</span></td>
<td id="A1.T8.1.5.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.5.8.1" class="ltx_text" style="font-size:80%;">[180K, 216K]</span></td>
<td id="A1.T8.1.5.9" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.5.9.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="A1.T8.1.6" class="ltx_tr">
<td id="A1.T8.1.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.6.1.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A1.T8.1.6.1.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="A1.T8.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.2.1" class="ltx_text" style="font-size:80%;">VizWiz</span></td>
<td id="A1.T8.1.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.3.1" class="ltx_text" style="font-size:80%;">Adamax</span></td>
<td id="A1.T8.1.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.4.1" class="ltx_text" style="font-size:80%;">24K</span></td>
<td id="A1.T8.1.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.5.1" class="ltx_text" style="font-size:80%;">128</span></td>
<td id="A1.T8.1.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.6.1" class="ltx_text" style="font-size:80%;">0.005</span></td>
<td id="A1.T8.1.6.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.7.1" class="ltx_text" style="font-size:80%;">0.01</span></td>
<td id="A1.T8.1.6.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.8.1" class="ltx_text" style="font-size:80%;">[14K]</span></td>
<td id="A1.T8.1.6.9" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.6.9.1" class="ltx_text" style="font-size:80%;">0.25</span></td>
</tr>
<tr id="A1.T8.1.7" class="ltx_tr">
<td id="A1.T8.1.7.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;">
<cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.7.1.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="A1.T8.1.7.1.2.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="footnote7" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">7</span></span></span></span>
</td>
<td id="A1.T8.1.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.2.1" class="ltx_text" style="font-size:80%;">COCO Karpathy split</span></td>
<td id="A1.T8.1.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.3.1" class="ltx_text" style="font-size:80%;">Adamax</span></td>
<td id="A1.T8.1.7.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.4.1" class="ltx_text" style="font-size:80%;">50K</span></td>
<td id="A1.T8.1.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.5.1" class="ltx_text" style="font-size:80%;">256</span></td>
<td id="A1.T8.1.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.6.1" class="ltx_text" style="font-size:80%;">0.002</span></td>
<td id="A1.T8.1.7.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.7.1" class="ltx_text" style="font-size:80%;">0.1</span></td>
<td id="A1.T8.1.7.8" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.8.1" class="ltx_text" style="font-size:80%;">[15K, 25K, 35K, 45K]</span></td>
<td id="A1.T8.1.7.9" class="ltx_td ltx_align_center" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.7.9.1" class="ltx_text" style="font-size:80%;">0.25</span></td>
</tr>
<tr id="A1.T8.1.8" class="ltx_tr">
<td id="A1.T8.1.8.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.8.1.1" class="ltx_text" style="font-size:80%;">e2e </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.T8.1.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A1.T8.1.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="A1.T8.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;">
<span id="A1.T8.1.8.2.1" class="ltx_text" style="font-size:80%;">VQA 2.0, </span><span id="A1.T8.1.8.2.2" class="ltx_text ltx_font_typewriter" style="font-size:80%;">train</span><span id="A1.T8.1.8.2.3" class="ltx_text" style="font-size:80%;"> +</span><span id="A1.T8.1.8.2.4" class="ltx_text ltx_font_typewriter" style="font-size:80%;">vqa-eval</span>
</td>
<td id="A1.T8.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.3.1" class="ltx_text" style="font-size:80%;">Adamax</span></td>
<td id="A1.T8.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.4.1" class="ltx_text" style="font-size:80%;">22K</span></td>
<td id="A1.T8.1.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.5.1" class="ltx_text" style="font-size:80%;">512</span></td>
<td id="A1.T8.1.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.6.1" class="ltx_text" style="font-size:80%;">0.002</span></td>
<td id="A1.T8.1.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.7.1" class="ltx_text" style="font-size:80%;">0.1</span></td>
<td id="A1.T8.1.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.8.1" class="ltx_text" style="font-size:80%;">[15K, 18K, 20K, 21K]</span></td>
<td id="A1.T8.1.8.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 5.0pt;"><span id="A1.T8.1.8.9.1" class="ltx_text" style="font-size:80%;">1</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="A1.T8.8.1" class="ltx_text ltx_font_bold">Summary of hyperparameters</span>. We follow the default setting for most of the models. For the image captioning model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the default initial learning rate is 0.01. We found 0.002 leads to slightly better results. For the end-to-end trained Pythia (e2e Pythia in the last row), we use initial learning rate of 0.002 and a larger value of 1 for the gradient clip when fine-tuning the ResNet model for feature extraction.</figcaption>
</figure><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>In the MCAN paper, the model is trained for 13 epochs, where each epoch contains 17,967 iterations.</span></span></span><span id="footnotex3" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>We use the implementation provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</span></span></span>
<div id="A1.p1" class="ltx_para">
<p id="A1.p1.4" class="ltx_p">Hyper-parameters of different models are summarized in Table <a href="#A1.T8" title="Table 8 ‣ Appendix A Details of Hyperparameters ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. For the SGD optimizer, the momentum is 0.9 and weight decay is 0.0001. For the Adamax optimizer, <math id="A1.p1.1.m1.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="A1.p1.1.m1.1a"><msub id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mi id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">β</mi><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">𝛽</ci><cn type="integer" id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\beta_{1}</annotation></semantics></math> and <math id="A1.p1.2.m2.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="A1.p1.2.m2.1a"><msub id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mi id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml">β</mi><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1">subscript</csymbol><ci id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2">𝛽</ci><cn type="integer" id="A1.p1.2.m2.1.1.3.cmml" xref="A1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">\beta_{2}</annotation></semantics></math> are 0.9 and 0.999, respectively. No weight decay is used. For the Adam optimizer used in MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, <math id="A1.p1.3.m3.1" class="ltx_Math" alttext="\beta_{1}" display="inline"><semantics id="A1.p1.3.m3.1a"><msub id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml"><mi id="A1.p1.3.m3.1.1.2" xref="A1.p1.3.m3.1.1.2.cmml">β</mi><mn id="A1.p1.3.m3.1.1.3" xref="A1.p1.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><apply id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A1.p1.3.m3.1.1.1.cmml" xref="A1.p1.3.m3.1.1">subscript</csymbol><ci id="A1.p1.3.m3.1.1.2.cmml" xref="A1.p1.3.m3.1.1.2">𝛽</ci><cn type="integer" id="A1.p1.3.m3.1.1.3.cmml" xref="A1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">\beta_{1}</annotation></semantics></math> and <math id="A1.p1.4.m4.1" class="ltx_Math" alttext="\beta_{2}" display="inline"><semantics id="A1.p1.4.m4.1a"><msub id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml"><mi id="A1.p1.4.m4.1.1.2" xref="A1.p1.4.m4.1.1.2.cmml">β</mi><mn id="A1.p1.4.m4.1.1.3" xref="A1.p1.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><apply id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="A1.p1.4.m4.1.1.1.cmml" xref="A1.p1.4.m4.1.1">subscript</csymbol><ci id="A1.p1.4.m4.1.1.2.cmml" xref="A1.p1.4.m4.1.1.2">𝛽</ci><cn type="integer" id="A1.p1.4.m4.1.1.3.cmml" xref="A1.p1.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">\beta_{2}</annotation></semantics></math> are 0.9 and 0.98, respectively. No weight decay is used.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">We follow the default setting of hyperparameters for most of models. For the image captioning model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the default initial learning rate is 0.01. We found 0.002 leads to slightly better results. For the end-to-end trained Pythia (e2e Pythia in the last row), we use an initial learning rate of 0.002 and a larger value of 1 for the gradient clip when fine-tuning the ResNet model for feature extraction.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Region Features from FPN</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In the Pythia implementation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, of bottom-up attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, a Feature Pyramid Network (FPN) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> is used to compute region features. This is different from the original Faster R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> used, and it is commonly believed that FPN can offer <em id="A2.p1.1.1" class="ltx_emph ltx_font_italic">better</em> object detection quality. Therefore, to reach a more solid conclusion, in this appendix we show extended results from the main paper to compare our grid features with FPN region features. The FPN model uses an entire ResNet model as the backbone, where the multi-scale feature maps of different blocks of the ResNet model are fused in a feature pyramid. Two randomly initialized fully-connect layers (denoted as <span id="A2.p1.1.2" class="ltx_text ltx_font_typewriter">fc6</span> and <span id="A2.p1.1.3" class="ltx_text ltx_font_typewriter">fc7</span> for simplicity) are added to predict object category, bounding box regression offsets, and attribute labels for each bounding box proposal. We follow the strategy used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to compute region features. Specifically, we use the output of the <span id="A2.p1.1.4" class="ltx_text ltx_font_typewriter">fc6</span> layer as input to a VQA or image captioning model, where the <span id="A2.p1.1.5" class="ltx_text ltx_font_typewriter">fc7</span> layer is also used and <em id="A2.p1.1.6" class="ltx_emph ltx_font_italic">fine-tuned</em> during VQA training.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">Accuracy on the VQA 2.0 <span id="A2.p2.1.1" class="ltx_text ltx_font_typewriter">test-dev</span> set and breakdown inference time of the FPN model, using a ResNet50 as the backbone, are summarized in Table <a href="#A2.T9" title="Table 9 ‣ Appendix B Region Features from FPN ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Different from the trend observed in object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, we find the FPN model, when used to provide region features for VQA, does not show clear advantage over the original C4 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, which in turn gives on-par results to our grid features. Speed-wise, despite the lighter pre-region computation, we find the region-related steps with FPN are still very expensive, and the efficiency advantage of our grid features is even more significant.</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">We also test the top 100 (<math id="A2.p3.1.m1.1" class="ltx_Math" alttext="N{=}100" display="inline"><semantics id="A2.p3.1.m1.1a"><mrow id="A2.p3.1.m1.1.1" xref="A2.p3.1.m1.1.1.cmml"><mi id="A2.p3.1.m1.1.1.2" xref="A2.p3.1.m1.1.1.2.cmml">N</mi><mo id="A2.p3.1.m1.1.1.1" xref="A2.p3.1.m1.1.1.1.cmml">=</mo><mn id="A2.p3.1.m1.1.1.3" xref="A2.p3.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p3.1.m1.1b"><apply id="A2.p3.1.m1.1.1.cmml" xref="A2.p3.1.m1.1.1"><eq id="A2.p3.1.m1.1.1.1.cmml" xref="A2.p3.1.m1.1.1.1"></eq><ci id="A2.p3.1.m1.1.1.2.cmml" xref="A2.p3.1.m1.1.1.2">𝑁</ci><cn type="integer" id="A2.p3.1.m1.1.1.3.cmml" xref="A2.p3.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p3.1.m1.1c">N{=}100</annotation></semantics></math>) regions using different backbones, VQA models, VQA tasks, and image captioning task, as we have done in Section 6 in the paper. Results are reported in Table <a href="#A2.T10.sf1" title="In Table 10 ‣ Appendix B Region Features from FPN ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10a</span></a>, <a href="#A2.T10.sf2" title="In Table 10 ‣ Appendix B Region Features from FPN ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10b</span></a>, <a href="#A2.T10.sf3" title="In Table 10 ‣ Appendix B Region Features from FPN ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10c</span></a>, and <a href="#A2.T10.sf4" title="In Table 10 ‣ Appendix B Region Features from FPN ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10d</span></a>. For the accuracy on the VQA 2.0 <span id="A2.p3.1.1" class="ltx_text ltx_font_typewriter">test-dev</span> set and VizWiz, the FPN model’s accuracy is lower than the results reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, because grid features (from an ImageNet pre-trained ResNet-152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> model) are used in <em id="A2.p3.1.2" class="ltx_emph ltx_font_italic">addition</em> to the region features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Using the MCAN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, the FPN model achieves better results than reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> but still performs worse than C4 and our grid features.</p>
</div>
<figure id="A2.T9" class="ltx_table">
<table id="A2.T9.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A2.T9.1.1" class="ltx_tr">
<td id="A2.T9.1.1.2" class="ltx_td ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="A2.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;" rowspan="2"><span id="A2.T9.1.1.1.1" class="ltx_text" style="font-size:80%;"><span id="A2.T9.1.1.1.1.2" class="ltx_text"></span> <span id="A2.T9.1.1.1.1.1" class="ltx_text">
<span id="A2.T9.1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T9.1.1.1.1.1.1.2" class="ltx_tr">
<span id="A2.T9.1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"># features</span></span>
<span id="A2.T9.1.1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T9.1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">(<math id="A2.T9.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="A2.T9.1.1.1.1.1.1.1.1.m1.1a"><mi id="A2.T9.1.1.1.1.1.1.1.1.m1.1.1" xref="A2.T9.1.1.1.1.1.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.T9.1.1.1.1.1.1.1.1.m1.1b"><ci id="A2.T9.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T9.1.1.1.1.1.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.1.1.1.1.1.1.1.1.m1.1c">N</annotation></semantics></math>)</span></span>
</span></span> <span id="A2.T9.1.1.1.1.3" class="ltx_text"></span></span></td>
<td id="A2.T9.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;" rowspan="2"><span id="A2.T9.1.1.3.1" class="ltx_text" style="font-size:80%;"><span id="A2.T9.1.1.3.1.1" class="ltx_text"></span> <span id="A2.T9.1.1.3.1.2" class="ltx_text">
<span id="A2.T9.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T9.1.1.3.1.2.1.1" class="ltx_tr">
<span id="A2.T9.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.1.3.1.2.1.1.1.1" class="ltx_text ltx_font_typewriter">test-dev</span></span></span>
<span id="A2.T9.1.1.3.1.2.1.2" class="ltx_tr">
<span id="A2.T9.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">accuracy</span></span>
</span></span> <span id="A2.T9.1.1.3.1.3" class="ltx_text"></span></span></td>
<td id="A2.T9.1.1.4" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;" colspan="5"><span id="A2.T9.1.1.4.1" class="ltx_text" style="font-size:80%;">inference time breakdown (ms)</span></td>
</tr>
<tr id="A2.T9.1.2" class="ltx_tr">
<td id="A2.T9.1.2.1" class="ltx_td ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"></td>
<td id="A2.T9.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="A2.T9.1.2.2.1" class="ltx_text"></span><span id="A2.T9.1.2.2.2" class="ltx_text" style="font-size:80%;"> </span><span id="A2.T9.1.2.2.3" class="ltx_text" style="font-size:80%;">
<span id="A2.T9.1.2.2.3.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T9.1.2.2.3.1.1" class="ltx_tr">
<span id="A2.T9.1.2.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">shared</span></span>
<span id="A2.T9.1.2.2.3.1.2" class="ltx_tr">
<span id="A2.T9.1.2.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">conv.</span></span>
</span></span><span id="A2.T9.1.2.2.4" class="ltx_text"></span><span id="A2.T9.1.2.2.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="A2.T9.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="A2.T9.1.2.3.1" class="ltx_text"></span><span id="A2.T9.1.2.3.2" class="ltx_text" style="font-size:80%;"> </span><span id="A2.T9.1.2.3.3" class="ltx_text" style="font-size:80%;">
<span id="A2.T9.1.2.3.3.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T9.1.2.3.3.1.1" class="ltx_tr">
<span id="A2.T9.1.2.3.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">region</span></span>
<span id="A2.T9.1.2.3.3.1.2" class="ltx_tr">
<span id="A2.T9.1.2.3.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">feat. comp.</span></span>
</span></span><span id="A2.T9.1.2.3.4" class="ltx_text"></span><span id="A2.T9.1.2.3.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="A2.T9.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span id="A2.T9.1.2.4.1" class="ltx_text"></span><span id="A2.T9.1.2.4.2" class="ltx_text" style="font-size:80%;"> </span><span id="A2.T9.1.2.4.3" class="ltx_text" style="font-size:80%;">
<span id="A2.T9.1.2.4.3.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T9.1.2.4.3.1.1" class="ltx_tr">
<span id="A2.T9.1.2.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">region</span></span>
<span id="A2.T9.1.2.4.3.1.2" class="ltx_tr">
<span id="A2.T9.1.2.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">selection</span></span>
</span></span><span id="A2.T9.1.2.4.4" class="ltx_text"></span><span id="A2.T9.1.2.4.5" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="A2.T9.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.2.5.1" class="ltx_text" style="font-size:80%;">VQA</span></td>
<td id="A2.T9.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.2.6.1" class="ltx_text" style="font-size:80%;">total</span></td>
</tr>
<tr id="A2.T9.1.3" class="ltx_tr">
<td id="A2.T9.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" rowspan="2"><span id="A2.T9.1.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">R</span></td>
<td id="A2.T9.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.2.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="A2.T9.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.3.1" class="ltx_text" style="font-size:80%;">66.13</span></td>
<td id="A2.T9.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.4.1" class="ltx_text" style="font-size:80%;">9</span></td>
<td id="A2.T9.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.5.1" class="ltx_text" style="font-size:80%;">326</span></td>
<td id="A2.T9.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.6.1" class="ltx_text" style="font-size:80%;">548</span></td>
<td id="A2.T9.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.7.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="A2.T9.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.3.8.1" class="ltx_text" style="font-size:80%;">889</span></td>
</tr>
<tr id="A2.T9.1.4" class="ltx_tr">
<td id="A2.T9.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.1.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="A2.T9.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.2.1" class="ltx_text" style="font-size:80%;">66.22</span></td>
<td id="A2.T9.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.3.1" class="ltx_text" style="font-size:80%;">9</span></td>
<td id="A2.T9.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.4.1" class="ltx_text" style="font-size:80%;">322</span></td>
<td id="A2.T9.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.5.1" class="ltx_text" style="font-size:80%;">544</span></td>
<td id="A2.T9.1.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.6.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="A2.T9.1.4.7" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.4.7.1" class="ltx_text" style="font-size:80%;">882</span></td>
</tr>
<tr id="A2.T9.1.5" class="ltx_tr">
<td id="A2.T9.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;" rowspan="2"><span id="A2.T9.1.5.1.1" class="ltx_text" style="font-size:80%;"><span id="A2.T9.1.5.1.1.1" class="ltx_text"></span> <span id="A2.T9.1.5.1.1.2" class="ltx_text">
<span id="A2.T9.1.5.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T9.1.5.1.1.2.1.1" class="ltx_tr">
<span id="A2.T9.1.5.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.1.1.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">R</span></span></span>
<span id="A2.T9.1.5.1.1.2.1.2" class="ltx_tr">
<span id="A2.T9.1.5.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">w/ FPN</span></span>
</span></span> <span id="A2.T9.1.5.1.1.3" class="ltx_text"></span></span></td>
<td id="A2.T9.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.2.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="A2.T9.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.3.1" class="ltx_text" style="font-size:80%;">66.01</span></td>
<td id="A2.T9.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.4.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="A2.T9.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.5.1" class="ltx_text" style="font-size:80%;">311</span></td>
<td id="A2.T9.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.6.1" class="ltx_text" style="font-size:80%;">690</span></td>
<td id="A2.T9.1.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.7.1" class="ltx_text" style="font-size:80%;">5</span></td>
<td id="A2.T9.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.5.8.1" class="ltx_text" style="font-size:80%;">1017</span></td>
</tr>
<tr id="A2.T9.1.6" class="ltx_tr">
<td id="A2.T9.1.6.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.1.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="A2.T9.1.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.2.1" class="ltx_text" style="font-size:80%;">66.36</span></td>
<td id="A2.T9.1.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.3.1" class="ltx_text" style="font-size:80%;">12</span></td>
<td id="A2.T9.1.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.4.1" class="ltx_text" style="font-size:80%;">323</span></td>
<td id="A2.T9.1.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.5.1" class="ltx_text" style="font-size:80%;">690</span></td>
<td id="A2.T9.1.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.6.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="A2.T9.1.6.7" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.6.7.1" class="ltx_text" style="font-size:80%;">1032</span></td>
</tr>
<tr id="A2.T9.1.7" class="ltx_tr">
<td id="A2.T9.1.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:80%;">G</span></td>
<td id="A2.T9.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.2.1" class="ltx_text" style="font-size:80%;">608</span></td>
<td id="A2.T9.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.3.1" class="ltx_text" style="font-size:80%;">66.27</span></td>
<td id="A2.T9.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.4.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="A2.T9.1.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="A2.T9.1.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="A2.T9.1.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.7.1" class="ltx_text" style="font-size:80%;">7</span></td>
<td id="A2.T9.1.7.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T9.1.7.8.1" class="ltx_text" style="font-size:80%;">18</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>This table extends Table 2 in the main paper for <span id="A2.T9.13.1" class="ltx_text ltx_font_bold">speed and accuracy comparisons</span> with added rows for region features with FPN. Results are reported on VQA 2.0 <span id="A2.T9.14.2" class="ltx_text ltx_font_typewriter">test-dev</span> with accuracy and inference time breakdown measured in milliseconds per image. Despite the advantages which FPN features have that 1) pools features from higher-resolution feature maps; and 2) fine-tunes the <span id="A2.T9.15.3" class="ltx_text ltx_font_typewriter">fc7</span> layer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> when training VQA; our grid features achieve comparable VQA accuracy to all region features and are much faster.</figcaption>
</figure>
<figure id="A2.T10" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.T10.sf1" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="A2.T10.sf1.1" class="ltx_p"><span id="A2.T10.sf1.1.1" class="ltx_text ltx_inline-block" style="width:195.1pt;">
<span id="A2.T10.sf1.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="A2.T10.sf1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf1.1.1.1.1.1" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"></span>
<span id="A2.T10.sf1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_colspan ltx_colspan_4" style="padding-top:0.8pt;padding-bottom:0.8pt;">VQA 2.0 accuracy</span>
<span id="A2.T10.sf1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_rowspan ltx_rowspan_2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf1.1.1.1.1.1.3.1" class="ltx_text"><span id="A2.T10.sf1.1.1.1.1.1.3.1.1" class="ltx_text"></span> <span id="A2.T10.sf1.1.1.1.1.1.3.1.2" class="ltx_text">
<span id="A2.T10.sf1.1.1.1.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf1.1.1.1.1.1.3.1.2.1.1" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">time</span></span>
<span id="A2.T10.sf1.1.1.1.1.1.3.1.2.1.2" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">(ms)</span></span>
</span></span> <span id="A2.T10.sf1.1.1.1.1.1.3.1.3" class="ltx_text"></span></span></span></span>
<span id="A2.T10.sf1.1.1.1.1.2" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.2.1" class="ltx_td ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"></span>
<span id="A2.T10.sf1.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Yes/No</span>
<span id="A2.T10.sf1.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Number</span>
<span id="A2.T10.sf1.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Other</span>
<span id="A2.T10.sf1.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Overall</span></span>
<span id="A2.T10.sf1.1.1.1.1.3" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>
<span id="A2.T10.sf1.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">-</span>
<span id="A2.T10.sf1.1.1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">-</span>
<span id="A2.T10.sf1.1.1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">-</span>
<span id="A2.T10.sf1.1.1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">68.31</span>
<span id="A2.T10.sf1.1.1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">-</span></span>
<span id="A2.T10.sf1.1.1.1.1.4" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf1.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="A2.T10.sf1.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">84.73</span>
<span id="A2.T10.sf1.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">46.88</span>
<span id="A2.T10.sf1.1.1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">58.98</span>
<span id="A2.T10.sf1.1.1.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">68.21</span>
<span id="A2.T10.sf1.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">929</span></span>
<span id="A2.T10.sf1.1.1.1.1.5" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf1.1.1.1.1.5.1.1" class="ltx_text ltx_font_smallcaps">R</span>, w/ FPN</span>
<span id="A2.T10.sf1.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">83.88</span>
<span id="A2.T10.sf1.1.1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">45.13</span>
<span id="A2.T10.sf1.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">58.12</span>
<span id="A2.T10.sf1.1.1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">67.26</span>
<span id="A2.T10.sf1.1.1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">1069</span></span>
<span id="A2.T10.sf1.1.1.1.1.6" class="ltx_tr">
<span id="A2.T10.sf1.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf1.1.1.1.1.6.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="A2.T10.sf1.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">84.13</span>
<span id="A2.T10.sf1.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">45.98</span>
<span id="A2.T10.sf1.1.1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">58.76</span>
<span id="A2.T10.sf1.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">67.76</span>
<span id="A2.T10.sf1.1.1.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">39</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>
</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.T10.sf2" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="A2.T10.sf2.1" class="ltx_p"><span id="A2.T10.sf2.1.1" class="ltx_text ltx_inline-block" style="width:195.1pt;">
<span id="A2.T10.sf2.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="A2.T10.sf2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf2.1.1.1.1.1" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"></span>
<span id="A2.T10.sf2.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_colspan ltx_colspan_4" style="padding-top:0.8pt;padding-bottom:0.8pt;">VQA 2.0 accuracy</span>
<span id="A2.T10.sf2.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_rowspan ltx_rowspan_2" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf2.1.1.1.1.1.3.1" class="ltx_text"><span id="A2.T10.sf2.1.1.1.1.1.3.1.1" class="ltx_text"></span> <span id="A2.T10.sf2.1.1.1.1.1.3.1.2" class="ltx_text">
<span id="A2.T10.sf2.1.1.1.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf2.1.1.1.1.1.3.1.2.1.1" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">time</span></span>
<span id="A2.T10.sf2.1.1.1.1.1.3.1.2.1.2" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">(ms)</span></span>
</span></span> <span id="A2.T10.sf2.1.1.1.1.1.3.1.3" class="ltx_text"></span></span></span></span>
<span id="A2.T10.sf2.1.1.1.1.2" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.2.1" class="ltx_td ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"></span>
<span id="A2.T10.sf2.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Yes/No</span>
<span id="A2.T10.sf2.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Number</span>
<span id="A2.T10.sf2.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Other</span>
<span id="A2.T10.sf2.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">Overall</span></span>
<span id="A2.T10.sf2.1.1.1.1.3" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite></span>
<span id="A2.T10.sf2.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">87.39</span>
<span id="A2.T10.sf2.1.1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">52.78</span>
<span id="A2.T10.sf2.1.1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">60.98</span>
<span id="A2.T10.sf2.1.1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">70.93</span>
<span id="A2.T10.sf2.1.1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">-</span></span>
<span id="A2.T10.sf2.1.1.1.1.4" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf2.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="A2.T10.sf2.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">88.19</span>
<span id="A2.T10.sf2.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">54.38</span>
<span id="A2.T10.sf2.1.1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">62.19</span>
<span id="A2.T10.sf2.1.1.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">72.01</span>
<span id="A2.T10.sf2.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">963</span></span>
<span id="A2.T10.sf2.1.1.1.1.5" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf2.1.1.1.1.5.1.1" class="ltx_text ltx_font_smallcaps">R</span>, w/ FPN</span>
<span id="A2.T10.sf2.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">87.77</span>
<span id="A2.T10.sf2.1.1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">54.72</span>
<span id="A2.T10.sf2.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">62.16</span>
<span id="A2.T10.sf2.1.1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:0.8pt;padding-bottom:0.8pt;">71.87</span>
<span id="A2.T10.sf2.1.1.1.1.5.6" class="ltx_td ltx_align_center" style="padding-top:0.8pt;padding-bottom:0.8pt;">1100</span></span>
<span id="A2.T10.sf2.1.1.1.1.6" class="ltx_tr">
<span id="A2.T10.sf2.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;"><span id="A2.T10.sf2.1.1.1.1.6.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="A2.T10.sf2.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">88.46</span>
<span id="A2.T10.sf2.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">55.68</span>
<span id="A2.T10.sf2.1.1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">62.85</span>
<span id="A2.T10.sf2.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">72.59</span>
<span id="A2.T10.sf2.1.1.1.1.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.8pt;padding-bottom:0.8pt;">72</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>
</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.T10.sf3" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="A2.T10.sf3.1" class="ltx_p"><span id="A2.T10.sf3.1.1" class="ltx_text ltx_inline-block" style="width:195.1pt;">
<span id="A2.T10.sf3.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="A2.T10.sf3.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf3.1.1.1.1.1" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding:0.8pt 3.0pt;"></span>
<span id="A2.T10.sf3.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_colspan ltx_colspan_5" style="padding:0.8pt 3.0pt;">VizWiz accuracy</span>
<span id="A2.T10.sf3.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_rowspan ltx_rowspan_2" style="padding:0.8pt 3.0pt;"><span id="A2.T10.sf3.1.1.1.1.1.3.1" class="ltx_text"><span id="A2.T10.sf3.1.1.1.1.1.3.1.1" class="ltx_text"></span> <span id="A2.T10.sf3.1.1.1.1.1.3.1.2" class="ltx_text">
<span id="A2.T10.sf3.1.1.1.1.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf3.1.1.1.1.1.3.1.2.1.1" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">time</span></span>
<span id="A2.T10.sf3.1.1.1.1.1.3.1.2.1.2" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;">(ms)</span></span>
</span></span> <span id="A2.T10.sf3.1.1.1.1.1.3.1.3" class="ltx_text"></span></span></span></span>
<span id="A2.T10.sf3.1.1.1.1.2" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.2.1" class="ltx_td ltx_border_r" style="padding:0.8pt 3.0pt;"></span>
<span id="A2.T10.sf3.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Yes/No</span>
<span id="A2.T10.sf3.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Number</span>
<span id="A2.T10.sf3.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Other</span>
<span id="A2.T10.sf3.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Un. Ans.</span>
<span id="A2.T10.sf3.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">Overall</span></span>
<span id="A2.T10.sf3.1.1.1.1.3" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>
<span id="A2.T10.sf3.1.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">-</span>
<span id="A2.T10.sf3.1.1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">-</span>
<span id="A2.T10.sf3.1.1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">-</span>
<span id="A2.T10.sf3.1.1.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">-</span>
<span id="A2.T10.sf3.1.1.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">54.22</span>
<span id="A2.T10.sf3.1.1.1.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">-</span></span>
<span id="A2.T10.sf3.1.1.1.1.4" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;"><span id="A2.T10.sf3.1.1.1.1.4.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="A2.T10.sf3.1.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">73.17</span>
<span id="A2.T10.sf3.1.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">28.89</span>
<span id="A2.T10.sf3.1.1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">83.63</span>
<span id="A2.T10.sf3.1.1.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">35.62</span>
<span id="A2.T10.sf3.1.1.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">54.28</span>
<span id="A2.T10.sf3.1.1.1.1.4.7" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">874</span></span>
<span id="A2.T10.sf3.1.1.1.1.5" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;"><span id="A2.T10.sf3.1.1.1.1.5.1.1" class="ltx_text"></span> <span id="A2.T10.sf3.1.1.1.1.5.1.2" class="ltx_text">
<span id="A2.T10.sf3.1.1.1.1.5.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf3.1.1.1.1.5.1.2.1.1" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.5.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 3.0pt;"><span id="A2.T10.sf3.1.1.1.1.5.1.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>, w/ FPN</span></span>
</span></span><span id="A2.T10.sf3.1.1.1.1.5.1.3" class="ltx_text"></span></span>
<span id="A2.T10.sf3.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">73.00</span>
<span id="A2.T10.sf3.1.1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">27.11</span>
<span id="A2.T10.sf3.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">82.02</span>
<span id="A2.T10.sf3.1.1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">33.59</span>
<span id="A2.T10.sf3.1.1.1.1.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 3.0pt;">52.50</span>
<span id="A2.T10.sf3.1.1.1.1.5.7" class="ltx_td ltx_align_center" style="padding:0.8pt 3.0pt;">1051</span></span>
<span id="A2.T10.sf3.1.1.1.1.6" class="ltx_tr">
<span id="A2.T10.sf3.1.1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;"><span id="A2.T10.sf3.1.1.1.1.6.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="A2.T10.sf3.1.1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">75.17</span>
<span id="A2.T10.sf3.1.1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">24.89</span>
<span id="A2.T10.sf3.1.1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">83.68</span>
<span id="A2.T10.sf3.1.1.1.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">35.35</span>
<span id="A2.T10.sf3.1.1.1.1.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 3.0pt;">54.17</span>
<span id="A2.T10.sf3.1.1.1.1.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 3.0pt;">38</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(c) </span>
</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.T10.sf4" class="ltx_table ltx_figure_panel ltx_align_center">
<p id="A2.T10.sf4.1" class="ltx_p"><span id="A2.T10.sf4.1.1" class="ltx_text ltx_inline-block" style="width:216.8pt;">
<span id="A2.T10.sf4.1.1.1" class="ltx_text" style="font-size:80%;">

<span id="A2.T10.sf4.1.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf4.1.1.1.1.1" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.1.1" class="ltx_td ltx_border_r" style="padding:0.8pt 4.0pt;"></span>
<span id="A2.T10.sf4.1.1.1.1.1.2" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">B4</span>
<span id="A2.T10.sf4.1.1.1.1.1.3" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">B3</span>
<span id="A2.T10.sf4.1.1.1.1.1.4" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">B2</span>
<span id="A2.T10.sf4.1.1.1.1.1.5" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">B1</span>
<span id="A2.T10.sf4.1.1.1.1.1.6" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">RL</span>
<span id="A2.T10.sf4.1.1.1.1.1.7" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">M</span>
<span id="A2.T10.sf4.1.1.1.1.1.8" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">C</span>
<span id="A2.T10.sf4.1.1.1.1.1.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 4.0pt;">S</span>
<span id="A2.T10.sf4.1.1.1.1.1.10" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;"><span id="A2.T10.sf4.1.1.1.1.1.10.1" class="ltx_text"></span> <span id="A2.T10.sf4.1.1.1.1.1.10.2" class="ltx_text">
<span id="A2.T10.sf4.1.1.1.1.1.10.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf4.1.1.1.1.1.10.2.1.1" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.1.10.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 4.0pt;">time</span></span>
<span id="A2.T10.sf4.1.1.1.1.1.10.2.1.2" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.1.10.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 4.0pt;">(ms)</span></span>
</span></span><span id="A2.T10.sf4.1.1.1.1.1.10.3" class="ltx_text"></span></span></span>
<span id="A2.T10.sf4.1.1.1.1.2" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 4.0pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite></span>
<span id="A2.T10.sf4.1.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">36.2</span>
<span id="A2.T10.sf4.1.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">-</span>
<span id="A2.T10.sf4.1.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">-</span>
<span id="A2.T10.sf4.1.1.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">77.2</span>
<span id="A2.T10.sf4.1.1.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">56.4</span>
<span id="A2.T10.sf4.1.1.1.1.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">27.0</span>
<span id="A2.T10.sf4.1.1.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">113.5</span>
<span id="A2.T10.sf4.1.1.1.1.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 4.0pt;">20.3</span>
<span id="A2.T10.sf4.1.1.1.1.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">-</span></span>
<span id="A2.T10.sf4.1.1.1.1.3" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 4.0pt;"><span id="A2.T10.sf4.1.1.1.1.3.1.1" class="ltx_text ltx_font_smallcaps">R</span></span>
<span id="A2.T10.sf4.1.1.1.1.3.2" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">36.2</span>
<span id="A2.T10.sf4.1.1.1.1.3.3" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">46.8</span>
<span id="A2.T10.sf4.1.1.1.1.3.4" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">60.4</span>
<span id="A2.T10.sf4.1.1.1.1.3.5" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">76.4</span>
<span id="A2.T10.sf4.1.1.1.1.3.6" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">56.5</span>
<span id="A2.T10.sf4.1.1.1.1.3.7" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">27.7</span>
<span id="A2.T10.sf4.1.1.1.1.3.8" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">113.9</span>
<span id="A2.T10.sf4.1.1.1.1.3.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 4.0pt;">20.8</span>
<span id="A2.T10.sf4.1.1.1.1.3.10" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">1101</span></span>
<span id="A2.T10.sf4.1.1.1.1.4" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 4.0pt;"><span id="A2.T10.sf4.1.1.1.1.4.1.1" class="ltx_text"></span> <span id="A2.T10.sf4.1.1.1.1.4.1.2" class="ltx_text">
<span id="A2.T10.sf4.1.1.1.1.4.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="A2.T10.sf4.1.1.1.1.4.1.2.1.1" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.4.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.8pt 4.0pt;"><span id="A2.T10.sf4.1.1.1.1.4.1.2.1.1.1.1" class="ltx_text ltx_font_smallcaps">R</span>, w/ FPN</span></span>
</span></span><span id="A2.T10.sf4.1.1.1.1.4.1.3" class="ltx_text"></span></span>
<span id="A2.T10.sf4.1.1.1.1.4.2" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">35.7</span>
<span id="A2.T10.sf4.1.1.1.1.4.3" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">46.5</span>
<span id="A2.T10.sf4.1.1.1.1.4.4" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">60.3</span>
<span id="A2.T10.sf4.1.1.1.1.4.5" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">76.6</span>
<span id="A2.T10.sf4.1.1.1.1.4.6" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">56.4</span>
<span id="A2.T10.sf4.1.1.1.1.4.7" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">27.5</span>
<span id="A2.T10.sf4.1.1.1.1.4.8" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">113.1</span>
<span id="A2.T10.sf4.1.1.1.1.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding:0.8pt 4.0pt;">20.6</span>
<span id="A2.T10.sf4.1.1.1.1.4.10" class="ltx_td ltx_align_center" style="padding:0.8pt 4.0pt;">1099</span></span>
<span id="A2.T10.sf4.1.1.1.1.5" class="ltx_tr">
<span id="A2.T10.sf4.1.1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 4.0pt;"><span id="A2.T10.sf4.1.1.1.1.5.1.1" class="ltx_text ltx_font_smallcaps">G</span></span>
<span id="A2.T10.sf4.1.1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">36.4</span>
<span id="A2.T10.sf4.1.1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">47.3</span>
<span id="A2.T10.sf4.1.1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">61.1</span>
<span id="A2.T10.sf4.1.1.1.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">76.7</span>
<span id="A2.T10.sf4.1.1.1.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">56.6</span>
<span id="A2.T10.sf4.1.1.1.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">27.4</span>
<span id="A2.T10.sf4.1.1.1.1.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">113.8</span>
<span id="A2.T10.sf4.1.1.1.1.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:0.8pt 4.0pt;">20.7</span>
<span id="A2.T10.sf4.1.1.1.1.5.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.8pt 4.0pt;">240</span></span>
</span>
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(d) </span>
</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>This table extends Table 6 in the main paper for <span id="A2.T10.6.1" class="ltx_text ltx_font_bold">generalization experiments</span>. From left to right: (a) Different <em id="A2.T10.7.2" class="ltx_emph ltx_font_italic">backbone.</em> We use a ResNeXt-101-32x8d instead of a ResNet-50 as the backbone. (b) Different <em id="A2.T10.8.3" class="ltx_emph ltx_font_italic">VQA model</em>. We use MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> implementation which is the state-of-the-art VQA model. (c) Accuracy on <em id="A2.T10.9.4" class="ltx_emph ltx_font_italic">VizWiz</em> using the same VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. (d) <em id="A2.T10.10.5" class="ltx_emph ltx_font_italic">Image captioning</em> on COCO Karpathy test split. Abbreviations: BLEU4 (B4), BLEU3 (B3), BLEU2 (B2), BLEU1 (B1), ROUGE_L (RL), METEOR (M), CIDEr (C), and SPICE (S). Our grid features generalize well by achieving results at-par with bottom-up region features while being significantly faster. </figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Details of PPM</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.3" class="ltx_p">In Section 7 of the main paper, we introduce end-to-end training of the Pythia VQA model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> with PPM (Pyramid Pooling Module) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.
A detailed illustration of this module is provided in Fig. <a href="#A3.F6" title="Figure 6 ‣ Appendix C Details of PPM ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Given a grid convolution feature map from a ResNet model, adaptive average pooling operations are performed at three different spatial resolutions: 1<math id="A3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A3.p1.1.m1.1a"><mo id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><times id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">\times</annotation></semantics></math>1, 4<math id="A3.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A3.p1.2.m2.1a"><mo id="A3.p1.2.m2.1.1" xref="A3.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.p1.2.m2.1b"><times id="A3.p1.2.m2.1.1.cmml" xref="A3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.m2.1c">\times</annotation></semantics></math>4, and 8<math id="A3.p1.3.m3.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A3.p1.3.m3.1a"><mo id="A3.p1.3.m3.1.1" xref="A3.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.p1.3.m3.1b"><times id="A3.p1.3.m3.1.1.cmml" xref="A3.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.m3.1c">\times</annotation></semantics></math>8. Three separate convolution layers (followed by batch normalization and ReLU) are added, where the kernel sizes are all set to 1 and output dimensions are all 512. Finally, the original grid feature map is concatenated together with the three ones obtained from PPM as the input for VQA.</p>
</div>
<figure id="A3.F6" class="ltx_figure"><img src="/html/2001.03615/assets/x5.png" id="A3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Illustration of <span id="A3.F6.2.1" class="ltx_text ltx_font_bold">PPM (Pyramid Pooling Module)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> experimented in the end-to-end model for VQA. See Section <a href="#A3" title="Appendix C Details of PPM ‣ In Defense of Grid Features for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> for details.</figcaption>
</figure>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Spice: Semantic propositional image caption evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Bottom-up and top-down attention for image captioning and visual
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Vqa: Visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr
Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco captions: Data collection and evaluation server.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1504.00325</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu
Cheng, and Jingjing Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Uniter: Learning universal image-text representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.11740</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Bert: Pre-training of deep bidirectional transformers for language
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NAACL</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Jacob Devlin, Saurabh Gupta, Ross Girshick, Margaret Mitchell, and C Lawrence
Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Exploring nearest neighbor approaches for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1505.04467</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach,
Subhashini Venugopalan, Kate Saenko, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Long-term recurrent convolutional networks for visual recognition and
description.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Multimodal compact bilinear pooling for visual question answering and
visual grounding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.01847</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Vizwiz grand challenge: Answering visual questions from blind people.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Stevan Harnad.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">The symbol grounding problem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Physica D: Nonlinear Phenomena</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 1990.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra, and Devi
Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Pythia v0.1: the winning entry to the vqa challenge 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.09956</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C
Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Clevr: A diagnostic dataset for compositional language and elementary
visual reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Andrej Karpathy and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Deep visual-semantic alignments for generating image descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Referitgame: Referring to objects in photographs of natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Bilinear attention networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Diederik P. Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Alon Lavie and Abhaya Agarwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Meteor: An automatic metric for mt evaluation with high levels of
correlation with human judgments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2007.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Visualbert: A simple and performant baseline for vision and language.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.03557</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Piotr Dollár, Ross B. Girshick, Kaiming He, Bharath
Hariharan, and Serge J. Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Feature pyramid networks for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Focal loss for dense object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander C Berg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Ssd: Single shot multibox detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Vilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Bleu: a method for automatic evaluation of machine translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ACL</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, 2002.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey Pennington, Richard Socher, and Christopher Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Glove: Global vectors for word representation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">EMNLP</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron
Courville.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Film: Visual reasoning with a general conditioning layer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">AAAI</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Robik Shrestha, Kushal Kafle, and Christopher Kanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Answer them all! toward universal visual question answering models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Towards vqa models that can read.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Vl-bert: Pre-training of generic visual-linguistic representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.08530</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Hao Tan and Mohit Bansal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Lxmert: Learning cross-modality encoder representations from
transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.07490</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Damien Teney, Peter Anderson, Xiaodong He, and Anton van den Hengel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Tips and tricks for visual question answering: Learnings from the
2017 challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,
Douglas Poland, Damian Borth, and Li-Jia Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Yfcc100m: The new data in multimedia research.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Communications of the ACM</span><span id="bib.bib40.4.2" class="ltx_text" style="font-size:90%;">, 59(2):64–73, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Alexander Trott, Caiming Xiong, and Richard Socher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Interpretable counting for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Attention is all you need.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Cider: Consensus-based image description evaluation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Show and tell: A neural image caption generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Unified perceptual parsing for scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Saining Xie, Ross B. Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Aggregated residual transformations for deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhudinov, Rich Zemel, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Show, attend and tell: Neural image caption generation with visual
attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Stacked attention networks for image question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh
Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Neural-symbolic VQA: disentangling reasoning from vision and
language understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Deep modular co-attention networks for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Multi-modal factorized bilinear pooling with co-attention learning
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Beyond bilinear: Generalized multimodal factorized high-order pooling
for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">TNNLS</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Yan Zhang, Jonathon S. Hare, and Adam Prügel-Bennett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Learning to count objects in natural images for visual question
answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Pyramid scene parsing network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng
Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Unified vision-language pre-training for image captioning and vqa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.11059</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Deformable convnets v2: More deformable, better results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2001.03613" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2001.03615" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2001.03615">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2001.03615" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2001.03618" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 06:50:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
