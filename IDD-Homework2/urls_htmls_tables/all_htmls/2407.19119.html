<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.19119] Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning</title><meta property="og:description" content="Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.19119">

<!--Generated on Mon Aug  5 15:22:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  Membership Inference Attack,  Accuracy,  Privacy.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Sayyed Farid Ahamed1,
Soumya Banerjee1,
Sandip Roy1,
Devin Quinn2,
Marc Vucovich2,


Kevin Choi2,
Abdul Rahman2,
Alison Hu2,
Edward Bowen2,
Sachin Shetty1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
1Center for Secure &amp; Intelligent Critical Systems, Old Dominion University, Virginia, USA

<br class="ltx_break">{saham001, s1banerj, sroy, sshetty}@odu.edu
</span>
<span class="ltx_contact ltx_role_affiliation">
2Deloitte &amp; Touche LLP

<br class="ltx_break">{devquinn, mvucovich, kevchoi, abdulrahman, aehu, edbowen}@deloitte.com
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In a recent study, Rezaei <span id="id1.id1.1" class="ltx_text ltx_font_italic">et al.</span> revealed the existence of an accuracy-privacy trade-off in deep ensembles and proposed a few fusion strategies to overcome it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, Membership Inference Attack, Accuracy, Privacy.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the past few years, federated learning (FL) has gained significant traction as a widely adopted method in the field of machine learning (ML) with a primary focus on preserving privacy. FL enables multiple clients to work together in building a shared and cohesive model, all the while safeguarding the privacy of their individual training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Although FL is intended to protect individual’s personal data,
recent studies have revealed that FL models can be susceptible to various attacks that expose sensitive information from their training datasets, including source inference, model inversion, and reconstruction attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.19119/assets/Figure_1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.3.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Accuracy-privacy correlation in FL:<span id="S1.F1.4.2.1" class="ltx_text ltx_font_medium"> Training EfficientNet on CIFAR10. Each curve illustrates the evolution of accuracy and privacy over the training period. Notably, the test accuracy consistently rises while privacy decreases, demonstrating independence from the number of FL clients.</span></span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Membership inference attacks (MIAs) are indeed considered a serious threat to FL, as they can compromise the privacy and confidentiality of participant data by revealing whether specific samples were part of the training dataset, potentially undermining the security and trustworthiness of the framework. The accuracy of the MIA is widely accepted as a de-facto measure of an ML model’s privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
In 2020, Nasr <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> showcased that in the realm of FL, adversarial participants can adeptly carry out active MIA against other participants, even in situations where the global model attains a high degree of predictive accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">FL and deep ensemble learning are two complementary approaches, both characterized by a collaborative nature, where multiple models or learners cooperate to enhance predictive performance in ML classifiers. FL addresses the challenges of privacy and decentralization by allowing multiple devices or servers to collaboratively train a global model without sharing raw data, while deep ensemble learning focuses on improving model performance and reliability by combining predictions from diverse, independently trained models. FL prioritizes privacy in a decentralized setting, with clients collaborating on a global model while keeping data locally. In contrast, ensemble learning focuses on improving accuracy by combining independently trained models without privacy concerns. The key difference lies in their primary objectives: privacy preservation for FL and enhanced predictive performance for ensemble learning.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In 2022, S. Rezaei has shown that the effectiveness of MIA increases when ensembling improves accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Their empirical study indicates that this accuracy-privacy trade-off exists even for more advanced and state-of-the-art ensembling techniques. Interestingly, they break this trade-off by changing the fusing mechanism of deep ensembles which improves accuracy and privacy simultaneously. Instead of calculating the average of confidence values, their approach provides the confidence score of the most confident model within the ensemble that predicts the same label.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this paper, <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">we aim to investigate two fundamental research inquiries</span> by exploring the inherent connection between deep ensembles and FL. Firstly, <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">“With respect to MIA, does there exist an accuracy-privacy trade-off in a FL environment?”.</span> Secondly, <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">“Do the various confidence-based metrics adopted to break the trade-off for deep-ensemble learning also hold for FL?”</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In <a href="#S1.F1" title="In I Introduction ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we present a visual representation of the accuracy-privacy dynamics in FL. Specifically, the figure showcases the training of EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> on CIFAR10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, where each curve depicts the evolution of accuracy and privacy with rounds. Notably, the curves illustrate a consistent rise in accuracy coupled with a decrease in privacy, revealing the independence of these dynamics from the number of FL clients.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">The main contributions of this paper, building upon the findings of <a href="#S1.F1" title="In I Introduction ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, are as follows:</p>
</div>
<div id="S1.p8" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We demonstrate that within an FL framework, there exists a strong correlation between model accuracy and privacy, particularly in the context of MIA. This correlation underscores a clear and inherent trade-off between model accuracy and privacy in the FL setting.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We make empirical studies with different numbers of clients for various datasets in a federated setting and establish that the number of clients is not monotonically correlated with accuracy and privacy.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We implement various confidence-metric-based fusion strategies, previously used in deep ensembles, to enhance accuracy and privacy. However, we identify an accuracy-privacy trade-off in the FL environment and provide analytical justification for its presence.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The rest of the paper is organized as follows. In <a href="#S2" title="II Background ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">II</span></a>, we introduce the relevant theoretical background and present a brief literature survey. <a href="#S3" title="III Threat Model ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">III</span></a> defines the threat model, describing the goals, knowledge, and capabilities of the attacker and the defender. <a href="#S4" title="IV Accuracy-Privacy Trade-off in FL ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">IV</span></a> provides an analysis of the accuracy-privacy trade-off in the FL. Experimental results and discussions are provided in <a href="#S5" title="V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">V</span></a>. Finally, we conclude our work and discuss future research scope in <a href="#S6" title="VI Conclusion and future Scope ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we discuss the preliminary concepts and the background works on <span id="S2.p1.1.1" class="ltx_text ltx_font_bold">deep ensembles and FL</span> along with various <span id="S2.p1.1.2" class="ltx_text ltx_font_bold">federated aggregation techniques</span>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Deep Ensembles and Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the deep learning domain, deep ensembles are the predominant and heavily used method, where base models are initialized with random weights and trained on the same dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
This approach consists of two steps: 1) training the base models on the same training dataset with random weights, and 2) fusing their prediction confidence by averaging to create the final output. This differs from classical ML ensembles, as diversity in deep ensembles primarily comes from the random initialization of base learners. Classical ensemble learning approaches, although adaptable, are seldom used in deep learning models due to their lower accuracy compared to deep ensembles <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The consistent thread running through multiple studies establishes the prevalent assumption that FL inherently assumes the role of an implicit ensemble. Shi <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span>’s Fed-ensemble method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, capitalizing on model ensembling within the FL paradigm, stands out for its consistently superior performance compared to various FL algorithms. Similarly, Chen <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span>’s FedBE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> introduces Bayesian model ensemble techniques to FL, illustrating the reliability of ensemble aggregation. Guha <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_italic">et al.</span>’s investigation into one-shot FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> inadvertently reinforces the notion of implicit ensembling by demonstrating substantial improvements in Area Under the Curve (AUC) through ensemble learning and knowledge aggregation. Furthermore, Avdiukhin and Kasiviswanathan’s exploration of asynchronous FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, while primarily focused on performance metrics, incidentally adds to the growing body of evidence suggesting that FL commonly embodies implicit ensemble characteristics. These recurrent findings collectively emphasize the widespread acknowledgment within the research community that FL naturally incorporates implicit ensemble mechanisms, contributing to improved generalization and overall performance.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Aggregation Techniques in Federated Learning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In FL, the training process involves iterations between the central server and clients until a termination criterion is reached. This criterion may be a maximum number of iterations or a threshold for model accuracy. Once the process concludes, the central server converges the FL model (<math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathcal{M}</annotation></semantics></math>), which is then distributed to each of the clients in the system.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Aggregation techniques play a pivotal role in FL, a privacy-preserving ML paradigm. In FL, models are trained across decentralized devices, and aggregating their updates is essential to construct a global model. Common aggregation methods include Federated Averaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, where local model updates are averaged, and Weighted Averaging, which considers the importance of each client’s contribution based on factors such as data size or computation capability. These techniques enable the collaborative learning process while preserving the privacy of individual client data, making aggregation a critical aspect of the FL framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In 2017, McMahan <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> introduced the Federated Averaging (FedAvg) algorithm using Stochastic Gradient Descent (SGD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. FedAvg involves the central server sharing global parameters and a model with a mini-batch of clients, which train the model on local data and share the updates. The global model is then created by averaging the weighted sum of the local updates at the centralized server, and training can be terminated based on a configurable round requirement. Followed by FedAvg, a set of improved averaging strategies were introduced in the literature, including Federated Proximal (FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>), Federated Multi-task Adaptation (FedMA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>), and Averaging with Hierarchical Systems (HierFAVG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>).</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Though each averaging strategy has its own merits, the classical FedAvg algorithm is relatively simple to implement and its reliability lies in its straightforward averaging of model updates, making it an ideal choice for fundamental investigations such as the one at hand.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2407.19119/assets/agrement_v1.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.5.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Distinguishability between training and testing by measuring the agreement among FL clients on correct classifications<span id="S2.F2.2.1.1" class="ltx_text ltx_font_medium">: The relative agreement among the <math id="S2.F2.2.1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.F2.2.1.1.m1.1b"><mn id="S2.F2.2.1.1.m1.1.1" xref="S2.F2.2.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.F2.2.1.1.m1.1c"><cn type="integer" id="S2.F2.2.1.1.m1.1.1.cmml" xref="S2.F2.2.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.2.1.1.m1.1d">10</annotation></semantics></math> FL clients in CIFAR10 using EfficientNet demonstrates a clearly apparent distributional shift between training and testing.</span></span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2407.19119/assets/Correct_Incorrect_v1.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="115" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S2.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Contrasting Confidence Between Correct and Incorrect Predictions<span id="S2.F3.4.2.1" class="ltx_text ltx_font_medium">: The distribution of model predictions confidence on the CIFAR100 dataset highlights that models show high confidence when predictions are correct, whereas the confidence is significantly lower for incorrect predictions.</span></span></figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Accuracy Issues of Membership Inference Attack</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The first introduced MIA on neural networks, known as Shokri’s attack, involves training an attack classifier to predict membership status <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Most MIA assumes that trained models exhibit higher confidence in member samples than non-member samples, using confidence levels as attack features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The attack classifier utilizes the victim model’s prediction confidence as input, assuming greater confidence values for training samples compared to non-training samples. The attacker, with access to a dataset with a similar distribution, trains shadow models to replicate the victim model. Knowing the membership status of the data used to train shadow models, the attacker leverages this information to train the attack classifier.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">With the exception of a few studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, most prior research has focused on using prediction confidence to infer membership status. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, the authors introduced MIA based on confidence levels, distance to the decision boundary, and gradients with respect to model weight and input, assuming white-box access. Interestingly, none of these methods outperformed confidence-based attacks. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, two attacks based on input transformation and distance to the boundary were proposed in a limited information testing (Black-Box) setting. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> introduced a sampling attack that randomly perturbs inputs to create a set of transformations, inferring membership status based on predicted labels. The rationale is that deep models are more robust on training samples, making perturbed samples less likely to be mislabeled. It demonstrated that Differentially Private Stochastic Gradient Descent (DP-SGD) effectively defends against the sampling attack, albeit at the cost of accuracy.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Synthesis:</span>
Existing research consistently highlights a trade-off between privacy and accuracy in ensemble learning, a challenge that resonates in the domain of FL. This study, aligning with this narrative, focuses on measuring the accuracy-privacy trade-off in FL. By employing MIA as a metric, the research aims to offer nuanced insights into the implications of privacy-preserving mechanisms in FL scenarios, addressing a critical gap in the current understanding.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Threat Model</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we outline the fundamental threat model, detailing the defender and attacker perspectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Defender’s Perspective</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Defender’s assumptions:</span>
In the context of FL, the defender utilizes the training dataset, using a disjoint subset of training samples for each client. A sample is classified as a member if it is used in at least one of the clients.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The defender provides API access, returning prediction confidence values, especially in multi-class classification. Base models can be trained from scratch, allowing for the exploration of techniques that enhance privacy, modify the training process, and consider alternative fusing methods beyond the traditional confidence averaging associated with deep ensembles.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Defender’s objectives:</span>
The defender’s primary objective is to mitigate MIA within the FL framework while leveraging the accuracy benefits of ensemble learning. Mitigation of computational expenses is a primary concern throughout the training and inference processes. The investigation aims to achieve increased privacy protection with minimal loss to prediction accuracy within the FL paradigm.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Adversary’s Perspective</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Adversary’s knowledge:</span> We examine a challenging scenario where the adversary is limited to obtaining restricted information about the target model and knowledge concerning the distribution of the target dataset. The training goals and model architecture are universally shared among the FL participants, rendering this level of attacker knowledge realistic. However, the adversary is incapable of acquiring information regarding the global training process (whether centralized or federated) or the distribution of the training data among the clients.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Adversary’s goals:</span> The adversary attempts to draw inferences or deductions of data from the initial training set. Subsequent to training, the attacker formulates an attack model that deduces private data through query-level access to the target model. The attacker refrains from modifying the parameters of the model and requires no additional information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Adversary’s Capability:</span> The assumed attacker is an ostensibly honest yet inquisitive user, granted query access to the target model but without the ability to access its internal weights and gradients. Despite these limitations, the attacker leverages the provided query access to execute an MIA, attempting to discern if specific data points were part of the original training dataset.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Accuracy-Privacy Trade-off in FL</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, the authors present how in the context of ensemble learning there exists a similar tradeoff between accuracy and privacy. Moreover, they demonstrated that this is proportional to the number of ensembles. They demonstrated that ensemble learning increases accuracy at the cost of privacy.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Membership inference attacks succeed in identifying the model reaction to the distributional difference between training and evaluation data. This is reflected in the difference between the evaluation confidence of the train and test samples of the model. <a href="#S2.F2" title="In II-B Aggregation Techniques in Federated Learning ‣ II Background ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> demonstrates how the different client’s independent predictions agree when correctly classifying the training and test samples. The figure highlights what percent of clients agree with the overall (FedAvg) correct classification.
From the figure, one can clearly distinguish the training and test samples from the agreement among the different clients. But thankfully for the defender, the adversary cannot directly exploit this information. However, MIA can leverage the effect of this agreement on the final model’s prediction confidence. <a href="#S2.F3" title="In II-B Aggregation Techniques in Federated Learning ‣ II Background ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>(a) demonstrates the prediction confidence of EfficientNet trained on CIFAR100 dataset. The distributional shift between the training and test samples is clearly observed. <a href="#S2.F3" title="In II-B Aggregation Techniques in Federated Learning ‣ II Background ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>(b) and (c) show the same results, separated by correct and incorrect predictions.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">In the FL setting, despite forming an implicit ensemble, the number of clients is not correlated with the accuracy-privacy tradeoff. This is because, unlike in ensemble learning, the different clients in an FL setting do not have access to the entire dataset. This phenomenon is investigated in the following section.</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.19119/assets/Overall_trends_EfficientNet.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">EfficientNet</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.19119/assets/Overall_trends_ResNet.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">ResNet18</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;"> <span id="S4.F4.4.2.1" class="ltx_text ltx_font_bold">Correlation of Accuracy and Privacy Across Datasets and Model Architecture</span>: Variation of accuracy and privacy with respect to the number of federated clients. While accuracy and privacy are strongly correlated across datasets and model architecture, the lack of correlation with the number of federated clients remains consistent.</span></figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Result Analysis and Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present our findings and discuss their implications. We provide various experimental results on the accuracy-privacy correlation and the effect of the trade-off for different datasets under various fusion strategies in FL.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.4" class="ltx_p"><a href="#S4.F4.sf1" title="In Figure 4 ‣ IV Accuracy-Privacy Trade-off in FL ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a> and <a href="#S4.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ IV Accuracy-Privacy Trade-off in FL ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> demonstrates that accuracy and privacy are strongly correlated in FL settings. We present how the training and latest accuracy (green and blue) are strongly correlated with the attacker’s accuracy with the MI attack (red) for different datasets, such as CIFAR10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, CIFAR100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>,
from the Canadian Institute For Advanced Research,
MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> from the Modified National Institute of Standards and Technology database, and Fashion-MNIST <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> from Zalando Research. The training data was evenly distributed among the clients in a disjoint way. We performed the experiment with EfficientNet and ResNet18<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> models over the FL architecture with <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn type="integer" id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">2</annotation></semantics></math>, <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S5.p2.2.m2.1a"><mn id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><cn type="integer" id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">5</annotation></semantics></math>, and <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.p2.3.m3.1a"><mn id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><cn type="integer" id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">10</annotation></semantics></math> clients. In our experiment, the client <math id="S5.p2.4.m4.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S5.p2.4.m4.1a"><mn id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><cn type="integer" id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1">0</cn></annotation-xml></semantics></math> is analogous to a centralized architecture.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<p id="S5.F5.1" class="ltx_p ltx_align_center"><span id="S5.F5.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<img src="/html/2407.19119/assets/FL_ACC.png" id="S5.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="550" height="265" alt="Refer to caption">
</span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.5.2" class="ltx_text" style="font-size:90%;"> <span id="S5.F5.5.2.1" class="ltx_text ltx_font_bold">Regularizing effect of the number of FL clients on accuracy</span>: with a larger number of clients, the models converge slower but tend to overfit less.</span></figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><a href="#S4.F4.sf1" title="In Figure 4 ‣ IV Accuracy-Privacy Trade-off in FL ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">4(a)</span></a> and <a href="#S4.F4.sf2" title="Figure 4(b) ‣ Figure 4 ‣ IV Accuracy-Privacy Trade-off in FL ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a> further shows that there exists no strong correlation between accuracy (and privacy) and the number of clients. To investigate this phenomenon, we trained a small basic convolutional neural network (CNN) architecture over the CIFAR10 dataset, with up to 50 clients. We repeated each experiment <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S5.p3.1.m1.1a"><mn id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><cn type="integer" id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">10</annotation></semantics></math> times and presented the results in <a href="#S5.F5" title="In V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>. We observed that the basic CNN model started overfitting due to its limited learning capacity. We did not implement early stopping for this experiment. This experiment demonstrated that FL provides a regularization effect on the model being trained.
We see a distinct trend where the model convergence rate is slower when the number of federated clients is larger. However, for a larger number of clients, the model finally converges into a slightly higher accuracy value.
It’s to be noted, that the improved accuracy over federated training was only observed for the basic CNN model, which did not have enough parameters to effectively learn the representation. For models such as EfficientNet, and ResNet18 such an increase is not observable.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, Rezaei <span id="S5.p4.1.1" class="ltx_text ltx_font_italic">et al.</span> proposed three methods to combine the ensemble model outputs that do not also make it easier for the MIA adversary to exploit. The three modes were to return a random base model, return the model that has the highest confidence, and return the model that has the highest agreed confidence. This approach works because the success of an MIA relies on a) confidence of the correct classification of the model, b) confidence of the incorrect classification of the model, and c) the level of agreement among the constituent models. Rezaei <span id="S5.p4.1.2" class="ltx_text ltx_font_italic">et al.</span> observed that in ensemble learning the third point, the level of agreement among the constituent models, becomes very significant. The three approaches proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> aim to disrupt the adversary’s ability to exploit this information.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:214.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/CIFAR_10.png" id="S5.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">CIFAR10 with EfficientNet.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/CIFAR_100.png" id="S5.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">CIFAR100 with EfficientNet.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/MNIST.png" id="S5.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">MNIST with EfficientNet.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/Fashion_MNIST.png" id="S5.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F6.sf4.3.2" class="ltx_text" style="font-size:90%;">Fashion-MNIST with EfficientNet.</span></figcaption>
</figure>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:214.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/CIFAR_10_ResNet.png" id="S5.F6.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S5.F6.sf5.3.2" class="ltx_text" style="font-size:90%;">CIFAR10 with ResNet18.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf6" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/CIFAR_100_ResNet.png" id="S5.F6.sf6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S5.F6.sf6.3.2" class="ltx_text" style="font-size:90%;">CIFAR100 with ResNet18.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf7" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/MNIST_ResNet.png" id="S5.F6.sf7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S5.F6.sf7.3.2" class="ltx_text" style="font-size:90%;">MNIST with ResNet18.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F6.sf8" class="ltx_figure ltx_figure_panel"><img src="/html/2407.19119/assets/Fashion_MNIST_ResNet.png" id="S5.F6.sf8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf8.2.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="S5.F6.sf8.3.2" class="ltx_text" style="font-size:90%;">Fashion-MNIST with ResNet18.</span></figcaption>
</figure>
</div>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Aggregation schemes for breaking the accuracy and privacy correlation proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> are ineffective in FL<span id="S5.F6.4.2.1" class="ltx_text ltx_font_medium">: across different datasets and model architectures the accuracy and privacy correlation persists regardless of the aggregation schemes.</span></span></figcaption>
</figure>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">We investigated the effect of these recommendations in a FL setting. We re-defined the return of a random base model into two schemes, return the <span id="S5.p5.1.1" class="ltx_text ltx_font_bold">first</span> model, and return the model identified by the round number <math id="S5.p5.1.m1.1" class="ltx_Math" alttext="modulo" display="inline"><semantics id="S5.p5.1.m1.1a"><mrow id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mi id="S5.p5.1.m1.1.1.2" xref="S5.p5.1.m1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S5.p5.1.m1.1.1.1" xref="S5.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S5.p5.1.m1.1.1.1a" xref="S5.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.p5.1.m1.1.1.4" xref="S5.p5.1.m1.1.1.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.p5.1.m1.1.1.1b" xref="S5.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.p5.1.m1.1.1.5" xref="S5.p5.1.m1.1.1.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S5.p5.1.m1.1.1.1c" xref="S5.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.p5.1.m1.1.1.6" xref="S5.p5.1.m1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.p5.1.m1.1.1.1d" xref="S5.p5.1.m1.1.1.1.cmml">​</mo><mi id="S5.p5.1.m1.1.1.7" xref="S5.p5.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><times id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1"></times><ci id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2">𝑚</ci><ci id="S5.p5.1.m1.1.1.3.cmml" xref="S5.p5.1.m1.1.1.3">𝑜</ci><ci id="S5.p5.1.m1.1.1.4.cmml" xref="S5.p5.1.m1.1.1.4">𝑑</ci><ci id="S5.p5.1.m1.1.1.5.cmml" xref="S5.p5.1.m1.1.1.5">𝑢</ci><ci id="S5.p5.1.m1.1.1.6.cmml" xref="S5.p5.1.m1.1.1.6">𝑙</ci><ci id="S5.p5.1.m1.1.1.7.cmml" xref="S5.p5.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">modulo</annotation></semantics></math> number of clients, we call this the <span id="S5.p5.1.2" class="ltx_text ltx_font_bold">round robin</span> scheme. We also tested the scheme where the most <span id="S5.p5.1.3" class="ltx_text ltx_font_bold">confident</span> model and the <span id="S5.p5.1.4" class="ltx_text ltx_font_bold">correct confident</span> model are returned.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.3" class="ltx_p">Considering EfficientNet architecture and CIFAR10, CIFAR100, MNIST, and Fashion-MNIST datasets, <a href="#S5.F6.sf1" title="In Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">6(a)</span></a>, <a href="#S5.F6.sf2" title="Figure 6(b) ‣ Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, <a href="#S5.F6.sf3" title="Figure 6(c) ‣ Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a> and <a href="#S5.F6.sf4" title="Figure 6(d) ‣ Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(d)</span></a> demonstrate the effect of the privacy-preserving averaging schemes as compared to FedAvg.
Similarly, <a href="#S5.F6.sf5" title="In Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">6(e)</span></a>, <a href="#S5.F6.sf6" title="Figure 6(f) ‣ Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(f)</span></a>, <a href="#S5.F6.sf7" title="Figure 6(g) ‣ Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(g)</span></a> and <a href="#S5.F6.sf8" title="Figure 6(h) ‣ Figure 6 ‣ V Result Analysis and Discussion ‣ Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(h)</span></a> demonstrate the effect of various privacy-preserving averaging schemes for ResNet18 architecture in FL environments. We can observe that, in terms of accuracy, each of the privacy-preserving averaging schemes performs worse than the FedAvg. This is a direct result of how these schemes are designed, as each of these schemes, at the end of the round, select the model from one single client to be retained. Thus the effective model is trained on approximately a dataset of <math id="S5.p6.1.m1.1" class="ltx_Math" alttext="N/n" display="inline"><semantics id="S5.p6.1.m1.1a"><mrow id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml"><mi id="S5.p6.1.m1.1.1.2" xref="S5.p6.1.m1.1.1.2.cmml">N</mi><mo id="S5.p6.1.m1.1.1.1" xref="S5.p6.1.m1.1.1.1.cmml">/</mo><mi id="S5.p6.1.m1.1.1.3" xref="S5.p6.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><apply id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1"><divide id="S5.p6.1.m1.1.1.1.cmml" xref="S5.p6.1.m1.1.1.1"></divide><ci id="S5.p6.1.m1.1.1.2.cmml" xref="S5.p6.1.m1.1.1.2">𝑁</ci><ci id="S5.p6.1.m1.1.1.3.cmml" xref="S5.p6.1.m1.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">N/n</annotation></semantics></math>, where <math id="S5.p6.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.p6.2.m2.1a"><mi id="S5.p6.2.m2.1.1" xref="S5.p6.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p6.2.m2.1b"><ci id="S5.p6.2.m2.1.1.cmml" xref="S5.p6.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.2.m2.1c">N</annotation></semantics></math> is the size of the original dataset and <math id="S5.p6.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.p6.3.m3.1a"><mi id="S5.p6.3.m3.1.1" xref="S5.p6.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.p6.3.m3.1b"><ci id="S5.p6.3.m3.1.1.cmml" xref="S5.p6.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.3.m3.1c">n</annotation></semantics></math> is the number of clients. This directly disadvantages settings with a large number of FL clients and accuracy gains through ensemble effects are canceled out. However, despite the drawbacks regarding the accuracy, when there is adequate data, and the effective model is trained to a high degree (e.g., MNIST), the proposed averaging schemes are remarkably effective in defending against MIA.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and future Scope</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This study explores the accuracy-privacy trade-off in FL for MIA and investigates the applicability of confidence-based measures from deep ensembles in FL. Using different numbers of clients, datasets, and confidence-metric-based fusion techniques for various deep learning models, a clear accuracy-privacy trade-off was identified and analytically justified. However, empirical studies reveal a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. The research identifies FL client data division as the primary contributor to accuracy loss. In the future, we aim to explore innovative approaches to enhance FL system privacy without compromising prediction accuracy.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Rezaei, Z. Shafiq, and X. Liu, “Accuracy-privacy trade-off in deep
ensemble: A membership inference perspective,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">2023 IEEE Symposium
on Security and Privacy (SP)</em>.   IEEE,
2023, pp. 364–381.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.   PMLR, 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
S. Banerjee, S. Roy, S. F. Ahamed, D. Quinn, M. Vucovich, D. Nandakumar,
K. Choi, A. Rahman, E. Bowen, and S. Shetty, “Mia-bad: An approach for
enhancing membership inference attack and its mitigation with federated
learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.00051</em>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Nasr, R. Shokri, and A. Houmansadr, “Comprehensive privacy analysis of deep
learning: Passive and active white-box inference attacks against centralized
and federated learning,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2019 IEEE symposium on security and
privacy (SP)</em>.   IEEE, 2019, pp.
739–753.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for convolutional
neural networks,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International conference on machine
learning</em>.   PMLR, 2019, pp. 6105–6114.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Krizhevsky, “Learning multiple layers of features from tiny images,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Technical report</em>, 2009.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. Lakshminarayanan, A. Pritzel, and C. Blundell, “Simple and scalable
predictive uncertainty estimation using deep ensembles,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in
neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
N. Shi, F. Lai, R. A. Kontar, and M. Chowdhury, “Fed-ensemble: Improving
generalization through model ensembling in federated learning,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2107.10663</em>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H.-Y. Chen and W.-L. Chao, “Fedbe: Making bayesian model ensemble applicable
to federated learning,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.01974</em>, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
N. Guha, A. Talwalkar, and V. Smith, “One-shot federated learning,”
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.11175</em>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
D. Avdiukhin and S. Kasiviswanathan, “Federated learning under arbitrary
communication patterns,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>.   PMLR, 2021, pp. 425–435.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha, and
G. Srivastava, “A survey on security and privacy of federated learning,”
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Future Generation Computer Systems</em>, vol. 115, pp. 619–640, 2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of
Machine learning and systems</em>, vol. 2, pp. 429–450, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni, “Federated
learning with matched averaging,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2002.06440</em>,
2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. Liu, J. Zhang, S. Song, and K. B. Letaief, “Client-edge-cloud hierarchical
federated learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICC 2020-2020 IEEE International Conference on
Communications (ICC)</em>.   IEEE, 2020, pp.
1–6.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
attacks against machine learning models,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2017 IEEE symposium on
security and privacy (SP)</em>.   IEEE,
2017, pp. 3–18.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. Rezaei and X. Liu, “On the difficulty of membership inference attacks,” in
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition</em>, 2021, pp. 7892–7900.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot, “Label-only
membership inference attacks,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International conference on machine
learning</em>.   PMLR, 2021, pp. 1964–1974.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Rahimian, T. Orekondy, and M. Fritz, “Sampling attacks: Amplification of
membership inference attacks by repeated queries,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2009.00395</em>, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Salem, Y. Zhang, M. Humbert, P. Berrang, M. Fritz, and M. Backes,
“Ml-leaks: Model and data independent membership inference attacks and
defenses on machine learning models,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1806.01246</em>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. LeCun, “The mnist database of handwritten digits,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">http://yann.
lecun. com/exdb/mnist/</em>, 1998.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1708.07747</em>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.19118" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.19119" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.19119">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.19119" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.19120" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 15:22:19 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
