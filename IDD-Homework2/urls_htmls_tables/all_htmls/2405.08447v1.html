<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AI-Resilient Interfaces [Working Draft]</title>
<!--Generated on Wed May 15 14:50:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="AI safety,  human-AI interaction,  interface design" lang="en" name="keywords"/>
<base href="/html/2405.08447v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S1" title="In AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S1.SS0.SSS0.Px1" title="In 1. Introduction ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Noticing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S1.SS0.SSS0.Px2" title="In 1. Introduction ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Judging (well)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2" title="In AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Motivating Examples</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS1" title="In 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>AI-assisted Search</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS1.SSS1" title="In 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Twins as a function of maternal age</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS1.SSS2" title="In 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Folate supplementation during pregnancy</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS1.SSS2.Px1" title="In 2.1.2. Folate supplementation during pregnancy ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Summary</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS2" title="In 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>AI-assisted Document Corpus Exploration</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS2.SSS0.Px1" title="In 2.2. AI-assisted Document Corpus Exploration ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Summary</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3" title="In AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Design Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS1" title="In 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>There often is no “best” AI choice.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS2" title="In 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Unnoticed (AI) choices</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS2.SSS0.Px1" title="In 3.2. Unnoticed (AI) choices ‣ 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">1. Invisible Choices</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS2.SSS0.Px2" title="In 3.2. Unnoticed (AI) choices ‣ 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Choices that are visible, but hard to notice</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS2.SSS0.Px3" title="In 3.2. Unnoticed (AI) choices ‣ 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Choices that are visible and noticed, but hard to understand</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS2.SSS0.Px4" title="In 3.2. Unnoticed (AI) choices ‣ 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Choices that are not recognized as a choice</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS2.SSS0.Px5" title="In 3.2. Unnoticed (AI) choices ‣ 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title">Choices that are visible, noticed, and understandable, but users choose not to make the effort to consider</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S3.SS3" title="In 3. Design Challenges ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Insufficient context to judge (AI) choices</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S4" title="In AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>AI-Resilient Interfaces</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S4.SS1" title="In 4. AI-Resilient Interfaces ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>A Concrete Example: An AI-Resilient Interface Alternative to AI Summarization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S4.SS1.SSS1" title="In 4.1. A Concrete Example: An AI-Resilient Interface Alternative to AI Summarization ‣ 4. AI-Resilient Interfaces ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Specific AI-Resilience Needs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S4.SS1.SSS2" title="In 4.1. A Concrete Example: An AI-Resilient Interface Alternative to AI Summarization ‣ 4. AI-Resilient Interfaces ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>AI-Resilient Approach</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S5" title="In AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Relationship with Trust and Design Principles</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S5.SS1" title="In 5. Relationship with Trust and Design Principles ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Trust</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S5.SS2" title="In 5. Relationship with Trust and Design Principles ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Existing Design Principles</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6" title="In AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>AI Resilience Audits of Current Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS1" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Automated Document Summarization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS2" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Recommender Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS3" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Unsupervised Pattern Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS4" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>AI-Assisted Writing Interfaces</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS5" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS6" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Code Generation for Programmers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S6.SS7" title="In 6. AI Resilience Audits of Current Approaches ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.7 </span>Discussion</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">AI-Resilient Interfaces [Working Draft]</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elena L. Glassman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:glassman@seas.harvard.edu">glassman@seas.harvard.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/https://orcid.org/0000-0001-5178-3496" title="ORCID identifier">https://orcid.org/0000-0001-5178-3496</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Harvard University</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">John A. Paulson School of Engineering &amp; Applied Sciences</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Boston</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">Massachusetts</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id6.6.id6">02134</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziwei Gu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:ziweigu@g.harvard.edu">ziweigu@g.harvard.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/https://orcid.org/0000-0001-9044-2651" title="ORCID identifier">https://orcid.org/0000-0001-9044-2651</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Harvard University</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">John A. Paulson School of Engineering &amp; Applied Sciences</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Boston</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">Massachusetts</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">USA</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">02134</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonathan K. Kummerfeld
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:jonathan.kummerfeld@sydney.edu.au">jonathan.kummerfeld@sydney.edu.au</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/https://orcid.org/0000-0001-5030-3016" title="ORCID identifier">https://orcid.org/0000-0001-5030-3016</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">The University of Sydney</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2">School of Computer Science Building (J12)</span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3">Darlington</span><span class="ltx_text ltx_affiliation_state" id="id16.4.id4">New South Wales</span><span class="ltx_text ltx_affiliation_country" id="id17.5.id5">Australia</span><span class="ltx_text ltx_affiliation_postcode" id="id18.6.id6">2006</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id19.id1">AI is powerful, but it can make choices that result in objective errors, contextually inappropriate outputs, and disliked options.
We need <em class="ltx_emph ltx_font_italic" id="id19.id1.1">AI-resilient interfaces</em> that help people be resilient to the AI choices that are not right, or not right for them. To support this goal, interfaces need to help users <span class="ltx_text ltx_font_italic" id="id19.id1.2">notice</span> and <span class="ltx_text ltx_font_italic" id="id19.id1.3">have the context to appropriately judge those AI choices</span>.
Existing human-AI interaction guidelines recommend efficient user dismissal, modification, or otherwise efficient recovery from AI choices that a user does not like. However, in order to recover from AI choices, the user must notice them first. This can be difficult. For example, when generating summaries of long documents, a system’s exclusion of a detail that is critically important to the user is hard for the user to notice. That detail can be hiding in a wall of text in the original document, and the existence of a summary may tempt the user not to read the original document as carefully.
Once noticed, judging AI choices well can also be challenging.
The interface may provide very little information that contextualizes the choices,
and the user may fall back on assumptions when deciding whether to dismiss, modify, or otherwise recover from an AI choice.
Building on prior work, this paper defines key aspects of AI-resilient interfaces, illustrated with examples. Designing interfaces for increased AI-resilience of users will improve AI safety, usability, and utility. This is especially critical where AI-powered systems are used for context- and preference-dominated open-ended AI-assisted tasks, like ideating, summarizing, searching, sensemaking, and the reading and writing of text or code.</p>
</div>
<div class="ltx_keywords">AI safety, human-AI interaction, interface design
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing HCI theory, concepts and models</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interactive systems and tools</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interaction design theory, concepts and paradigms</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Visualization theory, concepts and paradigms</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">AI and other forms of automation are powerful, but their computational <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">choices</em> can result in objective errors as well as contextually inappropriate or subjectively insufficient outputs.
For example, legal technology companies are piloting AI-written summaries for judges in lieu of those written by paralegals. A summary is a lossy representation that is useful if it appropriately frames any additional details in the original case document, i.e., preserving factors that are contextually or personally relevant to the reader, given the reader’s particular knowledge, context, preferences, values, and task. In the case of a judge, the details that the judge believes are crucial to contextualizing the offense might be left out.
The AI may “choose” not to include such relevant factors (omission), introduce statistically correlated—and therefore plausible and harder to trigger suspicion—info (hallucination), and
subtly or significantly change the semantic meaning through rewording or leaving some contextualizing information out (misrepresentation) <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib18" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text ltx_font_bold" id="S1.p2.1.1">We need <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1.1">AI-resilient interfaces</em> that help people be resilient to the AI choices that are not right, or not right for them.</span> To support this goal, interfaces need to help users <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">notice</span> and <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">have the context to appropriately judge those AI choices</span>. This will improve AI safety, usability, and utility.
In the example of automated summarization of long documents or document corpora, the reader has to read and/or remember both the summary and the source document(s) to notice and accurately judge whether the AI summary includes any omissions, hallucinations, and misrepresentations relevant to their task.
Catching these issues is onerous, and due to memory constraints, error-prone; it introduces extra work for the user if they want to be resilient to them. It therefore does not meet our design goals for AI-resilient interfaces. One could introduce additional AI to attempt to identify and call out these omissions, hallucinations, and misrepresentations, but that does not eliminate the need for the reader check the additional AI feature’s choices; it may help the user catch more issues between the original AI-generated summary and its summarized document(s), but it can still introduce a new class of objective errors and contextually inappropriate or subjectively insufficient choices.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Noticing</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">Existing human-AI interaction guidelines, e.g., <cite class="ltx_cite ltx_citemacro_citet">Amershi et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib3" title="">2019</a>)</cite>, often recommend including affordances for efficient user dismissal, modification, or otherwise efficient recovery from AI choices that a user does not like. However, in order to recover from AI choices, the user must first notice them, which is not always trivial, like the challenge of noticing a crucial detail that an AI left out. There are many ways, e.g., inattentional blindness <cite class="ltx_cite ltx_citemacro_citep">(Cherry, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib8" title="">2023</a>)</cite>, in which users may fail to notice consequential AI choices when examining system outputs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Judging (well)</h5>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p1.1">Once noticed, judging AI choices well is also not trivial. Humans do not have computers’ tireless capability to consume all the relevant data in its original form that <span class="ltx_text ltx_font_italic" id="S1.SS0.SSS0.Px2.p1.1.1">would</span> provide more of the necessary context to judge an AI choice. The interface may provide very little contextual information
and the user may fall—consciously or unconsciously, confidently or hesitantly—back on assumptions to guide them in their judgment. If the interface includes an AI system’s explanations of its own behavior, there can still be insufficient context provided for the human to recognize when to overrule it, if the user is sufficiently engaged with the explanation at all.
If the interface includes AI estimates of its own uncertainty, those estimates can be poorly calibrated and do not protect users when the AI is confidently wrong.
Likewise, the AI can also not be relied upon to make its own judgements about what AI choices are or are not part of the set that the user would need to see and understand to be resilient to the AI’s choices, because again the AI can be confidently wrong.
The user’s assessment of the situation, however well-informed or flawed, is what drives how the user wields the recommended affordances to dismiss, modify, or otherwise recover from an AI choice.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p2.1">In other words, the gulf of evaluation <cite class="ltx_cite ltx_citemacro_citep">(Norman and Draper, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib22" title="">1986</a>)</cite> when using AI-powered features is often very large and impractical or nearly impossible to traverse. Ideally, AI-resilient systems should narrow or nearly eliminate this gulf with the specific goal that, when the AI is making choices that are wrong (or “just” wrong for that particular user), the user is able to notice and accurately judge the quality of those choices without needing to significantly go out of their way to do so. (If an interface allows a user to recognize more wrong or suboptimal AI choices than existing interfaces, but it still requires significant additional user effort, one could say that the interface is allowing the user to be more AI-resilient, which is obviously good, while not fulfilling the design ideal.)</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p3.1">AI-resilient interfaces are especially important for the utility and usability of AI in context- and preference-dominated and/or open-ended AI-assisted tasks, like ideating, summarizing, searching, sensemaking, and the reading and writing of text or code. For example, when writing, there is no objectively correct next sentence, only a large design space of possible thoughts and their concrete instantiations. Only the writer knows their current, evolving beliefs, what communicative goals they have, and what literal text they now want.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p4">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p4.1">In this paper, using examples, we argue that many critical current interfaces poorly support this process of noticing and judging.
And designing specifically for these goals is not trivial.
Given the challenges of developing AI-resilient interfaces as defined above, it may seem like a fools’ errand to spend so much time defining this characteristic that perhaps no interface could ever instantiate. However, we have one example that we believe fully satisfies this specification: Grammar-Preserving Text Saliency Modulation <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib18" title="">2024</a>)</cite>, an alternative to AI-generated document summaries that allows the user to read the original document more quickly, with just as much comprehension, with AI suggestions for where to focus reified in the (always legible) word-by-word saliency.
Given this existence proof, this paper picks up where that previous paper left off: “generalizing this notion of AI-resiliency to additional tasks and domains.”</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Motivating Examples</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The critical importance of noticing and judging is illustrated in the following three examples of how <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">not</span> designing for noticing or judging can create usability, utility, and safety issues for users, even in mundane, pervasive interface types. The first two examples take place in the context of AI-assisted question answering using a search engine. The third example describes the corresponding noticing and judging challenges users face when using AI-assisted document clustering for analysis.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>AI-assisted Search</h3>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1. </span>Twins as a function of maternal age</h4>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="485" id="S2.F1.g1" src="extracted/2405.08447v1/figures/fraternal_twins_probability_with_maternal_age_genAI_long.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.3.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S2.F1.4.2" style="font-size:90%;">Generative AI’s output in response to the query <span class="ltx_text ltx_font_typewriter" id="S2.F1.4.2.1">fraternal twins probability with maternal age</span>.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Recently, the first author tried to look up the probability of having twins as a function of age, as they vaguely recalled that older mothers were more likely to have multiples. They searched Google for <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.1">fraternal twins probability with maternal age</span>.
The first answer on the search results page was attributed to Google’s experimental generative AI, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F1" title="Figure 1 ‣ 2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">1</span></a>. The AI-generated text named a source’s domain name (<span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.2">BabyCenter.com</span>), generated some introductory text restating some of the question being answered, i.e., <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.3">the chances of having fraternal twins are</span>, and then listed percentages as a function of two-year age ranges.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Interestingly, both the original query, <span class="ltx_text ltx_font_typewriter" id="footnote1.1">fraternal twins probability with maternal age</span>, and the introductory generated text, <span class="ltx_text ltx_font_typewriter" id="footnote1.2">the chances of having fraternal twins are</span>, make no mention of maternal age, but the answer only includes the relationship between twins and age for those of ”advanced maternal age”—35 or older. There are multiple possible reasons for this omission which are difficult or impossible to generate and distinguish between as a user of this black-box system; for example, perhaps the system leveraged its information about the searcher who was signed into their profile at the time and is already of ”advanced maternal age” or perhaps this simply propagated the omissions of the cited source material.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1">The generative AI answer continued, moving on to another source which presumably independently confirmed that <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p2.1.1">women aged 30 or older are more likely to conceive twins</span>, complete with a reasonable sounding reason, followed by a list of many additional factors associated with having twins. While the searcher found the specific rates of 6.9%, 6.8%, 5.1%, and 5.9% for women in various age brackets (all over 35) surprisingly high, the response appeared well cited, consistent, and comprehensive, and confirmed the searcher’s suspicions.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p3">
<p class="ltx_p" id="S2.SS1.SSS1.p3.1">Further down the page, another intelligent feature called “People Also Ask” listed a semantically consistent rephrasing of the searcher’s query as a grammatically correct sentence: <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p3.1.1">How common are twins by maternal age?</span> with an answer very similar to the generative AI’s answer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F2" title="Figure 2 ‣ 2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">2</span></a>). The automatically generated answer was much shorter: just a list of percentages as a function of maternal age, with a date (presumably the date of retrieval) followed by the specific source page’s title and domain name. The percentages were in the same range, with slightly different syntax and formatting. Given these superficial differences, a searcher may or may not notice that the numbers were actually exactly the same as those in the generative AI’s answer, as was the domain name of the source. But this at a glance illusion of independent confirmation is not the primary problem we have with this interface, as revealed by what happened next:</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S2.F2.g1" src="extracted/2405.08447v1/figures/peoplealsoask.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Another automated answer, which is a selected quote from the referenced page</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p4">
<p class="ltx_p" id="S2.SS1.SSS1.p4.1">The searcher then chose to click on the specific page cited for these numbers in the second generated answer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F2" title="Figure 2 ‣ 2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">2</span></a>). The searcher’s browser opened the page in a new window, which then automatically scrolled to and highlighted the text related to the search query (Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F3" title="Figure 3 ‣ 2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">3</span></a>, purple highlighting). This source page reveals several key facts:</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p5">
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">The answer in the “People Also Ask” feature is a contiguous excerpt from the linked page. (This, in and of itself, is not a problem.)</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">This extracted answer omits the data point prior to the extracted data points for women younger than 35, which is even higher than the twin rate for women in the age brackets over 35 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F3" title="Figure 3 ‣ 2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">3</span></a>, blue box content). This omission allows the searcher to <span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.1">fill in the missing younger women’s twin rate with their own expectations</span>, which are likely lower than these high quoted twin rates, since most people do not meet twins as frequently as these quoted rates would imply. <span class="ltx_text ltx_font_italic" id="S2.I1.i2.p1.1.2">The searcher may not even notice that they are doing this.</span></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">All this data actually only describes the twin rate for those using assisted reproductive technology (ART), where the practice of transferring multiple fertilized embryos in a single cycle is not uncommon (Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F3" title="Figure 3 ‣ 2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">3</span></a>, pink box content). This twinning rate has nothing to do with natural twinning rates that do in fact rise with maternal age and everything to do with irrelevant factors, i.e., common ART practices, and as evidenced by the omitted data point for younger women, does not increase with age.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p6">
<p class="ltx_p" id="S2.SS1.SSS1.p6.1">In other words, while it might still be true that unassisted older women have twins more often than younger women due to aging processes, the twin rate is not necessarily at the high rates observed in ART clinics that the AI quoted.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="423" id="S2.F3.g1" src="extracted/2405.08447v1/figures/Chances_of_having_twins__What_are_the_odds_of_having_twins____BabyCenter.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">The actual referenced page <cite class="ltx_cite ltx_citemacro_citep">(bab, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib2" title="">2021</a>)</cite>. The blue and pink boxes highlight two distinct, critical pieces of information present on the original page that were automatically omitted from the extracted or generated answers on the original Google search page, misleading the searcher.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2. </span>Folate supplementation during pregnancy</h4>
<figure class="ltx_figure" id="S2.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1296" id="S2.F4.sf1.g1" src="extracted/2405.08447v1/figures/folate_answers.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf1.3.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F4.sf1.4.2" style="font-size:90%;">Google’s extracted quotes as answers to the query <span class="ltx_text ltx_font_typewriter" id="S2.F4.sf1.4.2.1">folate daily for pregnancy</span>. The extracted answers from two different sources differ by 10x.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1296" id="S2.F4.sf2.g1" src="extracted/2405.08447v1/figures/folate_page1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F4.sf2.3.2" style="font-size:90%;">The first extracted quote in context. Is 400 mcg still the recommended dose? We only know from the local context of this quote that it was in 1992.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1296" id="S2.F4.sf3.g1" src="extracted/2405.08447v1/figures/folate_page2.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F4.sf3.3.2" style="font-size:90%;">The second extracted quote in context. Turns out this recommendation is only for pregnancies at higher risk for neural tube defects.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F4.3.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S2.F4.4.2" style="font-size:90%;">An example of answers to the query <span class="ltx_text ltx_font_typewriter" id="S2.F4.4.2.1">folate daily for pregnancy</span> as AI-extracted quotes initially provided with only the page title and organization name as context. The significant discrepancy in answers may be sufficient information scent for the user to take the necessary follow-up actions to be resilient to any of these AI choices that are not right for them.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.F4" title="Figure 4 ‣ 2.1.2. Folate supplementation during pregnancy ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">4</span></a>, the 10-fold magnitude difference in quoted folate dosage between the two AI-extracted answers from two different sources was large enough to be noticeable and considered significant enough by the first author to warrant gathering additional information. She opened up each resource, shown in (b) and (c) respectively, to better understand the discrepancy, and discovered that each answer was implicitly answering a slightly different question than she had originally intended to ask. That said, the diversity of answer revealed to her that there were questions she had not thought of ask—unknown unknowns—i.e., that some people are designated higher risk of gestating a fetus with a neural tube defect, and the recommended folate intake is different.</p>
</div>
<section class="ltx_paragraph" id="S2.SS1.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Summary</h5>
<div class="ltx_para" id="S2.SS1.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS2.Px1.p1.1">Both the generative AI-produced and automatically extracted answers left out context that the system incorrectly judged to be insufficiently relevant. As a result, the system presented correct information without the context necessary to interpret it correctly, and did so with no information scent about the complexity or context abstracted away (appropriately or inappropriately) to generate that answer. There was nothing in the page of search results and generated answers for the searcher to <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.Px1.p1.1.1">notice</span>—let alone judge well the appropriateness of excluding—the missing context, and, when the extracted answers reinforced rather than contradicted each other, there was no information scent that signaled that the searcher might want to investigate further on the original source page, aside from the existence of a link to it.
In these examples, to both (1) be correctly informed rather than misled and (2) have the context necessary to judge the AI output well, the searcher needed to have the curiousity to follow a source link and then notice that omitted text fundamentally changed how that quoted data should be interpreted.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.Px1.p2">
<p class="ltx_p" id="S2.SS1.SSS2.Px1.p2.1">To complicate matters, given inattentional blindness <cite class="ltx_cite ltx_citemacro_citep">(Cherry, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib8" title="">2023</a>)</cite>, a user being in a position to theoretically notice does not guarantee that they will indeed notice, and asking the system to draw users’ attention to its own mistakes puts users <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.Px1.p2.1.1">again</span> at the mercy of its mistakes. (The system can be confidently wrong about both its chosen output and any “meta cognition” about its output.) In this case, the automated highlighting of the specific source material within the source page at least helped the searcher in this scenario to notice the relevant omitted context nearby. There may have been other relevant context on the rest of the page which the searcher never noticed; exhaustively noting all relevant omitted context would require reading the entire original page.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>AI-assisted Document Corpus Exploration</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In a range of settings, people wish to understand large collections of documents, e.g., finding trends on social media, understanding patterns in news, and analyzing survey responses.
Understanding a corpus involves identifying patterns, relationships, and groupings across the documents (where ‘documents’ could be anything from single words to entire books).
As well as the patterns of commonality, there is also value in identifying group boundaries and distinctions, as well as heads and long contrasting tails of power-law distributions, and outliers.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">An interactive scatter plot is a standard method in this exploratory analysis.
Each point in the plot represents a document, and the spatial relationships between dots is determined by mathematical similarity functions applied to the documents’ representation.
Color is often used to encode document meta data or computed clusters.
Hovering over a point shows the content of the document or other information.
Sometimes denser groupings are labeled automatically with a short descriptive label that captures something that is more prevalent among the dots in the neighborhood than elsewhere.
More sophisticated interaction methods have also been explored, such as a lasso tool that allows users to select and view information about a selected subset of dots <cite class="ltx_cite ltx_citemacro_citep">(Raval et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib23" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Users may hope that these plots can both confirm expected relationships and reveal unexpected relationships.
In both cases, this hope is based on the ability to see a visual encoding of the entire document collection at once with semantic relationships represented spatially.
This is particularly beneficial when the document collection is inconveniently or even prohibitively large for users to read through in its entirety.
Producing this standard plot requires multiple computational processes, which may or may not capture or preserve relationships that the user currently cares about—or would care about if they knew about them.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>First, a model produces a high-dimensional vector representation of each document, which may or may not capture the aspects of that document that the user currently cares or would care about.
Optionally, second, an unsupervised clustering method identifies groups of vectors that are similar to each other, using a definition of similarity that may or may not capture relationships between documents that the user currently cares or would care about.
And third, a dimensionality reduction algorithm is used to convert the high-dimensional vectors into a 2D space, which may or may not preserve, in the final 2D spatial mapping, the relationships currently cares or would care about.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">The backend computational processes are making decisions for the user that are hard to meaningfully <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.1">notice</em> in the standard interface, because the documents are represented <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.2">as dots</em>. Nothing about that dot reveals the underlying text in the document that define between-document relationships—all the possible relationships, which is a spectrum from those that are computationally prioritized to those that the computer is functionally “blind” to.
The user may be aware of the computational steps involved in generating the scatter plot, but the interface does not provide an effective way to notice (1) the consequences of particular distance metrics and/or thresholds shaping the process or (2) whether the outcome of this process has placed points—that they would want to be spatially close—far away or placed points close to each other that the user would want far away from each other.
Finding such pairs of points amounts to the matching (memory) game,<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The Matching Game or Memory Game is typically played with a 2D array of upside-down cards where players have to identify matching cards by flipping only one card right-side-up at a time to reveal its identity</span></span></span> where flipping a card to reveal its contents is akin to hovering over a document dot. Descriptive automated short labels over clusters of dots choose to call out a single commonality for the user, and even then, users would need to manually spot-check documents associated with various dots near and far from the label to attempt judge the quality of the label, and trust that they can accurately generalize from those samples to infer an overall pattern. There’s no information scent for finding exceptions to the label’s suggested pattern within the neighborhood or where and how one neighborhood transitions into another. The user may prioritize examining dots that are spatial outliers or otherwise spatially remarkable, but these spatial deviations are a function of the embedding and, as a result, may or may not reflect deviations from the rest of the corpus along aspects that are relevant to the user’s task, context, and preferences. Exhaustive search and a superhuman working memory is necessary to fully take in what the embedding has actually done with the documents, let alone notice discrepancies along aspects they realize—as they examine documents—or already knew they care about.</p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">One response to these issues could be that we need better algorithms for embedding generation, clustering, and dimensionality reduction.
While improvements to the AI components could help, they will not solve the issues discussed above because those issues are fundamentally about the way the visualization communicates the AI output to the user.
This lack of interface support for fully seeing and understanding the AI choices makes it much harder for the user to see when the AI has missed an aspect the user values.
These algorithms also all have knobs that can be adjusted to shape their behavior, but it is difficult for the user to know which knob to adjust (gulf of execution <cite class="ltx_cite ltx_citemacro_citep">(Norman and Draper, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib22" title="">1986</a>)</cite>)—if they even see enough text to realize such adjustment is needed (gulf of evaluation <cite class="ltx_cite ltx_citemacro_citep">(Norman and Draper, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib22" title="">1986</a>)</cite>).
An AI-resilient method of visualizing documents sets and clusters should empower users both to look inside and across clusters, which in turn allows them to identify how the AI decisions do or do not suit their needs.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Summary</h5>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Not prioritizing user <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.1.1">noticing</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.1.2">judging</span>—or not supporting it at all—can lead to AI-powered interfaces misleading users or providing less utility than they are capable of.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Design Challenges</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>There often is no “best” AI choice.</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">AI choices are typically defined in terms of and evaluated relative to some “objective” ground truth, but in many contexts where AI-powered interfaces are now deployed, the user’s context, goals, opinions, values, preferences and risk tolerances dominate, rather than “objective” notions of accuracy. These aspects of the user’s situation
may be partially or completely unobservable. No matter how sophisticated the affordances are for the user to externalize the unobservable parts of their context, e.g., internal goals and relevant context, they would need to make the effort to identify and express them. In this respect, only the user is capable of determining whether an AI choice is wrong or not good enough in their eyes for their situation.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The “best” AI choice could be (1) “reading the user’s mind” (which is, for the foreseeable future, an impossible task) in order to provide what what the user currently wants, (2) providing a spectrum of options that may help the user recognize what they already want (or what they now <em class="ltx_emph ltx_font_italic" id="footnote4.1">realize</em> they want), or (3) not giving what the user wants to explicitly challenge them and perhaps drive the human-AI team to a location that the human, when reflecting after the fact, realizes is a better outcome than they had originally envisioned.</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Unnoticed (AI) choices</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Our recognition of AI choices
is not guaranteed, or even possible, in many interfaces. Sometimes those choices are literally hidden,
and sometimes they are hidden in plain sight.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">1. Invisible Choices</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">The user cannot see AI choices when the interface silently hides information based on that choice; AI choices users disagree with in this type of interface are devilishly hard to recover from.
For example, spam detection can make two types of errors.
A false negative will place spam in a user’s inbox, but that isn’t a big issue because the decision and the error are visible and easy to fix.
A false positive is more problematic, as the user will not even know of the message unless they check their spam folder: the choice and the error are invisible, with the potential to wreak havoc.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Both Glassman and Kummerfeld have experienced further difficulties with spam detection and mail forwarding. When messages are labeled as spam by the forwarding account, they are not forwarded and so while there is a unified inbox, there is not a unified spam-box.
We have been unnecessarily stressed by messages silently and erroneously moved to our spam folders, as well as embarrassed by the messages we find there, unresponded to, now that we’ve had enough painful experiences to ingrain the habit of checking it regularly.</span></span></span>
Another example is summarization, where there can be errors of omission, e.g., leaving out information that a judge would ordinarily consider to be critical context when determining a convicted defendant’s punishment.
There can also be errors of misrepresentation: producing a shorter text for which, given the task at hand, the semantics shift beyond what would be acceptable, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.1">if the user noticed</span>.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Legal language is particularly tricky—for example, using the verbs “will” instead of “shall” or “agree to assign” instead of “hereby assign” can completely change how a legal professional in the US court system will interpret the semantics of the sentence, but even a fine-tuned foundation model may not robustly respect the semantic differences of what otherwise would be synonyms with no semantic distinctions.</span></span></span>
The AI may also make more objective errors that, if not caught by the system, can be hard for the human to notice. For example, the errors introduced by LLM confabulation
within a summary look <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.2">plausible at a glance</span> by definition.
Critically, undetected errors have <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.3">both an immediate cost</span>—the user being insufficiently informed or inadvertently misled—<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.4">and a long-term cost</span>, as the user who did not notice these errors cannot flag the output as incorrect for further model training.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Choices that are visible, but hard to notice</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">A brief local change outside the narrow focus of a users’ attention may go completely unnoticed.
The classic example of this form of inattentional blindness is people not seeing a man in a gorilla suit walking among students passing a ball when focused on a task that involves watching the students and the ball <cite class="ltx_cite ltx_citemacro_citep">(Cherry, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib8" title="">2023</a>)</cite>.
Inattentional blindness can be caused by limited cognitive resources, a target object’s lack of salience, and the limitations of memory—<span class="ltx_text ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.1">a kind of seeing without noticing</span> <cite class="ltx_cite ltx_citemacro_citep">(Cherry, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib8" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Choices that are visible and noticed, but hard to understand</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">Even in cases when users are aware that a change occurred, they may not understand the nature of the change.
For example, a global change to the layout of a cloud of points could play out in the interface without the user’s ability to notice what actually changed; this is because noticing what changed would require having memory sufficient to capture the before state (which is now gone) and comparing it mentally to the current state.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Choices that are not recognized as a choice</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1">Sometimes choices go unnoticed because they are implicit. Perhaps we do not recognize a choice because it is presented as if it is inevitable or simply the truth rather than a choice of the designer or AI, making it an implicit choice. Other times, we do not recognize a choice because the option chosen for us is consistently chosen. This latter phenomenon shows up in a variety of different places: For example, in <cite class="ltx_cite ltx_citemacro_citet">Dow et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib10" title="">2011</a>)</cite>’s seminal work on parallel prototyping, participants commented on differences between analogous components<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Components can only be determined to be analogous given a structural mapping. <cite class="ltx_cite ltx_citemacro_citet">Gentner and Markman (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib12" title="">1997</a>)</cite>’s work concludes that participants implicitly look for potential structural mappings between objects, in order to find analogous components and note their differences.</span></span></span> across two prototypes (also called alignable differences) and neglected to comment on choices that were consistent across the two, either because (1) the participant did not think the designer was interested in feedback on that choice they made consistently across the two prototype designs or (2) the participants’ attention was drawn to the differences and they did not notice or cognitive engage much with the consistent choices.<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><cite class="ltx_cite ltx_citemacro_citet">Gentner and Markman (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib12" title="">1997</a>)</cite>’s work found that participants identify more numerous and nuanced alignable differences between similar objects (that have more obvious structural mappings) than differences between less similar objects (that have less obvious structural mappings). It is unclear from that work whether (1) human cognition preferentially attends to alignable differences or (2) alignable differences are cognitively easier for us to compute (or both). If human cognition preferentially attends to alignable difference, it could be explained by the need to distinguish between edible and poisonous versions of the same type of plant, e.g. berries, during our evolution <cite class="ltx_cite ltx_citemacro_citep">(Gentner and Markman, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib12" title="">1997</a>)</cite>.</span></span></span> Similarly, Variation Theory <cite class="ltx_cite ltx_citemacro_citep">(Marton, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib20" title="">2014</a>)</cite> points out that not experiencing variation over a variable value can render that variable either unnoticed or undiscernable<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>The simple example Marton provides is that if you have no concept of color, you have to experience more than one color (variation in color) to discern the concept of color, at which point you can also recognize distinct points in color space.</span></span></span>; in the language of choices, this implies that not experiencing different choices at a particular choice point can render the choice cognitively invisible.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Choices that are visible, noticed, and understandable, but users choose not to make the effort to consider</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px5.p1.1">Finally, sometimes we know the choices are there but we choose not to cognitively engage much, like most people scrolling through yet another update to the terms and conditions.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>And sometimes our habitual physical behaviors kick in before our cognitive engagement can, clicking on the button in the habitual location before we realize we have not actually thought about the information the button pertained to or even read the button label to confirm that the button was the one we habitually click!</span></span></span> This challenge is closely related to the challenge of fomenting and/or lowering barriers to cognitive engagement in AI outputs that have been acknowledged elsewhere, e.g., in a recent DARPA program call for systems that add friction in order to encourage users to engage with AI generated output more <cite class="ltx_cite ltx_citemacro_citep">(DARPA, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib9" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px5.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px5.p2.1">Cognitive engagement is a function of conscious and unconscious factors, but users cannot cognitively engage with something they do not notice.
And when AI choices go unnoticed, they cannot be judged on whether they are objectively correct, contextually appropriate, or subjectively preferable.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Insufficient context to judge (AI) choices</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In order to judge an AI choice, the user needs access to sufficient context.
To be sufficient, this context must be enough for the user to come up with their own well-informed opinion in parallel to the AI.
An even stronger requirement would be that the context is only sufficient when any additional context would not change the user’s choice.
<span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.1">Note that this is distinct from explanations of the AI’s choice, as in AI explainability research.</span></p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Making this judgement well can be hard for users, even when inattentional blindness is not at play, and we argue that many AI-powered features insufficiently support users in these tasks. This may be due to them (1) not being explicitly acknowledged in prior guidelines (and therefore more likely to be missed during the design and evaluation process), (2) being objectively difficult to design for, and/or (3) user resistance to slowing down when leveraging AI assistance, especially in low-stakes situations.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">For example, they need to read the entire summarized document as well as the summary to truly know whether they think, given their context and goals, the summary is appropriate. Interfaces that show the choice without all the relevant context risk either (1) the user confidently making a choice (while believing they have sufficient context) that does not reflect what they would have wanted if they had actually had all the context or (2) the user, knowing they do not have all the relevant context, being reduced to making a ‘gut’ decision they may feel unsure about.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>AI-Resilient Interfaces</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Relative to prior human-AI interaction design guidelines and usability heuristics, including old classics like  <cite class="ltx_cite ltx_citemacro_citet">Norman and Draper (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib22" title="">1986</a>)</cite> and new canon like <cite class="ltx_cite ltx_citemacro_citet">Amershi et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib3" title="">2019</a>)</cite>, the primary distinguishing goal of AI-resilient interfaces is to help users <em class="ltx_emph ltx_font_italic" id="S4.p1.1.1">recognize</em> (1) objectively wrong AI choices, (2) contextually inappropriate AI choices, and (3) AI choices they subjectively dislike. So that users can make use of those previously described affordances that “support efficient dismissal” and “support efficient correction — [making] it easy to edit, refine, or recover when the AI system is wrong” <cite class="ltx_cite ltx_citemacro_citep">(Amershi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib3" title="">2019</a>)</cite>, because they know it is necessary or desirable to traverse a gulf of execution <cite class="ltx_cite ltx_citemacro_citep">(Norman and Draper, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib22" title="">1986</a>)</cite> to recover from an AI choice. Especially in situations where the users’ private context dominates their judgements. Even when their notion of what they want is evolving, possibly but not necessarily in response to the AI-resilient interface’s features and affordances. Meeting these design goals should increase the safety, utility, and usability of the interface.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To make an AI-powered interface more AI-resilient, a designer may need to modify both (1) how AI choices are made visible to users, explicitly or implicitly, to support noticing and (2) providing sufficient information within the interface for the user to appropriately judge the correctness, appropriateness, and their subjective preferences over those AI choices.
This may make the interface more cognitively demanding and/or less traditionally usable, e.g., through less acceleration<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>This is referring to the acceleration of programmers described in  <cite class="ltx_cite ltx_citemacro_citet">Barke et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib7" title="">2023</a>)</cite></span></span></span> that can accumulate less thoughtful choices that become more painful to deal with later, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Vaithilingam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib25" title="">2022</a>)</cite>, but in both subjectively and objectively high-stakes situations, users may accept or even desire this if it makes them more resilient to AI choices they dislike or regard as wrong.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">This AI-resilience may actually accelerate the user’s intent formation, revision, and refinement—without driving it to a place that is globally worse than if the user had more slowly iterated on their intent without interface support. For example, in accordance with the Nielsen’s usability heuristic <em class="ltx_emph ltx_font_italic" id="S4.p3.1.1">Recognition over Recall</em>, seeing alternative AI choices (or at least sufficient information scent about alternative choices) may allow users to confidently navigate between choices that serve them best in the moment—<span class="ltx_text ltx_font_italic" id="S4.p3.1.2">or discern entire new dimensions of alternatives of their own imagining</span> just based on the contrasts across multiple AI choices or between the AI suggestions and their own mental model of what they want, as implied by Variation Theory <cite class="ltx_cite ltx_citemacro_citep">(Marton, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib20" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Given the challenges described previously, meeting each design goal depends on the context of the problem being solved.
We will start with an example of an AI-resilient interface, Grammar-Preserving Text Saliency Modulation <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib18" title="">2024</a>)</cite>, to show concretely how it achieves the design goals for reading and skimming documents, and then discuss more generic tactics, with tradeoffs.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>A Concrete Example: An AI-Resilient Interface Alternative to AI Summarization</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Specific AI-Resilience Needs</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">AI-powered systems can, however, make choices that are hard to notice. For example, when performing summarization, generative AI may (1) omit context critical to correctly understanding an output, (2) impute information that is statistically likely given the training data (and therefore likely plausible to the human consumer) but not present in the original document(s) or (3) subtly or significantly misrepresent the original document(s)’ meaning through alternative word choices or simplification. All three AI choices are tedious and memory-intensive for humans to check, negating a lot of the value of a summary, and delegating this check to another AI, which can be confidently wrong, does not solve the issue.</p>
</div>
<figure class="ltx_figure" id="S4.F5">
<p class="ltx_p" id="S4.F5.2">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S4.F5.2.1" style="width:170.7pt;">
<span class="ltx_p" id="S4.F5.2.1.1">The world is <span class="ltx_text" id="S4.F5.2.1.1.1" style="color:#BFBFBF;">at present</span> accumulating carbon dioxide <span class="ltx_text" id="S4.F5.2.1.1.2" style="color:#808080;">in the atmosphere</span> from two <span class="ltx_text" id="S4.F5.2.1.1.3" style="color:#BFBFBF;">well-known</span> sources: <span class="ltx_text" id="S4.F5.2.1.1.4" style="color:#808080;">the combustion of</span> fossil fuels and deforestation.</span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S4.F5.4.2" style="font-size:90%;">GP-TSM <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib18" title="">2024</a>)</cite> output, with multiple levels of text opacity revealing levels of AI-predicted semantic criticality, while keeping all original text (context) legible.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>AI-Resilient Approach</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">The goal of grammar-preserving text salience modulation (GP-TSM) <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib18" title="">2024</a>)</cite> was AI-resilient single-document summarization.
The approach used a large language model to perform recursive sentence compression, i.e., recursively identifying additional words that can be removed from the text while (a) retaining the core meaning and (b) leaving a grammatical result for ease of reading.
Each iteration of compression corresponds to a different level of text opacity.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S4.F5" title="Figure 5 ‣ 4.1.1. Specific AI-Resilience Needs ‣ 4.1. A Concrete Example: An AI-Resilient Interface Alternative to AI Summarization ‣ 4. AI-Resilient Interfaces ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">5</span></a> shows an example output, which shows the most concise summary (in black) and a skimmable sense of varying levels of detail (in varying shades of gray).
Even the lightest text is still legible so readers have all the context necessary to decide whether they agree with the de-emphasis of various words, and if they disagree, they can nearly effortlessly cognitively recover by reading more without taking a physical action. As described in <cite class="ltx_cite ltx_citemacro_citep">(Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib18" title="">2024</a>)</cite>, prior versions allowed users to hide text below a user-chosen threshold of salience, but this removed its AI-resiliency, since users cannot notice hidden words they do not think should be hidden.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">In terms of our AI-resilience design goals, this system (1) allows users to continuously choose what level of summarization at which to read, (2) visualizes the decisions made by the model using text saliency, and (3) retains readability of de-emphasized text with minimal effort. User studies indicate that readers could answer reading comprehension questions more accurately in less time when reading text rendered with GP-TSM, relative to both prior art in text salience modulation and normal, constant-salience text, and, because they did not read a summary, which isn’t an AI-resilient technology, they did not suffer from any AI choices they could not recognize and recover from.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Relationship with Trust and Design Principles</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Trust</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">When interacting with AI systems, users typically need to develop and maintain an accurate mental model of where it is and is not appropriate to trust the system <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib6" title="">2019</a>)</cite>.
But a less precise sense of trust is needed with an AI-resilient system because the user can more easily identify and recover from the model’s choices they disagree with.
AI-resilient systems may also make it easier for users to refine their sense of trust, because the interface design should help users see when the model makes choices (in)congruent with their context, preferences, and goals.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Existing Design Principles</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">A range of researchers, companies, organizations, and governments have proposed principles for responsible development of systems that incorporate AI.
As previously stated, (1) noticing AI choices and (2) having enough context to judge whether or not they agree with each AI choice are pre-requisites to using the affordances recommended by popular human-AI interaction design guidelines <cite class="ltx_cite ltx_citemacro_citep">(Amershi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib3" title="">2019</a>)</cite>, specifically <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">support efficient dismissal</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">support efficient correction</span>.
<cite class="ltx_cite ltx_citemacro_citet">Floridi and Cowls (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib11" title="">2019</a>)</cite> proposed several principles which relate to AI-resilience: <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.3">Autonomy: The Power to Decide</span>, <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.4">Justice: Promoting Prosperity, Preserving Solidarity, Avoiding Unfairness</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.5">Explicability: Enabling the Other Principles through Intelligibility and Accountability</span>.
Autonomy depends on being able to notice the AI choice in order to make decisions about it, and <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.6">meaningful autonomy</em> rather than a faux-autonomy requires enough context to make that decision well rather than arbitrarily.
The avoiding unfairness component of justice could be assisted by AI-resilience, as users can more easily identify decisions that are not suitable for them in their context.
Intelligibility is also well served by our notion of noticing and having the context to judge.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>AI Resilience Audits of Current Approaches</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Now that we have defined AI resilience, it is natural to ask how many existing approaches satisfy it?
Some tasks may not be amenable to this paradigm, or may require significant changes to the standard form of input and output.
In this section, we consider a range of tasks that have AI as a core part of common solutions.
For each task, we consider whether the dominant approach to supporting users is AI-resilient.
In a few cases, we also describe a specific interface and how AI resilient it is.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Automated Document Summarization</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S4.SS1" title="4.1. A Concrete Example: An AI-Resilient Interface Alternative to AI Summarization ‣ 4. AI-Resilient Interfaces ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">4.1</span></a>, we described an approach for building an AI resilient version of automated document summarization.
Here, we consider the typical formulation of the task.
In either single document or multi-document summarization, the input is a large volume of text and the output is a much shorter piece of text that is intended to contain the same core meaning.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">When reading a summary, the user does not know where each part is derived from, why they were chosen, or what alternatives were considered.
In the abstractive case, where the summary is newly generated text, it may contain entirely incorrect content.
Even in the extractive case, where the summary is composed of parts of the input, there is the possibility of misleading the user by leaving out important context or putting content together in ways that suggest invalid conclusions.
The standard fallback option for a user is to read the entire document or document collection, with a memory approaching super-humanity levels.
This does not meet any of our design goals for AI resilient interfaces, as the user does not see the choices made by the AI and the only means of judging and recovery is onerous.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Recommender Systems</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The search examples from Section <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S2.SS1.SSS1" title="2.1.1. Twins as a function of maternal age ‣ 2.1. AI-assisted Search ‣ 2. Motivating Examples ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">2.1.1</span></a> involve an AI model that ranks pages in response to the user query.
This is closely related to recommender systems, which rank items, often with the goal of being personalized for each user.
Unlike search, where there is a query driving the ranking, here the ranking is typically based on prior ratings provided by the user.
However, while the user data does lead to personalized results, the algorithm is the same across all users.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">In this setting, the user either does not see the options that were not recommended, or they may be far enough down the list of recommendations that they are essentially invisible.
The impact of user actions is also opaque, with no indication of how a user’s ratings are shaping the recommendations they receive.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Harper et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib19" title="">2015</a>)</cite> created one example of work that gives users some awareness and agency.
Specifically, the system provides users with some controls that directly change the equations in the algorithm.
Critically, the impact of those changes is shown to users by showing movie recommendations that were added and removed in response to the user action.
Their user study also showed that participants significantly preferred the results when given awareness and agency, demonstrating the value of methods that increase AI resilience.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Unsupervised Pattern Identification</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Vector representations, i.e., embeddings, are the core of modern AI methods across many subfields of AI, including for audio, visual, written, and state data.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>By state data, we intend to encompass systems that do planning and control.</span></span></span>
In a range of situations, it is helpful to use embeddings to identify patterns in data, e.g., characterizing a collection of documents, photos, or videos.
A common approach to this task is to take the high dimensional vector space (100s to 1,000s of dimensions), identify clusters in the space, project down to two dimensions, and show the space as a scatter plot, with the ability to reveal, at the user’s request, the item each dot represents, e.g., by hovering over the dots one by one.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">The choices in the process of producing these representations are entirely opaque to the user.
While all the data is accessible to users in this format, it is not practical to see much of it, let alone remember item contents in order to recognize relationships that the user cares about more or less than the AI that generated the embedding space.
The most computationally prioritized pattern(s) dominate the visualization, making it difficult to see more subtle global and local patterns.
There may also be patterns that are not captured by the vector space, which could involve items that are spread all over the space.
The hover affordance does not resolve this, as users can see at most a handful of items at a time and need to remember what they have seen previously to identify patterns.
Popular methods, such as t-SNE <cite class="ltx_cite ltx_citemacro_citep">(Van der Maaten and Hinton, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib26" title="">2008</a>)</cite>, also involve a range of configuration parameters that can radically change the appearance of the space <cite class="ltx_cite ltx_citemacro_citep">(Wattenberg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib27" title="">2016</a>)</cite>.
For these reasons, despite being superficially AI resilient (since all data is present and theoretically accessible via hovering or selecting regions <cite class="ltx_cite ltx_citemacro_citep">(Raval et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib23" title="">2023</a>)</cite>), these visualizations are not functionally AI resilient.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>AI-Assisted Writing Interfaces</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">AI-assistance for writing has dramatically changed in recent years.
While spelling and grammar correction and text prediction have existed in some form for decades, newer larger language model technologies can go considerably further in shaping the writing process.
The most common set-up involves providing one or a small sampling of potential alternative continuations of the writer’s current text, e.g., <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib24" title="">2023</a>; Gero and Chilton, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib14" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS4.p2">
<p class="ltx_p" id="S6.SS4.p2.1">Even when a few options are shown, prior work has shown that if the options are not carefully selected, people focus on certain choices while not realizing other options were available too <cite class="ltx_cite ltx_citemacro_citep">(Arnold et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib5" title="">2020</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Gero et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib13" title="">2022</a>)</cite> argue that many more samples are needed to accurately build intuition about the behavior of LLMs.
Since the space of possible text continuations is exponential, it is not feasible to show them all. Designing how and what to render in order to provide more utility to users while not overwhelming them is an open research direction.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5. </span>Machine Translation</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">Starting from the 1940s <cite class="ltx_cite ltx_citemacro_citep">(Weaver, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib28" title="">1949</a>)</cite>, the idea of automatic translation between human languages has been a major area in AI.<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>In fact, work on translation pre-dates the term Artificial Intelligence, which was coined in 1956 <cite class="ltx_cite ltx_citemacro_citep">(Moor, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib21" title="">2006</a>)</cite>.</span></span></span>
Today, widely used systems exist for translation between many language pairs and multiple modalities.</p>
</div>
<div class="ltx_para" id="S6.SS5.p2">
<p class="ltx_p" id="S6.SS5.p2.1">The AI-resilience of these systems is extremely variable.
The most critical factor is the language skills of the user.
If they have no knowledge at all of the other language, they will struggle to identify any mistake, while a somewhat knowledgeable user may notice egregious errors, and an experienced one may pick up on subtle model choices.</p>
</div>
<div class="ltx_para" id="S6.SS5.p3">
<p class="ltx_p" id="S6.SS5.p3.1">AI-resilience of the translation interface will also vary depending on the modality of the model and the setting in which translation occurs.
For speech-to-speech translation, it is not possible to statically view the two sides of the translation in order to recognize errors, while for text-to-text translation, alignment and glossing methods could help reveal misunderstandings and/or mistranslations.
In a conversational setting, the AI choices that result in translated content is transient, making user noticing and judgement of errors more difficult.<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Though at the same time, the other entity in the conversation may be able to assist in identifying and rectifying errors. We use entity rather than person since it is forseeable that the other interlocutor may be an AI model.</span></span></span>
In contrast, in an offline setting, there is time to examine and resolve issues over a longer time-scale.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6. </span>Code Generation for Programmers</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">One rapid uptake of large language models is for programming assistance.
Developers either receive spontaneous code suggestions from a model, or they write a description of the code they need and the model generates a solution.
In both cases, the results can often be used verbatim.
This is being used both for general purpose languages, e.g., with GitHub Copilot, and in specialized languages like spreadsheet macros, e.g., Google Sheets.</p>
</div>
<div class="ltx_para" id="S6.SS6.p2">
<p class="ltx_p" id="S6.SS6.p2.1">This may seem to satisfy many aspects of AI-resilience.
The developer is always shown the choice made by the model (i.e., what code to write), they have full access to their local context (i.e., their local code, which may be shared with the AI as common ground, and their not yet explicit programming goals and preferences) needed to judge the code’s usefulness, and they are able to choose whether or not to use it.
However, while they are shown the AI’s chosen generated code, they may not cognitively engage with it.
If, at a quick glance, the code seems plausibly correct, the developer may accept the suggestion without much thought, as programmers in <cite class="ltx_cite ltx_citemacro_citep">(Vaithilingam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib25" title="">2022</a>)</cite> may have been doing when they performed worse on programming problems than without AI assistance.
Even if they look closely, a developer may accept a suggestion without noticing it contains a bug and/or does not do exactly what they wanted.
And even if it is doing the task correctly, it may not be the most efficient approach, a fact the developer may not realise without putting in the effort to identify alternatives.</p>
</div>
<div class="ltx_para" id="S6.SS6.p3">
<p class="ltx_p" id="S6.SS6.p3.1">In some cases, the programmer can see other options the model generated, in a dropdown list, but the programmer may be left to their own devices to sort through the alternatives and compare and contrast each one manually without any precomputed commonalities and differences rendered in the interface, unlike the interface support available in prior systems for viewing tens, hundreds, or thousands related examples of text (like <cite class="ltx_cite ltx_citemacro_citet">Gero et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib13" title="">2022</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Gero et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib15" title="">2024</a>)</cite>) and code (like OverCode <cite class="ltx_cite ltx_citemacro_citep">(Glassman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib16" title="">2015</a>)</cite>, ExampleStack <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib30" title="">2019</a>)</cite>, Examplore <cite class="ltx_cite ltx_citemacro_citep">(Glassman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib17" title="">2018</a>)</cite>, and ParaLib <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib29" title="">2022</a>)</cite>).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.7. </span>Discussion</h3>
<div class="ltx_para" id="S6.SS7.p1">
<p class="ltx_p" id="S6.SS7.p1.1">AI-resilience is not a panacea nor easy to implement. As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#S5" title="5. Relationship with Trust and Design Principles ‣ AI-Resilient Interfaces [Working Draft]"><span class="ltx_text ltx_ref_tag">5</span></a>, it only addresses some of the AI principles and issues in systems today.</p>
</div>
<div class="ltx_para" id="S6.SS7.p2">
<p class="ltx_p" id="S6.SS7.p2.1">The analysis above also reveals a (hopefully productive) tension between users’ cognitive load and the AI-resilience their interface affords them.
Providing the additional context needed to notice and judge AI choices often incurs friction in the user experience.
In small amounts, that friction can be positive, but at some point the cost may be too high, leading users to stop using an AI-resilient system altogether.
Achieving that balance is a challenge, and one that requires challenging strongly held assumptions about the formulation of our tasks.
Even if an AI-resilient interface cannot be achieved, the exercise of trying to develop one may be enlightening, revealing issues in system design that are missed when we are following the established patterns of past work.</p>
</div>
<div class="ltx_para" id="S6.SS7.p3">
<p class="ltx_p" id="S6.SS7.p3.1">We may be reluctant to “burden” users with what would be heuristically evaluated as too much information if we cannot find a way, like GP-TSM did, to reify that information in a way that improves rather than detracts from user experience and performance. We may also underestimate how much revealing structure within the variation over a larger amount of data may support more confident sensemaking and reduce its cognitive burden.</p>
</div>
<div class="ltx_para" id="S6.SS7.p4">
<p class="ltx_p" id="S6.SS7.p4.1">We have also generally assumed that the user knows best.
What if the AI choices are better aligned with the user’s ultimate refined understanding of what they want but poorly aligned with what the user <em class="ltx_emph ltx_font_italic" id="S6.SS7.p4.1.1">currently</em> wants?
In other words, is it possible for the user <span class="ltx_text ltx_font_italic" id="S6.SS7.p4.1.2">not</span> to be “always right”?
There are several scenarios to consider: First, the user could be considered, by definition, always right (in that moment) about what serves them best, and the interaction with the AI-resilient system may help them eventually iterate their way to a set of beliefs about what they want and need from the AI interface that serves them better.
That seems like a successful human-AI interaction, even if the system is “only” helping the user converge on those final beliefs through its legible AI choices and alternatives and quickly reflecting the user’s current beliefs so that the user can understand how those beliefs in that moment do and do not need to be updated.
In contrast, if the system is less deferential to the user and stands its ground, more ‘paternalistically’ or antagonistically <cite class="ltx_cite ltx_citemacro_citep">(Arawjo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.08447v1#bib.bib4" title="">2024</a>)</cite>, how does that affect the rate at which users arrive at their final beliefs about what is best for them, and which final beliefs they arrive at?
How does that play out when the system is confidently wrong in some objective sense about the user’s context but manages to obscure that from the user?
The answers to these questions depend on many factors, including the task, the user, and the time available. While no one-size-fits-all AI-resilient interface affordances may crystallize in the next decade, we hope the concerns and tactics described here augment the human-AI design guidelines already available in a way that is critical to AI safety, utility, and usability.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This material is based upon work supported by the National Science Foundation under Grant No. IIS-2107391. This work was also supported by the Sloan Research Fellowship. Much gratitude to the expertise and effort of Ian Arawjo, who helped brainstorm and evaluate early versions of Grammar-Preserving Text Saliency Modulation.

</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bab (2021)</span>
<span class="ltx_bibblock">
2021.

</span>
<span class="ltx_bibblock">What are the chances of having twins?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.babycenter.com/pregnancy/your-baby/your-likelihood-of-having-twins-or-more_3575" title="">https://www.babycenter.com/pregnancy/your-baby/your-likelihood-of-having-twins-or-more_3575</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amershi et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, et al<span class="ltx_text" id="bib.bib3.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Guidelines for human-AI interaction. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.1">Proceedings of the 2019 chi conference on human factors in computing systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arawjo et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ian Arawjo, Alice Cai, and Elena L. Glassman. 2024.

</span>
<span class="ltx_bibblock">Antagonistic AI. (Jan 2024).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Submitted for publication.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arnold et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kenneth C Arnold, Krysta Chauncey, and Krzysztof Z Gajos. 2020.

</span>
<span class="ltx_bibblock">Predictive text encourages predictable writing. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 25th International Conference on Intelligent User Interfaces</em>. 128–138.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S Lasecki, Daniel S Weld, and Eric Horvitz. 2019.

</span>
<span class="ltx_bibblock">Beyond accuracy: The role of mental models in human-AI team performance. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">HCOMP</em>, Vol. 7. 2–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barke et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shraddha Barke, Michael B James, and Nadia Polikarpova. 2023.

</span>
<span class="ltx_bibblock">Grounded copilot: How programmers interact with code-generating models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the ACM on Programming Languages</em> 7, OOPSLA1 (2023), 85–111.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherry (2023)</span>
<span class="ltx_bibblock">
Kendra Cherry. 2023.

</span>
<span class="ltx_bibblock">Inattentional Blindness in Psychology.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.verywellmind.com/what-is-inattentional-blindness-2795020" title="">https://www.verywellmind.com/what-is-inattentional-blindness-2795020</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DARPA (2023)</span>
<span class="ltx_bibblock">
DARPA. 2023.

</span>
<span class="ltx_bibblock">Friction for Accountability in Conversational Transactions.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dow et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Steven P. Dow, Alana Glassco, Jonathan Kass, Melissa Schwarz, Daniel L. Schwartz, and Scott R. Klemmer. 2011.

</span>
<span class="ltx_bibblock">Parallel Prototyping Leads to Better Design Results, More Divergence, and Increased Self-Efficacy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">ACM Trans. Comput.-Hum. Interact.</em> 17, 4, Article 18 (dec 2011), 24 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1879831.1879836" title="">https://doi.org/10.1145/1879831.1879836</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Floridi and Cowls (2019)</span>
<span class="ltx_bibblock">
Luciano Floridi and Josh Cowls. 2019.

</span>
<span class="ltx_bibblock">A Unified Framework of Five Principles for AI in Society.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Harvard Data Science Review</em> 1, 1 (july 2019).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">https://hdsr.mitpress.mit.edu/pub/l0jsh9d1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gentner and Markman (1997)</span>
<span class="ltx_bibblock">
Dedre Gentner and Arthur B. Markman. 1997.

</span>
<span class="ltx_bibblock">Structure mapping in analogy and similarity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">American Psychologist</em> 52 (1997), 45–56.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gero et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Katy Gero, Jonathan K. Kummerfeld, and Elena L. Glassman. 2022.

</span>
<span class="ltx_bibblock">Sensemaking Interfaces for Human Evaluation of Language Model Outputs. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Human Evaluation of Generative Models Workshop</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gero and Chilton (2019)</span>
<span class="ltx_bibblock">
Katy Ilonka Gero and Lydia B Chilton. 2019.

</span>
<span class="ltx_bibblock">Metaphoria: An algorithmic companion for metaphor creation. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gero et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K Kummerfeld, and Elena L Glassman. 2024.

</span>
<span class="ltx_bibblock">Supporting Sensemaking of Large Language Model Outputs at Scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2401.13726</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glassman et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Elena L. Glassman, Jeremy Scott, Rishabh Singh, Philip J. Guo, and Robert C. Miller. 2015.

</span>
<span class="ltx_bibblock">OverCode: Visualizing Variation in Student Solutions to Programming Problems at Scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">ACM Trans. Comput.-Hum. Interact.</em> 22, 2, Article 7 (mar 2015), 35 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2699751" title="">https://doi.org/10.1145/2699751</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glassman et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Elena L. Glassman, Tianyi Zhang, Björn Hartmann, and Miryung Kim. 2018.

</span>
<span class="ltx_bibblock">Visualizing API Usage Examples at Scale. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(CHI ’18)</em>. Association for Computing Machinery, New York, NY, USA, 1–12.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3173574.3174154" title="">https://doi.org/10.1145/3173574.3174154</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ziwei Gu, Ian Arawjo, Kenneth Li, Jonathan K Kummerfeld, and Elena L Glassman. 2024.

</span>
<span class="ltx_bibblock">An AI-Resilient Text Rendering Technique for Reading and Skimming Documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">arXiv preprint arXiv:2401.10873</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
F. Maxwell Harper, Funing Xu, Harmanpreet Kaur, Kyle Condiff, Shuo Chang, and Loren Terveen. 2015.

</span>
<span class="ltx_bibblock">Putting Users in Control of their Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 9th ACM Conference on Recommender Systems</em> (Vienna, Austria) <em class="ltx_emph ltx_font_italic" id="bib.bib19.4.2">(RecSys ’15)</em>. Association for Computing Machinery, New York, NY, USA, 3–10.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2792838.2800179" title="">https://doi.org/10.1145/2792838.2800179</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marton (2014)</span>
<span class="ltx_bibblock">
Ference Marton. 2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Necessary conditions of learning</em>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moor (2006)</span>
<span class="ltx_bibblock">
James Moor. 2006.

</span>
<span class="ltx_bibblock">The Dartmouth College Artificial Intelligence Conference: The Next Fifty Years.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">AI Magazine</em> 27, 4 (Dec. 2006), 87.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609/aimag.v27i4.1911" title="">https://doi.org/10.1609/aimag.v27i4.1911</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Norman and Draper (1986)</span>
<span class="ltx_bibblock">
Donald A. Norman and Stephen W. Draper (Eds.). 1986.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">User centered system design : new perspectives on human-computer interaction</em>.

</span>
<span class="ltx_bibblock">Lawrence Erlbaum Associates, Hillsdale, N.J.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raval et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shivam Raval, Carolyn Wang, Fernanda Viégas, and Martin Wattenberg. 2023.

</span>
<span class="ltx_bibblock">Explain-and-Test: An Interactive Machine Learning Framework for Exploring Text Embeddings. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">2023 IEEE Visualization and Visual Analytics (VIS)</em>. IEEE, 216–220.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nikhil Singh, Guillermo Bernal, Daria Savchenko, and Elena L Glassman. 2023.

</span>
<span class="ltx_bibblock">Where to hide a stolen elephant: Leaps in creative writing with multimodal machine intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">ACM Transactions on Computer-Human Interaction</em> 30, 5 (2023), 1–57.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaithilingam et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Priyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. 2022.

</span>
<span class="ltx_bibblock">Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Chi conference on human factors in computing systems extended abstracts</em>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van der Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
Laurens Van der Maaten and Geoffrey Hinton. 2008.

</span>
<span class="ltx_bibblock">Visualizing data using t-SNE.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Journal of machine learning research</em> 9, 11 (2008).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wattenberg et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Martin Wattenberg, Fernanda Viégas, and Ian Johnson. 2016.

</span>
<span class="ltx_bibblock">How to Use t-SNE Effectively.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Distill</em> (2016).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.23915/distill.00002" title="">https://doi.org/10.23915/distill.00002</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weaver (1949)</span>
<span class="ltx_bibblock">
Warren Weaver. 1949.

</span>
<span class="ltx_bibblock">Translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">The Rockefeller Foundation</em> (July 1949).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Litao Yan, Miryung Kim, Björn Hartmann, Tianyi Zhang, and Elena L Glassman. 2022.

</span>
<span class="ltx_bibblock">Concept-annotated examples for library comparison. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em>. 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Di Yang, Crista Lopes, and Miryung Kim. 2019.

</span>
<span class="ltx_bibblock">Analyzing and supporting adaptation of online code examples. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)</em>. IEEE, 316–327.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May 15 14:50:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
