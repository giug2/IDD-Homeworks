<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1908.06472] Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles</title><meta property="og:description" content="This paper describes preliminary work in the recent promising approach of generating synthetic training data for facilitating
the learning procedure of deep learning (DL) models, with a focus on aerial photos produced …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1908.06472">

<!--Generated on Sat Mar 16 17:16:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="UAV Deep Learning Generative Data Aerial Imagery">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Pervasive Systems Group, Department of Computer Science
<br class="ltx_break">University of Twente, The Netherlands 
<br class="ltx_break"><span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>Email: a.kamilaris@utwente.nl, g.c.vandenbrink@student.utwente.nl</span></span></span>
<br class="ltx_break"><a target="_blank" href="https://www.utwente.nl/en/eemcs/ps/" title="" class="ltx_ref">https://www.utwente.nl/en/eemcs/ps/</a>
</span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Research Centre on Interactive Media, Smart Systems and Emerging Technologies (RISE), Nicosia, Cyprus
<br class="ltx_break"><span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>Email: a.kamilaris@rise.org.cy</span></span></span>
<br class="ltx_break"><a target="_blank" href="http://www.rise.org.cy/" title="" class="ltx_ref">http://www.rise.org.cy/</a>
</span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Department of Computer Science, University of Cyprus, Nicosia, Cyprus 
<br class="ltx_break"><span id="id3.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>Email: skarat01@cs.ucy.ac.cy</span></span></span>
<br class="ltx_break"><a target="_blank" href="https://www.cs.ucy.ac.cy" title="" class="ltx_ref">https://www.cs.ucy.ac.cy</a>
</span></span></span>
<h1 class="ltx_title ltx_title_document">Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Kamilaris
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Corjan van den Brink and Savvas Karatsiolis
</span><span class="ltx_author_notes">1133</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">This paper describes preliminary work in the recent promising approach of generating synthetic training data for facilitating
the learning procedure of deep learning (DL) models, with a focus on aerial photos produced by unmanned aerial vehicles (UAV). The general concept and methodology are described, and
preliminary results are presented, based on a classification problem of fire identification in forests as well as a counting problem of estimating number of houses in urban areas. The proposed technique
constitutes a new possibility for the DL community, especially related to UAV-based imagery analysis, with much potential, promising results, and unexplored ground for further research.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>UAV Deep Learning Generative Data Aerial Imagery
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning (DL) constitutes a recent, modern technique for image processing and data analysis with
large potential <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. DL belongs to the machine learning (ML) computational field and is similar to artificial neural networks (ANN). DL extends ML by adding more ”depth” (complexity) into the model,
transforming the data using various functions that allow data representation in a hierarchical way, through
several abstraction levels.
DL seems to be offering better precision results in classification and/or counting computer vision-related problems, in comparison to traditional techniques such as Scalable Vector Machines and Random Forests, according to relevant surveys <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">An advantage of DL is the reduced need of feature engineering (FE). Previously, traditional approaches
for image classification were based on hand-engineered features, whose performance affected the
results heavily <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Although DL does not require FE, it still needs appropriate datasets as input in DL models during learning. These datasets need to be large, to allow DL models to learn the problem elaborately, and
expressive, to capture the variation of classes/features that need to be classified/predicted at the model
output. An existing problem is the limited availability of such appropriate datasets. This limitation makes
DL models sometimes difficult to generalize and to learn the problem well, towards high precision.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Towards addressing this limitation, a recent possibility is the generation of synthetic datasets to train DL models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Models are trained using synthetic images, and they are then able to classify images of the real world, or count objects encountered in the real-world images, via this transfer learning-based method.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The contribution of this paper is twofold: on one hand, to present state of art research in generating synthetic data for training DL models. On the other hand, to present preliminary work on a classification problem of fire identification in forests and a counting problem of estimating number of houses in urban areas, based on two datasets comprised of aerial photos.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">DL is divided in discriminative and generative models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The
former is about predictions/classifications, and the latter about synthesis/generation of data similar to the
input datasets. The use of generative data to train DL models is promising, with
early attempts in agriculture indicating positive outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Table <a href="#S2.T1" title="Table 1 ‣ 2 Related Work ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists related work in the field of generating training data to train DL models. The year of publication for every paper reveals how modern this technique is. Please note that we avoided adding details about performance metrics and evaluation results for each paper, because each author used different metrics and experimented on different real-world datasets for testing. However, the general conclusion in all papers was that the performance according to the metric(s) used, was better than baseline (i.e. datasets not enhanced with synthetic data) or state-of-art related work.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">From Table <a href="#S2.T1" title="Table 1 ‣ 2 Related Work ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it is evident that related work has not entered yet the domain of UAV-based imagery analysis. The only exception is Meta-Sim <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which tries to learn a generative model of synthetic scenes automatically, via probabilistic scene grammars, and then it obtains images and their corresponding ground-truth via a graphics engine. Meta-Sim validates this idea addressing the problem of semantic segmentation of simulated aerial views of simple roadways. Beyond this work, to our knowledge, no other work has focused yet on generative data-based approaches for UAV-based imaging-related applications.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Related work in generative data for training DL models.</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1.1" class="ltx_p" style="width:28.5pt;"><span id="S2.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Year</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S2.T1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.2.1.1" class="ltx_p" style="width:284.5pt;"><span id="S2.T1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Purpose</span></span>
</span>
</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S2.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.3.1.1" class="ltx_p" style="width:22.8pt;"><span id="S2.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Ref.</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.1.1.1" class="ltx_p" style="width:28.5pt;">2007</span>
</span>
</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.2.1.1" class="ltx_p" style="width:284.5pt;">Simulating fluorescence microscope images of cell populations for automated image cytometry</span>
</span>
</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.2.1.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.1.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.2.1.1" class="ltx_p" style="width:284.5pt;">Enhancing soil images coming from X-ray tomography, generating roots to help the model identify the roots from the soils</span>
</span>
</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.3.2.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.1.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.2.1.1" class="ltx_p" style="width:284.5pt;">Simulating top-down images of overlapping plants on soil background, to classify 23 different weed species and maize.</span>
</span>
</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.4.3.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.1.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.2.1.1" class="ltx_p" style="width:284.5pt;">Generating fully labeled, dynamic, and photo-realistic proxy virtual world, with a focus on objects of interest, e.g. cars.</span>
</span>
</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.5.4.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.1.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.2.1.1" class="ltx_p" style="width:284.5pt;">Generating synthetic data for semantic segmentation of outdoor scenes, for recognizing aspects such as roads, buildings, cars, people, lights etc.</span>
</span>
</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.6.5.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.1.1.1" class="ltx_p" style="width:28.5pt;">2016</span>
</span>
</td>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.2.1.1" class="ltx_p" style="width:284.5pt;">Automatically generating realistic synthetic images with pixel-level annotations for semantic segmentation</span>
</span>
</td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.7.6.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.8.7" class="ltx_tr">
<td id="S2.T1.1.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.1.1.1" class="ltx_p" style="width:28.5pt;">2017</span>
</span>
</td>
<td id="S2.T1.1.8.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.2.1.1" class="ltx_p" style="width:284.5pt;">Creating synthetic images to predict number of tomatoes in the images.</span>
</span>
</td>
<td id="S2.T1.1.8.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.8.7.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.9.8" class="ltx_tr">
<td id="S2.T1.1.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.8.1.1.1" class="ltx_p" style="width:28.5pt;">2018</span>
</span>
</td>
<td id="S2.T1.1.9.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.8.2.1.1" class="ltx_p" style="width:284.5pt;">Generating synthetic data to identify melanoma skin cancer.</span>
</span>
</td>
<td id="S2.T1.1.9.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.9.8.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.10.9" class="ltx_tr">
<td id="S2.T1.1.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.10.9.1.1.1" class="ltx_p" style="width:28.5pt;">2018</span>
</span>
</td>
<td id="S2.T1.1.10.9.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.10.9.2.1.1" class="ltx_p" style="width:284.5pt;">Synthetic data for 2D bounding box car detection.</span>
</span>
</td>
<td id="S2.T1.1.10.9.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.10.9.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.11.10" class="ltx_tr">
<td id="S2.T1.1.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.10.1.1.1" class="ltx_p" style="width:28.5pt;">2018</span>
</span>
</td>
<td id="S2.T1.1.11.10.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.10.2.1.1" class="ltx_p" style="width:284.5pt;">Generating 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layout, for teaching an agent to navigate in an unseen 3D environment.</span>
</span>
</td>
<td id="S2.T1.1.11.10.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.11.10.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.12.11" class="ltx_tr">
<td id="S2.T1.1.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.11.1.1.1" class="ltx_p" style="width:28.5pt;">2018</span>
</span>
</td>
<td id="S2.T1.1.12.11.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.11.2.1.1" class="ltx_p" style="width:284.5pt;">Generating scenes for teaching an artificial agent to execute tasks in a simulated household environment.</span>
</span>
</td>
<td id="S2.T1.1.12.11.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S2.T1.1.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.12.11.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></span>
</span>
</td>
</tr>
<tr id="S2.T1.1.13.12" class="ltx_tr">
<td id="S2.T1.1.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S2.T1.1.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.12.1.1.1" class="ltx_p" style="width:28.5pt;">2019</span>
</span>
</td>
<td id="S2.T1.1.13.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.1.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.12.2.1.1" class="ltx_p" style="width:284.5pt;">a) Generating data for semantic segmentation of aerial views of roadways. b) Simulating urban scenes for object detection in urban car driving.</span>
</span>
</td>
<td id="S2.T1.1.13.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S2.T1.1.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.13.12.3.1.1" class="ltx_p" style="width:22.8pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The general methodology followed in this paper is illustrated in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Methodology ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. More advanced and recent proposals based on this general methodology will be discussed in Section <a href="#S5" title="5 Discussion ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. DL models are trained with synthetic
data, and then tested with real-world data. The precision/accuracy results are analyzed and compared with the state-of-art related work (if available), and the observations made are given as feedback to the creation process of the synthetic datasets, to become more detailed and complete (e.g. to include some aspects of the real-world data not included originally, but which affect the model’s prediction capabilities).</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/1908.06472/assets/method.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="445" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Basic methodology in generating data for training DL models.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Our approach in synthetic dataset design would be to understand how DL models perform classification,
based on the existing real-world datasets (i.e. problem under study for classification or counting).
To achieve this, we take advantage of the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which allows to visualize what happens inside DL models, i.e. which aspects/characteristics of the image are the ones that trigger the final classification. These characteristics could then be used to better design the synthetic datasets, emphasizing on these aspects when creating the simulated images.
In this paper, we focused on two different applications:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">A classification problem of identifying fires in forest areas from aerial photos.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">A counting problem of estimating number of houses from aerial photos.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The former is useful for UAV which monitor forest areas for fires and smoke, while the latter would be useful for policy-makers who want to understand distribution of houses in urban areas, possibilities for photovoltaic systems, urban gardening in roofs etc.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">For the problems under study, the synthetic datasets (used for training the DL model) have been created by means of Python, by using the Python Imaging Library<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Python Imaging Library. https://pypi.python.org/pypi/PIL</span></span></span> and
OpenCV<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>OpenCV. https://pypi.python.org/pypi/opencv-python</span></span></span>. PIL libraries allow to combine graphics creation, together with programming code and computer logic, using code in order to create dots, lines, rectangles, polygons, circles, ellipses and combinations, allowing to add color, transparency, borders and outlines, but also to include filters such as ”Gaussian Blur”, smoothen the image, enhance the edges etc.
By means of Python scripts, based on the PIL graphic features, we created more complex structures such as smoke, fire, houses, trees, fences, gardens etc. Samples of the synthetic data for the scenarios under study are depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methodology ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (top).</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Regarding the real-world datasets (used for testing the DL model), for the fire identification case, 100 aerial photos were downloaded from Google Images, 50 of them showing forest areas and another 50 showing a forest fire. For the counting houses case, 20 aerial photos from urban areas of Tanzania have been selected, from the Open AI Tanzania Challenge<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Open AI Tanzania Challenge. https://blog.werobotics.org/2018/08/06/welcome-to-the-open-ai-tanzania-challenge/</span></span></span>. We cropped these photos in 100x100 pixel images, and counted the number of houses manually at each cropped photo. The result was a dataset of 60 images, each having <math id="S3.p5.1.m1.2" class="ltx_Math" alttext="[0,38]" display="inline"><semantics id="S3.p5.1.m1.2a"><mrow id="S3.p5.1.m1.2.3.2" xref="S3.p5.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.p5.1.m1.2.3.2.1" xref="S3.p5.1.m1.2.3.1.cmml">[</mo><mn id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml">0</mn><mo id="S3.p5.1.m1.2.3.2.2" xref="S3.p5.1.m1.2.3.1.cmml">,</mo><mn id="S3.p5.1.m1.2.2" xref="S3.p5.1.m1.2.2.cmml">38</mn><mo stretchy="false" id="S3.p5.1.m1.2.3.2.3" xref="S3.p5.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.2b"><interval closure="closed" id="S3.p5.1.m1.2.3.1.cmml" xref="S3.p5.1.m1.2.3.2"><cn type="integer" id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1">0</cn><cn type="integer" id="S3.p5.1.m1.2.2.cmml" xref="S3.p5.1.m1.2.2">38</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.2c">[0,38]</annotation></semantics></math> houses from an aerial view. Samples of the real-world datasets for the two scenarios under study are depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methodology ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (bottom). Table <a href="#S3.T2" title="Table 2 ‣ 3 Methodology ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the number of images used for training and testing of the two scenarios under study.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Number of images used for training and testing of the DL models.</figcaption>
<table id="S3.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1.1" class="ltx_p" style="width:76.8pt;"><span id="S3.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Scenario</span></span>
</span>
</th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.2.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Purpose</span></span>
</span>
</th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p" style="width:202.0pt;"><span id="S3.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">No. of images</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.1.1.1" class="ltx_p" style="width:76.8pt;">Fire identification</span>
</span>
</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.2.1.1" class="ltx_p" style="width:56.9pt;">Training</span>
</span>
</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.2.1.3.1.1" class="ltx_p" style="width:202.0pt;">2,000 synthetic images</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.1.1.1" class="ltx_p" style="width:76.8pt;">Fire identification</span>
</span>
</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.2.1.1" class="ltx_p" style="width:56.9pt;">Testing</span>
</span>
</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.3.2.3.1.1" class="ltx_p" style="width:202.0pt;">100 real-world aerial photos (classified as 50 images of forest and 50 images of fire)</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.1.1.1" class="ltx_p" style="width:76.8pt;">Counting houses</span>
</span>
</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.2.1.1" class="ltx_p" style="width:56.9pt;">Training</span>
</span>
</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.4.3.3.1.1" class="ltx_p" style="width:202.0pt;">10,000 synthetic images (labelled with exact number of houses)</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.1.1.1" class="ltx_p" style="width:76.8pt;">Counting houses</span>
</span>
</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.2.1.1" class="ltx_p" style="width:56.9pt;">Testing</span>
</span>
</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.5.4.3.1.1" class="ltx_p" style="width:202.0pt;">60 real-world aerial photos (labelled with exact number of houses)</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/fire1.png" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="411" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/fire2.png" id="S3.F2.2.g1" class="ltx_graphics ltx_img_landscape" width="538" height="418" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.3" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/houses1.png" id="S3.F2.3.g1" class="ltx_graphics ltx_img_square" width="479" height="484" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/houses2.png" id="S3.F2.4.g1" class="ltx_graphics ltx_img_square" width="479" height="484" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.5" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/fire1_real.jpg" id="S3.F2.5.g1" class="ltx_graphics ltx_img_landscape" width="568" height="443" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.6" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/fire2_real.jpg" id="S3.F2.6.g1" class="ltx_graphics ltx_img_square" width="527" height="455" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.7" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/houses1_real.png" id="S3.F2.7.g1" class="ltx_graphics ltx_img_square" width="509" height="509" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S3.F2.8" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_middle" style="width:104.1pt;">
<img src="/html/1908.06472/assets/media/houses2_real.png" id="S3.F2.8.g1" class="ltx_graphics ltx_img_square" width="509" height="509" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example images from the synthetic datasets (top). Example images from the real-world datasets (bottom). Images on the left are for the fire identification scenario, while images on the right for the case of the estimation of number of houses.</figcaption>
</figure>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">As a DL model, we used the Inception-v3 convolutional neural network (CNN) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (with some adaptations, see below), as it is one of the fastest CNN architectures available, with high accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. We used the default class provided by Keras/TensorFlow during our experiments. Data augmentation was used too.
For the counting houses case, our early experiments indicated we should perform adaptations to Inception-v3, to become more optimized for counting correctly. The most important design considerations were the following:</p>
</div>
<div id="S3.p7" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">No pre-training with other datasets (e.g. ImageNet). Filters created by ImageNet are different than the filters required for counting houses.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Use of dropout (i.e. 35%).</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Max pooling instead of average pooling.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">Use larger filters at the convolutions at the beginning (i.e. 7x7) of the CNN.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p">Use a larger value for stride (i.e. stride=5).</p>
</div>
</li>
<li id="S3.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i6.p1" class="ltx_para">
<p id="S3.I2.i6.p1.1" class="ltx_p">Use a dense layer with only one output at the end of the CNN.</p>
</div>
</li>
<li id="S3.I2.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i7.p1" class="ltx_para">
<p id="S3.I2.i7.p1.1" class="ltx_p">Use ReLu for the prediction of final outcome.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">Figures <a href="#S4.F3" title="Figure 3 ‣ 4 Results ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4.F4" title="Figure 4 ‣ 4 Results ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> show the results of the training of the DL models (i.e. synthetic data) and of the testing of the model in real-world data, for the fire identification and counting houses case respectively. Classification accuracy (CA) was used as the performance metric for the fire identification case, while Mean Square Error (MSE) for the counting houses scenario. The fire identification case required 24 epochs of training for the model to learn how to classify with <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="CA=96\%" display="inline"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mrow id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml"><mi id="S4.p1.1.m1.1.1.2.2" xref="S4.p1.1.m1.1.1.2.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.p1.1.m1.1.1.2.1" xref="S4.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.p1.1.m1.1.1.2.3" xref="S4.p1.1.m1.1.1.2.3.cmml">A</mi></mrow><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml"><mn id="S4.p1.1.m1.1.1.3.2" xref="S4.p1.1.m1.1.1.3.2.cmml">96</mn><mo id="S4.p1.1.m1.1.1.3.1" xref="S4.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><apply id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2"><times id="S4.p1.1.m1.1.1.2.1.cmml" xref="S4.p1.1.m1.1.1.2.1"></times><ci id="S4.p1.1.m1.1.1.2.2.cmml" xref="S4.p1.1.m1.1.1.2.2">𝐶</ci><ci id="S4.p1.1.m1.1.1.2.3.cmml" xref="S4.p1.1.m1.1.1.2.3">𝐴</ci></apply><apply id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S4.p1.1.m1.1.1.3.1.cmml" xref="S4.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.p1.1.m1.1.1.3.2.cmml" xref="S4.p1.1.m1.1.1.3.2">96</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">CA=96\%</annotation></semantics></math> on the validation dataset, while counting houses needed 18 epochs for the model to learn how to count with <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="MSE=20" display="inline"><semantics id="S4.p1.2.m2.1a"><mrow id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mrow id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml"><mi id="S4.p1.2.m2.1.1.2.2" xref="S4.p1.2.m2.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.2.1" xref="S4.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.p1.2.m2.1.1.2.3" xref="S4.p1.2.m2.1.1.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.p1.2.m2.1.1.2.1a" xref="S4.p1.2.m2.1.1.2.1.cmml">​</mo><mi id="S4.p1.2.m2.1.1.2.4" xref="S4.p1.2.m2.1.1.2.4.cmml">E</mi></mrow><mo id="S4.p1.2.m2.1.1.1" xref="S4.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><eq id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1.1"></eq><apply id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2"><times id="S4.p1.2.m2.1.1.2.1.cmml" xref="S4.p1.2.m2.1.1.2.1"></times><ci id="S4.p1.2.m2.1.1.2.2.cmml" xref="S4.p1.2.m2.1.1.2.2">𝑀</ci><ci id="S4.p1.2.m2.1.1.2.3.cmml" xref="S4.p1.2.m2.1.1.2.3">𝑆</ci><ci id="S4.p1.2.m2.1.1.2.4.cmml" xref="S4.p1.2.m2.1.1.2.4">𝐸</ci></apply><cn type="integer" id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">MSE=20</annotation></semantics></math> on the validation dataset. In this case, MSE measures the average of the squares of the errors of the difference between the actual counts of houses in the images (i.e. ground truth counts of the real-world dataset) and the counts predicted by the DL model.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1908.06472/assets/media/fire_learning.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="330" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Training and validation classification accuracy at the fire identification challenge.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/1908.06472/assets/media/houses_learning.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="356" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Training and validation MSE at the counting houses challenge.</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Results of the two scenarios under study indicate that synthetic data can prove useful for training DL models, particularly related to UAV-based aerial imagery. This evidence is backed by related work, listed in Section <a href="#S2" title="2 Related Work ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Nevertheless, we need to be cautious with these indications, because the DL models were optimized to perform well in the specific validation datasets. It is questionable (and it has not been tested) whether the DL models can produce similar results in different real-world datasets that focus on similar problems and applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.3" class="ltx_p">The DL model for the fire identification case had very high CA. We achieved this accuracy via a hybrid approach, adding background of real forest images to the generated smoke and fire. Before this, validation CA was around 86%.
On the other hand, the DL model for the counting problem still needs improvement. A <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="MSE=20" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mrow id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml"><mi id="S5.p2.1.m1.1.1.2.2" xref="S5.p2.1.m1.1.1.2.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.p2.1.m1.1.1.2.1" xref="S5.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.p2.1.m1.1.1.2.3" xref="S5.p2.1.m1.1.1.2.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.p2.1.m1.1.1.2.1a" xref="S5.p2.1.m1.1.1.2.1.cmml">​</mo><mi id="S5.p2.1.m1.1.1.2.4" xref="S5.p2.1.m1.1.1.2.4.cmml">E</mi></mrow><mo id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><eq id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1"></eq><apply id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2"><times id="S5.p2.1.m1.1.1.2.1.cmml" xref="S5.p2.1.m1.1.1.2.1"></times><ci id="S5.p2.1.m1.1.1.2.2.cmml" xref="S5.p2.1.m1.1.1.2.2">𝑀</ci><ci id="S5.p2.1.m1.1.1.2.3.cmml" xref="S5.p2.1.m1.1.1.2.3">𝑆</ci><ci id="S5.p2.1.m1.1.1.2.4.cmml" xref="S5.p2.1.m1.1.1.2.4">𝐸</ci></apply><cn type="integer" id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">MSE=20</annotation></semantics></math> means that the model can predict the number of houses with an error of <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="\pm 4.47" display="inline"><semantics id="S5.p2.2.m2.1a"><mrow id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml"><mo id="S5.p2.2.m2.1.1a" xref="S5.p2.2.m2.1.1.cmml">±</mo><mn id="S5.p2.2.m2.1.1.2" xref="S5.p2.2.m2.1.1.2.cmml">4.47</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><apply id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.p2.2.m2.1.1.1.cmml" xref="S5.p2.2.m2.1.1">plus-or-minus</csymbol><cn type="float" id="S5.p2.2.m2.1.1.2.cmml" xref="S5.p2.2.m2.1.1.2">4.47</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">\pm 4.47</annotation></semantics></math> houses. For example, for a photo with 20 houses, the model would predict in the range of <math id="S5.p2.3.m3.2" class="ltx_Math" alttext="[16,24]" display="inline"><semantics id="S5.p2.3.m3.2a"><mrow id="S5.p2.3.m3.2.3.2" xref="S5.p2.3.m3.2.3.1.cmml"><mo stretchy="false" id="S5.p2.3.m3.2.3.2.1" xref="S5.p2.3.m3.2.3.1.cmml">[</mo><mn id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">16</mn><mo id="S5.p2.3.m3.2.3.2.2" xref="S5.p2.3.m3.2.3.1.cmml">,</mo><mn id="S5.p2.3.m3.2.2" xref="S5.p2.3.m3.2.2.cmml">24</mn><mo stretchy="false" id="S5.p2.3.m3.2.3.2.3" xref="S5.p2.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.2b"><interval closure="closed" id="S5.p2.3.m3.2.3.1.cmml" xref="S5.p2.3.m3.2.3.2"><cn type="integer" id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1">16</cn><cn type="integer" id="S5.p2.3.m3.2.2.cmml" xref="S5.p2.3.m3.2.2">24</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.2c">[16,24]</annotation></semantics></math>. There is definitely space for further work on this. We note that we reached this MSE after many weeks of observations and the iterative process of adding more details to the generated synthetic dataset (see Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Methodology ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). These details included trees, grass, swimming pools, fences etc.. Each of them helped to reduce the overall error.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Applications of the proposed approach can be found in various research domains and scientific disciplines, such as agriculture, life sciences, microbiology, earth sciences etc. The approach of generative data for training DL models would be extremely useful for UAV and robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, where computer vision is involved. It could improve operation and accuracy of automatic robots collecting crops, removing weeds or estimating yields of crops <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. It could also be used in disaster monitoring and surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, where remote sensing (i.e. satellites or UAV) is used to identify events of interest (e.g. disasters, violence incidents, land cover mapping, effects on climate change etc.). Finallly, it could be used in environmental studies, e.g. to understand the environmental impact of livestock agriculture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Moreover, we highlight some recent state-of-art work in this domain, which relates to our proposed methodology, showing promising results in application areas other than UAV aerial imagery. First, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> incorporates two significant improvements: layered boosting (i.e. a layered approach, where training is done in stages) and selective sampling (i.e. streamline the training process by reducing the impact of the low quality samples, such as trivial cases or outliers). Second, the concept of Structured Domain Randomization (SDR) places objects and distractors randomly according to probability distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and from probabilistic scene grammars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, which arise from the specific problem at hand. In this manner, SDR-generated imagery enables the neural network to take the context around an object into consideration during detection. Third, related specifically to counting, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> evades the hard task of learning to detect and localize individual object instances. Instead, it casts the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region.
Furthermore, our work, as well as the aforementioned promising approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> can be combined with Generative Adversarial Networks (GANs), to stylize synthetic images to look more like those captured in the real world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Finally, we note another recent possibility, that of utilizing the <span id="S5.p5.1.1" class="ltx_text ltx_font_italic">Aerial Informatics and Robotics</span> platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> for generating seamlessly training data related to UAV-based aerial imagery. This platform acts as an easy-to-use simulator that aims to enable designers and developers of robotic systems to generate graphical data. Its biggest advantage is that it uses recent advances in computation and graphics to simulate the physics and perception such that the environment realistically reflects the actual world.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper has described preliminary work in the approach of generating synthetic training data for facilitating
the learning procedure of DL models, with a focus on UAV-based aerial imagery. The general methodology of this approach was described, and preliminary results were presented, focused on two different challenges: a classification problem of fire identification in forests as well as a counting problem of estimating number of houses in urban areas. Results were promising, but there is still space for improvement, especially in the counting houses case.
Use of synthetic data for training DL models in aerial imagery is a new exciting possibility for the research community working in this area, especially in cases where ground-truth data is scarce or expensive to produce.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">For future work, we aim to experiment with more realistic generation of synthetic data, by using game engines such as the Unity development platform<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Unity. https://unity.com</span></span></span>. We also aim to apply our methodology in new UAV-related applications such as human crowd counting, identification and counting of endangered species and wild animals etc., enhancing our methodology with the new proposals described in Section <a href="#S5" title="5 Discussion ‣ Training Deep Learning Models via Synthetic Data: Application in Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, in order to minimize the distribution
gap between the rendered outputs of the synthetic data and the target real-world data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Amara, J., Bouaziz, B., Algergawy, A., et al.: A deep learning-based approach
for banana leaf diseases classification. In: BTW (Workshops). pp. 79–88
(2017)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Canziani, A., Paszke, A., Culurciello, E.: An analysis of deep neural network
models for practical applications. arXiv preprint arXiv:1605.07678 (2016)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Douarre, C., Schielein, R., Frindel, C., Gerth, S., Rousseau, D.: Deep learning
based root-soil segmentation from x-ray tomography. bioRxiv p. 071662 (2016)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Dyrmann, M., Mortensen, A.K., Midtiby, H.S., Jorgensen, R.N., et al.:
Pixel-wise classification of weeds and crops in images by using a fully
convolutional neural network. In: Proceedings of the International Conference
on Agricultural Engineering, Aarhus, Denmark. pp. 26–29 (2016)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Gaidon, A., Wang, Q., Cabon, Y., Vig, E.: Virtual worlds as proxy for
multi-object tracking analysis. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4340–4349 (2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in
neural information processing systems. pp. 2672–2680 (2014)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kamilaris, A.: Simulating training data for deep learning models. In: Machine
Learning in the Environmental Sciences Workshop, in Proc. of EnviroInfo 2018.
Munich, Germany (September 2018)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kamilaris, A., Assumpcio, A., Blasi, A.B., Torrellas, M., Prenafeta-Boldu,
F.X.: Estimating the environmental impact of agriculture by means of
geospatial and big data analysis: The case of catalonia. In: Proc. of
EnviroInfo. Luxembourg (September 2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kamilaris, A., Prenafeta-Boldu, F.X.: Disaster monitoring using unmanned aerial
vehicles and deep learning. In: Disaster Management for Resilience and Public
Safety Workshop, in Proc. of EnviroInfo2017. Luxembourg (September 2017)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kamilaris, A., Prenafeta-Boldu, F.X.: Deep learning in agriculture: A survey.
Computers and Electronics in Agriculture <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">147</span>, 70–90 (2018)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Kar, A., Prakash, A., Liu, M.Y., Cameracci, E., Yuan, J., Rusiniak, M., Acuna,
D., Torralba, A., Fidler, S.: Meta-sim: Learning to generate synthetic
datasets. arXiv preprint arXiv:1904.11621 (2019)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Lehmussola, A., Ruusuvuori, P., Selinummi, J., Huttunen, H., Yli-Harja, O.:
Computational framework for simulating fluorescence microscope images with
cell populations. IEEE transactions on medical imaging <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">26</span>(7),
1010–1016 (2007)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Lempitsky, V., Zisserman, A.: Learning to count objects in images. In: Advances
in neural information processing systems. pp. 1324–1332 (2010)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Li, P., Liang, X., Jia, D., Xing, E.P.: Semantic-aware grad-gan for
virtual-to-real urban scene adaption. arXiv preprint arXiv:1801.01726 (2018)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Olah, C., Satyanarayan, A., Johnson, I., Carter, S., Schubert, L., Ye, K.,
Mordvintsev, A.: The building blocks of interpretability. Distill
<span id="bib.bib15.1.1" class="ltx_text ltx_font_bold">3</span>(3),  e10 (2018)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Prakash, A., Boochoon, S., Brophy, M., Acuna, D., Cameracci, E., State, G.,
Shapira, O., Birchfield, S.: Structured domain randomization: Bridging the
reality gap by context-aware synthetic data. arXiv preprint arXiv:1810.10093
(2018)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Puig, X., Ra, K., Boben, M., Li, J., Wang, T., Fidler, S., Torralba, A.:
Virtualhome: Simulating household activities via programs. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. pp.
8494–8502 (2018)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Rahnemoonfar, M., Sheppard, C.: Deep count: fruit counting based on deep
simulated learning. Sensors <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">17</span>(4),  905 (2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: Ground truth
from computer games. In: European Conference on Computer Vision. pp.
102–118. Springer (2016)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3234–3243 (2016)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Schmidhuber, J.: Deep learning in neural networks: An overview. Neural networks
<span id="bib.bib21.1.1" class="ltx_text ltx_font_bold">61</span>, 85–117 (2015)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Shah, S., Dey, D., Lovett, C., Kapoor, A.: Aerial informatics and robotics
platform. Washigton: Microsoft Research (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the
inception architecture for computer vision. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 2818–2826 (2016)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Walach, E., Wolf, L.: Learning to count with cnn boosting. In: European
conference on computer vision. pp. 660–676. Springer (2016)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Wu, Y., Wu, Y., Gkioxari, G., Tian, Y.: Building generalizable agents with a
realistic and rich 3d environment. arXiv preprint arXiv:1801.02209 (2018)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image
translation using cycle-consistent adversarial networks. In: Proceedings of
the IEEE international conference on computer vision. pp. 2223–2232 (2017)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1908.06471" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1908.06472" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1908.06472">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1908.06472" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1908.06473" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 17:16:27 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
