<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.14154] Where is the Testbed for my Federated Learning Research?</title><meta property="og:description" content="Progressing beyond centralized AI is of paramount importance, yet, distributed AI solutions, in particular various federated learning (FL) algorithms, are often not comprehensively assessed, which prevents the research…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Where is the Testbed for my Federated Learning Research?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Where is the Testbed for my Federated Learning Research?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.14154">

<!--Generated on Mon Aug  5 17:47:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Where is the Testbed for my
<br class="ltx_break">Federated Learning Research?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Janez Božič21 <a target="_blank" href="https://orcid.org/0009-0003-0115-5901" title="" class="ltx_ref ltx_href"><span id="id6.6.6.id1" class="ltx_ERROR undefined">\scalerel</span>*
 <svg id="id1.1.1.1.pic1" class="ltx_picture" height="354.23" overflow="visible" version="1.1" width="354.23"><g transform="translate(0,354.23) matrix(1 0 0 -1 0 0) translate(0,354.23)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke="#A6CE39" fill="#A6CE39" color="#A6CE39"><path d="M 354.23 -177.11 C 354.23 -274.94 274.94 -354.23 177.11 -354.23 C 79.29 -354.23 0 -274.94 0 -177.11 C 0 -79.29 79.29 0 177.11 0 C 274.94 0 354.23 -79.29 354.23 -177.11 Z" style="stroke:none"></path></g><g stroke="#FFFFFF" fill="#FFFFFF" color="#FFFFFF"><path d="M 119.41 -257.64 L 98.1 -257.64 L 98.1 -109.45 L 119.41 -109.45 L 119.41 -176.42 L 119.41 -257.64 Z M 150.68 -109.45 L 208.25 -109.45 C 263.04 -109.45 287.12 -148.61 287.12 -183.62 C 287.12 -221.67 257.37 -257.78 208.52 -257.78 L 150.68 -257.78 L 150.68 -109.45 Z M 171.99 -238.55 L 205.89 -238.55 C 254.19 -238.55 265.26 -201.88 265.26 -183.62 C 265.26 -153.87 246.3 -128.68 204.79 -128.68 L 171.99 -128.68 L 171.99 -238.55 Z M 122.73 -78.59 C 122.73 -86.2 116.51 -92.57 108.76 -92.57 C 101.01 -92.57 94.78 -86.2 94.78 -78.59 C 94.78 -70.85 101.01 -64.62 108.76 -64.62 C 116.51 -64.62 122.73 -70.98 122.73 -78.59 Z" style="stroke:none"></path></g></g></svg>
—</a>,
Amândio R. Faustino31 <a target="_blank" href="https://orcid.org/0009-0009-6877-1999" title="" class="ltx_ref ltx_href"><span id="id7.7.7.id1" class="ltx_ERROR undefined">\scalerel</span>*
 <svg id="id2.2.2.1.pic1" class="ltx_picture" height="354.23" overflow="visible" version="1.1" width="354.23"><g transform="translate(0,354.23) matrix(1 0 0 -1 0 0) translate(0,354.23)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke="#A6CE39" fill="#A6CE39" color="#A6CE39"><path d="M 354.23 -177.11 C 354.23 -274.94 274.94 -354.23 177.11 -354.23 C 79.29 -354.23 0 -274.94 0 -177.11 C 0 -79.29 79.29 0 177.11 0 C 274.94 0 354.23 -79.29 354.23 -177.11 Z" style="stroke:none"></path></g><g stroke="#FFFFFF" fill="#FFFFFF" color="#FFFFFF"><path d="M 119.41 -257.64 L 98.1 -257.64 L 98.1 -109.45 L 119.41 -109.45 L 119.41 -176.42 L 119.41 -257.64 Z M 150.68 -109.45 L 208.25 -109.45 C 263.04 -109.45 287.12 -148.61 287.12 -183.62 C 287.12 -221.67 257.37 -257.78 208.52 -257.78 L 150.68 -257.78 L 150.68 -109.45 Z M 171.99 -238.55 L 205.89 -238.55 C 254.19 -238.55 265.26 -201.88 265.26 -183.62 C 265.26 -153.87 246.3 -128.68 204.79 -128.68 L 171.99 -128.68 L 171.99 -238.55 Z M 122.73 -78.59 C 122.73 -86.2 116.51 -92.57 108.76 -92.57 C 101.01 -92.57 94.78 -86.2 94.78 -78.59 C 94.78 -70.85 101.01 -64.62 108.76 -64.62 C 116.51 -64.62 122.73 -70.98 122.73 -78.59 Z" style="stroke:none"></path></g></g></svg>
—</a>,
Boris Radovič231 <a target="_blank" href="https://orcid.org/0009-0008-4142-931X" title="" class="ltx_ref ltx_href"><span id="id8.8.8.id1" class="ltx_ERROR undefined">\scalerel</span>*
 <svg id="id3.3.3.1.pic1" class="ltx_picture" height="354.23" overflow="visible" version="1.1" width="354.23"><g transform="translate(0,354.23) matrix(1 0 0 -1 0 0) translate(0,354.23)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke="#A6CE39" fill="#A6CE39" color="#A6CE39"><path d="M 354.23 -177.11 C 354.23 -274.94 274.94 -354.23 177.11 -354.23 C 79.29 -354.23 0 -274.94 0 -177.11 C 0 -79.29 79.29 0 177.11 0 C 274.94 0 354.23 -79.29 354.23 -177.11 Z" style="stroke:none"></path></g><g stroke="#FFFFFF" fill="#FFFFFF" color="#FFFFFF"><path d="M 119.41 -257.64 L 98.1 -257.64 L 98.1 -109.45 L 119.41 -109.45 L 119.41 -176.42 L 119.41 -257.64 Z M 150.68 -109.45 L 208.25 -109.45 C 263.04 -109.45 287.12 -148.61 287.12 -183.62 C 287.12 -221.67 257.37 -257.78 208.52 -257.78 L 150.68 -257.78 L 150.68 -109.45 Z M 171.99 -238.55 L 205.89 -238.55 C 254.19 -238.55 265.26 -201.88 265.26 -183.62 C 265.26 -153.87 246.3 -128.68 204.79 -128.68 L 171.99 -128.68 L 171.99 -238.55 Z M 122.73 -78.59 C 122.73 -86.2 116.51 -92.57 108.76 -92.57 C 101.01 -92.57 94.78 -86.2 94.78 -78.59 C 94.78 -70.85 101.01 -64.62 108.76 -64.62 C 116.51 -64.62 122.73 -70.98 122.73 -78.59 Z" style="stroke:none"></path></g></g></svg>
—</a>,
Marco Canini3 <a target="_blank" href="https://orcid.org/0000-0002-5051-4283" title="" class="ltx_ref ltx_href"><span id="id9.9.9.id1" class="ltx_ERROR undefined">\scalerel</span>*
 <svg id="id4.4.4.1.pic1" class="ltx_picture" height="354.23" overflow="visible" version="1.1" width="354.23"><g transform="translate(0,354.23) matrix(1 0 0 -1 0 0) translate(0,354.23)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke="#A6CE39" fill="#A6CE39" color="#A6CE39"><path d="M 354.23 -177.11 C 354.23 -274.94 274.94 -354.23 177.11 -354.23 C 79.29 -354.23 0 -274.94 0 -177.11 C 0 -79.29 79.29 0 177.11 0 C 274.94 0 354.23 -79.29 354.23 -177.11 Z" style="stroke:none"></path></g><g stroke="#FFFFFF" fill="#FFFFFF" color="#FFFFFF"><path d="M 119.41 -257.64 L 98.1 -257.64 L 98.1 -109.45 L 119.41 -109.45 L 119.41 -176.42 L 119.41 -257.64 Z M 150.68 -109.45 L 208.25 -109.45 C 263.04 -109.45 287.12 -148.61 287.12 -183.62 C 287.12 -221.67 257.37 -257.78 208.52 -257.78 L 150.68 -257.78 L 150.68 -109.45 Z M 171.99 -238.55 L 205.89 -238.55 C 254.19 -238.55 265.26 -201.88 265.26 -183.62 C 265.26 -153.87 246.3 -128.68 204.79 -128.68 L 171.99 -128.68 L 171.99 -238.55 Z M 122.73 -78.59 C 122.73 -86.2 116.51 -92.57 108.76 -92.57 C 101.01 -92.57 94.78 -86.2 94.78 -78.59 C 94.78 -70.85 101.01 -64.62 108.76 -64.62 C 116.51 -64.62 122.73 -70.98 122.73 -78.59 Z" style="stroke:none"></path></g></g></svg>
—</a>,
Veljko Pejović2 <a target="_blank" href="https://orcid.org/0000-0002-9009-0024" title="" class="ltx_ref ltx_href"><span id="id10.10.10.id1" class="ltx_ERROR undefined">\scalerel</span>*
 <svg id="id5.5.5.1.pic1" class="ltx_picture" height="354.23" overflow="visible" version="1.1" width="354.23"><g transform="translate(0,354.23) matrix(1 0 0 -1 0 0) translate(0,354.23)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g stroke="#A6CE39" fill="#A6CE39" color="#A6CE39"><path d="M 354.23 -177.11 C 354.23 -274.94 274.94 -354.23 177.11 -354.23 C 79.29 -354.23 0 -274.94 0 -177.11 C 0 -79.29 79.29 0 177.11 0 C 274.94 0 354.23 -79.29 354.23 -177.11 Z" style="stroke:none"></path></g><g stroke="#FFFFFF" fill="#FFFFFF" color="#FFFFFF"><path d="M 119.41 -257.64 L 98.1 -257.64 L 98.1 -109.45 L 119.41 -109.45 L 119.41 -176.42 L 119.41 -257.64 Z M 150.68 -109.45 L 208.25 -109.45 C 263.04 -109.45 287.12 -148.61 287.12 -183.62 C 287.12 -221.67 257.37 -257.78 208.52 -257.78 L 150.68 -257.78 L 150.68 -109.45 Z M 171.99 -238.55 L 205.89 -238.55 C 254.19 -238.55 265.26 -201.88 265.26 -183.62 C 265.26 -153.87 246.3 -128.68 204.79 -128.68 L 171.99 -128.68 L 171.99 -238.55 Z M 122.73 -78.59 C 122.73 -86.2 116.51 -92.57 108.76 -92.57 C 101.01 -92.57 94.78 -86.2 94.78 -78.59 C 94.78 -70.85 101.01 -64.62 108.76 -64.62 C 116.51 -64.62 122.73 -70.98 122.73 -78.59 Z" style="stroke:none"></path></g></g></svg>
—</a>
</span><span class="ltx_author_notes">1Equal contribution. Work done in part while Janez Božič was interning at KAUST.
<span class="ltx_contact ltx_role_affiliation">2 University of Ljubljana
</span>
<span class="ltx_contact ltx_role_affiliation">3 KAUST
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Progressing beyond centralized AI is of paramount importance, yet, distributed AI solutions, in particular various federated learning (FL) algorithms, are often not comprehensively assessed, which prevents the research community from identifying the most promising approaches and practitioners from being convinced that a certain solution is deployment-ready. The largest hurdle towards FL algorithm evaluation is the difficulty of conducting real-world experiments over a variety of FL client devices and different platforms, with different datasets and data distribution, all while assessing various dimensions of algorithm performance, such as inference accuracy, energy consumption, and time to convergence, to name a few. In this paper, we present CoLExT, a real-world testbed for FL research. CoLExT is designed to streamline experimentation with custom FL algorithms in a rich testbed configuration space, with a large number of heterogeneous edge devices, ranging from single-board computers to smartphones, and provides real-time collection and visualization of a variety of metrics through automatic instrumentation. According to our evaluation, porting FL algorithms to CoLExT requires minimal involvement from the developer, and the instrumentation introduces minimal resource usage overhead. Furthermore, through an initial investigation involving popular FL algorithms running on CoLExT, we reveal previously unknown trade-offs, inefficiencies, and programming bugs.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Data is a most precious resource, and the one that, with the growth of privacy awareness, users tend to be less inclined to share with third parties. Centralized AI has made tremendous advances in the last couple of decades. Yet, virtually all of the publicly available data, such as the content of the World Wide Web, are already being used for large foundational models. The next breakthrough in AI will necessarily have to rely on high-quality private data residing on end-user devices.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Harnessing individuals’ data while maintaining privacy is a challenging feat, and various research approaches have been proposed to tackle this issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Federated learning (FL) allows distributed collaborative training of machine learning (ML) models over a group of participating devices, with a centralized server used merely for training orchestration, essentially aggregating clients’ model updates and sharing the newly-created model within the group <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Its conceptual simplicity makes FL the most popular solution for privacy-preserving distributed AI, especially when deep learning (DL) models are involved.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The original FL algorithm, FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> was shown to underperform when different assumptions, such as the uniformity of data distribution or computational capabilities over clients, are lifted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
A large body of theoretical work has followed, and various attempts to alleviate the issue have been made <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Nevertheless, these scientific contributions very rarely trickle down to practice, primarily because they do not provide readily usable implementations and fail to convince that the claimed improvements indeed translate to real-world deployments. Instead, most proposals remain at the level of a simulation running on a single server, and the real-world behavior in terms of the algorithm running time, computational/memory/energy demand, and performance under various real-world constraints, such as with data/network/device heterogeneity, remains unknown.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.14154/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.5.2" class="ltx_text" style="font-size:90%;">Max. validation accuracy and energy to accuracy (ETA) for three FL algorithms on the CIFAR-10 dataset.
In FedAvg and FedProx, all clients use either a <span id="S1.F1.5.2.1" class="ltx_text ltx_font_italic">Small</span> or a <span id="S1.F1.5.2.2" class="ltx_text ltx_font_italic">Large</span> model, while in HeteroFL, clients use one of the two depending on the computational power. The ETA axis values cannot be (reliably) assessed without real-world experimentation provided by CoLExT.</span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we aim to increase the credibility of FL research by providing CoLExT – a solution for reproducible FL experimentation over real-world devices while also enabling a gamut of relevant performance metrics to be collected. Our solution is tailored to support virtually any existing and future algorithm constructed on top of the currently most popular FL framework, Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, and enables a range of scenario-defining parameters, such as client heterogeneity, metric collection configurations, and others, to be set. We implement CoLExT in a federation of 28 single-board computers (SBCs) and 20 Android smartphones and demonstrate its utility for objectively assessing FL algorithms over different dimensions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">As an illustrative example, in <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, we depict the result of CoLExT experimentation over three popular FL algorithms (FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and HeteroFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>), with two different model sizes (small – with 35k and large – with 380k parameters), and a different ratio of clients participating in every training round. Traditional means of simulation-based assessment would “see” only the <math id="S1.p5.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S1.p5.1.m1.1a"><mi id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><ci id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">y</annotation></semantics></math>-axis in the figure, essentially comparing algorithms based on the highest accuracy achieved, and would identify FedProx over a 380k parameter model with full client participation as the most promising solution. CoLExT, on the other hand, uncovers other metrics that may be relevant, such as the memory, CPU, and energy usage, as well as the training duration, and juxtaposes them with the achieved accuracy. In <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, for instance, CoLExT reveals that the amount of energy needed for reaching a particular level of accuracy (i.e., energy-to-accuracy, ETA) differs drastically among points that achieve very similar accuracy. Thus, the previously identified FedProx configuration reaches the top accuracy while consuming almost 350 kJ (kilo Joules). At the same time, involving only 40% of the clients in each training round, the FedAvg algorithm incurs a 4 p.p. (percentage points) decrease in accuracy while consuming less than a third of the energy (i.e., 100 kJ).</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The above is just one of the examples of how CoLExT can uncover nuances related to the performance of FL algorithms in real-world environments and can do this with minimal involvement of the algorithm developer. More broadly, our work brings the following contributions to FL research:</p>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.4" class="ltx_p"><math id="S1.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p7.1.m1.1a"><mo id="S1.p7.1.m1.1.1" xref="S1.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.1.m1.1b"><ci id="S1.p7.1.m1.1.1.cmml" xref="S1.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S1.p7.4.1" class="ltx_text ltx_font_bold">We design and implement a framework for experimentation with FL algorithms that readily supports a wide range of existing and future FL solutions.</span> Experimenters need to make minor changes (three lines of code) to run their Flower-ready algorithms on CoLExT.
<br class="ltx_break"><math id="S1.p7.2.m2.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p7.2.m2.1a"><mo id="S1.p7.2.m2.1.1" xref="S1.p7.2.m2.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.2.m2.1b"><ci id="S1.p7.2.m2.1.1.cmml" xref="S1.p7.2.m2.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.2.m2.1c">\bullet</annotation></semantics></math> <span id="S1.p7.4.2" class="ltx_text ltx_font_bold">We instrument algorithms to collect a range of metrics.</span> We implement low-level acquisition of CPU/GPU utilization, memory consumption, training time, and network usage. We also expose real-world energy measurements through power meters deployed in our setup.
<br class="ltx_break"><math id="S1.p7.3.m3.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p7.3.m3.1a"><mo id="S1.p7.3.m3.1.1" xref="S1.p7.3.m3.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.3.m3.1b"><ci id="S1.p7.3.m3.1.1.cmml" xref="S1.p7.3.m3.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.3.m3.1c">\bullet</annotation></semantics></math> <span id="S1.p7.4.3" class="ltx_text ltx_font_bold">We develop an expressive interface for defining the experimentation scenario.</span> An experimenter can decide on the level of heterogeneity among devices, change the experiment’s training parameters, and configure metric collection settings.
<br class="ltx_break"><math id="S1.p7.4.m4.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S1.p7.4.m4.1a"><mo id="S1.p7.4.m4.1.1" xref="S1.p7.4.m4.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.p7.4.m4.1b"><ci id="S1.p7.4.m4.1.1.cmml" xref="S1.p7.4.m4.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p7.4.m4.1c">\bullet</annotation></semantics></math> <span id="S1.p7.4.4" class="ltx_text ltx_font_bold">We deploy CoLExT in a heterogeneous device setting and perform thorough experimentation with a number of popular FL algorithms.</span> Our CoLExT testbed includes both SBCs and Android devices. Through microbenchmarks, we confirm that CoLExT can support a range of algorithms with negligible impact on the device’s resources, while use-case experimentation demonstrates how CoLExT can be harnessed to uncover implementation inefficiencies and trade-off issues related to the real-world use of FL.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">CoLExT testbed will be open-sourced and made available to interested researchers. While this paper unveils only a few important and interesting revelations (such as the one described in <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>), we believe that CoLExT will help practitioners navigate the trade-offs that different FL algorithms avail and will also help researchers identify the most promising directions for future development of FL algorithms.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background and Obstacles to Realistic FL Experimentation</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">FL Primer and Algorithm Variations</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.12" class="ltx_p">FL is arguably the most promising means of training AI models in a distributed manner so that the data of individual training participants (clients) remain private. In its simplest form, FL operates in rounds <math id="S2.SS1.p1.1.m1.4" class="ltx_Math" alttext="t=0,1,\ldots,T-1" display="inline"><semantics id="S2.SS1.p1.1.m1.4a"><mrow id="S2.SS1.p1.1.m1.4.4" xref="S2.SS1.p1.1.m1.4.4.cmml"><mi id="S2.SS1.p1.1.m1.4.4.3" xref="S2.SS1.p1.1.m1.4.4.3.cmml">t</mi><mo id="S2.SS1.p1.1.m1.4.4.2" xref="S2.SS1.p1.1.m1.4.4.2.cmml">=</mo><mrow id="S2.SS1.p1.1.m1.4.4.1.1" xref="S2.SS1.p1.1.m1.4.4.1.2.cmml"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">0</mn><mo id="S2.SS1.p1.1.m1.4.4.1.1.2" xref="S2.SS1.p1.1.m1.4.4.1.2.cmml">,</mo><mn id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">1</mn><mo id="S2.SS1.p1.1.m1.4.4.1.1.3" xref="S2.SS1.p1.1.m1.4.4.1.2.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml">…</mi><mo id="S2.SS1.p1.1.m1.4.4.1.1.4" xref="S2.SS1.p1.1.m1.4.4.1.2.cmml">,</mo><mrow id="S2.SS1.p1.1.m1.4.4.1.1.1" xref="S2.SS1.p1.1.m1.4.4.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.4.4.1.1.1.2" xref="S2.SS1.p1.1.m1.4.4.1.1.1.2.cmml">T</mi><mo id="S2.SS1.p1.1.m1.4.4.1.1.1.1" xref="S2.SS1.p1.1.m1.4.4.1.1.1.1.cmml">−</mo><mn id="S2.SS1.p1.1.m1.4.4.1.1.1.3" xref="S2.SS1.p1.1.m1.4.4.1.1.1.3.cmml">1</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.4b"><apply id="S2.SS1.p1.1.m1.4.4.cmml" xref="S2.SS1.p1.1.m1.4.4"><eq id="S2.SS1.p1.1.m1.4.4.2.cmml" xref="S2.SS1.p1.1.m1.4.4.2"></eq><ci id="S2.SS1.p1.1.m1.4.4.3.cmml" xref="S2.SS1.p1.1.m1.4.4.3">𝑡</ci><list id="S2.SS1.p1.1.m1.4.4.1.2.cmml" xref="S2.SS1.p1.1.m1.4.4.1.1"><cn type="integer" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">0</cn><cn type="integer" id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">1</cn><ci id="S2.SS1.p1.1.m1.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3">…</ci><apply id="S2.SS1.p1.1.m1.4.4.1.1.1.cmml" xref="S2.SS1.p1.1.m1.4.4.1.1.1"><minus id="S2.SS1.p1.1.m1.4.4.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.4.4.1.1.1.1"></minus><ci id="S2.SS1.p1.1.m1.4.4.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.4.4.1.1.1.2">𝑇</ci><cn type="integer" id="S2.SS1.p1.1.m1.4.4.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.4.4.1.1.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.4c">t=0,1,\ldots,T-1</annotation></semantics></math>, and lets each client <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">k</annotation></semantics></math> from a set of clients <math id="S2.SS1.p1.3.m3.4" class="ltx_Math" alttext="\mathcal{K}=\{1,2,\ldots,K\}" display="inline"><semantics id="S2.SS1.p1.3.m3.4a"><mrow id="S2.SS1.p1.3.m3.4.5" xref="S2.SS1.p1.3.m3.4.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.3.m3.4.5.2" xref="S2.SS1.p1.3.m3.4.5.2.cmml">𝒦</mi><mo id="S2.SS1.p1.3.m3.4.5.1" xref="S2.SS1.p1.3.m3.4.5.1.cmml">=</mo><mrow id="S2.SS1.p1.3.m3.4.5.3.2" xref="S2.SS1.p1.3.m3.4.5.3.1.cmml"><mo stretchy="false" id="S2.SS1.p1.3.m3.4.5.3.2.1" xref="S2.SS1.p1.3.m3.4.5.3.1.cmml">{</mo><mn id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">1</mn><mo id="S2.SS1.p1.3.m3.4.5.3.2.2" xref="S2.SS1.p1.3.m3.4.5.3.1.cmml">,</mo><mn id="S2.SS1.p1.3.m3.2.2" xref="S2.SS1.p1.3.m3.2.2.cmml">2</mn><mo id="S2.SS1.p1.3.m3.4.5.3.2.3" xref="S2.SS1.p1.3.m3.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p1.3.m3.3.3" xref="S2.SS1.p1.3.m3.3.3.cmml">…</mi><mo id="S2.SS1.p1.3.m3.4.5.3.2.4" xref="S2.SS1.p1.3.m3.4.5.3.1.cmml">,</mo><mi id="S2.SS1.p1.3.m3.4.4" xref="S2.SS1.p1.3.m3.4.4.cmml">K</mi><mo stretchy="false" id="S2.SS1.p1.3.m3.4.5.3.2.5" xref="S2.SS1.p1.3.m3.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.4b"><apply id="S2.SS1.p1.3.m3.4.5.cmml" xref="S2.SS1.p1.3.m3.4.5"><eq id="S2.SS1.p1.3.m3.4.5.1.cmml" xref="S2.SS1.p1.3.m3.4.5.1"></eq><ci id="S2.SS1.p1.3.m3.4.5.2.cmml" xref="S2.SS1.p1.3.m3.4.5.2">𝒦</ci><set id="S2.SS1.p1.3.m3.4.5.3.1.cmml" xref="S2.SS1.p1.3.m3.4.5.3.2"><cn type="integer" id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">1</cn><cn type="integer" id="S2.SS1.p1.3.m3.2.2.cmml" xref="S2.SS1.p1.3.m3.2.2">2</cn><ci id="S2.SS1.p1.3.m3.3.3.cmml" xref="S2.SS1.p1.3.m3.3.3">…</ci><ci id="S2.SS1.p1.3.m3.4.4.cmml" xref="S2.SS1.p1.3.m3.4.4">𝐾</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.4c">\mathcal{K}=\{1,2,\ldots,K\}</annotation></semantics></math> independently train a model <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{w}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">𝐰</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝐰</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathbf{w}</annotation></semantics></math> over its local dataset <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">𝒟</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝒟</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">\mathcal{D}_{k}</annotation></semantics></math> using stochastic gradient descent (SGD) or a variation thereof. After <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mi id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><ci id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">E</annotation></semantics></math> local training epochs, the updated versions of the model <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{w}_{k}" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><msub id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">𝐰</mi><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">𝐰</ci><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">\mathbf{w}_{k}</annotation></semantics></math> are sent to the server, which then aggregates them in a new version of the global model, usually weighing the contribution of each client according to the number of data samples <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">n</mi><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">𝑛</ci><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">n_{k}</annotation></semantics></math> in <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathcal{D}_{k}" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><msub id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.9.m9.1.1.2" xref="S2.SS1.p1.9.m9.1.1.2.cmml">𝒟</mi><mi id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2">𝒟</ci><ci id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">\mathcal{D}_{k}</annotation></semantics></math> : <math id="S2.SS1.p1.10.m10.6" class="ltx_Math" alttext="\mathbf{w}^{(t+1)}=\frac{1}{\sum_{k\in\mathcal{K}^{(t)}}n_{k}}\sum_{k\in\mathcal{K}^{(t)}}n_{k}\mathbf{w}_{k}^{(t,E)}." display="inline"><semantics id="S2.SS1.p1.10.m10.6a"><mrow id="S2.SS1.p1.10.m10.6.6.1" xref="S2.SS1.p1.10.m10.6.6.1.1.cmml"><mrow id="S2.SS1.p1.10.m10.6.6.1.1" xref="S2.SS1.p1.10.m10.6.6.1.1.cmml"><msup id="S2.SS1.p1.10.m10.6.6.1.1.2" xref="S2.SS1.p1.10.m10.6.6.1.1.2.cmml"><mi id="S2.SS1.p1.10.m10.6.6.1.1.2.2" xref="S2.SS1.p1.10.m10.6.6.1.1.2.2.cmml">𝐰</mi><mrow id="S2.SS1.p1.10.m10.1.1.1.1" xref="S2.SS1.p1.10.m10.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m10.1.1.1.1.2" xref="S2.SS1.p1.10.m10.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.10.m10.1.1.1.1.1" xref="S2.SS1.p1.10.m10.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.10.m10.1.1.1.1.1.2" xref="S2.SS1.p1.10.m10.1.1.1.1.1.2.cmml">t</mi><mo id="S2.SS1.p1.10.m10.1.1.1.1.1.1" xref="S2.SS1.p1.10.m10.1.1.1.1.1.1.cmml">+</mo><mn id="S2.SS1.p1.10.m10.1.1.1.1.1.3" xref="S2.SS1.p1.10.m10.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S2.SS1.p1.10.m10.1.1.1.1.3" xref="S2.SS1.p1.10.m10.1.1.1.1.1.cmml">)</mo></mrow></msup><mo id="S2.SS1.p1.10.m10.6.6.1.1.1" xref="S2.SS1.p1.10.m10.6.6.1.1.1.cmml">=</mo><mrow id="S2.SS1.p1.10.m10.6.6.1.1.3" xref="S2.SS1.p1.10.m10.6.6.1.1.3.cmml"><mfrac id="S2.SS1.p1.10.m10.2.2" xref="S2.SS1.p1.10.m10.2.2.cmml"><mn id="S2.SS1.p1.10.m10.2.2.3" xref="S2.SS1.p1.10.m10.2.2.3.cmml">1</mn><mrow id="S2.SS1.p1.10.m10.2.2.1" xref="S2.SS1.p1.10.m10.2.2.1.cmml"><mstyle displaystyle="false" id="S2.SS1.p1.10.m10.2.2.1.2" xref="S2.SS1.p1.10.m10.2.2.1.2.cmml"><msub id="S2.SS1.p1.10.m10.2.2.1.2a" xref="S2.SS1.p1.10.m10.2.2.1.2.cmml"><mo id="S2.SS1.p1.10.m10.2.2.1.2.2" xref="S2.SS1.p1.10.m10.2.2.1.2.2.cmml">∑</mo><mrow id="S2.SS1.p1.10.m10.2.2.1.1.1" xref="S2.SS1.p1.10.m10.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.10.m10.2.2.1.1.1.3" xref="S2.SS1.p1.10.m10.2.2.1.1.1.3.cmml">k</mi><mo id="S2.SS1.p1.10.m10.2.2.1.1.1.2" xref="S2.SS1.p1.10.m10.2.2.1.1.1.2.cmml">∈</mo><msup id="S2.SS1.p1.10.m10.2.2.1.1.1.4" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.10.m10.2.2.1.1.1.4.2" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4.2.cmml">𝒦</mi><mrow id="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.3" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.3.1" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4.cmml">(</mo><mi id="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.1" xref="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.3.2" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4.cmml">)</mo></mrow></msup></mrow></msub></mstyle><msub id="S2.SS1.p1.10.m10.2.2.1.3" xref="S2.SS1.p1.10.m10.2.2.1.3.cmml"><mi id="S2.SS1.p1.10.m10.2.2.1.3.2" xref="S2.SS1.p1.10.m10.2.2.1.3.2.cmml">n</mi><mi id="S2.SS1.p1.10.m10.2.2.1.3.3" xref="S2.SS1.p1.10.m10.2.2.1.3.3.cmml">k</mi></msub></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.SS1.p1.10.m10.6.6.1.1.3.1" xref="S2.SS1.p1.10.m10.6.6.1.1.3.1.cmml">​</mo><mrow id="S2.SS1.p1.10.m10.6.6.1.1.3.2" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.cmml"><msub id="S2.SS1.p1.10.m10.6.6.1.1.3.2.1" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.cmml"><mo id="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.2" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.2.cmml">∑</mo><mrow id="S2.SS1.p1.10.m10.3.3.1" xref="S2.SS1.p1.10.m10.3.3.1.cmml"><mi id="S2.SS1.p1.10.m10.3.3.1.3" xref="S2.SS1.p1.10.m10.3.3.1.3.cmml">k</mi><mo id="S2.SS1.p1.10.m10.3.3.1.2" xref="S2.SS1.p1.10.m10.3.3.1.2.cmml">∈</mo><msup id="S2.SS1.p1.10.m10.3.3.1.4" xref="S2.SS1.p1.10.m10.3.3.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.10.m10.3.3.1.4.2" xref="S2.SS1.p1.10.m10.3.3.1.4.2.cmml">𝒦</mi><mrow id="S2.SS1.p1.10.m10.3.3.1.1.1.3" xref="S2.SS1.p1.10.m10.3.3.1.4.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m10.3.3.1.1.1.3.1" xref="S2.SS1.p1.10.m10.3.3.1.4.cmml">(</mo><mi id="S2.SS1.p1.10.m10.3.3.1.1.1.1" xref="S2.SS1.p1.10.m10.3.3.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p1.10.m10.3.3.1.1.1.3.2" xref="S2.SS1.p1.10.m10.3.3.1.4.cmml">)</mo></mrow></msup></mrow></msub><mrow id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.cmml"><msub id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.cmml"><mi id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.2" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.2.cmml">n</mi><mi id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.3" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.1" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.1.cmml">​</mo><msubsup id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.cmml"><mi id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.2" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.2.cmml">𝐰</mi><mi id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.3" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.3.cmml">k</mi><mrow id="S2.SS1.p1.10.m10.5.5.2.4" xref="S2.SS1.p1.10.m10.5.5.2.3.cmml"><mo stretchy="false" id="S2.SS1.p1.10.m10.5.5.2.4.1" xref="S2.SS1.p1.10.m10.5.5.2.3.cmml">(</mo><mi id="S2.SS1.p1.10.m10.4.4.1.1" xref="S2.SS1.p1.10.m10.4.4.1.1.cmml">t</mi><mo id="S2.SS1.p1.10.m10.5.5.2.4.2" xref="S2.SS1.p1.10.m10.5.5.2.3.cmml">,</mo><mi id="S2.SS1.p1.10.m10.5.5.2.2" xref="S2.SS1.p1.10.m10.5.5.2.2.cmml">E</mi><mo stretchy="false" id="S2.SS1.p1.10.m10.5.5.2.4.3" xref="S2.SS1.p1.10.m10.5.5.2.3.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow></mrow><mo lspace="0em" id="S2.SS1.p1.10.m10.6.6.1.2" xref="S2.SS1.p1.10.m10.6.6.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.6b"><apply id="S2.SS1.p1.10.m10.6.6.1.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1"><eq id="S2.SS1.p1.10.m10.6.6.1.1.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.1"></eq><apply id="S2.SS1.p1.10.m10.6.6.1.1.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.6.6.1.1.2.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.2">superscript</csymbol><ci id="S2.SS1.p1.10.m10.6.6.1.1.2.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.2.2">𝐰</ci><apply id="S2.SS1.p1.10.m10.1.1.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1.1.1"><plus id="S2.SS1.p1.10.m10.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1.1.1.1.1"></plus><ci id="S2.SS1.p1.10.m10.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.1.1.1.2">𝑡</ci><cn type="integer" id="S2.SS1.p1.10.m10.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3"><times id="S2.SS1.p1.10.m10.6.6.1.1.3.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.1"></times><apply id="S2.SS1.p1.10.m10.2.2.cmml" xref="S2.SS1.p1.10.m10.2.2"><divide id="S2.SS1.p1.10.m10.2.2.2.cmml" xref="S2.SS1.p1.10.m10.2.2"></divide><cn type="integer" id="S2.SS1.p1.10.m10.2.2.3.cmml" xref="S2.SS1.p1.10.m10.2.2.3">1</cn><apply id="S2.SS1.p1.10.m10.2.2.1.cmml" xref="S2.SS1.p1.10.m10.2.2.1"><apply id="S2.SS1.p1.10.m10.2.2.1.2.cmml" xref="S2.SS1.p1.10.m10.2.2.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.2.2.1.2.1.cmml" xref="S2.SS1.p1.10.m10.2.2.1.2">subscript</csymbol><sum id="S2.SS1.p1.10.m10.2.2.1.2.2.cmml" xref="S2.SS1.p1.10.m10.2.2.1.2.2"></sum><apply id="S2.SS1.p1.10.m10.2.2.1.1.1.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1"><in id="S2.SS1.p1.10.m10.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1.2"></in><ci id="S2.SS1.p1.10.m10.2.2.1.1.1.3.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1.3">𝑘</ci><apply id="S2.SS1.p1.10.m10.2.2.1.1.1.4.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.2.2.1.1.1.4.1.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4">superscript</csymbol><ci id="S2.SS1.p1.10.m10.2.2.1.1.1.4.2.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1.4.2">𝒦</ci><ci id="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.10.m10.2.2.1.1.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="S2.SS1.p1.10.m10.2.2.1.3.cmml" xref="S2.SS1.p1.10.m10.2.2.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.2.2.1.3.1.cmml" xref="S2.SS1.p1.10.m10.2.2.1.3">subscript</csymbol><ci id="S2.SS1.p1.10.m10.2.2.1.3.2.cmml" xref="S2.SS1.p1.10.m10.2.2.1.3.2">𝑛</ci><ci id="S2.SS1.p1.10.m10.2.2.1.3.3.cmml" xref="S2.SS1.p1.10.m10.2.2.1.3.3">𝑘</ci></apply></apply></apply><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2"><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.1">subscript</csymbol><sum id="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.1.2"></sum><apply id="S2.SS1.p1.10.m10.3.3.1.cmml" xref="S2.SS1.p1.10.m10.3.3.1"><in id="S2.SS1.p1.10.m10.3.3.1.2.cmml" xref="S2.SS1.p1.10.m10.3.3.1.2"></in><ci id="S2.SS1.p1.10.m10.3.3.1.3.cmml" xref="S2.SS1.p1.10.m10.3.3.1.3">𝑘</ci><apply id="S2.SS1.p1.10.m10.3.3.1.4.cmml" xref="S2.SS1.p1.10.m10.3.3.1.4"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.3.3.1.4.1.cmml" xref="S2.SS1.p1.10.m10.3.3.1.4">superscript</csymbol><ci id="S2.SS1.p1.10.m10.3.3.1.4.2.cmml" xref="S2.SS1.p1.10.m10.3.3.1.4.2">𝒦</ci><ci id="S2.SS1.p1.10.m10.3.3.1.1.1.1.cmml" xref="S2.SS1.p1.10.m10.3.3.1.1.1.1">𝑡</ci></apply></apply></apply><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2"><times id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.1"></times><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.2">𝑛</ci><ci id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.3.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.2.3">𝑘</ci></apply><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3">superscript</csymbol><apply id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.1.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3">subscript</csymbol><ci id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.2.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.2">𝐰</ci><ci id="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.3.cmml" xref="S2.SS1.p1.10.m10.6.6.1.1.3.2.2.3.2.3">𝑘</ci></apply><interval closure="open" id="S2.SS1.p1.10.m10.5.5.2.3.cmml" xref="S2.SS1.p1.10.m10.5.5.2.4"><ci id="S2.SS1.p1.10.m10.4.4.1.1.cmml" xref="S2.SS1.p1.10.m10.4.4.1.1">𝑡</ci><ci id="S2.SS1.p1.10.m10.5.5.2.2.cmml" xref="S2.SS1.p1.10.m10.5.5.2.2">𝐸</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.6c">\mathbf{w}^{(t+1)}=\frac{1}{\sum_{k\in\mathcal{K}^{(t)}}n_{k}}\sum_{k\in\mathcal{K}^{(t)}}n_{k}\mathbf{w}_{k}^{(t,E)}.</annotation></semantics></math> After <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.p1.11.m11.1a"><mi id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><ci id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">T</annotation></semantics></math> rounds, the global model <math id="S2.SS1.p1.12.m12.1" class="ltx_Math" alttext="\mathbf{w}^{(T)}" display="inline"><semantics id="S2.SS1.p1.12.m12.1a"><msup id="S2.SS1.p1.12.m12.1.2" xref="S2.SS1.p1.12.m12.1.2.cmml"><mi id="S2.SS1.p1.12.m12.1.2.2" xref="S2.SS1.p1.12.m12.1.2.2.cmml">𝐰</mi><mrow id="S2.SS1.p1.12.m12.1.1.1.3" xref="S2.SS1.p1.12.m12.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.12.m12.1.1.1.3.1" xref="S2.SS1.p1.12.m12.1.2.cmml">(</mo><mi id="S2.SS1.p1.12.m12.1.1.1.1" xref="S2.SS1.p1.12.m12.1.1.1.1.cmml">T</mi><mo stretchy="false" id="S2.SS1.p1.12.m12.1.1.1.3.2" xref="S2.SS1.p1.12.m12.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.12.m12.1b"><apply id="S2.SS1.p1.12.m12.1.2.cmml" xref="S2.SS1.p1.12.m12.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.12.m12.1.2.1.cmml" xref="S2.SS1.p1.12.m12.1.2">superscript</csymbol><ci id="S2.SS1.p1.12.m12.1.2.2.cmml" xref="S2.SS1.p1.12.m12.1.2.2">𝐰</ci><ci id="S2.SS1.p1.12.m12.1.1.1.1.cmml" xref="S2.SS1.p1.12.m12.1.1.1.1">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.12.m12.1c">\mathbf{w}^{(T)}</annotation></semantics></math> is considered trained.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.4" class="ltx_p">The above algorithm, which is essentially a form of distributed SGD with local steps, is termed FedAvg and represents the de-facto baseline from which numerous other FL algorithms have been developed and have been pitted against. For instance, FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> extends the FedAvg local training loss function <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="F_{k}(\mathbf{w})" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.2" xref="S2.SS1.p2.1.m1.1.2.cmml"><msub id="S2.SS1.p2.1.m1.1.2.2" xref="S2.SS1.p2.1.m1.1.2.2.cmml"><mi id="S2.SS1.p2.1.m1.1.2.2.2" xref="S2.SS1.p2.1.m1.1.2.2.2.cmml">F</mi><mi id="S2.SS1.p2.1.m1.1.2.2.3" xref="S2.SS1.p2.1.m1.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p2.1.m1.1.2.1" xref="S2.SS1.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S2.SS1.p2.1.m1.1.2.3.2" xref="S2.SS1.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.1.2.3.2.1" xref="S2.SS1.p2.1.m1.1.2.cmml">(</mo><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">𝐰</mi><mo stretchy="false" id="S2.SS1.p2.1.m1.1.2.3.2.2" xref="S2.SS1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.2"><times id="S2.SS1.p2.1.m1.1.2.1.cmml" xref="S2.SS1.p2.1.m1.1.2.1"></times><apply id="S2.SS1.p2.1.m1.1.2.2.cmml" xref="S2.SS1.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.2.2.1.cmml" xref="S2.SS1.p2.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.2.2.2.cmml" xref="S2.SS1.p2.1.m1.1.2.2.2">𝐹</ci><ci id="S2.SS1.p2.1.m1.1.2.2.3.cmml" xref="S2.SS1.p2.1.m1.1.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝐰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">F_{k}(\mathbf{w})</annotation></semantics></math> with a proximal term <math id="S2.SS1.p2.2.m2.2" class="ltx_Math" alttext="\frac{\mu}{2}\|\mathbf{w}-\mathbf{w}^{(t)}\|^{2}" display="inline"><semantics id="S2.SS1.p2.2.m2.2a"><mrow id="S2.SS1.p2.2.m2.2.2" xref="S2.SS1.p2.2.m2.2.2.cmml"><mfrac id="S2.SS1.p2.2.m2.2.2.3" xref="S2.SS1.p2.2.m2.2.2.3.cmml"><mi id="S2.SS1.p2.2.m2.2.2.3.2" xref="S2.SS1.p2.2.m2.2.2.3.2.cmml">μ</mi><mn id="S2.SS1.p2.2.m2.2.2.3.3" xref="S2.SS1.p2.2.m2.2.2.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.2.2.2" xref="S2.SS1.p2.2.m2.2.2.2.cmml">​</mo><msup id="S2.SS1.p2.2.m2.2.2.1" xref="S2.SS1.p2.2.m2.2.2.1.cmml"><mrow id="S2.SS1.p2.2.m2.2.2.1.1.1" xref="S2.SS1.p2.2.m2.2.2.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.2.2.1.1.1.2" xref="S2.SS1.p2.2.m2.2.2.1.1.2.1.cmml">‖</mo><mrow id="S2.SS1.p2.2.m2.2.2.1.1.1.1" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.cmml"><mi id="S2.SS1.p2.2.m2.2.2.1.1.1.1.2" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.2.cmml">𝐰</mi><mo id="S2.SS1.p2.2.m2.2.2.1.1.1.1.1" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.1.cmml">−</mo><msup id="S2.SS1.p2.2.m2.2.2.1.1.1.1.3" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.cmml"><mi id="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.2" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.2.cmml">𝐰</mi><mrow id="S2.SS1.p2.2.m2.1.1.1.3" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.1.3.1" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.cmml">(</mo><mi id="S2.SS1.p2.2.m2.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.1.3.2" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.cmml">)</mo></mrow></msup></mrow><mo stretchy="false" id="S2.SS1.p2.2.m2.2.2.1.1.1.3" xref="S2.SS1.p2.2.m2.2.2.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.SS1.p2.2.m2.2.2.1.3" xref="S2.SS1.p2.2.m2.2.2.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.2b"><apply id="S2.SS1.p2.2.m2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2"><times id="S2.SS1.p2.2.m2.2.2.2.cmml" xref="S2.SS1.p2.2.m2.2.2.2"></times><apply id="S2.SS1.p2.2.m2.2.2.3.cmml" xref="S2.SS1.p2.2.m2.2.2.3"><divide id="S2.SS1.p2.2.m2.2.2.3.1.cmml" xref="S2.SS1.p2.2.m2.2.2.3"></divide><ci id="S2.SS1.p2.2.m2.2.2.3.2.cmml" xref="S2.SS1.p2.2.m2.2.2.3.2">𝜇</ci><cn type="integer" id="S2.SS1.p2.2.m2.2.2.3.3.cmml" xref="S2.SS1.p2.2.m2.2.2.3.3">2</cn></apply><apply id="S2.SS1.p2.2.m2.2.2.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.2.2.1.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1">superscript</csymbol><apply id="S2.SS1.p2.2.m2.2.2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1"><csymbol cd="latexml" id="S2.SS1.p2.2.m2.2.2.1.1.2.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.2">norm</csymbol><apply id="S2.SS1.p2.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1"><minus id="S2.SS1.p2.2.m2.2.2.1.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.1"></minus><ci id="S2.SS1.p2.2.m2.2.2.1.1.1.1.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.2">𝐰</ci><apply id="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.1.1.3.2">𝐰</ci><ci id="S2.SS1.p2.2.m2.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1">𝑡</ci></apply></apply></apply><cn type="integer" id="S2.SS1.p2.2.m2.2.2.1.3.cmml" xref="S2.SS1.p2.2.m2.2.2.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.2c">\frac{\mu}{2}\|\mathbf{w}-\mathbf{w}^{(t)}\|^{2}</annotation></semantics></math>, where <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\mu</annotation></semantics></math> is a non-negative constant, that penalizes large deviations of the local model from the global model. i.e. the loss function becomes: <math id="S2.SS1.p2.4.m4.4" class="ltx_Math" alttext="F_{k}^{\text{prox}}(\mathbf{w})=F_{k}(\mathbf{w})+\frac{\mu}{2}\|\mathbf{w}-\mathbf{w}^{(t)}\|^{2}" display="inline"><semantics id="S2.SS1.p2.4.m4.4a"><mrow id="S2.SS1.p2.4.m4.4.4" xref="S2.SS1.p2.4.m4.4.4.cmml"><mrow id="S2.SS1.p2.4.m4.4.4.3" xref="S2.SS1.p2.4.m4.4.4.3.cmml"><msubsup id="S2.SS1.p2.4.m4.4.4.3.2" xref="S2.SS1.p2.4.m4.4.4.3.2.cmml"><mi id="S2.SS1.p2.4.m4.4.4.3.2.2.2" xref="S2.SS1.p2.4.m4.4.4.3.2.2.2.cmml">F</mi><mi id="S2.SS1.p2.4.m4.4.4.3.2.2.3" xref="S2.SS1.p2.4.m4.4.4.3.2.2.3.cmml">k</mi><mtext id="S2.SS1.p2.4.m4.4.4.3.2.3" xref="S2.SS1.p2.4.m4.4.4.3.2.3a.cmml">prox</mtext></msubsup><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.4.4.3.1" xref="S2.SS1.p2.4.m4.4.4.3.1.cmml">​</mo><mrow id="S2.SS1.p2.4.m4.4.4.3.3.2" xref="S2.SS1.p2.4.m4.4.4.3.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.4.4.3.3.2.1" xref="S2.SS1.p2.4.m4.4.4.3.cmml">(</mo><mi id="S2.SS1.p2.4.m4.2.2" xref="S2.SS1.p2.4.m4.2.2.cmml">𝐰</mi><mo stretchy="false" id="S2.SS1.p2.4.m4.4.4.3.3.2.2" xref="S2.SS1.p2.4.m4.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p2.4.m4.4.4.2" xref="S2.SS1.p2.4.m4.4.4.2.cmml">=</mo><mrow id="S2.SS1.p2.4.m4.4.4.1" xref="S2.SS1.p2.4.m4.4.4.1.cmml"><mrow id="S2.SS1.p2.4.m4.4.4.1.3" xref="S2.SS1.p2.4.m4.4.4.1.3.cmml"><msub id="S2.SS1.p2.4.m4.4.4.1.3.2" xref="S2.SS1.p2.4.m4.4.4.1.3.2.cmml"><mi id="S2.SS1.p2.4.m4.4.4.1.3.2.2" xref="S2.SS1.p2.4.m4.4.4.1.3.2.2.cmml">F</mi><mi id="S2.SS1.p2.4.m4.4.4.1.3.2.3" xref="S2.SS1.p2.4.m4.4.4.1.3.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.4.4.1.3.1" xref="S2.SS1.p2.4.m4.4.4.1.3.1.cmml">​</mo><mrow id="S2.SS1.p2.4.m4.4.4.1.3.3.2" xref="S2.SS1.p2.4.m4.4.4.1.3.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.4.4.1.3.3.2.1" xref="S2.SS1.p2.4.m4.4.4.1.3.cmml">(</mo><mi id="S2.SS1.p2.4.m4.3.3" xref="S2.SS1.p2.4.m4.3.3.cmml">𝐰</mi><mo stretchy="false" id="S2.SS1.p2.4.m4.4.4.1.3.3.2.2" xref="S2.SS1.p2.4.m4.4.4.1.3.cmml">)</mo></mrow></mrow><mo id="S2.SS1.p2.4.m4.4.4.1.2" xref="S2.SS1.p2.4.m4.4.4.1.2.cmml">+</mo><mrow id="S2.SS1.p2.4.m4.4.4.1.1" xref="S2.SS1.p2.4.m4.4.4.1.1.cmml"><mfrac id="S2.SS1.p2.4.m4.4.4.1.1.3" xref="S2.SS1.p2.4.m4.4.4.1.1.3.cmml"><mi id="S2.SS1.p2.4.m4.4.4.1.1.3.2" xref="S2.SS1.p2.4.m4.4.4.1.1.3.2.cmml">μ</mi><mn id="S2.SS1.p2.4.m4.4.4.1.1.3.3" xref="S2.SS1.p2.4.m4.4.4.1.1.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m4.4.4.1.1.2" xref="S2.SS1.p2.4.m4.4.4.1.1.2.cmml">​</mo><msup id="S2.SS1.p2.4.m4.4.4.1.1.1" xref="S2.SS1.p2.4.m4.4.4.1.1.1.cmml"><mrow id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.2" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.2.1.cmml">‖</mo><mrow id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.2" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.2.cmml">𝐰</mi><mo id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.1" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.1.cmml">−</mo><msup id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.cmml"><mi id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.2" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.2.cmml">𝐰</mi><mrow id="S2.SS1.p2.4.m4.1.1.1.3" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S2.SS1.p2.4.m4.1.1.1.3.1" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.cmml">(</mo><mi id="S2.SS1.p2.4.m4.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p2.4.m4.1.1.1.3.2" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.cmml">)</mo></mrow></msup></mrow><mo stretchy="false" id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.3" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S2.SS1.p2.4.m4.4.4.1.1.1.3" xref="S2.SS1.p2.4.m4.4.4.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.4b"><apply id="S2.SS1.p2.4.m4.4.4.cmml" xref="S2.SS1.p2.4.m4.4.4"><eq id="S2.SS1.p2.4.m4.4.4.2.cmml" xref="S2.SS1.p2.4.m4.4.4.2"></eq><apply id="S2.SS1.p2.4.m4.4.4.3.cmml" xref="S2.SS1.p2.4.m4.4.4.3"><times id="S2.SS1.p2.4.m4.4.4.3.1.cmml" xref="S2.SS1.p2.4.m4.4.4.3.1"></times><apply id="S2.SS1.p2.4.m4.4.4.3.2.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.4.4.3.2.1.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2">superscript</csymbol><apply id="S2.SS1.p2.4.m4.4.4.3.2.2.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.4.4.3.2.2.1.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2">subscript</csymbol><ci id="S2.SS1.p2.4.m4.4.4.3.2.2.2.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2.2.2">𝐹</ci><ci id="S2.SS1.p2.4.m4.4.4.3.2.2.3.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2.2.3">𝑘</ci></apply><ci id="S2.SS1.p2.4.m4.4.4.3.2.3a.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2.3"><mtext mathsize="70%" id="S2.SS1.p2.4.m4.4.4.3.2.3.cmml" xref="S2.SS1.p2.4.m4.4.4.3.2.3">prox</mtext></ci></apply><ci id="S2.SS1.p2.4.m4.2.2.cmml" xref="S2.SS1.p2.4.m4.2.2">𝐰</ci></apply><apply id="S2.SS1.p2.4.m4.4.4.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1"><plus id="S2.SS1.p2.4.m4.4.4.1.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.2"></plus><apply id="S2.SS1.p2.4.m4.4.4.1.3.cmml" xref="S2.SS1.p2.4.m4.4.4.1.3"><times id="S2.SS1.p2.4.m4.4.4.1.3.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.3.1"></times><apply id="S2.SS1.p2.4.m4.4.4.1.3.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.3.2"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.4.4.1.3.2.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.3.2">subscript</csymbol><ci id="S2.SS1.p2.4.m4.4.4.1.3.2.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.3.2.2">𝐹</ci><ci id="S2.SS1.p2.4.m4.4.4.1.3.2.3.cmml" xref="S2.SS1.p2.4.m4.4.4.1.3.2.3">𝑘</ci></apply><ci id="S2.SS1.p2.4.m4.3.3.cmml" xref="S2.SS1.p2.4.m4.3.3">𝐰</ci></apply><apply id="S2.SS1.p2.4.m4.4.4.1.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1"><times id="S2.SS1.p2.4.m4.4.4.1.1.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.2"></times><apply id="S2.SS1.p2.4.m4.4.4.1.1.3.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.3"><divide id="S2.SS1.p2.4.m4.4.4.1.1.3.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.3"></divide><ci id="S2.SS1.p2.4.m4.4.4.1.1.3.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.3.2">𝜇</ci><cn type="integer" id="S2.SS1.p2.4.m4.4.4.1.1.3.3.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.3.3">2</cn></apply><apply id="S2.SS1.p2.4.m4.4.4.1.1.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.4.4.1.1.1.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1">superscript</csymbol><apply id="S2.SS1.p2.4.m4.4.4.1.1.1.1.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p2.4.m4.4.4.1.1.1.1.2.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.2">norm</csymbol><apply id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1"><minus id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.1"></minus><ci id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.2">𝐰</ci><apply id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3">superscript</csymbol><ci id="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.1.1.1.3.2">𝐰</ci><ci id="S2.SS1.p2.4.m4.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1">𝑡</ci></apply></apply></apply><cn type="integer" id="S2.SS1.p2.4.m4.4.4.1.1.1.3.cmml" xref="S2.SS1.p2.4.m4.4.4.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.4c">F_{k}^{\text{prox}}(\mathbf{w})=F_{k}(\mathbf{w})+\frac{\mu}{2}\|\mathbf{w}-\mathbf{w}^{(t)}\|^{2}</annotation></semantics></math>. Other solutions may introduce other modifications and innovations, for instance, by changing the way in which local weight updates are aggregated on the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, enabling knowledge distillation among the global and the local models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, or, as is the case with HeteroFL (results of which are depicted in <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>) by allowing aggregation of models of different sizes. Notably, even without any modifications, FedAvg already enables significant customization, as several hyperparameters, such as the number of local epochs, the number of clients per round, the deadline for receiving an update from a client, and the number of clients that have to report to a server for a round to be considered successful, all can be tuned and have been shown to influence the performance of the algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Experimentation Challenges</span>
</h3>

<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.5.1.1" class="ltx_text">II-B</span>1 </span>Impact of Heterogeneity on FL</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In a centralized setting, SGD is performed on the same computing device, and the data comes from the same distribution in each iteration of the algorithm. When SGD is, through FL, deployed over multiple clients, the convergence of the resulting distributed algorithm may be affected by the heterogeneity of a real-world setting. For practical purposes, critical heterogeneities affecting FL include data, hardware, and platform heterogeneities.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p"><span id="S2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Data heterogeneity.</span> The nature of data distribution among clients plays a pivotal role in FL. In Independent and Identically Distributed (IID) scenarios, each client’s data conforms to a uniform distribution, simplifying model aggregation across devices. Conversely, non-IID data presents a more challenging landscape where client dataset distributions vary significantly. This diversity demands nuanced strategies to adaptively reconcile differences between local and global models while preserving data privacy and achieving robust model performance.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">Data heterogeneity is the most broadly examined property of realistic FL, and a plethora of algorithms, including previously described FedProx, have been proposed to address the issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
At least a part of the reason for intensive research in this direction can be explained by the ease at which one can experiment with FL over non-IID data – the actual clients may remain simulated, while datasets assigned to these clients can be made artificially non-IID.</p>
</div>
<div id="S2.SS2.SSS1.p4" class="ltx_para">
<p id="S2.SS2.SSS1.p4.1" class="ltx_p"><span id="S2.SS2.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Hardware heterogeneity.</span> “Stragglers,” clients who, usually due to poor computing capabilities, take prohibitively long to complete a round of local training, present a major issue in practical FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Under the presence of stragglers, FL becomes highly inefficient because either other clients must wait for the stragglers, or the stragglers’ local updates must be discarded for the learning to advance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Alternatives, such as asynchronous FL, have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, but a common approach in practice is to select clients with uniform hardware specifications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Unfortunately, such an approach severely limits the applicability of FL and may introduce bias in the resulting models, as clients of certain characteristics (and, consequently, data properties) do not feature in the training.</p>
</div>
<div id="S2.SS2.SSS1.p5" class="ltx_para">
<p id="S2.SS2.SSS1.p5.1" class="ltx_p">Solutions for learning over clients of heterogeneous capabilities have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, yet independently evaluating such solutions remains challenging, as it would require a testbed of sufficiently heterogeneous clients. Furthermore, with heterogeneous clients come heterogeneous processing speeds, power consumption, memory usage, and other metrics, which suddenly expand the dimensionality in which the optimal FL solution should be sought. As seen in <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, introducing just one dimension (energy to accuracy) may alter the way we assess the optimality of an FL algorithm.</p>
</div>
<div id="S2.SS2.SSS1.p6" class="ltx_para">
<p id="S2.SS2.SSS1.p6.1" class="ltx_p"><span id="S2.SS2.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Platform heterogeneity.</span> FL is promoted as a solution for edge AI. Yet, “edge” encompasses a wide range of devices, from embedded devices and single board computers (SBCs) common in the Internet of Things (IoT) deployments, to smartwatches, smartphones, and beyond. Nevertheless, FL algorithms are rarely tested in actual deployments, and even more rarely are evaluated on multiple platforms. Mobile devices, in particular, tend to be highly underrepresented when it comes to evaluating FL solutions. FL over different platforms is challenging to implement, and, to the best of our knowledge, only one framework – Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> – enables distributed training over both Linux and Android platforms.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Flower does not allow a mix of different platforms in the same FL setting.</span></span></span></p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.5.1.1" class="ltx_text">II-B</span>2 </span>Experiment Orchestration and Testbed Implementation</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p"><span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Defining, scoping, and monitoring experiments.</span> Thorough examination of FL algorithms should encompass experimentation over different datasets and different data distributions, with a varying number of clients involved and reporting, to name just a few experiment parameters that should be considered. Without an easy-to-use support for such experimentation, researchers either limit the richness of the experimental scenarios or develop their own infrastructure for such experimentation, which is both time consuming and error prone.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">Furthermore, a realistic view of algorithm performance necessitates its realistic deployment. Constructing a full-fledged hardware testbed requires significant resources, both in terms of time and money. A high-end mobile device can cost about $1,000, an SBC can cost up to $500, while a high-frequency power meter costs about $1,000. Equipping an FL testbed with a few dozen devices can be prohibitively expensive for many smaller research groups to do. Moreover, hardware failures (especially networking connectivity) and software updates require active DevOps effort to maintain testbed usability.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Performance metric collection and analysis.</span> In Section <a href="#S1" title="I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we have demonstrated the need for assessing FL algorithms along different dimensions. Metrics, such as CPU/GPU utilization, energy and power consumption, memory consumption, data transfer sizes, and others, can paint a different picture of an algorithm’s performance compared to merely inspecting the inference accuracy on a test set. However, capturing these metrics without requiring changes in the algorithm code and without affecting the execution of the algorithm is challenging. Moreover, the data should be collected throughout the experiment, reliably transferred and stored, and presented to the experimenter in an appropriate manner.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Realistic experimentation is at the core of computer science research. With the rise of distributed and networked computing, a need for more elaborate testbeds has appeared. Consequently, testbeds tailored to allow multiple researchers to conduct relatively diverse experiments have appeared. Planetlab, for example, was a global testbed for computer networking services research that spawned over more than 1,000 distributed nodes at its peak <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Emulab, on the other hand, allows experimentation with various networking topologies that are emulated over a cluster of networking devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Building upon the Emulab software, the CMU wireless emulator enables remote emulation of wireless propagation conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Also on the wireless front, the ORBIT testbed allowed indoor and outdoor evaluation of wireless protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, while massive MIMO testbeds, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, enable experimentation with future 5G and 6G wireless transmission protocols.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The above testbeds have had a significant impact on networking research. Yet, besides the networking aspect, practical FL encompasses mobile systems and ML aspects. Testbeds, such as CityLab <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, facilitate IoT systems research, however, do not cover mobile, especially smartphone-based, computation, and do not readilly support distributed ML applications. ML testbeds, on the other hand, focus on cloud computing (e.g., CloudLab <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>) and do not support FL over edge devices.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">FL research is supported by several open-source frameworks that have emerged in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. These frameworks typically provide APIs for users to express DL model architectures, data loading, and model training algorithms. Often, the frameworks separate the client-side training logic from the server-side aggregation logic. In several cases, such as with Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, these frameworks include a simulation backend that allows an FL system to be run in a simulated environment without any substantial code changes.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Simulating FL is essential to expedite the prototyping of FL algorithms, system designs and evaluations thereof. Generally speaking, simulation can facilitate studying FL systems in different scenarios where the user can control the number of clients, the conditions in which they operate, the algorithms they execute, and other factors. To aid in simplifying large-scale simulation, several toolkits have been proposed to support simulating FL workloads <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.
To supplement FL simulation with realistic characteristics of heterogeneous devices, Protea <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> proposes to profile devices to obtain information regarding resource consumption and computation time. Similarly, FedScale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> builds a simulation infrastructure on top of realistic client device behavior traces in order to account for system-level heterogeneities that might affect FL. Nevertheless, certain aspects of FL performance, for instance, individual devices’ energy consumption, cannot be reliably assessed through trace-based simulation.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Critical for FL experimentation is the need to take real-world heterogeneities into account. Several studies have analyzed heterogeneity at the level of data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>,
system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and client availability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and have shown that these factors significantly impact the performance of FL. Consequently, several benchmarks that include comprehensive data partitioning strategies to cover the typical non-IID data cases have been introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">Nevertheless, to this date, most FL research results have been obtained through simulation, and only relatively few studies (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>) make use of actual experimental testbeds. Wong et al. perform a thorough study of FedAvg on a real-world edge computing testbed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. Their study demonstrates the utility of experimentation over heterogeneous hardware (the authors use Raspberry Pi 3/4, Jetson Nano and Jetson TX2), with various experiment settings (e.g., different data distributions), while different metrics (such as CPU utilization) are collected. Grounded in these findings, but not limited to FedAvg, CoLExT enables real-world experimentation on a wide span of hardware platforms, including 28 SBCs across 6 hardware types and 20 Android mobile phones across 8 models and 5 vendors, all while a wide range of metrics, from CPU utilization, to the amount of transferred data, to high-frequency energy measurements are collected.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">CoLExT: Federated Learning Testbed Overview</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we develop CoLExT, a testbed for comprehensive experimentation with FL over real-world devices. CoLExT is designed as a solution for seamless deployment of arbitrary FL algorithms and supports a range of experimental scenarios. Characteristically, the testbed supports highly heterogeneous deployments and a comprehensive set of performance metrics.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The overview of the testbed is shown in <a href="#S4.F2" title="Figure 2 ‣ IV CoLExT: Federated Learning Testbed Overview ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. The <span id="S4.p2.1.1" class="ltx_text ltx_font_bold">CoLExT server</span> acts as a central entity for both experiment coordination as well as model aggregation within an FL algorithm. The experiment configuration, <span id="S4.p2.1.2" class="ltx_text ltx_font_bold">CoLExT Config</span>, is provided by the experimenter. This configuration includes a reference to the code of the FL algorithm under test, <span id="S4.p2.1.3" class="ltx_text ltx_font_bold">FL Code</span>, and the selection of <span id="S4.p2.1.4" class="ltx_text ltx_font_bold">CoLExT clients</span> that will participate in the experiment. Clients can be selected from a range of devices, which, in the current implementation of CoLExT, include ARM and x86 SBCs (some having CUDA-capable GPUs) and Android smartphones.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The <span id="S4.p3.1.1" class="ltx_text ltx_font_bold">CoLExT server</span> instantiates the experiment by packaging experimenter-provided FL code into deployable units and deploying these to the appropriate devices: for SBCs, CoLExT packages the code into containers and deploys them with Kubernetes; for Android smartphones, the code is integrated within the CoLExT Android app code, packed into an APK, and deployed with Android Debug Bridge (ADB). The <span id="S4.p3.1.2" class="ltx_text ltx_font_bold">CoLExT server</span> also executes the FL server code provided by the experimenter.
Separately, the experiment devices collaborate to collect relevant metrics. These are collected partly on clients and partly on the server and are transferred to the <span id="S4.p3.1.3" class="ltx_text ltx_font_bold">CoLExT database</span> for further inspection.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Once the experiment is completed, the researcher will find a rich range of hardware and system metrics and statistics to gain insights into the performance of their FL algorithm, compare different FL algorithms based on the metrics, and even identify the performance difference across various devices. To facilitate the analysis, CoLExT also provides a Grafana-based <span id="S4.p4.1.1" class="ltx_text ltx_font_bold">CoLExT dashboard </span>where the metrics collected during an experiment can be visualized in real-time.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2407.14154/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">CoLExT workflow.</span></figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Using CoLExT in a Nutshell</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Interaction with CoLExT is designed to be succinct and in line with the workflow employed when using currently popular FL simulation environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. In a nutshell, the experimenter has to:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Access the CoLExT server running Python, and in a local Python environment, install our <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">colext</span> package.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">In the FL code, import the above library and wrap the FL client and strategy code with CoLExT decorators. If used outside of the testbed, these decorators do not modify the program behavior and thus can safely be included in the code in general.
An example of decorated client and strategy would be:</p>
</div>
<figure id="S4.I1.i2.fig1" class="ltx_figure">
<div id="S4.I1.i2.fig1.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,CmZyb20gY29sZXh0IGltcG9ydCBNb25pdG9yRmx3ckNsaWVudCwgTW9uaXRvckZsd3JTdHJhdGVneQpccGFyQE1vbml0b3JGbHdyQ2xpZW50CmNsYXNzIEZsb3dlckNsaWVudChmbC5jbGllbnQuTnVtUHlDbGllbnQpOgpbLi4uXQpATW9uaXRvckZsd3JTdHJhdGVneQpjbGFzcyBGbG93ZXJTdHJhdGVneShmbHdyLnNlcnZlci5zdHJhdGVneS5TdHJhdGVneSk6ClsuLi5dCg==" download="">⬇</a></div>
<div id="lstnumberx1" class="ltx_listingline">
</div>
<div id="lstnumberx2" class="ltx_listingline">
<span id="lstnumberx2.1" class="ltx_text ltx_lst_identifier">from</span><span id="lstnumberx2.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.3" class="ltx_text ltx_lst_identifier">colext</span><span id="lstnumberx2.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.5" class="ltx_text ltx_lst_identifier">import</span><span id="lstnumberx2.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.7" class="ltx_text ltx_lst_identifier">MonitorFlwrClient</span>,<span id="lstnumberx2.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx2.9" class="ltx_text ltx_lst_identifier">MonitorFlwrStrategy</span>
</div>
<div id="lstnumberx3" class="ltx_listingline">\<span id="lstnumberx3.1" class="ltx_text ltx_lst_identifier">par@MonitorFlwrClient</span>
</div>
<div id="lstnumberx4" class="ltx_listingline">
<span id="lstnumberx4.1" class="ltx_text ltx_lst_identifier">class</span><span id="lstnumberx4.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx4.3" class="ltx_text ltx_lst_identifier">FlowerClient</span>(<span id="lstnumberx4.4" class="ltx_text ltx_lst_identifier">fl</span>.<span id="lstnumberx4.5" class="ltx_text ltx_lst_identifier">client</span>.<span id="lstnumberx4.6" class="ltx_text ltx_lst_identifier">NumPyClient</span>):
</div>
<div id="lstnumberx5" class="ltx_listingline">[…]
</div>
<div id="lstnumberx6" class="ltx_listingline">
<span id="lstnumberx6.1" class="ltx_text ltx_lst_identifier">@MonitorFlwrStrategy</span>
</div>
<div id="lstnumberx7" class="ltx_listingline">
<span id="lstnumberx7.1" class="ltx_text ltx_lst_identifier">class</span><span id="lstnumberx7.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx7.3" class="ltx_text ltx_lst_identifier">FlowerStrategy</span>(<span id="lstnumberx7.4" class="ltx_text ltx_lst_identifier">flwr</span>.<span id="lstnumberx7.5" class="ltx_text ltx_lst_identifier">server</span>.<span id="lstnumberx7.6" class="ltx_text ltx_lst_identifier">strategy</span>.<span id="lstnumberx7.7" class="ltx_text ltx_lst_identifier">Strategy</span>):
</div>
<div id="lstnumberx8" class="ltx_listingline">[…]
</div>
</div>
</figure>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Declare the pip <span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">requirements.txt</span> file with the dependencies.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p">Declare the <span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">CoLExT_config.yaml</span> experiment configuration file, which specifies the client and server entry points and the testbed devices on which the experiment will run. An example of such a file would be:</p>
</div>
<figure id="S4.I1.i4.fig1" class="ltx_figure">
<div id="S4.I1.i4.fig1.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,CmNvZGU6CmNsaWVudDoKZW50cnlwb2ludDogImNsaWVudC5weSIKYXJnczoKLSAiLS1zZXJ2ZXJfYWRkcj0ke0NPTEVYVF9TRVJWRVJfQUREUkVTU30iCi0gIi0tY2xpZW50X2lkPSR7Q09MRVhUX0NMSUVOVF9JRH0iCnNlcnZlcjoKZW50cnlwb2ludDogInNlcnZlci5weSIKYXJnczogIi0tbl9jbGllbnRzPSR7Q09MRVhUX05fQ0xJRU5UU30gLS1uX3JvdW5kcz0zIgpkZXZpY2VzOgotIHsgZGV2X3R5cGU6IExhdHRlUGFuZGFEZWx0YTMsIGNvdW50OiA0IH0KLSB7IGRldl90eXBlOiBPcmFuZ2VQaTVCLCBjb3VudDogMiB9Ci0geyBkZXZfdHlwZTogSmV0c29uT3Jpbk5hbm8sIGNvdW50OiA0IH0KbW9uaXRvcmluZzoKc2NyYXBwaW5nX2ludGVydmFsOiAwLjMgIyBpbiBzZWNvbmRzCnB1c2hfdG9fZGJfaW50ZXJ2YWw6IDEwICMgaW4gc2Vjb25kcwo=" download="">⬇</a></div>
<div id="lstnumberx9" class="ltx_listingline">
</div>
<div id="lstnumberx10" class="ltx_listingline">
<span id="lstnumberx10.1" class="ltx_text ltx_lst_identifier">code</span>:
</div>
<div id="lstnumberx11" class="ltx_listingline">
<span id="lstnumberx11.1" class="ltx_text ltx_lst_identifier">client</span>:
</div>
<div id="lstnumberx12" class="ltx_listingline">
<span id="lstnumberx12.1" class="ltx_text ltx_lst_identifier">entrypoint</span>:<span id="lstnumberx12.2" class="ltx_text ltx_lst_space"> </span>”<span id="lstnumberx12.3" class="ltx_text ltx_lst_identifier">client</span>.<span id="lstnumberx12.4" class="ltx_text ltx_lst_identifier">py</span>”
</div>
<div id="lstnumberx13" class="ltx_listingline">
<span id="lstnumberx13.1" class="ltx_text ltx_lst_identifier">args</span>:
</div>
<div id="lstnumberx14" class="ltx_listingline">-<span id="lstnumberx14.1" class="ltx_text ltx_lst_space"> </span>”–<span id="lstnumberx14.2" class="ltx_text ltx_lst_identifier">server_addr</span>=<span id="lstnumberx14.3" class="ltx_text ltx_lst_identifier">$</span>{<span id="lstnumberx14.4" class="ltx_text ltx_lst_identifier">COLEXT_SERVER_ADDRESS</span>}”
</div>
<div id="lstnumberx15" class="ltx_listingline">-<span id="lstnumberx15.1" class="ltx_text ltx_lst_space"> </span>”–<span id="lstnumberx15.2" class="ltx_text ltx_lst_identifier">client_id</span>=<span id="lstnumberx15.3" class="ltx_text ltx_lst_identifier">$</span>{<span id="lstnumberx15.4" class="ltx_text ltx_lst_identifier">COLEXT_CLIENT_ID</span>}”
</div>
<div id="lstnumberx16" class="ltx_listingline">
<span id="lstnumberx16.1" class="ltx_text ltx_lst_identifier">server</span>:
</div>
<div id="lstnumberx17" class="ltx_listingline">
<span id="lstnumberx17.1" class="ltx_text ltx_lst_identifier">entrypoint</span>:<span id="lstnumberx17.2" class="ltx_text ltx_lst_space"> </span>”<span id="lstnumberx17.3" class="ltx_text ltx_lst_identifier">server</span>.<span id="lstnumberx17.4" class="ltx_text ltx_lst_identifier">py</span>”
</div>
<div id="lstnumberx18" class="ltx_listingline">
<span id="lstnumberx18.1" class="ltx_text ltx_lst_identifier">args</span>:<span id="lstnumberx18.2" class="ltx_text ltx_lst_space"> </span>”–<span id="lstnumberx18.3" class="ltx_text ltx_lst_identifier">n_clients</span>=<span id="lstnumberx18.4" class="ltx_text ltx_lst_identifier">$</span>{<span id="lstnumberx18.5" class="ltx_text ltx_lst_identifier">COLEXT_N_CLIENTS</span>}<span id="lstnumberx18.6" class="ltx_text ltx_lst_space"> </span>–<span id="lstnumberx18.7" class="ltx_text ltx_lst_identifier">n_rounds</span>=3”
</div>
<div id="lstnumberx19" class="ltx_listingline">
<span id="lstnumberx19.1" class="ltx_text ltx_lst_identifier">devices</span>:
</div>
<div id="lstnumberx20" class="ltx_listingline">-<span id="lstnumberx20.1" class="ltx_text ltx_lst_space"> </span>{<span id="lstnumberx20.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.3" class="ltx_text ltx_lst_identifier">dev_type</span>:<span id="lstnumberx20.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.5" class="ltx_text ltx_lst_identifier">LattePandaDelta3</span>,<span id="lstnumberx20.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx20.7" class="ltx_text ltx_lst_identifier">count</span>:<span id="lstnumberx20.8" class="ltx_text ltx_lst_space"> </span>4<span id="lstnumberx20.9" class="ltx_text ltx_lst_space"> </span>}
</div>
<div id="lstnumberx21" class="ltx_listingline">-<span id="lstnumberx21.1" class="ltx_text ltx_lst_space"> </span>{<span id="lstnumberx21.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx21.3" class="ltx_text ltx_lst_identifier">dev_type</span>:<span id="lstnumberx21.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx21.5" class="ltx_text ltx_lst_identifier">OrangePi5B</span>,<span id="lstnumberx21.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx21.7" class="ltx_text ltx_lst_identifier">count</span>:<span id="lstnumberx21.8" class="ltx_text ltx_lst_space"> </span>2<span id="lstnumberx21.9" class="ltx_text ltx_lst_space"> </span>}
</div>
<div id="lstnumberx22" class="ltx_listingline">-<span id="lstnumberx22.1" class="ltx_text ltx_lst_space"> </span>{<span id="lstnumberx22.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.3" class="ltx_text ltx_lst_identifier">dev_type</span>:<span id="lstnumberx22.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.5" class="ltx_text ltx_lst_identifier">JetsonOrinNano</span>,<span id="lstnumberx22.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx22.7" class="ltx_text ltx_lst_identifier">count</span>:<span id="lstnumberx22.8" class="ltx_text ltx_lst_space"> </span>4<span id="lstnumberx22.9" class="ltx_text ltx_lst_space"> </span>}
</div>
<div id="lstnumberx23" class="ltx_listingline">
<span id="lstnumberx23.1" class="ltx_text ltx_lst_identifier">monitoring</span>:
</div>
<div id="lstnumberx24" class="ltx_listingline">
<span id="lstnumberx24.1" class="ltx_text ltx_lst_identifier">scrapping_interval</span>:<span id="lstnumberx24.2" class="ltx_text ltx_lst_space"> </span>0.3<span id="lstnumberx24.3" class="ltx_text ltx_lst_space"> </span>#<span id="lstnumberx24.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.5" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx24.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx24.7" class="ltx_text ltx_lst_identifier">seconds</span>
</div>
<div id="lstnumberx25" class="ltx_listingline">
<span id="lstnumberx25.1" class="ltx_text ltx_lst_identifier">push_to_db_interval</span>:<span id="lstnumberx25.2" class="ltx_text ltx_lst_space"> </span>10<span id="lstnumberx25.3" class="ltx_text ltx_lst_space"> </span>#<span id="lstnumberx25.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.5" class="ltx_text ltx_lst_identifier">in</span><span id="lstnumberx25.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx25.7" class="ltx_text ltx_lst_identifier">seconds</span>
</div>
</div>
</figure>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para">
<p id="S4.I1.i5.p1.1" class="ltx_p">Deploy the experiment using the <span id="S4.I1.i5.p1.1.1" class="ltx_text ltx_font_typewriter">colext_launch_job</span> command, after which the experiment performance metrics will be available for real-time monitoring on the CoLExT dashboard. Optionally, after the experiment is completed, the experimenter can call <span id="S4.I1.i5.p1.1.2" class="ltx_text ltx_font_typewriter">colext_get_metrics</span> to retrieve the collected data in the form of CSV files from the CoLExT database.
In the terminal, it would look like:</p>
</div>
<figure id="S4.I1.i5.fig1" class="ltx_figure">
<div id="S4.I1.i5.fig1.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,CiQgY29sZXh0X2xhdW5jaF9qb2IgLS1jb25maWcgPHBhdGgtdG8tY29uZmlnPgojIFByaW50cyBhIGpvYi1pZCBhbmQgYSBHcmFmYW5hIGRhc2hib2FyZCBsaW5rClxwYXIjIEFmdGVyIHRoZSBqb2IgZmluaXNoZXMsIHJldHJpZXZlIG1ldHJpY3MgZm9yIGpvYi1pZAokIGNvbGV4dF9nZXRfbWV0cmljcyAtLWpvYl9pZCA8am9iLWlkPgo=" download="">⬇</a></div>
<div id="lstnumberx26" class="ltx_listingline">
</div>
<div id="lstnumberx27" class="ltx_listingline">
<span id="lstnumberx27.1" class="ltx_text ltx_lst_identifier">$</span><span id="lstnumberx27.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx27.3" class="ltx_text ltx_lst_identifier">colext_launch_job</span><span id="lstnumberx27.4" class="ltx_text ltx_lst_space"> </span>–<span id="lstnumberx27.5" class="ltx_text ltx_lst_identifier">config</span><span id="lstnumberx27.6" class="ltx_text ltx_lst_space"> </span>&lt;<span id="lstnumberx27.7" class="ltx_text ltx_lst_identifier">path</span>-<span id="lstnumberx27.8" class="ltx_text ltx_lst_identifier">to</span>-<span id="lstnumberx27.9" class="ltx_text ltx_lst_identifier">config</span>&gt;
</div>
<div id="lstnumberx28" class="ltx_listingline">#<span id="lstnumberx28.1" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.2" class="ltx_text ltx_lst_identifier">Prints</span><span id="lstnumberx28.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.4" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx28.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.6" class="ltx_text ltx_lst_identifier">job</span>-<span id="lstnumberx28.7" class="ltx_text ltx_lst_identifier">id</span><span id="lstnumberx28.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.9" class="ltx_text ltx_lst_identifier">and</span><span id="lstnumberx28.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.11" class="ltx_text ltx_lst_identifier">a</span><span id="lstnumberx28.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.13" class="ltx_text ltx_lst_identifier">Grafana</span><span id="lstnumberx28.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.15" class="ltx_text ltx_lst_identifier">dashboard</span><span id="lstnumberx28.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx28.17" class="ltx_text ltx_lst_identifier">link</span>
</div>
<div id="lstnumberx29" class="ltx_listingline">\<span id="lstnumberx29.1" class="ltx_text ltx_lst_identifier">par</span>#<span id="lstnumberx29.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.3" class="ltx_text ltx_lst_identifier">After</span><span id="lstnumberx29.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.5" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx29.6" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.7" class="ltx_text ltx_lst_identifier">job</span><span id="lstnumberx29.8" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.9" class="ltx_text ltx_lst_identifier">finishes</span>,<span id="lstnumberx29.10" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.11" class="ltx_text ltx_lst_identifier">retrieve</span><span id="lstnumberx29.12" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.13" class="ltx_text ltx_lst_identifier">metrics</span><span id="lstnumberx29.14" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.15" class="ltx_text ltx_lst_identifier">for</span><span id="lstnumberx29.16" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx29.17" class="ltx_text ltx_lst_identifier">job</span>-<span id="lstnumberx29.18" class="ltx_text ltx_lst_identifier">id</span>
</div>
<div id="lstnumberx30" class="ltx_listingline">
<span id="lstnumberx30.1" class="ltx_text ltx_lst_identifier">$</span><span id="lstnumberx30.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx30.3" class="ltx_text ltx_lst_identifier">colext_get_metrics</span><span id="lstnumberx30.4" class="ltx_text ltx_lst_space"> </span>–<span id="lstnumberx30.5" class="ltx_text ltx_lst_identifier">job_id</span><span id="lstnumberx30.6" class="ltx_text ltx_lst_space"> </span>&lt;<span id="lstnumberx30.7" class="ltx_text ltx_lst_identifier">job</span>-<span id="lstnumberx30.8" class="ltx_text ltx_lst_identifier">id</span>&gt;
</div>
</div>
</figure>
</li>
</ol>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">CoLExT Implementation</span>
</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Underlying FL Framework</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">CoLExT is designed to provide a realistic picture of FL algorithm performance in a real-world deployment, yet, at the same time, it aims to minimize the effort one needs to put into the experimentation. Therefore, rather than developing a custom solution for FL, we base CoLExT on an existing framework, offering researchers a convenient way to deploy their existing code with minimal modification.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">From the available frameworks, we opted for Flower. This framework offers a simple interface that facilitates FL research and is currently the most popular FL framework, with a large community of developers actively using and developing the framework.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Flower GitHub page has been “starred” 4.4K times, while the TesorFlow Federated page has accumulated 2.3K stars. In addition, the Flower website states that the framework is associated with “The world’s largest Federated Learning conference” – Flower AI Summit 2024.</span></span></span> Moreover, the Flower community led a substantial effort towards reproducing FL research, which has generated a pool of high quality baselines coded against the Flower API. From the technical side, Flower supports synchronous FL, relying on gRPC<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> protocol, and ensures that on-client training, on-server aggregation, and communication between the FL clients and the server are executed.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">While CoLExT relies on Flower, it is by no means locked into using this framework. Indeed, Flower could be substituted with any other framework that supports on-device execution of FL, as long as, besides the FL training control commands, the framework exposes API hooks for indicating the beginning/end of an FL round at the server and the beginning/end of the local training on the client.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">CoLExT Server and CoLExT Client</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The majority of FL research has been developed in Python and evaluated on Linux-based x86 machines due to the ease of use of this platform. Real-world FL deployments, on the other hand, are expected to include other platforms, such as the Tegra architecture of NVIDIA Jetsons, or even different operating systems, such as Android. With CoLExT, we aim to support code execution on different devices with minimal effort from the experimenter’s side.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Android and Linux-based clients are handled differently in Flower. Consequently, in CoLExT we devise a separate Linux FL client and an Android FL client.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Linux client</span> leverages containers cross-compiled using Docker Buildx to support multiple architectures, including AMD64 and ARM64. These containers are then stored on our private container registry using Harbor<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>. CoLExT expects that experimenters have their FL code written in Python and will use the supplied pip <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">requirements.txt</span> file to install the dependencies within the container.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Due to the unavailability of specific Python packages on the Python Package Index (PyPI) for some of the SBC architectures used in our testbed, we manually pre-compiled multiple versions of certain Python packages (such as PyTorch with GPU support for ARM) and enabled their inclusion during the container packaging process.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para">
<p id="S5.SS2.p5.1" class="ltx_p">In the client code, CoLExT expects that CoLExT decorators have been applied to automatically collect performance metrics from the FL code.
Finally, while the common situation of an experimenter providing the Python code with the pip <span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_typewriter">requirements.txt</span> file is handled through automatic container packaging (the configuration specification is described in <a href="#S4.SS1" title="IV-A Using CoLExT in a Nutshell ‣ IV CoLExT: Federated Learning Testbed Overview ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">IV-A</span></span></a>), other setups, such as those involving custom dependency compilation, are possible with manual container building.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Android client</span> is based on the CoLExT Android app, within which an experimenter copies their Java/Kotlin code defining the client behavior. The provided app exposes the TensorFlow Lite support for on-device DL training, handles communication with the server, and collects performance metrics.</p>
</div>
<div id="S5.SS2.p7" class="ltx_para">
<p id="S5.SS2.p7.1" class="ltx_p"><span id="S5.SS2.p7.1.1" class="ltx_text ltx_font_bold">FL Server</span> is provided as a Python script, irrespective of whether the clients are SBCs or smartphones, and is expected to run on a Linux x86 machine. Similar to the Linux clients, the FL server code is containerized with all its dependencies installed. CoLExT expects the server code to utilize the server-side CoLExT decorator to collect performance metrics, e.g., round timings. Additionally, the container is configured to have access to the GPU on the host machine in case the server code can benefit from the accelerator.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Collecting Performance Metrics</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In CoLExT, we aim to seamlessly capture experiment performance metrics with minimal modification to the FL research code and also support intuitive and informative visualization of the captured metrics.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The performance of FL has traditionally been evaluated along a single dimension – inference accuracy of the resulting model. CoLExT naturally supports the collection of this metric by logging the returned accuracy values from Flower’s server and client evaluation functions. Nevertheless, as shown in our introductory example in <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, metrics, such as the energy consumption per device, execution time, and others, are necessary for a holistic evaluation of FL algorithms. In CoLExT, such metrics are collected by a background scraper that periodically records them, timestamps them with the local time, and aggregates the data in batches before sending them to CoLExT database. The measurements are unrelated to FL rounds. However, as the devices in the testbed are synchronized using NTP, CoLExT can associate the performance data with rounds by grouping the data according to the round start/finish time.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The way the metrics are collected differs between Linux and Android clients. <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">In Linux</span>, standard hardware metrics such as CPU, memory, and network utilization are obtained using the <span id="S5.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">psutils</span> Python package from a separate background process. Power consumption, however, is tracked differently for different devices. Thus, for NVIDIA Jetsons, we obtain the power consumption of the entire board using <span id="S5.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">jetson-stats</span> monitoring tool. For LattePanda devices, on the other hand, such a tool is not available, and we capture the CPU power consumption using Intel “Running Average Power Limit” (RAPL) through the <span id="S5.SS3.p3.1.4" class="ltx_text ltx_font_typewriter">pyRAPL</span> Python package. Finally, for OrangePi devices, no suitable software solution exists; thus, we resort to physical power consumption measurement using the Monsoon power meter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Regarding GPUs, even though all SBCs have one, we were only able to train ML models on NVIDIA Jetsons’ GPUs. The other GPUs, the Intel HD Graphics GPU on the LattePanda and the Mali GPU on the Orange Pi, lack support from major ML frameworks, as these GPUs primarily focus on graphics processing, thus have limited memory and lack support for precision formats required for machine learning. NVIDIA Jetson integrated GPUs, on the other hand, support CUDA and, for the purpose of deep learning, can be treated as discrete GPUs. Metrics for NVIDIA Jetson GPUs were collected using the <span id="S5.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">jetson-stats</span> package.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text ltx_font_bold">In Android</span>, accessing performance metrics is more challenging than in Linux. While in the past, parsing <span id="S5.SS3.p5.1.2" class="ltx_text ltx_font_typewriter">/proc/stat</span> allowed one to retrieve device utilization metrics, in newer versions of Android (as of Android 12), this is not available anymore. We therefore develop a set of techniques for accessing various metrics on Android devices. First, we assign special privileges to our app in Android OS, where we register the app as the device owner. Then, we use the official memory utilization API to obtain the memory usage and <span id="S5.SS3.p5.1.3" class="ltx_text ltx_font_typewriter">/sys/devices/system/cpu</span> scraping for CPU utilization statistics. Obtaining GPU usage statistics is highly challenging, as there is no official API for GPU statistics on Android, nor do the usage statistics files necessarily exist on the file system. Our investigation with multiple phone makes, and models (Google Pixel 7, Xiaomi 12, Samsung Galaxy S21 FE, M54, XCover 6 Pro, ROG Phone 6, OnePlus Nord 2T 5G, and Xiaomi Poco X5 Pro) finds that only Samsung devices reliably expose the GPU utilization files, and only in case the devices are rooted. We, thus, collect GPU statics for Samsung devices within the CoLExT testbed. Finally, when it comes to power consumption, the official Android <span id="S5.SS3.p5.1.4" class="ltx_text ltx_font_typewriter">BatteryManager</span> API allows us to collect power consumption attributed to different apps and the system as a whole, all from our app that was previously given device owner privileges.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.5.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.6.2" class="ltx_text ltx_font_italic">Experiment Orchestration</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">CoLExT automates the deployment and running of FL experiments on real-world clients. Since the underlying FL framework handles Android and Linux clients separately (see also Section <a href="#S5.SS2" title="V-B CoLExT Server and CoLExT Client ‣ V CoLExT Implementation ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>), the experiment orchestration varies between SBCs and smartphones.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">For SBCs</span>, we create a Kubernetes cluster containing Linux clients and the server. We use the <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_typewriter">microk8s</span> orchestration tool as it is geared towards edge devices and conveniently available as a <span id="S5.SS4.p2.1.3" class="ltx_text ltx_font_typewriter">snap</span> package, which isolates the Kubernetes installation from the host filesystem while natively running the entire Kubernetes stack without the need for containers. Container deployment in Kubernetes requires configuring pods, the deployment unit in Kubernetes. CoLExT prepares FL client and server pod configurations using <span id="S5.SS4.p2.1.4" class="ltx_text ltx_font_typewriter">Jinja2</span> template files that can be configured with the required device type for the client, entry point, mounted directories for dataset caching, and added CoLExT related environment variables (listed in <a href="#S5.T1" title="TABLE I ‣ V-D Experiment Orchestration ‣ V CoLExT Implementation ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table I</span></a>), including a client identifier and device type.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">Exposing and utilizing GPU computation through a Kubernetes container is done through <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_typewriter">microk8s</span>, yet the support is limited to discrete GPUs and is incompatible with integrated GPUs of NVIDIA Jetsons. To overcome this, we configured the underlying container runtime on those devices to use the NVIDIA container runtime as the default runtime, which exposes the GPU to any containers running on the device.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">Finally, we also had issues with the default <span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_typewriter">microk8s</span> Container Network Interface (CNI), Calico, because the <span id="S5.SS4.p4.1.2" class="ltx_text ltx_font_typewriter">ipset</span> kernel module is missing from Jetsons. To avoid this issue, we switched to another CNI, Flannel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S5.T1.3.2" class="ltx_text" style="font-size:90%;">CoLExT environment variables.</span></figcaption>
<table id="S5.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.4.1.1" class="ltx_tr">
<th id="S5.T1.4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Environment Variable</th>
<th id="S5.T1.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Description</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.4.2.1" class="ltx_tr">
<td id="S5.T1.4.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">COLEXT_SERVER_ADDRESS</td>
<td id="S5.T1.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Server address (host:port)</td>
</tr>
<tr id="S5.T1.4.3.2" class="ltx_tr">
<td id="S5.T1.4.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">COLEXT_N_CLIENTS</td>
<td id="S5.T1.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Number of clients</td>
</tr>
<tr id="S5.T1.4.4.3" class="ltx_tr">
<td id="S5.T1.4.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">COLEXT_CLIENT_ID</td>
<td id="S5.T1.4.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Client ID (0…n_clients)</td>
</tr>
<tr id="S5.T1.4.5.4" class="ltx_tr">
<td id="S5.T1.4.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">COLEXT_CLIENT_DEV_TYPE</td>
<td id="S5.T1.4.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Client device type</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_bold">For smartphones</span>, a Kubernetes-based solution is not an option. Instead, we build our own deployment system using a combination of Python scripts, Bash scripts, and ADB. ADB allows us to administer Android devices and issue commands for installing and running our applications, while Python scripts, along with Bash commands, provide an interface between CoLExT server (also written in Python) and deployment scripts. The smartphones are connected to the server in Debug mode, which allows us to transfer files, install the refreshed application (if needed), and run the application for the clients through ADB commands.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS5.5.1.1" class="ltx_text">V-E</span> </span><span id="S5.SS5.6.2" class="ltx_text ltx_font_italic">CoLExT Dashboard</span>
</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2407.14154/assets/figures/dashboards/power_gpu_some_dev.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="444" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.3.2" class="ltx_text" style="font-size:90%;">Example capture of CoLExT Dashboard.</span></figcaption>
</figure>
<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">CoLExT Dashboard provides a visual depiction of the collected performance metrics. To avoid cluttering the dashboard, an experimenter can limit the metrics to a subset of rounds and clients. A cropped screenshot of the dashboard can be seen in <a href="#S5.F3" title="Figure 3 ‣ V-E CoLExT Dashboard ‣ V CoLExT Implementation ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>. Hovering over the graph provides information across different clients, while the x-axis denotes time.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">The dashboard also allows highlighting periods the FL algorithm spends on different training stages (“Show stages”): training in blue, and evaluation in red. By clearly distinguishing between these two phases, the dashboard helps identify patterns of each phase and reveals the cause of metric spikes (power and GPU utilization) that are present in the example depicted in <a href="#S5.F3" title="Figure 3 ‣ V-E CoLExT Dashboard ‣ V CoLExT Implementation ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">In addition to per-client metrics, the dashboard also contains a section with aggregated metrics over device types to assist with cross-device comparison. Finally, to simplify debugging, CoLExT can also collect logs and display them directly in the dashboard while the experiment is running.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">CoLExT Testbed Deployment</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We deployed CoLExT testbed in a dedicated server room at our institution premises. The testbed comprises of heterogeneous edge devices, including SBCs with integrated GPUs, such as the NVIDIA Jetsons, x86 SBCs, such as LattePandas, ARM-based SBCs, OrangePis, and Nvidia Jetsons, and Android devices, selected based on their AI task performance scores from the benchmark AI-Benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, so to cover low-, middle-, and high-end phones. In total, 48 devices, of which 28 are SBCs of 6 model types, and 20 Android smartphones of 8 models (5 vendors), are included in the testbed. More details, including the quantity of each device type, are present in <a href="#S6.T2" title="TABLE II ‣ VI CoLExT Testbed Deployment ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table II</span></a>, while a picture of the testbed can be seen in <a href="#S6.F4" title="Figure 4 ‣ VI CoLExT Testbed Deployment ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>. Furthermore, a server-grade machine equipped with an Intel(R) Xeon(R) Gold 6442Y CPU (48 cores @ 2.6GHz), 256GB of RAM, and an NVIDIA RTX A6000 GPU acts as the CoLExT server. Finally, the testbed also includes a Monsoon High Voltage Power Monitor (PM), which acts as a power source, while simultaneously monitoring the amount of power supplied to the device.
PM measures current and voltage at a sampling rate of 300 kHz. PM can power SBCs directly through appropriate pins. For Android devices, the battery must be removed and the PM’s wires need to be connected to the battery pins.
The Samsung Galaxy XCover series, with its removable batteries, offers the most straightforward implementation of the above, depicted in <a href="#S6.F5" title="Figure 5 ‣ VI CoLExT Testbed Deployment ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2407.14154/assets/figures/Testbed.jpg" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="437" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S6.F4.3.2" class="ltx_text" style="font-size:90%;">CoLExT testbed devices.</span></figcaption>
</figure>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2407.14154/assets/figures/Monsoon_with_samsung.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="633" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S6.F5.3.2" class="ltx_text" style="font-size:90%;">CoLExT Samsung Galaxy XCover 6 Pro powered by Monsoon PM.</span></figcaption>
</figure>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.2.1.1" class="ltx_text" style="font-size:90%;">TABLE II</span>: </span><span id="S6.T2.3.2" class="ltx_text" style="font-size:90%;">Testbed device specifications. SBC’s AI Performance was retrieved from NVIDIA Jetson Benchmarks. Score values for Android devices represent the Phone score followed by the SoC score obtained from AI-Benchmark. Unavailable phone scores are not shown.</span></figcaption>
<div id="S6.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:317.9pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.9pt,3.6pt) scale(0.978027075764875,0.978027075764875) ;">
<table id="S6.T2.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.4.1.1.1" class="ltx_tr">
<td id="S6.T2.4.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5">Single Board Computer (SBC)</td>
</tr>
<tr id="S6.T2.4.1.2.2" class="ltx_tr">
<td id="S6.T2.4.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Device</td>
<td id="S6.T2.4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Qty</td>
<td id="S6.T2.4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CPU (Core@GHz)</td>
<td id="S6.T2.4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AI Perf (TOPS)</td>
<td id="S6.T2.4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mem (GB@GHz)</td>
</tr>
<tr id="S6.T2.4.1.3.3" class="ltx_tr">
<td id="S6.T2.4.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Orange Pi 5B</td>
<td id="S6.T2.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
<td id="S6.T2.4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8@2.3</td>
<td id="S6.T2.4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S6.T2.4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16@2.3</td>
</tr>
<tr id="S6.T2.4.1.4.4" class="ltx_tr">
<td id="S6.T2.4.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Latte Panda Delta 3</td>
<td id="S6.T2.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
<td id="S6.T2.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4@2.9</td>
<td id="S6.T2.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S6.T2.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8@2.9</td>
</tr>
<tr id="S6.T2.4.1.5.5" class="ltx_tr">
<td id="S6.T2.4.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Jetson AGX Orin</td>
<td id="S6.T2.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12@2.2</td>
<td id="S6.T2.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">275</td>
<td id="S6.T2.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64@3.2</td>
</tr>
<tr id="S6.T2.4.1.6.6" class="ltx_tr">
<td id="S6.T2.4.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Jetson Orin Nano</td>
<td id="S6.T2.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4</td>
<td id="S6.T2.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6@1.5</td>
<td id="S6.T2.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">40</td>
<td id="S6.T2.4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8@2.1</td>
</tr>
<tr id="S6.T2.4.1.7.7" class="ltx_tr">
<td id="S6.T2.4.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Jetson Xavier NX</td>
<td id="S6.T2.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6@1.9</td>
<td id="S6.T2.4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">21</td>
<td id="S6.T2.4.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8@1.8</td>
</tr>
<tr id="S6.T2.4.1.8.8" class="ltx_tr">
<td id="S6.T2.4.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Jetson Nano</td>
<td id="S6.T2.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
<td id="S6.T2.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4@1.5</td>
<td id="S6.T2.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.472</td>
<td id="S6.T2.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4@1.6</td>
</tr>
<tr id="S6.T2.4.1.9.9" class="ltx_tr">
<td id="S6.T2.4.1.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5">Android Mobile Phone</td>
</tr>
<tr id="S6.T2.4.1.10.10" class="ltx_tr">
<td id="S6.T2.4.1.10.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Device</td>
<td id="S6.T2.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Qty</td>
<td id="S6.T2.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SoC</td>
<td id="S6.T2.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Score</td>
<td id="S6.T2.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mem (GB)</td>
</tr>
<tr id="S6.T2.4.1.11.11" class="ltx_tr">
<td id="S6.T2.4.1.11.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Asus ROG 6</td>
<td id="S6.T2.4.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Snapdragon 8+ Gen 1</td>
<td id="S6.T2.4.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(1447 / 1000 score)</td>
<td id="S6.T2.4.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">16</td>
</tr>
<tr id="S6.T2.4.1.12.12" class="ltx_tr">
<td id="S6.T2.4.1.12.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Xiaomi 12</td>
<td id="S6.T2.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Snapdragon 8 Gen 1</td>
<td id="S6.T2.4.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(1355 / 1046 score)</td>
<td id="S6.T2.4.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12</td>
</tr>
<tr id="S6.T2.4.1.13.13" class="ltx_tr">
<td id="S6.T2.4.1.13.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Google Pixel 7</td>
<td id="S6.T2.4.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S6.T2.4.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Google Tensor G2</td>
<td id="S6.T2.4.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(720 / 525 score)</td>
<td id="S6.T2.4.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
</tr>
<tr id="S6.T2.4.1.14.14" class="ltx_tr">
<td id="S6.T2.4.1.14.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Samsung XCover 6 Pro</td>
<td id="S6.T2.4.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3</td>
<td id="S6.T2.4.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Snapdragon 778G</td>
<td id="S6.T2.4.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">( / 257 score)</td>
<td id="S6.T2.4.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6</td>
</tr>
<tr id="S6.T2.4.1.15.15" class="ltx_tr">
<td id="S6.T2.4.1.15.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Xiaomi Poco X5 Pro</td>
<td id="S6.T2.4.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Snapdragon 778G</td>
<td id="S6.T2.4.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">( / 257 score)</td>
<td id="S6.T2.4.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
</tr>
<tr id="S6.T2.4.1.16.16" class="ltx_tr">
<td id="S6.T2.4.1.16.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Samsung Galaxy S21 FE</td>
<td id="S6.T2.4.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Exynos 2100</td>
<td id="S6.T2.4.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(262 / 196 score)</td>
<td id="S6.T2.4.1.16.16.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
</tr>
<tr id="S6.T2.4.1.17.17" class="ltx_tr">
<td id="S6.T2.4.1.17.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">OnePlus Nord 2T 5G</td>
<td id="S6.T2.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Dimensity 1300</td>
<td id="S6.T2.4.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">(240 / 177 score)</td>
<td id="S6.T2.4.1.17.17.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8</td>
</tr>
<tr id="S6.T2.4.1.18.18" class="ltx_tr">
<td id="S6.T2.4.1.18.18.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Samsung Galaxy M54</td>
<td id="S6.T2.4.1.18.18.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2</td>
<td id="S6.T2.4.1.18.18.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Exynos 1380</td>
<td id="S6.T2.4.1.18.18.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">( / 75 score)</td>
<td id="S6.T2.4.1.18.18.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">8</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">The testbed operates over its dedicated network. CoLExT devices are connected through a switch and a WiFi access point (AP) – the 28 SBCs, the FL server, and the WiFi AP are connected to a switch using Ethernet cables, while the 20 smartphones connect to AP via WiFi. The FL server runs a DHCP service, configured with statically assigned IPs, and is the gateway to the Internet.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The testbed devices are pre-configured with the appropriate operating system and software environment. For SBCs, we opt for Ansible playbooks to automate the configuration of the devices, including the configuration of the NTP server and the installation and configuration of microk8s, which also adds the node to the Kubernetes node pool. For Android, some device configuration is possible through ADB, including the NTP server setup, but other configurations, like the connection to ADB, require manual intervention. Due to hardware differences, the OS and the environment vary across SBCs. The OS also varies for Android devices, depending on the vendor. The OS environment for every device type is described in <a href="#S6.T3" title="TABLE III ‣ VI CoLExT Testbed Deployment ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table III</span></a>.</p>
</div>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T3.2.1.1" class="ltx_text" style="font-size:90%;">TABLE III</span>: </span><span id="S6.T3.3.2" class="ltx_text" style="font-size:90%;">OS environment per device type in CoLExT.</span></figcaption>
<div id="S6.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:413.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(42.3pt,-44.9pt) scale(1.27713870065569,1.27713870065569) ;">
<table id="S6.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.4.1.1.1" class="ltx_tr">
<th id="S6.T3.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" colspan="2"><span id="S6.T3.4.1.1.1.1.1" class="ltx_text ltx_font_bold">SBCs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.4.1.2.1" class="ltx_tr">
<td id="S6.T3.4.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Device</span></td>
<td id="S6.T3.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.4.1.2.1.2.1" class="ltx_text ltx_font_bold">OS environment</span></td>
</tr>
<tr id="S6.T3.4.1.3.2" class="ltx_tr">
<td id="S6.T3.4.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Jetson AGX Orin</td>
<td id="S6.T3.4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Jetpack 5.1.2 + Linux for Tegra 35.4.1</td>
</tr>
<tr id="S6.T3.4.1.4.3" class="ltx_tr">
<td id="S6.T3.4.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Jetson Orin Nano</td>
<td id="S6.T3.4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Jetpack 5.1.2 + Linux for Tegra 35.4.1</td>
</tr>
<tr id="S6.T3.4.1.5.4" class="ltx_tr">
<td id="S6.T3.4.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Jetson Xavier NX</td>
<td id="S6.T3.4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Jetpack 5.1.2 + Linux for Tegra 35.4.1</td>
</tr>
<tr id="S6.T3.4.1.6.5" class="ltx_tr">
<td id="S6.T3.4.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Jetson Nano</td>
<td id="S6.T3.4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Jetpack 4.6.4 + Linux for Tegra 32.7.4</td>
</tr>
<tr id="S6.T3.4.1.7.6" class="ltx_tr">
<td id="S6.T3.4.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Latte Panda Delta 3</td>
<td id="S6.T3.4.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ubuntu server 22.04</td>
</tr>
<tr id="S6.T3.4.1.8.7" class="ltx_tr">
<td id="S6.T3.4.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Orange Pi 5B</td>
<td id="S6.T3.4.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ubuntu server 22.04<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite>
</td>
</tr>
<tr id="S6.T3.4.1.9.8" class="ltx_tr">
<td id="S6.T3.4.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2"><span id="S6.T3.4.1.9.8.1.1" class="ltx_text ltx_font_bold">Android Smartphones</span></td>
</tr>
<tr id="S6.T3.4.1.10.9" class="ltx_tr">
<td id="S6.T3.4.1.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S6.T3.4.1.10.9.1.1" class="ltx_text ltx_font_bold">Device</span></td>
<td id="S6.T3.4.1.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S6.T3.4.1.10.9.2.1" class="ltx_text ltx_font_bold">OS environment</span></td>
</tr>
<tr id="S6.T3.4.1.11.10" class="ltx_tr">
<td id="S6.T3.4.1.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Google Pixel 7</td>
<td id="S6.T3.4.1.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 13</td>
</tr>
<tr id="S6.T3.4.1.12.11" class="ltx_tr">
<td id="S6.T3.4.1.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Samsung Galaxy S21 FE</td>
<td id="S6.T3.4.1.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 13</td>
</tr>
<tr id="S6.T3.4.1.13.12" class="ltx_tr">
<td id="S6.T3.4.1.13.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Samsung Galaxy M54</td>
<td id="S6.T3.4.1.13.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 13</td>
</tr>
<tr id="S6.T3.4.1.14.13" class="ltx_tr">
<td id="S6.T3.4.1.14.13.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Samsung XCover 6 Pro</td>
<td id="S6.T3.4.1.14.13.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 13</td>
</tr>
<tr id="S6.T3.4.1.15.14" class="ltx_tr">
<td id="S6.T3.4.1.15.14.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Xiaomi 12</td>
<td id="S6.T3.4.1.15.14.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 12</td>
</tr>
<tr id="S6.T3.4.1.16.15" class="ltx_tr">
<td id="S6.T3.4.1.16.15.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Xiaomi Poco X5 Pro</td>
<td id="S6.T3.4.1.16.15.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 12</td>
</tr>
<tr id="S6.T3.4.1.17.16" class="ltx_tr">
<td id="S6.T3.4.1.17.16.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">OnePlus Nord 2T 5G</td>
<td id="S6.T3.4.1.17.16.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Android 12</td>
</tr>
<tr id="S6.T3.4.1.18.17" class="ltx_tr">
<td id="S6.T3.4.1.18.17.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Asus ROG 6</td>
<td id="S6.T3.4.1.18.17.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Android 13</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Validating CoLExT</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We validate CoLExT for its ability to provide the metrics of interest without significant overhead and its ability to support a range of FL algorithms out-of-the-box.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS1.5.1.1" class="ltx_text">VII-A</span> </span><span id="S7.SS1.6.2" class="ltx_text ltx_font_italic">Quantifying Metric Collection Overhead</span>
</h3>

<figure id="S7.F6" class="ltx_figure"><img src="/html/2407.14154/assets/x3.png" id="S7.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="230" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F6.4.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S7.F6.2.1" class="ltx_text" style="font-size:90%;">The overhead of performance metric collection on SBCs. The results show average CPU and memory utilization over 1,000 metric collection events, with error bars indicating the 95th percentile. On each platform the CPU utilization remains below 9%, while the memory utilization remains very low (<math id="S7.F6.2.1.m1.1" class="ltx_Math" alttext="&lt;50" display="inline"><semantics id="S7.F6.2.1.m1.1b"><mrow id="S7.F6.2.1.m1.1.1" xref="S7.F6.2.1.m1.1.1.cmml"><mi id="S7.F6.2.1.m1.1.1.2" xref="S7.F6.2.1.m1.1.1.2.cmml"></mi><mo id="S7.F6.2.1.m1.1.1.1" xref="S7.F6.2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S7.F6.2.1.m1.1.1.3" xref="S7.F6.2.1.m1.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.F6.2.1.m1.1c"><apply id="S7.F6.2.1.m1.1.1.cmml" xref="S7.F6.2.1.m1.1.1"><lt id="S7.F6.2.1.m1.1.1.1.cmml" xref="S7.F6.2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S7.F6.2.1.m1.1.1.2.cmml" xref="S7.F6.2.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S7.F6.2.1.m1.1.1.3.cmml" xref="S7.F6.2.1.m1.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F6.2.1.m1.1d">&lt;50</annotation></semantics></math>MiB).</span></figcaption>
</figure>
<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Frequent sampling of performance metrics may require significant resources of the host machine, which, in turn, could affect the execution of the FL task. To ensure this is not the case in CoLExT, we profile the metric scrapper on SBCs. The profiling ran for 5 minutes, scrapping every 0.3 seconds and pushing metrics to the database every 10 seconds. In total, we collected 1,000 sample points. We assess the excess CPU and memory usage caused by such scraping in <a href="#S7.F6" title="Figure 6 ‣ VII-A Quantifying Metric Collection Overhead ‣ VII Validating CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.</p>
</div>
<div id="S7.SS1.p2" class="ltx_para">
<p id="S7.SS1.p2.1" class="ltx_p">For most devices, we see that CPU usage stays below 2.5%, i.e., remains rather insignificant. The lowest CPU utilization is associated with the OrangePi, which only collects metrics from the <span id="S7.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">psutils</span> package. The second lowest utilization belongs to the LattePanda devices, where, on top of <span id="S7.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">psutils</span>, CoLExT resorts to <span id="S7.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">PyRAPL</span> to collect power measurements, hence requiring additional CPU utilization. Jetson AGX Orin and Orin Nano experience increased CPU usage, as these devices call <span id="S7.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">psutils</span> and then <span id="S7.SS1.p2.1.5" class="ltx_text ltx_font_typewriter">jetson-stats</span> for power measurements and GPU utilization. Finally, we see a rather unexpected spike in CPU usage for Jetson XavierNX and Jetson Nano, where the CPU utilization is over 3 times higher than for the other Jetson devices, despite using the same data collection method. After profiling the code and benchmarking the CPUs with the <span id="S7.SS1.p2.1.6" class="ltx_text ltx_font_typewriter">sysbench</span> program, we discovered that the CPUs of these devices are approximately twice as slow as the CPUs of other Jetson devices. However, the CPU utilization of XavierNX can be reduced by using a power mode that favors CPU speed by using fewer cores (6@1.4GHz or 2@1.9GHz). Nevertheless, we believe that with less than 10% CPU overhead, the burden imposed by fine-grain metric collection remains acceptable irrespective of the device type, while the rate of metric collection can be reduced in case lowering the overhead is necessary.</p>
</div>
<div id="S7.SS1.p3" class="ltx_para">
<p id="S7.SS1.p3.1" class="ltx_p">In <a href="#S7.F6" title="Figure 6 ‣ VII-A Quantifying Metric Collection Overhead ‣ VII Validating CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>, we also depict the memory usage, which is shown to stay consistently low across devices when metrics are periodically pushed to the DB. However, if metrics are not periodically pushed, they are stored in memory, causing memory utilization to increase by about 10 KiB for every 1k samples collected, as each sample (of all the metrics) requires an average of 10 bytes. Related to this, the network usage is also determined by how frequently metrics are pushed to the DB. Given the small size of the metrics, only minimal bandwidth is required. Moreover, potential interference of data transmission and the operation of the FL experiment can be entirely avoided by sending the metrics to the DB only after the experiment has finished.</p>
</div>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S7.SS2.5.1.1" class="ltx_text">VII-B</span> </span><span id="S7.SS2.6.2" class="ltx_text ltx_font_italic">Supporting Different FL Algorithms</span>
</h3>

<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S7.SS2.SSS1.5.1.1" class="ltx_text">VII-B</span>1 </span>Porting FL algorithms on SBCs</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p id="S7.SS2.SSS1.p1.1" class="ltx_p">To confirm CoLExT’s ease of use and compatibility with different FL algorithms, we execute a collection of FL algorithms on our CoLExT testbed. We select these algorithms from open-source implementations developed within the Flower “Summer of Reproducibility” initiative <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, during which a cash reward was provided for the Flower-based implementations of published FL algorithms. Currently, this collection includes 20 baselines, of which seven (listed in <a href="#S7.T4" title="TABLE IV ‣ VII-B1 Porting FL algorithms on SBCs ‣ VII-B Supporting Different FL Algorithms ‣ VII Validating CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table IV</span></a>) were used for our experiments.</p>
</div>
<div id="S7.SS2.SSS1.p2" class="ltx_para">
<p id="S7.SS2.SSS1.p2.1" class="ltx_p">These baselines are written so that their folder structure and code organization are uniform, which makes comparison across algorithms straightforward. Nevertheless, due to varying authorship, the coding style, code efficiency, and package dependencies varied noticeably among the algorithms. Thus, we believe that with this set of algorithms, we can comprehensively test the CoLExT’s ability to support different FL algorithms. Note, however, that all of the baselines were initially implemented with a simulation environment in mind, and consequently, certain modifications are necessary to get them working in a real-world client-server deployment. In case the algorithms do not assume shared data between clients and the server, and when the data communicated with the server can be serialized by the client, the required changes are minor. In all cases, the entry script needs to be updated to support the separate start-up of client and server as follows:</p>
</div>
<figure id="S7.SS2.SSS1.fig1" class="ltx_figure">
<div id="S7.SS2.SSS1.fig1.1" class="ltx_listing ltx_lstlisting ltx_listing">
<div class="ltx_listing_data"><a href="data:text/plain;base64,CiMgQmVmb3JlOiBTaW11bGF0aW9uCmhpc3RvcnkgPSBmbC5zaW11bGF0aW9uLnN0YXJ0X3NpbXVsYXRpb24oCmNsaWVudF9mbj1jbGllbnRfZm4sCm51bV9jbGllbnRzPWNmZy5udW1fY2xpZW50cywKY29uZmlnPWZsLnNlcnZlci5TZXJ2ZXJDb25maWcoY2ZnLm51bV9yb3VuZHMpLApjbGllbnRfcmVzb3VyY2VzPXsuLi59LApzdHJhdGVneT1zdHJhdGVneSwKKQpccGFyIyBBZnRlcjogQ2xpZW50IC0gU2VydmVyCiMgaXNfY2xpZW50LCBpc19zZXJ2ZXIsIHNlcnZlcl9hZGRyLCBudW1fcm91bmRzLCBjbGllbnRfaWQ6CiMgcGFzc2VkIGFzIGFyZ3VtZW50cyB2aWEgdGhlIGNvbmZpZ3VyYXRpb24gZmlsZQppZiBpc19jbGllbnQ6CmZsLmNsaWVudC5zdGFydF9udW1weV9jbGllbnQoCnNlcnZlcl9hZGRyZXNzPWNmZy5zZXJ2ZXJfYWRkciwKY2xpZW50PWNsaWVudF9mbihjZmcuY2xpZW50X2lkKSwKKQplbGlmIGlzX3NlcnZlcjoKZmwuc2VydmVyLnN0YXJ0X3NlcnZlcigKc2VydmVyX2FkZHJlc3M9IjAuMC4wLjA6ODA4MCIsCmNvbmZpZz1mbC5zZXJ2ZXIuU2VydmVyQ29uZmlnKGNmZy5udW1fcm91bmRzKSwKc3RyYXRlZ3k9c3RyYXRlZ3ksCikK" download="">⬇</a></div>
<div id="lstnumberx31" class="ltx_listingline">
</div>
<div id="lstnumberx32" class="ltx_listingline">#<span id="lstnumberx32.1" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx32.2" class="ltx_text ltx_lst_identifier">Before</span>:<span id="lstnumberx32.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx32.4" class="ltx_text ltx_lst_identifier">Simulation</span>
</div>
<div id="lstnumberx33" class="ltx_listingline">
<span id="lstnumberx33.1" class="ltx_text ltx_lst_identifier">history</span><span id="lstnumberx33.2" class="ltx_text ltx_lst_space"> </span>=<span id="lstnumberx33.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx33.4" class="ltx_text ltx_lst_identifier">fl</span>.<span id="lstnumberx33.5" class="ltx_text ltx_lst_identifier">simulation</span>.<span id="lstnumberx33.6" class="ltx_text ltx_lst_identifier">start_simulation</span>(
</div>
<div id="lstnumberx34" class="ltx_listingline">
<span id="lstnumberx34.1" class="ltx_text ltx_lst_identifier">client_fn</span>=<span id="lstnumberx34.2" class="ltx_text ltx_lst_identifier">client_fn</span>,
</div>
<div id="lstnumberx35" class="ltx_listingline">
<span id="lstnumberx35.1" class="ltx_text ltx_lst_identifier">num_clients</span>=<span id="lstnumberx35.2" class="ltx_text ltx_lst_identifier">cfg</span>.<span id="lstnumberx35.3" class="ltx_text ltx_lst_identifier">num_clients</span>,
</div>
<div id="lstnumberx36" class="ltx_listingline">
<span id="lstnumberx36.1" class="ltx_text ltx_lst_identifier">config</span>=<span id="lstnumberx36.2" class="ltx_text ltx_lst_identifier">fl</span>.<span id="lstnumberx36.3" class="ltx_text ltx_lst_identifier">server</span>.<span id="lstnumberx36.4" class="ltx_text ltx_lst_identifier">ServerConfig</span>(<span id="lstnumberx36.5" class="ltx_text ltx_lst_identifier">cfg</span>.<span id="lstnumberx36.6" class="ltx_text ltx_lst_identifier">num_rounds</span>),
</div>
<div id="lstnumberx37" class="ltx_listingline">
<span id="lstnumberx37.1" class="ltx_text ltx_lst_identifier">client_resources</span>={…},
</div>
<div id="lstnumberx38" class="ltx_listingline">
<span id="lstnumberx38.1" class="ltx_text ltx_lst_identifier">strategy</span>=<span id="lstnumberx38.2" class="ltx_text ltx_lst_identifier">strategy</span>,
</div>
<div id="lstnumberx39" class="ltx_listingline">)
</div>
<div id="lstnumberx40" class="ltx_listingline">\<span id="lstnumberx40.1" class="ltx_text ltx_lst_identifier">par</span>#<span id="lstnumberx40.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.3" class="ltx_text ltx_lst_identifier">After</span>:<span id="lstnumberx40.4" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.5" class="ltx_text ltx_lst_identifier">Client</span><span id="lstnumberx40.6" class="ltx_text ltx_lst_space"> </span>-<span id="lstnumberx40.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx40.8" class="ltx_text ltx_lst_identifier">Server</span>
</div>
<div id="lstnumberx41" class="ltx_listingline">#<span id="lstnumberx41.1" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.2" class="ltx_text ltx_lst_identifier">is_client</span>,<span id="lstnumberx41.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.4" class="ltx_text ltx_lst_identifier">is_server</span>,<span id="lstnumberx41.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.6" class="ltx_text ltx_lst_identifier">server_addr</span>,<span id="lstnumberx41.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.8" class="ltx_text ltx_lst_identifier">num_rounds</span>,<span id="lstnumberx41.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx41.10" class="ltx_text ltx_lst_identifier">client_id</span>:
</div>
<div id="lstnumberx42" class="ltx_listingline">#<span id="lstnumberx42.1" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.2" class="ltx_text ltx_lst_identifier">passed</span><span id="lstnumberx42.3" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.4" class="ltx_text ltx_lst_identifier">as</span><span id="lstnumberx42.5" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.6" class="ltx_text ltx_lst_identifier">arguments</span><span id="lstnumberx42.7" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.8" class="ltx_text ltx_lst_identifier">via</span><span id="lstnumberx42.9" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.10" class="ltx_text ltx_lst_identifier">the</span><span id="lstnumberx42.11" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.12" class="ltx_text ltx_lst_identifier">configuration</span><span id="lstnumberx42.13" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx42.14" class="ltx_text ltx_lst_identifier">file</span>
</div>
<div id="lstnumberx43" class="ltx_listingline">
<span id="lstnumberx43.1" class="ltx_text ltx_lst_identifier">if</span><span id="lstnumberx43.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx43.3" class="ltx_text ltx_lst_identifier">is_client</span>:
</div>
<div id="lstnumberx44" class="ltx_listingline">
<span id="lstnumberx44.1" class="ltx_text ltx_lst_identifier">fl</span>.<span id="lstnumberx44.2" class="ltx_text ltx_lst_identifier">client</span>.<span id="lstnumberx44.3" class="ltx_text ltx_lst_identifier">start_numpy_client</span>(
</div>
<div id="lstnumberx45" class="ltx_listingline">
<span id="lstnumberx45.1" class="ltx_text ltx_lst_identifier">server_address</span>=<span id="lstnumberx45.2" class="ltx_text ltx_lst_identifier">cfg</span>.<span id="lstnumberx45.3" class="ltx_text ltx_lst_identifier">server_addr</span>,
</div>
<div id="lstnumberx46" class="ltx_listingline">
<span id="lstnumberx46.1" class="ltx_text ltx_lst_identifier">client</span>=<span id="lstnumberx46.2" class="ltx_text ltx_lst_identifier">client_fn</span>(<span id="lstnumberx46.3" class="ltx_text ltx_lst_identifier">cfg</span>.<span id="lstnumberx46.4" class="ltx_text ltx_lst_identifier">client_id</span>),
</div>
<div id="lstnumberx47" class="ltx_listingline">)
</div>
<div id="lstnumberx48" class="ltx_listingline">
<span id="lstnumberx48.1" class="ltx_text ltx_lst_identifier">elif</span><span id="lstnumberx48.2" class="ltx_text ltx_lst_space"> </span><span id="lstnumberx48.3" class="ltx_text ltx_lst_identifier">is_server</span>:
</div>
<div id="lstnumberx49" class="ltx_listingline">
<span id="lstnumberx49.1" class="ltx_text ltx_lst_identifier">fl</span>.<span id="lstnumberx49.2" class="ltx_text ltx_lst_identifier">server</span>.<span id="lstnumberx49.3" class="ltx_text ltx_lst_identifier">start_server</span>(
</div>
<div id="lstnumberx50" class="ltx_listingline">
<span id="lstnumberx50.1" class="ltx_text ltx_lst_identifier">server_address</span>=”0.0.0.0:8080”,
</div>
<div id="lstnumberx51" class="ltx_listingline">
<span id="lstnumberx51.1" class="ltx_text ltx_lst_identifier">config</span>=<span id="lstnumberx51.2" class="ltx_text ltx_lst_identifier">fl</span>.<span id="lstnumberx51.3" class="ltx_text ltx_lst_identifier">server</span>.<span id="lstnumberx51.4" class="ltx_text ltx_lst_identifier">ServerConfig</span>(<span id="lstnumberx51.5" class="ltx_text ltx_lst_identifier">cfg</span>.<span id="lstnumberx51.6" class="ltx_text ltx_lst_identifier">num_rounds</span>),
</div>
<div id="lstnumberx52" class="ltx_listingline">
<span id="lstnumberx52.1" class="ltx_text ltx_lst_identifier">strategy</span>=<span id="lstnumberx52.2" class="ltx_text ltx_lst_identifier">strategy</span>,
</div>
<div id="lstnumberx53" class="ltx_listingline">)
</div>
</div>
</figure>
<div id="S7.SS2.SSS1.p3" class="ltx_para">
<p id="S7.SS2.SSS1.p3.1" class="ltx_p">As we identify in <a href="#S7.T4" title="TABLE IV ‣ VII-B1 Porting FL algorithms on SBCs ‣ VII-B Supporting Different FL Algorithms ‣ VII Validating CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table IV</span></a>, we successfully complete the experiments with five out of seven selected algorithms – two baselines do not comply with the above requirements and hence cannot be deployed on the testbed without corrections to their codebase. The process was relatively straightforward, and besides the expected augmentation of the code (i.e., 3 lines of code for decorating the FL client and strategy, detailed in Section <a href="#S4.SS1" title="IV-A Using CoLExT in a Nutshell ‣ IV CoLExT: Federated Learning Testbed Overview ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>, and the above-shown changes to the entry script), we had to take the following additional steps: 1) Flower uses Poetry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite> to specify dependencies, thus we had to convert the list of dependencies into the format CoLExT supports, i.e., the pip <span id="S7.SS2.SSS1.p3.1.1" class="ltx_text ltx_font_typewriter">requirements.txt</span> file, using the <span id="S7.SS2.SSS1.p3.1.2" class="ltx_text ltx_font_typewriter">poetry export</span> command; 2) package dependencies for aarch64 sometimes needed adjustments. From our analysis, all baselines were tested on an x86 machine. However, when we try to use the same (identical) dependencies, we sometimes find that minor versions of packages are not available on aarch64. When we encounter these issues, we bump the minor version for one that supports aarch64; 3) we had to replace the use of dependencies explicitly targeting x86 architectures. In Flower baselines, torch and torchvision dependencies are specified by prebuilt wheels targeting x86. We had to comment out these dependencies so that aarch64 wheels could be used for our aarch64 devices.</p>
</div>
<div id="S7.SS2.SSS1.p4" class="ltx_para">
<p id="S7.SS2.SSS1.p4.1" class="ltx_p">We now describe the challenges encountered when porting the baselines. For FedAVGm, we had to degrade TensorFlow from 2.11.1 to 2.11.0 because a dependency required by the original version was not available for aarch64. Additionally, the PyPI TensorFlow wheel for x86 CPUs assumes support for the AVX instruction, which is not available on the LattePandas, so they cannot use those wheels. Certain parts of the Moon baseline code assume that a GPU is available for the movement of data to/from the GPU. This prevents the experiment from running on CPU-only devices. FedPara is deployable, but we encountered issues with the PyTorch DataLoader module, causing segmentation faults on OrangePis and LattePandas. The exact cause remains unclear, but it could be related to the data partitioning strategy not supporting a reduced number of clients.
The HeteroFL baseline requires some code decoupling to work in a client-server setup. In the simulated environment code, the client information is assumed to be available to the server before the experiment starts, but in a client-server setup, it needs to be shared by the client.
FjORD’s client implementation attempts to send nested dictionaries and model weights, whereas Flower supports only string-to-scalar dictionaries, thus, the algorithm was not portable.</p>
</div>
<figure id="S7.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S7.T4.2.1.1" class="ltx_text" style="font-size:90%;">TABLE IV</span>: </span><span id="S7.T4.3.2" class="ltx_text" style="font-size:90%;">FL algorithms tested on CoLExT SBCs.</span></figcaption>
<div id="S7.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:135pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.0pt,4.5pt) scale(0.93758752155882,0.93758752155882) ;">
<table id="S7.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T4.4.1.1.1" class="ltx_tr">
<th id="S7.T4.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Algorithm</th>
<th id="S7.T4.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Deployed</th>
<th id="S7.T4.4.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Issues</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T4.4.1.2.1" class="ltx_tr">
<td id="S7.T4.4.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedAVGm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S7.T4.4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S7.T4.4.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Aarch64 &amp; TensorFlow on LattePanda</td>
</tr>
<tr id="S7.T4.4.1.3.2" class="ltx_tr">
<td id="S7.T4.4.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S7.T4.4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S7.T4.4.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S7.T4.4.1.4.3" class="ltx_tr">
<td id="S7.T4.4.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Moon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S7.T4.4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S7.T4.4.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">GPU only</td>
</tr>
<tr id="S7.T4.4.1.5.4" class="ltx_tr">
<td id="S7.T4.4.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>
</td>
<td id="S7.T4.4.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S7.T4.4.1.5.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">-</td>
</tr>
<tr id="S7.T4.4.1.6.5" class="ltx_tr">
<td id="S7.T4.4.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedPara <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>
</td>
<td id="S7.T4.4.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S7.T4.4.1.6.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Dataloader segfault LattePanda + OrangePi</td>
</tr>
<tr id="S7.T4.4.1.7.6" class="ltx_tr">
<td id="S7.T4.4.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">HeteroFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</td>
<td id="S7.T4.4.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">X</td>
<td id="S7.T4.4.1.7.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Code decoupling needed</td>
</tr>
<tr id="S7.T4.4.1.8.7" class="ltx_tr">
<td id="S7.T4.4.1.8.7.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">FjORD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>
</td>
<td id="S7.T4.4.1.8.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">X</td>
<td id="S7.T4.4.1.8.7.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Unsupported serialization</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S7.SS2.SSS1.p5" class="ltx_para">
<p id="S7.SS2.SSS1.p5.1" class="ltx_p">To confirm that the experimentation in CoLExT was not only successful but also produced the expected results, we executed the Moon <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> baseline and compared it with the FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> algorithm with 10 clients using the CIFAR-10 dataset as instructed in the README for the Moon baseline. The accuracy achieved for the final models was slightly lower than the values reported in the README, more specifically 0.1% and 1.4% lower for Moon and FedProx, respectively. This discrepancy can be explained by the alteration of the random number generator sequences when switching from a simulated environment to a client-server setup.</p>
</div>
<div id="S7.SS2.SSS1.p6" class="ltx_para">
<p id="S7.SS2.SSS1.p6.1" class="ltx_p">In conclusion, the experiments conducted in CoLExT demonstrate that the FL code can be successfully deployed in the testbed environment with only minor potential variations in accuracy compared to the simulated environment.</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S7.SS2.SSS2.5.1.1" class="ltx_text">VII-B</span>2 </span>Porting algorithms to Android devices</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p id="S7.SS2.SSS2.p1.1" class="ltx_p">On Android devices, Flower’s support limits us to use TFLite. One significant constraint of TFLite is its lack of support for stateful FL algorithms.
To confirm the support of different algorithms in CoLExT, we needed to port stateless FL algorithms to Android.</p>
</div>
<div id="S7.SS2.SSS2.p2" class="ltx_para">
<p id="S7.SS2.SSS2.p2.1" class="ltx_p">Given the limitations, we developed an application that supports stateless algorithms, and we implemented and tested the algorithms presented in <a href="#S7.T5" title="TABLE V ‣ VII-B2 Porting algorithms to Android devices ‣ VII-B Supporting Different FL Algorithms ‣ VII Validating CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table V</span></a>. The algorithms were ported from the Flower code repository. FedYogi and FedAdam modify the adaptive momentum estimation (Adam) optimizer to stabilize the learning process and improve convergence in heterogeneous federated settings. FedAdagrad adapts the Adagrad optimizer for FL by using per-parameter learning rates that adjust based on the history of gradients, accommodating non-IID data distributions. FedOpt serves as a generalized framework for federated optimization, encompassing various adaptive optimizers providing flexibility and robustness.</p>
</div>
<div id="S7.SS2.SSS2.p3" class="ltx_para">
<p id="S7.SS2.SSS2.p3.1" class="ltx_p">All of the stateless algorithms only require changes on the server side. Currently, the implementations in the Flower repository for all algorithms support serialization designed for Python implementations. To make the algorithms work with Android, we had to make certain changes to the code. The initial code from Flower relies on NumPy for the serialization of weights transferred to the clients and back. As we are running these clients on Android, we do not have access to NumPy, so we harnessed the existing implementation of a suitable serialization that is present in the FedAvgAndroid code from the Flower repository.
Furthermore, the above algorithms require a random starting model. We added a mechanism similar to what FedAvg uses, whereby if there is no initial model present at the server, such a model is pulled from one randomly chosen client and broadcast to the others.</p>
</div>
<figure id="S7.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S7.T5.2.1.1" class="ltx_text" style="font-size:90%;">TABLE V</span>: </span><span id="S7.T5.3.2" class="ltx_text" style="font-size:90%;">FL algorithms tested on CoLExT smartphones.</span></figcaption>
<table id="S7.T5.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T5.4.1.1" class="ltx_tr">
<th id="S7.T5.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Algorithm</th>
<th id="S7.T5.4.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Deployed</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T5.4.2.1" class="ltx_tr">
<td id="S7.T5.4.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S7.T5.4.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S7.T5.4.3.2" class="ltx_tr">
<td id="S7.T5.4.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedAdam <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S7.T5.4.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S7.T5.4.4.3" class="ltx_tr">
<td id="S7.T5.4.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedYogi <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S7.T5.4.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S7.T5.4.5.4" class="ltx_tr">
<td id="S7.T5.4.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedAdagrad <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>
</td>
<td id="S7.T5.4.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
</tr>
<tr id="S7.T5.4.6.5" class="ltx_tr">
<td id="S7.T5.4.6.5.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">FedOpt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
<td id="S7.T5.4.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">FL Profiling with CoLExT</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Heterogeneity of both SBC and smartphone devices included in our testbed, support for a broad span of algorithms and datasets, together with a range of performance metrics produced by the framework, ensure that CoLExT provides a rich experimentation space for FL research In this section, through a small number of use cases, we show how CoLExT experiments can be used to improve our understanding of FL.</p>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS1.5.1.1" class="ltx_text">VIII-A</span> </span><span id="S8.SS1.6.2" class="ltx_text ltx_font_italic">Revealing accuracy–resource usage trade-off</span>
</h3>

<figure id="S8.F7" class="ltx_figure"><img src="/html/2407.14154/assets/x4.png" id="S8.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F7.8.4.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S8.F7.6.3" class="ltx_text" style="font-size:90%;">TTA and ETA consumed to reach <math id="S8.F7.4.1.m1.1" class="ltx_Math" alttext="64\%" display="inline"><semantics id="S8.F7.4.1.m1.1b"><mrow id="S8.F7.4.1.m1.1.1" xref="S8.F7.4.1.m1.1.1.cmml"><mn id="S8.F7.4.1.m1.1.1.2" xref="S8.F7.4.1.m1.1.1.2.cmml">64</mn><mo id="S8.F7.4.1.m1.1.1.1" xref="S8.F7.4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S8.F7.4.1.m1.1c"><apply id="S8.F7.4.1.m1.1.1.cmml" xref="S8.F7.4.1.m1.1.1"><csymbol cd="latexml" id="S8.F7.4.1.m1.1.1.1.cmml" xref="S8.F7.4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S8.F7.4.1.m1.1.1.2.cmml" xref="S8.F7.4.1.m1.1.1.2">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.4.1.m1.1d">64\%</annotation></semantics></math> accuracy on CIFAR-10 with <math id="S8.F7.5.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S8.F7.5.2.m2.1b"><mn id="S8.F7.5.2.m2.1.1" xref="S8.F7.5.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S8.F7.5.2.m2.1c"><cn type="integer" id="S8.F7.5.2.m2.1.1.cmml" xref="S8.F7.5.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.5.2.m2.1d">20</annotation></semantics></math> clients and Dirichlet distribution (<math id="S8.F7.6.3.m3.1" class="ltx_Math" alttext="\alpha=1.0" display="inline"><semantics id="S8.F7.6.3.m3.1b"><mrow id="S8.F7.6.3.m3.1.1" xref="S8.F7.6.3.m3.1.1.cmml"><mi id="S8.F7.6.3.m3.1.1.2" xref="S8.F7.6.3.m3.1.1.2.cmml">α</mi><mo id="S8.F7.6.3.m3.1.1.1" xref="S8.F7.6.3.m3.1.1.1.cmml">=</mo><mn id="S8.F7.6.3.m3.1.1.3" xref="S8.F7.6.3.m3.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.F7.6.3.m3.1c"><apply id="S8.F7.6.3.m3.1.1.cmml" xref="S8.F7.6.3.m3.1.1"><eq id="S8.F7.6.3.m3.1.1.1.cmml" xref="S8.F7.6.3.m3.1.1.1"></eq><ci id="S8.F7.6.3.m3.1.1.2.cmml" xref="S8.F7.6.3.m3.1.1.2">𝛼</ci><cn type="float" id="S8.F7.6.3.m3.1.1.3.cmml" xref="S8.F7.6.3.m3.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.F7.6.3.m3.1d">\alpha=1.0</annotation></semantics></math>).</span></figcaption>
</figure>
<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">The final model inference accuracy is the most reported metric in FL research. Yet, the accuracy is seldom juxtaposed against the resources needed to achieve it. In <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> of <a href="#S1" title="I Introduction ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section I</span></a>, we reveal that substantial energy costs can be incurred to achieve a very modest gain in accuracy. We now expand this investigation and assess how both the time and the energy vary as we aim to achieve a certain inference accuracy through different FL algorithms and settings.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p">We focus on the SBC clients in our testbed and assign both a training and a validation dataset sampled from the same distribution to each client, whereas distributions differ among the clients (i.e., non-IID data) according to the Dirichlet distribution with parameter <math id="S8.SS1.p2.1.m1.1" class="ltx_Math" alttext="\alpha=1.0" display="inline"><semantics id="S8.SS1.p2.1.m1.1a"><mrow id="S8.SS1.p2.1.m1.1.1" xref="S8.SS1.p2.1.m1.1.1.cmml"><mi id="S8.SS1.p2.1.m1.1.1.2" xref="S8.SS1.p2.1.m1.1.1.2.cmml">α</mi><mo id="S8.SS1.p2.1.m1.1.1.1" xref="S8.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S8.SS1.p2.1.m1.1.1.3" xref="S8.SS1.p2.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p2.1.m1.1b"><apply id="S8.SS1.p2.1.m1.1.1.cmml" xref="S8.SS1.p2.1.m1.1.1"><eq id="S8.SS1.p2.1.m1.1.1.1.cmml" xref="S8.SS1.p2.1.m1.1.1.1"></eq><ci id="S8.SS1.p2.1.m1.1.1.2.cmml" xref="S8.SS1.p2.1.m1.1.1.2">𝛼</ci><cn type="float" id="S8.SS1.p2.1.m1.1.1.3.cmml" xref="S8.SS1.p2.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p2.1.m1.1c">\alpha=1.0</annotation></semantics></math>. We employ FL with different algorithms (FedAvg, FedProx, and HeteroFL), using different model sizes (<em id="S8.SS1.p2.1.1" class="ltx_emph ltx_font_italic">Small</em> – 35k and <em id="S8.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Large</em> – 380k parameters) and a different number of clients harnessed in each round. After each training round, each client evaluates the global model on its validation dataset and reports the resulting accuracy to the server. The experiment continues until the average validation accuracy across all clients achieves a pre-defined target value.</p>
</div>
<div id="S8.SS1.p3" class="ltx_para">
<p id="S8.SS1.p3.3" class="ltx_p">We are interested in resources spent for training, thus we measure the wall clock (i.e., time-to-accuracy – TTA) and the energy (i.e., energy-to-accuracy – ETA) required to reach the target accuracy. To determine the energy spent on training, we first measure the average idle power consumption of the devices and subtract this “average idle power” from the power measurements during training. For instance, our experiments indicate that Jetson XavierNX consumes <math id="S8.SS1.p3.1.m1.1" class="ltx_Math" alttext="2.9W" display="inline"><semantics id="S8.SS1.p3.1.m1.1a"><mrow id="S8.SS1.p3.1.m1.1.1" xref="S8.SS1.p3.1.m1.1.1.cmml"><mn id="S8.SS1.p3.1.m1.1.1.2" xref="S8.SS1.p3.1.m1.1.1.2.cmml">2.9</mn><mo lspace="0em" rspace="0em" id="S8.SS1.p3.1.m1.1.1.1" xref="S8.SS1.p3.1.m1.1.1.1.cmml">​</mo><mi id="S8.SS1.p3.1.m1.1.1.3" xref="S8.SS1.p3.1.m1.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p3.1.m1.1b"><apply id="S8.SS1.p3.1.m1.1.1.cmml" xref="S8.SS1.p3.1.m1.1.1"><times id="S8.SS1.p3.1.m1.1.1.1.cmml" xref="S8.SS1.p3.1.m1.1.1.1"></times><cn type="float" id="S8.SS1.p3.1.m1.1.1.2.cmml" xref="S8.SS1.p3.1.m1.1.1.2">2.9</cn><ci id="S8.SS1.p3.1.m1.1.1.3.cmml" xref="S8.SS1.p3.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p3.1.m1.1c">2.9W</annotation></semantics></math> (Watt) at rest. Hence, when such a device consumes, on average, <math id="S8.SS1.p3.2.m2.1" class="ltx_Math" alttext="5W" display="inline"><semantics id="S8.SS1.p3.2.m2.1a"><mrow id="S8.SS1.p3.2.m2.1.1" xref="S8.SS1.p3.2.m2.1.1.cmml"><mn id="S8.SS1.p3.2.m2.1.1.2" xref="S8.SS1.p3.2.m2.1.1.2.cmml">5</mn><mo lspace="0em" rspace="0em" id="S8.SS1.p3.2.m2.1.1.1" xref="S8.SS1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S8.SS1.p3.2.m2.1.1.3" xref="S8.SS1.p3.2.m2.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p3.2.m2.1b"><apply id="S8.SS1.p3.2.m2.1.1.cmml" xref="S8.SS1.p3.2.m2.1.1"><times id="S8.SS1.p3.2.m2.1.1.1.cmml" xref="S8.SS1.p3.2.m2.1.1.1"></times><cn type="integer" id="S8.SS1.p3.2.m2.1.1.2.cmml" xref="S8.SS1.p3.2.m2.1.1.2">5</cn><ci id="S8.SS1.p3.2.m2.1.1.3.cmml" xref="S8.SS1.p3.2.m2.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p3.2.m2.1c">5W</annotation></semantics></math> during the experiment, we consider that <math id="S8.SS1.p3.3.m3.1" class="ltx_Math" alttext="2.1W" display="inline"><semantics id="S8.SS1.p3.3.m3.1a"><mrow id="S8.SS1.p3.3.m3.1.1" xref="S8.SS1.p3.3.m3.1.1.cmml"><mn id="S8.SS1.p3.3.m3.1.1.2" xref="S8.SS1.p3.3.m3.1.1.2.cmml">2.1</mn><mo lspace="0em" rspace="0em" id="S8.SS1.p3.3.m3.1.1.1" xref="S8.SS1.p3.3.m3.1.1.1.cmml">​</mo><mi id="S8.SS1.p3.3.m3.1.1.3" xref="S8.SS1.p3.3.m3.1.1.3.cmml">W</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p3.3.m3.1b"><apply id="S8.SS1.p3.3.m3.1.1.cmml" xref="S8.SS1.p3.3.m3.1.1"><times id="S8.SS1.p3.3.m3.1.1.1.cmml" xref="S8.SS1.p3.3.m3.1.1.1"></times><cn type="float" id="S8.SS1.p3.3.m3.1.1.2.cmml" xref="S8.SS1.p3.3.m3.1.1.2">2.1</cn><ci id="S8.SS1.p3.3.m3.1.1.3.cmml" xref="S8.SS1.p3.3.m3.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p3.3.m3.1c">2.1W</annotation></semantics></math> is the “average active power” used for training. We then compute the energy used for training by multiplying the “average active power” by the time when the device is actively learning.</p>
</div>
<div id="S8.SS1.p4" class="ltx_para">
<p id="S8.SS1.p4.1" class="ltx_p">In <a href="#S8.F7" title="Figure 7 ‣ VIII-A Revealing accuracy–resource usage trade-off ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>, we compare TTA and ETA for the three algorithms when the target accuracy is set to 64%. We observe that using the <em id="S8.SS1.p4.1.1" class="ltx_emph ltx_font_italic">Large</em> model (denoted with larger symbols) significantly increases the training time. A close look reveals that the cause for this lies in stragglers, as the server waits until it receives the updated models from all the sampled clients. Note that the FedAvg (square symbols) and the FedProx (inverse triangle symbols) algorithms require all the clients to train the same model architecture, while the HeteroFL algorithm (star symbols) goes beyond this limitation, allowing clients to train a model with a size proportional to their computational power. Consequently, the TTA for HeteroFL is comparable to the algorithms in which all clients train the <em id="S8.SS1.p4.1.2" class="ltx_emph ltx_font_italic">Small</em> model (denoted with smaller symbols). For the cluster of points on the left of the figure, which reflect the training of the <em id="S8.SS1.p4.1.3" class="ltx_emph ltx_font_italic">Small</em> model with the FedAvg and FedProx algorithms and heterogeneous model sizes with HeteroFL, we see a trend: increasing the percentage of clients sampled for training in each training round (“fraction fit”) decreases the TTA, as each update to the server model is obtained with more data. However, this causes an increase in ETA as more devices are used for training.</p>
</div>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS2.5.1.1" class="ltx_text">VIII-B</span> </span><span id="S8.SS2.6.2" class="ltx_text ltx_font_italic">Revealing (in)efficiency of on-device training</span>
</h3>

<figure id="S8.F8" class="ltx_figure"><img src="/html/2407.14154/assets/x5.png" id="S8.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="369" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S8.F8.3.2" class="ltx_text" style="font-size:90%;">Comparing per batch (pb) efficiency of four NVIDIA Jetson models using the Moon and FedProx algorithms. All metrics have been Normalized (N) to the smallest value. The plotted data represents averages over 100 round samples, with error bars indicating the 95th percentiles.</span></figcaption>
</figure>
<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">CoLExT can also be used to assess the efficiency of different device types when handling the same workload.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para">
<p id="S8.SS2.p2.1" class="ltx_p">We focus on the Moon and FedProx implementations that we experimented with in <a href="#S7.SS2" title="VII-B Supporting Different FL Algorithms ‣ VII Validating CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">VII-B</span></span></a>. To profile efficiency, we measure the time and the energy required to process one batch of data, calculated by dividing the measured time and energy required to finish a training round by the number of batches in the round. We deploy and run the algorithms on GPU-enabled SBCs in our testbed.</p>
</div>
<div id="S8.SS2.p3" class="ltx_para">
<p id="S8.SS2.p3.1" class="ltx_p">The results, shown in <a href="#S8.F8" title="Figure 8 ‣ VIII-B Revealing (in)efficiency of on-device training ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> (top-left), reveal that the fastest device is, unsurprisingly, AGXOrin, which is also advertised as the most powerful SBC in our setup. However, for both algorithms, AGXOrin is not significantly faster than OrinNano, yet, it uses noticeably more energy (top-right plot). Thus, we ask: How can we quantify whether the decrease in training time justifies the additional energy cost?</p>
</div>
<div id="S8.SS2.p4" class="ltx_para">
<p id="S8.SS2.p4.1" class="ltx_p">This led us to consider another metric common in digital electronics, the <em id="S8.SS2.p4.1.1" class="ltx_emph ltx_font_italic">Energy Delay Product</em> (EDP). This metric multiplies the time taken to complete a task with the energy required for the task completion. Thus, a (preferred) low EDP indicates that a solution is both fast and energy-efficient. In <a href="#S8.F8" title="Figure 8 ‣ VIII-B Revealing (in)efficiency of on-device training ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> (bottom-left), we observe that, according to EDP, OrinNano is the most efficient device in both algorithms. In other words, considering our subset of devices, if we only use OrinNano devices for this algorithm, we would minimize the time taken to train a batch for a given energy budget and vice-versa. The data also shows that XavierNX appears to be the slowest device, despite being a more recent model and having better hardware compared to the oldest model, the Jetson Nano (<a href="#S6.T2" title="TABLE II ‣ VI CoLExT Testbed Deployment ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table II</span></a> for device specifications). However, when FedProx is employed, despite XavierNX being the slowest device, it is still more efficient than a faster device, as is shown by the lower EDP value compared to Jetson Nano.</p>
</div>
<div id="S8.SS2.p5" class="ltx_para">
<p id="S8.SS2.p5.1" class="ltx_p">As exemplified by our trials, the EDP ranking reported is algorithm-specific since some algorithm operations may be more efficient on the underlying hardware of certain devices. In addition, our comparison is performed without fine-tuning the boards (e.g., the CPU operating frequency, etc.) for optimal efficiency; it is possible that running AGXOrin in a lower power mode could make it more efficient than OrinNano. Nevertheless, the testbed allows users to compare device efficiencies and observe, and consequently address, potential performance issues with FL on real-world devices.</p>
</div>
</section>
<section id="S8.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS3.5.1.1" class="ltx_text">VIII-C</span> </span><span id="S8.SS3.6.2" class="ltx_text ltx_font_italic">Revealing algorithm implementation issues</span>
</h3>

<div id="S8.SS3.p1" class="ltx_para">
<p id="S8.SS3.p1.1" class="ltx_p">The testbed also allows us to analyze the algorithms’ performance at the FL round level. We once again turn back to the SBC-based experiment we performed with FedProx and Moon in the previous section. The algorithms are rather similar, yet we expect that results will reveal the cost of extra forward passes conducted by Moon – compared to FedProx, which completes one forward pass, Moon performs three (one for the client model, two for the server model) in each round. However, the results of the experimentation (depicted in <a href="#S8.F8" title="Figure 8 ‣ VIII-B Revealing (in)efficiency of on-device training ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> left) show that both algorithms take a very similar amount of time per batch (and, consequently, per round). To further unpack the issue, instead of using the <em id="S8.SS3.p1.1.1" class="ltx_emph ltx_font_italic">Small</em> model, which might prevent the expected differences from being noticed, we tasked the algorithms with training the <em id="S8.SS3.p1.1.2" class="ltx_emph ltx_font_italic">Large</em> model. This larger model exceeds the 4 GB memory of the Jetson Nano devices, so this experiment was only done with other Jetson device types. The results, shown in <a href="#S8.F9" title="Figure 9 ‣ VIII-C Revealing algorithm implementation issues ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a> (with L indicating the <em id="S8.SS3.p1.1.3" class="ltx_emph ltx_font_italic">Large</em> model training), indicate that the difference in time between the two algorithms is obvious, if not unexpectedly large.</p>
</div>
<div id="S8.SS3.p2" class="ltx_para">
<p id="S8.SS3.p2.1" class="ltx_p">Surprised by such a large difference in the execution time (Moon <math id="S8.SS3.p2.1.m1.1" class="ltx_math_unparsed" alttext="1.75\times" display="inline"><semantics id="S8.SS3.p2.1.m1.1a"><mrow id="S8.SS3.p2.1.m1.1b"><mn id="S8.SS3.p2.1.m1.1.1">1.75</mn><mo lspace="0.222em" id="S8.SS3.p2.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S8.SS3.p2.1.m1.1c">1.75\times</annotation></semantics></math> slower), we further inspected the code and identified unnecessary data movements from the GPU to the CPU memory in the Moon codebase. By addressing this and optimizing the code (denoted with L+O in <a href="#S8.F9" title="Figure 9 ‣ VIII-C Revealing algorithm implementation issues ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>), the results finally get in line with the theoretical expectations regarding the two algorithms.</p>
</div>
<div id="S8.SS3.p3" class="ltx_para">
<p id="S8.SS3.p3.1" class="ltx_p">This experiment highlights how code optimizations and model size modifications can drastically change how two algorithms compare in terms of the training time, making their implementation and real-world performance analysis critical.</p>
</div>
<figure id="S8.F9" class="ltx_figure"><img src="/html/2407.14154/assets/x6.png" id="S8.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S8.F9.3.2" class="ltx_text" style="font-size:90%;">Normalized (N) round level metrics for Moon and FedProx using an 11.6M parameter model (L), with the addition of a small optimization (O) to the original Moon baseline code. Results show average round data statistics over 30 rounds, with error bars indicating the 95th percentiles.</span></figcaption>
</figure>
</section>
<section id="S8.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S8.SS4.5.1.1" class="ltx_text">VIII-D</span> </span><span id="S8.SS4.6.2" class="ltx_text ltx_font_italic">Identifying causes for stragglers in real-world mobiles</span>
</h3>

<figure id="S8.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F10.1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.14154/assets/figures/android_eval/cpu_problem/cifar10_cpu_util.png" id="S8.F10.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="237" alt="Refer to caption">
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S8.F10.2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.14154/assets/figures/android_eval/cpu_problem/cifar10_power_consumption.png" id="S8.F10.2.g1" class="ltx_graphics ltx_img_landscape" width="538" height="238" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F10.4.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S8.F10.5.2" class="ltx_text" style="font-size:90%;">CPU utilization and power consumption on heterogeneous mobile hardware. Data collected over 1 round of training on CIFAR-10 dataset</span></figcaption>
</figure>
<figure id="S8.F11" class="ltx_figure"><img src="/html/2407.14154/assets/figures/android_eval/network_problem/femnist_data_upload.png" id="S8.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="234" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S8.F11.3.2" class="ltx_text" style="font-size:90%;">Uploaded bytes over time after a round of training on FEMNIST dataset using CNN model.</span></figcaption>
</figure>
<div id="S8.SS4.p1" class="ltx_para">
<p id="S8.SS4.p1.1" class="ltx_p">We perform extensive experimentation with Android devices in CoLExT and identify two main reasons for devices becoming stragglers, both due to hardware heterogeneity:</p>
</div>
<div id="S8.SS4.p2" class="ltx_para ltx_noindent">
<p id="S8.SS4.p2.1" class="ltx_p"><span id="S8.SS4.p2.1.1" class="ltx_text ltx_font_bold">Compute speed</span>: Hardware heterogeneity introduces varying compute capabilities among devices, which is particularly noticeable during model training. The top graph of <a href="#S8.F10" title="Figure 10 ‣ VIII-D Identifying causes for stragglers in real-world mobiles ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a> shows the CPU utilization and power consumption during one round of training on three devices: Google Pixel 7, Xiaomi 12, and OnePlus Nord 2 5G. The start and end of local training can be identified by shifts in CPU utilization, from near idle (close to 0%) to high utilization (over 100% – indicating that multiple CPU cores are active) and then back to idle.
We observe that OnePlus Nord takes the longest to complete a training round, with almost 2.3<math id="S8.SS4.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.SS4.p2.1.m1.1a"><mo id="S8.SS4.p2.1.m1.1.1" xref="S8.SS4.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.SS4.p2.1.m1.1b"><times id="S8.SS4.p2.1.m1.1.1.cmml" xref="S8.SS4.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.SS4.p2.1.m1.1c">\times</annotation></semantics></math> longer round time than the fastest participating device in this training (Google Pixel 7). In the bottom graph of <a href="#S8.F10" title="Figure 10 ‣ VIII-D Identifying causes for stragglers in real-world mobiles ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>, significant power consumption spikes are evident for two devices, contrasting with the OnePlus as a result of it being severely underclocked. Despite having similar CPUs, the default underclocking of the CPU in OnePlus makes a substantial difference in a real-world deployment of FL.</p>
</div>
<div id="S8.SS4.p3" class="ltx_para ltx_noindent">
<p id="S8.SS4.p3.1" class="ltx_p"><span id="S8.SS4.p3.1.1" class="ltx_text ltx_font_bold">Data Reception and Transmission</span>: Even with similar computing capabilities, devices can vary significantly in other hardware components, notably the wireless communication module (WiFi/cellular chip). With larger models requiring the transfer of many weights, differences in data transmission speeds become more evident. <a href="#S8.F11" title="Figure 11 ‣ VIII-D Identifying causes for stragglers in real-world mobiles ‣ VIII FL Profiling with CoLExT ‣ Where is the Testbed for my Federated Learning Research?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 11</span></a> illustrates this, showing the Xiaomi 12, with the default transmission parameters, as a straggler, taking 3.2<math id="S8.SS4.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S8.SS4.p3.1.m1.1a"><mo id="S8.SS4.p3.1.m1.1.1" xref="S8.SS4.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S8.SS4.p3.1.m1.1b"><times id="S8.SS4.p3.1.m1.1.1.cmml" xref="S8.SS4.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S8.SS4.p3.1.m1.1c">\times</annotation></semantics></math> longer transmission time for the same amount of data compared to the OnePlus 2T 5G.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IX </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Limitations</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Deep learning has evolved separately on desktop and server environments, where it is often implemented in Python, harnessing PyTorch or Keras libraries, and on mobile devices, where deep learning is usually based on the TensorFlow Lite (TFLite) library and written in Kotlin or Java. Consequently, FL is also supported in different manners on the two groups of devices present in CoLExT testbed – SBCs and Android phones. While we rely on Flower, due to its support for both of these device groups, we still face the barrier of different serialization formats used by PyTorch and TFLite, which hampers FL over a group of clients where Android and SBC devices are intermixed. A potential solution could be a compatibility-providing mapping between the PyTorch and TFLite serialization formats, something that ONNX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> promises to provide. However, our extensive experiments with such a mapping failed to produce a reliable solution. Alternatively, deploying TFLite on single-board computers (SBCs) could be considered. However, this approach is not widely adopted among FL researchers, and TFLite may not offer the same level of usability as PyTorch for certain applications. In future versions of CoLExT we aim to address this issue and enable a combination of models trained with PyTorch and TFLite to be used within the same experiment.</p>
</div>
</section>
<section id="S10" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">X </span><span id="S10.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S10.p1" class="ltx_para">
<p id="S10.p1.1" class="ltx_p">Reproducible and realistic experimentation is necessary for the future growth of distributed AI. Despite its research popularity, FL remains mostly confined to simulations, as the effort of deploying FL solutions over heterogeneous edge devices and collecting performance metrics remains insurmountable to most research groups. In this paper, we present CoLExT, an FL experimentation framework and a testbed that allow an arbitrary FL algorithm to be run in a heterogeneous environment and assessed from various performance aspects. Our testbed already employs over 40 devices and collects a range of metrics, including inference accuracy, detailed energy usage, and CPU/GPU/memory utilization information. Nevertheless, we believe that by making CoLExT publicly available, we will support the organic growth of our testbed so that it addresses the up-to-date needs of FL researchers and practitioners.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This publication is based upon work supported by the King Abdullah University of Science and Technology (KAUST) Office of Research Administration (ORA) under Award No. ORA-CRG2021-4699, and partly supported by the Slovenian Research Agency (research projects J2-3047 and P2-0098).
We are thankful to Abdullah Alamoudi, Suliman Alharbi, Rasheed Alhaddad for their helpful contributions to a first prototype of our system.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He, “A survey on federated learning systems: Vision, hype and reality for data privacy and protection,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge &amp; Data Engineering</em>, vol. 35, no. 04, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, R. G. L. D’Oliveira, H. Eichner, S. E. Rouayheb, D. Evans, J. Gardner, Z. Garrett, A. Gascón, B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Harchaoui, C. He, L. He, Z. Huo, B. Hutchinson, J. Hsu, M. Jaggi, T. Javidi, G. Joshi, M. Khodak, J. Konecný, A. Korolova, F. Koushanfar, S. Koyejo, T. Lepoint, Y. Liu, P. Mittal, M. Mohri, R. Nock, A. Özgür, R. Pagh, H. Qi, D. Ramage, R. Raskar, M. Raykova, D. Song, W. Song, S. U. Stich, Z. Sun, A. T. Suresh, F. Tramèr, P. Vepakomma, J. Wang, L. Xiong, Z. Xu, Q. Yang, F. X. Yu, H. Yu, and S. Zhao, “Advances and open problems in federated learning,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in Machine Learning</em>, vol. 14, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient learning of deep networks from decentralized data,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">AISTATS</em>, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konečný, H. B. McMahan, V. Smith, and A. Talwalkar, “Leaf: A benchmark for federated settings,” 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Hsieh, A. Phanishayee, O. Mutlu, and P. Gibbons, “The non-iid data quagmire of decentralized machine learning,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. Yang, Q. Wang, M. Xu, Z. Chen, K. Bian, Y. Liu, and X. Liu, “Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">WWW</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Wu, T. Li, Z. Charles, Y. Xiao, Z. Liu, Z. Xu, and V. Smith, “Motley: Benchmarking heterogeneity and personalization in federated learning,” 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. M. Abdelmoniem, C.-Y. Ho, P. Papageorgiou, and M. Canini, “A comprehensive empirical study of heterogeneity in federated learning,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
G. A. Baumgart, J. Shin, A. Payani, M. Lee, and R. R. Kompella, “Not all federated learning algorithms are created equal: A performance evaluation study,” 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J. Zhang, S. Zeng, M. Zhang, R. Wang, F. Wang, Y. Zhou, P. P. Liang, and L. Qu, “Flhetbench: Benchmarking device and state heterogeneity in federated learning,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T.-M. H. Hsu, H. Qi, and M. Brown, “Measuring the effects of non-identical data distribution for federated visual classification,” 2019. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/1909.06335" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1909.06335</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith, “Federated optimization in heterogeneous networks,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">MLSys</em>, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Q. Li, B. He, and D. Song, “Model-contrastive federated learning,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for robust model fusion in federated learning,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh, “Scaffold: Stochastic controlled averaging for federated learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the objective inconsistency problem in heterogeneous federated optimization,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
F. Fourati, S. Kharrat, V. Aggarwal, M.-S. Alouini, and M. Canini, “Filfl: Client filtering for optimized client participation in federated learning,” 2023. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2302.06599" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2302.06599</a>

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. J. Beutel, T. Topal, A. Mathur, X. Qiu, J. Fernandez-Marques, Y. Gao, L. Sani, K. H. Li, T. Parcollet, P. P. B. de Gusmão, and N. D. Lane, “Flower: A friendly federated learning research framework,” 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
E. Diao, J. Ding, and V. Tarokh, “Heterofl: Computation and communication efficient federated learning for heterogeneous clients,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Aljahdali, A. M. Abdelmoniem, M. Canini, and S. Horváth, “Flashback: Understanding and mitigating forgetting in federated learning,” 2024. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2402.05558" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2402.05558</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush, J. Konečný, S. Kumar, and H. B. McMahan, “Adaptive federated optimization,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F. Haddadpour and M. Mahdavi, “On the convergence of local descent methods in federated learning,” 2019. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/1910.14425" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1910.14425</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
X. Zhang, M. Hong, S. Dhople, W. Yin, and Y. Liu, “Fedpd: A federated learning framework with adaptivity to non-iid data,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Signal Processing</em>, vol. 69, 2021. [Online]. Available: <a target="_blank" href="http://dx.doi.org/10.1109/TSP.2021.3115952" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/TSP.2021.3115952</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov, C. Kiddon, J. Konečný, S. Mazzocchi, H. B. McMahan, T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards federated learning at scale: System design,” in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">MLSys</em>, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
A. M. Abdelmoniem, A. N. Sahu, M. Canini, and S. A. Fahmy, “REFL: Resource-Efficient Federated Learning,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">EuroSys</em>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
T. Ortega and H. Jafarkhani, “Asynchronous federated learning with bidirectional quantized communications and buffered aggregation,” 2023. [Online]. Available: <a target="_blank" href="https://arxiv.org/abs/2308.00263" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2308.00263</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
F. Lai, X. Zhu, H. V. Madhyastha, and M. Chowdhury, “Efficient Federated Learning via Guided Participant Selection,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">OSDI</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L. Peterson, T. Anderson, D. Culler, and T. Roscoe, “A blueprint for introducing disruptive technology into the internet,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">SIGCOMM Comput. Commun. Rev.</em>, vol. 33, no. 1, 2003. [Online]. Available: <a target="_blank" href="https://doi.org/10.1145/774763.774772" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/774763.774772</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
B. White, L. Stoller, R. Ricci, S. Guruprasad, M. N. bold, M. Hibler, C. Barb, and A. Joglekar, “An integrated experimental environment for distributed systems and networks,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">OSDI</em>, 2002. [Online]. Available: <a target="_blank" href="https://www.usenix.org/conference/osdi-02/integrated-experimental-environment-distributed-systems-and-networks" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/osdi-02/integrated-experimental-environment-distributed-systems-and-networks</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G. Judd and P. Steenkiste, “Using emulation to understand and improve wireless networks and applications,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">NSDI</em>, 2005.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D. Raychaudhuri, I. Seskar, M. Ott, S. Ganu, K. Ramachandran, H. Kremo, R. Siracusa, H. Liu, and M. Singh, “Overview of the orbit radio grid testbed for evaluation of next-generation wireless network protocols,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">WCNC</em>, 2005.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. Sakhnini, S. De Bast, M. Guenach, A. Bourdoux, H. Sahli, and S. Pollin, “Near-field coherent radar sensing using a massive mimo communication testbed,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Wireless Communications</em>, vol. 21, no. 8, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J. Struye, B. Braem, S. Latré, and J. Marquez-Barja, “The citylab testbed – large-scale multi-technology wireless experimentation in a city environment: Neural network-based interference prediction in a smart city,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">INFOCOM Workshops</em>, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
D. Duplyakin, R. Ricci, A. Maricq, G. Wong, J. Duerig, E. Eide, L. Stoller, M. Hibler, D. Johnson, K. Webb, A. Akella, K. Wang, G. Ricart, L. Landweber, C. Elliott, M. Zink, E. Cecchet, S. Kar, and P. Mishra, “The Design and Operation of CloudLab,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">USENIX ATC</em>, 2019. [Online]. Available: <a target="_blank" href="https://www.usenix.org/conference/atc19/presentation/duplyakin" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.usenix.org/conference/atc19/presentation/duplyakin</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
P. Foley, M. J. Sheller, B. Edwards, S. Pati, W. Riviera, M. Sharma, P. N. Moorthy, S. han Wang, J. Martin, P. Mirhaji, P. Shah, and S. Bakas, “Openfl: the open federated learning library,” <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Physics in Medicine &amp; Biology</em>, vol. 67, no. 21, 2022.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
H. R. Roth, Y. Cheng, Y. Wen, I. Yang, Z. Xu, Y.-T. Hsieh, K. Kersten, A. Harouni, C. Zhao, K. Lu, Z. Zhang, W. Li, A. Myronenko, D. Yang, S. Yang, N. Rieke, A. Quraini, C. Chen, D. Xu, N. Ma, P. Dogra, M. Flores, and A. Feng, “Nvidia flare: Federated learning from simulation to real-world,” 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Xie, Z. Wang, D. Gao, D. Chen, L. Yao, W. Kuang, Y. Li, B. Ding, and J. Zhou, “Federatedscope: A flexible federated learning platform for heterogeneity,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em>, vol. 16, no. 5, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
C. He, S. Li, J. So, X. Zeng, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh, H. Qiu, X. Zhu, J. Wang, L. Shen, P. Zhao, Y. Kang, Y. Liu, R. Raskar, Q. Yang, M. Annavaram, and S. Avestimehr, “Fedml: A research library and benchmark for federated machine learning,” 2020.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Liu, T. Fan, T. Chen, Q. Xu, and Q. Yang, “Fate: An industrial grade platform for collaborative learning with data protection,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, vol. 22, no. 226, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Ziller, A. Trask, A. Lopardo, B. Szymkow, B. Wagner, E. Bluemke, J.-M. Nounahon, J. Passerat-Palmbach, K. Prakash, N. Rose, T. Ryffel, Z. N. Reza, and G. Kaissis, <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">PySyft: A Library for Easy Federated Learning</em>.   Springer International Publishing, 2021.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
H. Daga, J. Shin, D. Garg, A. Gavrilovska, M. Lee, and R. R. Kompella, “Flame: Simplifying topology extension in federated learning,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">SoCC</em>, 2023.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M. N. Galtier and C. Marini, “Substra: a framework for privacy-preserving, traceable and collaborative machine learning,” 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
N. Kourtellis, K. Katevas, and D. Perino, “Flaas: Federated learning as a service,” in <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">DistributedML</em>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D. Chen, D. Gao, Y. Xie, X. Pan, Z. Li, Y. Li, B. Ding, and J. Zhou, “Fs-real: Towards real-world cross-device federated learning,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">KDD</em>, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
TensorFlow Team, “Tensorflow federated,” 2024. [Online]. Available: <a target="_blank" href="https://github.com/tensorflow/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tensorflow/federated</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
H. Ludwig, N. Baracaldo, G. Thomas, Y. Zhou, A. Anwar, S. Rajamoni, Y. Ong, J. Radhakrishnan, A. Verma, M. Sinn, M. Purcell, A. Rawat, T. Minh, N. Holohan, S. Chakraborty, S. Whitherspoon, D. Steuer, L. Wynter, H. Hassan, S. Laguna, M. Yurochkin, M. Agarwal, E. Chuba, and A. Abay, “Ibm federated learning: an enterprise framework white paper v0.1,” 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
M. Zhang, F. Yu, Y. Yu, M. Zhang, A. Li, and X. Chen, “Fedhc: A scalable federated learning framework for heterogeneous and resource-constrained clients,” 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
F. Lai, Y. Dai, S. S. Singapuram, J. Liu, X. Zhu, H. V. Madhyastha, and M. Chowdhury, “Fedscale: Benchmarking model and system performance of federated learning at scale,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">ICML</em>, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
M. H. Garcia, A. Manoel, D. M. Diaz, F. Mireshghallah, R. Sim, and D. Dimitriadis, “Flute: A scalable, extensible framework for high-performance federated learning simulations,” 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
L. Li, J. Wang, and C. Xu, “Flsim: An extensible and reusable simulation framework for federated learning,” in <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Simulation Tools and Techniques</em>, 2021.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
D. Zeng, S. Liang, X. Hu, H. Wang, and Z. Xu, “Fedlab: A flexible federated learning framework,” 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
J. H. Ro, A. T. Suresh, and K. Wu, “Fedjax: Federated learning simulation with jax,” 2021.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
V. Mugunthan, A. Peraire-Bueno, and L. Kagal, “Privacyfl: A simulator for privacy-preserving and secure federated learning,” in <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">CIKM</em>, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
W. Zhuang, X. Gan, Y. Wen, and S. Zhang, “Easyfl: A low-code federated learning platform for dummies,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 9, no. 15, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
W. Zhao, X. Qiu, J. Fernandez-Marques, P. P. B. de Gusmão, and N. D. Lane, “Protea: Client profiling within federated systems using flower,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">FedEdge</em>, 2022.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
A. Nilsson, S. Smith, G. Ulm, E. Gustavsson, and M. Jirstrand, “A performance evaluation of federated learning algorithms,” in <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">DIDL</em>, 2018.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
K. Selialia, Y. Chandio, and F. M. Anwar, “Federated learning biases in heterogeneous edge-devices: A case-study,” in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">SenSys</em>, 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Q. Li, Y. Diao, Q. Chen, and B. He, “Federated learning on non-iid data silos: An experimental study,” in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">ICDE</em>, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
J. O. du Terrail, S.-S. Ayed, E. Cyffers, F. Grimberg, C. He, R. Loeb, P. Mangold, T. Marchand, O. Marfoq, E. Mushtaq, B. Muzellec, C. Philippenko, S. Silva, M. Teleńczuk, S. Albarqouni, S. Avestimehr, A. Bellet, A. Dieuleveut, M. Jaggi, S. P. Karimireddy, M. Lorenzi, G. Neglia, M. Tommasi, and M. Andreux, “Flamby: Datasets and benchmarks for cross-silo federated learning in realistic healthcare settings,” 2023.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Z. Zhang, X. Hu, J. Zhang, Y. Zhang, H. Wang, L. Qu, and Z. Xu, “Fedlegal: The first real-world federated learning benchmark for legal nlp,” in <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">ACL</em>, 2023.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
S. Hu, Y. Li, X. Liu, Q. Li, Z. Wu, and B. He, “The oarf benchmark suite: Characterization and implications for federated learning systems,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>, vol. 13, no. 4, 2022.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
C. Song, F. Granqvist, and K. Talwar, “Flair: Federated learning annotated image repository,” in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2022.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
T.-M. H. Hsu, H. Qi, and M. Brown, “Federated visual classification with real-world data distribution,” 2020.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
R. Sun, Y. Li, T. Shah, R. W. H. Sham, T. Szydlo, B. Qian, D. Thakker, and R. Ranjan, “Fedmsa: A model selection and adaptation system for federated learning,” <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 19, 2022.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
J. Shin, Y. Li, Y. Liu, and S.-J. Lee, “Fedbalancer: Data and pace control for efficient federated learning on heterogeneous clients,” in <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">MobiSys</em>, 2022.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
X. Ouyang, Z. Xie, J. Zhou, J. Huang, and G. Xing, “Clusterfl: A similarity-aware federated learning system for human activity recognition,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">MobiSys</em>, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
J. Mills, J. Hu, and G. Min, “Communication-efficient federated learning for wireless edge intelligence in iot,” <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 7, no. 7, 2020.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
H. Woisetschläger, A. Isenko, R. Mayer, and H.-A. Jacobsen, “Fledge: Benchmarking federated machine learning applications in edge computing systems,” 2023.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
K.-S. Wong, M. Nguyen-Duc, K. Le-Huy, L. Ho-Tuan, C. Do-Danh, and D. Le-Phuoc, “An empirical study of federated learning on iot-edge devices: Resource allocation and heterogeneity,” 2023.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
(2024) grpc: A high-performance, open source universal rpc framework. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://grpc.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://grpc.io</a>

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
(2024) Harbor: Cloud native registry. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://goharbor.io" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://goharbor.io</a>

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
(2024) Hvpm: High voltage power monitor. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="http://msoon.github.io/powermonitor/HVPM.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://msoon.github.io/powermonitor/HVPM.html</a>

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
K. Tsakalozos. (2024) Microk8s issue #2. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://github.com/canonical/microk8s/issues/2" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/canonical/microk8s/issues/2</a>

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
(2024) Ai benchmark ranking. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://ai-benchmark.com/ranking.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai-benchmark.com/ranking.html</a>

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J. Riek. (2024) Ubuntu rockship. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://github.com/Joshua-Riek/ubuntu-rockchip" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Joshua-Riek/ubuntu-rockchip</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
F. L. GmbH. (2024) Flower summer: A federated learning initiative. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://flower.ai/summer/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://flower.ai/summer/</a>

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
S. Eustace. (2024) Poetry: Python dependency management and packaging made easy. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://python-poetry.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://python-poetry.org/</a>

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the objective inconsistency problem in heterogeneous federated optimization,” in <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
N. Hyeon-Woo, M. Ye-Bin, and T.-H. Oh, “Fedpara: Low-rank hadamard product for communication-efficient federated learning,” in <em id="bib.bib79.1.1" class="ltx_emph ltx_font_italic">ICLR</em>, 2022.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
S. Horvath, S. Laskaridis, M. Almeida, I. Leontiadis, S. Venieris, and N. Lane, “Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout,” in <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2021.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
D. Wu, R. Ullah, P. Harvey, P. Kilpatrick, I. Spence, and B. Varghese, “Fedadapt: Adaptive offloading for iot devices in federated learning,” <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>, vol. 9, no. 21, 2022.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
M. Asad, A. Moustafa, and T. Ito, “Fedopt: Towards communication efficiency and privacy preservation in federated learning,” <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 10, no. 8, 2020. [Online]. Available: <a target="_blank" href="https://www.mdpi.com/2076-3417/10/8/2864" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/2076-3417/10/8/2864</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
L. Foundation. (2024) Onnx: Open neural network exchange. Accessed: 2024-07-05. [Online]. Available: <a target="_blank" href="https://onnx.ai" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://onnx.ai</a>

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:language" content="en"></div>

<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.14153" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.14154" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.14154">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.14154" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.14155" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 17:47:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
