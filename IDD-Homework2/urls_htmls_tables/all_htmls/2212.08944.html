<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2212.08944] Toward Data Heterogeneity of Federated Learning</title><meta property="og:description" content="Federated learning is a popular paradigm for machine learning. Ideally, federated learning works best when all clients share a similar data distribution. However, it is not always the case in the real world. Therefore,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Toward Data Heterogeneity of Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Toward Data Heterogeneity of Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2212.08944">

<!--Generated on Fri Mar  1 10:35:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on  %don’t␣want␣date␣printed .-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:144%;">Toward Data Heterogeneity of Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yuchuan Huang
<br class="ltx_break">University of Minnesota
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chen Hu
<br class="ltx_break">University of Minnesota
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text ltx_font_italic">Federated learning is a popular paradigm for machine learning. Ideally, federated learning works best when all clients share a similar data distribution. However, it is not always the case in the real world. Therefore, the topic of federated learning on heterogeneous data has gained more and more effort from both academia and industry. In this project, we first do extensive experiments to show how data skew and quantity skew will affect the performance of state-of-art federated learning algorithms. Then we propose a new algorithm FedMix which adjusts existing federated learning algorithms and we show its performance. We find that existing state-of-art algorithms such as FedProx and FedNova do not have a significant improvement in all testing cases. But by testing the existing and new algorithms, it seems that tweaking the client side is more effective than tweaking
the server side.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning is a powerful paradigm for machine learning, which enables multiple clients to collaboratively learn a shared prediction model without sending all their data to a centralized server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Owing to the increasing privacy concerns and data regulations such as GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, federated learning attempts to address the fundamental problems including isolation, ownership, and locality of data. Remote devices or siloed data centers, such as mobile phones or hospitals are considered to be application scenarios for federated learning.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A classic federated learning workflow runs in an iterative manner: (1) <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">broadcasting:</span> a central server broadcasts a model to all the clients (it chooses an ML model to be trained if it is the first iteration); (2) <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">training:</span> each client takes the model structure and state, trains the model using its local dataset and send the gradients/model update to the central server; (3) <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">averaging:</span> central server takes the gradients/model update from all clients and generates a global model update. Then the system will repeat the <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">"broadcasting-training-averaging"</span> paradigm until it achieves the desired accuracy on every client or exceeds the training round threshold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">Ideally, all the clients would better share a similar distribution on the number of samples for each label (supposing a supervised learning classification task) and each client should contain an equal amount of data and training power. Therefore, the averaged model will perform equally on every client, which makes it easier to achieve the desired accuracy on every client. However, it is not always the case in the real world<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. For example, supposing federated learning for handwritten digit recognition with clients scattered in China and U.S., given the fact that the ZIP code system in China has much higher frequency on <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S1.p3.1.m1.1a"><mn id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><cn type="integer" id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">0</cn></annotation-xml></semantics></math> than in the U.S., the models from Chinese clients are likely to have better performance on <math id="S1.p3.2.m2.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S1.p3.2.m2.1a"><mn id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><cn type="integer" id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">0</cn></annotation-xml></semantics></math> than the ones from the U.S. clients<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Once the models were averaged, it is undetermined whether the global model will: (1) lean towards better performance; (2) have an average performance; (3) lean towards worse performance. Also considering China has more mail/clients than the U.S. since China has a larger population. Whether the global model would overfit the Chinese clients’ handwritten datasets and has a low performance in U.S clients’ handwritten datasets needs investigation.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Overall, non-independent, identically (non-IID) imbalanced local data not only affects the prediction accuracy in each client as well as the global server, but also has an impact on optimization speed, and in the worst case, prevents the global model from converging. Many researchers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> have proposed different federated learning algorithms and claimed they had significant improvements in non-IID imbalanced data settings. However, other researchers who evaluated these algorithms criticized them and stated that no algorithm consistently performed the best in all settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Comprehensively evaluating state-of-art federated learning algorithms becomes crucial, as it’s the prior step of applying robust federated learning algorithms in actual applications. Also, understanding the intrinsic reasons why some algorithms overperform others will help develop advanced federated algorithms. To conclude, it’s of paramount importance to evaluate the federated learning framework in various non-IID datasets and propose statistically stable models to improve the performance of heterogeneous federated learning.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The high degree of data heterogeneity has been recognized as one of the key challenges in federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. To tackle this problem, especially to improve training efficiency and local personalized prediction, different optimization algorithms and methods have been proposed. The most famous and fundamental one is FedAvg <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. FedAvg first locally performs several epochs of stochastic gradient descent (SGD) on a small fraction of total devices and then averages the local models after they are updated to a central server. Despite its effectiveness, FedAvg has been shown to diverge empirically from a statistical perspective, in settings where the data is highly non-identically distributed across devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. To improve the stability and overall accuracy of federated learning in heterogeneous settings, FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and FedNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, which are two popular successors of FedAvg, both provide convergence guarantees. In FedProx, a proximal term is added to the local objective function to restrict the local updates to be closer to the global model. It also allows for a variable amount of work to be performed locally across devices and accounts for straggler devices. Meanwhile, FedNova uses momentum to correctly weight local models when updating the global models. This simple tweak in aggregation weights eliminates inconsistency in the solution. While the two aforementioned algorithms only vary either on the client or server side, SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, which stands for "Stochastic Controlled Averaging for Federated Learning", uses a control variable to correct for the "client-drift" in local updates and aggregate that along with the global model. This add-on term from both the client and server side promotes the algorithm to be as fast as SGD and outperforms FedAvg on non-convex experiments.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">While many of the latest federated learning algorithms tackle the problem of convergence and training efficiency, others argue that only maximizing the performance of the global model would confine the capacity of the local model to personalize. Therefore an adaptive personalized federated learning (APFL) algorithm is introduced <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In this algorithm, each client will train and maintain their local model while contributing to the global model. The personalized model, formed as a mixture of the local and global models, is controlled by the mixing coefficient <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\alpha</annotation></semantics></math> which is updated based on the correlation between the difference between the personalized and global models. While giving personalized prediction, this model also improves training accuracy and efficiency.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Even though each algorithm states that they surpass the foregoing algorithms and proves their algorithms converge in the non-IID settings theoretically. There lacks a systematic evaluation and comparisons of current state-of-art algorithms in various no-IID data scenarios. One research tried its own comprehensive data partition strategies and found none of the existing state-of-the-art FL algorithms outperforms others in all cases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This raises concerns about how to effectively and universally evaluate proposed federated learning algorithms, which becomes one part of our investigation in this research.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Besides changing the underlining algorithms, other approaches are also proposed to improve the performance of heterogeneous federated learning. For example, by creating a small subset of data that is globally shared between edge devices, the training accuracy can be increased by 30% <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. However, this method requires data transmission across networks which may not be feasible in actual applications. Due to the limitation of the scope of this project, we focus on the effect of various algorithms.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FedMix</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this project, besides evaluating the state-of-art federated learning algorithms such as FedAvg, FedProx, and FedNova in various data partition scenarios, we also propose our own federated learning algorithm FedMix based on the aforementioned algorithms. Current algorithms either change the local updating function or the global averaging function. For example, as depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 FedMix ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, FedProx (denoted as red color) only involves changes in clients while FedNova (denoted as orange color) only shows changes in servers. Inspired by SCAFFOLD, we propose a simple idea in which we combine FedProx and FedNova and name it FedMix. FedMix uses the proximal term in the local updating function which restricts the local model from deviating too much from the global model. It also considers the drift between the local models and the global model on the server side and updates the global model based on the updating weight <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\tau</annotation></semantics></math> from local models. In this way, it double utilizes the difference between the local models and the global model, and we hypothesized it would outperform the baseline models FedProx and FedNova.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2212.08944/assets/figure/algorithm.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="879" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">A summary of FL algorithms including FedAvg/FedProx/FedNova/FedMix. We use red and orange colors to mark the part specially included in FedProx and FedNova, respectively. FedMix is a combination of FedProx and FedNova. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite></span></figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Design</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this project, we discover the resilience of different federated learning algorithms on data heterogeneity. We first generate test datasets of different types of data heterogeneity. Then we test different federated learning algorithms on these datasets to observe how the overall learning process converges and how each client performs throughout the learning process.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Dataset.</span> We generate test datasets from the EMNIST dataset. The original EMNIST dataset has 62 labels (i.e., 0-9, A-Z, a-z) with 697932 training data and 116323 testing data.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Implementation.</span> We implement the experiment code in Python3.10. We use Flask for the client-server implementation and use PyTorch as the ML framework. The experiment code is publicly accessible<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/csci8980/fl</span></span></span>. The experiment environment is Ubuntu 22.04 with 64GB memory and Intel® Xeon(R) CPU E5-1620 v3 @ 3.50GHz × 8 CPU. We conduct our experiment with 10 clients.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Model.</span> We use a CNN model in the experiment. The CNN model has two convolutional layers and follows by a fully connected layer. <span id="S4.SS1.p4.1.2" class="ltx_text ltx_font_italic">ReLU</span> and max-pooling are used in between layers. The loss function is chosen to be cross entropy and the optimizer is chosen to be <span id="S4.SS1.p4.1.3" class="ltx_text ltx_font_italic">Adam</span>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Heterogeneous Data Generation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We look into two types of data heterogeneity in this project: (1) skewed label distribution and (2) skewed data amount distribution.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For skewed label distribution, we create datasets of labels of either even distribution or Zipf distribution. Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Heterogeneous Data Generation ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the percentage of each label in the two distributions. In even distribution, each label has an equal amount of training and testing data. In Zipf distribution, a few labels make up the majority of the data while the rest of the labels have only a few samples.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2212.08944/assets/figure/zipf.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="449" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.4.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.2.1" class="ltx_text" style="font-size:90%;">Label distribution in even and Zipf distribution. Distribution parameter <math id="S4.F2.2.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.F2.2.1.m1.1b"><mi id="S4.F2.2.1.m1.1.1" xref="S4.F2.2.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.F2.2.1.m1.1c"><ci id="S4.F2.2.1.m1.1.1.cmml" xref="S4.F2.2.1.m1.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.2.1.m1.1d">a</annotation></semantics></math> is set to 2 for Zipf distribution</span></figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">For skewed data amount distribution, we set the standard data amount for a client as 6000 training data and 1000 testing data. Then an abnormal client will only have 600 training data and 100 testing data.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">In summary, from a label distribution perspective, each client may have either even distribution (namely <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">even</span>) or Zipf distribution (namely <span id="S4.SS2.p4.1.2" class="ltx_text ltx_font_italic">zipf</span>); from a data amount perspective, each client may have more data amount (namely <span id="S4.SS2.p4.1.3" class="ltx_text ltx_font_italic">more</span>) or less data amount (namely <span id="S4.SS2.p4.1.4" class="ltx_text ltx_font_italic">less</span>). We mix and match the options of these two dimensions and create four types of datasets, i.e., <span id="S4.SS2.p4.1.5" class="ltx_text ltx_font_italic">even-more</span>, <span id="S4.SS2.p4.1.6" class="ltx_text ltx_font_italic">even-less</span>, <span id="S4.SS2.p4.1.7" class="ltx_text ltx_font_italic">zipf-more</span>, <span id="S4.SS2.p4.1.8" class="ltx_text ltx_font_italic">zipf-less</span>. We create 10 datasets of each type. Each dataset contains data randomly sampled from the original EMNIST dataset. For Zipf dataset, each dataset has distribution skewed on different labels, which is also randomly generated.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>FedAvg on Data Heterogeneity</h3>

<figure id="S4.F3" class="ltx_figure">
<div id="S4.F3.2" class="ltx_block">
<figure id="S4.F3.sf1" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedAvg_10_even_more.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F3.sf1.3.2" class="ltx_text" style="font-size:90%;">10 even-more clients. </span></figcaption>
</figure>
<figure id="S4.F3.sf2" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedAvg_1_zipf_more.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F3.sf2.3.2" class="ltx_text" style="font-size:90%;">1 zipf-more, 9 even-more. </span></figcaption>
</figure>
<figure id="S4.F3.sf3" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedAvg_1_even_less.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F3.sf3.3.2" class="ltx_text" style="font-size:90%;">1 even-less, 9 even-more. </span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.4.2" class="ltx_text" style="font-size:90%;">FedAvg on Data Heterogeneity.</span></figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.3 FedAvg on Data Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows how FedAvg performs under different scenarios. To set a baseline of this learning task, we first run FedAvg with 10 <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">even-more</span> clients to observe how the accuracy of each client evolves throughout the learning process. As shown in Figure <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4.3 FedAvg on Data Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>, the final accuracy range is <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.6-0.7" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">0.6</mn><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><minus id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></minus><cn type="float" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">0.6</cn><cn type="float" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.6-0.7</annotation></semantics></math> and it achieves the range after about 2-3 epochs.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">We then test FedAvg with label distribution heterogeneity. Figure <a href="#S4.F3.sf2" title="In Figure 3 ‣ 4.3 FedAvg on Data Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> shows the learning process with 1 <span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_italic">zipf-more</span> client and 9 <span id="S4.SS3.p2.2.2" class="ltx_text ltx_font_italic">even-more</span> clients. The <span id="S4.SS3.p2.2.3" class="ltx_text ltx_font_italic">zipf-more</span> client reaches a high accuracy range of <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="0.9-1.0" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">0.9</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS3.p2.1.m1.1.1.3" xref="S4.SS3.p2.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><minus id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1"></minus><cn type="float" id="S4.SS3.p2.1.m1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.2">0.9</cn><cn type="float" id="S4.SS3.p2.1.m1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">0.9-1.0</annotation></semantics></math> while the other clients stay in a range of <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="0.5-0.7" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">0.5</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS3.p2.2.m2.1.1.3" xref="S4.SS3.p2.2.m2.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><minus id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1"></minus><cn type="float" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">0.5</cn><cn type="float" id="S4.SS3.p2.2.m2.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">0.5-0.7</annotation></semantics></math>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Figure <a href="#S4.F3.sf3" title="In Figure 3 ‣ 4.3 FedAvg on Data Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a> shows the learning process with data amount heterogeneity, with 1 <span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_italic">even-less</span> client and 9 <span id="S4.SS3.p3.1.2" class="ltx_text ltx_font_italic">even-more</span> clients. As shown in the figure, the <span id="S4.SS3.p3.1.3" class="ltx_text ltx_font_italic">even-less</span> client evolves significantly slower than the other clients in the first few epochs. Eventually, they all reach an accuracy range of <math id="S4.SS3.p3.1.m1.1" class="ltx_Math" alttext="0.6-0.7" display="inline"><semantics id="S4.SS3.p3.1.m1.1a"><mrow id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mn id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">0.6</mn><mo id="S4.SS3.p3.1.m1.1.1.1" xref="S4.SS3.p3.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><minus id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1.1"></minus><cn type="float" id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">0.6</cn><cn type="float" id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">0.6-0.7</annotation></semantics></math>, which is the accuracy in the IID scenario.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Conclusion. </span> The label distribution heterogeneity does hurt the general federated learning effectiveness while the data amount heterogeneity does not affect the learning effectiveness.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.5" class="ltx_p">In the label distribution heterogeneity scenario, the <span id="S4.SS3.p5.5.1" class="ltx_text ltx_font_italic">zipf-more</span> client achieves higher accuracy than the IID clients, while the rest clients reach a lower accuracy than in the IID scenario. For the <span id="S4.SS3.p5.5.2" class="ltx_text ltx_font_italic">zipf-more</span> client, it has the same Zipf distribution for both training data and testing data. Suppose the Zipf distribution skews on label <math id="S4.SS3.p5.1.m1.1" class="ltx_Math" alttext="\zeta" display="inline"><semantics id="S4.SS3.p5.1.m1.1a"><mi id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml">ζ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><ci id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">𝜁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">\zeta</annotation></semantics></math>, the training data will have more than half of <math id="S4.SS3.p5.2.m2.1" class="ltx_Math" alttext="\zeta" display="inline"><semantics id="S4.SS3.p5.2.m2.1a"><mi id="S4.SS3.p5.2.m2.1.1" xref="S4.SS3.p5.2.m2.1.1.cmml">ζ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.2.m2.1b"><ci id="S4.SS3.p5.2.m2.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1">𝜁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.2.m2.1c">\zeta</annotation></semantics></math>, which means the training model will be fed with more <math id="S4.SS3.p5.3.m3.1" class="ltx_Math" alttext="\zeta" display="inline"><semantics id="S4.SS3.p5.3.m3.1a"><mi id="S4.SS3.p5.3.m3.1.1" xref="S4.SS3.p5.3.m3.1.1.cmml">ζ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.3.m3.1b"><ci id="S4.SS3.p5.3.m3.1.1.cmml" xref="S4.SS3.p5.3.m3.1.1">𝜁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.3.m3.1c">\zeta</annotation></semantics></math>, so the model will have a more accurate prediction on <math id="S4.SS3.p5.4.m4.1" class="ltx_Math" alttext="\zeta" display="inline"><semantics id="S4.SS3.p5.4.m4.1a"><mi id="S4.SS3.p5.4.m4.1.1" xref="S4.SS3.p5.4.m4.1.1.cmml">ζ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.4.m4.1b"><ci id="S4.SS3.p5.4.m4.1.1.cmml" xref="S4.SS3.p5.4.m4.1.1">𝜁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.4.m4.1c">\zeta</annotation></semantics></math>. Meanwhile, testing data will also have more than half of <math id="S4.SS3.p5.5.m5.1" class="ltx_Math" alttext="\zeta" display="inline"><semantics id="S4.SS3.p5.5.m5.1a"><mi id="S4.SS3.p5.5.m5.1.1" xref="S4.SS3.p5.5.m5.1.1.cmml">ζ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.5.m5.1b"><ci id="S4.SS3.p5.5.m5.1.1.cmml" xref="S4.SS3.p5.5.m5.1.1">𝜁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.5.m5.1c">\zeta</annotation></semantics></math>, which is exactly what the model good at. However, this skewed model does hurt the performance of the averaged model.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para">
<p id="S4.SS3.p6.1" class="ltx_p">In the data amount heterogeneity scenario, although the <span id="S4.SS3.p6.1.1" class="ltx_text ltx_font_italic">even-less</span> client has lower accuracy in the first few epochs, it joins the average accuracy soon. This proves one of the biggest advantages of federated learning: training models with data from different silos. Therefore the <span id="S4.SS3.p6.1.2" class="ltx_text ltx_font_italic">even-less</span> client also enjoys the knowledge learned by other clients with more data.</p>
</div>
<div id="S4.SS3.p7" class="ltx_para">
<p id="S4.SS3.p7.1" class="ltx_p">Therefore, in the rest of the experiment, we focus on the label distribution heterogeneity scenario and discover how different federated learning algorithms react to label distribution heterogeneity.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Label Distribution Heterogeneity</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.2" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-<a href="#S4.F7" title="Figure 7 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> show how FedAvg, FedNova, FedProx, and FedMix perform under different levels of label distribution heterogeneity. Generally speaking, <span id="S4.SS4.p1.2.1" class="ltx_text ltx_font_italic">zipf-more</span> clients achieve a higher accuracy range of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="0.8-1.0" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mrow id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mn id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">0.8</mn><mo id="S4.SS4.p1.1.m1.1.1.1" xref="S4.SS4.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><minus id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1.1"></minus><cn type="float" id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">0.8</cn><cn type="float" id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">0.8-1.0</annotation></semantics></math>, while <span id="S4.SS4.p1.2.2" class="ltx_text ltx_font_italic">even-more</span> clients arrive at a lower accuracy range of <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="0.5-0.7" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">0.5</mn><mo id="S4.SS4.p1.2.m2.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><minus id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1"></minus><cn type="float" id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2">0.5</cn><cn type="float" id="S4.SS4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">0.5-0.7</annotation></semantics></math>.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">Some interesting observations are summarized as follows:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">In Figure <a href="#S4.F5.sf1" title="In Figure 5 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>, some <span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">even-more</span> clients in FedNova experienced an abnormal period in the first few epochs. It is not a rare case and happened several times in our experiment. It is unclear why it happens.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.4" class="ltx_p">Although all <span id="S4.I1.i2.p1.4.1" class="ltx_text ltx_font_italic">even-more</span> clients fall into the accuracy range of <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="0.5-0.7" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml"><mn id="S4.I1.i2.p1.1.m1.1.1.2" xref="S4.I1.i2.p1.1.m1.1.1.2.cmml">0.5</mn><mo id="S4.I1.i2.p1.1.m1.1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.I1.i2.p1.1.m1.1.1.3" xref="S4.I1.i2.p1.1.m1.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><apply id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1"><minus id="S4.I1.i2.p1.1.m1.1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1.1"></minus><cn type="float" id="S4.I1.i2.p1.1.m1.1.1.2.cmml" xref="S4.I1.i2.p1.1.m1.1.1.2">0.5</cn><cn type="float" id="S4.I1.i2.p1.1.m1.1.1.3.cmml" xref="S4.I1.i2.p1.1.m1.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">0.5-0.7</annotation></semantics></math>, there is some slight difference between the algorithms. For example, FedNova barely reaches beyond <math id="S4.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.I1.i2.p1.2.m2.1a"><mn id="S4.I1.i2.p1.2.m2.1.1" xref="S4.I1.i2.p1.2.m2.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.2.m2.1b"><cn type="float" id="S4.I1.i2.p1.2.m2.1.1.cmml" xref="S4.I1.i2.p1.2.m2.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.2.m2.1c">0.7</annotation></semantics></math> throughout the learning process (see Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>), while FedProx and FedMix can often reach beyond <math id="S4.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="0.7" display="inline"><semantics id="S4.I1.i2.p1.3.m3.1a"><mn id="S4.I1.i2.p1.3.m3.1.1" xref="S4.I1.i2.p1.3.m3.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.3.m3.1b"><cn type="float" id="S4.I1.i2.p1.3.m3.1.1.cmml" xref="S4.I1.i2.p1.3.m3.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.3.m3.1c">0.7</annotation></semantics></math> even get close to <math id="S4.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="0.8" display="inline"><semantics id="S4.I1.i2.p1.4.m4.1a"><mn id="S4.I1.i2.p1.4.m4.1.1" xref="S4.I1.i2.p1.4.m4.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.4.m4.1b"><cn type="float" id="S4.I1.i2.p1.4.m4.1.1.cmml" xref="S4.I1.i2.p1.4.m4.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.4.m4.1c">0.8</annotation></semantics></math> (see Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#S4.F7" title="Figure 7 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.2" class="ltx_p">Although <span id="S4.I1.i3.p1.2.1" class="ltx_text ltx_font_italic">zipf-more</span> clients stays in a relatively high accuracy range of <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="0.8-1.0" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><mrow id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml"><mn id="S4.I1.i3.p1.1.m1.1.1.2" xref="S4.I1.i3.p1.1.m1.1.1.2.cmml">0.8</mn><mo id="S4.I1.i3.p1.1.m1.1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.I1.i3.p1.1.m1.1.1.3" xref="S4.I1.i3.p1.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><apply id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1"><minus id="S4.I1.i3.p1.1.m1.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1.1"></minus><cn type="float" id="S4.I1.i3.p1.1.m1.1.1.2.cmml" xref="S4.I1.i3.p1.1.m1.1.1.2">0.8</cn><cn type="float" id="S4.I1.i3.p1.1.m1.1.1.3.cmml" xref="S4.I1.i3.p1.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">0.8-1.0</annotation></semantics></math>, there is no trend of convergence when there are multiple clients of them. As shown in Figure <a href="#S4.F4.sf2" title="In Figure 4 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>, <a href="#S4.F4.sf3" title="In Figure 4 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(c)</span></a>, <a href="#S4.F5.sf2" title="In Figure 5 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>, <a href="#S4.F5.sf3" title="In Figure 5 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(c)</span></a>, <a href="#S4.F6.sf2" title="In Figure 6 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, <a href="#S4.F6.sf3" title="In Figure 6 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>, <a href="#S4.F7.sf2" title="In Figure 7 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a>, <a href="#S4.F7.sf3" title="In Figure 7 ‣ 4.4 Label Distribution Heterogeneity ‣ 4 Experiment ‣ Toward Data Heterogeneity of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(c)</span></a>, the accuracy of the <span id="S4.I1.i3.p1.2.2" class="ltx_text ltx_font_italic">zipf-more</span> clients keep fluctuating between <math id="S4.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="0.8-1.0" display="inline"><semantics id="S4.I1.i3.p1.2.m2.1a"><mrow id="S4.I1.i3.p1.2.m2.1.1" xref="S4.I1.i3.p1.2.m2.1.1.cmml"><mn id="S4.I1.i3.p1.2.m2.1.1.2" xref="S4.I1.i3.p1.2.m2.1.1.2.cmml">0.8</mn><mo id="S4.I1.i3.p1.2.m2.1.1.1" xref="S4.I1.i3.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.I1.i3.p1.2.m2.1.1.3" xref="S4.I1.i3.p1.2.m2.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.2.m2.1b"><apply id="S4.I1.i3.p1.2.m2.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1"><minus id="S4.I1.i3.p1.2.m2.1.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1.1"></minus><cn type="float" id="S4.I1.i3.p1.2.m2.1.1.2.cmml" xref="S4.I1.i3.p1.2.m2.1.1.2">0.8</cn><cn type="float" id="S4.I1.i3.p1.2.m2.1.1.3.cmml" xref="S4.I1.i3.p1.2.m2.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.2.m2.1c">0.8-1.0</annotation></semantics></math>.</p>
</div>
</li>
</ol>
</div>
<figure id="S4.F4" class="ltx_figure">
<div id="S4.F4.2" class="ltx_block">
<figure id="S4.F4.sf1" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedAvg_1_zipf_more.png" id="S4.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F4.sf1.3.2" class="ltx_text" style="font-size:90%;">1 zipf-more, 9 even-more. </span></figcaption>
</figure>
<figure id="S4.F4.sf2" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedAvg_5_zipf.png" id="S4.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F4.sf2.3.2" class="ltx_text" style="font-size:90%;">5 zipf-more, 5 even-more. </span></figcaption>
</figure>
<figure id="S4.F4.sf3" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedAvg_9_zipf.png" id="S4.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F4.sf3.3.2" class="ltx_text" style="font-size:90%;">9 even-less, 1 even-more. </span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">FedAvg on Label Distribution Heterogeneity.</span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure">
<div id="S4.F5.2" class="ltx_block">
<figure id="S4.F5.sf1" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedNova_1_zipf.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">1 zipf-more, 9 even-more. </span></figcaption>
</figure>
<figure id="S4.F5.sf2" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedNova_5_zipf.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">5 zipf-more, 5 even-more. </span></figcaption>
</figure>
<figure id="S4.F5.sf3" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedNova_9_zipf.png" id="S4.F5.sf3.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F5.sf3.3.2" class="ltx_text" style="font-size:90%;">9 even-less, 1 even-more. </span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.4.2" class="ltx_text" style="font-size:90%;">FedNova on Label Distribution Heterogeneity.</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div id="S4.F6.2" class="ltx_block">
<figure id="S4.F6.sf1" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedProx_1_zipf.png" id="S4.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">1 zipf-more, 9 even-more. </span></figcaption>
</figure>
<figure id="S4.F6.sf2" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedProx_5_zipf.png" id="S4.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">5 zipf-more, 5 even-more. </span></figcaption>
</figure>
<figure id="S4.F6.sf3" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedProx_9_zipf.png" id="S4.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F6.sf3.3.2" class="ltx_text" style="font-size:90%;">9 even-less, 1 even-more. </span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.4.2" class="ltx_text" style="font-size:90%;">FedProx on Label Distribution Heterogeneity.</span></figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure">
<div id="S4.F7.2" class="ltx_block">
<figure id="S4.F7.sf1" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedMix_1_zipf.png" id="S4.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">1 zipf-more, 9 even-more. </span></figcaption>
</figure>
<figure id="S4.F7.sf2" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedMix_5_zipf.png" id="S4.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">5 zipf-more, 5 even-more. </span></figcaption>
</figure>
<figure id="S4.F7.sf3" class="ltx_figure ltx_align_center"><img src="/html/2212.08944/assets/figure/FedMix_9_zipf.png" id="S4.F7.sf3.g1" class="ltx_graphics ltx_img_landscape" width="198" height="148" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F7.sf3.3.2" class="ltx_text" style="font-size:90%;">9 even-less, 1 even-more. </span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">FedMix on Label Distribution Heterogeneity.</span></figcaption>
</figure>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Experiment Discussion</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Based on the experiment result, we conclude observations as follows:</p>
<ol id="S4.I2" class="ltx_enumerate">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.1" class="ltx_p">Existing work FedNova, and FedProx do not show a significant advantage over FedAvg, at least in our test case with our ML model. This conclusion is also drawn by other researches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p">FedProx performs slightly better than FedNova, and FedAvg in our experiment. We think maybe this suggests client side tweak is more effective than server side tweak. Based on the mathematical formulation, FedProx has direct regulation between the global model and the local model while FedNova indirectly formulates the connection based on weight parameter <math id="S4.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.I2.i2.p1.1.m1.1a"><mi id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><ci id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">\tau</annotation></semantics></math>. This might be the intristic reason why tweaking local updating function is more effective.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p id="S4.I2.i3.p1.1" class="ltx_p">FedMix, as a combination of FedNova and FedProx, has performance more towards to FedProx. This may also suggest that client side tweak has a more significant effect on the performance than server side tweak.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Direction</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this project, we evaluated state-of-art federated learning algorithms FedAvg, FedProx, and FedNova on both two types of data heterogeneity: skewed label distribution and skewed data amount distribution. We also proposed our own federated learning algorithm FedMix and compared its performance with the aforementioned algorithms. We found that FedNova, and FedProx did not show a significant advantage over FedAvg even though this was stated in the proposed papers. Our implementation FedMix had slightly better performance and tweaking the client side was more effective than tweaking sever side. Our evaluation might have limitations since we only look at accuracy scores. For imbalanced/skew data, F1 scores should be an alternative reference. In the future, we will investigate more datasets and evaluation metrics. We foresee universal robust federated learning algorithms can be proposed and used in actual applications in the near future.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
The Most Populated Zip Codes in America – 24/7 Wall St.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano
Mazzocchi, Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, 1:374–388, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">Adaptive personalized federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.13461</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine Learning</span>,
14(1–2):1–210, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi,
Sebastian U Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for on-device federated
learning.

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Kelvin.

</span>
<span class="ltx_bibblock">FedMD: Heterogeneous Federated Learning via Model
Distillation.

</span>
<span class="ltx_bibblock">https://towardsdatascience.com/fedmd-heterogeneous-federated-learning-via-model-distillation-e84676183eb4,
December 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kelvin.

</span>
<span class="ltx_bibblock">Introduction to Federated Learning and Challenges.

</span>
<span class="ltx_bibblock">https://towardsdatascience.com/introduction-to-federated-learning-and-challenges-ea7e02f260ca,
November 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.

</span>
<span class="ltx_bibblock">Federated learning on non-iid data silos: An experimental study.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">2022 IEEE 38th International Conference on Data Engineering
(ICDE)</span>, pages 965–978. IEEE, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 37(3):50–60, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, 2:429–450, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Artificial intelligence and statistics</span>, pages 1273–1282.
PMLR, 2017.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Anit Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">On the convergence of federated optimization in heterogeneous
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.06127</span>, 3:3, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Paul Voigt and Axel Von dem Bussche.

</span>
<span class="ltx_bibblock">The eu general data protection regulation (gdpr).

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">A Practical Guide, 1st Ed., Cham: Springer International
Publishing</span>, 10(3152676):10–5555, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Tackling the objective inconsistency problem in heterogeneous
federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:7611–7623, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1806.00582</span>, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2212.08943" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2212.08944" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2212.08944">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2212.08944" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2212.08945" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 10:35:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
