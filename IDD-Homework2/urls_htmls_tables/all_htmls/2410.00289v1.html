<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Delving Deep into Engagement Prediction of Short Videos</title>
<!--Generated on Mon Sep 30 23:48:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Engagement Prediction Short-form Videos" lang="en" name="keywords"/>
<base href="/html/2410.00289v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S1" title="In Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S2" title="In Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3" title="In Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>SnapUGC Engagement Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.SS1" title="In 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Pilot Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.SS2" title="In 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Dataset Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.SS3" title="In 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Engagement Metrics Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4" title="In Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS1" title="In 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Problem Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS2" title="In 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comprehensive Features for Engagement Prediction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS3" title="In 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Network Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS4" title="In 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Evaluation Criteria</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5" title="In Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.SS1" title="In 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.SS2" title="In 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Engagement Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.SS3" title="In 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S6" title="In Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/dasongli1/SnapUGC_Engagement" title="">https://github.com/dasongli1/SnapUGC_Engagement</a></span></span></span>
<h1 class="ltx_title ltx_title_document">Delving Deep into Engagement Prediction of Short Videos</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Dasong Li<sup class="ltx_sup" id="id11.9.id1"><span class="ltx_text ltx_font_italic" id="id11.9.id1.1">1,</span></sup>  Wenjie Li<sup class="ltx_sup" id="id12.10.id2"><span class="ltx_text ltx_font_italic" id="id12.10.id2.1">2</span></sup> Baili Lu<sup class="ltx_sup" id="id13.11.id3"><span class="ltx_text ltx_font_italic" id="id13.11.id3.1">2</span></sup> Hongsheng Li<sup class="ltx_sup" id="id14.12.id4"><span class="ltx_text ltx_font_italic" id="id14.12.id4.1">1,3</span></sup> Sizhuo Ma<sup class="ltx_sup" id="id15.13.id5"><span class="ltx_text ltx_font_italic" id="id15.13.id5.1">2</span></sup> Gurunandan Krishnan<sup class="ltx_sup" id="id16.14.id6"><span class="ltx_text ltx_font_italic" id="id16.14.id6.1">2</span></sup> Jian Wang<sup class="ltx_sup" id="id17.15.id7"><span class="ltx_text ltx_font_italic" id="id17.15.id7.1">2,</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id18.16.id8">1</sup>MMLab
</span><span class="ltx_author_notes">First author. Main work was completed during an internship at Snap.Corresponding author</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CUHK <sup class="ltx_sup" id="id19.3.id1">2</sup>Snap Inc.
<br class="ltx_break"/><sup class="ltx_sup" id="id20.4.id2">3</sup>Centre for Perceptual and Interactive Intelligence Limited

<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id21.5.id3" style="font-size:90%;">dasongli@link.cuhk.edu.hk, jwang4@snapchat.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id22.id1">Understanding and modeling the popularity of User Generated Content (UGC) short videos on social media platforms presents a critical challenge with broad implications for content creators and recommendation systems. This study delves deep into the intricacies of predicting engagement for newly published videos with limited user interactions. Surprisingly, our findings reveal that Mean Opinion Scores from previous video quality assessment datasets do not strongly correlate with video engagement levels.
To address this, we introduce a substantial dataset comprising 90,000 real-world UGC short videos from Snapchat.
Rather than relying on view count, average watch time, or rate of likes, we propose two metrics: normalized average watch percentage (NAWP) and engagement continuation rate (ECR) to describe the engagement levels of short videos.
Comprehensive multi-modal features, including visual content, background music, and text data, are investigated to enhance engagement prediction. With the proposed dataset and two key metrics, our method demonstrates its ability to predict engagements of short videos purely from video content.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Engagement Prediction Short-form Videos
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the rapid advancement of social media, an increasing number of content creators post short videos to document and share their daily lives on streaming media platforms such as TikTok, Instagram Reels, Youtube Shorts, and Snapchat Spotlight. Simultaneously, a substantial portion of users spend a significant amount of time in consuming short videos across these platforms.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Social media platforms receive a constant stream of newly published short videos.
Therefore, it is important to determine to what extent each video should be recommended to users.
Recommending high-quality User Generated Content (UGC) videos enhances viewer engagement and consequently encourages content creators, especially novice creators. The effective dissemination of newly published videos remains a core goal of social media platforms.
However, owing to their limited user reactions, accurate recommendation of such <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">cold-start items</em> is usually a challenge. Typically, platforms would present each new video to a restricted number of users, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">e.g</em>.<span class="ltx_text" id="S1.p2.1.3"></span> one hundred.
The latent popularity of each video is estimated based on the engagement metrics such as watch times from these initial users, serving as a basis for further recommendations. The cold start problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib62" title="">62</a>]</cite> arises from the sampling bias in such limited initial interactions, resulting in noisy and inaccurate predictions of recommendation extents. This creates a negative feedback loop within the ecosystem, hindering the recommendation of high-quality videos to users.
Content creators may also face delays in gauging their videos’ popularity, slowing their adjustments based on viewer feedback and thus discouraging them from posting more quality content.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.2">
<tr class="ltx_tr" id="S1.T1.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S1.T1.2.1.1" rowspan="2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.1.1.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.2.1.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.1.2.1" style="font-size:70%;">Trained</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S1.T1.2.1.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.1.3.1" style="font-size:70%;">Correlation of different durations</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.2.2.1" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.2.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.2" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.2.2.1" style="font-size:70%;">[19, 21)</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.2.3.1" style="font-size:70%;">[29, 31)</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.2.4.1" style="font-size:70%;">[39, 41)</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.2.2.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.2.5.1" style="font-size:70%;">[49, 51)</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.3.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S1.T1.2.3.1.1" style="font-size:70%;">UVQ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.2.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a><span class="ltx_text" id="S1.T1.2.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.2.3.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S1.T1.2.3.2.1" style="font-size:70%;">UGC </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.2.3.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a><span class="ltx_text" id="S1.T1.2.3.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.3.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.3.3.1" style="font-size:70%;">0.084</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.3.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.3.4.1" style="font-size:70%;">0.156</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.3.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.3.5.1" style="font-size:70%;">0.290</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.2.3.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.3.6.1" style="font-size:70%;">0.289</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.2.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.2.4.1" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S1.T1.2.4.1.1" style="font-size:70%;">DOVER </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.2.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a><span class="ltx_text" id="S1.T1.2.4.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.2.4.2" style="padding-left:8.0pt;padding-right:8.0pt;">
<span class="ltx_text" id="S1.T1.2.4.2.1" style="font-size:70%;">LSVQ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.T1.2.4.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a><span class="ltx_text" id="S1.T1.2.4.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.2.4.3" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.4.3.1" style="font-size:70%;">0.073</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.2.4.4" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.4.4.1" style="font-size:70%;">0.148</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.2.4.5" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.4.5.1" style="font-size:70%;">0.305</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.2.4.6" style="padding-left:8.0pt;padding-right:8.0pt;"><span class="ltx_text" id="S1.T1.2.4.6.1" style="font-size:70%;">0.286</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.5.1.1" style="font-size:129%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.6.2" style="font-size:129%;">Correlation between the predicted mean opinion score (MOS) scores and average watch time. The correlations are separately calculated for videos from 4 disjoint ranges of durations. “[19, 21)” refers to the videos of durations in the range of 19s to 21s, and similarly for “[29, 31)”, “[39, 41)”, and “[49, 51)”. Small ranges are chosen to minimize the variation within each group.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Previous video quality assessment (VQA) datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>]</cite> rely on subjective scores from relatively small groups of annotators (<em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">e.g</em>.<span class="ltx_text" id="S1.p3.1.2"></span> 40). These subjective scores often exhibit biases due to raters’ diverse preferences and limited participation, which may not faithfully reflect a video’s popularity among its true audience, gauged via metrics like average watch times.
Our experiments in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">1</span></a> reveal that VQA models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite> trained on these existing datasets yield very poor correlation with the popularity of short videos. While these VQA methods mainly focus on video visuals, short video engagement can be influenced by other factors like background music, content category, title, <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">etc</em>.
Existing engagement prediction datasets such as Wu <em class="ltx_emph ltx_font_italic" id="S1.p3.1.4">et al</em>.<span class="ltx_text" id="S1.p3.1.5"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib3" title="">3</a>]</cite> focus on limited categories of longer videos, which is not suitable for studying the engagement of short-form videos across diverse categories.
Moreover, certain prerequisites <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite> for historical creator information limits their applicability to videos from new creators.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To overcome the issues encountered in previous VQA datasets, we collect a large-scale UGC short video dataset named SnapUGC, which comprises publicly accessible short videos from Snapchat Spotlight. To mitigate potential biases arising from limited number of annotators, we propose to leverage engagement data from <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">real users</em>. For quantifying engagement levels of short videos, we propose to employ two key metrics: normalized average watch percentage (NAWP) and engagement continuation rate (ECR). NAWP provides an indication of the overall engagement level for videos with different durations. Meanwhile, ECR represents the probability of watch time exceeding 5 seconds, which assesses whether the video’s outset is captivating enough to retain viewers’ interest in continuing to watch.
It is worth noting that the two metrics are derived through aggregation from more than 2000 viewers and the dataset does not contain individual viewers’ history or personal information, ensuring user privacy.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To predict engagement levels with limited user interactions, we formulate the challenge as extracting engagement solely from video content, independent of user, creator, or contextual cues. To enhance the modeling of engagement in short videos, we move beyond previous visual features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite>. Our methodology incorporates comprehensive multi-modal features such as video captioning, sound classification, titles, descriptions, and more to model the engagement levels of short videos. The seamless integration of these multi-modal features is achieved through the adoption of a cross-modal attention mechanism, enabling the harmonious fusion of visual and language-based attributes. In contrast to previous Video Quality Assessment (VQA) methods, our approach capitalizes on the incorporation of these comprehensive multi-modal features, resulting in superior performance in the engagement prediction for short videos.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The contributions of this study include:
1) We introduce a large-scale dataset to facilitate research in predicting engagement for real UGC short videos.
2) We employ two novel metrics, normalized average watch percentage and engagement continuation rate, to characterize engagement levels of short videos.
3) We investigate a diverse set of multi-modal features to strengthen the capacity of engagement prediction.
4) Using the proposed dataset and engagement metrics, our method demonstrates the ability to estimate short videos engagement in a cold start setup, highlighting its significance in the field.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Video quality assessment methods</span>
Classical VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib24" title="">24</a>]</cite> utilize handcrafted features to evaluate video quality. Given the subjectivity and complexity of video quality, handcrafted features fall short in capturing the nuances of video quality assessment. Most previous deep VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib26" title="">26</a>]</cite> follow a two-step process: they begin by extracting deep features and subsequently train a temporal regression network using these fixed features. These deep features involves per-frame semantic features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite> from image classification networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib41" title="">41</a>]</cite> trained on ImageNet-1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib7" title="">7</a>]</cite>, per-frame low-level distortion features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>]</cite> from low-level distortion recognition networks, and multi-frame semantic features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite> from action recognition networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib11" title="">11</a>]</cite> trained on Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib15" title="">15</a>]</cite>.
Gated Recurrent unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib6" title="">6</a>]</cite>, InceptionTime <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib14" title="">14</a>]</cite> and simple average operations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite> are utilized for temporal regression of Mean Opinion Scores (MOS). Recent approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite> have emerged that opt for an end-to-end methodology, jointly optimizing feature extraction and final regression. However, these aforementioned VQA methods focus on exploring visual features while disregarding the potential contributions of additional information provided by content creators, such as background sound, title, descriptions, <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">etc</em>. The underexplored domain of vision-language correspondence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib33" title="">33</a>]</cite> in video quality assessment becomes apparent.</p>
</div>
<figure class="ltx_figure" id="S2.F1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.F1.12.12">
<tr class="ltx_tr" id="S2.F1.6.6.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.1.1.1.1" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.1.1.1.1.g1" src="extracted/5891354/pics/UHA_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.2.2.2.2" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.2.2.2.2.g1" src="extracted/5891354/pics/ytAEj_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.3.3.3.3" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.3.3.3.3.g1" src="extracted/5891354/pics/Wnwb_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.4.4.4.4" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.4.4.4.4.g1" src="extracted/5891354/pics/Fzz6_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.5.5.5.5" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.5.5.5.5.g1" src="extracted/5891354/pics/wa8bf_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.6.6.6.6" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.6.6.6.6.g1" src="extracted/5891354/pics/XHI8_resize.png" width="96"/></td>
</tr>
<tr class="ltx_tr" id="S2.F1.12.12.12">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.7.7.7.1" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.7.7.7.1.g1" src="extracted/5891354/pics/uDGL_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.8.8.8.2" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.8.8.8.2.g1" src="extracted/5891354/pics/oaah_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.9.9.9.3" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.9.9.9.3.g1" src="extracted/5891354/pics/IxSc_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.10.10.10.4" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.10.10.10.4.g1" src="extracted/5891354/pics/Q2D3_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.11.11.11.5" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.11.11.11.5.g1" src="extracted/5891354/pics/7xe_resize.png" width="96"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S2.F1.12.12.12.6" style="padding-left:1.5pt;padding-right:1.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="120" id="S2.F1.12.12.12.6.g1" src="extracted/5891354/pics/3fjl_resize.png" width="96"/></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Sample frames of the short videos in our dataset. The frame samples are cropped to exclude sensitive content such as human faces and watermarks for display.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Video quality datasets</span> Early datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib8" title="">8</a>]</cite> are often designed with specialized distortions (such as noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib59" title="">59</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib60" title="">60</a>]</cite> and blur<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib20" title="">20</a>]</cite>) to facilitate the examination of low-level video consistency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib39" title="">39</a>]</cite> and quality. In contrast, more recent VQA datasets, such as KoNViD-1k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib13" title="">13</a>]</cite>, YouTube-UGC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib54" title="">54</a>]</cite>, LIVE-VQC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib40" title="">40</a>]</cite>, YT-UGC<sup class="ltx_sup" id="S2.p2.1.2">+</sup> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>]</cite>, and LSVQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>]</cite>, are introduced with the aim of characterizing the subjective quality of videos. These datasets typically involve the labeling of Mean Opinion Scores (MOS) by a relatively small group of individuals. However, a notable domain gap exists between short videos and the videos in these VQA datasets.
On social media platforms, users may swiftly skip uninteresting videos instead of watching the whole video, while annotators of VQA datasets tend to watch the entire video. This unique property of short videos introduces a discrepancy between engagement levels and previous MOS scores.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">Engagement prediction</span>
Previous datasets focus on analyzing engagement of video lectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib3" title="">3</a>]</cite> and YouTube videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite>. Regrettably, there is a scarcity of publicly available datasets specifically tailored for predicting engagement for short videos. Commonly employed metrics for video engagement include view counts, average watch time, and average watch percentage. Video duration emerges as a critical covariate affecting both average watch time and average watch percentage, as illustrated in <em class="ltx_emph ltx_font_italic" id="S2.p3.1.2">et al</em>.<span class="ltx_text" id="S2.p3.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib56" title="">56</a>]</cite>. Intuitively, longer videos are less likely to be watched in their entirety compared to shorter videos, a phenomenon attributed to the diminishing attention span of viewers. In response, Wu <em class="ltx_emph ltx_font_italic" id="S2.p3.1.4">et al</em>.<span class="ltx_text" id="S2.p3.1.5"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite> propose a relative engagement metric that accounts for varying video durations.
However, the relative engagement metric takes into account the mutual connections and ranking orders among videos with similar durations. This approach may yield unstable results in the presence of sparse or uneven distributions of average watch times, as mentioned in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>.
Zhan <em class="ltx_emph ltx_font_italic" id="S2.p3.1.6">et al</em>.<span class="ltx_text" id="S2.p3.1.7"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib56" title="">56</a>]</cite> propose to train the videos of different durations separately to remove the bias of video duration.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T2.2">
<tr class="ltx_tr" id="S2.T2.2.3">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S2.T2.2.3.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S2.T2.2.3.2"><span class="ltx_text" id="S2.T2.2.3.2.1" style="font-size:70%;">Content</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T2.2.3.3"><span class="ltx_text" id="S2.T2.2.3.3.1" style="font-size:70%;">Metrics</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.4">
<td class="ltx_td ltx_border_r" id="S2.T2.2.4.1"></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.4.2"><span class="ltx_text" id="S2.T2.2.4.2.1" style="font-size:70%;">Video</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.4.3"><span class="ltx_text" id="S2.T2.2.4.3.1" style="font-size:70%;">Audio</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T2.2.4.4"><span class="ltx_text" id="S2.T2.2.4.4.1" style="font-size:70%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.4.5"><span class="ltx_text" id="S2.T2.2.4.5.1" style="font-size:70%;">Annotators number</span></td>
<td class="ltx_td ltx_align_center" id="S2.T2.2.4.6"><span class="ltx_text" id="S2.T2.2.4.6.1" style="font-size:70%;">Metric Sources</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S2.T2.1.1.2"><span class="ltx_text" id="S2.T2.1.1.2.1" style="font-size:70%;">VQA datasets</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.3"><span class="ltx_text" id="S2.T2.1.1.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.4"><span class="ltx_text" id="S2.T2.1.1.4.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.5"><span class="ltx_text" id="S2.T2.1.1.5.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.1">
<math alttext="\leq" class="ltx_Math" display="inline" id="S2.T2.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.m1.1.1" mathsize="70%" xref="S2.T2.1.1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.m1.1b"><leq id="S2.T2.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.m1.1d">≤</annotation></semantics></math><span class="ltx_text" id="S2.T2.1.1.1.1" style="font-size:70%;"> 40</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T2.1.1.6"><span class="ltx_text" id="S2.T2.1.1.6.1" style="font-size:70%;">Labeling Scores</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.2.2">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S2.T2.2.2.2"><span class="ltx_text" id="S2.T2.2.2.2.1" style="font-size:70%;">Our datasets</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.2.3"><span class="ltx_text" id="S2.T2.2.2.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.2.4"><span class="ltx_text" id="S2.T2.2.2.4.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T2.2.2.5"><span class="ltx_text" id="S2.T2.2.2.5.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.2.1">
<math alttext="\geq" class="ltx_Math" display="inline" id="S2.T2.2.2.1.m1.1"><semantics id="S2.T2.2.2.1.m1.1a"><mo id="S2.T2.2.2.1.m1.1.1" mathsize="70%" xref="S2.T2.2.2.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.T2.2.2.1.m1.1b"><geq id="S2.T2.2.2.1.m1.1.1.cmml" xref="S2.T2.2.2.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.2.2.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S2.T2.2.2.1.m1.1d">≥</annotation></semantics></math><span class="ltx_text" id="S2.T2.2.2.1.1" style="font-size:70%;"> 2000</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T2.2.2.6"><span class="ltx_text" id="S2.T2.2.2.6.1" style="font-size:70%;">Real User Interactions</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T2.6.1.1" style="font-size:129%;">Table 2</span>: </span><span class="ltx_text" id="S2.T2.7.2" style="font-size:129%;">We provide a detailed comparison with the VQA datasts. Our dataset contains multi-modal content to better measure the quality of videos. Moreover, our metrics are derived from thousands of real-world user interactions.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>SnapUGC Engagement Dataset</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Pilot Study</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To model the engagement levels of the videos, we initially explore the use of mainstream video quality assessment (VQA) methods, commonly used for evaluating video quality. We conducted assessments using state-of-the-art video quality assessment methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite> on a collection of real-world UGC short videos sourced from Snapchat Spotlight. These VQA methods were originally pre-trained on diverse VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>]</cite>.
As shown in Wu <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">et al</em>.<span class="ltx_text" id="S3.SS1.p1.1.2"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite>, the average watch time can reflect the engagement levels of the videos with similar durations. Consequently, we conduct an evaluation aimed at evaluating the generalization capability of models trained on VQA datasets by calculating the correlation between Mean Opinion Score (MOS) and the engagement levels.
To mitigate the potential influence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite> of video duration, we categorize the real short videos into distinct groups based on their respective durations. Within each group of similar durations, we assessed the correlation between the average watch time and the predicted MOS scores for videos.
Our observation, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">1</span></a>, reveals a lack of correlation between the learned quality of pre-trained VQA methods and the engagement levels of the videos. This observation demonstrates that existing MOS scores provided by mainstream VQA datasets have difficulties in accurately reflecting the engagement levels.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Collection</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">While several previous datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib32" title="">32</a>]</cite> are proposed for applications on short videos, they do not focus on video engagement analysis.
To precisely model the engagement levels of real UGC short videos, we first collect a large-scale short video dataset, named SnapUGC. Our dataset comprises 90,000 short videos, all of which were published on Snapchat Spotlight. For each video, we have curated corresponding aggregated engagement data derived from viewing statistics.
All short videos in our dataset have a duration ranging from 10 to 60 seconds. To mitigate sampling bias from small number of views, only short videos with view numbers exceeding 2000 are selected.
The dataset is notably diverse, encompassing a wide range of video types, including Family, Food &amp; Dining, Pets, Hobbies, Travel, Music Appreciation, Sports, etc. Several frames are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S2.F1" title="Figure 1 ‣ 2 Related Works ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">1</span></a>. We provide a comprehensive comparison with traditional VQA datasets in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S2.T2" title="Table 2 ‣ 2 Related Works ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.F2.11.11">
<tr class="ltx_tr" id="S3.F2.11.11.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.1.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="S3.F2.1.1.1.1.1">
<tr class="ltx_tr" id="S3.F2.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.1.1.1.1.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="301" id="S3.F2.1.1.1.1.1.1.1.g1" src="x1.png" width="33"/></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11" style="padding-left:0.5pt;padding-right:0.5pt;">
<span class="ltx_text" id="S3.F2.11.11.11.11.11" style="font-size:50%;">
</span>
<table class="ltx_tabular ltx_align_middle" id="S3.F2.11.11.11.11.10">
<tr class="ltx_tr" id="S3.F2.4.4.4.4.3.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.2.2.2.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.2.2.2.2.1.1.1.g1" src="extracted/5891354/pics/vis0928_sparse_points_watch_2out.png" width="186"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.3.3.3.3.2.2.2" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.3.3.3.3.2.2.2.g1" src="extracted/5891354/pics/vis0928_sparse_points_percentage_2out.png" width="186"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.4.4.4.4.3.3.3" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.4.4.4.4.3.3.3.g1" src="extracted/5891354/pics/vis0928_curve2_new5_out.png" width="186"/></td>
</tr>
<tr class="ltx_tr" id="S3.F2.11.11.11.11.10.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.11.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.11.1.1" style="font-size:50%;">(a) Average watch time (AWT).</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.11.2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.11.2.1" style="font-size:50%;">(b) Average watch percentage (AWP).</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.11.3" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.11.3.1" style="font-size:50%;">(c) Fitting top 3 % of AWT.</span></td>
</tr>
<tr class="ltx_tr" id="S3.F2.7.7.7.7.6.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.5.5.5.5.4.4.1" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.5.5.5.5.4.4.1.g1" src="extracted/5891354/pics/vis0928_sparse_points_percentage2_30_2.png" width="186"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.6.6.6.6.5.5.2" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.6.6.6.6.5.5.2.g1" src="extracted/5891354/pics/vis0928_sparse_points_prob_2out.png" width="186"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.7.7.7.7.6.6.3" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.7.7.7.7.6.6.3.g1" src="extracted/5891354/pics/test_density2_out.png" width="186"/></td>
</tr>
<tr class="ltx_tr" id="S3.F2.11.11.11.11.10.12">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.12.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.12.1.1" style="font-size:50%;">(d) Normalized average watch</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.12.2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.12.2.1" style="font-size:50%;">(e) Engagement continuation rate</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.12.3" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.12.3.1" style="font-size:50%;">(f) NAWP follows a bimodal</span></td>
</tr>
<tr class="ltx_tr" id="S3.F2.8.8.8.8.7.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.8.8.8.8.7.7.2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.8.8.8.8.7.7.2.1" style="font-size:50%;">percentage (NAWP)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.8.8.8.8.7.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">
<span class="ltx_text" id="S3.F2.8.8.8.8.7.7.1.1" style="font-size:50%;">(ECR) </span><math alttext="\mathbb{P}" class="ltx_Math" display="inline" id="S3.F2.8.8.8.8.7.7.1.m1.1"><semantics id="S3.F2.8.8.8.8.7.7.1.m1.1a"><mi id="S3.F2.8.8.8.8.7.7.1.m1.1.1" mathsize="50%" xref="S3.F2.8.8.8.8.7.7.1.m1.1.1.cmml">ℙ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.8.8.8.8.7.7.1.m1.1b"><ci id="S3.F2.8.8.8.8.7.7.1.m1.1.1.cmml" xref="S3.F2.8.8.8.8.7.7.1.m1.1.1">ℙ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.8.8.8.8.7.7.1.m1.1c">\mathbb{P}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.8.8.8.8.7.7.1.m1.1d">blackboard_P</annotation></semantics></math><span class="ltx_text" id="S3.F2.8.8.8.8.7.7.1.2" style="font-size:50%;"> (watch &gt;5s).</span>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.8.8.8.8.7.7.3" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.8.8.8.8.7.7.3.1" style="font-size:50%;">distrbution.</span></td>
</tr>
<tr class="ltx_tr" id="S3.F2.11.11.11.11.10.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.9.9.9.9.8.8.1" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.9.9.9.9.8.8.1.g1" src="extracted/5891354/pics/test_density_out.png" width="186"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.10.10.10.10.9.9.2" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="141" id="S3.F2.10.10.10.10.9.9.2.g1" src="extracted/5891354/pics/vis0928_corr_prob_time_out.png" width="186"/></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.10.3" style="padding-left:0.5pt;padding-right:0.5pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="142" id="S3.F2.11.11.11.11.10.10.3.g1" src="extracted/5891354/pics/like_density.png" width="186"/></td>
</tr>
<tr class="ltx_tr" id="S3.F2.11.11.11.11.10.13">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.13.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.13.1.1" style="font-size:50%;">(g) ECR follows a bimodal</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.13.2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.13.2.1" style="font-size:50%;">(h) Correlation between ECR</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.13.3" rowspan="2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.13.3.1" style="font-size:50%;">(i) Like rate.</span></td>
</tr>
<tr class="ltx_tr" id="S3.F2.11.11.11.11.10.14">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.14.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.14.1.1" style="font-size:50%;">distribution.</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.F2.11.11.11.11.10.14.2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text" id="S3.F2.11.11.11.11.10.14.2.1" style="font-size:50%;">and NAWP.</span></td>
</tr>
</table>
</td>
</tr>
</table>
<figcaption class="ltx_caption" style="font-size:50%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.18.2.1" style="font-size:180%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.13.1" style="font-size:180%;">(a), (b), (e): The distributions of average watch time (AWT), average watch percentage (AWP) and engagement continuation rate (ECR), respectively. ECR, calculated as the probability of watch time exceeding 5 seconds: <math alttext="\mathbb{P}" class="ltx_Math" display="inline" id="S3.F2.13.1.m1.1"><semantics id="S3.F2.13.1.m1.1b"><mi id="S3.F2.13.1.m1.1.1" xref="S3.F2.13.1.m1.1.1.cmml">ℙ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.13.1.m1.1c"><ci id="S3.F2.13.1.m1.1.1.cmml" xref="S3.F2.13.1.m1.1.1">ℙ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.13.1.m1.1d">\mathbb{P}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.13.1.m1.1e">blackboard_P</annotation></semantics></math> (watch &gt; 5s), is more duration-independent.
(c): We fit top 3% of average watch times to derive a universal metric for videos of different durations. (d): Further normalization of the average time is achieved by fitting a line, resulting in the normalized average watch percentage (NAWP).
A color mapping is used to encode the distribution densities in (a), (b), (d) and (e). (f), (g): Distributions of NAWP and ECR. Both two metrics follow bimodal distribution, reflecting the unique property of user’s swiftly skipping uninteresting videos or spend relative longer time on their interesting videos in short videos platforms. (h): The strong correlation between ECR and NAWP. (i): The distribution of like rate.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Engagement Metrics Analysis</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">For short videos, there are three straightforward metrics to measure viewer engagement: view numbers, like rates, and average watch time. However, each metric has its drawbacks. View numbers can be heavily influenced by recommendation systems, leading to potential bias. Short videos created by well-known content creators may receive significantly higher view numbers compared to those of new creators. Like rates, although reflective of viewer interest, often yield extremely small and indistinguishable values across different videos, posing challenges for effective learning. A detailed study on like rates is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(i) and supplementary. Average watch time (AWT), while common, faces limitations when comparing videos of different durations.
In this section, we first analyze the distribution and drawback of AWT, and then propose normalized average watch percentage (NAWP) as a novel engagement metric. Recognizing that users swiftly navigate through uninteresting content but persist in watching engaging videos, we introduce an additional metric: engagement continuation rate (ECR). Calculated for each video, this metric represents <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.1">the proportion of viewers who watched the video for at least 5 seconds</em>. It serves as an indicator of a video’s ability to captivate viewers at the beginning. Unlike Kim <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.2">et al</em>.<span class="ltx_text" id="S3.SS3.p1.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib16" title="">16</a>]</cite> measuring entire videos’ dropout probability, ECR focuses on
he contents of first several seconds, which determines whether the users would continue to watch and substantially affects watch times.
The experiment in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.T5" title="Table 5 ‣ 5.2 Engagement Results ‣ 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a> also demonstrates the effectiveness of ECR on help learning NAWP.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.2.1">Average watch time (AWT).</span>
We analyze average watch times (AWT) of various video durations <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mi id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_d</annotation></semantics></math> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(a).
A similar metric, average watch percentage (AWP), is calculated as AWT divided by <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">italic_d</annotation></semantics></math>, and its distribution with video duration is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(b). When the AWT of a video surpasses its duration, AWP exceeds 1, signifying that the video is popular to be watched repeatedly.
Importantly, the distributions of AWT and AWP vary for different video durations, showing diverse user engagement patterns. Videos exhibit decreasing AWP as video duration increased, suggesting users’ reduced likelihood of watching longer videos, potentially a result of declining attention spans.
Due to this duration-dependent behavior, comparing the popularity of short videos with different durations using AWT or AWP is challenging. For instance, a 30-second video with an AWT of 30 seconds and a 60-second video with the same watch time tend to have different engagement levels. Similarly, a 10-second video with an AWP of 1.0 and a 30-second video with an AWP of 1.0 may differ in engagement levels, because a shorter video is easier to be fully watched.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.2"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.2.1">Normalized average watch percentage (NAWP).</span> We introduce a straightforward metric called normalized average watch percentage (NAWP) to provide a generalized measure for videos with different durations.
It is observed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(a) that the largest values under different durations align with a linear trend. Based on the observation, we make the assumption that videos with top 3% of highest AWT, regardless of their durations, are equally most popular, while videos with an average watch time of 0 seconds are deemed the least popular.
For example, a 40-second video with an AWT of 30 seconds and a 60-second video with an AWT of 40 seconds are regarded as equally most popular. Similarly, a 40-second video with an AWT of 0 seconds and a 60-second video with an AWT of 0 seconds are considered equally least popular.
The maximum average watch time <math alttext="f_{\text{max}}(d)" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.2" xref="S3.SS3.p3.1.m1.1.2.cmml"><msub id="S3.SS3.p3.1.m1.1.2.2" xref="S3.SS3.p3.1.m1.1.2.2.cmml"><mi id="S3.SS3.p3.1.m1.1.2.2.2" xref="S3.SS3.p3.1.m1.1.2.2.2.cmml">f</mi><mtext id="S3.SS3.p3.1.m1.1.2.2.3" xref="S3.SS3.p3.1.m1.1.2.2.3a.cmml">max</mtext></msub><mo id="S3.SS3.p3.1.m1.1.2.1" xref="S3.SS3.p3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.1.m1.1.2.3.2" xref="S3.SS3.p3.1.m1.1.2.cmml"><mo id="S3.SS3.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.1.m1.1.2.cmml">(</mo><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">d</mi><mo id="S3.SS3.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.2"><times id="S3.SS3.p3.1.m1.1.2.1.cmml" xref="S3.SS3.p3.1.m1.1.2.1"></times><apply id="S3.SS3.p3.1.m1.1.2.2.cmml" xref="S3.SS3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.2.2.1.cmml" xref="S3.SS3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.2.2.2.cmml" xref="S3.SS3.p3.1.m1.1.2.2.2">𝑓</ci><ci id="S3.SS3.p3.1.m1.1.2.2.3a.cmml" xref="S3.SS3.p3.1.m1.1.2.2.3"><mtext id="S3.SS3.p3.1.m1.1.2.2.3.cmml" mathsize="70%" xref="S3.SS3.p3.1.m1.1.2.2.3">max</mtext></ci></apply><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">f_{\text{max}}(d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_f start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_d )</annotation></semantics></math> for most popular videos and minimum average watch time <math alttext="f_{\text{min}}(d)" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.2" xref="S3.SS3.p3.2.m2.1.2.cmml"><msub id="S3.SS3.p3.2.m2.1.2.2" xref="S3.SS3.p3.2.m2.1.2.2.cmml"><mi id="S3.SS3.p3.2.m2.1.2.2.2" xref="S3.SS3.p3.2.m2.1.2.2.2.cmml">f</mi><mtext id="S3.SS3.p3.2.m2.1.2.2.3" xref="S3.SS3.p3.2.m2.1.2.2.3a.cmml">min</mtext></msub><mo id="S3.SS3.p3.2.m2.1.2.1" xref="S3.SS3.p3.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.2.m2.1.2.3.2" xref="S3.SS3.p3.2.m2.1.2.cmml"><mo id="S3.SS3.p3.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">d</mi><mo id="S3.SS3.p3.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.2.cmml" xref="S3.SS3.p3.2.m2.1.2"><times id="S3.SS3.p3.2.m2.1.2.1.cmml" xref="S3.SS3.p3.2.m2.1.2.1"></times><apply id="S3.SS3.p3.2.m2.1.2.2.cmml" xref="S3.SS3.p3.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.2.2.1.cmml" xref="S3.SS3.p3.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.2.2.2.cmml" xref="S3.SS3.p3.2.m2.1.2.2.2">𝑓</ci><ci id="S3.SS3.p3.2.m2.1.2.2.3a.cmml" xref="S3.SS3.p3.2.m2.1.2.2.3"><mtext id="S3.SS3.p3.2.m2.1.2.2.3.cmml" mathsize="70%" xref="S3.SS3.p3.2.m2.1.2.2.3">min</mtext></ci></apply><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">f_{\text{min}}(d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_f start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ( italic_d )</annotation></semantics></math> for the least popular videos can be modeled by two linear functions:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f_{\text{max}}(d)=0.556\times d+5.64;~{}f_{\text{min}}(d)=0." class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1"><mrow id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml">f</mi><mtext id="S3.E1.m1.3.3.1.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3a.cmml">max</mtext></msub><mo id="S3.E1.m1.3.3.1.1.1.1.2.1" xref="S3.E1.m1.3.3.1.1.1.1.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.2.3.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">d</mi><mo id="S3.E1.m1.3.3.1.1.1.1.2.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml"><mrow id="S3.E1.m1.3.3.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.cmml"><mn id="S3.E1.m1.3.3.1.1.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2.cmml">0.556</mn><mo id="S3.E1.m1.3.3.1.1.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.3.3.1.1.1.1.3.2.1.cmml">×</mo><mi id="S3.E1.m1.3.3.1.1.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3.cmml">d</mi></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.3.1" xref="S3.E1.m1.3.3.1.1.1.1.3.1.cmml">+</mo><mn id="S3.E1.m1.3.3.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.3.3.cmml">5.64</mn></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.3" rspace="0.497em" xref="S3.E1.m1.3.3.1.1.3a.cmml">;</mo><mrow id="S3.E1.m1.3.3.1.1.2.2" xref="S3.E1.m1.3.3.1.1.2.2.cmml"><mrow id="S3.E1.m1.3.3.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml"><msub id="S3.E1.m1.3.3.1.1.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.2.2.2.2.2" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2.cmml">f</mi><mtext id="S3.E1.m1.3.3.1.1.2.2.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3a.cmml">min</mtext></msub><mo id="S3.E1.m1.3.3.1.1.2.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.2.1.cmml">⁢</mo><mrow id="S3.E1.m1.3.3.1.1.2.2.2.3.2" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml"><mo id="S3.E1.m1.3.3.1.1.2.2.2.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">d</mi><mo id="S3.E1.m1.3.3.1.1.2.2.2.3.2.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.2.2.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2.2.1" xref="S3.E1.m1.3.3.1.1.2.2.1.cmml">=</mo><mn id="S3.E1.m1.3.3.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.2.2.3.cmml">0</mn></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3a.cmml" xref="S3.E1.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><eq id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1"></eq><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><times id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.1"></times><apply id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2">𝑓</ci><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.3a.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3"><mtext id="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml" mathsize="70%" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3">max</mtext></ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑑</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3"><plus id="S3.E1.m1.3.3.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.1"></plus><apply id="S3.E1.m1.3.3.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2"><times id="S3.E1.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.1"></times><cn id="S3.E1.m1.3.3.1.1.1.1.3.2.2.cmml" type="float" xref="S3.E1.m1.3.3.1.1.1.1.3.2.2">0.556</cn><ci id="S3.E1.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3.2.3">𝑑</ci></apply><cn id="S3.E1.m1.3.3.1.1.1.1.3.3.cmml" type="float" xref="S3.E1.m1.3.3.1.1.1.1.3.3">5.64</cn></apply></apply><apply id="S3.E1.m1.3.3.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2"><eq id="S3.E1.m1.3.3.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.1"></eq><apply id="S3.E1.m1.3.3.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2"><times id="S3.E1.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.1"></times><apply id="S3.E1.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.2">𝑓</ci><ci id="S3.E1.m1.3.3.1.1.2.2.2.2.3a.cmml" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3"><mtext id="S3.E1.m1.3.3.1.1.2.2.2.2.3.cmml" mathsize="70%" xref="S3.E1.m1.3.3.1.1.2.2.2.2.3">min</mtext></ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑑</ci></apply><cn id="S3.E1.m1.3.3.1.1.2.2.3.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.2.2.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">f_{\text{max}}(d)=0.556\times d+5.64;~{}f_{\text{min}}(d)=0.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">italic_f start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_d ) = 0.556 × italic_d + 5.64 ; italic_f start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ( italic_d ) = 0 .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.7"><math alttext="f_{\text{max}}(d)" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m1.1"><semantics id="S3.SS3.p3.3.m1.1a"><mrow id="S3.SS3.p3.3.m1.1.2" xref="S3.SS3.p3.3.m1.1.2.cmml"><msub id="S3.SS3.p3.3.m1.1.2.2" xref="S3.SS3.p3.3.m1.1.2.2.cmml"><mi id="S3.SS3.p3.3.m1.1.2.2.2" xref="S3.SS3.p3.3.m1.1.2.2.2.cmml">f</mi><mtext id="S3.SS3.p3.3.m1.1.2.2.3" xref="S3.SS3.p3.3.m1.1.2.2.3a.cmml">max</mtext></msub><mo id="S3.SS3.p3.3.m1.1.2.1" xref="S3.SS3.p3.3.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.3.m1.1.2.3.2" xref="S3.SS3.p3.3.m1.1.2.cmml"><mo id="S3.SS3.p3.3.m1.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.3.m1.1.2.cmml">(</mo><mi id="S3.SS3.p3.3.m1.1.1" xref="S3.SS3.p3.3.m1.1.1.cmml">d</mi><mo id="S3.SS3.p3.3.m1.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m1.1b"><apply id="S3.SS3.p3.3.m1.1.2.cmml" xref="S3.SS3.p3.3.m1.1.2"><times id="S3.SS3.p3.3.m1.1.2.1.cmml" xref="S3.SS3.p3.3.m1.1.2.1"></times><apply id="S3.SS3.p3.3.m1.1.2.2.cmml" xref="S3.SS3.p3.3.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m1.1.2.2.1.cmml" xref="S3.SS3.p3.3.m1.1.2.2">subscript</csymbol><ci id="S3.SS3.p3.3.m1.1.2.2.2.cmml" xref="S3.SS3.p3.3.m1.1.2.2.2">𝑓</ci><ci id="S3.SS3.p3.3.m1.1.2.2.3a.cmml" xref="S3.SS3.p3.3.m1.1.2.2.3"><mtext id="S3.SS3.p3.3.m1.1.2.2.3.cmml" mathsize="70%" xref="S3.SS3.p3.3.m1.1.2.2.3">max</mtext></ci></apply><ci id="S3.SS3.p3.3.m1.1.1.cmml" xref="S3.SS3.p3.3.m1.1.1">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m1.1c">f_{\text{max}}(d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m1.1d">italic_f start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_d )</annotation></semantics></math> is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(c). The NAWP for any video of <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m2.1"><semantics id="S3.SS3.p3.4.m2.1a"><mi id="S3.SS3.p3.4.m2.1.1" xref="S3.SS3.p3.4.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m2.1b"><ci id="S3.SS3.p3.4.m2.1.1.cmml" xref="S3.SS3.p3.4.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m2.1d">italic_d</annotation></semantics></math> seconds, with average watch time <math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p3.5.m3.1"><semantics id="S3.SS3.p3.5.m3.1a"><mi id="S3.SS3.p3.5.m3.1.1" xref="S3.SS3.p3.5.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m3.1b"><ci id="S3.SS3.p3.5.m3.1.1.cmml" xref="S3.SS3.p3.5.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.m3.1d">italic_t</annotation></semantics></math> is derived through normalization between <math alttext="f_{\text{min}}(d)" class="ltx_Math" display="inline" id="S3.SS3.p3.6.m4.1"><semantics id="S3.SS3.p3.6.m4.1a"><mrow id="S3.SS3.p3.6.m4.1.2" xref="S3.SS3.p3.6.m4.1.2.cmml"><msub id="S3.SS3.p3.6.m4.1.2.2" xref="S3.SS3.p3.6.m4.1.2.2.cmml"><mi id="S3.SS3.p3.6.m4.1.2.2.2" xref="S3.SS3.p3.6.m4.1.2.2.2.cmml">f</mi><mtext id="S3.SS3.p3.6.m4.1.2.2.3" xref="S3.SS3.p3.6.m4.1.2.2.3a.cmml">min</mtext></msub><mo id="S3.SS3.p3.6.m4.1.2.1" xref="S3.SS3.p3.6.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.6.m4.1.2.3.2" xref="S3.SS3.p3.6.m4.1.2.cmml"><mo id="S3.SS3.p3.6.m4.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.6.m4.1.2.cmml">(</mo><mi id="S3.SS3.p3.6.m4.1.1" xref="S3.SS3.p3.6.m4.1.1.cmml">d</mi><mo id="S3.SS3.p3.6.m4.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.6.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m4.1b"><apply id="S3.SS3.p3.6.m4.1.2.cmml" xref="S3.SS3.p3.6.m4.1.2"><times id="S3.SS3.p3.6.m4.1.2.1.cmml" xref="S3.SS3.p3.6.m4.1.2.1"></times><apply id="S3.SS3.p3.6.m4.1.2.2.cmml" xref="S3.SS3.p3.6.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m4.1.2.2.1.cmml" xref="S3.SS3.p3.6.m4.1.2.2">subscript</csymbol><ci id="S3.SS3.p3.6.m4.1.2.2.2.cmml" xref="S3.SS3.p3.6.m4.1.2.2.2">𝑓</ci><ci id="S3.SS3.p3.6.m4.1.2.2.3a.cmml" xref="S3.SS3.p3.6.m4.1.2.2.3"><mtext id="S3.SS3.p3.6.m4.1.2.2.3.cmml" mathsize="70%" xref="S3.SS3.p3.6.m4.1.2.2.3">min</mtext></ci></apply><ci id="S3.SS3.p3.6.m4.1.1.cmml" xref="S3.SS3.p3.6.m4.1.1">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m4.1c">f_{\text{min}}(d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.6.m4.1d">italic_f start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ( italic_d )</annotation></semantics></math> and <math alttext="f_{\text{max}}(d)" class="ltx_Math" display="inline" id="S3.SS3.p3.7.m5.1"><semantics id="S3.SS3.p3.7.m5.1a"><mrow id="S3.SS3.p3.7.m5.1.2" xref="S3.SS3.p3.7.m5.1.2.cmml"><msub id="S3.SS3.p3.7.m5.1.2.2" xref="S3.SS3.p3.7.m5.1.2.2.cmml"><mi id="S3.SS3.p3.7.m5.1.2.2.2" xref="S3.SS3.p3.7.m5.1.2.2.2.cmml">f</mi><mtext id="S3.SS3.p3.7.m5.1.2.2.3" xref="S3.SS3.p3.7.m5.1.2.2.3a.cmml">max</mtext></msub><mo id="S3.SS3.p3.7.m5.1.2.1" xref="S3.SS3.p3.7.m5.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.7.m5.1.2.3.2" xref="S3.SS3.p3.7.m5.1.2.cmml"><mo id="S3.SS3.p3.7.m5.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.7.m5.1.2.cmml">(</mo><mi id="S3.SS3.p3.7.m5.1.1" xref="S3.SS3.p3.7.m5.1.1.cmml">d</mi><mo id="S3.SS3.p3.7.m5.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.7.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m5.1b"><apply id="S3.SS3.p3.7.m5.1.2.cmml" xref="S3.SS3.p3.7.m5.1.2"><times id="S3.SS3.p3.7.m5.1.2.1.cmml" xref="S3.SS3.p3.7.m5.1.2.1"></times><apply id="S3.SS3.p3.7.m5.1.2.2.cmml" xref="S3.SS3.p3.7.m5.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.7.m5.1.2.2.1.cmml" xref="S3.SS3.p3.7.m5.1.2.2">subscript</csymbol><ci id="S3.SS3.p3.7.m5.1.2.2.2.cmml" xref="S3.SS3.p3.7.m5.1.2.2.2">𝑓</ci><ci id="S3.SS3.p3.7.m5.1.2.2.3a.cmml" xref="S3.SS3.p3.7.m5.1.2.2.3"><mtext id="S3.SS3.p3.7.m5.1.2.2.3.cmml" mathsize="70%" xref="S3.SS3.p3.7.m5.1.2.2.3">max</mtext></ci></apply><ci id="S3.SS3.p3.7.m5.1.1.cmml" xref="S3.SS3.p3.7.m5.1.1">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m5.1c">f_{\text{max}}(d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.7.m5.1d">italic_f start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_d )</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{NAWP}(\text{AWT},d)=\min\left(\frac{\text{AWT}-f_{\text{min}}(d)}{f_{%
\text{max}}(d)-f_{\text{min}}(d)},1\right)." class="ltx_Math" display="block" id="S3.E2.m1.8"><semantics id="S3.E2.m1.8a"><mrow id="S3.E2.m1.8.8.1" xref="S3.E2.m1.8.8.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1" xref="S3.E2.m1.8.8.1.1.cmml"><mrow id="S3.E2.m1.8.8.1.1.2" xref="S3.E2.m1.8.8.1.1.2.cmml"><mtext id="S3.E2.m1.8.8.1.1.2.2" xref="S3.E2.m1.8.8.1.1.2.2a.cmml">NAWP</mtext><mo id="S3.E2.m1.8.8.1.1.2.1" xref="S3.E2.m1.8.8.1.1.2.1.cmml">⁢</mo><mrow id="S3.E2.m1.8.8.1.1.2.3.2" xref="S3.E2.m1.8.8.1.1.2.3.1.cmml"><mo id="S3.E2.m1.8.8.1.1.2.3.2.1" stretchy="false" xref="S3.E2.m1.8.8.1.1.2.3.1.cmml">(</mo><mtext id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4a.cmml">AWT</mtext><mo id="S3.E2.m1.8.8.1.1.2.3.2.2" xref="S3.E2.m1.8.8.1.1.2.3.1.cmml">,</mo><mi id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml">d</mi><mo id="S3.E2.m1.8.8.1.1.2.3.2.3" stretchy="false" xref="S3.E2.m1.8.8.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.8.8.1.1.1" xref="S3.E2.m1.8.8.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.8.8.1.1.3.2" xref="S3.E2.m1.8.8.1.1.3.1.cmml"><mi id="S3.E2.m1.6.6" xref="S3.E2.m1.6.6.cmml">min</mi><mo id="S3.E2.m1.8.8.1.1.3.2a" xref="S3.E2.m1.8.8.1.1.3.1.cmml">⁡</mo><mrow id="S3.E2.m1.8.8.1.1.3.2.1" xref="S3.E2.m1.8.8.1.1.3.1.cmml"><mo id="S3.E2.m1.8.8.1.1.3.2.1.1" xref="S3.E2.m1.8.8.1.1.3.1.cmml">(</mo><mfrac id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mtext id="S3.E2.m1.1.1.1.3" xref="S3.E2.m1.1.1.1.3a.cmml">AWT</mtext><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml">−</mo><mrow id="S3.E2.m1.1.1.1.4" xref="S3.E2.m1.1.1.1.4.cmml"><msub id="S3.E2.m1.1.1.1.4.2" xref="S3.E2.m1.1.1.1.4.2.cmml"><mi id="S3.E2.m1.1.1.1.4.2.2" xref="S3.E2.m1.1.1.1.4.2.2.cmml">f</mi><mtext id="S3.E2.m1.1.1.1.4.2.3" xref="S3.E2.m1.1.1.1.4.2.3a.cmml">min</mtext></msub><mo id="S3.E2.m1.1.1.1.4.1" xref="S3.E2.m1.1.1.1.4.1.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.4.3.2" xref="S3.E2.m1.1.1.1.4.cmml"><mo id="S3.E2.m1.1.1.1.4.3.2.1" stretchy="false" xref="S3.E2.m1.1.1.1.4.cmml">(</mo><mi id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml">d</mi><mo id="S3.E2.m1.1.1.1.4.3.2.2" stretchy="false" xref="S3.E2.m1.1.1.1.4.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E2.m1.3.3.3" xref="S3.E2.m1.3.3.3.cmml"><mrow id="S3.E2.m1.3.3.3.4" xref="S3.E2.m1.3.3.3.4.cmml"><msub id="S3.E2.m1.3.3.3.4.2" xref="S3.E2.m1.3.3.3.4.2.cmml"><mi id="S3.E2.m1.3.3.3.4.2.2" xref="S3.E2.m1.3.3.3.4.2.2.cmml">f</mi><mtext id="S3.E2.m1.3.3.3.4.2.3" xref="S3.E2.m1.3.3.3.4.2.3a.cmml">max</mtext></msub><mo id="S3.E2.m1.3.3.3.4.1" xref="S3.E2.m1.3.3.3.4.1.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.3.4.3.2" xref="S3.E2.m1.3.3.3.4.cmml"><mo id="S3.E2.m1.3.3.3.4.3.2.1" stretchy="false" xref="S3.E2.m1.3.3.3.4.cmml">(</mo><mi id="S3.E2.m1.2.2.2.1" xref="S3.E2.m1.2.2.2.1.cmml">d</mi><mo id="S3.E2.m1.3.3.3.4.3.2.2" stretchy="false" xref="S3.E2.m1.3.3.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3.cmml">−</mo><mrow id="S3.E2.m1.3.3.3.5" xref="S3.E2.m1.3.3.3.5.cmml"><msub id="S3.E2.m1.3.3.3.5.2" xref="S3.E2.m1.3.3.3.5.2.cmml"><mi id="S3.E2.m1.3.3.3.5.2.2" xref="S3.E2.m1.3.3.3.5.2.2.cmml">f</mi><mtext id="S3.E2.m1.3.3.3.5.2.3" xref="S3.E2.m1.3.3.3.5.2.3a.cmml">min</mtext></msub><mo id="S3.E2.m1.3.3.3.5.1" xref="S3.E2.m1.3.3.3.5.1.cmml">⁢</mo><mrow id="S3.E2.m1.3.3.3.5.3.2" xref="S3.E2.m1.3.3.3.5.cmml"><mo id="S3.E2.m1.3.3.3.5.3.2.1" stretchy="false" xref="S3.E2.m1.3.3.3.5.cmml">(</mo><mi id="S3.E2.m1.3.3.3.2" xref="S3.E2.m1.3.3.3.2.cmml">d</mi><mo id="S3.E2.m1.3.3.3.5.3.2.2" stretchy="false" xref="S3.E2.m1.3.3.3.5.cmml">)</mo></mrow></mrow></mrow></mfrac><mo id="S3.E2.m1.8.8.1.1.3.2.1.2" xref="S3.E2.m1.8.8.1.1.3.1.cmml">,</mo><mn id="S3.E2.m1.7.7" xref="S3.E2.m1.7.7.cmml">1</mn><mo id="S3.E2.m1.8.8.1.1.3.2.1.3" xref="S3.E2.m1.8.8.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.8.8.1.2" lspace="0em" xref="S3.E2.m1.8.8.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.8b"><apply id="S3.E2.m1.8.8.1.1.cmml" xref="S3.E2.m1.8.8.1"><eq id="S3.E2.m1.8.8.1.1.1.cmml" xref="S3.E2.m1.8.8.1.1.1"></eq><apply id="S3.E2.m1.8.8.1.1.2.cmml" xref="S3.E2.m1.8.8.1.1.2"><times id="S3.E2.m1.8.8.1.1.2.1.cmml" xref="S3.E2.m1.8.8.1.1.2.1"></times><ci id="S3.E2.m1.8.8.1.1.2.2a.cmml" xref="S3.E2.m1.8.8.1.1.2.2"><mtext id="S3.E2.m1.8.8.1.1.2.2.cmml" xref="S3.E2.m1.8.8.1.1.2.2">NAWP</mtext></ci><interval closure="open" id="S3.E2.m1.8.8.1.1.2.3.1.cmml" xref="S3.E2.m1.8.8.1.1.2.3.2"><ci id="S3.E2.m1.4.4a.cmml" xref="S3.E2.m1.4.4"><mtext id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">AWT</mtext></ci><ci id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5">𝑑</ci></interval></apply><apply id="S3.E2.m1.8.8.1.1.3.1.cmml" xref="S3.E2.m1.8.8.1.1.3.2"><min id="S3.E2.m1.6.6.cmml" xref="S3.E2.m1.6.6"></min><apply id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3"><divide id="S3.E2.m1.3.3.4.cmml" xref="S3.E2.m1.3.3"></divide><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><minus id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"></minus><ci id="S3.E2.m1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.3"><mtext id="S3.E2.m1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.3">AWT</mtext></ci><apply id="S3.E2.m1.1.1.1.4.cmml" xref="S3.E2.m1.1.1.1.4"><times id="S3.E2.m1.1.1.1.4.1.cmml" xref="S3.E2.m1.1.1.1.4.1"></times><apply id="S3.E2.m1.1.1.1.4.2.cmml" xref="S3.E2.m1.1.1.1.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.4.2.1.cmml" xref="S3.E2.m1.1.1.1.4.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.4.2.2.cmml" xref="S3.E2.m1.1.1.1.4.2.2">𝑓</ci><ci id="S3.E2.m1.1.1.1.4.2.3a.cmml" xref="S3.E2.m1.1.1.1.4.2.3"><mtext id="S3.E2.m1.1.1.1.4.2.3.cmml" mathsize="70%" xref="S3.E2.m1.1.1.1.4.2.3">min</mtext></ci></apply><ci id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1">𝑑</ci></apply></apply><apply id="S3.E2.m1.3.3.3.cmml" xref="S3.E2.m1.3.3.3"><minus id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3"></minus><apply id="S3.E2.m1.3.3.3.4.cmml" xref="S3.E2.m1.3.3.3.4"><times id="S3.E2.m1.3.3.3.4.1.cmml" xref="S3.E2.m1.3.3.3.4.1"></times><apply id="S3.E2.m1.3.3.3.4.2.cmml" xref="S3.E2.m1.3.3.3.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.4.2.1.cmml" xref="S3.E2.m1.3.3.3.4.2">subscript</csymbol><ci id="S3.E2.m1.3.3.3.4.2.2.cmml" xref="S3.E2.m1.3.3.3.4.2.2">𝑓</ci><ci id="S3.E2.m1.3.3.3.4.2.3a.cmml" xref="S3.E2.m1.3.3.3.4.2.3"><mtext id="S3.E2.m1.3.3.3.4.2.3.cmml" mathsize="70%" xref="S3.E2.m1.3.3.3.4.2.3">max</mtext></ci></apply><ci id="S3.E2.m1.2.2.2.1.cmml" xref="S3.E2.m1.2.2.2.1">𝑑</ci></apply><apply id="S3.E2.m1.3.3.3.5.cmml" xref="S3.E2.m1.3.3.3.5"><times id="S3.E2.m1.3.3.3.5.1.cmml" xref="S3.E2.m1.3.3.3.5.1"></times><apply id="S3.E2.m1.3.3.3.5.2.cmml" xref="S3.E2.m1.3.3.3.5.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.3.5.2.1.cmml" xref="S3.E2.m1.3.3.3.5.2">subscript</csymbol><ci id="S3.E2.m1.3.3.3.5.2.2.cmml" xref="S3.E2.m1.3.3.3.5.2.2">𝑓</ci><ci id="S3.E2.m1.3.3.3.5.2.3a.cmml" xref="S3.E2.m1.3.3.3.5.2.3"><mtext id="S3.E2.m1.3.3.3.5.2.3.cmml" mathsize="70%" xref="S3.E2.m1.3.3.3.5.2.3">min</mtext></ci></apply><ci id="S3.E2.m1.3.3.3.2.cmml" xref="S3.E2.m1.3.3.3.2">𝑑</ci></apply></apply></apply><cn id="S3.E2.m1.7.7.cmml" type="integer" xref="S3.E2.m1.7.7">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.8c">\text{NAWP}(\text{AWT},d)=\min\left(\frac{\text{AWT}-f_{\text{min}}(d)}{f_{%
\text{max}}(d)-f_{\text{min}}(d)},1\right).</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.8d">NAWP ( AWT , italic_d ) = roman_min ( divide start_ARG AWT - italic_f start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ( italic_d ) end_ARG start_ARG italic_f start_POSTSUBSCRIPT max end_POSTSUBSCRIPT ( italic_d ) - italic_f start_POSTSUBSCRIPT min end_POSTSUBSCRIPT ( italic_d ) end_ARG , 1 ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.8">The relationship between the video duration and NAWP is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(d). The NAWP falls within the range of [0, 1] and NAWP of videos with top 3% average watch time is set to be 1. The experiments in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.T5" title="Table 5 ‣ 5.2 Engagement Results ‣ 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a> shows that training with NAWP achieves much better performances than AWT or AWP.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Engagement continuation rate (ECR).</span> As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(e), engagement continuation rate (ECR), calculated as <math alttext="\mathbb{P}" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">ℙ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">ℙ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\mathbb{P}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">blackboard_P</annotation></semantics></math> (watch &gt;5s), demonstrates stable behavior across different video durations. The majority of values fall within the range of [0, 0.8]. The observation aligns with the metric’s focus on frames within first 5 seconds. Furthermore, we observe a robust correlation of 0.926 between ECR and NAWP, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(h). Videos with higher probabilities of watch time surpassing 5 seconds tend to exhibit longer average watch times, illustrating a strong correlation between these two metrics. This finding offers valuable insights for designing the network structure and joint training strategy, to be shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS3" title="4.3 Network Details ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">4.3</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.T5" title="Table 5 ‣ 5.2 Engagement Results ‣ 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Bimodal distributions.</span> It is observed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(f) and (g), that distributions of NAWP and ECR exhibit a bimodal pattern. Compared with the single peak distribution of MOS scores <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib1" title="">1</a>]</cite>, this bimodal distribution is <span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.2">unique</span> to our dataset.
This behavior exists due to the common UI designs that encourages “swiping” to skip boring videos in short video platforms. Users usually quickly skip through uninteresting videos, whereas they tend to dedicate relatively longer time to engaging with videos they find interesting. Consequently, it results in two separate peaks in the distributions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p6.1.1">Generalizability of NAWP.</span>
While NAWP is designed based on the linearity observation on our SnapUGC dataset,
It is obversed in supplementary that <em class="ltx_emph ltx_font_italic" id="S3.SS3.p6.1.2">the linear approximation</em> can generalize to average watch time of <em class="ltx_emph ltx_font_italic" id="S3.SS3.p6.1.3">Kuaishou <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib56" title="">56</a>]</cite> and Youtube <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite> datasets</em> for videos with short durations (<math alttext="\leq" class="ltx_Math" display="inline" id="S3.SS3.p6.1.m1.1"><semantics id="S3.SS3.p6.1.m1.1a"><mo id="S3.SS3.p6.1.m1.1.1" xref="S3.SS3.p6.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p6.1.m1.1b"><leq id="S3.SS3.p6.1.m1.1.1.cmml" xref="S3.SS3.p6.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p6.1.m1.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p6.1.m1.1d">≤</annotation></semantics></math> 60s), which are exactly the domain of most short videos, explored in this paper.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methods</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we begin by presenting the natural bias of recommendation systems and formulate the engagement prediction. Then we conduct an in-depth exploration of the multi-modal features that aid engagement prediction in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS2" title="4.2 Comprehensive Features for Engagement Prediction ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">4.2</span></a>. In Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS3" title="4.3 Network Details ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">4.3</span></a>, we provide details about our network, and in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.SS4" title="4.4 Evaluation Criteria ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">4.4</span></a>, we outline the evaluation criteria.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="560" id="S4.F3.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">The effectiveness of comprehensive multi-modal features to enhance engagement prediction. The blue bars represent incrementally incorporating new features to achieve improved SRCC, while a gray bar indicates that the modification was not adopted. These multi-modal features incorporated into our network leads to increasingly better performance than previous VQA features.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Problem Formulation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.7">Notably, the normalized average watch percentage (NAWP) and engagement continuation rate (ECR) are contingent upon the recommendation system, denoted as <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">bold_R</annotation></semantics></math>. Recommendation systems often employ machine learning classifiers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib11" title="">11</a>]</cite> to categorize short videos and analyze user preferences based on their historical engagements with various video types. These systems balance exploitation (recommending familiar contents and familiar creators) and exploration (introducing new contents and creators) to users. Consequently, the preference distribution for a given short video may vary depending on the exploitation strategy employed by different recommendation systems. The engagement metrics are biased due to the preference distribution provided by the recommendation system <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">bold_R</annotation></semantics></math>.
Therefore, we formulate engagement prediction as a realistic conditional problem. For a given short video <math alttext="v" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">v</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_v</annotation></semantics></math> and the recommendation system <math alttext="\mathbf{R}" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">𝐑</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝐑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\mathbf{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">bold_R</annotation></semantics></math>, our network <math alttext="G" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">G</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_G</annotation></semantics></math> predicts the normalized average watch percentage <math alttext="\widehat{\text{NAWP}}" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m6.1"><semantics id="S4.SS1.p1.6.m6.1a"><mover accent="true" id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mtext id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2a.cmml">NAWP</mtext><mo id="S4.SS1.p1.6.m6.1.1.1" xref="S4.SS1.p1.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><ci id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1.1">^</ci><ci id="S4.SS1.p1.6.m6.1.1.2a.cmml" xref="S4.SS1.p1.6.m6.1.1.2"><mtext id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">NAWP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\widehat{\text{NAWP}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m6.1d">over^ start_ARG NAWP end_ARG</annotation></semantics></math> and the engagement continuation rate <math alttext="\widehat{\text{ECR}}" class="ltx_Math" display="inline" id="S4.SS1.p1.7.m7.1"><semantics id="S4.SS1.p1.7.m7.1a"><mover accent="true" id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml"><mtext id="S4.SS1.p1.7.m7.1.1.2" xref="S4.SS1.p1.7.m7.1.1.2a.cmml">ECR</mtext><mo id="S4.SS1.p1.7.m7.1.1.1" xref="S4.SS1.p1.7.m7.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><apply id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1"><ci id="S4.SS1.p1.7.m7.1.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1.1">^</ci><ci id="S4.SS1.p1.7.m7.1.1.2a.cmml" xref="S4.SS1.p1.7.m7.1.1.2"><mtext id="S4.SS1.p1.7.m7.1.1.2.cmml" xref="S4.SS1.p1.7.m7.1.1.2">ECR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">\widehat{\text{ECR}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.7.m7.1d">over^ start_ARG ECR end_ARG</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\widehat{\text{NAWP}},~{}\widehat{\text{ECR}})=G(v~{}|~{}\mathbf{R})." class="ltx_Math" display="block" id="S4.E3.m1.3"><semantics id="S4.E3.m1.3a"><mrow id="S4.E3.m1.3.3.1" xref="S4.E3.m1.3.3.1.1.cmml"><mrow id="S4.E3.m1.3.3.1.1" xref="S4.E3.m1.3.3.1.1.cmml"><mrow id="S4.E3.m1.3.3.1.1.3.2" xref="S4.E3.m1.3.3.1.1.3.1.cmml"><mo id="S4.E3.m1.3.3.1.1.3.2.1" stretchy="false" xref="S4.E3.m1.3.3.1.1.3.1.cmml">(</mo><mover accent="true" id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mtext id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2a.cmml">NAWP</mtext><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">^</mo></mover><mo id="S4.E3.m1.3.3.1.1.3.2.2" rspace="0.497em" xref="S4.E3.m1.3.3.1.1.3.1.cmml">,</mo><mover accent="true" id="S4.E3.m1.2.2" xref="S4.E3.m1.2.2.cmml"><mtext id="S4.E3.m1.2.2.2" xref="S4.E3.m1.2.2.2a.cmml">ECR</mtext><mo id="S4.E3.m1.2.2.1" xref="S4.E3.m1.2.2.1.cmml">^</mo></mover><mo id="S4.E3.m1.3.3.1.1.3.2.3" stretchy="false" xref="S4.E3.m1.3.3.1.1.3.1.cmml">)</mo></mrow><mo id="S4.E3.m1.3.3.1.1.2" xref="S4.E3.m1.3.3.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.3.3.1.1.1" xref="S4.E3.m1.3.3.1.1.1.cmml"><mi id="S4.E3.m1.3.3.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.3.cmml">G</mi><mo id="S4.E3.m1.3.3.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.3.3.1.1.1.1.1.2" stretchy="false" xref="S4.E3.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.3.3.1.1.1.1.1.1" xref="S4.E3.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.2" xref="S4.E3.m1.3.3.1.1.1.1.1.1.2.cmml">v</mi><mo fence="false" id="S4.E3.m1.3.3.1.1.1.1.1.1.1" lspace="0.608em" rspace="0.608em" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1.cmml">|</mo><mi id="S4.E3.m1.3.3.1.1.1.1.1.1.3" xref="S4.E3.m1.3.3.1.1.1.1.1.1.3.cmml">𝐑</mi></mrow><mo id="S4.E3.m1.3.3.1.1.1.1.1.3" stretchy="false" xref="S4.E3.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E3.m1.3.3.1.2" lspace="0em" xref="S4.E3.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.3b"><apply id="S4.E3.m1.3.3.1.1.cmml" xref="S4.E3.m1.3.3.1"><eq id="S4.E3.m1.3.3.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.2"></eq><interval closure="open" id="S4.E3.m1.3.3.1.1.3.1.cmml" xref="S4.E3.m1.3.3.1.1.3.2"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><ci id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1">^</ci><ci id="S4.E3.m1.1.1.2a.cmml" xref="S4.E3.m1.1.1.2"><mtext id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2">NAWP</mtext></ci></apply><apply id="S4.E3.m1.2.2.cmml" xref="S4.E3.m1.2.2"><ci id="S4.E3.m1.2.2.1.cmml" xref="S4.E3.m1.2.2.1">^</ci><ci id="S4.E3.m1.2.2.2a.cmml" xref="S4.E3.m1.2.2.2"><mtext id="S4.E3.m1.2.2.2.cmml" xref="S4.E3.m1.2.2.2">ECR</mtext></ci></apply></interval><apply id="S4.E3.m1.3.3.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1"><times id="S4.E3.m1.3.3.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.2"></times><ci id="S4.E3.m1.3.3.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.3">𝐺</ci><apply id="S4.E3.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.1">conditional</csymbol><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.2">𝑣</ci><ci id="S4.E3.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.3.3.1.1.1.1.1.1.3">𝐑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.3c">(\widehat{\text{NAWP}},~{}\widehat{\text{ECR}})=G(v~{}|~{}\mathbf{R}).</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.3d">( over^ start_ARG NAWP end_ARG , over^ start_ARG ECR end_ARG ) = italic_G ( italic_v | bold_R ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p1.8">We only focus on aggregated metric in this work as individual user’s metric is subject to legal and privacy concerns.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comprehensive Features for Engagement Prediction</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To precisely model the engagement levels of short videos, we investigate a comprehensive set of multi-modal features. The evaluation of various features is conducted using the Spearman Rank Correlation Coefficient (SRCC) of the normalized average watch percentage (NAWP). We utilize T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib34" title="">34</a>]</cite> as the text encoder to encode the text data.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.F3" title="Figure 3 ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">3</span></a>, we show the procedure and the incremental performance achieved by gradually incorporating each feature.
In particular, our exploration focuses on the following aspects:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">VQA features.</span> Building on established video quality assessment methods UVQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>]</cite> and MD-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite>, we extract per-frame semantic features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib41" title="">41</a>]</cite> per-frame distortion features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>]</cite>, and action recognition features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib11" title="">11</a>]</cite> for video clips. These features collectively offer a fundamental assessment of both content and objective quality. This baseline gives a correlation of 0.625.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Background sound.</span> Creators usually incorporate background music in short videos to enhance the atmosphere and attract viewers. We employ YAMNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib2" title="">2</a>]</cite>, a 521-class audio event classification model, to discern various types of background music. The top 5 classification results, presented as text, are then utilized as an additional network input to augment the modeling of video engagement. This improves the performance from 0.625 to 0.636.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Title and descriptions</span> are usually provided along with the short videos by the creators, which can emphasize key content and provide additional context information, enhancing the overall understanding of the videos. Incorporating the title and description leads to an increase from 0.636 to 0.651.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Video captioning.</span>
Video captioning provides fine-grained understanding of the short videos. Leveraging mid-layer features and captions generated by mPLUG-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib52" title="">52</a>]</cite> as complementary features enhances engagement predictions. The captions would also provide new insights for interpreting video popularity. The inclusion of captions increases the performance slightly to 0.657. Adding intermediate features as additional input visual features brings a significant improvements from 0.657 to 0.689.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Transcripts.</span> Ideally, transcripts would facilitate a better understanding of video content. However, our findings indicate that adding transcripts does not yield improvements. This observation can be attributed to the fact that only 30% of short videos include effective transcripts. Additionally, viewers often decide whether to continue watching based on the initial seconds, during which they only catch a small amount of the spoken content.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i6.p1.1.1">Human asethetic preference.</span>
While the semantic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib41" title="">41</a>]</cite> and action features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib11" title="">11</a>]</cite> described above contain semantic information,
they may not directly capture human reactions and feelings when watching videos. In response, Wu <em class="ltx_emph ltx_font_italic" id="S4.I1.i6.p1.1.2">et al</em>.<span class="ltx_text" id="S4.I1.i6.p1.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite> proposed a mean aesthetic option score to measure human quality opinions solely from an aesthetic perspective. Leveraging human aesthetic preferences may contribute to modeling the popularity of short videos.
Therefore, we integrate the aesthetic features extracted from pretrained models in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite>, resulting in an increase from 0.689 to 0.696.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i7.p1">
<p class="ltx_p" id="S4.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i7.p1.1.1">Visual emotion.</span> Creators often convey emotions through short videos, and these emotions can be reflected in the visual sentiment captured in individual frames.
To evaluate the potential benefits of emotion information, we employ WSCNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib36" title="">36</a>]</cite>, trained on the WEBEmo dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib31" title="">31</a>]</cite> to obtain intermediate features. The observed change from 0.696 to 0.690 suggests a limited correlation between visual sentiment and engagement levels.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="367" id="S4.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">The overview of multi-modal feature extractions. The learnable Multilayer Perceptron (MLP) to process extracted features is omitted for simplicity.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Network Details</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.17">Following MD-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite>, we split the video into several clips for efficient feature extraction.
Given a video with frame count <math alttext="M" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_M</annotation></semantics></math> and frame rate <math alttext="r" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mi id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">r</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">italic_r</annotation></semantics></math>, we create <math alttext="\frac{M}{L}" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mfrac id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mi id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">M</mi><mi id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">L</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><divide id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"></divide><ci id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">𝑀</ci><ci id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">\frac{M}{L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">divide start_ARG italic_M end_ARG start_ARG italic_L end_ARG</annotation></semantics></math> clips <math alttext="\{C_{i}\}_{i=1}^{M/L}" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.1"><semantics id="S4.SS3.p1.4.m4.1a"><msubsup id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><mrow id="S4.SS3.p1.4.m4.1.1.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.1.2.cmml"><mo id="S4.SS3.p1.4.m4.1.1.1.1.1.2" stretchy="false" xref="S4.SS3.p1.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S4.SS3.p1.4.m4.1.1.1.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.4.m4.1.1.1.1.1.1.2" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1.2.cmml">C</mi><mi id="S4.SS3.p1.4.m4.1.1.1.1.1.1.3" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS3.p1.4.m4.1.1.1.1.1.3" stretchy="false" xref="S4.SS3.p1.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS3.p1.4.m4.1.1.1.3" xref="S4.SS3.p1.4.m4.1.1.1.3.cmml"><mi id="S4.SS3.p1.4.m4.1.1.1.3.2" xref="S4.SS3.p1.4.m4.1.1.1.3.2.cmml">i</mi><mo id="S4.SS3.p1.4.m4.1.1.1.3.1" xref="S4.SS3.p1.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S4.SS3.p1.4.m4.1.1.1.3.3" xref="S4.SS3.p1.4.m4.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml"><mi id="S4.SS3.p1.4.m4.1.1.3.2" xref="S4.SS3.p1.4.m4.1.1.3.2.cmml">M</mi><mo id="S4.SS3.p1.4.m4.1.1.3.1" xref="S4.SS3.p1.4.m4.1.1.3.1.cmml">/</mo><mi id="S4.SS3.p1.4.m4.1.1.3.3" xref="S4.SS3.p1.4.m4.1.1.3.3.cmml">L</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1">superscript</csymbol><apply id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.4.m4.1.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1">subscript</csymbol><set id="S4.SS3.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.1"><apply id="S4.SS3.p1.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1.2">𝐶</ci><ci id="S4.SS3.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S4.SS3.p1.4.m4.1.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.1.3"><eq id="S4.SS3.p1.4.m4.1.1.1.3.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1.3.1"></eq><ci id="S4.SS3.p1.4.m4.1.1.1.3.2.cmml" xref="S4.SS3.p1.4.m4.1.1.1.3.2">𝑖</ci><cn id="S4.SS3.p1.4.m4.1.1.1.3.3.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS3.p1.4.m4.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3"><divide id="S4.SS3.p1.4.m4.1.1.3.1.cmml" xref="S4.SS3.p1.4.m4.1.1.3.1"></divide><ci id="S4.SS3.p1.4.m4.1.1.3.2.cmml" xref="S4.SS3.p1.4.m4.1.1.3.2">𝑀</ci><ci id="S4.SS3.p1.4.m4.1.1.3.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">\{C_{i}\}_{i=1}^{M/L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.1d">{ italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M / italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> with each clip <math alttext="C_{i}" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><msub id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><mi id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml">C</mi><mi id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2">𝐶</ci><ci id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> containing <math alttext="L" class="ltx_Math" display="inline" id="S4.SS3.p1.6.m6.1"><semantics id="S4.SS3.p1.6.m6.1a"><mi id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><ci id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.6.m6.1d">italic_L</annotation></semantics></math> frames <math alttext="\{C_{i}^{k}\}_{k=1}^{L}" class="ltx_Math" display="inline" id="S4.SS3.p1.7.m7.1"><semantics id="S4.SS3.p1.7.m7.1a"><msubsup id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml"><mrow id="S4.SS3.p1.7.m7.1.1.1.1.1" xref="S4.SS3.p1.7.m7.1.1.1.1.2.cmml"><mo id="S4.SS3.p1.7.m7.1.1.1.1.1.2" stretchy="false" xref="S4.SS3.p1.7.m7.1.1.1.1.2.cmml">{</mo><msubsup id="S4.SS3.p1.7.m7.1.1.1.1.1.1" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.2" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.2.cmml">C</mi><mi id="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.3" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.3.cmml">i</mi><mi id="S4.SS3.p1.7.m7.1.1.1.1.1.1.3" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.3.cmml">k</mi></msubsup><mo id="S4.SS3.p1.7.m7.1.1.1.1.1.3" stretchy="false" xref="S4.SS3.p1.7.m7.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS3.p1.7.m7.1.1.1.3" xref="S4.SS3.p1.7.m7.1.1.1.3.cmml"><mi id="S4.SS3.p1.7.m7.1.1.1.3.2" xref="S4.SS3.p1.7.m7.1.1.1.3.2.cmml">k</mi><mo id="S4.SS3.p1.7.m7.1.1.1.3.1" xref="S4.SS3.p1.7.m7.1.1.1.3.1.cmml">=</mo><mn id="S4.SS3.p1.7.m7.1.1.1.3.3" xref="S4.SS3.p1.7.m7.1.1.1.3.3.cmml">1</mn></mrow><mi id="S4.SS3.p1.7.m7.1.1.3" xref="S4.SS3.p1.7.m7.1.1.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.1b"><apply id="S4.SS3.p1.7.m7.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.7.m7.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1">superscript</csymbol><apply id="S4.SS3.p1.7.m7.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.7.m7.1.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1">subscript</csymbol><set id="S4.SS3.p1.7.m7.1.1.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1"><apply id="S4.SS3.p1.7.m7.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.7.m7.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1">superscript</csymbol><apply id="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.2.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.2">𝐶</ci><ci id="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.3.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S4.SS3.p1.7.m7.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.7.m7.1.1.1.1.1.1.3">𝑘</ci></apply></set><apply id="S4.SS3.p1.7.m7.1.1.1.3.cmml" xref="S4.SS3.p1.7.m7.1.1.1.3"><eq id="S4.SS3.p1.7.m7.1.1.1.3.1.cmml" xref="S4.SS3.p1.7.m7.1.1.1.3.1"></eq><ci id="S4.SS3.p1.7.m7.1.1.1.3.2.cmml" xref="S4.SS3.p1.7.m7.1.1.1.3.2">𝑘</ci><cn id="S4.SS3.p1.7.m7.1.1.1.3.3.cmml" type="integer" xref="S4.SS3.p1.7.m7.1.1.1.3.3">1</cn></apply></apply><ci id="S4.SS3.p1.7.m7.1.1.3.cmml" xref="S4.SS3.p1.7.m7.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.1c">\{C_{i}^{k}\}_{k=1}^{L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.7.m7.1d">{ italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>. Our network takes visual features and text data as inputs. The feature extraction is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.F4" title="Figure 4 ‣ 4.2 Comprehensive Features for Engagement Prediction ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">4</span></a>.
For each clip <math alttext="C_{i}" class="ltx_Math" display="inline" id="S4.SS3.p1.8.m8.1"><semantics id="S4.SS3.p1.8.m8.1a"><msub id="S4.SS3.p1.8.m8.1.1" xref="S4.SS3.p1.8.m8.1.1.cmml"><mi id="S4.SS3.p1.8.m8.1.1.2" xref="S4.SS3.p1.8.m8.1.1.2.cmml">C</mi><mi id="S4.SS3.p1.8.m8.1.1.3" xref="S4.SS3.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m8.1b"><apply id="S4.SS3.p1.8.m8.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.8.m8.1.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1">subscript</csymbol><ci id="S4.SS3.p1.8.m8.1.1.2.cmml" xref="S4.SS3.p1.8.m8.1.1.2">𝐶</ci><ci id="S4.SS3.p1.8.m8.1.1.3.cmml" xref="S4.SS3.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m8.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.8.m8.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, we extract semantic and distortion features for each of the <math alttext="L" class="ltx_Math" display="inline" id="S4.SS3.p1.9.m9.1"><semantics id="S4.SS3.p1.9.m9.1a"><mi id="S4.SS3.p1.9.m9.1.1" xref="S4.SS3.p1.9.m9.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m9.1b"><ci id="S4.SS3.p1.9.m9.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m9.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.9.m9.1d">italic_L</annotation></semantics></math> frames <math alttext="{C_{i}^{k}}" class="ltx_Math" display="inline" id="S4.SS3.p1.10.m10.1"><semantics id="S4.SS3.p1.10.m10.1a"><msubsup id="S4.SS3.p1.10.m10.1.1" xref="S4.SS3.p1.10.m10.1.1.cmml"><mi id="S4.SS3.p1.10.m10.1.1.2.2" xref="S4.SS3.p1.10.m10.1.1.2.2.cmml">C</mi><mi id="S4.SS3.p1.10.m10.1.1.2.3" xref="S4.SS3.p1.10.m10.1.1.2.3.cmml">i</mi><mi id="S4.SS3.p1.10.m10.1.1.3" xref="S4.SS3.p1.10.m10.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.10.m10.1b"><apply id="S4.SS3.p1.10.m10.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.10.m10.1.1.1.cmml" xref="S4.SS3.p1.10.m10.1.1">superscript</csymbol><apply id="S4.SS3.p1.10.m10.1.1.2.cmml" xref="S4.SS3.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.10.m10.1.1.2.1.cmml" xref="S4.SS3.p1.10.m10.1.1">subscript</csymbol><ci id="S4.SS3.p1.10.m10.1.1.2.2.cmml" xref="S4.SS3.p1.10.m10.1.1.2.2">𝐶</ci><ci id="S4.SS3.p1.10.m10.1.1.2.3.cmml" xref="S4.SS3.p1.10.m10.1.1.2.3">𝑖</ci></apply><ci id="S4.SS3.p1.10.m10.1.1.3.cmml" xref="S4.SS3.p1.10.m10.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.10.m10.1c">{C_{i}^{k}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.10.m10.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, while the entire clip <math alttext="C_{i}" class="ltx_Math" display="inline" id="S4.SS3.p1.11.m11.1"><semantics id="S4.SS3.p1.11.m11.1a"><msub id="S4.SS3.p1.11.m11.1.1" xref="S4.SS3.p1.11.m11.1.1.cmml"><mi id="S4.SS3.p1.11.m11.1.1.2" xref="S4.SS3.p1.11.m11.1.1.2.cmml">C</mi><mi id="S4.SS3.p1.11.m11.1.1.3" xref="S4.SS3.p1.11.m11.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.11.m11.1b"><apply id="S4.SS3.p1.11.m11.1.1.cmml" xref="S4.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.11.m11.1.1.1.cmml" xref="S4.SS3.p1.11.m11.1.1">subscript</csymbol><ci id="S4.SS3.p1.11.m11.1.1.2.cmml" xref="S4.SS3.p1.11.m11.1.1.2">𝐶</ci><ci id="S4.SS3.p1.11.m11.1.1.3.cmml" xref="S4.SS3.p1.11.m11.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.11.m11.1c">C_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.11.m11.1d">italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is used for action feature extraction, asethetic feature extraction and video captioning feature extraction. The text data, which include background sound classification, title, descriptions, and generated captions, are shared among all the clips. We process the visual features with learnable Multi-Layer Perceptrons (MLP) and employ cross-attention to merge visual action features with text data. Then the multi-modal features are fused by 8 MLP layers to obtain the fused features <math alttext="\{O_{i}\}_{i=1}^{M/L}" class="ltx_Math" display="inline" id="S4.SS3.p1.12.m12.1"><semantics id="S4.SS3.p1.12.m12.1a"><msubsup id="S4.SS3.p1.12.m12.1.1" xref="S4.SS3.p1.12.m12.1.1.cmml"><mrow id="S4.SS3.p1.12.m12.1.1.1.1.1" xref="S4.SS3.p1.12.m12.1.1.1.1.2.cmml"><mo id="S4.SS3.p1.12.m12.1.1.1.1.1.2" stretchy="false" xref="S4.SS3.p1.12.m12.1.1.1.1.2.cmml">{</mo><msub id="S4.SS3.p1.12.m12.1.1.1.1.1.1" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.12.m12.1.1.1.1.1.1.2" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1.2.cmml">O</mi><mi id="S4.SS3.p1.12.m12.1.1.1.1.1.1.3" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS3.p1.12.m12.1.1.1.1.1.3" stretchy="false" xref="S4.SS3.p1.12.m12.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS3.p1.12.m12.1.1.1.3" xref="S4.SS3.p1.12.m12.1.1.1.3.cmml"><mi id="S4.SS3.p1.12.m12.1.1.1.3.2" xref="S4.SS3.p1.12.m12.1.1.1.3.2.cmml">i</mi><mo id="S4.SS3.p1.12.m12.1.1.1.3.1" xref="S4.SS3.p1.12.m12.1.1.1.3.1.cmml">=</mo><mn id="S4.SS3.p1.12.m12.1.1.1.3.3" xref="S4.SS3.p1.12.m12.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S4.SS3.p1.12.m12.1.1.3" xref="S4.SS3.p1.12.m12.1.1.3.cmml"><mi id="S4.SS3.p1.12.m12.1.1.3.2" xref="S4.SS3.p1.12.m12.1.1.3.2.cmml">M</mi><mo id="S4.SS3.p1.12.m12.1.1.3.1" xref="S4.SS3.p1.12.m12.1.1.3.1.cmml">/</mo><mi id="S4.SS3.p1.12.m12.1.1.3.3" xref="S4.SS3.p1.12.m12.1.1.3.3.cmml">L</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.12.m12.1b"><apply id="S4.SS3.p1.12.m12.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.12.m12.1.1.2.cmml" xref="S4.SS3.p1.12.m12.1.1">superscript</csymbol><apply id="S4.SS3.p1.12.m12.1.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.12.m12.1.1.1.2.cmml" xref="S4.SS3.p1.12.m12.1.1">subscript</csymbol><set id="S4.SS3.p1.12.m12.1.1.1.1.2.cmml" xref="S4.SS3.p1.12.m12.1.1.1.1.1"><apply id="S4.SS3.p1.12.m12.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.12.m12.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.12.m12.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1.2">𝑂</ci><ci id="S4.SS3.p1.12.m12.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.12.m12.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S4.SS3.p1.12.m12.1.1.1.3.cmml" xref="S4.SS3.p1.12.m12.1.1.1.3"><eq id="S4.SS3.p1.12.m12.1.1.1.3.1.cmml" xref="S4.SS3.p1.12.m12.1.1.1.3.1"></eq><ci id="S4.SS3.p1.12.m12.1.1.1.3.2.cmml" xref="S4.SS3.p1.12.m12.1.1.1.3.2">𝑖</ci><cn id="S4.SS3.p1.12.m12.1.1.1.3.3.cmml" type="integer" xref="S4.SS3.p1.12.m12.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS3.p1.12.m12.1.1.3.cmml" xref="S4.SS3.p1.12.m12.1.1.3"><divide id="S4.SS3.p1.12.m12.1.1.3.1.cmml" xref="S4.SS3.p1.12.m12.1.1.3.1"></divide><ci id="S4.SS3.p1.12.m12.1.1.3.2.cmml" xref="S4.SS3.p1.12.m12.1.1.3.2">𝑀</ci><ci id="S4.SS3.p1.12.m12.1.1.3.3.cmml" xref="S4.SS3.p1.12.m12.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.12.m12.1c">\{O_{i}\}_{i=1}^{M/L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.12.m12.1d">{ italic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M / italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>.
Subsequently, we utilize a 8-layer self-attention architecture to combine the fused features <math alttext="\{O_{i}\}_{i=1}^{M/L}" class="ltx_Math" display="inline" id="S4.SS3.p1.13.m13.1"><semantics id="S4.SS3.p1.13.m13.1a"><msubsup id="S4.SS3.p1.13.m13.1.1" xref="S4.SS3.p1.13.m13.1.1.cmml"><mrow id="S4.SS3.p1.13.m13.1.1.1.1.1" xref="S4.SS3.p1.13.m13.1.1.1.1.2.cmml"><mo id="S4.SS3.p1.13.m13.1.1.1.1.1.2" stretchy="false" xref="S4.SS3.p1.13.m13.1.1.1.1.2.cmml">{</mo><msub id="S4.SS3.p1.13.m13.1.1.1.1.1.1" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.13.m13.1.1.1.1.1.1.2" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1.2.cmml">O</mi><mi id="S4.SS3.p1.13.m13.1.1.1.1.1.1.3" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS3.p1.13.m13.1.1.1.1.1.3" stretchy="false" xref="S4.SS3.p1.13.m13.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS3.p1.13.m13.1.1.1.3" xref="S4.SS3.p1.13.m13.1.1.1.3.cmml"><mi id="S4.SS3.p1.13.m13.1.1.1.3.2" xref="S4.SS3.p1.13.m13.1.1.1.3.2.cmml">i</mi><mo id="S4.SS3.p1.13.m13.1.1.1.3.1" xref="S4.SS3.p1.13.m13.1.1.1.3.1.cmml">=</mo><mn id="S4.SS3.p1.13.m13.1.1.1.3.3" xref="S4.SS3.p1.13.m13.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S4.SS3.p1.13.m13.1.1.3" xref="S4.SS3.p1.13.m13.1.1.3.cmml"><mi id="S4.SS3.p1.13.m13.1.1.3.2" xref="S4.SS3.p1.13.m13.1.1.3.2.cmml">M</mi><mo id="S4.SS3.p1.13.m13.1.1.3.1" xref="S4.SS3.p1.13.m13.1.1.3.1.cmml">/</mo><mi id="S4.SS3.p1.13.m13.1.1.3.3" xref="S4.SS3.p1.13.m13.1.1.3.3.cmml">L</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.13.m13.1b"><apply id="S4.SS3.p1.13.m13.1.1.cmml" xref="S4.SS3.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.13.m13.1.1.2.cmml" xref="S4.SS3.p1.13.m13.1.1">superscript</csymbol><apply id="S4.SS3.p1.13.m13.1.1.1.cmml" xref="S4.SS3.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.13.m13.1.1.1.2.cmml" xref="S4.SS3.p1.13.m13.1.1">subscript</csymbol><set id="S4.SS3.p1.13.m13.1.1.1.1.2.cmml" xref="S4.SS3.p1.13.m13.1.1.1.1.1"><apply id="S4.SS3.p1.13.m13.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.13.m13.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.13.m13.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1.2">𝑂</ci><ci id="S4.SS3.p1.13.m13.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.13.m13.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S4.SS3.p1.13.m13.1.1.1.3.cmml" xref="S4.SS3.p1.13.m13.1.1.1.3"><eq id="S4.SS3.p1.13.m13.1.1.1.3.1.cmml" xref="S4.SS3.p1.13.m13.1.1.1.3.1"></eq><ci id="S4.SS3.p1.13.m13.1.1.1.3.2.cmml" xref="S4.SS3.p1.13.m13.1.1.1.3.2">𝑖</ci><cn id="S4.SS3.p1.13.m13.1.1.1.3.3.cmml" type="integer" xref="S4.SS3.p1.13.m13.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS3.p1.13.m13.1.1.3.cmml" xref="S4.SS3.p1.13.m13.1.1.3"><divide id="S4.SS3.p1.13.m13.1.1.3.1.cmml" xref="S4.SS3.p1.13.m13.1.1.3.1"></divide><ci id="S4.SS3.p1.13.m13.1.1.3.2.cmml" xref="S4.SS3.p1.13.m13.1.1.3.2">𝑀</ci><ci id="S4.SS3.p1.13.m13.1.1.3.3.cmml" xref="S4.SS3.p1.13.m13.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.13.m13.1c">\{O_{i}\}_{i=1}^{M/L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.13.m13.1d">{ italic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M / italic_L end_POSTSUPERSCRIPT</annotation></semantics></math> of all the clips to obtain temporal aggregated features <math alttext="\{H_{i}\}_{i=1}^{M/L}" class="ltx_Math" display="inline" id="S4.SS3.p1.14.m14.1"><semantics id="S4.SS3.p1.14.m14.1a"><msubsup id="S4.SS3.p1.14.m14.1.1" xref="S4.SS3.p1.14.m14.1.1.cmml"><mrow id="S4.SS3.p1.14.m14.1.1.1.1.1" xref="S4.SS3.p1.14.m14.1.1.1.1.2.cmml"><mo id="S4.SS3.p1.14.m14.1.1.1.1.1.2" stretchy="false" xref="S4.SS3.p1.14.m14.1.1.1.1.2.cmml">{</mo><msub id="S4.SS3.p1.14.m14.1.1.1.1.1.1" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1.cmml"><mi id="S4.SS3.p1.14.m14.1.1.1.1.1.1.2" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1.2.cmml">H</mi><mi id="S4.SS3.p1.14.m14.1.1.1.1.1.1.3" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.SS3.p1.14.m14.1.1.1.1.1.3" stretchy="false" xref="S4.SS3.p1.14.m14.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS3.p1.14.m14.1.1.1.3" xref="S4.SS3.p1.14.m14.1.1.1.3.cmml"><mi id="S4.SS3.p1.14.m14.1.1.1.3.2" xref="S4.SS3.p1.14.m14.1.1.1.3.2.cmml">i</mi><mo id="S4.SS3.p1.14.m14.1.1.1.3.1" xref="S4.SS3.p1.14.m14.1.1.1.3.1.cmml">=</mo><mn id="S4.SS3.p1.14.m14.1.1.1.3.3" xref="S4.SS3.p1.14.m14.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S4.SS3.p1.14.m14.1.1.3" xref="S4.SS3.p1.14.m14.1.1.3.cmml"><mi id="S4.SS3.p1.14.m14.1.1.3.2" xref="S4.SS3.p1.14.m14.1.1.3.2.cmml">M</mi><mo id="S4.SS3.p1.14.m14.1.1.3.1" xref="S4.SS3.p1.14.m14.1.1.3.1.cmml">/</mo><mi id="S4.SS3.p1.14.m14.1.1.3.3" xref="S4.SS3.p1.14.m14.1.1.3.3.cmml">L</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.14.m14.1b"><apply id="S4.SS3.p1.14.m14.1.1.cmml" xref="S4.SS3.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.14.m14.1.1.2.cmml" xref="S4.SS3.p1.14.m14.1.1">superscript</csymbol><apply id="S4.SS3.p1.14.m14.1.1.1.cmml" xref="S4.SS3.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.14.m14.1.1.1.2.cmml" xref="S4.SS3.p1.14.m14.1.1">subscript</csymbol><set id="S4.SS3.p1.14.m14.1.1.1.1.2.cmml" xref="S4.SS3.p1.14.m14.1.1.1.1.1"><apply id="S4.SS3.p1.14.m14.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.14.m14.1.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.14.m14.1.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1.2">𝐻</ci><ci id="S4.SS3.p1.14.m14.1.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.14.m14.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S4.SS3.p1.14.m14.1.1.1.3.cmml" xref="S4.SS3.p1.14.m14.1.1.1.3"><eq id="S4.SS3.p1.14.m14.1.1.1.3.1.cmml" xref="S4.SS3.p1.14.m14.1.1.1.3.1"></eq><ci id="S4.SS3.p1.14.m14.1.1.1.3.2.cmml" xref="S4.SS3.p1.14.m14.1.1.1.3.2">𝑖</ci><cn id="S4.SS3.p1.14.m14.1.1.1.3.3.cmml" type="integer" xref="S4.SS3.p1.14.m14.1.1.1.3.3">1</cn></apply></apply><apply id="S4.SS3.p1.14.m14.1.1.3.cmml" xref="S4.SS3.p1.14.m14.1.1.3"><divide id="S4.SS3.p1.14.m14.1.1.3.1.cmml" xref="S4.SS3.p1.14.m14.1.1.3.1"></divide><ci id="S4.SS3.p1.14.m14.1.1.3.2.cmml" xref="S4.SS3.p1.14.m14.1.1.3.2">𝑀</ci><ci id="S4.SS3.p1.14.m14.1.1.3.3.cmml" xref="S4.SS3.p1.14.m14.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.14.m14.1c">\{H_{i}\}_{i=1}^{M/L}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.14.m14.1d">{ italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M / italic_L end_POSTSUPERSCRIPT</annotation></semantics></math>.
Finally, our network utilizes 2 MLP layers <math alttext="F_{\text{out}}^{1},F_{\text{out}}^{2}" class="ltx_Math" display="inline" id="S4.SS3.p1.15.m15.2"><semantics id="S4.SS3.p1.15.m15.2a"><mrow id="S4.SS3.p1.15.m15.2.2.2" xref="S4.SS3.p1.15.m15.2.2.3.cmml"><msubsup id="S4.SS3.p1.15.m15.1.1.1.1" xref="S4.SS3.p1.15.m15.1.1.1.1.cmml"><mi id="S4.SS3.p1.15.m15.1.1.1.1.2.2" xref="S4.SS3.p1.15.m15.1.1.1.1.2.2.cmml">F</mi><mtext id="S4.SS3.p1.15.m15.1.1.1.1.2.3" xref="S4.SS3.p1.15.m15.1.1.1.1.2.3a.cmml">out</mtext><mn id="S4.SS3.p1.15.m15.1.1.1.1.3" xref="S4.SS3.p1.15.m15.1.1.1.1.3.cmml">1</mn></msubsup><mo id="S4.SS3.p1.15.m15.2.2.2.3" xref="S4.SS3.p1.15.m15.2.2.3.cmml">,</mo><msubsup id="S4.SS3.p1.15.m15.2.2.2.2" xref="S4.SS3.p1.15.m15.2.2.2.2.cmml"><mi id="S4.SS3.p1.15.m15.2.2.2.2.2.2" xref="S4.SS3.p1.15.m15.2.2.2.2.2.2.cmml">F</mi><mtext id="S4.SS3.p1.15.m15.2.2.2.2.2.3" xref="S4.SS3.p1.15.m15.2.2.2.2.2.3a.cmml">out</mtext><mn id="S4.SS3.p1.15.m15.2.2.2.2.3" xref="S4.SS3.p1.15.m15.2.2.2.2.3.cmml">2</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.15.m15.2b"><list id="S4.SS3.p1.15.m15.2.2.3.cmml" xref="S4.SS3.p1.15.m15.2.2.2"><apply id="S4.SS3.p1.15.m15.1.1.1.1.cmml" xref="S4.SS3.p1.15.m15.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.15.m15.1.1.1.1.1.cmml" xref="S4.SS3.p1.15.m15.1.1.1.1">superscript</csymbol><apply id="S4.SS3.p1.15.m15.1.1.1.1.2.cmml" xref="S4.SS3.p1.15.m15.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.15.m15.1.1.1.1.2.1.cmml" xref="S4.SS3.p1.15.m15.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p1.15.m15.1.1.1.1.2.2.cmml" xref="S4.SS3.p1.15.m15.1.1.1.1.2.2">𝐹</ci><ci id="S4.SS3.p1.15.m15.1.1.1.1.2.3a.cmml" xref="S4.SS3.p1.15.m15.1.1.1.1.2.3"><mtext id="S4.SS3.p1.15.m15.1.1.1.1.2.3.cmml" mathsize="70%" xref="S4.SS3.p1.15.m15.1.1.1.1.2.3">out</mtext></ci></apply><cn id="S4.SS3.p1.15.m15.1.1.1.1.3.cmml" type="integer" xref="S4.SS3.p1.15.m15.1.1.1.1.3">1</cn></apply><apply id="S4.SS3.p1.15.m15.2.2.2.2.cmml" xref="S4.SS3.p1.15.m15.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p1.15.m15.2.2.2.2.1.cmml" xref="S4.SS3.p1.15.m15.2.2.2.2">superscript</csymbol><apply id="S4.SS3.p1.15.m15.2.2.2.2.2.cmml" xref="S4.SS3.p1.15.m15.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p1.15.m15.2.2.2.2.2.1.cmml" xref="S4.SS3.p1.15.m15.2.2.2.2">subscript</csymbol><ci id="S4.SS3.p1.15.m15.2.2.2.2.2.2.cmml" xref="S4.SS3.p1.15.m15.2.2.2.2.2.2">𝐹</ci><ci id="S4.SS3.p1.15.m15.2.2.2.2.2.3a.cmml" xref="S4.SS3.p1.15.m15.2.2.2.2.2.3"><mtext id="S4.SS3.p1.15.m15.2.2.2.2.2.3.cmml" mathsize="70%" xref="S4.SS3.p1.15.m15.2.2.2.2.2.3">out</mtext></ci></apply><cn id="S4.SS3.p1.15.m15.2.2.2.2.3.cmml" type="integer" xref="S4.SS3.p1.15.m15.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.15.m15.2c">F_{\text{out}}^{1},F_{\text{out}}^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.15.m15.2d">italic_F start_POSTSUBSCRIPT out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_F start_POSTSUBSCRIPT out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> to jointly predict <math alttext="\widehat{\text{NAWP}}" class="ltx_Math" display="inline" id="S4.SS3.p1.16.m16.1"><semantics id="S4.SS3.p1.16.m16.1a"><mover accent="true" id="S4.SS3.p1.16.m16.1.1" xref="S4.SS3.p1.16.m16.1.1.cmml"><mtext id="S4.SS3.p1.16.m16.1.1.2" xref="S4.SS3.p1.16.m16.1.1.2a.cmml">NAWP</mtext><mo id="S4.SS3.p1.16.m16.1.1.1" xref="S4.SS3.p1.16.m16.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.16.m16.1b"><apply id="S4.SS3.p1.16.m16.1.1.cmml" xref="S4.SS3.p1.16.m16.1.1"><ci id="S4.SS3.p1.16.m16.1.1.1.cmml" xref="S4.SS3.p1.16.m16.1.1.1">^</ci><ci id="S4.SS3.p1.16.m16.1.1.2a.cmml" xref="S4.SS3.p1.16.m16.1.1.2"><mtext id="S4.SS3.p1.16.m16.1.1.2.cmml" xref="S4.SS3.p1.16.m16.1.1.2">NAWP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.16.m16.1c">\widehat{\text{NAWP}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.16.m16.1d">over^ start_ARG NAWP end_ARG</annotation></semantics></math> and <math alttext="\widehat{\text{ECR}}" class="ltx_Math" display="inline" id="S4.SS3.p1.17.m17.1"><semantics id="S4.SS3.p1.17.m17.1a"><mover accent="true" id="S4.SS3.p1.17.m17.1.1" xref="S4.SS3.p1.17.m17.1.1.cmml"><mtext id="S4.SS3.p1.17.m17.1.1.2" xref="S4.SS3.p1.17.m17.1.1.2a.cmml">ECR</mtext><mo id="S4.SS3.p1.17.m17.1.1.1" xref="S4.SS3.p1.17.m17.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.17.m17.1b"><apply id="S4.SS3.p1.17.m17.1.1.cmml" xref="S4.SS3.p1.17.m17.1.1"><ci id="S4.SS3.p1.17.m17.1.1.1.cmml" xref="S4.SS3.p1.17.m17.1.1.1">^</ci><ci id="S4.SS3.p1.17.m17.1.1.2a.cmml" xref="S4.SS3.p1.17.m17.1.1.2"><mtext id="S4.SS3.p1.17.m17.1.1.2.cmml" xref="S4.SS3.p1.17.m17.1.1.2">ECR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.17.m17.1c">\widehat{\text{ECR}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.17.m17.1d">over^ start_ARG ECR end_ARG</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\widehat{\text{NAWP}}=\frac{L}{M}\sum_{i=1}^{M/L}F_{\text{out}}^{1}(H_{i});%
\widehat{\text{ECR}}=\frac{L}{5r}\sum_{i=1}^{5r/L}F_{\text{out}}^{2}(H_{i})," class="ltx_Math" display="block" id="S4.E4.m1.1"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1.1"><mrow id="S4.E4.m1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.3.cmml"><mrow id="S4.E4.m1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E4.m1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.3.cmml"><mtext id="S4.E4.m1.1.1.1.1.1.1.3.2" xref="S4.E4.m1.1.1.1.1.1.1.3.2a.cmml">NAWP</mtext><mo id="S4.E4.m1.1.1.1.1.1.1.3.1" xref="S4.E4.m1.1.1.1.1.1.1.3.1.cmml">^</mo></mover><mo id="S4.E4.m1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.cmml"><mfrac id="S4.E4.m1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.3.2" xref="S4.E4.m1.1.1.1.1.1.1.1.3.2.cmml">L</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.3.3.cmml">M</mi></mfrac><mo id="S4.E4.m1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.cmml"><munderover id="S4.E4.m1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">M</mi><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">/</mo><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">L</mi></mrow></munderover><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">F</mi><mtext id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3a.cmml">out</mtext><mn id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msubsup><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">H</mi><mi id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.E4.m1.1.1.1.1.2.3" xref="S4.E4.m1.1.1.1.1.3a.cmml">;</mo><mrow id="S4.E4.m1.1.1.1.1.2.2" xref="S4.E4.m1.1.1.1.1.2.2.cmml"><mover accent="true" id="S4.E4.m1.1.1.1.1.2.2.3" xref="S4.E4.m1.1.1.1.1.2.2.3.cmml"><mtext id="S4.E4.m1.1.1.1.1.2.2.3.2" xref="S4.E4.m1.1.1.1.1.2.2.3.2a.cmml">ECR</mtext><mo id="S4.E4.m1.1.1.1.1.2.2.3.1" xref="S4.E4.m1.1.1.1.1.2.2.3.1.cmml">^</mo></mover><mo id="S4.E4.m1.1.1.1.1.2.2.2" xref="S4.E4.m1.1.1.1.1.2.2.2.cmml">=</mo><mrow id="S4.E4.m1.1.1.1.1.2.2.1" xref="S4.E4.m1.1.1.1.1.2.2.1.cmml"><mfrac id="S4.E4.m1.1.1.1.1.2.2.1.3" xref="S4.E4.m1.1.1.1.1.2.2.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.2.2.1.3.2" xref="S4.E4.m1.1.1.1.1.2.2.1.3.2.cmml">L</mi><mrow id="S4.E4.m1.1.1.1.1.2.2.1.3.3" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.cmml"><mn id="S4.E4.m1.1.1.1.1.2.2.1.3.3.2" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.2.cmml">5</mn><mo id="S4.E4.m1.1.1.1.1.2.2.1.3.3.1" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.1.cmml">⁢</mo><mi id="S4.E4.m1.1.1.1.1.2.2.1.3.3.3" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.3.cmml">r</mi></mrow></mfrac><mo id="S4.E4.m1.1.1.1.1.2.2.1.2" xref="S4.E4.m1.1.1.1.1.2.2.1.2.cmml">⁢</mo><mrow id="S4.E4.m1.1.1.1.1.2.2.1.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.cmml"><munderover id="S4.E4.m1.1.1.1.1.2.2.1.1.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.cmml"><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.2" movablelimits="false" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.cmml"><mi id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.cmml"><mrow id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.cmml"><mn id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.2.cmml">5</mn><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.1.cmml">⁢</mo><mi id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.3.cmml">r</mi></mrow><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.1.cmml">/</mo><mi id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.3.cmml">L</mi></mrow></munderover><mrow id="S4.E4.m1.1.1.1.1.2.2.1.1.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.cmml"><msubsup id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml"><mi id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.2.cmml">F</mi><mtext id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.3a.cmml">out</mtext><mn id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.3.cmml">2</mn></msubsup><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.1.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml"><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.2" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml">H</mi><mi id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.3" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S4.E4.m1.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.3a.cmml" xref="S4.E4.m1.1.1.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1"><eq id="S4.E4.m1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.2"></eq><apply id="S4.E4.m1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3"><ci id="S4.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E4.m1.1.1.1.1.1.1.3.2a.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.2"><mtext id="S4.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.3.2">NAWP</mtext></ci></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1"><times id="S4.E4.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.2"></times><apply id="S4.E4.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.3"><divide id="S4.E4.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.3"></divide><ci id="S4.E4.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.3.2">𝐿</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.3.3">𝑀</ci></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1"><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3"><eq id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3"><divide id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.1"></divide><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.2">𝑀</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.2.3.3">𝐿</ci></apply></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1"><times id="S4.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.2">𝐹</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3a.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3"><mtext id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" mathsize="70%" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.2.3">out</mtext></ci></apply><cn id="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply><apply id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐻</ci><ci id="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply><apply id="S4.E4.m1.1.1.1.1.2.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2"><eq id="S4.E4.m1.1.1.1.1.2.2.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.2"></eq><apply id="S4.E4.m1.1.1.1.1.2.2.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.3"><ci id="S4.E4.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.3.1">^</ci><ci id="S4.E4.m1.1.1.1.1.2.2.3.2a.cmml" xref="S4.E4.m1.1.1.1.1.2.2.3.2"><mtext id="S4.E4.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.3.2">ECR</mtext></ci></apply><apply id="S4.E4.m1.1.1.1.1.2.2.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1"><times id="S4.E4.m1.1.1.1.1.2.2.1.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.2"></times><apply id="S4.E4.m1.1.1.1.1.2.2.1.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.3"><divide id="S4.E4.m1.1.1.1.1.2.2.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.3"></divide><ci id="S4.E4.m1.1.1.1.1.2.2.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.3.2">𝐿</ci><apply id="S4.E4.m1.1.1.1.1.2.2.1.3.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3"><times id="S4.E4.m1.1.1.1.1.2.2.1.3.3.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.1"></times><cn id="S4.E4.m1.1.1.1.1.2.2.1.3.3.2.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.2">5</cn><ci id="S4.E4.m1.1.1.1.1.2.2.1.3.3.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.3.3.3">𝑟</ci></apply></apply><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1"><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.2.2.1.1.2.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2">subscript</csymbol><sum id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.2"></sum><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3"><eq id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.1"></eq><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.2">𝑖</ci><cn id="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.3.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.2.3.3">1</cn></apply></apply><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3"><divide id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.1"></divide><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2"><times id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.1"></times><cn id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.2.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.2">5</cn><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.2.3">𝑟</ci></apply><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.2.3.3">𝐿</ci></apply></apply><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1"><times id="S4.E4.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.2"></times><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3">superscript</csymbol><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.2">𝐹</ci><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.3a.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.3"><mtext id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.3.cmml" mathsize="70%" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.2.3">out</mtext></ci></apply><cn id="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.3.cmml" type="integer" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.3.3">2</cn></apply><apply id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.2">𝐻</ci><ci id="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E4.m1.1.1.1.1.2.2.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\widehat{\text{NAWP}}=\frac{L}{M}\sum_{i=1}^{M/L}F_{\text{out}}^{1}(H_{i});%
\widehat{\text{ECR}}=\frac{L}{5r}\sum_{i=1}^{5r/L}F_{\text{out}}^{2}(H_{i}),</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.1d">over^ start_ARG NAWP end_ARG = divide start_ARG italic_L end_ARG start_ARG italic_M end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M / italic_L end_POSTSUPERSCRIPT italic_F start_POSTSUBSCRIPT out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ; over^ start_ARG ECR end_ARG = divide start_ARG italic_L end_ARG start_ARG 5 italic_r end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 5 italic_r / italic_L end_POSTSUPERSCRIPT italic_F start_POSTSUBSCRIPT out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.p1.21">where <math alttext="\widehat{\text{ECR}}" class="ltx_Math" display="inline" id="S4.SS3.p1.18.m1.1"><semantics id="S4.SS3.p1.18.m1.1a"><mover accent="true" id="S4.SS3.p1.18.m1.1.1" xref="S4.SS3.p1.18.m1.1.1.cmml"><mtext id="S4.SS3.p1.18.m1.1.1.2" xref="S4.SS3.p1.18.m1.1.1.2a.cmml">ECR</mtext><mo id="S4.SS3.p1.18.m1.1.1.1" xref="S4.SS3.p1.18.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.18.m1.1b"><apply id="S4.SS3.p1.18.m1.1.1.cmml" xref="S4.SS3.p1.18.m1.1.1"><ci id="S4.SS3.p1.18.m1.1.1.1.cmml" xref="S4.SS3.p1.18.m1.1.1.1">^</ci><ci id="S4.SS3.p1.18.m1.1.1.2a.cmml" xref="S4.SS3.p1.18.m1.1.1.2"><mtext id="S4.SS3.p1.18.m1.1.1.2.cmml" xref="S4.SS3.p1.18.m1.1.1.2">ECR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.18.m1.1c">\widehat{\text{ECR}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.18.m1.1d">over^ start_ARG ECR end_ARG</annotation></semantics></math> is derived from frames within first 5 seconds. The joint training loss <math alttext="L" class="ltx_Math" display="inline" id="S4.SS3.p1.19.m2.1"><semantics id="S4.SS3.p1.19.m2.1a"><mi id="S4.SS3.p1.19.m2.1.1" xref="S4.SS3.p1.19.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.19.m2.1b"><ci id="S4.SS3.p1.19.m2.1.1.cmml" xref="S4.SS3.p1.19.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.19.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.19.m2.1d">italic_L</annotation></semantics></math> for <math alttext="\widehat{\text{NAWP}}" class="ltx_Math" display="inline" id="S4.SS3.p1.20.m3.1"><semantics id="S4.SS3.p1.20.m3.1a"><mover accent="true" id="S4.SS3.p1.20.m3.1.1" xref="S4.SS3.p1.20.m3.1.1.cmml"><mtext id="S4.SS3.p1.20.m3.1.1.2" xref="S4.SS3.p1.20.m3.1.1.2a.cmml">NAWP</mtext><mo id="S4.SS3.p1.20.m3.1.1.1" xref="S4.SS3.p1.20.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.20.m3.1b"><apply id="S4.SS3.p1.20.m3.1.1.cmml" xref="S4.SS3.p1.20.m3.1.1"><ci id="S4.SS3.p1.20.m3.1.1.1.cmml" xref="S4.SS3.p1.20.m3.1.1.1">^</ci><ci id="S4.SS3.p1.20.m3.1.1.2a.cmml" xref="S4.SS3.p1.20.m3.1.1.2"><mtext id="S4.SS3.p1.20.m3.1.1.2.cmml" xref="S4.SS3.p1.20.m3.1.1.2">NAWP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.20.m3.1c">\widehat{\text{NAWP}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.20.m3.1d">over^ start_ARG NAWP end_ARG</annotation></semantics></math> and <math alttext="\widehat{\text{ECR}}" class="ltx_Math" display="inline" id="S4.SS3.p1.21.m4.1"><semantics id="S4.SS3.p1.21.m4.1a"><mover accent="true" id="S4.SS3.p1.21.m4.1.1" xref="S4.SS3.p1.21.m4.1.1.cmml"><mtext id="S4.SS3.p1.21.m4.1.1.2" xref="S4.SS3.p1.21.m4.1.1.2a.cmml">ECR</mtext><mo id="S4.SS3.p1.21.m4.1.1.1" xref="S4.SS3.p1.21.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.21.m4.1b"><apply id="S4.SS3.p1.21.m4.1.1.cmml" xref="S4.SS3.p1.21.m4.1.1"><ci id="S4.SS3.p1.21.m4.1.1.1.cmml" xref="S4.SS3.p1.21.m4.1.1.1">^</ci><ci id="S4.SS3.p1.21.m4.1.1.2a.cmml" xref="S4.SS3.p1.21.m4.1.1.2"><mtext id="S4.SS3.p1.21.m4.1.1.2.cmml" xref="S4.SS3.p1.21.m4.1.1.2">ECR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.21.m4.1c">\widehat{\text{ECR}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.21.m4.1d">over^ start_ARG ECR end_ARG</annotation></semantics></math> is derived:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L=||\text{NAWP}-\widehat{\text{NAWP}}||_{2}+||\text{ECR}-\widehat{\text{ECR}}|%
|_{2}." class="ltx_Math" display="block" id="S4.E5.m1.1"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.4" xref="S4.E5.m1.1.1.1.1.4.cmml">L</mi><mo id="S4.E5.m1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.cmml">=</mo><mrow id="S4.E5.m1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.2.cmml"><msub id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E5.m1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E5.m1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2a.cmml">NAWP</mtext><mo id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2a.cmml">NAWP</mtext><mo id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S4.E5.m1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E5.m1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S4.E5.m1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.2.3.cmml">+</mo><msub id="S4.E5.m1.1.1.1.1.2.2" xref="S4.E5.m1.1.1.1.1.2.2.cmml"><mrow id="S4.E5.m1.1.1.1.1.2.2.1.1" xref="S4.E5.m1.1.1.1.1.2.2.1.2.cmml"><mo id="S4.E5.m1.1.1.1.1.2.2.1.1.2" stretchy="false" xref="S4.E5.m1.1.1.1.1.2.2.1.2.1.cmml">‖</mo><mrow id="S4.E5.m1.1.1.1.1.2.2.1.1.1" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.cmml"><mtext id="S4.E5.m1.1.1.1.1.2.2.1.1.1.2" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.2a.cmml">ECR</mtext><mo id="S4.E5.m1.1.1.1.1.2.2.1.1.1.1" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml">−</mo><mover accent="true" id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.cmml"><mtext id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.2a.cmml">ECR</mtext><mo id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.1" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.1.cmml">^</mo></mover></mrow><mo id="S4.E5.m1.1.1.1.1.2.2.1.1.3" stretchy="false" xref="S4.E5.m1.1.1.1.1.2.2.1.2.1.cmml">‖</mo></mrow><mn id="S4.E5.m1.1.1.1.1.2.2.3" xref="S4.E5.m1.1.1.1.1.2.2.3.cmml">2</mn></msub></mrow></mrow><mo id="S4.E5.m1.1.1.1.2" lspace="0em" xref="S4.E5.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"><eq id="S4.E5.m1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3"></eq><ci id="S4.E5.m1.1.1.1.1.4.cmml" xref="S4.E5.m1.1.1.1.1.4">𝐿</ci><apply id="S4.E5.m1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.2"><plus id="S4.E5.m1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.2.3"></plus><apply id="S4.E5.m1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E5.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E5.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1"><minus id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2"><mtext id="S4.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.2">NAWP</mtext></ci><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3"><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2"><mtext id="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.3.2">NAWP</mtext></ci></apply></apply></apply><cn id="S4.E5.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E5.m1.1.1.1.1.1.1.3">2</cn></apply><apply id="S4.E5.m1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2">subscript</csymbol><apply id="S4.E5.m1.1.1.1.1.2.2.1.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1"><csymbol cd="latexml" id="S4.E5.m1.1.1.1.1.2.2.1.2.1.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.2">norm</csymbol><apply id="S4.E5.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1"><minus id="S4.E5.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.1"></minus><ci id="S4.E5.m1.1.1.1.1.2.2.1.1.1.2a.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.2"><mtext id="S4.E5.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.2">ECR</mtext></ci><apply id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3"><ci id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.1">^</ci><ci id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.2a.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.2"><mtext id="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2.1.1.1.3.2">ECR</mtext></ci></apply></apply></apply><cn id="S4.E5.m1.1.1.1.1.2.2.3.cmml" type="integer" xref="S4.E5.m1.1.1.1.1.2.2.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">L=||\text{NAWP}-\widehat{\text{NAWP}}||_{2}+||\text{ECR}-\widehat{\text{ECR}}|%
|_{2}.</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.1d">italic_L = | | NAWP - over^ start_ARG NAWP end_ARG | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + | | ECR - over^ start_ARG ECR end_ARG | | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">It is observed in our experiments (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.T5" title="Table 5 ‣ 5.2 Engagement Results ‣ 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a>) that training these two highly correlated metrics jointly leads to enhanced overall performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation Criteria</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.4">We evaluate our method using common criteria in Video Quality Assessment (VQA) research, including Spearman Rank Correlation Coefficient (SRCC), Pearson Linear Correlation Coefficient (PLCC) and Root Mean Square Error (RMSE) for both NAWP and ECR.
Drawing insights from the observations in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S3.F2" title="Figure 2 ‣ 3.2 Dataset Collection ‣ 3 SnapUGC Engagement Dataset ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">2</span></a>(f) and (g), we empirically note that only 10% to 20% of uploaded short videos, centered around the second peak of the bimodal distributions, emerge as popular and are prioritized by the recommendation system. Therefore, we consider the top <math alttext="K" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_K</annotation></semantics></math>% of <math alttext="N" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">italic_N</annotation></semantics></math> test videos with the highest <span class="ltx_text ltx_markedasmath" id="S4.SS4.p1.4.1">NAWP</span>. For these selected videos <math alttext="\{v_{j}\}_{j=0}^{K\times N/100}" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><msubsup id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mrow id="S4.SS4.p1.4.m4.1.1.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.2.cmml"><mo id="S4.SS4.p1.4.m4.1.1.1.1.1.2" stretchy="false" xref="S4.SS4.p1.4.m4.1.1.1.1.2.cmml">{</mo><msub id="S4.SS4.p1.4.m4.1.1.1.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.1.1.1.1.2" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1.2.cmml">v</mi><mi id="S4.SS4.p1.4.m4.1.1.1.1.1.1.3" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1.3.cmml">j</mi></msub><mo id="S4.SS4.p1.4.m4.1.1.1.1.1.3" stretchy="false" xref="S4.SS4.p1.4.m4.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S4.SS4.p1.4.m4.1.1.1.3" xref="S4.SS4.p1.4.m4.1.1.1.3.cmml"><mi id="S4.SS4.p1.4.m4.1.1.1.3.2" xref="S4.SS4.p1.4.m4.1.1.1.3.2.cmml">j</mi><mo id="S4.SS4.p1.4.m4.1.1.1.3.1" xref="S4.SS4.p1.4.m4.1.1.1.3.1.cmml">=</mo><mn id="S4.SS4.p1.4.m4.1.1.1.3.3" xref="S4.SS4.p1.4.m4.1.1.1.3.3.cmml">0</mn></mrow><mrow id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml"><mrow id="S4.SS4.p1.4.m4.1.1.3.2" xref="S4.SS4.p1.4.m4.1.1.3.2.cmml"><mi id="S4.SS4.p1.4.m4.1.1.3.2.2" xref="S4.SS4.p1.4.m4.1.1.3.2.2.cmml">K</mi><mo id="S4.SS4.p1.4.m4.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS4.p1.4.m4.1.1.3.2.1.cmml">×</mo><mi id="S4.SS4.p1.4.m4.1.1.3.2.3" xref="S4.SS4.p1.4.m4.1.1.3.2.3.cmml">N</mi></mrow><mo id="S4.SS4.p1.4.m4.1.1.3.1" xref="S4.SS4.p1.4.m4.1.1.3.1.cmml">/</mo><mn id="S4.SS4.p1.4.m4.1.1.3.3" xref="S4.SS4.p1.4.m4.1.1.3.3.cmml">100</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1">superscript</csymbol><apply id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1">subscript</csymbol><set id="S4.SS4.p1.4.m4.1.1.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1"><apply id="S4.SS4.p1.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1.2">𝑣</ci><ci id="S4.SS4.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.1.1.1.1.3">𝑗</ci></apply></set><apply id="S4.SS4.p1.4.m4.1.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.1.3"><eq id="S4.SS4.p1.4.m4.1.1.1.3.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1.3.1"></eq><ci id="S4.SS4.p1.4.m4.1.1.1.3.2.cmml" xref="S4.SS4.p1.4.m4.1.1.1.3.2">𝑗</ci><cn id="S4.SS4.p1.4.m4.1.1.1.3.3.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.1.3.3">0</cn></apply></apply><apply id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3"><divide id="S4.SS4.p1.4.m4.1.1.3.1.cmml" xref="S4.SS4.p1.4.m4.1.1.3.1"></divide><apply id="S4.SS4.p1.4.m4.1.1.3.2.cmml" xref="S4.SS4.p1.4.m4.1.1.3.2"><times id="S4.SS4.p1.4.m4.1.1.3.2.1.cmml" xref="S4.SS4.p1.4.m4.1.1.3.2.1"></times><ci id="S4.SS4.p1.4.m4.1.1.3.2.2.cmml" xref="S4.SS4.p1.4.m4.1.1.3.2.2">𝐾</ci><ci id="S4.SS4.p1.4.m4.1.1.3.2.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3.2.3">𝑁</ci></apply><cn id="S4.SS4.p1.4.m4.1.1.3.3.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.3.3">100</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">\{v_{j}\}_{j=0}^{K\times N/100}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">{ italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K × italic_N / 100 end_POSTSUPERSCRIPT</annotation></semantics></math>, we calculate RMSE of top 10% of NAWP as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{RMSE}_{\text{top10\%}}=\sqrt{\frac{(\sum_{j=0}^{K\times N/100}(\widehat{%
\text{NAWP}_{j}}-\text{NAWP}_{j})^{2})}{K\times N/100}}," class="ltx_Math" display="block" id="S4.E6.m1.2"><semantics id="S4.E6.m1.2a"><mrow id="S4.E6.m1.2.2.1" xref="S4.E6.m1.2.2.1.1.cmml"><mrow id="S4.E6.m1.2.2.1.1" xref="S4.E6.m1.2.2.1.1.cmml"><msub id="S4.E6.m1.2.2.1.1.2" xref="S4.E6.m1.2.2.1.1.2.cmml"><mtext id="S4.E6.m1.2.2.1.1.2.2" xref="S4.E6.m1.2.2.1.1.2.2a.cmml">RMSE</mtext><mtext id="S4.E6.m1.2.2.1.1.2.3" xref="S4.E6.m1.2.2.1.1.2.3a.cmml">top10%</mtext></msub><mo id="S4.E6.m1.2.2.1.1.1" xref="S4.E6.m1.2.2.1.1.1.cmml">=</mo><msqrt id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml"><mfrac id="S4.E6.m1.1.1.1" xref="S4.E6.m1.1.1.1.cmml"><mrow id="S4.E6.m1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.E6.m1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6.m1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E6.m1.1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.1.2.cmml"><mo id="S4.E6.m1.1.1.1.1.1.1.1.2.2.2" lspace="0em" rspace="0em" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.2" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.1" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.3" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.3.cmml">0</mn></mrow><mrow id="S4.E6.m1.1.1.1.1.1.1.1.2.3" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.cmml"><mrow id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.2" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.2.cmml">K</mi><mo id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.1.cmml">×</mo><mi id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.3" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.3.cmml">N</mi></mrow><mo id="S4.E6.m1.1.1.1.1.1.1.1.2.3.1" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.1.cmml">/</mo><mn id="S4.E6.m1.1.1.1.1.1.1.1.2.3.3" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.3.cmml">100</mn></mrow></msubsup><msup id="S4.E6.m1.1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mtext id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2a.cmml">NAWP</mtext><mi id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">j</mi></msub><mo id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msub id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mtext id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml">NAWP</mtext><mi id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.E6.m1.1.1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow><mo id="S4.E6.m1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E6.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S4.E6.m1.1.1.1.3" xref="S4.E6.m1.1.1.1.3.cmml"><mrow id="S4.E6.m1.1.1.1.3.2" xref="S4.E6.m1.1.1.1.3.2.cmml"><mi id="S4.E6.m1.1.1.1.3.2.2" xref="S4.E6.m1.1.1.1.3.2.2.cmml">K</mi><mo id="S4.E6.m1.1.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S4.E6.m1.1.1.1.3.2.1.cmml">×</mo><mi id="S4.E6.m1.1.1.1.3.2.3" xref="S4.E6.m1.1.1.1.3.2.3.cmml">N</mi></mrow><mo id="S4.E6.m1.1.1.1.3.1" xref="S4.E6.m1.1.1.1.3.1.cmml">/</mo><mn id="S4.E6.m1.1.1.1.3.3" xref="S4.E6.m1.1.1.1.3.3.cmml">100</mn></mrow></mfrac></msqrt></mrow><mo id="S4.E6.m1.2.2.1.2" xref="S4.E6.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.2b"><apply id="S4.E6.m1.2.2.1.1.cmml" xref="S4.E6.m1.2.2.1"><eq id="S4.E6.m1.2.2.1.1.1.cmml" xref="S4.E6.m1.2.2.1.1.1"></eq><apply id="S4.E6.m1.2.2.1.1.2.cmml" xref="S4.E6.m1.2.2.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.1.1.2.1.cmml" xref="S4.E6.m1.2.2.1.1.2">subscript</csymbol><ci id="S4.E6.m1.2.2.1.1.2.2a.cmml" xref="S4.E6.m1.2.2.1.1.2.2"><mtext id="S4.E6.m1.2.2.1.1.2.2.cmml" xref="S4.E6.m1.2.2.1.1.2.2">RMSE</mtext></ci><ci id="S4.E6.m1.2.2.1.1.2.3a.cmml" xref="S4.E6.m1.2.2.1.1.2.3"><mtext id="S4.E6.m1.2.2.1.1.2.3.cmml" mathsize="70%" xref="S4.E6.m1.2.2.1.1.2.3">top10%</mtext></ci></apply><apply id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"><root id="S4.E6.m1.1.1a.cmml" xref="S4.E6.m1.1.1"></root><apply id="S4.E6.m1.1.1.1.cmml" xref="S4.E6.m1.1.1.1"><divide id="S4.E6.m1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1"></divide><apply id="S4.E6.m1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1"><apply id="S4.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E6.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.E6.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3"><eq id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.2">𝑗</ci><cn id="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S4.E6.m1.1.1.1.1.1.1.1.2.2.3.3">0</cn></apply></apply><apply id="S4.E6.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3"><divide id="S4.E6.m1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.1"></divide><apply id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2"><times id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.1"></times><ci id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.2">𝐾</ci><ci id="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.2.3">𝑁</ci></apply><cn id="S4.E6.m1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S4.E6.m1.1.1.1.1.1.1.1.2.3.3">100</cn></apply></apply><apply id="S4.E6.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1"><minus id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2"><ci id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.1">^</ci><apply id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2a.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2"><mtext id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.2">NAWP</mtext></ci><ci id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.2.3">𝑗</ci></apply></apply><apply id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2a.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2"><mtext id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.2">NAWP</mtext></ci><ci id="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><cn id="S4.E6.m1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.E6.m1.1.1.1.1.1.1.1.1.3">2</cn></apply></apply><apply id="S4.E6.m1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.3"><divide id="S4.E6.m1.1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.1.3.1"></divide><apply id="S4.E6.m1.1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.1.3.2"><times id="S4.E6.m1.1.1.1.3.2.1.cmml" xref="S4.E6.m1.1.1.1.3.2.1"></times><ci id="S4.E6.m1.1.1.1.3.2.2.cmml" xref="S4.E6.m1.1.1.1.3.2.2">𝐾</ci><ci id="S4.E6.m1.1.1.1.3.2.3.cmml" xref="S4.E6.m1.1.1.1.3.2.3">𝑁</ci></apply><cn id="S4.E6.m1.1.1.1.3.3.cmml" type="integer" xref="S4.E6.m1.1.1.1.3.3">100</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.2c">\text{RMSE}_{\text{top10\%}}=\sqrt{\frac{(\sum_{j=0}^{K\times N/100}(\widehat{%
\text{NAWP}_{j}}-\text{NAWP}_{j})^{2})}{K\times N/100}},</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.2d">RMSE start_POSTSUBSCRIPT top10% end_POSTSUBSCRIPT = square-root start_ARG divide start_ARG ( ∑ start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K × italic_N / 100 end_POSTSUPERSCRIPT ( over^ start_ARG NAWP start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - NAWP start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_K × italic_N / 100 end_ARG end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.p1.5">which is similar for the RMSE of top <math alttext="K" class="ltx_Math" display="inline" id="S4.SS4.p1.5.m1.1"><semantics id="S4.SS4.p1.5.m1.1a"><mi id="S4.SS4.p1.5.m1.1.1" xref="S4.SS4.p1.5.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m1.1b"><ci id="S4.SS4.p1.5.m1.1.1.cmml" xref="S4.SS4.p1.5.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.5.m1.1d">italic_K</annotation></semantics></math>% of ECR. (K=10 in our evaluation.)</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T3.8">
<tr class="ltx_tr" id="S4.T3.8.9">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T3.8.9.1" rowspan="2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.9.1.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.8.9.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.9.2.1" style="font-size:70%;">NAWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.8.9.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.9.3.1" style="font-size:70%;">ECR</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.8.9.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.9.4.1" style="font-size:70%;">NAWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.8.9.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.9.5.1" style="font-size:70%;">ECR</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.8">
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.1.1.1.1" style="font-size:70%;">SRCC</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:70%;">PLCC</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.3.3.3.1" style="font-size:70%;">RMSE</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.4.4.4.1" style="font-size:70%;">SRCC</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.m1.1"><semantics id="S4.T3.4.4.4.m1.1a"><mo id="S4.T3.4.4.4.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.5.5.5.1" style="font-size:70%;">PLCC</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.m1.1"><semantics id="S4.T3.5.5.5.m1.1a"><mo id="S4.T3.5.5.5.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.5.m1.1b"><ci id="S4.T3.5.5.5.m1.1.1.cmml" xref="S4.T3.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.5.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.6.6.6.1" style="font-size:70%;">RMSE</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.6.6.6.m1.1"><semantics id="S4.T3.6.6.6.m1.1a"><mo id="S4.T3.6.6.6.m1.1.1" mathsize="70%" stretchy="false" xref="S4.T3.6.6.6.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.6.m1.1b"><ci id="S4.T3.6.6.6.m1.1.1.cmml" xref="S4.T3.6.6.6.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.6.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.6.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.7.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.7.7.7.1" style="font-size:70%;">RMSE</span><math alttext="{}_{\text{top 10\%}}\downarrow" class="ltx_math_unparsed" display="inline" id="S4.T3.7.7.7.m1.1"><semantics id="S4.T3.7.7.7.m1.1a"><mmultiscripts id="S4.T3.7.7.7.m1.1.1"><mo id="S4.T3.7.7.7.m1.1.1.2" mathsize="70%" stretchy="false">↓</mo><mprescripts id="S4.T3.7.7.7.m1.1.1a"></mprescripts><mtext id="S4.T3.7.7.7.m1.1.1.3" mathsize="70%">top 10%</mtext><mrow id="S4.T3.7.7.7.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="S4.T3.7.7.7.m1.1b">{}_{\text{top 10\%}}\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.7.m1.1c">start_FLOATSUBSCRIPT top 10% end_FLOATSUBSCRIPT ↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.8.8.8.1" style="font-size:70%;">RMSE</span><math alttext="{}_{\text{top 10\%}}\downarrow" class="ltx_math_unparsed" display="inline" id="S4.T3.8.8.8.m1.1"><semantics id="S4.T3.8.8.8.m1.1a"><mmultiscripts id="S4.T3.8.8.8.m1.1.1"><mo id="S4.T3.8.8.8.m1.1.1.2" mathsize="70%" stretchy="false">↓</mo><mprescripts id="S4.T3.8.8.8.m1.1.1a"></mprescripts><mtext id="S4.T3.8.8.8.m1.1.1.3" mathsize="70%">top 10%</mtext><mrow id="S4.T3.8.8.8.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="S4.T3.8.8.8.m1.1b">{}_{\text{top 10\%}}\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.8.m1.1c">start_FLOATSUBSCRIPT top 10% end_FLOATSUBSCRIPT ↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.10">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.8.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.8.10.1.1" style="font-size:70%;">VSFA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.8.10.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib23" title="">23</a><span class="ltx_text" id="S4.T3.8.10.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.2.1" style="font-size:70%;">0.609</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.10.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.3.1" style="font-size:70%;">0.615</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.10.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.4.1" style="font-size:70%;">0.192</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.10.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.5.1" style="font-size:70%;">0.576</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.6.1" style="font-size:70%;">0.591</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.10.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.7.1" style="font-size:70%;">0.197</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.10.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.8.1" style="font-size:70%;">0.199</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.10.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.10.9.1" style="font-size:70%;">0.174</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.8.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.8.11.1.1" style="font-size:70%;">PVQ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.8.11.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a><span class="ltx_text" id="S4.T3.8.11.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.2.1" style="font-size:70%;">0.590</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.11.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.3.1" style="font-size:70%;">0.607</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.4.1" style="font-size:70%;">0.197</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.5.1" style="font-size:70%;">0.587</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.6.1" style="font-size:70%;">0.602</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.7.1" style="font-size:70%;">0.194</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.11.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.8.1" style="font-size:70%;">0.189</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.11.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.11.9.1" style="font-size:70%;">0.170</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.8.12.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.8.12.1.1" style="font-size:70%;">MD-VQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.8.12.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a><span class="ltx_text" id="S4.T3.8.12.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.12.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.2.1" style="font-size:70%;">0.606</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.12.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.3.1" style="font-size:70%;">0.614</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.12.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.4.1" style="font-size:70%;">0.193</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.12.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.5.1" style="font-size:70%;">0.592</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.12.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.6.1" style="font-size:70%;">0.608</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.12.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.7.1" style="font-size:70%;">0.191</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.12.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.8.1" style="font-size:70%;">0.187</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.12.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.12.9.1" style="font-size:70%;">0.166</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.8.13.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.8.13.1.1" style="font-size:70%;">FastVQA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.8.13.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib47" title="">47</a><span class="ltx_text" id="S4.T3.8.13.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.13.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.2.1" style="font-size:70%;">0.587</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.13.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.3.1" style="font-size:70%;">0.590</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.13.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.4.1" style="font-size:70%;">0.218</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.13.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.5.1" style="font-size:70%;">0.581</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.13.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.6.1" style="font-size:70%;">0.585</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.13.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.7.1" style="font-size:70%;">0.223</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.13.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.8.1" style="font-size:70%;">0.232</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.13.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.13.9.1" style="font-size:70%;">0.201</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.14">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.8.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="S4.T3.8.14.1.1" style="font-size:70%;">DOVER </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.8.14.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a><span class="ltx_text" id="S4.T3.8.14.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.14.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.2.1" style="font-size:70%;">0.635</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.14.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.3.1" style="font-size:70%;">0.636</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.14.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.4.1" style="font-size:70%;">0.206</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.14.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.5.1" style="font-size:70%;">0.619</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.14.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.6.1" style="font-size:70%;">0.622</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.14.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.7.1" style="font-size:70%;">0.203</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.8.14.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.8.1" style="font-size:70%;">0.216</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.14.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.14.9.1" style="font-size:70%;">0.189</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.15">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.8.15.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.1.1" style="font-size:70%;">Ours-VQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.15.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.2.1" style="font-size:70%;">0.625</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.15.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.3.1" style="font-size:70%;">0.632</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.15.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.4.1" style="font-size:70%;">0.188</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.15.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.5.1" style="font-size:70%;">0.605</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.15.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.6.1" style="font-size:70%;">0.620</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.15.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.7.1" style="font-size:70%;">0.189</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.15.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.8.1" style="font-size:70%;">0.191</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.8.15.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.15.9.1" style="font-size:70%;">0.171</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.16">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.8.16.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T3.8.16.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.8.16.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.2.1" style="font-size:70%;">0.696</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.8.16.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.3.1" style="font-size:70%;">0.701</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.8.16.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.4.1" style="font-size:70%;">0.172</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.8.16.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.5.1" style="font-size:70%;">0.675</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.8.16.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.6.1" style="font-size:70%;">0.688</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.8.16.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.7.1" style="font-size:70%;">0.174</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.8.16.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.8.1" style="font-size:70%;">0.181</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.8.16.9" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.8.16.9.1" style="font-size:70%;">0.152</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.12.1.1" style="font-size:129%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.13.2" style="font-size:129%;">Experimental performances of NAWP and ECR on the proposed engagement prediction dataset. “Ours-VQA” denotes merely utilizing VQA features (per-frame semantic features, per-frame distortion features and per-clip action recognition features).</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.9">For the SnapUGC dataset, we adhere to the common practice and spilt the dataset with an 90%<math alttext="\sim" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">∼</annotation></semantics></math>10% train-test ratio. Our network <math alttext="G" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">G</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">italic_G</annotation></semantics></math> takes extracted features as input and regresses <math alttext="\widehat{\text{NAWP}}" class="ltx_Math" display="inline" id="S5.SS1.p1.3.m3.1"><semantics id="S5.SS1.p1.3.m3.1a"><mover accent="true" id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mtext id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2a.cmml">NAWP</mtext><mo id="S5.SS1.p1.3.m3.1.1.1" xref="S5.SS1.p1.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><ci id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1">^</ci><ci id="S5.SS1.p1.3.m3.1.1.2a.cmml" xref="S5.SS1.p1.3.m3.1.1.2"><mtext id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">NAWP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">\widehat{\text{NAWP}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.3.m3.1d">over^ start_ARG NAWP end_ARG</annotation></semantics></math> and <math alttext="\widehat{\text{ECR}}" class="ltx_Math" display="inline" id="S5.SS1.p1.4.m4.1"><semantics id="S5.SS1.p1.4.m4.1a"><mover accent="true" id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mtext id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2a.cmml">ECR</mtext><mo id="S5.SS1.p1.4.m4.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><ci id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1">^</ci><ci id="S5.SS1.p1.4.m4.1.1.2a.cmml" xref="S5.SS1.p1.4.m4.1.1.2"><mtext id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2">ECR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">\widehat{\text{ECR}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.4.m4.1d">over^ start_ARG ECR end_ARG</annotation></semantics></math>. All feature extraction networks are pre-trained separately. We follow UVQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>]</cite> to train a distortion recognition network on KADIS-700K and KADID-10K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib25" title="">25</a>]</cite>. The per-frame semantic features are extracted by EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib41" title="">41</a>]</cite>, pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib7" title="">7</a>]</cite>. The per-clip action recognition features are extracted by ResNet-3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib11" title="">11</a>]</cite>, pre-trained on Kinetics-400 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib15" title="">15</a>]</cite>. The video caption and mid-layer features are extracted from the pre-trained video captioning model mPLUG-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib52" title="">52</a>]</cite>. Human aesthetic features are extracted by the pre-trained model in DOVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite>. We utilize T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib34" title="">34</a>]</cite> as a text encoder to encode the text data, including generated captions, sound classification results, titles, and descriptions. The network is trained with a batch size of 8 for 70,000 iterations. We use the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib17" title="">17</a>]</cite> and the learning rate is decreased from <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.p1.5.m5.1"><semantics id="S5.SS1.p1.5.m5.1a"><mrow id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml"><mn id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml">1</mn><mo id="S5.SS1.p1.5.m5.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p1.5.m5.1.1.1.cmml">×</mo><msup id="S5.SS1.p1.5.m5.1.1.3" xref="S5.SS1.p1.5.m5.1.1.3.cmml"><mn id="S5.SS1.p1.5.m5.1.1.3.2" xref="S5.SS1.p1.5.m5.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p1.5.m5.1.1.3.3" xref="S5.SS1.p1.5.m5.1.1.3.3.cmml"><mo id="S5.SS1.p1.5.m5.1.1.3.3a" xref="S5.SS1.p1.5.m5.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p1.5.m5.1.1.3.3.2" xref="S5.SS1.p1.5.m5.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1"><times id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1.1"></times><cn id="S5.SS1.p1.5.m5.1.1.2.cmml" type="integer" xref="S5.SS1.p1.5.m5.1.1.2">1</cn><apply id="S5.SS1.p1.5.m5.1.1.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.5.m5.1.1.3.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3">superscript</csymbol><cn id="S5.SS1.p1.5.m5.1.1.3.2.cmml" type="integer" xref="S5.SS1.p1.5.m5.1.1.3.2">10</cn><apply id="S5.SS1.p1.5.m5.1.1.3.3.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3"><minus id="S5.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S5.SS1.p1.5.m5.1.1.3.3"></minus><cn id="S5.SS1.p1.5.m5.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p1.5.m5.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.5.m5.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> to <math alttext="1\times 10^{-7}" class="ltx_Math" display="inline" id="S5.SS1.p1.6.m6.1"><semantics id="S5.SS1.p1.6.m6.1a"><mrow id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml"><mn id="S5.SS1.p1.6.m6.1.1.2" xref="S5.SS1.p1.6.m6.1.1.2.cmml">1</mn><mo id="S5.SS1.p1.6.m6.1.1.1" lspace="0.222em" rspace="0.222em" xref="S5.SS1.p1.6.m6.1.1.1.cmml">×</mo><msup id="S5.SS1.p1.6.m6.1.1.3" xref="S5.SS1.p1.6.m6.1.1.3.cmml"><mn id="S5.SS1.p1.6.m6.1.1.3.2" xref="S5.SS1.p1.6.m6.1.1.3.2.cmml">10</mn><mrow id="S5.SS1.p1.6.m6.1.1.3.3" xref="S5.SS1.p1.6.m6.1.1.3.3.cmml"><mo id="S5.SS1.p1.6.m6.1.1.3.3a" xref="S5.SS1.p1.6.m6.1.1.3.3.cmml">−</mo><mn id="S5.SS1.p1.6.m6.1.1.3.3.2" xref="S5.SS1.p1.6.m6.1.1.3.3.2.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><apply id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1"><times id="S5.SS1.p1.6.m6.1.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1.1"></times><cn id="S5.SS1.p1.6.m6.1.1.2.cmml" type="integer" xref="S5.SS1.p1.6.m6.1.1.2">1</cn><apply id="S5.SS1.p1.6.m6.1.1.3.cmml" xref="S5.SS1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS1.p1.6.m6.1.1.3.1.cmml" xref="S5.SS1.p1.6.m6.1.1.3">superscript</csymbol><cn id="S5.SS1.p1.6.m6.1.1.3.2.cmml" type="integer" xref="S5.SS1.p1.6.m6.1.1.3.2">10</cn><apply id="S5.SS1.p1.6.m6.1.1.3.3.cmml" xref="S5.SS1.p1.6.m6.1.1.3.3"><minus id="S5.SS1.p1.6.m6.1.1.3.3.1.cmml" xref="S5.SS1.p1.6.m6.1.1.3.3"></minus><cn id="S5.SS1.p1.6.m6.1.1.3.3.2.cmml" type="integer" xref="S5.SS1.p1.6.m6.1.1.3.3.2">7</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">1\times 10^{-7}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.6.m6.1d">1 × 10 start_POSTSUPERSCRIPT - 7 end_POSTSUPERSCRIPT</annotation></semantics></math> according to the cosine annealing strategy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib27" title="">27</a>]</cite>. The parameter <math alttext="L" class="ltx_Math" display="inline" id="S5.SS1.p1.7.m7.1"><semantics id="S5.SS1.p1.7.m7.1a"><mi id="S5.SS1.p1.7.m7.1.1" xref="S5.SS1.p1.7.m7.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.7.m7.1b"><ci id="S5.SS1.p1.7.m7.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.7.m7.1c">L</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.7.m7.1d">italic_L</annotation></semantics></math> is set to be 16. We optimize <math alttext="\widehat{\text{NAWP}}" class="ltx_Math" display="inline" id="S5.SS1.p1.8.m8.1"><semantics id="S5.SS1.p1.8.m8.1a"><mover accent="true" id="S5.SS1.p1.8.m8.1.1" xref="S5.SS1.p1.8.m8.1.1.cmml"><mtext id="S5.SS1.p1.8.m8.1.1.2" xref="S5.SS1.p1.8.m8.1.1.2a.cmml">NAWP</mtext><mo id="S5.SS1.p1.8.m8.1.1.1" xref="S5.SS1.p1.8.m8.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.8.m8.1b"><apply id="S5.SS1.p1.8.m8.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1"><ci id="S5.SS1.p1.8.m8.1.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1.1">^</ci><ci id="S5.SS1.p1.8.m8.1.1.2a.cmml" xref="S5.SS1.p1.8.m8.1.1.2"><mtext id="S5.SS1.p1.8.m8.1.1.2.cmml" xref="S5.SS1.p1.8.m8.1.1.2">NAWP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.8.m8.1c">\widehat{\text{NAWP}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.8.m8.1d">over^ start_ARG NAWP end_ARG</annotation></semantics></math> and <math alttext="\widehat{\text{ECR}}" class="ltx_Math" display="inline" id="S5.SS1.p1.9.m9.1"><semantics id="S5.SS1.p1.9.m9.1a"><mover accent="true" id="S5.SS1.p1.9.m9.1.1" xref="S5.SS1.p1.9.m9.1.1.cmml"><mtext id="S5.SS1.p1.9.m9.1.1.2" xref="S5.SS1.p1.9.m9.1.1.2a.cmml">ECR</mtext><mo id="S5.SS1.p1.9.m9.1.1.1" xref="S5.SS1.p1.9.m9.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.9.m9.1b"><apply id="S5.SS1.p1.9.m9.1.1.cmml" xref="S5.SS1.p1.9.m9.1.1"><ci id="S5.SS1.p1.9.m9.1.1.1.cmml" xref="S5.SS1.p1.9.m9.1.1.1">^</ci><ci id="S5.SS1.p1.9.m9.1.1.2a.cmml" xref="S5.SS1.p1.9.m9.1.1.2"><mtext id="S5.SS1.p1.9.m9.1.1.2.cmml" xref="S5.SS1.p1.9.m9.1.1.2">ECR</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.9.m9.1c">\widehat{\text{ECR}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.9.m9.1d">over^ start_ARG ECR end_ARG</annotation></semantics></math>
jointly following Eq (<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.E5" title="Equation 5 ‣ 4.3 Network Details ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a>). More details are provided in supplementary materials.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Engagement Results</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">To evaluate the performance of the proposed framework, we select popular quality assessment methods for comparisons, including VSFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib23" title="">23</a>]</cite>, PVQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>]</cite>, MD-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite>, FastVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib47" title="">47</a>]</cite>, and DOVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite>. <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Our network and these VQA methods are trained</span> on the proposed engagement dataset to learn the normalized average watch percentage (NAWP) and engagement continuation rate (ECR) jointly.
As conventional distortion features in MD-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite> are not available, we substitute them with distortion networks from UVQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib46" title="">46</a>]</cite>. We enhance the models by adding an additional final layer of VSFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib23" title="">23</a>]</cite>, PVQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib55" title="">55</a>]</cite>, and MD-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite> to make them adaptive for joint training with two metrics.
Due to the frames sampling in FastVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib47" title="">47</a>]</cite> and DOVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite>, we train two separate models to predict NAWP and ECR. The sampling range for NAWP is set to frames of whole videos, while the sampling range for ECR is set to frames within the first 5 seconds. All VQA models are trained with the default parameters defined by their respective authors.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="S5.T5.3">
<tr class="ltx_tr" id="S5.T5.3.4">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.3.4.1" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.4.1.1" style="font-size:70%;">Learning metrics</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.3.4.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.4.2.1" style="font-size:70%;">Duration as input</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.3.4.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.4.3.1" style="font-size:70%;">Average SRCC</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.5.1" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.5.1.1" style="font-size:70%;">AWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.5.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.5.2.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.5.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.5.3.1" style="font-size:70%;">0.665</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1">
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.1.1.2.1" style="font-size:70%;">AWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.1.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.1.1.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.1" style="padding-left:11.0pt;padding-right:11.0pt;">
<span class="ltx_text" id="S5.T5.1.1.1.1" style="font-size:70%;">0.681 (</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.m1.1.1" mathsize="70%" stretchy="false" xref="S5.T5.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text" id="S5.T5.1.1.1.2" style="font-size:70%;">)</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.6">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.6.1" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.6.1.1" style="font-size:70%;">AWT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.6.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.6.2.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.6.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.6.3.1" style="font-size:70%;">0.668</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.2.2">
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.2.2.2.1" style="font-size:70%;">AWT</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.2.2.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.2.2.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.2.2.1" style="padding-left:11.0pt;padding-right:11.0pt;">
<span class="ltx_text" id="S5.T5.2.2.1.1" style="font-size:70%;">0.683 (</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.2.2.1.m1.1"><semantics id="S5.T5.2.2.1.m1.1a"><mo id="S5.T5.2.2.1.m1.1.1" mathsize="70%" stretchy="false" xref="S5.T5.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.1.m1.1b"><ci id="S5.T5.2.2.1.m1.1.1.cmml" xref="S5.T5.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.1.m1.1d">↑</annotation></semantics></math><span class="ltx_text" id="S5.T5.2.2.1.2" style="font-size:70%;">)</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.7">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.7.1" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.7.1.1" style="font-size:70%;">NAWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.7.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.7.2.1" style="font-size:70%;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.7.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.7.3.1" style="font-size:70%;">0.696</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.2" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.3.2.1" style="font-size:70%;">NAWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.3.3.3" style="padding-left:11.0pt;padding-right:11.0pt;"><span class="ltx_text" id="S5.T5.3.3.3.1" style="font-size:70%;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.3.1" style="padding-left:11.0pt;padding-right:11.0pt;">
<span class="ltx_text" id="S5.T5.3.3.1.1" style="font-size:70%;">0.689 (</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.3.3.1.m1.1"><semantics id="S5.T5.3.3.1.m1.1a"><mo id="S5.T5.3.3.1.m1.1.1" mathsize="70%" stretchy="false" xref="S5.T5.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.1.m1.1b"><ci id="S5.T5.3.3.1.m1.1.1.cmml" xref="S5.T5.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.1.m1.1d">↓</annotation></semantics></math><span class="ltx_text" id="S5.T5.3.3.1.2" style="font-size:70%;">)</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.7.1.1" style="font-size:129%;">Table 4</span>: </span><span class="ltx_text" id="S5.T5.8.2" style="font-size:129%;">We compare proposed NAWP with average watch percentage (AWP) and average watch time (AWT). “Duration as input” means adding the video duration as a network input. We divide the videos to different groups according to their video durations and average the SRCC of different groups to obtain “Average SRCC”.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="S5.T5.9">
<tr class="ltx_tr" id="S5.T5.9.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T5.9.1.1" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.1.1.1" style="font-size:70%;">       Training setting</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.9.1.2" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.1.2.1" style="font-size:70%;">       NAWP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.9.1.3" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.1.3.1" style="font-size:70%;">       ECR</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T5.9.2.1" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.2.1.1" style="font-size:70%;">       Separate training</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.2.2" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.2.2.1" style="font-size:70%;">       0.662</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.2.3" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.2.3.1" style="font-size:70%;">       0.681</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T5.9.3.1" style="padding-left:22.0pt;padding-right:22.0pt;"><span class="ltx_text" id="S5.T5.9.3.1.1" style="font-size:70%;">       Joint training</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.9.3.2" style="padding-left:22.0pt;padding-right:22.0pt;">
<span class="ltx_text" id="S5.T5.9.3.2.1" style="font-size:70%;">       </span><span class="ltx_text ltx_font_bold" id="S5.T5.9.3.2.2" style="font-size:70%;">0.675</span><span class="ltx_text" id="S5.T5.9.3.2.3" style="font-size:70%;"></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.9.3.3" style="padding-left:22.0pt;padding-right:22.0pt;">
<span class="ltx_text" id="S5.T5.9.3.3.1" style="font-size:70%;">       </span><span class="ltx_text ltx_font_bold" id="S5.T5.9.3.3.2" style="font-size:70%;">0.696</span><span class="ltx_text" id="S5.T5.9.3.3.3" style="font-size:70%;"></span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.12.1.1" style="font-size:129%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.13.2" style="font-size:129%;">Ablation of joint training normalized average watch percentage (NAWP) and engagement continuation rate (ECR).</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">The experimental performance on the proposed dataset is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S4.T3" title="Table 3 ‣ 4.4 Evaluation Criteria ‣ 4 Methods ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">3</span></a>.
“Ours-VQA” denotes the model merely incorporating VQA features (per-frame semantic features, per-frame distortion features and per-clip action recognition features). The difference between “Ours-VQA” and MD-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib61" title="">61</a>]</cite> lies in the utilization of self-attention layers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib44" title="">44</a>]</cite>, resulting in a 0.17 improvement in SRCC for NAWP.
Although DOVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib49" title="">49</a>]</cite> outperforms ‘Ours-VQA’ on SRCC, it exhibits significantly poorer results on RMSE. “Ours-VQA” achieves balanced performance across SRCC, PLCC, and RMSE.
Benefiting from the integration of complementary multi-modal features, our method outperforms state-of-the-art VQA models by a clear margin.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Study</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">Normalized average watch percentage.</span> To evaluate the performance of normalized average watch percentage (NAWP), we train two models with average watch time (AWT) and average watch percentage (AWP). Since both AWT and AWP are duration-dependent metrics, calculating the correlation among videos of different durations is not feasible.
Therefore, we categorize videos into groups based on their durations and compute SRCC for average watch percentage within each group.
The average SRCC across these groups served as the evaluation metric in this ablation study. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.T5" title="Table 5 ‣ 5.2 Engagement Results ‣ 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates that the proposed NAWP outperforms AWT and AWP by a significant margin.
We also explore incorporating video duration as a network input, as Wu <em class="ltx_emph ltx_font_italic" id="S5.SS3.p1.1.2">et al</em>.<span class="ltx_text" id="S5.SS3.p1.1.3"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#bib.bib50" title="">50</a>]</cite> do. Although incorporating video duration as input leads to improvements in the learning performance associated with AWT and AWP, their performances are still worse than learning the proposed NAWP. Given that NAWP is more duration-independent, the incorporation of video duration as a network input cannot yield better improvements and can lead to potential confusion and overfitting. Furthermore, the models trained with AWT and AWP are unsuitable for comparing two videos with different durations, as detailed in supplementary materials.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Jointly training with two metrics.</span>
We conducted an experiment between joint training of two metrics and separate training of each metric. As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.00289v1#S5.T5" title="Table 5 ‣ 5.2 Engagement Results ‣ 5 Experiments ‣ Delving Deep into Engagement Prediction of Short Videos"><span class="ltx_text ltx_ref_tag">5</span></a>, joint training significantly enhances the performance of both metrics. The boost performance indicates that the strong correlation between the two metrics contributes to the ability of joint training to achieve higher performances.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we first reveal the limitation of using mean opinion scores from previous video quality datasets to model popularity. To overcome this, we curate a large-scale dataset of real-world short videos and conduct a detailed analysis of engagement metrics and their correlations. We further investigate comprehensive multi-modal features to enhances the model’s performance. The resultant model achieves state-of-the-art performance in predicting engagement for short videos.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Wang, Haiqiang and Li, Gary and Liu, Shan and Kuo, C.-C. Jay, “ICME 2021 UGC-VQA Challenge.”, [Online] Available: http://ugcvqa.com/

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., Zheng, X.: TensorFlow: Large-scale machine learning on heterogeneous systems (2015), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.tensorflow.org/" title="">https://www.tensorflow.org/</a>, software available from tensorflow.org

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bulathwela, S., Perez-Ortiz, M., Yilmaz, E., Shawe-Taylor, J.: VLEngagement: A Dataset of Scientific Video Lectures for Evaluating Population-based Engagement. arXiv e-prints arXiv:2011.02273 (Nov 2020). https://doi.org/10.48550/arXiv.2011.02273

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, B., Zhu, L., Li, G., Lu, F., Fan, H., Wang, S.: Learning generalized spatial-temporal deep feature representation for no-reference video quality assessment. IEEE Transactions on Circuits and Systems for Video Technology <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">32</span>(4), 1903–1916 (2022). https://doi.org/10.1109/TCSVT.2021.3088505

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Chen, P., Li, L., Ma, L., Wu, J., Shi, G.: Rirnet: Recurrent-in-recurrent network for video quality assessment. In: Proceedings of the 28th ACM International Conference on Multimedia. p. 834–842. MM ’20, Association for Computing Machinery, New York, NY, USA (2020). https://doi.org/10.1145/3394171.3413717, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3394171.3413717" title="">https://doi.org/10.1145/3394171.3413717</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Cho, K., van Merrienboer, B., Gülçehre, Ç., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y.: Learning phrase representations using RNN encoder-decoder for statistical machine translation. In: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 1724–1734. ACL (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 248–255 (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ghadiyaram, D., Pan, J., Bovik, A.C., Moorthy, A.K., Panda, P., Yang, K.C.: In-capture mobile video distortions: A study of subjective behavior and objective algorithms. IEEE Transactions on Circuits and Systems for Video Technology <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">28</span>(9), 2061–2077 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Götz-Hahn, F., Hosu, V., Lin, H., Saupe, D.: Konvid-150k: A dataset for no-reference video quality assessment of videos in-the-wild. In: IEEE Access 9. pp. 72139–72160. IEEE (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Gupta, V., Mittal, T., Mathur, P., Mishra, V., Maheshwari, M., Bera, A., Mukherjee, D., Manocha, D.: 3massiv: Multilingual, multimodal and multi-aspect dataset of social media short videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 21064–21075 (June 2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Hara, K., Kataoka, H., Satoh, Y.: Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 6546–6555 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 770–778 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hosu, V., Hahn, F., Jenadeleh, M., Lin, H., Men, H., Szirányi, T., Li, S., Saupe, D.: The konstanz natural video database (konvid-1k). In: Ninth International Conference on Quality of Multimedia Experience (QoMEX). pp. 1–6 (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ismail Fawaz, H., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D.F., Weber, J., Webb, G.I., Idoumghar, L., Muller, P.A., Petitjean, F.: Inceptiontime: Finding alexnet for time series classification. Data Mining and Knowledge Discovery (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, A., Suleyman, M., Zisserman, A.: The kinetics human action video dataset. ArXiv <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">abs/1705.06950</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kim, J., Guo, P.J., Seaton, D.T., Mitros, P., Gajos, K.Z., Miller, R.C.: Understanding in-video dropouts and interaction peaks inonline lecture videos. In: Proceedings of the First ACM Conference on Learning @ Scale Conference. p. 31–40. L@S ’14, Association for Computing Machinery, New York, NY, USA (2014). https://doi.org/10.1145/2556325.2566237, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2556325.2566237" title="">https://doi.org/10.1145/2556325.2566237</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings (2015), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1412.6980" title="">http://arxiv.org/abs/1412.6980</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Korhonen, J.: Two-level approach for no-reference consumer video quality assessment. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">28</span>(12), 5923–5938 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Lee, H., Im, J., Jang, S., Cho, H., Chung, S.: Melu: Meta-learned user preference estimator for cold-start recommendation. In: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. p. 1073–1082. KDD ’19, Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/10.1145/3292500.3330859, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3292500.3330859" title="">https://doi.org/10.1145/3292500.3330859</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Li, D., Shi, X., Zhang, Y., Cheung, K.C., See, S., Wang, X., Qin, H., Li, H.: A simple baseline for video restoration with grouped spatial-temporal shift. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 9822–9832 (June 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Li, D., Zhang, Y., Cheung, K.C., Wang, X., Qin, H., Li, H.: Learning degradation representations for image deblurring. In: Avidan, S., Brostow, G., Cissé, M., Farinella, G.M., Hassner, T. (eds.) Computer Vision – ECCV 2022. pp. 736–753. Springer Nature Switzerland, Cham (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Li, D., Zhang, Y., Law, K.L., Wang, X., Qin, H., Li, H.: Efficient burst raw denoising with variance stabilization and multi-frequency denoising network (2022). https://doi.org/10.48550/ARXIV.2205.04721, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2205.04721" title="">https://arxiv.org/abs/2205.04721</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Li, D., Jiang, T., Jiang, M.: Quality assessment of in-the-wild videos. In: Proceedings of the 27th ACM International Conference on Multimedia. p. 2351–2359. MM ’19, Association for Computing Machinery, New York, NY, USA (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Liao, L., Xu, K., Wu, H., Chen, C., Sun, W., Yan, Q., Lin, W.: Exploring the effectiveness of video perceptual representation in blind video quality assessment. In: Proceedings of the 30th ACM International Conference on Multimedia (ACM MM) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lin, H., Hosu, V., Saupe, D.: Kadid-10k: A large-scale artificially distorted iqa database. In: 2019 Eleventh International Conference on Quality of Multimedia Experience (QoMEX). pp. 1–3 (2019). https://doi.org/10.1109/QoMEX.2019.8743252

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Liu, Y., Zhou, X., Yin, H., Wang, H., Yan, C.: Efficient video quality assessment with deeper spatiotemporal feature extraction and integration. Journal of Electronic Imaging <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">30</span>, 063034 (Nov 2021). https://doi.org/10.1117/1.JEI.30.6.063034

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts. In: 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Skq89Scxx" title="">https://openreview.net/forum?id=Skq89Scxx</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Mittal, A., Saad, M.A., Bovik, A.C.: A completely blind video integrity oracle. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">25</span>(1), 289–300 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Nuutinen, M., Virtanen, T., Vaahteranoksa, M., Vuori, T., Oittinen, P., Häkkinen, J.: Cvd2014—a database for evaluating no-reference video quality assessment algorithms. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib29.1.1">25</span>(7), 3073–3086 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Pan, F., Li, S., Ao, X., Tang, P., He, Q.: Warm up cold-start advertisements: Improving ctr predictions via learning to learn id embeddings. In: Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 695–704. SIGIR’19, Association for Computing Machinery, New York, NY, USA (2019). https://doi.org/10.1145/3331184.3331268, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3331184.3331268" title="">https://doi.org/10.1145/3331184.3331268</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Panda, R., Zhang, J., Li, H., Lee, J.Y., Lu, X., Roy-Chowdhury, A.K.: Contemplating visual emotions: Understanding and overcoming dataset bias. In: European Conference on Computer Vision (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Qing-Yuan, J., Yi, H., Gen, L., Jian, L., Lei, L., Wu-Jun, L.: SVD: A large-scale short video dataset for near-duplicate video retrieval. In: Proceedings of International Conference on Computer Vision (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. Proceedings of Machine Learning Research, vol. 139, pp. 8748–8763. PMLR (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v139/radford21a.html" title="">http://proceedings.mlr.press/v139/radford21a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research <span class="ltx_text ltx_font_bold" id="bib.bib34.1.1">21</span>(140), 1–67 (2020), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" title="">http://jmlr.org/papers/v21/20-074.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Saad, M.A., Bovik, A.C., Charrier, C.: Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib35.1.1">21</span>(8), 3339–3352 (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
She, D., Yang, J., Cheng, M.M., Lai, Y.K., Rosin, P.L., Wang, L.: Wscnet: Weakly supervised coupled networks for visual sentiment classification and detection. IEEE Transactions on Multimedia (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Shi, X., Huang, Z., Bian, W., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H., Dai, J., Li, H.: Videoflow: Exploiting temporal cues for multi-frame optical flow estimation. arXiv preprint arXiv:2303.08340 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Shi, X., Huang, Z., Li, D., Zhang, M., Cheung, K.C., See, S., Qin, H., Dai, J., Li, H.: Flowformer++: Masked cost volume autoencoding for pretraining optical flow estimation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1599–1610 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Shi, X., Huang, Z., Wang, F.Y., Bian, W., Li, D., Zhang, Y., Zhang, M., Cheung, K.C., See, S., Qin, H., et al.: Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. SIGGRAPH 2024 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Sinno, Z., Bovik, A.C.: Large-scale study of perceptual video quality. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib40.1.1">28</span>(2), 612–627 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Tan, M., Le, Q.V.: Efficientnetv2: Smaller models and faster training. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event. Proceedings of Machine Learning Research, vol. 139, pp. 10096–10106. PMLR (2021), <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://proceedings.mlr.press/v139/tan21a.html" title="">http://proceedings.mlr.press/v139/tan21a.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Tu, Z., Chen, C.J., Wang, Y., Birkbeck, N., Adsumilli, B., Bovik, A.C.: Efficient user-generated video quality prediction. In: 2021 Picture Coding Symposium (PCS). pp. 1–5 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Tu, Z., Wang, Y., Birkbeck, N., Adsumilli, B., Bovik, A.C.: Ugc-vqa: Benchmarking blind video quality assessment for user generated content. IEEE Transactions on Image Processing <span class="ltx_text ltx_font_bold" id="bib.bib43.1.1">30</span>, 4449–4464 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I.: Attention is all you need. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. p. 6000–6010. NIPS’17, Curran Associates Inc., Red Hook, NY, USA (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Volkovs, M., Yu, G., Poutanen, T.: Dropoutnet: Addressing cold start in recommender systems. In: Proceedings of the 31st International Conference on Neural Information Processing Systems. p. 4964–4973. NIPS’17, Curran Associates Inc., Red Hook, NY, USA (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Wang, Y., Ke, J., Talebi, H., Yim, J.G., Birkbeck, N., Adsumilli, B., Milanfar, P., Yang, F.: Rich features for perceptual quality assessment of ugc videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 13435–13444 (June 2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Wu, H., Chen, C., Hou, J., Liao, L., Wang, A., Sun, W., Yan, Q., Lin, W.: Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling. Proceedings of European Conference of Computer Vision (ECCV) (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Wu, H., Chen, C., Liao, L., Hou, J., Sun, W., Yan, Q., Gu, J., Lin, W.: Neighbourhood representative sampling for efficient end-to-end video quality assessment (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Wu, H., Zhang, E., Liao, L., Chen, C., Hou, J., Wang, A., Sun, W., Yan, Q., Lin, W.: Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 20144–20154 (October 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Wu, S., Rizoiu, M.A., Xie, L.: Beyond views: Measuring and predicting engagement in online videos. Proceedings of the International AAAI Conference on Web and Social Media <span class="ltx_text ltx_font_bold" id="bib.bib50.1.1">12</span>(1) (Jun 2018). https://doi.org/10.1609/icwsm.v12i1.15031, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/ICWSM/article/view/15031" title="">https://ojs.aaai.org/index.php/ICWSM/article/view/15031</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Wu, X., Hu, P., Wu, Y., Lyu, X., Cao, Y.P., Shan, Y., Yang, W., Sun, Z., Qi, X.: Speech2lip: High-fidelity speech to lip generation by learning from a short video. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 22168–22177 (October 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Xu, H., Ye, Q., Yan, M., Shi, Y., Ye, J., Xu, Y., Li, C., Bi, B., Qian, Q., Wang, W., Xu, G., Zhang, J., Huang, S., Huang, F., Zhou, J.: mplug-2: A modularized multi-modal foundation model across text, image and video. ArXiv <span class="ltx_text ltx_font_bold" id="bib.bib52.1.1">abs/2302.00402</span> (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Yang, J., She, D., Lai, Y.K., Rosin, P.L., Yang, M.H.: Weakly supervised coupled networks for visual sentiment analysis. In: The IEEE Conference on Computer Vision and Pattern Recognition (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Yim, J.G., Wang, Y., Birkbeck, N., Adsumilli, B.: Subjective quality assessment for youtube ugc dataset. In: 2020 IEEE International Conference on Image Processing (ICIP). pp. 131–135 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Ying, Z., Mandal, M., Ghadiyaram, D., Bovik, A.: Patch-vq: ’patching up’ the video quality problem. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)). pp. 14019–14029 (June 2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Zhan, R., Pei, C., Su, Q., Wen, J., Wang, X., Mu, G., Zheng, D., Jiang, P., Gai, K.: Deconfounding duration bias in watch-time prediction for video recommendation. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. p. 4472–4481. KDD ’22, Association for Computing Machinery, New York, NY, USA (2022). https://doi.org/10.1145/3534678.3539092, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534678.3539092" title="">https://doi.org/10.1145/3534678.3539092</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Zhang, W., Zhai, G., Wei, Y., Yang, X., Ma, K.: Blind image quality assessment via vision-language correspondence: A multitask learning perspective. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 14071–14081 (June 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Zhang, Y., Li, D., Law, K.L., Wang, X., Qin, H., Li, H.: Idr: Self-supervised image denoising via iterative data refinement. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Zhang, Y., Li, D., Shi, X., He, D., Song, K., Wang, X., Qin, H., Li, H.: Kbnet: Kernel basis network for image restoration. arXiv preprint arXiv:2303.02881 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Zhang, Y., Shi, X., Li, D., Wang, X., Wang, J., Li, H.: A unified conditional framework for diffusion-based image restoration. arXiv preprint arXiv:2305.20049 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Zhang, Z., Wu, W., Sun, W., Tu, D., Lu, W., Min, X., Chen, Y., Zhai, G.: Md-vqa: Multi-dimensional quality assessment for ugc live videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 1746–1755 (June 2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Zhu, Y., Xie, R., Zhuang, F., Ge, K., Sun, Y., Zhang, X., Lin, L., Cao, J.: Learning to warm up cold item embeddings for cold-start recommendation with meta scaling and shifting networks. In: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. p. 1167–1176. SIGIR ’21, Association for Computing Machinery, New York, NY, USA (2021). https://doi.org/10.1145/3404835.3462843, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3404835.3462843" title="">https://doi.org/10.1145/3404835.3462843</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 30 23:48:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
