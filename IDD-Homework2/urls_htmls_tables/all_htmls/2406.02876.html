<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation</title>
<!--Generated on Thu Jun  6 03:13:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.02876v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S1" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2.SS1" title="In 2 Background â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Multilingual Neural Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2.SS2" title="In 2 Background â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Language Tag Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2.SS3" title="In 2 Background â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Off-Target issue in Zero-Shot Translation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Probing Off-Target issue in MNMT</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS1" title="In 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Metrics of Off-Target issue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS2" title="In 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Error Distribution of Off-Target Issue</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS3" title="In 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Fine-grained Language Accuracy along Decoding Steps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS4" title="In 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Language Indication in the Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS5" title="In 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Language Converter Strategy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S4" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S4.SS1" title="In 4 Experiments â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S4.SS2" title="In 4 Experiments â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS1" title="In 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Application to Stronger Approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS2" title="In 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Selection of Hyperparameter <math alttext="k" class="ltx_Math" display="inline"><semantics><mi>k</mi><annotation-xml encoding="MathML-Content"><ci>ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS3" title="In 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Effect of LCS on deeper encoders</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S6" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset &amp; Training details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.SS1" title="In Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>MultiUN</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.SS2" title="In Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>TED</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.SS3" title="In Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>OPUS-100</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.SS4" title="In Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>mBART</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.SS5" title="In Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Deep Encoder Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.SS6" title="In Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Impact of the Language Tag Placement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A3" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Selection of <math alttext="k" class="ltx_Math" display="inline"><semantics><mi>k</mi><annotation-xml encoding="MathML-Content"><ci>ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex">k</annotation><annotation encoding="application/x-llamapun">italic_k</annotation></semantics></math> in deeper encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A4" title="In LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>T-SNE Visualization</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zengkui Sun<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Yijin Liu<sup class="ltx_sup" id="id2.2.id2">2</sup>,
Fandong Meng<sup class="ltx_sup" id="id3.3.id3">2</sup>,
Jinan Xu<sup class="ltx_sup" id="id4.4.id4">1</sup>,
Yufeng Chen<sup class="ltx_sup" id="id5.5.id5">1</sup>
and Jie Zhou<sup class="ltx_sup" id="id6.6.id6">2</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">1</sup>Beijing Jiaotong University, China 
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.id8">2</sup>Pattern Recognition Center, WeChat AI, Tencent Inc, China 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id9">{zengksun,jaxu,chenyf}@bjtu.edu.cn</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id10">{yijinliu,fandongmeng,withtomzhou}@tencent.com</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes"> Â Â Work was done when Zengkui Sun was an intern at Pattern Recognition Center, WeChat AI, Tencent Inc, China. Â Â Yufeng Chen is the corresponding author.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">Multilingual neural machine translation models generally distinguish translation directions by the language tag (LT) in front of the source or target sentences.
However, current LT strategies cannot indicate the desired target language as expected on zero-shot translation, <em class="ltx_emph ltx_font_italic" id="id11.id1.1">i.e.</em>, the <span class="ltx_text ltx_font_italic" id="id11.id1.2">off-target</span> issue.
Our analysis reveals that the indication of the target language is sensitive to the placement of the target LT.
For example, when placing the target LT on the decoder side, the indication would rapidly degrade along with decoding steps, while placing the target LT on the encoder side would lead to copying or paraphrasing the source input.
To address the above issues, we propose a simple yet effective strategy named <span class="ltx_text ltx_font_bold" id="id11.id1.3">L</span>anguage <span class="ltx_text ltx_font_bold" id="id11.id1.4">C</span>onverter <span class="ltx_text ltx_font_bold" id="id11.id1.5">S</span>trategy (<span class="ltx_text ltx_font_bold" id="id11.id1.6">LCS</span>).
By introducing the target language embedding into the top encoder layers, LCS mitigates confusion in the encoder and ensures stable language indication for the decoder.
Experimental results on MultiUN, TED, and OPUS-100 datasets demonstrate that LCS could significantly mitigate the <span class="ltx_text ltx_font_italic" id="id11.id1.7">off-target</span> issue, with language accuracy up to 95.28%, 96.21%, and 85.35% meanwhile outperforming the vanilla LT strategy by 3.07, 3,3, and 7.93 BLEU scores on zero-shot translation, respectively.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Multilingual Neural Machine Translation (MNMT) aims to build a unified model to support the translation between any language pairs <cite class="ltx_cite ltx_citemacro_citep">(Dabre etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib3" title="">2020</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib40" title="">2021</a>; Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib6" title="">2021</a>)</cite>.
One main challenge is the indication of the translation direction.
Pioneers <cite class="ltx_cite ltx_citemacro_citep">(Dong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib4" title="">2015</a>; Luong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib19" title="">2015</a>; Firat etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib7" title="">2016a</a>; Lu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib18" title="">2018</a>)</cite> utilize the language-specific encoder or decoder to distinguish source or target language.
To simplify the architecture, <cite class="ltx_cite ltx_citemacro_citet">Firat etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib8" title="">2016b</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Johnson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>)</cite> propose the language tag (LT) strategy, which places an artificial language tag in front of the source input sentence, to indicate the desired target language without modification on the vanilla NMT architecture and training objective.
Due to its simplicity and efficiency, the LT strategy has become the <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">de facto</span> strategy to build unified MNMT models <cite class="ltx_cite ltx_citemacro_citep">(Johnson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>; Dabre etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib3" title="">2020</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib39" title="">2020</a>; Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib6" title="">2021</a>)</cite>, even other unified models <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib41" title="">2023</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib31" title="">2022b</a>; Liang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib14" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.1" style="width:208.1pt;height:56.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-95.8pt,25.9pt) scale(0.520609923919811,0.520609923919811) ;">
<table class="ltx_tabular ltx_align_middle" id="S1.T1.1.1">
<tr class="ltx_tr" id="S1.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S1.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.2.1">LT Strategies</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S1.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.1.1">Translation Pair (<span class="ltx_text" id="S1.T1.1.1.1.1.1.1" style="color:#0000FF;">De<math alttext="\to" class="ltx_Math" display="inline" id="S1.T1.1.1.1.1.1.1.m1.1"><semantics id="S1.T1.1.1.1.1.1.1.m1.1a"><mo id="S1.T1.1.1.1.1.1.1.m1.1.1" mathcolor="#000000" stretchy="false" xref="S1.T1.1.1.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.1.1.m1.1b"><ci id="S1.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S1.T1.1.1.1.1.1.1.m1.1d">â†’</annotation></semantics></math><span class="ltx_text" id="S1.T1.1.1.1.1.1.1.1" style="color:#1AB333;">Fr</span></span>)</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.1.2.1"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.2.1.1">Source</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.2.2">Wie sind sie durch die Sicherheitskontrolle gekommen?</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.2.3"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.2.3.1" style="color:#0000FF;">(German)</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.1.1.3.1"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.3.1.1">Reference</span></td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.3.2">Comment avez-vous passÃ© le contrÃ´le de sÃ©curitÃ©?</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.1.3.3"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.3.3.1" style="color:#1AB333;">(French)</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.1.4.1">T-Enc</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.4.2">Wie sind sie durch die Sicherheitskontrolle gekommen?</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.4.3"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.4.3.1" style="color:#0000FF;">(German)</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.1.1.5.1">S-Enc-T-Dec</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.5.2">Howâ€™d they get through the security?</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.5.3"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.5.3.1" style="color:#FF0000;">(English)</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S1.T1.1.1.6.1">LCS (ours)</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S1.T1.1.1.6.2">Comment sont-elles venues par les contrÃ´les de sÃ©curitÃ©?</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S1.T1.1.1.6.3"><span class="ltx_text ltx_font_italic" id="S1.T1.1.1.6.3.1" style="color:#1AB333;">(French)</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Translation results of the same translation pair (<span class="ltx_text" id="S1.T1.3.1" style="color:#0000FF;">German<math alttext="\to" class="ltx_Math" display="inline" id="S1.T1.3.1.m1.1"><semantics id="S1.T1.3.1.m1.1b"><mo id="S1.T1.3.1.m1.1.1" mathcolor="#000000" stretchy="false" xref="S1.T1.3.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S1.T1.3.1.m1.1c"><ci id="S1.T1.3.1.m1.1.1.cmml" xref="S1.T1.3.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.1.m1.1d">\to</annotation><annotation encoding="application/x-llamapun" id="S1.T1.3.1.m1.1e">â†’</annotation></semantics></math><span class="ltx_text" id="S1.T1.3.1.1" style="color:#1AB333;">French</span></span>) with three LT strategies<sup class="ltx_sup" id="S1.T1.8.2"><a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#footnote1" title="footnote 1 â€£ 1 Introduction â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a></sup>. T-Enc leads to mistranslating into the undesired <span class="ltx_text" id="S1.T1.9.3" style="color:#0000FF;">German</span> sentence and S-Enc-T-Dec leads to mistranslating into the <span class="ltx_text" id="S1.T1.10.4" style="color:#FF0000;">English</span> sentence, while LCS accurately leads to translating into the <span class="ltx_text" id="S1.T1.11.5" style="color:#1AB333;">French</span> sentence.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">With the LT strategy, the MNMT models theoretically support many-to-many translation, and even zero-shot translation <cite class="ltx_cite ltx_citemacro_cite">Johnson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>); Dabre etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib3" title="">2020</a>); Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib9" title="">2023</a>)</cite>.
However, in practice, the MNMT models frequently mistranslate the source language to the wrong target language on zero-shot translation, referred to as the <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">off-target</span> issue <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib39" title="">2020</a>)</cite>.
Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S1.T1" title="Table 1 â€£ 1 Introduction â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example of zero-shot translation from German to French, in which T-Enc and S-Enc-T-Dec<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Sâ€‰/â€‰T represents the sourceâ€‰/â€‰target LT, and Encâ€‰/â€‰Dec represents the LT placed on the Encoderâ€‰/â€‰Decoder side. Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2.T2" title="Table 2 â€£ 2.1 Multilingual Neural Machine Translation â€£ 2 Background â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a> shows more detailed examples.</span></span></span> are commonly used LT strategies <cite class="ltx_cite ltx_citemacro_citep">(Johnson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>; Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib6" title="">2021</a>; Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib35" title="">2021</a>)</cite>.
In this case, neither of both strategies could help MNMT models translate the sentence into the correct target language.
The T-Enc strategy leads to the <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">To-Source</span> issue (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">i.e.</em>, paraphrase or copy the input of source language), while S-Enc-T-Dec leads to the <span class="ltx_text ltx_font_italic" id="S1.p2.1.4">To-English</span> issue (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.5">i.e.</em>, translate into English).
For the <span class="ltx_text ltx_font_italic" id="S1.p2.1.6">To-Source</span> issue, prior studies <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib33" title="">2022c</a>)</cite> suggest that the data noises make this issue.
However, our experiments display that the <span class="ltx_text ltx_font_italic" id="S1.p2.1.7">To-Source</span> issue still exists after data denoise and the different language tag strategies result in different error distribution on both issues (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS2" title="3.2 Error Distribution of Off-Target Issue â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
And for the <span class="ltx_text ltx_font_italic" id="S1.p2.1.8">To-English</span> issue, we suspect it comes from the inadequate language indication of the target language and the most-common language (<em class="ltx_emph ltx_font_italic" id="S1.p2.1.9">i.e.</em>, English) is model preferred.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To further investigate the above issues, we conduct experiments to explore the indication of the target language in various LT strategies.
And we summarize some preliminary findings:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Placing the target language tag on the encoder side yields a more stable indication, while placing it on the decoder side delivers a decreasing indication throughout decoding steps and results in the <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">To-English</span> issue (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS3" title="3.3 Fine-grained Language Accuracy along Decoding Steps â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">The encoder tends to convert the states to be target language-specific on the top layers, and placing the target LT at the top encoder layers could mitigate the <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">To-Source</span> issue (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS4" title="3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Unfortunately, mainstream LT strategies suffer at least one issue between the <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">To-Source</span> and <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.2">To-English</span> in different degrees, even data denoising or placing the target language tag at the top encoder layers (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS2" title="3.2 Error Distribution of Off-Target Issue â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.2</span></a>â€‰&amp;â€‰Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS4" title="3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">On these grounds, we propose a simple yet effective strategy, <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">L</span>anguage <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">C</span>onverter <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">S</span>trategy (<span class="ltx_text ltx_font_bold" id="S1.p4.1.4">LCS</span>), to address the above issues.
Specifically, we first split the encoder layers and introduce the target language information to the deeper layers, which is named Language Converter by us.
Compared to the mere placement of language tag, we supply the target language embedding, which contains language-specific featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Bjerva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib1" title="">2019</a>; Oncevay etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib21" title="">2020</a>; Jin and Xiong, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>, into each input state in each layer of the Language Converter layers.
In this manner, LCS could provide stable and sufficient target language indication for MNMT model, avoiding both <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">To-Source</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.6">To-English</span> issues.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Experimentally, LCS could significantly mitigate the <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">off-target</span> issue and boost the performance of zero-shot translation.
Specifically, compared to the most widely-used strategy, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">i.e.</em>, the T-Enc strategy, LCS effectively mitigates the <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">off-target</span> issue by improving language accuracy up to 95.28% (<span class="ltx_text ltx_font_italic" id="S1.p5.1.4">+2.7%</span>), 96.21% (<span class="ltx_text ltx_font_italic" id="S1.p5.1.5">+1.71%</span>), 85.35% (<span class="ltx_text ltx_font_italic" id="S1.p5.1.6">+40.97%</span>), and 86.67% (<span class="ltx_text ltx_font_italic" id="S1.p5.1.7">+5.03%</span>) on zero-shot translation of MultiUN, TED, OPUS-100 (noise and denoised) datasets, respectively.
Furthermore, LCS outperforms the T-Enc strategy by 3.07, 3.3, 7.93, and 2.93 BLEU scores improvements on zero-shot translation of these datasets, respectively.
Meanwhile, LCS maintains the performance of the supervised translation and performs well on the noise data.
Moreover, LCS is well compatible with other approaches, <em class="ltx_emph ltx_font_italic" id="S1.p5.1.8">e.g.</em>, denoising-encoderÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>)</cite>, contrastive learningÂ <cite class="ltx_cite ltx_citemacro_citep">(Pan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib24" title="">2021</a>)</cite>, LEEÂ <cite class="ltx_cite ltx_citemacro_citep">(Jin and Xiong, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>, and mBARTÂ Â <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib17" title="">2020</a>)</cite>, and yields further improvements when applied to them.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The main contributions of this paper can be summarized as follows<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Codes are released at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Acerkoo/LCS" title="">https://github.com/Acerkoo/LCS</a>.</span></span></span>:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We take the analysis of the <span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.1">To-Source</span> and <span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.2">To-English</span> issues in the <span class="ltx_text ltx_font_italic" id="S1.I2.i1.p1.1.3">off-target</span> issue, and explore their causes in terms of the language representation variation of the encoder and language generation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We propose a simple yet effective strategy, LCS, to address the <span class="ltx_text ltx_font_italic" id="S1.I2.i2.p1.1.1">off-target</span> issue and further improve zero-shot translation quality, without introducing extra parameters.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">Experimental results demonstrate that LCS could significantly mitigate the <span class="ltx_text ltx_font_italic" id="S1.I2.i3.p1.1.1">off-target</span> issue and further boost the performance of zero-shot translation, with strong compatibility.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multilingual Neural Machine Translation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.4">In bilingual neural machine translation (NMT), given a source sentence with <math alttext="n" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_n</annotation></semantics></math> tokens <math alttext="\mathbf{x}=\{x_{1},x_{2},\dots,x_{n}\}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.4"><semantics id="S2.SS1.p1.2.m2.4a"><mrow id="S2.SS1.p1.2.m2.4.4" xref="S2.SS1.p1.2.m2.4.4.cmml"><mi id="S2.SS1.p1.2.m2.4.4.5" xref="S2.SS1.p1.2.m2.4.4.5.cmml">ğ±</mi><mo id="S2.SS1.p1.2.m2.4.4.4" xref="S2.SS1.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.2.m2.4.4.3.3" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml"><mo id="S2.SS1.p1.2.m2.4.4.3.3.4" stretchy="false" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.2.m2.2.2.1.1.1" xref="S2.SS1.p1.2.m2.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.2.m2.2.2.1.1.1.2" xref="S2.SS1.p1.2.m2.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS1.p1.2.m2.2.2.1.1.1.3" xref="S2.SS1.p1.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.2.m2.4.4.3.3.5" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.2.m2.3.3.2.2.2" xref="S2.SS1.p1.2.m2.3.3.2.2.2.cmml"><mi id="S2.SS1.p1.2.m2.3.3.2.2.2.2" xref="S2.SS1.p1.2.m2.3.3.2.2.2.2.cmml">x</mi><mn id="S2.SS1.p1.2.m2.3.3.2.2.2.3" xref="S2.SS1.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.2.m2.4.4.3.3.6" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><mi id="S2.SS1.p1.2.m2.1.1" mathvariant="normal" xref="S2.SS1.p1.2.m2.1.1.cmml">â€¦</mi><mo id="S2.SS1.p1.2.m2.4.4.3.3.7" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.2.m2.4.4.3.3.3" xref="S2.SS1.p1.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS1.p1.2.m2.4.4.3.3.3.2" xref="S2.SS1.p1.2.m2.4.4.3.3.3.2.cmml">x</mi><mi id="S2.SS1.p1.2.m2.4.4.3.3.3.3" xref="S2.SS1.p1.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S2.SS1.p1.2.m2.4.4.3.3.8" stretchy="false" xref="S2.SS1.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.4b"><apply id="S2.SS1.p1.2.m2.4.4.cmml" xref="S2.SS1.p1.2.m2.4.4"><eq id="S2.SS1.p1.2.m2.4.4.4.cmml" xref="S2.SS1.p1.2.m2.4.4.4"></eq><ci id="S2.SS1.p1.2.m2.4.4.5.cmml" xref="S2.SS1.p1.2.m2.4.4.5">ğ±</ci><set id="S2.SS1.p1.2.m2.4.4.3.4.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3"><apply id="S2.SS1.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.2.m2.2.2.1.1.1.2">ğ‘¥</ci><cn id="S2.SS1.p1.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS1.p1.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.2.m2.3.3.2.2.2.2">ğ‘¥</ci><cn id="S2.SS1.p1.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS1.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">â€¦</ci><apply id="S2.SS1.p1.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3.2">ğ‘¥</ci><ci id="S2.SS1.p1.2.m2.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.2.m2.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.4c">\mathbf{x}=\{x_{1},x_{2},\dots,x_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.4d">bold_x = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math> and its target sentence with <math alttext="m" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_m</annotation></semantics></math> tokens <math alttext="\mathbf{y}=\{y_{1},y_{2},\dots,y_{m}\}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.4"><semantics id="S2.SS1.p1.4.m4.4a"><mrow id="S2.SS1.p1.4.m4.4.4" xref="S2.SS1.p1.4.m4.4.4.cmml"><mi id="S2.SS1.p1.4.m4.4.4.5" xref="S2.SS1.p1.4.m4.4.4.5.cmml">ğ²</mi><mo id="S2.SS1.p1.4.m4.4.4.4" xref="S2.SS1.p1.4.m4.4.4.4.cmml">=</mo><mrow id="S2.SS1.p1.4.m4.4.4.3.3" xref="S2.SS1.p1.4.m4.4.4.3.4.cmml"><mo id="S2.SS1.p1.4.m4.4.4.3.3.4" stretchy="false" xref="S2.SS1.p1.4.m4.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p1.4.m4.2.2.1.1.1" xref="S2.SS1.p1.4.m4.2.2.1.1.1.cmml"><mi id="S2.SS1.p1.4.m4.2.2.1.1.1.2" xref="S2.SS1.p1.4.m4.2.2.1.1.1.2.cmml">y</mi><mn id="S2.SS1.p1.4.m4.2.2.1.1.1.3" xref="S2.SS1.p1.4.m4.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p1.4.m4.4.4.3.3.5" xref="S2.SS1.p1.4.m4.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.4.m4.3.3.2.2.2" xref="S2.SS1.p1.4.m4.3.3.2.2.2.cmml"><mi id="S2.SS1.p1.4.m4.3.3.2.2.2.2" xref="S2.SS1.p1.4.m4.3.3.2.2.2.2.cmml">y</mi><mn id="S2.SS1.p1.4.m4.3.3.2.2.2.3" xref="S2.SS1.p1.4.m4.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p1.4.m4.4.4.3.3.6" xref="S2.SS1.p1.4.m4.4.4.3.4.cmml">,</mo><mi id="S2.SS1.p1.4.m4.1.1" mathvariant="normal" xref="S2.SS1.p1.4.m4.1.1.cmml">â€¦</mi><mo id="S2.SS1.p1.4.m4.4.4.3.3.7" xref="S2.SS1.p1.4.m4.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p1.4.m4.4.4.3.3.3" xref="S2.SS1.p1.4.m4.4.4.3.3.3.cmml"><mi id="S2.SS1.p1.4.m4.4.4.3.3.3.2" xref="S2.SS1.p1.4.m4.4.4.3.3.3.2.cmml">y</mi><mi id="S2.SS1.p1.4.m4.4.4.3.3.3.3" xref="S2.SS1.p1.4.m4.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S2.SS1.p1.4.m4.4.4.3.3.8" stretchy="false" xref="S2.SS1.p1.4.m4.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.4b"><apply id="S2.SS1.p1.4.m4.4.4.cmml" xref="S2.SS1.p1.4.m4.4.4"><eq id="S2.SS1.p1.4.m4.4.4.4.cmml" xref="S2.SS1.p1.4.m4.4.4.4"></eq><ci id="S2.SS1.p1.4.m4.4.4.5.cmml" xref="S2.SS1.p1.4.m4.4.4.5">ğ²</ci><set id="S2.SS1.p1.4.m4.4.4.3.4.cmml" xref="S2.SS1.p1.4.m4.4.4.3.3"><apply id="S2.SS1.p1.4.m4.2.2.1.1.1.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.2.2.1.1.1.2.cmml" xref="S2.SS1.p1.4.m4.2.2.1.1.1.2">ğ‘¦</ci><cn id="S2.SS1.p1.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS1.p1.4.m4.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p1.4.m4.3.3.2.2.2.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.3.3.2.2.2.1.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.4.m4.3.3.2.2.2.2.cmml" xref="S2.SS1.p1.4.m4.3.3.2.2.2.2">ğ‘¦</ci><cn id="S2.SS1.p1.4.m4.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS1.p1.4.m4.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">â€¦</ci><apply id="S2.SS1.p1.4.m4.4.4.3.3.3.cmml" xref="S2.SS1.p1.4.m4.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.4.4.3.3.3.1.cmml" xref="S2.SS1.p1.4.m4.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.4.m4.4.4.3.3.3.2.cmml" xref="S2.SS1.p1.4.m4.4.4.3.3.3.2">ğ‘¦</ci><ci id="S2.SS1.p1.4.m4.4.4.3.3.3.3.cmml" xref="S2.SS1.p1.4.m4.4.4.3.3.3.3">ğ‘š</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.4c">\mathbf{y}=\{y_{1},y_{2},\dots,y_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.4d">bold_y = { italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math>,
and NMT models are generally optimized by the cross-entropy loss:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\rm NMT}(\theta)=-\sum_{j=1}^{m}\log p(y_{j}|\mathbf{y}_{&lt;j},%
\mathbf{x};\theta)," class="ltx_Math" display="block" id="S2.E1.m1.4"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.3" xref="S2.E1.m1.4.4.1.1.3.cmml"><msub id="S2.E1.m1.4.4.1.1.3.2" xref="S2.E1.m1.4.4.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.4.4.1.1.3.2.2" xref="S2.E1.m1.4.4.1.1.3.2.2.cmml">â„’</mi><mi id="S2.E1.m1.4.4.1.1.3.2.3" xref="S2.E1.m1.4.4.1.1.3.2.3.cmml">NMT</mi></msub><mo id="S2.E1.m1.4.4.1.1.3.1" xref="S2.E1.m1.4.4.1.1.3.1.cmml">â¢</mo><mrow id="S2.E1.m1.4.4.1.1.3.3.2" xref="S2.E1.m1.4.4.1.1.3.cmml"><mo id="S2.E1.m1.4.4.1.1.3.3.2.1" stretchy="false" xref="S2.E1.m1.4.4.1.1.3.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">Î¸</mi><mo id="S2.E1.m1.4.4.1.1.3.3.2.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.2" xref="S2.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.E1.m1.4.4.1.1.1" xref="S2.E1.m1.4.4.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.1a" xref="S2.E1.m1.4.4.1.1.1.cmml">âˆ’</mo><mrow id="S2.E1.m1.4.4.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.cmml"><munderover id="S2.E1.m1.4.4.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.2.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.2.2.2" movablelimits="false" xref="S2.E1.m1.4.4.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.2.2.3" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.2.2.3.2" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S2.E1.m1.4.4.1.1.1.1.2.2.3.1" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E1.m1.4.4.1.1.1.1.2.2.3.3" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E1.m1.4.4.1.1.1.1.2.3" xref="S2.E1.m1.4.4.1.1.1.1.2.3.cmml">m</mi></munderover><mrow id="S2.E1.m1.4.4.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.4.4.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.3.1" xref="S2.E1.m1.4.4.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E1.m1.4.4.1.1.1.1.1.3.cmml">â¡</mo><mi id="S2.E1.m1.4.4.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml">y</mi><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub><mo fence="false" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">|</mo><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ğ²</mi><mrow id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">ğ±</mi><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">Î¸</mi></mrow></mrow><mo id="S2.E1.m1.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.4.4.1.2" xref="S2.E1.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1"><eq id="S2.E1.m1.4.4.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.2"></eq><apply id="S2.E1.m1.4.4.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.3"><times id="S2.E1.m1.4.4.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.3.1"></times><apply id="S2.E1.m1.4.4.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.3.2.1.cmml" xref="S2.E1.m1.4.4.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.3.2.2.cmml" xref="S2.E1.m1.4.4.1.1.3.2.2">â„’</ci><ci id="S2.E1.m1.4.4.1.1.3.2.3.cmml" xref="S2.E1.m1.4.4.1.1.3.2.3">NMT</ci></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğœƒ</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1"><minus id="S2.E1.m1.4.4.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1"></minus><apply id="S2.E1.m1.4.4.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1"><apply id="S2.E1.m1.4.4.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2">superscript</csymbol><apply id="S2.E1.m1.4.4.1.1.1.1.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.2.2.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2">subscript</csymbol><sum id="S2.E1.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.2.2"></sum><apply id="S2.E1.m1.4.4.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3"><eq id="S2.E1.m1.4.4.1.1.1.1.2.2.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.1"></eq><ci id="S2.E1.m1.4.4.1.1.1.1.2.2.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.2">ğ‘—</ci><cn id="S2.E1.m1.4.4.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S2.E1.m1.4.4.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.4.4.1.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.2.3">ğ‘š</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1"><times id="S2.E1.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.2"></times><apply id="S2.E1.m1.4.4.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.3"><log id="S2.E1.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.3.1"></log><ci id="S2.E1.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.3.2">ğ‘</ci></apply><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.2">conditional</csymbol><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.2">ğ‘¦</ci><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply><list id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1"><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2">ğ²</ci><apply id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3"><lt id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ±</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğœƒ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\mathcal{L}_{\rm NMT}(\theta)=-\sum_{j=1}^{m}\log p(y_{j}|\mathbf{y}_{&lt;j},%
\mathbf{x};\theta),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.4d">caligraphic_L start_POSTSUBSCRIPT roman_NMT end_POSTSUBSCRIPT ( italic_Î¸ ) = - âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | bold_y start_POSTSUBSCRIPT &lt; italic_j end_POSTSUBSCRIPT , bold_x ; italic_Î¸ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p1.8">where <math alttext="j" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m1.1"><semantics id="S2.SS1.p1.5.m1.1a"><mi id="S2.SS1.p1.5.m1.1.1" xref="S2.SS1.p1.5.m1.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m1.1b"><ci id="S2.SS1.p1.5.m1.1.1.cmml" xref="S2.SS1.p1.5.m1.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m1.1c">j</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m1.1d">italic_j</annotation></semantics></math> is the index of each decoding step, <math alttext="\mathbf{y}_{&lt;j}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m2.1"><semantics id="S2.SS1.p1.6.m2.1a"><msub id="S2.SS1.p1.6.m2.1.1" xref="S2.SS1.p1.6.m2.1.1.cmml"><mi id="S2.SS1.p1.6.m2.1.1.2" xref="S2.SS1.p1.6.m2.1.1.2.cmml">ğ²</mi><mrow id="S2.SS1.p1.6.m2.1.1.3" xref="S2.SS1.p1.6.m2.1.1.3.cmml"><mi id="S2.SS1.p1.6.m2.1.1.3.2" xref="S2.SS1.p1.6.m2.1.1.3.2.cmml"></mi><mo id="S2.SS1.p1.6.m2.1.1.3.1" xref="S2.SS1.p1.6.m2.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS1.p1.6.m2.1.1.3.3" xref="S2.SS1.p1.6.m2.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m2.1b"><apply id="S2.SS1.p1.6.m2.1.1.cmml" xref="S2.SS1.p1.6.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m2.1.1.1.cmml" xref="S2.SS1.p1.6.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m2.1.1.2.cmml" xref="S2.SS1.p1.6.m2.1.1.2">ğ²</ci><apply id="S2.SS1.p1.6.m2.1.1.3.cmml" xref="S2.SS1.p1.6.m2.1.1.3"><lt id="S2.SS1.p1.6.m2.1.1.3.1.cmml" xref="S2.SS1.p1.6.m2.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS1.p1.6.m2.1.1.3.2.cmml" xref="S2.SS1.p1.6.m2.1.1.3.2">absent</csymbol><ci id="S2.SS1.p1.6.m2.1.1.3.3.cmml" xref="S2.SS1.p1.6.m2.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m2.1c">\mathbf{y}_{&lt;j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m2.1d">bold_y start_POSTSUBSCRIPT &lt; italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the target-side previous context for <math alttext="y_{j}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m3.1"><semantics id="S2.SS1.p1.7.m3.1a"><msub id="S2.SS1.p1.7.m3.1.1" xref="S2.SS1.p1.7.m3.1.1.cmml"><mi id="S2.SS1.p1.7.m3.1.1.2" xref="S2.SS1.p1.7.m3.1.1.2.cmml">y</mi><mi id="S2.SS1.p1.7.m3.1.1.3" xref="S2.SS1.p1.7.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m3.1b"><apply id="S2.SS1.p1.7.m3.1.1.cmml" xref="S2.SS1.p1.7.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m3.1.1.1.cmml" xref="S2.SS1.p1.7.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m3.1.1.2.cmml" xref="S2.SS1.p1.7.m3.1.1.2">ğ‘¦</ci><ci id="S2.SS1.p1.7.m3.1.1.3.cmml" xref="S2.SS1.p1.7.m3.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m3.1c">y_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m3.1d">italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m4.1"><semantics id="S2.SS1.p1.8.m4.1a"><mi id="S2.SS1.p1.8.m4.1.1" xref="S2.SS1.p1.8.m4.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m4.1b"><ci id="S2.SS1.p1.8.m4.1.1.cmml" xref="S2.SS1.p1.8.m4.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m4.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m4.1d">italic_Î¸</annotation></semantics></math> represents the model parameter.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.7">While in MNMT, for the language pair <math alttext="(\mathbf{x}^{s}" class="ltx_math_unparsed" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1b"><mo id="S2.SS1.p2.1.m1.1.1" stretchy="false">(</mo><msup id="S2.SS1.p2.1.m1.1.2"><mi id="S2.SS1.p2.1.m1.1.2.2">ğ±</mi><mi id="S2.SS1.p2.1.m1.1.2.3">s</mi></msup></mrow><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">(\mathbf{x}^{s}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">( bold_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{y}^{t})" class="ltx_math_unparsed" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1b"><msup id="S2.SS1.p2.2.m2.1.1"><mi id="S2.SS1.p2.2.m2.1.1.2">ğ²</mi><mi id="S2.SS1.p2.2.m2.1.1.3">t</mi></msup><mo id="S2.SS1.p2.2.m2.1.2" stretchy="false">)</mo></mrow><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\mathbf{y}^{t})</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )</annotation></semantics></math> where <math alttext="s" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">italic_s</annotation></semantics></math> and <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.1"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">italic_t</annotation></semantics></math> represent the source language <math alttext="s" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.1"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.1d">italic_s</annotation></semantics></math> and the target language <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p2.6.m6.1"><semantics id="S2.SS1.p2.6.m6.1a"><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m6.1d">italic_t</annotation></semantics></math>, and <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S2.SS1.p2.7.m7.1"><semantics id="S2.SS1.p2.7.m7.1a"><mi id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml">ğ­</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><ci id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1">ğ­</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">\mathbf{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.7.m7.1d">bold_t</annotation></semantics></math> denotes the target language tag, the MNMT models are generally optimized by the cross-entropy loss:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\rm MNMT}(\theta)=-\sum_{j=1}^{m}\log p(y_{j}|\mathbf{y}^{t}_{&lt;j}%
,\mathbf{x}^{s},\mathbf{t};\theta)." class="ltx_Math" display="block" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.3" xref="S2.E2.m1.4.4.1.1.3.cmml"><msub id="S2.E2.m1.4.4.1.1.3.2" xref="S2.E2.m1.4.4.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.1.1.3.2.2" xref="S2.E2.m1.4.4.1.1.3.2.2.cmml">â„’</mi><mi id="S2.E2.m1.4.4.1.1.3.2.3" xref="S2.E2.m1.4.4.1.1.3.2.3.cmml">MNMT</mi></msub><mo id="S2.E2.m1.4.4.1.1.3.1" xref="S2.E2.m1.4.4.1.1.3.1.cmml">â¢</mo><mrow id="S2.E2.m1.4.4.1.1.3.3.2" xref="S2.E2.m1.4.4.1.1.3.cmml"><mo id="S2.E2.m1.4.4.1.1.3.3.2.1" stretchy="false" xref="S2.E2.m1.4.4.1.1.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Î¸</mi><mo id="S2.E2.m1.4.4.1.1.3.3.2.2" stretchy="false" xref="S2.E2.m1.4.4.1.1.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.2" xref="S2.E2.m1.4.4.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1a" xref="S2.E2.m1.4.4.1.1.1.cmml">âˆ’</mo><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml"><munderover id="S2.E2.m1.4.4.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.2.2.2" movablelimits="false" xref="S2.E2.m1.4.4.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.2.2.3.2" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.2.cmml">j</mi><mo id="S2.E2.m1.4.4.1.1.1.1.2.2.3.1" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.4.4.1.1.1.1.2.2.3.3" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.4.4.1.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.1.2.3.cmml">m</mi></munderover><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.3.1.cmml">log</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.3a" lspace="0.167em" xref="S2.E2.m1.4.4.1.1.1.1.1.3.cmml">â¡</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2.cmml">y</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3.cmml">j</mi></msub><mo fence="false" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml"><msubsup id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğ²</mi><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2.cmml">ğ±</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3.cmml">s</mi></msup><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">ğ­</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.5" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml">;</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">Î¸</mi></mrow></mrow><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.4.4.1.2" lspace="0em" xref="S2.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1"><eq id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.2"></eq><apply id="S2.E2.m1.4.4.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.3"><times id="S2.E2.m1.4.4.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.3.1"></times><apply id="S2.E2.m1.4.4.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.3.2.2">â„’</ci><ci id="S2.E2.m1.4.4.1.1.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.3.2.3">MNMT</ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">ğœƒ</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><minus id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1"></minus><apply id="S2.E2.m1.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1"><apply id="S2.E2.m1.4.4.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2">superscript</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2.2"></sum><apply id="S2.E2.m1.4.4.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3"><eq id="S2.E2.m1.4.4.1.1.1.1.2.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.4.4.1.1.1.1.2.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.2">ğ‘—</ci><cn id="S2.E2.m1.4.4.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.4.4.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3">ğ‘š</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.2"></times><apply id="S2.E2.m1.4.4.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3"><log id="S2.E2.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.1"></log><ci id="S2.E2.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.3.2">ğ‘</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.2">ğ‘¦</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.4.3">ğ‘—</ci></apply><list id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2"><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ²</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.3">ğ‘¡</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3"><lt id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.2">ğ±</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.2.2.2.3">ğ‘ </ci></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ğ­</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">ğœƒ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\mathcal{L}_{\rm MNMT}(\theta)=-\sum_{j=1}^{m}\log p(y_{j}|\mathbf{y}^{t}_{&lt;j}%
,\mathbf{x}^{s},\mathbf{t};\theta).</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">caligraphic_L start_POSTSUBSCRIPT roman_MNMT end_POSTSUBSCRIPT ( italic_Î¸ ) = - âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT roman_log italic_p ( italic_y start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | bold_y start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT &lt; italic_j end_POSTSUBSCRIPT , bold_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT , bold_t ; italic_Î¸ ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In this paper, we focus on the analysis of the placement of <math alttext="\mathbf{t}" class="ltx_Math" display="inline" id="S2.SS1.p3.1.m1.1"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">ğ­</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">ğ­</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\mathbf{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">bold_t</annotation></semantics></math> and explore the better way to indicate the target language for zero-shot translation.</p>
</div>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.1" style="width:206.0pt;height:81.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-108.9pt,43.0pt) scale(0.486005398895853,0.486005398895853) ;">
<table class="ltx_tabular ltx_align_middle" id="S2.T2.1.1">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S2.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1">LT Strategies</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T2.1.1.1.1">
<span class="ltx_text" id="S2.T2.1.1.1.1.2"></span><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1"> <span class="ltx_text" id="S2.T2.1.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.1.1.1.1.1">
<span class="ltx_tr" id="S2.T2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.1.1.1.1.1.1.1">Example (En <math alttext="\to" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.1.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1c">\to</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.1.1.1.1.1.m1.1d">â†’</annotation></semantics></math> De)</span></span>
</span></span><span class="ltx_text" id="S2.T2.1.1.1.1.1.2"></span></span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.2.1" rowspan="2"><span class="ltx_text" id="S2.T2.1.1.2.1.1"><span class="ltx_text" id="S2.T2.1.1.2.1.1.1"></span> <span class="ltx_text" id="S2.T2.1.1.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.2.1.1.2.1">
<span class="ltx_tr" id="S2.T2.1.1.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.2.1.1.2.1.1.1">T-Enc</span></span>
<span class="ltx_tr" id="S2.T2.1.1.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.2.1.1.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(Johnson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>)</cite></span></span>
</span></span> <span class="ltx_text" id="S2.T2.1.1.2.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.2.2">&lt;de&gt; Hello, how are you?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.3">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.3.1">Hallo, wie gehtâ€™s?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.4.1" rowspan="2"><span class="ltx_text" id="S2.T2.1.1.4.1.1"><span class="ltx_text" id="S2.T2.1.1.4.1.1.1"></span> <span class="ltx_text" id="S2.T2.1.1.4.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.4.1.1.2.1">
<span class="ltx_tr" id="S2.T2.1.1.4.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.4.1.1.2.1.1.1">S-Enc-T-Dec</span></span>
<span class="ltx_tr" id="S2.T2.1.1.4.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.4.1.1.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib6" title="">2021</a>)</cite></span></span>
</span></span> <span class="ltx_text" id="S2.T2.1.1.4.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.4.2">&lt;en&gt; Hello, how are you?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.5">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.5.1">&lt;de&gt; Hallo, wie gehtâ€™s?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T2.1.1.6.1" rowspan="2"><span class="ltx_text" id="S2.T2.1.1.6.1.1"><span class="ltx_text" id="S2.T2.1.1.6.1.1.1"></span> <span class="ltx_text" id="S2.T2.1.1.6.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.6.1.1.2.1">
<span class="ltx_tr" id="S2.T2.1.1.6.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.6.1.1.2.1.1.1">ST-Enc</span></span>
<span class="ltx_tr" id="S2.T2.1.1.6.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.6.1.1.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(Xue etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib36" title="">2021</a>)</cite></span></span>
</span></span> <span class="ltx_text" id="S2.T2.1.1.6.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.6.2">&lt;en&gt; &lt;de&gt; Hello, how are you?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.7">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.7.1">Hallo, wie gehtâ€™s?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S2.T2.1.1.8.1" rowspan="2"><span class="ltx_text" id="S2.T2.1.1.8.1.1"><span class="ltx_text" id="S2.T2.1.1.8.1.1.1"></span> <span class="ltx_text" id="S2.T2.1.1.8.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T2.1.1.8.1.1.2.1">
<span class="ltx_tr" id="S2.T2.1.1.8.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.8.1.1.2.1.1.1">ST-Enc-T-Dec</span></span>
<span class="ltx_tr" id="S2.T2.1.1.8.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T2.1.1.8.1.1.2.1.2.1"><cite class="ltx_cite ltx_citemacro_citep">(ElNokrashy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib5" title="">2022</a>)</cite></span></span>
</span></span> <span class="ltx_text" id="S2.T2.1.1.8.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.8.2">&lt;en&gt; &lt;de&gt; Hello, how are you?</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T2.1.1.9.1">&lt;de&gt; Hallo, wie gehtâ€™s?</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Several examples of language tag strategies.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Language Tag Strategy</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Pioneered by <cite class="ltx_cite ltx_citemacro_citet">Johnson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>)</cite>, the LT strategy has become the <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">de facto</span> strategy to build the unified MNMT model.
Recently, many varieties are proposed to adjust the placement of the LT and we list several popular samples in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2.T2" title="Table 2 â€£ 2.1 Multilingual Neural Machine Translation â€£ 2 Background â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.
Among these strategies, the T-Enc and S-Enc-T-Dec strategies are the most widely used ones to build MNMT models.
The first proposed T-Enc strategy performs the best on zero-shot translation <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib35" title="">2021</a>; Wicks and Duh, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib34" title="">2022</a>)</cite>.
And S-Enc-T-Dec is also widely used in establishing MNMT models, <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.2">e.g.</em>, M2M100 <cite class="ltx_cite ltx_citemacro_citep">(Fan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib6" title="">2021</a>)</cite>, mBART <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib17" title="">2020</a>)</cite>, and so on.
Although some studies <cite class="ltx_cite ltx_citemacro_citep">(Xue etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib36" title="">2021</a>; ElNokrashy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib5" title="">2022</a>)</cite> propose to place double tags on the encoder side, the capability of indicating the target language still remains insufficient <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib35" title="">2021</a>)</cite>.
Therefore, this paper mainly focuses on the two most widely used strategies, <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">i.e,</span> T-Enc and S-Enc-T-Dec, to investigate the influence of the placement of LT.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Off-Target issue in Zero-Shot Translation</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">off-target</span> issue describes the wrong target languages translated by the MNMT models on zero-shot translation <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib39" title="">2020</a>)</cite>.
Prior studies <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib10" title="">2019</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib39" title="">2020</a>; Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib15" title="">2021</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>; Yang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib37" title="">2021</a>; Mao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib20" title="">2023</a>; Zan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib38" title="">2023</a>)</cite> reveal that the spurious correlations between language pairs within supervised data aggravate this issue and make efforts to overcome it in terms of adjusting training strategy, modifying the residual connection and generating auxiliary data.
Besides, <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib33" title="">2022c</a>)</cite> points out that data noises also make the <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">off-target</span> issue, and <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib33" title="">2022c</a>); Jin and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>); Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib2" title="">2023</a>)</cite> enhance the modelâ€™s awareness to the vocabulary of target language during generation.
However, the cause of fundamental language tag strategies, which perform various on this issue, remains unclear.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Probing Off-Target issue in MNMT</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we probe the <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">off-target</span> issue in MNMT models with different LT strategies.
Firstly, we introduce the language rate/accuracy metric on this issue (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS1" title="3.1 Metrics of Off-Target issue â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Secondly, we conduct experiments in terms of the distribution of this issue (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS2" title="3.2 Error Distribution of Off-Target Issue â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.2</span></a>), the fine-grained language accuracy along decoding steps (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS3" title="3.3 Fine-grained Language Accuracy along Decoding Steps â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a>) and the language indication in the encoder (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS4" title="3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a>).
Lastly, we expand our conclusion to propose <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">LCS</span> to mitigate this issue (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS5" title="3.5 Language Converter Strategy â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.5</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Metrics of Off-Target issue</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To quantify this issue, we adopt the language rate as the metric to observe the error language distribution of zero-shot translation.
We adopt the langdetect<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/Mimino666/langdetect</span></span></span> toolkit to identify the language of generated sentences, following prior studies <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib39" title="">2020</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>; Jin and Xiong, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>.
And we calculate the language rate of language <math alttext="\ell" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p1.1.m1.1.1.cmml">â„“</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">â„“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\ell</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">roman_â„“</annotation></semantics></math> as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathit{Rate}(\ell)=\frac{\sum_{i=1}^{N}\mathbbm{1}_{lang(y^{(i)})=\ell}}{N}%
\times 100\%," class="ltx_Math" display="block" id="S3.E3.m1.4"><semantics id="S3.E3.m1.4a"><mrow id="S3.E3.m1.4.4.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml"><mrow id="S3.E3.m1.4.4.1.1.2" xref="S3.E3.m1.4.4.1.1.2.cmml"><mi id="S3.E3.m1.4.4.1.1.2.2" xref="S3.E3.m1.4.4.1.1.2.2.cmml">ğ‘…ğ‘ğ‘¡ğ‘’</mi><mo id="S3.E3.m1.4.4.1.1.2.1" xref="S3.E3.m1.4.4.1.1.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.4.4.1.1.2.3.2" xref="S3.E3.m1.4.4.1.1.2.cmml"><mo id="S3.E3.m1.4.4.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.4.4.1.1.2.cmml">(</mo><mi id="S3.E3.m1.3.3" mathvariant="normal" xref="S3.E3.m1.3.3.cmml">â„“</mi><mo id="S3.E3.m1.4.4.1.1.2.3.2.2" stretchy="false" xref="S3.E3.m1.4.4.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.4.4.1.1.1" xref="S3.E3.m1.4.4.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.4.4.1.1.3" xref="S3.E3.m1.4.4.1.1.3.cmml"><mfrac id="S3.E3.m1.2.2" xref="S3.E3.m1.2.2.cmml"><mrow id="S3.E3.m1.2.2.2" xref="S3.E3.m1.2.2.2.cmml"><msubsup id="S3.E3.m1.2.2.2.3" xref="S3.E3.m1.2.2.2.3.cmml"><mo id="S3.E3.m1.2.2.2.3.2.2" xref="S3.E3.m1.2.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.2.2.2.3.2.3" xref="S3.E3.m1.2.2.2.3.2.3.cmml"><mi id="S3.E3.m1.2.2.2.3.2.3.2" xref="S3.E3.m1.2.2.2.3.2.3.2.cmml">i</mi><mo id="S3.E3.m1.2.2.2.3.2.3.1" xref="S3.E3.m1.2.2.2.3.2.3.1.cmml">=</mo><mn id="S3.E3.m1.2.2.2.3.2.3.3" xref="S3.E3.m1.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.2.2.2.3.3" xref="S3.E3.m1.2.2.2.3.3.cmml">N</mi></msubsup><msub id="S3.E3.m1.2.2.2.4" xref="S3.E3.m1.2.2.2.4.cmml"><mn id="S3.E3.m1.2.2.2.4.2" xref="S3.E3.m1.2.2.2.4.2.cmml">ğŸ™</mn><mrow id="S3.E3.m1.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.cmml"><mrow id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.2.3.cmml">l</mi><mo id="S3.E3.m1.2.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml">â¢</mo><mi id="S3.E3.m1.2.2.2.2.2.2.4" xref="S3.E3.m1.2.2.2.2.2.2.4.cmml">a</mi><mo id="S3.E3.m1.2.2.2.2.2.2.2a" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml">â¢</mo><mi id="S3.E3.m1.2.2.2.2.2.2.5" xref="S3.E3.m1.2.2.2.2.2.2.5.cmml">n</mi><mo id="S3.E3.m1.2.2.2.2.2.2.2b" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml">â¢</mo><mi id="S3.E3.m1.2.2.2.2.2.2.6" xref="S3.E3.m1.2.2.2.2.2.2.6.cmml">g</mi><mo id="S3.E3.m1.2.2.2.2.2.2.2c" xref="S3.E3.m1.2.2.2.2.2.2.2.cmml">â¢</mo><mrow id="S3.E3.m1.2.2.2.2.2.2.1.1" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml"><mo id="S3.E3.m1.2.2.2.2.2.2.1.1.2" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml">(</mo><msup id="S3.E3.m1.2.2.2.2.2.2.1.1.1" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2.1.1.1.2" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.2.cmml">y</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.1" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.3.2" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S3.E3.m1.2.2.2.2.2.2.1.1.3" stretchy="false" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.2.2.2.2.2.3" xref="S3.E3.m1.2.2.2.2.2.3.cmml">=</mo><mi id="S3.E3.m1.2.2.2.2.2.4" mathvariant="normal" xref="S3.E3.m1.2.2.2.2.2.4.cmml">â„“</mi></mrow></msub></mrow><mi id="S3.E3.m1.2.2.4" xref="S3.E3.m1.2.2.4.cmml">N</mi></mfrac><mo id="S3.E3.m1.4.4.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E3.m1.4.4.1.1.3.1.cmml">Ã—</mo><mrow id="S3.E3.m1.4.4.1.1.3.2" xref="S3.E3.m1.4.4.1.1.3.2.cmml"><mn id="S3.E3.m1.4.4.1.1.3.2.2" xref="S3.E3.m1.4.4.1.1.3.2.2.cmml">100</mn><mo id="S3.E3.m1.4.4.1.1.3.2.1" xref="S3.E3.m1.4.4.1.1.3.2.1.cmml">%</mo></mrow></mrow></mrow><mo id="S3.E3.m1.4.4.1.2" xref="S3.E3.m1.4.4.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.4b"><apply id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1"><eq id="S3.E3.m1.4.4.1.1.1.cmml" xref="S3.E3.m1.4.4.1.1.1"></eq><apply id="S3.E3.m1.4.4.1.1.2.cmml" xref="S3.E3.m1.4.4.1.1.2"><times id="S3.E3.m1.4.4.1.1.2.1.cmml" xref="S3.E3.m1.4.4.1.1.2.1"></times><ci id="S3.E3.m1.4.4.1.1.2.2.cmml" xref="S3.E3.m1.4.4.1.1.2.2">ğ‘…ğ‘ğ‘¡ğ‘’</ci><ci id="S3.E3.m1.3.3.cmml" xref="S3.E3.m1.3.3">â„“</ci></apply><apply id="S3.E3.m1.4.4.1.1.3.cmml" xref="S3.E3.m1.4.4.1.1.3"><times id="S3.E3.m1.4.4.1.1.3.1.cmml" xref="S3.E3.m1.4.4.1.1.3.1"></times><apply id="S3.E3.m1.2.2.cmml" xref="S3.E3.m1.2.2"><divide id="S3.E3.m1.2.2.3.cmml" xref="S3.E3.m1.2.2"></divide><apply id="S3.E3.m1.2.2.2.cmml" xref="S3.E3.m1.2.2.2"><apply id="S3.E3.m1.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.3.1.cmml" xref="S3.E3.m1.2.2.2.3">superscript</csymbol><apply id="S3.E3.m1.2.2.2.3.2.cmml" xref="S3.E3.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.3.2.1.cmml" xref="S3.E3.m1.2.2.2.3">subscript</csymbol><sum id="S3.E3.m1.2.2.2.3.2.2.cmml" xref="S3.E3.m1.2.2.2.3.2.2"></sum><apply id="S3.E3.m1.2.2.2.3.2.3.cmml" xref="S3.E3.m1.2.2.2.3.2.3"><eq id="S3.E3.m1.2.2.2.3.2.3.1.cmml" xref="S3.E3.m1.2.2.2.3.2.3.1"></eq><ci id="S3.E3.m1.2.2.2.3.2.3.2.cmml" xref="S3.E3.m1.2.2.2.3.2.3.2">ğ‘–</ci><cn id="S3.E3.m1.2.2.2.3.2.3.3.cmml" type="integer" xref="S3.E3.m1.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.2.2.2.3.3.cmml" xref="S3.E3.m1.2.2.2.3.3">ğ‘</ci></apply><apply id="S3.E3.m1.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.4.1.cmml" xref="S3.E3.m1.2.2.2.4">subscript</csymbol><cn id="S3.E3.m1.2.2.2.4.2.cmml" type="integer" xref="S3.E3.m1.2.2.2.4.2">1</cn><apply id="S3.E3.m1.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2"><eq id="S3.E3.m1.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.3"></eq><apply id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2"><times id="S3.E3.m1.2.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.2"></times><ci id="S3.E3.m1.2.2.2.2.2.2.3.cmml" xref="S3.E3.m1.2.2.2.2.2.2.3">ğ‘™</ci><ci id="S3.E3.m1.2.2.2.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.2.2.2.4">ğ‘</ci><ci id="S3.E3.m1.2.2.2.2.2.2.5.cmml" xref="S3.E3.m1.2.2.2.2.2.2.5">ğ‘›</ci><ci id="S3.E3.m1.2.2.2.2.2.2.6.cmml" xref="S3.E3.m1.2.2.2.2.2.2.6">ğ‘”</ci><apply id="S3.E3.m1.2.2.2.2.2.2.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.1">superscript</csymbol><ci id="S3.E3.m1.2.2.2.2.2.2.1.1.1.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2.1.1.1.2">ğ‘¦</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">ğ‘–</ci></apply></apply><ci id="S3.E3.m1.2.2.2.2.2.4.cmml" xref="S3.E3.m1.2.2.2.2.2.4">â„“</ci></apply></apply></apply><ci id="S3.E3.m1.2.2.4.cmml" xref="S3.E3.m1.2.2.4">ğ‘</ci></apply><apply id="S3.E3.m1.4.4.1.1.3.2.cmml" xref="S3.E3.m1.4.4.1.1.3.2"><csymbol cd="latexml" id="S3.E3.m1.4.4.1.1.3.2.1.cmml" xref="S3.E3.m1.4.4.1.1.3.2.1">percent</csymbol><cn id="S3.E3.m1.4.4.1.1.3.2.2.cmml" type="integer" xref="S3.E3.m1.4.4.1.1.3.2.2">100</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.4c">\mathit{Rate}(\ell)=\frac{\sum_{i=1}^{N}\mathbbm{1}_{lang(y^{(i)})=\ell}}{N}%
\times 100\%,</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.4d">italic_Rate ( roman_â„“ ) = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_l italic_a italic_n italic_g ( italic_y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ) = roman_â„“ end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG Ã— 100 % ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.5">where <math alttext="\mathrm{y}^{(i)}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m1.1"><semantics id="S3.SS1.p1.2.m1.1a"><msup id="S3.SS1.p1.2.m1.1.2" xref="S3.SS1.p1.2.m1.1.2.cmml"><mi id="S3.SS1.p1.2.m1.1.2.2" mathvariant="normal" xref="S3.SS1.p1.2.m1.1.2.2.cmml">y</mi><mrow id="S3.SS1.p1.2.m1.1.1.1.3" xref="S3.SS1.p1.2.m1.1.2.cmml"><mo id="S3.SS1.p1.2.m1.1.1.1.3.1" stretchy="false" xref="S3.SS1.p1.2.m1.1.2.cmml">(</mo><mi id="S3.SS1.p1.2.m1.1.1.1.1" xref="S3.SS1.p1.2.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p1.2.m1.1.1.1.3.2" stretchy="false" xref="S3.SS1.p1.2.m1.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m1.1b"><apply id="S3.SS1.p1.2.m1.1.2.cmml" xref="S3.SS1.p1.2.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m1.1.2.1.cmml" xref="S3.SS1.p1.2.m1.1.2">superscript</csymbol><ci id="S3.SS1.p1.2.m1.1.2.2.cmml" xref="S3.SS1.p1.2.m1.1.2.2">y</ci><ci id="S3.SS1.p1.2.m1.1.1.1.1.cmml" xref="S3.SS1.p1.2.m1.1.1.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m1.1c">\mathrm{y}^{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m1.1d">roman_y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> denotes the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m2.1"><semantics id="S3.SS1.p1.3.m2.1a"><mi id="S3.SS1.p1.3.m2.1.1" xref="S3.SS1.p1.3.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m2.1b"><ci id="S3.SS1.p1.3.m2.1.1.cmml" xref="S3.SS1.p1.3.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m2.1d">italic_i</annotation></semantics></math>-th generated sentence, <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m3.1"><semantics id="S3.SS1.p1.4.m3.1a"><mi id="S3.SS1.p1.4.m3.1.1" xref="S3.SS1.p1.4.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m3.1b"><ci id="S3.SS1.p1.4.m3.1.1.cmml" xref="S3.SS1.p1.4.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m3.1d">italic_N</annotation></semantics></math> denotes the scale of the test set, and <math alttext="lang(\cdot)" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m4.1"><semantics id="S3.SS1.p1.5.m4.1a"><mrow id="S3.SS1.p1.5.m4.1.2" xref="S3.SS1.p1.5.m4.1.2.cmml"><mi id="S3.SS1.p1.5.m4.1.2.2" xref="S3.SS1.p1.5.m4.1.2.2.cmml">l</mi><mo id="S3.SS1.p1.5.m4.1.2.1" xref="S3.SS1.p1.5.m4.1.2.1.cmml">â¢</mo><mi id="S3.SS1.p1.5.m4.1.2.3" xref="S3.SS1.p1.5.m4.1.2.3.cmml">a</mi><mo id="S3.SS1.p1.5.m4.1.2.1a" xref="S3.SS1.p1.5.m4.1.2.1.cmml">â¢</mo><mi id="S3.SS1.p1.5.m4.1.2.4" xref="S3.SS1.p1.5.m4.1.2.4.cmml">n</mi><mo id="S3.SS1.p1.5.m4.1.2.1b" xref="S3.SS1.p1.5.m4.1.2.1.cmml">â¢</mo><mi id="S3.SS1.p1.5.m4.1.2.5" xref="S3.SS1.p1.5.m4.1.2.5.cmml">g</mi><mo id="S3.SS1.p1.5.m4.1.2.1c" xref="S3.SS1.p1.5.m4.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.p1.5.m4.1.2.6.2" xref="S3.SS1.p1.5.m4.1.2.cmml"><mo id="S3.SS1.p1.5.m4.1.2.6.2.1" stretchy="false" xref="S3.SS1.p1.5.m4.1.2.cmml">(</mo><mo id="S3.SS1.p1.5.m4.1.1" lspace="0em" rspace="0em" xref="S3.SS1.p1.5.m4.1.1.cmml">â‹…</mo><mo id="S3.SS1.p1.5.m4.1.2.6.2.2" stretchy="false" xref="S3.SS1.p1.5.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m4.1b"><apply id="S3.SS1.p1.5.m4.1.2.cmml" xref="S3.SS1.p1.5.m4.1.2"><times id="S3.SS1.p1.5.m4.1.2.1.cmml" xref="S3.SS1.p1.5.m4.1.2.1"></times><ci id="S3.SS1.p1.5.m4.1.2.2.cmml" xref="S3.SS1.p1.5.m4.1.2.2">ğ‘™</ci><ci id="S3.SS1.p1.5.m4.1.2.3.cmml" xref="S3.SS1.p1.5.m4.1.2.3">ğ‘</ci><ci id="S3.SS1.p1.5.m4.1.2.4.cmml" xref="S3.SS1.p1.5.m4.1.2.4">ğ‘›</ci><ci id="S3.SS1.p1.5.m4.1.2.5.cmml" xref="S3.SS1.p1.5.m4.1.2.5">ğ‘”</ci><ci id="S3.SS1.p1.5.m4.1.1.cmml" xref="S3.SS1.p1.5.m4.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m4.1c">lang(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m4.1d">italic_l italic_a italic_n italic_g ( â‹… )</annotation></semantics></math> denotes the language detect function.
The <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.5.1">accruacy</span> denotes the language rate of the desired target language, where the higher language accuracy denotes the slighter <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.5.2">off-target</span> issue.
Generally, language accuracy is regarded as an indicator of the performance of zero-shot translation, where the higher accuracy usually guides to the better performance.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To further observe the variation of language rate throughout decoding steps, we calculate the rate of continuous intervals with 5 words in it along generated sentences.
To accurately detect the language of short text, we choose the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">lingua-py<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote4.1.1.1">4</span></span><span class="ltx_text ltx_font_upright" id="footnote4.5">https://github.com/pemistahl/lingua-py</span></span></span></span></span> toolkit, which has higher accuracy in detecting the short texts<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>More accurate as described as their GitHub repository.</span></span></span>.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.4" style="width:208.1pt;height:103.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.2pt,29.4pt) scale(0.637312255150964,0.637312255150964) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T3.4.4">
<tr class="ltx_tr" id="S3.T3.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.4.4.4.5">
<span class="ltx_text ltx_font_bold" id="S3.T3.4.4.4.5.1">Noise</span>?</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T3.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.4.6.1">Strategy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1" style="background-color:#D4E7CF;">Acc<span class="ltx_text ltx_font_medium" id="S3.T3.1.1.1.1.1.1"> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.1.1.m1.1.1" mathbackground="#D4E7CF" stretchy="false" xref="S3.T3.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.1.1.m1.1d">â†‘</annotation></semantics></math></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.2.2.2">
<span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.2.1">To-Src</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.2.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.2.m1.1a"><mo id="S3.T3.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T3.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.2.m1.1b"><ci id="S3.T3.2.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.3.3.3.3">
<span class="ltx_text ltx_font_bold" id="S3.T3.3.3.3.3.1">To-En</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.3.3.3.3.m1.1"><semantics id="S3.T3.3.3.3.3.m1.1a"><mo id="S3.T3.3.3.3.3.m1.1.1" stretchy="false" xref="S3.T3.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.3.m1.1b"><ci id="S3.T3.3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.4.4.4.4">
<span class="ltx_text ltx_font_bold" id="S3.T3.4.4.4.4.1">To-Other</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.4.4.4.4.m1.1"><semantics id="S3.T3.4.4.4.4.m1.1a"><mo id="S3.T3.4.4.4.4.m1.1.1" stretchy="false" xref="S3.T3.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.4.m1.1b"><ci id="S3.T3.4.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.4.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.4.4.5.1" rowspan="4"><span class="ltx_text" id="S3.T3.4.4.5.1.1"><span class="ltx_text" id="S3.T3.4.4.5.1.1.1"></span> <span class="ltx_text" id="S3.T3.4.4.5.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.4.4.5.1.1.2.1">
<span class="ltx_tr" id="S3.T3.4.4.5.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.4.4.5.1.1.2.1.1.1">Noise</span></span>
</span></span> <span class="ltx_text" id="S3.T3.4.4.5.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.4.4.5.2">T-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.5.3" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T3.4.4.5.3.1" style="background-color:#D4E7CF;">44.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.5.4">38.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.5.5">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T3.4.4.5.5.1">3.62</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.5.6">13.29</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.4.4.6.1">S-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.6.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T3.4.4.6.2.1" style="background-color:#D4E7CF;">14.09</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.6.3">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T3.4.4.6.3.1">0.44</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.6.4">72.46</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.6.5">13.01</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.4.4.7.1">ST-Enc</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.7.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T3.4.4.7.2.1" style="background-color:#D4E7CF;">â€„â€„2.15</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.7.3">â€„â€„3.71</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.7.4">81.96</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.7.5"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.7.5.1">12.18</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.4.4.8.1">ST-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.8.2" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.8.2.1" style="background-color:#D4E7CF;">55.99</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.8.3">â€„â€„7.22</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.8.4">12.08</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.8.5">24.71</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.9">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.4.4.9.1" rowspan="4"><span class="ltx_text" id="S3.T3.4.4.9.1.1"><span class="ltx_text" id="S3.T3.4.4.9.1.1.1"></span> <span class="ltx_text" id="S3.T3.4.4.9.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.4.4.9.1.1.2.1">
<span class="ltx_tr" id="S3.T3.4.4.9.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.4.4.9.1.1.2.1.1.1">De-</span></span>
<span class="ltx_tr" id="S3.T3.4.4.9.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.4.4.9.1.1.2.1.2.1">Noise</span></span>
</span></span> <span class="ltx_text" id="S3.T3.4.4.9.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.4.4.9.2">T-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.9.3" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.9.3.1" style="background-color:#D4E7CF;">81.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.9.4">â€„â€„2.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.9.5">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T3.4.4.9.5.1">3.17</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.4.9.6"><span class="ltx_text ltx_font_bold" id="S3.T3.4.4.9.6.1">12.68</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.4.4.10.1">S-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.10.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T3.4.4.10.2.1" style="background-color:#D4E7CF;">28.78</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.10.3">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T3.4.4.10.3.1">0.38</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.10.4">57.55</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.10.5">13.29</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T3.4.4.11.1">ST-Enc</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.11.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T3.4.4.11.2.1" style="background-color:#D4E7CF;">36.06</span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.11.3">â€„â€„6.96</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.11.4">39.40</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.4.11.5">17.58</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4.12">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.4.4.12.1">ST-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.12.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T3.4.4.12.2.1" style="background-color:#D4E7CF;">25.83</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.12.3">â€„â€„0.69</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.12.4">52.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.12.5">21.08</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
The average language rate (%) of several LT strategies on the OPUS-100 dataset.
<span class="ltx_text" id="S3.T3.6.1" style="background-color:#D4E7CF;">Acc</span>, To-Src, To-En, and To-Other denote the language rate of the expected target language, the source language, English, and other undesired languages in translation, respectively.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Error Distribution of Off-Target Issue</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we first conduct experiments on the OPUS-100 dataset, which contains 100 languages, to count the error distribution of the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">off-target</span> issue.
Prior study <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib33" title="">2022c</a>)</cite> points out that the noise in data is an important factor of the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">To-Source</span> issue.
Thus, we also count the error distribution with the denoised data<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Training details of both settings are shown in Appendix.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1" title="Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span>, which filters the noise language pairs with target sentences in wrong languages, following <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib33" title="">2022c</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T3" title="Table 3 â€£ 3.1 Metrics of Off-Target issue â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, the mainstream LT strategies suffer from the serious <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">off-target</span> issue.
Most mainstream LT strategies endure the grave <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">To-English</span> issue, which ranges from 39.40% to 57.55% even after data denoise, signifying that the language generation is disturbed by English.
Besides, the T-Enc and ST-Enc strategies tolerate the severer <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.3">To-Source</span> issue than other strategies, indicating that the target tag on the encoder side may be mixed with the indication to the source language.
Error distributions on both data settings suggest that the placement of LT has a non-ignorable impact on the <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.4">off-target</span> issue.
To understand the impact of LT better, we explore the language variation of language generation and the encoderâ€™s modeling tendency within different LT strategies.
Specifically, we employ the widely-used T-Enc and S-Enc-T-Dec strategies on the denoised OPUS-100 dataset to conduct analysis experiments.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Fine-grained Language Accuracy along Decoding Steps</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this section, we explore the variation of language indication in generation, by calculating the fine-grained language rate throughout decoding steps.</p>
</div>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S3.F1.3" style="width:211.9pt;height:358.4pt;vertical-align:-358.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.5pt,0.0pt) scale(0.485,0.485) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.1.1">
<div class="ltx_block ltx_minipage ltx_align_top" id="S3.F1.1.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S3.F1.1.1.1.g1" src="x1.png" width="830"/>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.2.2">
<div class="ltx_block ltx_minipage ltx_align_top" id="S3.F1.2.2.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S3.F1.2.2.1.g1" src="x2.png" width="830"/>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Fine-grained language rate of the desired target language (<em class="ltx_emph ltx_font_italic" id="S3.F1.5.1">i.e.</em>, language accuracy) and undesired English throughout decoding steps on the zero-shot testset of denoised OPUS-100, with 5 words in each interval of the final translation results.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.F1" title="Figure 1 â€£ 3.3 Fine-grained Language Accuracy along Decoding Steps â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, in terms of language accuracy, the S-Enc-T-Dec strategy exhibits a considerable decrease of approximately 20%.
Besides, as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.F1" title="Figure 1 â€£ 3.3 Fine-grained Language Accuracy along Decoding Steps â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, the rate of mistranslating into English of S-Enc-T-Dec increases by around 20%.
These variations signify that the indication from the S-Enc-T-Dec strategy is rapidly degraded and switched to English after generating a few tokens.
Hence, we conclude that the language indication of the S-Enc-T-Dec is decreasing after a few tokens, resulting in the bias to the most common language in the training set, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.1">i.e.</em>, English.
Conversely, the T-Enc strategy exhibits a more sufficient and stable language indication throughout the decoding steps.
Comparing both the above strategies, we conclude that the indication from the encoder could be more sufficient and stable and avoid being diminished during generation, while this setting suffers from the <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">To-Source</span> issue.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Language Indication in the Encoder</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To invest the target language indication in the encoder and the <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.1">To-Source</span> issue, we focus on two questions:
1) <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.2">How does the encoder model the target language tag and the input sentence of the source language, to obtain sufficient target language indication?</span>
2) <span class="ltx_text ltx_font_italic" id="S3.SS4.p1.1.3">Is there the encoderâ€™s language representation connected with the To-Source Issue?</span></p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">To answer the first question, we visualize the variation of language representation along the encoder layers, by calculating the similarity between the source and target languages in the zero-shot sentence pairs on the OPUS-100 dataset.
Following <cite class="ltx_cite ltx_citemacro_citet">Pan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib24" title="">2021</a>)</cite>, we adopt the average-pooled encoder layer output as the sentence representation, and then calculate their cosine similarity.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.F2" title="Figure 2 â€£ 3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, for the 6-layer encoder, the similarity scores of both LT strategies maintain the upward trend from the 1st layer to the 5th layer, and drop in the 6th layer, which is consistent with <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib35" title="">2021</a>)</cite>.
The variation suggests that the encoder tends to generate language-agnostic representation from different languages first and then reduce the similarity across languages.
In fact, the representation of the top layer in the T-Enc strategy tends to be target-language-specific, whereas the one of the S-Enc-T-Dec strategy tends to be source-language-specific, which is verified in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A4" title="Appendix D T-SNE Visualization â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">D</span></a>.
Hence, we could answer the first question that the encoder has a first-agnositc-second-specific tendency on language representation, the target LT mainly indicates the desired target language on the top encoder layer.
Further, we conjecture that the target LT is mixed with the source language features in the first stage of the tendency, resulting in the <span class="ltx_text ltx_font_italic" id="S3.SS4.p3.1.1">To-Source</span> issue.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F2.1.1" style="width:162.6pt;height:121.6pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-136.5pt,101.7pt) scale(0.37337436418783,0.37337436418783) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S3.F2.1.1.g1" src="x3.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Curves of the similarity of the language pairs along encoder layers on the zero-shot testset of denoised OPUS-100.
The higher similarity denotes the representation is more similar and language-agnostic.
</figcaption>
</figure>
<figure class="ltx_table" id="S3.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T4.4" style="width:208.1pt;height:82.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-54.6pt,21.7pt) scale(0.655889292081687,0.655889292081687) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T4.4.4">
<tr class="ltx_tr" id="S3.T4.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T4.4.4.4.5">
<span class="ltx_text ltx_font_bold" id="S3.T4.4.4.4.5.1">Noise</span>?</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T4.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S3.T4.4.4.4.6.1">Strategy</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.1.1" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.1.1" style="background-color:#D4E7CF;">Acc<span class="ltx_text ltx_font_medium" id="S3.T4.1.1.1.1.1.1"> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T4.1.1.1.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.1.1.m1.1.1" mathbackground="#D4E7CF" stretchy="false" xref="S3.T4.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.1.1.1.m1.1d">â†‘</annotation></semantics></math></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.2.2.2.2">
<span class="ltx_text ltx_font_bold" id="S3.T4.2.2.2.2.1">To-Src</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T4.2.2.2.2.m1.1"><semantics id="S3.T4.2.2.2.2.m1.1a"><mo id="S3.T4.2.2.2.2.m1.1.1" stretchy="false" xref="S3.T4.2.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.2.m1.1b"><ci id="S3.T4.2.2.2.2.m1.1.1.cmml" xref="S3.T4.2.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.2.2.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.3.3.3.3">
<span class="ltx_text ltx_font_bold" id="S3.T4.3.3.3.3.1">To-En</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T4.3.3.3.3.m1.1"><semantics id="S3.T4.3.3.3.3.m1.1a"><mo id="S3.T4.3.3.3.3.m1.1.1" stretchy="false" xref="S3.T4.3.3.3.3.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.3.m1.1b"><ci id="S3.T4.3.3.3.3.m1.1.1.cmml" xref="S3.T4.3.3.3.3.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.3.3.m1.1d">â†“</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.4.4.4.4">
<span class="ltx_text ltx_font_bold" id="S3.T4.4.4.4.4.1">To-Other</span> <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T4.4.4.4.4.m1.1"><semantics id="S3.T4.4.4.4.4.m1.1a"><mo id="S3.T4.4.4.4.4.m1.1.1" stretchy="false" xref="S3.T4.4.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.4.4.m1.1b"><ci id="S3.T4.4.4.4.4.m1.1.1.cmml" xref="S3.T4.4.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.4.4.m1.1d">â†“</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.4.4.5.1" rowspan="3"><span class="ltx_text" id="S3.T4.4.4.5.1.1"><span class="ltx_text" id="S3.T4.4.4.5.1.1.1"></span> <span class="ltx_text" id="S3.T4.4.4.5.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.4.4.5.1.1.2.1">
<span class="ltx_tr" id="S3.T4.4.4.5.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.4.5.1.1.2.1.1.1">Noise</span></span>
</span></span> <span class="ltx_text" id="S3.T4.4.4.5.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.4.4.5.2">T-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.5.3" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T4.4.4.5.3.1" style="background-color:#D4E7CF;">44.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.5.4">38.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.5.5">â€„â€„3.62</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.5.6">13.29</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.4.4.6.1">T-Enc-Mask</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.6.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T4.4.4.6.2.1" style="background-color:#D4E7CF;">48.49</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.6.3">13.01</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.6.4">25.61</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.6.5">12.89</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.4.4.7.1">LCS (ours)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.7.2" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T4.4.4.7.2.1" style="background-color:#D4E7CF;">85.35</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.7.3">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T4.4.4.7.3.1">0.27</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.7.4">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T4.4.4.7.4.1">2.65</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.7.5"><span class="ltx_text ltx_font_bold" id="S3.T4.4.4.7.5.1">11.73</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T4.4.4.8.1" rowspan="3"><span class="ltx_text" id="S3.T4.4.4.8.1.1"><span class="ltx_text" id="S3.T4.4.4.8.1.1.1"></span> <span class="ltx_text" id="S3.T4.4.4.8.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T4.4.4.8.1.1.2.1">
<span class="ltx_tr" id="S3.T4.4.4.8.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.4.8.1.1.2.1.1.1">De-</span></span>
<span class="ltx_tr" id="S3.T4.4.4.8.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T4.4.4.8.1.1.2.1.2.1">Noise</span></span>
</span></span> <span class="ltx_text" id="S3.T4.4.4.8.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.4.4.8.2">T-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.8.3" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T4.4.4.8.3.1" style="background-color:#D4E7CF;">81.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.8.4">â€„â€„2.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.8.5">â€„â€„3.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.4.4.8.6">12.68</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.4.4.9.1">T-Enc-Mask</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.9.2" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T4.4.4.9.2.1" style="background-color:#D4E7CF;">78.62</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.9.3">â€„â€„0.39</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.9.4">â€„â€„9.62</td>
<td class="ltx_td ltx_align_center" id="S3.T4.4.4.9.5">11.37</td>
</tr>
<tr class="ltx_tr" id="S3.T4.4.4.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T4.4.4.10.1">LCS (ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.4.4.10.2" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T4.4.4.10.2.1" style="background-color:#D4E7CF;">86.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.4.4.10.3">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T4.4.4.10.3.1">0.19</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.4.4.10.4">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T4.4.4.10.4.1">2.08</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.4.4.10.5"><span class="ltx_text ltx_font_bold" id="S3.T4.4.4.10.5.1">11.06</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The average language rate (%) on the zero-shot testset of OPUS-100.
T-Enc-Mask denotes the source LT is masked in each layer of 4 shallow encoder layers.
<span class="ltx_text" id="S3.T4.6.1" style="background-color:#D4E7CF;">Acc</span>, To-Src, To-En, and To-Other denote the language rate of the expected target language, the source language, English, and other undesired languages in translation, respectively.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.2">To answer the second question and verify our conjecture, we apply a simple operation to the T-Enc strategy.
Specifically, we mask the target language tag in the 4 shallow encoder layers and restore it in the 5th layer<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Our prior experiment shows that restoring the LT in the 5th layer performs better.
And we roughly implement it via masking and adding the initial embedding of the tag.</span></span></span>.
As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T4" title="Table 4 â€£ 3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, this operation could significantly mitigate the <span class="ltx_text ltx_font_italic" id="S3.SS4.p4.2.1">To-Source</span> issue in the T-Enc strategy, reducing to 13.01% (<math alttext="\mathit{\Delta}" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1.1"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">ğ›¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\mathit{\Delta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.1.m1.1d">italic_Î”</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.SS4.p4.2.2">=-25.7%</span>) and 0.39% (<math alttext="\mathit{\Delta}" class="ltx_Math" display="inline" id="S3.SS4.p4.2.m2.1"><semantics id="S3.SS4.p4.2.m2.1a"><mi id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><ci id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1">ğ›¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">\mathit{\Delta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p4.2.m2.1d">italic_Î”</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S3.SS4.p4.2.3">=-2.12%</span>) on this issue.
The remission on <span class="ltx_text ltx_font_italic" id="S3.SS4.p4.2.4">To-Source</span> issue verifies our conjecture and responds to the second question that the first-agnostic-second-specific tendency introduces bias into the target LT in the first stage.</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">In conclusion, we summarize our findings on the impact of the placement of LT on the <span class="ltx_text ltx_font_italic" id="S3.SS4.p5.1.1">off-target</span> issue as follows:
1) The language indication from the encoder side is more sufficient and stable, without being diminished during decoding;
2) The target language is mainly indicated on the top encoder layers, and placing the target LT at the bottom layer of the encoder introduces the <span class="ltx_text ltx_font_italic" id="S3.SS4.p5.1.2">To-Source</span> issue.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.F3.1.1" style="width:206.0pt;height:115.7pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-114.8pt,64.2pt) scale(0.472946922664891,0.472946922664891) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="467" id="S3.F3.1.1.g1" src="x4.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Illustration of the encoder of LCS.
For the target, only the language tag could be seen by the encoder.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Language Converter Strategy</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Based on the conclusions in Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS3" title="3.3 Fine-grained Language Accuracy along Decoding Steps â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.3</span></a> and Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS4" title="3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a>, we propose <span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.1">L</span>anguage <span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.2">C</span>onverter <span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.3">S</span>trategy (<span class="ltx_text ltx_font_bold" id="S3.SS5.p1.1.4">LCS</span>) to enhance the language indication to mitigate the <span class="ltx_text ltx_font_italic" id="S3.SS5.p1.1.5">off-target</span> issue, which further improves the quality of zero-shot translation.</p>
</div>
<figure class="ltx_table" id="S3.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T5.3" style="width:433.6pt;height:98.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-103.1pt,23.2pt) scale(0.67782431468837,0.67782431468837) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T5.3.1">
<tr class="ltx_tr" id="S3.T5.3.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T5.3.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S3.T5.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.1.2.1">MultiUN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S3.T5.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.1.3.1">TED</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T5.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.1.4.1">OPUS-100</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.2">
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.1.1">Supervised</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.2"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.2.1">Zero-Shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.2.3" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.3.1" style="background-color:#D4E7CF;">Accuracy</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.4"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.4.1">Supervised</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.5"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.5.1">Zero-Shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.2.6" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.6.1" style="background-color:#D4E7CF;">Accuracy</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.7"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.7.1">Supervised</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.8"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.8.1">Zero-Shot</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.2.9" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.2.9.1" style="background-color:#D4E7CF;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.3.1.3.1">T-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.2">50.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.3">32.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.3.1.3.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.3.4.1" style="background-color:#D4E7CF;">92.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.5">25.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.6">10.41</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.3.1.3.7" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.3.7.1" style="background-color:#D4E7CF;">94.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.8">24.72 / 22.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.9">â€„â€„7.29 / 12.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.3.1.3.10" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.3.10.1" style="background-color:#D4E7CF;">44.38 / 81.64</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.4.1">S-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.2">50.71</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.3">23.63</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.4.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.4.4.1" style="background-color:#D4E7CF;">71.85</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.5"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.4.5.1">25.27</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.6">â€„â€„2.36</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.4.7" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.4.7.1" style="background-color:#D4E7CF;">62.88</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.8">24.74 / 22.07</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.9">â€„â€„3.80 / 4.71</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.4.10" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.4.10.1" style="background-color:#D4E7CF;">14.09 / 28.78</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.5.1">ST-Enc</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.2">50.66</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.3">â€„â€„1.15</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.5.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.5.4.1" style="background-color:#D4E7CF;">â€„â€„0.27</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.5">24.47</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.6">â€„â€„1.37</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.5.7" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.5.7.1" style="background-color:#D4E7CF;">13.66</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.8">24.82 / 22.82</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.9">â€„â€„1.92 / 7.38</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.5.10" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.5.10.1" style="background-color:#D4E7CF;">â€„â€„2.15 / 36.06</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.6.1">ST-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.2">50.66</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.3">19.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.6.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.6.4.1" style="background-color:#D4E7CF;">64.15</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.5">24.51</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.6">â€„â€„4.59</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.6.7" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.6.7.1" style="background-color:#D4E7CF;">79.02</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.8">
<span class="ltx_text ltx_font_bold" id="S3.T5.3.1.6.8.1">25.05</span> / 22.11</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.9">â€„â€„4.23 / 3.60</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.6.10" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.6.10.1" style="background-color:#D4E7CF;">55.99 / 25.83</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.7.1">T-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.2"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.7.2.1">50.83</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.3">32.23</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.7.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.7.4.1" style="background-color:#D4E7CF;">91.10</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.5">24.53</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.6">12.07</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.3.1.7.7" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.7.7.1" style="background-color:#D4E7CF;">95.73</span></td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.8">24.75 / <span class="ltx_text ltx_font_bold" id="S3.T5.3.1.7.8.1">22.89</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.9">â€„â€„5.97 / 12.66</td>
<td class="ltx_td ltx_align_center" id="S3.T5.3.1.7.10" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.7.10.1" style="background-color:#D4E7CF;">35.26 / 82.73</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.3.1.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T5.3.1.8.1">LCS (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.2">50.75</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.3">â€„<span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.3.1">36.03</span>*</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T5.3.1.8.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.8.4.1" style="background-color:#D4E7CF;">â€„<span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.4.1.1">95.28</span>*</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.5"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.5.1">25.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.6">â€„â€„<span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.6.1">13.71</span>*</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T5.3.1.8.7" style="background-color:#D4E7CF;"><span class="ltx_text" id="S3.T5.3.1.8.7.1" style="background-color:#D4E7CF;">â€„<span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.7.1.1">96.21</span>*</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.8">24.80 / 22.09</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.9">
<span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.9.1">15.22</span>* / <span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.9.2">15.58</span>*</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.3.1.8.10" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S3.T5.3.1.8.10.1" style="background-color:#D4E7CF;">85.35<span class="ltx_text ltx_font_medium" id="S3.T5.3.1.8.10.1.1">* / </span>86.67<span class="ltx_text ltx_font_medium" id="S3.T5.3.1.8.10.1.2">*</span></span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>
Experiments in several LT strategies on the MultiUN, TED, and OPUS-100 datasets.
Supervised and Zero-Shot denote the average BLEU scores of the supervised and zero-shot translation directions.
<span class="ltx_text" id="S3.T5.6.1" style="background-color:#D4E7CF;">Accuracy</span> denotes the averaged language accuracy (%) of zero-shot translation.
"A / B" separates the scores of noise data and denoise data in OPUS-100, where â€˜Aâ€™ and â€˜Bâ€™ represent the result of the noise and denoised version, respectively.
Results with */** are statistically better than â€˜T-Encâ€™ in all translation directions with <math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S3.T5.2.m1.1"><semantics id="S3.T5.2.m1.1b"><mrow id="S3.T5.2.m1.1.1" xref="S3.T5.2.m1.1.1.cmml"><mi id="S3.T5.2.m1.1.1.2" xref="S3.T5.2.m1.1.1.2.cmml">p</mi><mo id="S3.T5.2.m1.1.1.1" xref="S3.T5.2.m1.1.1.1.cmml">&lt;</mo><mn id="S3.T5.2.m1.1.1.3" xref="S3.T5.2.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.2.m1.1c"><apply id="S3.T5.2.m1.1.1.cmml" xref="S3.T5.2.m1.1.1"><lt id="S3.T5.2.m1.1.1.1.cmml" xref="S3.T5.2.m1.1.1.1"></lt><ci id="S3.T5.2.m1.1.1.2.cmml" xref="S3.T5.2.m1.1.1.2">ğ‘</ci><cn id="S3.T5.2.m1.1.1.3.cmml" type="float" xref="S3.T5.2.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.m1.1d">p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S3.T5.2.m1.1e">italic_p &lt; 0.01</annotation></semantics></math>.
<span class="ltx_text ltx_font_bold" id="S3.T5.7.2">Bold</span> denotes the best performance.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">According to our conclusions, as shown in FigÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.F3" title="Figure 3 â€£ 3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, we divide the encoder layers into the shallow layers and the deep language converter layers by the <math alttext="k" class="ltx_Math" display="inline" id="S3.SS5.p2.1.m1.1"><semantics id="S3.SS5.p2.1.m1.1a"><mi id="S3.SS5.p2.1.m1.1.1" xref="S3.SS5.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p2.1.m1.1b"><ci id="S3.SS5.p2.1.m1.1.1.cmml" xref="S3.SS5.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p2.1.m1.1d">italic_k</annotation></semantics></math>-th<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><math alttext="k" class="ltx_Math" display="inline" id="footnote8.m1.1"><semantics id="footnote8.m1.1b"><mi id="footnote8.m1.1.1" xref="footnote8.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote8.m1.1c"><ci id="footnote8.m1.1.1.cmml" xref="footnote8.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="footnote8.m1.1e">italic_k</annotation></semantics></math> is a hyperparameter, describing the number of the language converter layers.
We set it to 2 in the 6-layer encoder and explore the selection of <math alttext="k" class="ltx_Math" display="inline" id="footnote8.m2.1"><semantics id="footnote8.m2.1b"><mi id="footnote8.m2.1.1" xref="footnote8.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote8.m2.1c"><ci id="footnote8.m2.1.1.cmml" xref="footnote8.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="footnote8.m2.1e">italic_k</annotation></semantics></math> in Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS2" title="5.2 Selection of Hyperparameter ğ‘˜ â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</span></span></span> deepest layer.
In the shallow layers, we place the source language tag in front of the sentences to avoid the <span class="ltx_text ltx_font_italic" id="S3.SS5.p2.1.1">To-Source</span> issue.
In the deep language converter layers, we introduce the target language embedding as auxiliary signals to prompt the desired target language.
As pointed out by prior studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Bjerva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib1" title="">2019</a>; Oncevay etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib21" title="">2020</a>; Jin and Xiong, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>, the target language embeddings contain target language-specific features, which could provide sufficient indication of the target language into the top encoder layers.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.6">Then the final calculation of the self-attention block <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib29" title="">2017</a>)</cite> in language converter layers is as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{h}_{i}=h_{i}+e^{t}," class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E4.m1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.2.2.cmml"><mi id="S3.E4.m1.1.1.1.1.2.2.2" xref="S3.E4.m1.1.1.1.1.2.2.2.cmml">h</mi><mo id="S3.E4.m1.1.1.1.1.2.2.1" xref="S3.E4.m1.1.1.1.1.2.2.1.cmml">~</mo></mover><mi id="S3.E4.m1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><msub id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2.2" xref="S3.E4.m1.1.1.1.1.3.2.2.cmml">h</mi><mi id="S3.E4.m1.1.1.1.1.3.2.3" xref="S3.E4.m1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S3.E4.m1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.3.1.cmml">+</mo><msup id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml">e</mi><mi id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">t</mi></msup></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"></eq><apply id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2">subscript</csymbol><apply id="S3.E4.m1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2"><ci id="S3.E4.m1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.2.2.1">~</ci><ci id="S3.E4.m1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.2.2.2">â„</ci></apply><ci id="S3.E4.m1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><plus id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1"></plus><apply id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.2.1.cmml" xref="S3.E4.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2.2">â„</ci><ci id="S3.E4.m1.1.1.1.1.3.2.3.cmml" xref="S3.E4.m1.1.1.1.1.3.2.3">ğ‘–</ci></apply><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3">superscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">ğ‘’</ci><ci id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\tilde{h}_{i}=h_{i}+e^{t},</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">over~ start_ARG italic_h end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_e start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s=LayerNorm(\tilde{h}+SelfAttn(\tilde{h}))," class="ltx_Math" display="block" id="S3.E5.m1.2"><semantics id="S3.E5.m1.2a"><mrow id="S3.E5.m1.2.2.1" xref="S3.E5.m1.2.2.1.1.cmml"><mrow id="S3.E5.m1.2.2.1.1" xref="S3.E5.m1.2.2.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.3" xref="S3.E5.m1.2.2.1.1.3.cmml">s</mi><mo id="S3.E5.m1.2.2.1.1.2" xref="S3.E5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.2.2.1.1.1" xref="S3.E5.m1.2.2.1.1.1.cmml"><mi id="S3.E5.m1.2.2.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.3.cmml">L</mi><mo id="S3.E5.m1.2.2.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.4" xref="S3.E5.m1.2.2.1.1.1.4.cmml">a</mi><mo id="S3.E5.m1.2.2.1.1.1.2a" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.5" xref="S3.E5.m1.2.2.1.1.1.5.cmml">y</mi><mo id="S3.E5.m1.2.2.1.1.1.2b" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.6" xref="S3.E5.m1.2.2.1.1.1.6.cmml">e</mi><mo id="S3.E5.m1.2.2.1.1.1.2c" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.7" xref="S3.E5.m1.2.2.1.1.1.7.cmml">r</mi><mo id="S3.E5.m1.2.2.1.1.1.2d" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.8" xref="S3.E5.m1.2.2.1.1.1.8.cmml">N</mi><mo id="S3.E5.m1.2.2.1.1.1.2e" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.9" xref="S3.E5.m1.2.2.1.1.1.9.cmml">o</mi><mo id="S3.E5.m1.2.2.1.1.1.2f" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.10" xref="S3.E5.m1.2.2.1.1.1.10.cmml">r</mi><mo id="S3.E5.m1.2.2.1.1.1.2g" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.11" xref="S3.E5.m1.2.2.1.1.1.11.cmml">m</mi><mo id="S3.E5.m1.2.2.1.1.1.2h" xref="S3.E5.m1.2.2.1.1.1.2.cmml">â¢</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E5.m1.2.2.1.1.1.1.1.1.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.2.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.2.cmml">h</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.2.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.1.cmml">~</mo></mover><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.cmml">S</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.3" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.3.cmml">e</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1a" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.cmml">l</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1b" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.5" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.5.cmml">f</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1c" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.6" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.6.cmml">A</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1d" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.7" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.7.cmml">t</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1e" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.8" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.8.cmml">t</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1f" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E5.m1.2.2.1.1.1.1.1.1.3.9" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.9.cmml">n</mi><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1g" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S3.E5.m1.2.2.1.1.1.1.1.1.3.10.2" xref="S3.E5.m1.1.1.cmml"><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.10.2.1" stretchy="false" xref="S3.E5.m1.1.1.cmml">(</mo><mover accent="true" id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><mi id="S3.E5.m1.1.1.2" xref="S3.E5.m1.1.1.2.cmml">h</mi><mo id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml">~</mo></mover><mo id="S3.E5.m1.2.2.1.1.1.1.1.1.3.10.2.2" stretchy="false" xref="S3.E5.m1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.2.2.1.2" xref="S3.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.2b"><apply id="S3.E5.m1.2.2.1.1.cmml" xref="S3.E5.m1.2.2.1"><eq id="S3.E5.m1.2.2.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.2"></eq><ci id="S3.E5.m1.2.2.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.3">ğ‘ </ci><apply id="S3.E5.m1.2.2.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1"><times id="S3.E5.m1.2.2.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.2"></times><ci id="S3.E5.m1.2.2.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.3">ğ¿</ci><ci id="S3.E5.m1.2.2.1.1.1.4.cmml" xref="S3.E5.m1.2.2.1.1.1.4">ğ‘</ci><ci id="S3.E5.m1.2.2.1.1.1.5.cmml" xref="S3.E5.m1.2.2.1.1.1.5">ğ‘¦</ci><ci id="S3.E5.m1.2.2.1.1.1.6.cmml" xref="S3.E5.m1.2.2.1.1.1.6">ğ‘’</ci><ci id="S3.E5.m1.2.2.1.1.1.7.cmml" xref="S3.E5.m1.2.2.1.1.1.7">ğ‘Ÿ</ci><ci id="S3.E5.m1.2.2.1.1.1.8.cmml" xref="S3.E5.m1.2.2.1.1.1.8">ğ‘</ci><ci id="S3.E5.m1.2.2.1.1.1.9.cmml" xref="S3.E5.m1.2.2.1.1.1.9">ğ‘œ</ci><ci id="S3.E5.m1.2.2.1.1.1.10.cmml" xref="S3.E5.m1.2.2.1.1.1.10">ğ‘Ÿ</ci><ci id="S3.E5.m1.2.2.1.1.1.11.cmml" xref="S3.E5.m1.2.2.1.1.1.11">ğ‘š</ci><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1"><plus id="S3.E5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.1"></plus><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2"><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.1">~</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.2.2">â„</ci></apply><apply id="S3.E5.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3"><times id="S3.E5.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.1"></times><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.2">ğ‘†</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.3">ğ‘’</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.4.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.4">ğ‘™</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.5.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.5">ğ‘“</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.6.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.6">ğ´</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.7.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.7">ğ‘¡</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.8.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.8">ğ‘¡</ci><ci id="S3.E5.m1.2.2.1.1.1.1.1.1.3.9.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.9">ğ‘›</ci><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.2.2.1.1.1.1.1.1.3.10.2"><ci id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1">~</ci><ci id="S3.E5.m1.1.1.2.cmml" xref="S3.E5.m1.1.1.2">â„</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.2c">s=LayerNorm(\tilde{h}+SelfAttn(\tilde{h})),</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.2d">italic_s = italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m ( over~ start_ARG italic_h end_ARG + italic_S italic_e italic_l italic_f italic_A italic_t italic_t italic_n ( over~ start_ARG italic_h end_ARG ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS5.p3.5">where <math alttext="h_{i}" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><msub id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml">h</mi><mi id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">â„</ci><ci id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">h_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> denotes the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS5.p3.2.m2.1"><semantics id="S3.SS5.p3.2.m2.1a"><mi id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><ci id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">italic_i</annotation></semantics></math>-th state of input tokens to each converter layer, <math alttext="s" class="ltx_Math" display="inline" id="S3.SS5.p3.3.m3.1"><semantics id="S3.SS5.p3.3.m3.1a"><mi id="S3.SS5.p3.3.m3.1.1" xref="S3.SS5.p3.3.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.3.m3.1b"><ci id="S3.SS5.p3.3.m3.1.1.cmml" xref="S3.SS5.p3.3.m3.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.3.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.3.m3.1d">italic_s</annotation></semantics></math> denotes the output states of self-attention block in each converter layer, <math alttext="SelfAttn" class="ltx_Math" display="inline" id="S3.SS5.p3.4.m4.1"><semantics id="S3.SS5.p3.4.m4.1a"><mrow id="S3.SS5.p3.4.m4.1.1" xref="S3.SS5.p3.4.m4.1.1.cmml"><mi id="S3.SS5.p3.4.m4.1.1.2" xref="S3.SS5.p3.4.m4.1.1.2.cmml">S</mi><mo id="S3.SS5.p3.4.m4.1.1.1" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.3" xref="S3.SS5.p3.4.m4.1.1.3.cmml">e</mi><mo id="S3.SS5.p3.4.m4.1.1.1a" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.4" xref="S3.SS5.p3.4.m4.1.1.4.cmml">l</mi><mo id="S3.SS5.p3.4.m4.1.1.1b" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.5" xref="S3.SS5.p3.4.m4.1.1.5.cmml">f</mi><mo id="S3.SS5.p3.4.m4.1.1.1c" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.6" xref="S3.SS5.p3.4.m4.1.1.6.cmml">A</mi><mo id="S3.SS5.p3.4.m4.1.1.1d" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.7" xref="S3.SS5.p3.4.m4.1.1.7.cmml">t</mi><mo id="S3.SS5.p3.4.m4.1.1.1e" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.8" xref="S3.SS5.p3.4.m4.1.1.8.cmml">t</mi><mo id="S3.SS5.p3.4.m4.1.1.1f" xref="S3.SS5.p3.4.m4.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.4.m4.1.1.9" xref="S3.SS5.p3.4.m4.1.1.9.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.4.m4.1b"><apply id="S3.SS5.p3.4.m4.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1"><times id="S3.SS5.p3.4.m4.1.1.1.cmml" xref="S3.SS5.p3.4.m4.1.1.1"></times><ci id="S3.SS5.p3.4.m4.1.1.2.cmml" xref="S3.SS5.p3.4.m4.1.1.2">ğ‘†</ci><ci id="S3.SS5.p3.4.m4.1.1.3.cmml" xref="S3.SS5.p3.4.m4.1.1.3">ğ‘’</ci><ci id="S3.SS5.p3.4.m4.1.1.4.cmml" xref="S3.SS5.p3.4.m4.1.1.4">ğ‘™</ci><ci id="S3.SS5.p3.4.m4.1.1.5.cmml" xref="S3.SS5.p3.4.m4.1.1.5">ğ‘“</ci><ci id="S3.SS5.p3.4.m4.1.1.6.cmml" xref="S3.SS5.p3.4.m4.1.1.6">ğ´</ci><ci id="S3.SS5.p3.4.m4.1.1.7.cmml" xref="S3.SS5.p3.4.m4.1.1.7">ğ‘¡</ci><ci id="S3.SS5.p3.4.m4.1.1.8.cmml" xref="S3.SS5.p3.4.m4.1.1.8">ğ‘¡</ci><ci id="S3.SS5.p3.4.m4.1.1.9.cmml" xref="S3.SS5.p3.4.m4.1.1.9">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.4.m4.1c">SelfAttn</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.4.m4.1d">italic_S italic_e italic_l italic_f italic_A italic_t italic_t italic_n</annotation></semantics></math> denotes the calculation of self-attention and <math alttext="LayerNorm" class="ltx_Math" display="inline" id="S3.SS5.p3.5.m5.1"><semantics id="S3.SS5.p3.5.m5.1a"><mrow id="S3.SS5.p3.5.m5.1.1" xref="S3.SS5.p3.5.m5.1.1.cmml"><mi id="S3.SS5.p3.5.m5.1.1.2" xref="S3.SS5.p3.5.m5.1.1.2.cmml">L</mi><mo id="S3.SS5.p3.5.m5.1.1.1" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.3" xref="S3.SS5.p3.5.m5.1.1.3.cmml">a</mi><mo id="S3.SS5.p3.5.m5.1.1.1a" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.4" xref="S3.SS5.p3.5.m5.1.1.4.cmml">y</mi><mo id="S3.SS5.p3.5.m5.1.1.1b" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.5" xref="S3.SS5.p3.5.m5.1.1.5.cmml">e</mi><mo id="S3.SS5.p3.5.m5.1.1.1c" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.6" xref="S3.SS5.p3.5.m5.1.1.6.cmml">r</mi><mo id="S3.SS5.p3.5.m5.1.1.1d" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.7" xref="S3.SS5.p3.5.m5.1.1.7.cmml">N</mi><mo id="S3.SS5.p3.5.m5.1.1.1e" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.8" xref="S3.SS5.p3.5.m5.1.1.8.cmml">o</mi><mo id="S3.SS5.p3.5.m5.1.1.1f" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.9" xref="S3.SS5.p3.5.m5.1.1.9.cmml">r</mi><mo id="S3.SS5.p3.5.m5.1.1.1g" xref="S3.SS5.p3.5.m5.1.1.1.cmml">â¢</mo><mi id="S3.SS5.p3.5.m5.1.1.10" xref="S3.SS5.p3.5.m5.1.1.10.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.5.m5.1b"><apply id="S3.SS5.p3.5.m5.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1"><times id="S3.SS5.p3.5.m5.1.1.1.cmml" xref="S3.SS5.p3.5.m5.1.1.1"></times><ci id="S3.SS5.p3.5.m5.1.1.2.cmml" xref="S3.SS5.p3.5.m5.1.1.2">ğ¿</ci><ci id="S3.SS5.p3.5.m5.1.1.3.cmml" xref="S3.SS5.p3.5.m5.1.1.3">ğ‘</ci><ci id="S3.SS5.p3.5.m5.1.1.4.cmml" xref="S3.SS5.p3.5.m5.1.1.4">ğ‘¦</ci><ci id="S3.SS5.p3.5.m5.1.1.5.cmml" xref="S3.SS5.p3.5.m5.1.1.5">ğ‘’</ci><ci id="S3.SS5.p3.5.m5.1.1.6.cmml" xref="S3.SS5.p3.5.m5.1.1.6">ğ‘Ÿ</ci><ci id="S3.SS5.p3.5.m5.1.1.7.cmml" xref="S3.SS5.p3.5.m5.1.1.7">ğ‘</ci><ci id="S3.SS5.p3.5.m5.1.1.8.cmml" xref="S3.SS5.p3.5.m5.1.1.8">ğ‘œ</ci><ci id="S3.SS5.p3.5.m5.1.1.9.cmml" xref="S3.SS5.p3.5.m5.1.1.9">ğ‘Ÿ</ci><ci id="S3.SS5.p3.5.m5.1.1.10.cmml" xref="S3.SS5.p3.5.m5.1.1.10">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.5.m5.1c">LayerNorm</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.5.m5.1d">italic_L italic_a italic_y italic_e italic_r italic_N italic_o italic_r italic_m</annotation></semantics></math> represents the LayerNorm function.
Since the language tags have already been included in the vocabulary, LCS introduces no extra parameters.
Besides, we maintain the cross-entropy loss to optimize the MNMT model.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1">Moreover, we place the target language tag in front of the decoder input to indicate the target language better.
We verify the effectiveness of this placement in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2" title="Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T6.1" style="width:206.0pt;height:65.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.0pt,12.1pt) scale(0.730540666303198,0.730540666303198) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T6.1.1">
<tr class="ltx_tr" id="S3.T6.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T6.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S3.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.2.1">Langs &amp; Dirs</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T6.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.3.1">Train / Valid / Test</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.2">
<td class="ltx_td ltx_align_center" id="S3.T6.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.2.1.1">Supervised</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.2.2.1">Zero-Shot</span></td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T6.1.1.3.1">MultiUN</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.1.1.3.2">4â€„â€„ &amp; â€‰â€„6</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.1.1.3.3">3â€„ &amp; â€„â€‰6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T6.1.1.3.4">2Mâ€„Â â€ƒ/ 4K / 4K</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T6.1.1.4.1">TED</td>
<td class="ltx_td ltx_align_center" id="S3.T6.1.1.4.2">20â€„ &amp; â€‰38</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.1.1.4.3">19 &amp; 342</td>
<td class="ltx_td ltx_align_right" id="S3.T6.1.1.4.4">14K-22K / 5K / 5K</td>
</tr>
<tr class="ltx_tr" id="S3.T6.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T6.1.1.5.1">OPUS-100</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.1.1.5.2">100 â€‰&amp; 198</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T6.1.1.5.3">6â€„ &amp; â€„30</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T6.1.1.5.4">10K-1Mâ€„ / 2K / 2K</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>
Data statistics.
Langs&amp;Dirs represents the number of languages and translation directions involved in the supervised and zero-shot translation.
Train / Valid / Test represents the number of samples in each translation direction in the training / validation / test set.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We conduct experiments on three popular datasets, MultiUN, TED, and OPUS-100 <cite class="ltx_cite ltx_citemacro_citep">(Gu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib10" title="">2019</a>; Qi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib25" title="">2018</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib39" title="">2020</a>)</cite>.
The statistics of the datasets of each translation direction are presented in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T6" title="Table 6 â€£ 3.5 Language Converter Strategy â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>.
(Please refer to AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1" title="Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a> for more details.)
For direct comparison, we report scores of noise and denoised data versions of OPUS-100.
For all these datasets, English is the central language in the training sets, serving as either the source or the target language in each sentence pair.
Following <cite class="ltx_cite ltx_citemacro_citet">Johnson etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib12" title="">2017</a>)</cite>, we consider translation directions involving English as supervised translation and directions between non-English languages as zero-shot translation.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We employ open-source toolkit fairseq<cite class="ltx_cite ltx_citemacro_citep">(Ott etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib22" title="">2019</a>)</cite> to implement the Transformer models, with mixed precision <cite class="ltx_cite ltx_citemacro_citep">(Ott etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib23" title="">2018</a>)</cite>.
During inference, we set beam size to 5 and length penalty to 1.0, following existing studies <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>; Jin and Xiong, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>.
We apply SacreBLEU to calculate BLEU and report the averaged scores for all models.
More details about the dataset, training, and evaluation can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1" title="Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We respectively establish MNMT models in the several LT strategies in Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S2.SS2" title="2.2 Language Tag Strategy â€£ 2 Background â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2.2</span></a> and our LCS on the three datasets and list the results in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T5" title="Table 5 â€£ 3.5 Language Converter Strategy â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>.
Besides, we conduct experiments in the T-Enc-T-Dec, which seems to combine the indication from both encoder and decoder sides.
Moreover, we also report the results of the denoised OPUS-100 dataset for a comprehensive comparison, where MultiUN and TED are low-noise datasets.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T5" title="Table 5 â€£ 3.5 Language Converter Strategy â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, our proposed LCS yields the highest language accuracy and best performance of zero-shot translation on all datasets among all LT strategies.
Specifically, compared to the widely-used T-Enc strategy, LCS effectively mitigates the <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">off-target</span> issue by improving language accuracy up to 95.28% (<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">+2.7%</span>), 96.21% (<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">+1.71%</span>), 85.35% (<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.4">+40.97%</span>), and 86.67% (<span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.5">+5.03%</span>) on zero-shot translation of MultiUN, TED, OPUS-100 (noise and denoised) datasets, respectively.
Furthermore, LCS outperforms the T-Enc strategy by 3.07, 3.30, 7.93, and 2.93 BLEU scores improvements on zero-shot translation of these datasets, respectively.
While bringing conspicuous improvement to zero-shot translation, LCS maintains the performance of supervised translation.
Unfortunately, since the language detection toolkit is lowly accurate for medium- and low-resource languages, the performance on related translation pairs is reduced in the denoised OPUS-100.
In this case, LCS performs well and robust on zero-shot translation on the noise version of the OPUS-100 dataset, compared to the denoise version.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We also probe the performance of our method in prior aspects, <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.1">i.e.</em>, the distribution of the <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.2">off-target</span> issue, and the language variation in the encoder.
As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T4" title="Table 4 â€£ 3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, LCS could yield the highest language accuracy, and the lowest rates on the <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">To-Source</span> and <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.4">To-English</span> issue.
These scores suggest that LCS could provide the most accurate target language indication for zero-shot translation, and avoid being mixed with the indication of source language.
Since LCS performs better than T-Enc on the <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.5">To-English</span> issue, we consider that LCS also provides sufficient and stable indication during generation.
Besides, in terms of language representation variation, our proposed LCS yields the lowest score in the 6-th layer than the T-Enc strategy, meaning that LCS could generate more target-language-specific representation to indicate the target language better.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we explore the details of LCS to understand it better.
We invest the generalizability of LCS to other approaches (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS1" title="5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5.1</span></a>) and the effect of hyperparameter <math alttext="k" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_k</annotation></semantics></math> (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS2" title="5.2 Selection of Hyperparameter ğ‘˜ â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5.2</span></a>), and the application of LCS to deeper encoders (Â§<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.SS3" title="5.3 Effect of LCS on deeper encoders â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Application to Stronger Approaches</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We evaluate the generalizability of LCS on OPUS-100 by applying it to the following stronger approaches:</p>
</div>
<figure class="ltx_table" id="S5.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T7.1" style="width:206.0pt;height:186.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.8pt,41.6pt) scale(0.692148883752323,0.692148883752323) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T7.1.1">
<tr class="ltx_tr" id="S5.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.2.1">Supervised</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.3.1">Zero-Shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.1.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.1.4.1" style="background-color:#D4E7CF;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.1.1.2.1">S-Enc-T-Dec</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.2">24.74 / 22.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.3">â€„â€„3.80 / â€„â€„4.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.2.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.2.4.1" style="background-color:#D4E7CF;">14.09 / 28.78</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.3.1">FT &amp; LCS</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.3.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.2.1">25.29</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.2.2">22.34</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.3.3">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.3.1">15.45</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.3.2">15.90</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.3.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.3.4.1" style="background-color:#D4E7CF;">85.26<span class="ltx_text ltx_font_medium" id="S5.T7.1.1.3.4.1.1"> / </span>86.32</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.1.1.4.1">DisPI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.4.2">24.62 / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.4.2.1">22.09</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.4.3">â€„â€„4.80 / â€„â€„4.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.4.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.4.4.1" style="background-color:#D4E7CF;">18.91 / 30.36</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.5.1">DisPI &amp; LCS</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.5.2.1">24.78</span> / 21.87</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.3">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.5.3.1">15.71</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.5.3.2">15.80</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.5.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.5.4.1" style="background-color:#D4E7CF;">85.34<span class="ltx_text ltx_font_medium" id="S5.T7.1.1.5.4.1.1"> / </span>87.35</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.1.1.6.1">DNâ€ </td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.6.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.6.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.6.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.7.1">DN</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.2">19.04 / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.7.2.1">22.83</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.3">â€„â€„3.02 / 12.62</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.7.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.7.4.1" style="background-color:#D4E7CF;">21.75 / 81.66</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.8.1">DN &amp; LCS</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.2.1">24.32</span> / 22.10</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.3">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.3.1">14.00</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.3.2">15.55</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.8.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.8.4.1" style="background-color:#D4E7CF;">81.81<span class="ltx_text ltx_font_medium" id="S5.T7.1.1.8.4.1.1"> / </span>86.60</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.1.1.9.1">LEEâ€ </td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.9.2.1">24.98</span> / -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.3">10.08 / -</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.9.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.9.4.1" style="background-color:#D4E7CF;">79.90 / -</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.10.1">LEE</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.2">24.13 / 21.56</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.3">11.88 / 12.30</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.10.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.10.4.1" style="background-color:#D4E7CF;">73.69 / 80.00</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.11.1">LEE &amp; LCS</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.2">24.88 / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.11.2.1">21.91</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.3">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.11.3.1">15.20</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.11.3.2">15.63</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.11.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.11.4.1" style="background-color:#D4E7CF;">85.91<span class="ltx_text ltx_font_medium" id="S5.T7.1.1.11.4.1.1"> / </span>86.97</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T7.1.1.12.1">CTS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.12.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.12.2.1">24.21</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.12.2.2">21.72</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.12.3">12.77 / 10.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T7.1.1.12.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.12.4.1" style="background-color:#D4E7CF;">80.09 / 62.35</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T7.1.1.13.1">CTS &amp; LCS</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.13.2">24.19 / 21.52</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.13.3">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.3.1">15.13</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.3.2">15.43</span>
</td>
<td class="ltx_td ltx_align_center" id="S5.T7.1.1.13.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.13.4.1" style="background-color:#D4E7CF;">86.85<span class="ltx_text ltx_font_medium" id="S5.T7.1.1.13.4.1.1"> / </span>87.86</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T7.1.1.14.1">mBART &amp; FT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.14.2">29.03 / 29.53</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.14.3">3.47 / 4.94</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T7.1.1.14.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="S5.T7.1.1.14.4.1" style="background-color:#D4E7CF;">5.10 / 13.05</span></td>
</tr>
<tr class="ltx_tr" id="S5.T7.1.1.15">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T7.1.1.15.1">mBART &amp; LCS</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.1.15.2">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.15.2.1">30.84</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.15.2.2">31.33</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.1.15.3">
<span class="ltx_text ltx_font_bold" id="S5.T7.1.1.15.3.1">21.15</span> / <span class="ltx_text ltx_font_bold" id="S5.T7.1.1.15.3.2">21.24</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T7.1.1.15.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="S5.T7.1.1.15.4.1" style="background-color:#D4E7CF;">86.14<span class="ltx_text ltx_font_medium" id="S5.T7.1.1.15.4.1.1"> / </span>87.51</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>
Experiments about the application of LCS to other approaches.
â€˜â€ â€™ represents that the results are cited from the corresponding papers, and the rest models are reproduced by us.
"/" separates the scores of noise data and denoise data in OPUS-100.
</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Fine-Tune (FT)</span>.
We first train the model in the S-Enc-T-Dec strategy with 100K steps, and fine-tune the model in our strategy, with the same total training steps as other models.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Denosing Encoder (DN)</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>)</cite>.
DN introduces the denoising auto-encoder training objective to bridge the connection between zero-shot language pairs.
</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Disentangling Positional Information (DisPI)</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib15" title="">2021</a>)</cite>.
DisPI removes the residual connection of the encoder middle layer to yield the language-agnostic representation.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Contrastive Learning (CTS)</span> <cite class="ltx_cite ltx_citemacro_citep">(Pan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib24" title="">2021</a>)</cite>.
CTS introduces the contrastive learning training objective to close the representation gap of similar sentences.
We apply this objective to our shallow encoder layers since the function is similar to the first encoder stage of LCS.
</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">Language Embedding Embodiment (LEE)</span> <cite class="ltx_cite ltx_citemacro_citep">(Jin and Xiong, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>.
LEE adopts the target language embedding added to each state at the decoder side to indicate the desired target language without any LT strategies.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i6.p1">
<p class="ltx_p" id="S5.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i6.p1.1.1">mBART</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib17" title="">2020</a>)</cite>.
mBART is a widely-used pretrained multilingual sequence-to-sequence model.
We finetune mBART in vanilla S-Enc-T-Dec and LCS strategies, with the same training setting.
Training setting of mBART is a bit different from others, details can be referred in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1" title="Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Results</span>.
As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.T7" title="Table 7 â€£ 5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>, LCS is effective in enhancing the performance of existing approaches.
Specifically, with regard to zero-shot translation and language accuracy, most approaches achieve significant improvements with the application of LCS.
On the OPUS-100 dataset, all approaches yield significant improvements with the application of LCS, with 11.65, 10.91, 10.98, 3.32, and 2.36 average BLEU scores improvements on the noise data version, respectively.
Compared to the noise version of the OPUS-100 dataset, LCS also could yield similar improvements on the denoise version, even achieving the 15.90 BLEU score by the FT method and the 87.86% accuracy by the CTS&amp;LCS method.
Further, on the pretrained multilingual model, LCS exhibits advanced performance to enhance the performance of mBART on both supervised and zero-shot translation.
While in terms of supervised translation, the application of LCS generally maintains or slightly improves the performance of most approaches.
In conclusion, these results demonstrate that LCS is well-compatible with other approaches and can achieve further improvement when combined with them, without introducing extra parameters.
</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.F4.1.1" style="width:206.0pt;height:154pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-114.8pt,85.5pt) scale(0.472946922664891,0.472946922664891) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S5.F4.1.1.g1" src="x5.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Curves of the similarity of the language pairs along encoder layers in S-Enc-T-Dec on the zero-shot testset of noise and denoised OPUS-100.
</figcaption>
</figure>
<figure class="ltx_table" id="S5.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T8.1" style="width:206.0pt;height:86.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-47.4pt,19.9pt) scale(0.684670582241787,0.684670582241787) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T8.1.1">
<tr class="ltx_tr" id="S5.T8.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T8.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T8.1.1.1.1.1"><math alttext="\mathbf{k}" class="ltx_Math" display="inline" id="S5.T8.1.1.1.1.1.m1.1"><semantics id="S5.T8.1.1.1.1.1.m1.1a"><mi id="S5.T8.1.1.1.1.1.m1.1.1" xref="S5.T8.1.1.1.1.1.m1.1.1.cmml">ğ¤</mi><annotation-xml encoding="MathML-Content" id="S5.T8.1.1.1.1.1.m1.1b"><ci id="S5.T8.1.1.1.1.1.m1.1.1.cmml" xref="S5.T8.1.1.1.1.1.m1.1.1">ğ¤</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.1.1.1.1.1.m1.1c">\mathbf{k}</annotation><annotation encoding="application/x-llamapun" id="S5.T8.1.1.1.1.1.m1.1d">bold_k</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S5.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.2.1">Supervised</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.1.3.1">Zero-Shot</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.2.1.1">6-layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.2.2.1">12-layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.2.3"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.2.3.1">6-layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.2.4"><span class="ltx_text ltx_font_bold" id="S5.T8.1.1.2.4.1">12-layer</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.3.1">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.3.2">
<span class="ltx_text ltx_font_bold" id="S5.T8.1.1.3.2.1">24.80</span> / <span class="ltx_text ltx_font_bold" id="S5.T8.1.1.3.2.2">22.09</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.3.3">
<span class="ltx_text ltx_font_bold" id="S5.T8.1.1.3.3.1">26.62</span> / <span class="ltx_text ltx_font_bold" id="S5.T8.1.1.3.3.2">23.40</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T8.1.1.3.4">15.22 / <span class="ltx_text ltx_font_bold" id="S5.T8.1.1.3.4.1">15.58</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.1.3.5">16.86 / <span class="ltx_text ltx_font_bold" id="S5.T8.1.1.3.5.1">17.39</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.4.1">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.4.2">24.64 / 21.76</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.4.3">26.60 / 22.85</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.4.4">
<span class="ltx_text ltx_font_bold" id="S5.T8.1.1.4.4.1">15.32</span> / 15.51</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.1.4.5">
<span class="ltx_text ltx_font_bold" id="S5.T8.1.1.4.5.1">17.01</span> / 16.86</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.5.1">4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.5.2">24.13 / 21.22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.5.3">â€„â€„2.62 / 9.03</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.5.4">15.06 / 15.06</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.1.5.5">â€„â€„2.33 / 4.29</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.6.1">5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.6.2">23.20 / 20.81</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.6.3">23.75 / 19.93</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T8.1.1.6.4">14.03 / 14.75</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.1.6.5">15.40 / 14.62</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T8.1.1.7.1">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T8.1.1.7.2">23.62 / 20.99</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T8.1.1.7.3">22.70 / 20.15</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T8.1.1.7.4">12.79 / 14.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T8.1.1.7.5">14.45 / 14.66</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>
BLEU scores of LCS with different <math alttext="k" class="ltx_Math" display="inline" id="S5.T8.5.m1.1"><semantics id="S5.T8.5.m1.1b"><mi id="S5.T8.5.m1.1.1" xref="S5.T8.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T8.5.m1.1c"><ci id="S5.T8.5.m1.1.1.cmml" xref="S5.T8.5.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.5.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.T8.5.m1.1e">italic_k</annotation></semantics></math> on the noise and denoise OPUS-100.
<math alttext="k" class="ltx_Math" display="inline" id="S5.T8.6.m2.1"><semantics id="S5.T8.6.m2.1b"><mi id="S5.T8.6.m2.1.1" xref="S5.T8.6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T8.6.m2.1c"><ci id="S5.T8.6.m2.1.1.cmml" xref="S5.T8.6.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.6.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.T8.6.m2.1e">italic_k</annotation></semantics></math> denotes the hyperparameter <math alttext="k" class="ltx_Math" display="inline" id="S5.T8.7.m3.1"><semantics id="S5.T8.7.m3.1b"><mi id="S5.T8.7.m3.1.1" xref="S5.T8.7.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.T8.7.m3.1c"><ci id="S5.T8.7.m3.1.1.cmml" xref="S5.T8.7.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T8.7.m3.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.T8.7.m3.1e">italic_k</annotation></semantics></math> and 6/12-layer denotes the encoderâ€™s depth.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Selection of Hyperparameter <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.1.m1.1"><semantics id="S5.SS2.1.m1.1b"><mi id="S5.SS2.1.m1.1.1" xref="S5.SS2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.1.m1.1c"><ci id="S5.SS2.1.m1.1.1.cmml" xref="S5.SS2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.1.m1.1e">italic_k</annotation></semantics></math>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.3">The selection of hyperparameter <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math> is an important factor of LCS.
Indeed, the selection of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mi id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><ci id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">italic_k</annotation></semantics></math> mainly relies on the variation of language similarity among encoder layers, which is described in SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.SS4" title="3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
We expand the variation into the deeper encoder with 12 layers, and display the variation in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.F4" title="Figure 4 â€£ 5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>.
We list the performance of different values of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">italic_k</annotation></semantics></math> in the 6-layer and 12-layer encoder in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.T8" title="Table 8 â€£ 5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.5">As the results show, the optimal selection of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math> is around the inflection point of the variation of language similarity.
We could observe from Fig.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.F4" title="Figure 4 â€£ 5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a> and Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.T8" title="Table 8 â€£ 5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">8</span></a>, when the selected value <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">italic_k</annotation></semantics></math> is around the inflection point, LCS could yield better translation quality.
For most settings of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">italic_k</annotation></semantics></math>, the performance of LCS on zero-shot translation is much better than other methods (as shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.T5" title="Table 5 â€£ 3.5 Language Converter Strategy â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> and Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.T7" title="Table 7 â€£ 5.1 Application to Stronger Approaches â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>).
We further probe the selection of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">italic_k</annotation></semantics></math> in other deeper encoders (24-layer and 48-layer), and conclude that the better range selection of <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mi id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><ci id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">italic_k</annotation></semantics></math> is the nearby integers of 15% of the encoder depth.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Effect of LCS on deeper encoders</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conduct experiments to compare the effectiveness of LCS with the T-Enc and S-Enc-T-Dec, where we set <math alttext="k" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> to 2 for 6- and 12-layer encoders, 5 for the 24-layer encoder, and 6 for the 48-layer encoder<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>In spired by <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib30" title="">2022a</a>)</cite>, we train 24- and 48-layer encoder with the assistance of DeepNorm.</span></span></span>.
As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S5.F5" title="Figure 5 â€£ 5.3 Effect of LCS on deeper encoders â€£ 5 Analysis â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, although T-Enc and S-Enc-T-Dec could yield better translation on supervised translation with deeper encoders, both cannot perform much better than the 6-layer encoder on zero-shot translation.
Compared to them, with deeper encoders, LCS exhibits an upward trend on zero-shot translation, where the 48-layer encoder improves around 2.50 BLEU score compared to the 6-layer encoder.
Besides, in deep encoders, LCS maintains a similar supervised performance with the S-Enc-T-Dec strategy.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.F5.3" style="width:211.9pt;height:352.6pt;vertical-align:-352.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.5pt,0.0pt) scale(0.485,0.485) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.1.1">
<div class="ltx_block ltx_minipage ltx_align_top" id="S5.F5.1.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S5.F5.1.1.1.g1" src="x6.png" width="830"/>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F5.2.2">
<div class="ltx_block ltx_minipage ltx_align_top" id="S5.F5.2.2.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="S5.F5.2.2.1.g1" src="x7.png" width="830"/>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Performance of deeper encoder on the supervised and zero-shot testset of the noise OPUS-100.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we have identified that the <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">off-target</span> issue is sensitive to the placement of LT and provide the analysis for the details of the issue with the prevalent LT strategies.
We reveal that placing LT on the encoder side provides a more stable and sufficient language indication than the decoder side, and introducing the target information into the top layers of the encoder will mitigate the confusion between the source and the target language.
Based on our findings, we propose LCS to mitigate the <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">off-target</span> issue, and further improve the performance of zero-shot translation.
Extensive experiments and analysis suggest that LCS boosts the performance of zero-shot translation and significantly mitigates the <span class="ltx_text ltx_font_italic" id="S6.p1.1.3">off-target</span> issue without introducing extra parameters. Besides, LCS could perform well in the noised data set.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Compared to the boosted performance on zero-shot translation, LCS yields limited improvements on the supervised translation, while it is designed to enhance the language indication for zero-shot translation.
We intend to explore ways to improve the performance of zero-shot translation further and achieve greater improvement in supervised translation.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">The research work described in this paper has been supported by the National Nature Science Foundation of China (No. 61976016, 62376019, 61976015), and the authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bjerva etÂ al. (2019)</span>
<span class="ltx_bibblock">
Johannes Bjerva, Robert Ã–stling, MariaÂ Han Veiga, JÃ¶rg Tiedemann, and
Isabelle Augenstein. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/coli_a_00351" title="">What do language
representations really represent?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Computational Linguistics</em>, 45(2):381â€“389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Liang Chen, Shuming Ma, Dongdong Zhang, Furu Wei, and Baobao Chang. 2023.

</span>
<span class="ltx_bibblock">On the off-target problem of zero-shot multilingual neural machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2305.10930</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dabre etÂ al. (2020)</span>
<span class="ltx_bibblock">
Raj Dabre, Chenhui Chu, and Anoop Kunchukuttan. 2020.

</span>
<span class="ltx_bibblock">A survey of multilingual neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ACM Computing Surveys (CSUR)</em>, 53(5):1â€“38.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong etÂ al. (2015)</span>
<span class="ltx_bibblock">
Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/v1/P15-1166" title="">Multi-task learning for
multiple language translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1723â€“1732,
Beijing, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ElNokrashy etÂ al. (2022)</span>
<span class="ltx_bibblock">
Muhammad ElNokrashy, Amr Hendy, Mohamed Maher, Mohamed Afify, and HanyÂ Hassan
Awadalla. 2022.

</span>
<span class="ltx_bibblock">Language tokens: A frustratingly simple approach improves zero-shot
performance of multilingual translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2208.05852</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
Chaudhary, etÂ al. 2021.

</span>
<span class="ltx_bibblock">Beyond english-centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">J. Mach. Learn. Res.</em>, 22(107):1â€“48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Firat etÂ al. (2016a)</span>
<span class="ltx_bibblock">
Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N16-1101" title="">Multi-way, multilingual
neural machine translation with a shared attention mechanism</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 866â€“875, San Diego, California. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Firat etÂ al. (2016b)</span>
<span class="ltx_bibblock">
Orhan Firat, Baskaran Sankaran, Yaser Al-onaizan, FatosÂ T. YarmanÂ Vural, and
Kyunghyun Cho. 2016b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D16-1026" title="">Zero-resource
translation with multi-lingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing</em>, pages 268â€“277, Austin, Texas. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Pengzhi Gao, Liwen Zhang, Zhongjun He, Hua Wu, and Haifeng Wang. 2023.

</span>
<span class="ltx_bibblock">Improving zero-shot multilingual neural machine translation by
leveraging cross-lingual consistency regularization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2305.07310</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jiatao Gu, Yong Wang, Kyunghyun Cho, and VictorÂ O.K. Li. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1121" title="">Improved zero-shot
neural machine translation via ignoring spurious correlations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1258â€“1268, Florence, Italy.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin and Xiong (2022)</span>
<span class="ltx_bibblock">
Renren Jin and Deyi Xiong. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2209.01530" title="">Informative language
representation learning for massively multilingual neural machine
translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2209.01530</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2017)</span>
<span class="ltx_bibblock">
Melvin Johnson, Mike Schuster, QuocÂ V. Le, Maxim Krikun, Yonghui Wu, Zhifeng
Chen, Nikhil Thorat, Fernanda ViÃ©gas, Martin Wattenberg, Greg Corrado,
Macduff Hughes, and Jeffrey Dean. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00065" title="">Googleâ€™s
multilingual neural machine translation system: Enabling zero-shot
translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Transactions of the Association for Computational Linguistics</em>,
5:339â€“351.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A
simple and language independent subword tokenizer and detokenizer for neural
text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pages 66â€“71, Brussels,
Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunlong Liang, Fandong Meng, Jinan Xu, Jiaan Wang, Yufeng Chen, and Jie Zhou.
2023.

</span>
<span class="ltx_bibblock">Unified model learning for various neural machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2305.02777</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Danni Liu, Jan Niehues, James Cross, Francisco GuzmÃ¡n, and Xian Li. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.101" title="">Improving
zero-shot translation by disentangling positional information</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1259â€“1273,
Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock">Branchnorm: Robustly scaling extremely deep transformers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2305.02790</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00343" title="">Multilingual denoising
pre-training for neural machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Transactions of the Association for Computational Linguistics</em>,
8:726â€“742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2018)</span>
<span class="ltx_bibblock">
Yichao Lu, Phillip Keung, Faisal Ladhak, Vikas Bhardwaj, Shaonan Zhang, and
Jason Sun. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6309" title="">A neural interlingua
for multilingual machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 84â€“92, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luong etÂ al. (2015)</span>
<span class="ltx_bibblock">
Thang Luong, Hieu Pham, and ChristopherÂ D. Manning. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D15-1166" title="">Effective approaches to
attention-based neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing</em>, pages 1412â€“1421, Lisbon, Portugal. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhuoyuan Mao, Raj Dabre, Qianying Liu, Haiyue Song, Chenhui Chu, and Sadao
Kurohashi. 2023.

</span>
<span class="ltx_bibblock">Exploring the impact of layer normalization for zero-shot neural
machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2305.09312</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oncevay etÂ al. (2020)</span>
<span class="ltx_bibblock">
Arturo Oncevay, Barry Haddow, and Alexandra Birch. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.187" title="">Bridging
linguistic typology and multilingual machine translation with multi-view
language representations</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 2391â€“2406, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott etÂ al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics (Demonstrations)</em>,
pages 48â€“53.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott etÂ al. (2018)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6301" title="">Scaling neural machine
translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the Third Conference on Machine Translation:
Research Papers</em>, pages 1â€“9, Brussels, Belgium. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.21" title="">Contrastive
learning for many-to-many multilingual neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 244â€“258, Online.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qi etÂ al. (2018)</span>
<span class="ltx_bibblock">
YeÂ Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig.
2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N18-2084" title="">When and why are
pre-trained word embeddings useful for neural machine translation?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers)</em>, pages 529â€“535, New Orleans,
Louisiana. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu and Watanabe (2022)</span>
<span class="ltx_bibblock">
Zhi Qu and Taro Watanabe. 2022.

</span>
<span class="ltx_bibblock">Adapting to non-centered languages for zero-shot multilingual
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2209.04138</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich etÂ al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1162" title="">Neural machine
translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715â€“1725,
Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ der Maaten and Hinton (2008)</span>
<span class="ltx_bibblock">
Laurens VanÂ der Maaten and Geoffrey Hinton. 2008.

</span>
<span class="ltx_bibblock">Visualizing data using t-sne.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Journal of machine learning research</em>, 9(11).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Hongyu Wang, Shuming Ma, LiÂ Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
2022a.

</span>
<span class="ltx_bibblock">Deepnet: Scaling transformers to 1,000 layers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2203.00555</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Jiaan Wang, Fandong Meng, Ziyao Lu, Duo Zheng, Zhixu Li, Jianfeng Qu, and Jie
Zhou. 2022b.

</span>
<span class="ltx_bibblock">Clidsum: A benchmark dataset for cross-lingual dialogue
summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2202.05599</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Weizhi Wang, Zhirui Zhang, Yichao Du, Boxing Chen, Jun Xie, and Weihua Luo.
2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-emnlp.366" title="">Rethinking zero-shot neural machine translation: From a perspective of
latent variables</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 4321â€“4327, Punta Cana, Dominican Republic. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022c)</span>
<span class="ltx_bibblock">
Wenxuan Wang, Wenxiang Jiao, Shuo Wang, Zhaopeng Tu, and MichaelÂ R Lyu.
2022c.

</span>
<span class="ltx_bibblock">Understanding and mitigating the uncertainty in zero-shot
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:2205.10068</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wicks and Duh (2022)</span>
<span class="ltx_bibblock">
Rachel Wicks and Kevin Duh. 2022.

</span>
<span class="ltx_bibblock">The effects of language token prefixing for multilingual machine
translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2nd Conference of the Asia-Pacific
Chapter of the Association for Computational Linguistics and the 12th
International Joint Conference on Natural Language Processing</em>, pages
148â€“153.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Liwei Wu, Shanbo Cheng, Mingxuan Wang, and Lei Li. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.findings-acl.264" title="">Language
tags matter for zero-shot neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Findings of the Association for Computational Linguistics:
ACL-IJCNLP 2021</em>, pages 3001â€“3007, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.41" title="">mT5: A
massively multilingual pre-trained text-to-text transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 483â€“498, Online. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yilin Yang, Akiko Eriguchi, Alexandre Muzio, Prasad Tadepalli, Stefan Lee, and
Hany Hassan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.578" title="">Improving
multilingual translation by representation and gradient regularization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7266â€“7279, Online and Punta Cana,
Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Changtong Zan, Liang Ding, LiÂ Shen, Yibin Lei, Yibing Zhan, Weifeng Liu, and
Dacheng Tao. 2023.

</span>
<span class="ltx_bibblock">Unlikelihood tuning on negative samples amazingly improves zero-shot
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2309.16599</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.148" title="">Improving
massively multilingual neural machine translation and zero-shot translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 1628â€“1639, Online. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mingliang Zhang, Fandong Meng, Yunhai Tong, and Jie Zhou. 2021.

</span>
<span class="ltx_bibblock">Competence-based curriculum learning for multilingual machine
translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2109.04002</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xiang Zhang, Ning Shi, Bradley Hauer, and Grzegorz Kondrak. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.eacl-main.205" title="">Bridging the
gap between BabelNet and HowNet: Unsupervised sense alignment and
sememe prediction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 17th Conference of the European Chapter
of the Association for Computational Linguistics</em>, pages 2789â€“2798,
Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset &amp; Training details</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We show the detail of the MultiUN<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>https://conferences.unite.un.org/UNCORPUS</span></span></span>, TED<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://github.com/neulab/word-embeddings-for-nmt</span></span></span>, and OPUS-100<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>https://opus.nlpl.eu/opus-100.php</span></span></span> as the following sections.
We average the last five checkpoints to form the final tested checkpoint for all models in our experiments.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>MultiUN</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We select four languages distributed in various language families, Arabic (Ar), English (En), Russian (Ru), and Chinese(Zh), following <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>)</cite>.
In final, the training set consists of 6M sentence pairs.
We select Transformer-base architecture based on post-norm to conduct experiments, and set 6 encoder/decoder layers with 8 attention heads, embedding size of 512, inner size of 2048, the dropout rate of 0.1, the maximum learning rate of 0.0007 and label smoothing rate of 0.1.
We share the vocabulary for all languages and segment words into subwords using byte pair encoding (BPE) <cite class="ltx_cite ltx_citemacro_citep">(Sennrich etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib27" title="">2016</a>)</cite> with 40k merge operations, following <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>)</cite>.
In training, we set the maximum batch size per GPU to 4096 tokens and trained on 8 GPUs with 300K steps.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>TED</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">It includes 60 languages in total <cite class="ltx_cite ltx_citemacro_citep">(Qi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib25" title="">2018</a>)</cite> and we choose the top 20 languages following <cite class="ltx_cite ltx_citemacro_citet">Qu and Watanabe (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib26" title="">2022</a>)</cite>.
In final, the training set consists of 3.5M sentence pairs.
We choose Transformer-base architecture based on post-norm to conduct experiments, and set 6 encoder/decoder layers with 8 attention heads, embedding size of 512, inner size of 2048, dropout rate of 0.1, maximum learning rate of 0.0005 and label smoothing rate of 0.1.
In this dataset, we use SentencePiece to segment words into subwords with 64k merge operations, following <cite class="ltx_cite ltx_citemacro_citet">Qu and Watanabe (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib26" title="">2022</a>)</cite>.
In training, we set the maximum batch size per GPU to 6400 tokens and trained on 8 GPUs with 100K steps.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>OPUS-100</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">It includes 100 languages in total and consists of 55M training sentence pairs with up to 1M samples per language pair.
We choose Transformer-base architecture based on post-norm to conduct experiments, and set 6 encoder/decoder layers with 8 attention heads, embedding size of 512, inner size of 2048, dropout rate of 0.1, maximum learning rate of 0.0005 and label smoothing rate of 0.1.
In this dataset, we use SentencePiece <cite class="ltx_cite ltx_citemacro_citep">(Kudo and Richardson, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib13" title="">2018</a>)</cite> to segment words into subwords with 64k merge operations, following <cite class="ltx_cite ltx_citemacro_citet">Jin and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>.
In training, we set the maximum batch size per GPU to 6400 and trained on 8 GPUs with 400K steps.
We train models in both noise and denoise data versions with the same parameter settings and similar epochs.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>mBART</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">Since the number of languages in mBART is less than OPUS-100, we mainly select the six languages in the zero-shot test set of OPUS-100 (<em class="ltx_emph ltx_font_italic" id="A1.SS4.p1.1.1">i.e.</em>, Arabic, German, French, Russian, and Chinese), and add English to conduct experiments.
The training set is still English-centric.
We select the mBART-Large<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz" title="">https://dl.fbaipublicfiles.com/fairseq/models/mbart/mbart.cc25.v2.tar.gz</a></span></span></span> with 25 languages.
During training, we set dropout to 0.3, and learning rate to 0.00003 with 2500 steps to warmup.
We set the maximum batch size per GPU to 1024 tokens and trained on 8 GPUs with 100K steps.
For fine-tuning mBART in LCS strategies, we set <math alttext="k" class="ltx_Math" display="inline" id="A1.SS4.p1.1.m1.1"><semantics id="A1.SS4.p1.1.m1.1a"><mi id="A1.SS4.p1.1.m1.1.1" xref="A1.SS4.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS4.p1.1.m1.1b"><ci id="A1.SS4.p1.1.m1.1.1.cmml" xref="A1.SS4.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS4.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS4.p1.1.m1.1d">italic_k</annotation></semantics></math> to 2.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Deep Encoder Training</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.2">Inspired by the successful application of DeepNorm <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib30" title="">2022a</a>)</cite> and BranchNorm <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib16" title="">2023</a>)</cite>, we utilize DeepNorm to stabilize the training of deep models (24-layer and 48-layer), which applies such a constraint to the early stage of model training.
And the <math alttext="\alpha" class="ltx_Math" display="inline" id="A1.SS5.p1.1.m1.1"><semantics id="A1.SS5.p1.1.m1.1a"><mi id="A1.SS5.p1.1.m1.1.1" xref="A1.SS5.p1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.1.m1.1b"><ci id="A1.SS5.p1.1.m1.1.1.cmml" xref="A1.SS5.p1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.1.m1.1d">italic_Î±</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="A1.SS5.p1.2.m2.1"><semantics id="A1.SS5.p1.2.m2.1a"><mi id="A1.SS5.p1.2.m2.1.1" xref="A1.SS5.p1.2.m2.1.1.cmml">Î²</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.2.m2.1b"><ci id="A1.SS5.p1.2.m2.1.1.cmml" xref="A1.SS5.p1.2.m2.1.1">ğ›½</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.2.m2.1d">italic_Î²</annotation></semantics></math> are following DeepNorm, as the following formulas:</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}\begin{aligned} \alpha_{encoder}&amp;=0.81(N^{4}M)^{\frac{1}{16}},\\
\beta_{encoder}&amp;=0.87(N^{4}M)^{-\frac{1}{16}},\\
\alpha_{encoder}&amp;=(3M)^{\frac{1}{4}},\\
\beta_{decoder}&amp;=(12M)^{-\frac{1}{4}},\end{aligned}\end{split}" class="ltx_Math" display="block" id="A1.E6.m1.1"><semantics id="A1.E6.m1.1a"><mtable displaystyle="true" id="A1.E6.m1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtr id="A1.E6.m1.1.1a" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="A1.E6.m1.1.1b" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtable columnspacing="0pt" displaystyle="true" id="A1.E6.m1.1.1.1.1.1.1" rowspacing="0pt" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtr id="A1.E6.m1.1.1.1.1.1.1a" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="A1.E6.m1.1.1.1.1.1.1b" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><msub id="A1.E6.m1.1.1.1.1.1.1.1.2.1" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.2" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.2.cmml">Î±</mi><mrow id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.2.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.3" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.3.cmml">n</mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1a" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.4" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.4.cmml">c</mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1b" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.5" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.5.cmml">o</mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1c" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.6" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.6.cmml">d</mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1d" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.7" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.7.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1e" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.8" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.8.cmml">r</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="A1.E6.m1.1.1.1.1.1.1c" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"></mi><mo id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">=</mo><mrow id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">0.81</mn><mo id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">â¢</mo><msup id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msup id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">N</mi><mn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">4</mn></msup><mo id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">M</mi></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mfrac id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">16</mn></mfrac></msup></mrow></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="A1.E6.m1.1.1.1.1.1.1d" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="A1.E6.m1.1.1.1.1.1.1e" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><msub id="A1.E6.m1.1.1.1.1.1.1.2.2.1" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.2" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.2.cmml">Î²</mi><mrow id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.2.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.3" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.3.cmml">n</mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1a" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.4" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.4.cmml">c</mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1b" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.5" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.5.cmml">o</mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1c" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.6" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.6.cmml">d</mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1d" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.7" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.7.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1e" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.8" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.8.cmml">r</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="A1.E6.m1.1.1.1.1.1.1f" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.3.cmml"></mi><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.2.cmml">=</mo><mrow id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml">0.87</mn><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml">â¢</mo><msup id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml"><msup id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.2.cmml">N</mi><mn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.3" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.3.cmml">4</mn></msup><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml">M</mi></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml"><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3a" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml">âˆ’</mo><mfrac id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.2.cmml">1</mn><mn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.3" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.3.cmml">16</mn></mfrac></mrow></msup></mrow></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="A1.E6.m1.1.1.1.1.1.1g" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="A1.E6.m1.1.1.1.1.1.1h" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><msub id="A1.E6.m1.1.1.1.1.1.1.3.2.1" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.2" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.2.cmml">Î±</mi><mrow id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.2.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.3" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.3.cmml">n</mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1a" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.4" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.4.cmml">c</mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1b" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.5" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.5.cmml">o</mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1c" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.6" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.6.cmml">d</mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1d" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.7" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.7.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1e" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.8" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.8.cmml">r</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="A1.E6.m1.1.1.1.1.1.1i" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.3.cmml"></mi><mo id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.2.cmml">=</mo><msup id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.cmml"><mo id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.2.cmml">3</mn><mo id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.3.cmml">M</mi></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mfrac id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.2.cmml">1</mn><mn id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.3" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.3.cmml">4</mn></mfrac></msup></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="A1.E6.m1.1.1.1.1.1.1j" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="A1.E6.m1.1.1.1.1.1.1k" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><msub id="A1.E6.m1.1.1.1.1.1.1.4.2.1" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.2" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.2.cmml">Î²</mi><mrow id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.2.cmml">d</mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.3" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.3.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1a" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.4" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.4.cmml">c</mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1b" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.5" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.5.cmml">o</mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1c" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.6" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.6.cmml">d</mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1d" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.7" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.7.cmml">e</mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1e" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.8" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.8.cmml">r</mi></mrow></msub></mtd><mtd class="ltx_align_left" columnalign="left" id="A1.E6.m1.1.1.1.1.1.1l" xref="A1.E6.m1.1.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.cmml"><mi id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.3.cmml"></mi><mo id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.2.cmml">=</mo><msup id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.cmml"><mrow id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.cmml"><mo id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.2" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.2.cmml">12</mn><mo id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.1" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.1.cmml">â¢</mo><mi id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.3.cmml">M</mi></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.3" stretchy="false" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.cmml"><mo id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3a" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.cmml">âˆ’</mo><mfrac id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.cmml"><mn id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.2" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.2.cmml">1</mn><mn id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.3" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.3.cmml">4</mn></mfrac></mrow></msup></mrow><mo id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.2" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="A1.E6.m1.1b"><matrix id="A1.E6.m1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1"><matrixrow id="A1.E6.m1.1.1.1.1.1.1a.cmml" xref="A1.E6.m1.1.1"><apply id="A1.E6.m1.1.1.1.1.1.1.1.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.1.2.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1">subscript</csymbol><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.2">ğ›¼</ci><apply id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3"><times id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.1"></times><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.2">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.3">ğ‘›</ci><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.4.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.4">ğ‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.5.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.5">ğ‘œ</ci><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.6.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.6">ğ‘‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.7.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.7">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.8.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.2.1.3.8">ğ‘Ÿ</ci></apply></apply><apply id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1"><eq id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.2"></eq><csymbol cd="latexml" id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.3">absent</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><cn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" type="float" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">0.81</cn><apply id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘</ci><cn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.3">4</cn></apply><ci id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘€</ci></apply><apply id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><divide id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"></divide><cn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">1</cn><cn id="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">16</cn></apply></apply></apply></apply></matrixrow><matrixrow id="A1.E6.m1.1.1.1.1.1.1b.cmml" xref="A1.E6.m1.1.1"><apply id="A1.E6.m1.1.1.1.1.1.1.2.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.2.2.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1">subscript</csymbol><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.2">ğ›½</ci><apply id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3"><times id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.1"></times><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.2">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.3">ğ‘›</ci><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.4.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.4">ğ‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.5.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.5">ğ‘œ</ci><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.6.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.6">ğ‘‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.7.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.7">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.8.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.2.1.3.8">ğ‘Ÿ</ci></apply></apply><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1"><eq id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.2"></eq><csymbol cd="latexml" id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.3">absent</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1"><times id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.2"></times><cn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.3.cmml" type="float" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.3">0.87</cn><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1">superscript</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1"><times id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.1"></times><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><ci id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.2">ğ‘</ci><cn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.2.3">4</cn></apply><ci id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.1.1.1.3">ğ‘€</ci></apply><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3"><minus id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3"></minus><apply id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2"><divide id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2"></divide><cn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.2.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.2">1</cn><cn id="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.3.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.2.1.1.1.1.1.1.3.2.3">16</cn></apply></apply></apply></apply></apply></matrixrow><matrixrow id="A1.E6.m1.1.1.1.1.1.1c.cmml" xref="A1.E6.m1.1.1"><apply id="A1.E6.m1.1.1.1.1.1.1.3.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.3.2.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1">subscript</csymbol><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.2">ğ›¼</ci><apply id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3"><times id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.1"></times><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.2">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.3">ğ‘›</ci><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.4.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.4">ğ‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.5.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.5">ğ‘œ</ci><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.6.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.6">ğ‘‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.7.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.7">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.8.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.2.1.3.8">ğ‘Ÿ</ci></apply></apply><apply id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1"><eq id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.2"></eq><csymbol cd="latexml" id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.3">absent</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1">superscript</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1"><times id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.1"></times><cn id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.2">3</cn><ci id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.1.1.1.3">ğ‘€</ci></apply><apply id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3"><divide id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3"></divide><cn id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.2.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.2">1</cn><cn id="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.3.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.3.1.1.1.1.1.3.3">4</cn></apply></apply></apply></matrixrow><matrixrow id="A1.E6.m1.1.1.1.1.1.1d.cmml" xref="A1.E6.m1.1.1"><apply id="A1.E6.m1.1.1.1.1.1.1.4.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.4.2.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1">subscript</csymbol><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.2">ğ›½</ci><apply id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3"><times id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.1"></times><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.2">ğ‘‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.3">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.4.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.4">ğ‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.5.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.5">ğ‘œ</ci><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.6.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.6">ğ‘‘</ci><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.7.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.7">ğ‘’</ci><ci id="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.8.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.2.1.3.8">ğ‘Ÿ</ci></apply></apply><apply id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1"><eq id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.2"></eq><csymbol cd="latexml" id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.3">absent</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1">superscript</csymbol><apply id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1"><times id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.1"></times><cn id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.2">12</cn><ci id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.1.1.1.3">ğ‘€</ci></apply><apply id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3"><minus id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3"></minus><apply id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2"><divide id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.1.cmml" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2"></divide><cn id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.2.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.2">1</cn><cn id="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.3.cmml" type="integer" xref="A1.E6.m1.1.1.1.1.1.1.4.1.1.1.1.1.3.2.3">4</cn></apply></apply></apply></apply></matrixrow></matrix></annotation-xml><annotation encoding="application/x-tex" id="A1.E6.m1.1c">\begin{split}\begin{aligned} \alpha_{encoder}&amp;=0.81(N^{4}M)^{\frac{1}{16}},\\
\beta_{encoder}&amp;=0.87(N^{4}M)^{-\frac{1}{16}},\\
\alpha_{encoder}&amp;=(3M)^{\frac{1}{4}},\\
\beta_{decoder}&amp;=(12M)^{-\frac{1}{4}},\end{aligned}\end{split}</annotation><annotation encoding="application/x-llamapun" id="A1.E6.m1.1d">start_ROW start_CELL start_ROW start_CELL italic_Î± start_POSTSUBSCRIPT italic_e italic_n italic_c italic_o italic_d italic_e italic_r end_POSTSUBSCRIPT end_CELL start_CELL = 0.81 ( italic_N start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_M ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 16 end_ARG end_POSTSUPERSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_Î² start_POSTSUBSCRIPT italic_e italic_n italic_c italic_o italic_d italic_e italic_r end_POSTSUBSCRIPT end_CELL start_CELL = 0.87 ( italic_N start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT italic_M ) start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 16 end_ARG end_POSTSUPERSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_Î± start_POSTSUBSCRIPT italic_e italic_n italic_c italic_o italic_d italic_e italic_r end_POSTSUBSCRIPT end_CELL start_CELL = ( 3 italic_M ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 4 end_ARG end_POSTSUPERSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_Î² start_POSTSUBSCRIPT italic_d italic_e italic_c italic_o italic_d italic_e italic_r end_POSTSUBSCRIPT end_CELL start_CELL = ( 12 italic_M ) start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 4 end_ARG end_POSTSUPERSCRIPT , end_CELL end_ROW end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A1.SS5.p1.4">where <math alttext="N" class="ltx_Math" display="inline" id="A1.SS5.p1.3.m1.1"><semantics id="A1.SS5.p1.3.m1.1a"><mi id="A1.SS5.p1.3.m1.1.1" xref="A1.SS5.p1.3.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.3.m1.1b"><ci id="A1.SS5.p1.3.m1.1.1.cmml" xref="A1.SS5.p1.3.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.3.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.3.m1.1d">italic_N</annotation></semantics></math> and <math alttext="M" class="ltx_Math" display="inline" id="A1.SS5.p1.4.m2.1"><semantics id="A1.SS5.p1.4.m2.1a"><mi id="A1.SS5.p1.4.m2.1.1" xref="A1.SS5.p1.4.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.4.m2.1b"><ci id="A1.SS5.p1.4.m2.1.1.cmml" xref="A1.SS5.p1.4.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.4.m2.1c">M</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.4.m2.1d">italic_M</annotation></semantics></math> denote the depth of the encoder and decoder for a standard Transformer.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Evaluation</h3>
<div class="ltx_para" id="A1.SS6.p1">
<p class="ltx_p" id="A1.SS6.p1.1">In MultiUN, we calculate the case-insensitive sacreBLEU scores, following <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib32" title="">2021</a>)</cite>, and calculate case-sensitive sacreBLEU scores for TED and OPUS-100 datasets, following <cite class="ltx_cite ltx_citemacro_citet">Jin and Xiong (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib11" title="">2022</a>)</cite>.
We respectively average the scores of overall supervised or zero-shot translation directions to report in tables.
Specifically, we apply different tokenizer for all Chinese testset<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>SacreBLEU signatures: BLEU+case.mixed+numrefs.1
<br class="ltx_break"/>+smooth.exp+tok.zh+version.2.0.0.</span></span></span>, compared to the rest languages<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>SacreBLEU signatures: BLEU+case.mixed+numrefs.1
<br class="ltx_break"/>+smooth.exp+tok.13a+version.2.0.0.</span></span></span>.</p>
</div>
<figure class="ltx_table" id="A1.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T9.1" style="width:188.6pt;height:130.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-79.3pt,54.7pt) scale(0.543322252229425,0.543322252229425) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T9.1.1">
<tr class="ltx_tr" id="A1.T9.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.2">
<span class="ltx_text" id="A1.T9.1.1.1.2.1"></span> <span class="ltx_text" id="A1.T9.1.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T9.1.1.1.2.2.1">
<span class="ltx_tr" id="A1.T9.1.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T9.1.1.1.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.2.2.1.1.1.1">Strategy</span></span></span>
</span></span><span class="ltx_text" id="A1.T9.1.1.1.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.3.1">Supervised</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.4.1">Zero-Shot</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.1.1.1.5" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.5.1" style="background-color:#D4E7CF;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.1.2.1" rowspan="6"><span class="ltx_text" id="A1.T9.1.1.2.1.1"><span class="ltx_text" id="A1.T9.1.1.2.1.1.1"></span> <span class="ltx_text" id="A1.T9.1.1.2.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T9.1.1.2.1.1.2.1">
<span class="ltx_tr" id="A1.T9.1.1.2.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T9.1.1.2.1.1.2.1.1.1">MultiUN</span></span>
<span class="ltx_tr" id="A1.T9.1.1.2.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T9.1.1.2.1.1.2.1.2.1">(Small)</span></span>
</span></span> <span class="ltx_text" id="A1.T9.1.1.2.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.1.2.2">sS-cS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.2.3"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.2.3.1">50.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.2.4"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.2.4.1">36.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.2.5" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.2.5.1" style="background-color:#D4E7CF;">95.28</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.3.1">sS-cT</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.3.2"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.3.2.1">50.75</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.3.3">35.51</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.3.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.3.4.1" style="background-color:#D4E7CF;">94.69</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.4.1">sS-c_</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.2">50.73</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.3">35.93</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.4.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.4.4.1" style="background-color:#D4E7CF;">95.12</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.5.1">s_-c_</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.2">50.63</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.3">35.76</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.5.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.5.4.1" style="background-color:#D4E7CF;">95.04</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.6.1">sT-cT</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.2">34.68</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.3">11.88</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.6.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.6.4.1" style="background-color:#D4E7CF;">18.53</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.1.7.1">Remove</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.7.2">50.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.7.3">35.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.7.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.7.4.1" style="background-color:#D4E7CF;">95.17</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T9.1.1.8.1" rowspan="6"><span class="ltx_text" id="A1.T9.1.1.8.1.1"><span class="ltx_text" id="A1.T9.1.1.8.1.1.1"></span> <span class="ltx_text" id="A1.T9.1.1.8.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T9.1.1.8.1.1.2.1">
<span class="ltx_tr" id="A1.T9.1.1.8.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T9.1.1.8.1.1.2.1.1.1">OPUS-100</span></span>
<span class="ltx_tr" id="A1.T9.1.1.8.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T9.1.1.8.1.1.2.1.2.1">(Large)</span></span>
</span></span> <span class="ltx_text" id="A1.T9.1.1.8.1.1.3"></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.1.1.8.2">sS-cS</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.8.3">24.80 / 22.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.8.4">
<span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.4.1">15.22</span> / <span class="ltx_text ltx_font_bold" id="A1.T9.1.1.8.4.2">15.58</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.1.1.8.5" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.8.5.1" style="background-color:#D4E7CF;">85.35 / 86.67</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.9.1">sS-cT</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.9.2">24.82 / <span class="ltx_text ltx_font_bold" id="A1.T9.1.1.9.2.1">22.51</span>
</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.9.3">15.18 / 15.56</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.9.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.9.4.1" style="background-color:#D4E7CF;">85.28 / 86.95</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.10.1">sS-c_</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.2">
<span class="ltx_text ltx_font_bold" id="A1.T9.1.1.10.2.1">24.83</span> / 22.15</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.3">15.14 / 15.56</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.10.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.10.4.1" style="background-color:#D4E7CF;">85.08 / 86.79</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.11.1">s_-c_</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.2">23.84 / 21.63</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.3">14.43 / 15.10</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.11.4" style="background-color:#D4E7CF;"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.11.4.1" style="background-color:#D4E7CF;">85.95<span class="ltx_text ltx_font_medium" id="A1.T9.1.1.11.4.1.1"> / </span>87.02</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.1.1.12.1">sT-cT</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.2">24.77 / 22.08</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.3">â€„â€„5.37 / 12.36</td>
<td class="ltx_td ltx_align_center" id="A1.T9.1.1.12.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.12.4.1" style="background-color:#D4E7CF;">29.56 / 79.11</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.1.13">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T9.1.1.13.1">Remove</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T9.1.1.13.2">24.73 / 21.96</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T9.1.1.13.3">12.22 / 13.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T9.1.1.13.4" style="background-color:#D4E7CF;"><span class="ltx_text" id="A1.T9.1.1.13.4.1" style="background-color:#D4E7CF;">52.44 / 74.95</span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>
Experiments of the LCS strategy variants. â€˜s/câ€™ denotes the shallow stage or the language converter stage in the encoder of LCS, â€˜S/Tâ€™ denotes the source or target LT in front of sentences, and â€˜_â€™ denotes no LT in this stage.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Impact of the Language Tag Placement</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">In this section, we investigate the impact of the LT placement for LCS.
We first conduct experiments to explore the influence of the source and target LT in two stages of LCS.
As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.T9" title="Table 9 â€£ A.6 Evaluation â€£ Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">9</span></a>, when placing the source language tag on the shallow stage, the choice of language tag on the language converter stage will yield a relatively small impact on both datasets.
However, placing the target LT or no placing performs unstable on the shallow stage, with bad BLEU scores.
We also explore the impact of removing the target LT on the decoder side on LCS, as listed as â€˜Removeâ€™ in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.T9" title="Table 9 â€£ A.6 Evaluation â€£ Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">9</span></a>.
Compared to the vanilla LCS strategy (â€˜sS-cSâ€™ in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A1.T9" title="Table 9 â€£ A.6 Evaluation â€£ Appendix A Dataset &amp; Training details â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">9</span></a>), â€˜Removeâ€™ degrades 0.17 and 3.00 BLEU scores on zero-shot translation of two datasets, suggesting that placing the target LT on the decoder side is beneficial for LCS to perform better on zero-shot translation.
Therefore, we conclude that placing the source LT on the encoder side and the target LT on the decoder side is optimal for LCS.</p>
</div>
<figure class="ltx_table" id="A2.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T10.1" style="width:201.6pt;height:84.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.6pt,20.8pt) scale(0.670242558983684,0.670242558983684) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T10.1.1">
<tr class="ltx_tr" id="A2.T10.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A2.T10.1.1.1.1" rowspan="2"><span class="ltx_text" id="A2.T10.1.1.1.1.1"><math alttext="\mathbf{k}" class="ltx_Math" display="inline" id="A2.T10.1.1.1.1.1.m1.1"><semantics id="A2.T10.1.1.1.1.1.m1.1a"><mi id="A2.T10.1.1.1.1.1.m1.1.1" xref="A2.T10.1.1.1.1.1.m1.1.1.cmml">ğ¤</mi><annotation-xml encoding="MathML-Content" id="A2.T10.1.1.1.1.1.m1.1b"><ci id="A2.T10.1.1.1.1.1.m1.1.1.cmml" xref="A2.T10.1.1.1.1.1.m1.1.1">ğ¤</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.1.1.1.1.1.m1.1c">\mathbf{k}</annotation><annotation encoding="application/x-llamapun" id="A2.T10.1.1.1.1.1.m1.1d">bold_k</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A2.T10.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.2.1">Supervised</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A2.T10.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A2.T10.1.1.1.3.1">Zero-Shot</span></td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.2.1"><span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.1.1">24-layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.2.2"><span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.2.1">48-layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.2.3"><span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.3.1">24-layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.2.4"><span class="ltx_text ltx_font_bold" id="A2.T10.1.1.2.4.1">48-layer</span></td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.3.1">2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.3.2">28.01 / 24.20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.3.3">29.51 / 25.58</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.1.3.4">â€„â€„4.68 / 5.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.1.3.5">â€„â€„3.51 / 4.42</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.4.1">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.4.2">28.16 / 24.46</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.4.3">29.38 / <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.4.3.1">25.73</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.4.4">â€„â€„4.55 / 5.03</td>
<td class="ltx_td ltx_align_center" id="A2.T10.1.1.4.5">â€„â€„4.07 / 9.48</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.5.1">4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.5.2">
<span class="ltx_text ltx_font_bold" id="A2.T10.1.1.5.2.1">28.21</span> / 24.72</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.5.3">29.28 / 25.68</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.5.4">16.19 / 17.25</td>
<td class="ltx_td ltx_align_center" id="A2.T10.1.1.5.5">10.34 / 6.19</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.6.1">5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.6.2">28.04 / <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.2.1">24.77</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.6.3">
<span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.3.1">29.97</span> / 25.45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.1.6.4">
<span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.4.1">17.29</span> / <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.6.4.2">18.29</span>
</td>
<td class="ltx_td ltx_align_center" id="A2.T10.1.1.6.5">16.61 / 18.26</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T10.1.1.7.1">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T10.1.1.7.2">27.11 / 24.51</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T10.1.1.7.3">28.51 / 25.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T10.1.1.7.4">17.20 / 18.11</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T10.1.1.7.5">
<span class="ltx_text ltx_font_bold" id="A2.T10.1.1.7.5.1">17.75</span> / <span class="ltx_text ltx_font_bold" id="A2.T10.1.1.7.5.2">18.65</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>
BLEU scores of LCS with different <math alttext="k" class="ltx_Math" display="inline" id="A2.T10.5.m1.1"><semantics id="A2.T10.5.m1.1b"><mi id="A2.T10.5.m1.1.1" xref="A2.T10.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.T10.5.m1.1c"><ci id="A2.T10.5.m1.1.1.cmml" xref="A2.T10.5.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.5.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="A2.T10.5.m1.1e">italic_k</annotation></semantics></math> on the noise and denoise OPUS-100.
<math alttext="k" class="ltx_Math" display="inline" id="A2.T10.6.m2.1"><semantics id="A2.T10.6.m2.1b"><mi id="A2.T10.6.m2.1.1" xref="A2.T10.6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.T10.6.m2.1c"><ci id="A2.T10.6.m2.1.1.cmml" xref="A2.T10.6.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.6.m2.1d">k</annotation><annotation encoding="application/x-llamapun" id="A2.T10.6.m2.1e">italic_k</annotation></semantics></math> denotes the hyperparameter <math alttext="k" class="ltx_Math" display="inline" id="A2.T10.7.m3.1"><semantics id="A2.T10.7.m3.1b"><mi id="A2.T10.7.m3.1.1" xref="A2.T10.7.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.T10.7.m3.1c"><ci id="A2.T10.7.m3.1.1.cmml" xref="A2.T10.7.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.7.m3.1d">k</annotation><annotation encoding="application/x-llamapun" id="A2.T10.7.m3.1e">italic_k</annotation></semantics></math> and 24/48-layer denotes the encoderâ€™s depth.
</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="A2.F6.1" style="width:353.1pt;height:359.2pt;vertical-align:-359.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.2pt,0.0pt) scale(0.875,0.875) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F6.sf1">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="A2.F6.sf1.1" style="width:130.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F6.sf1.1.g1" src="x8.png" width="830"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Many-to-One, T-Enc</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F6.sf2">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="A2.F6.sf2.1" style="width:130.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F6.sf2.1.g1" src="x9.png" width="830"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Many-to-One, S-Enc-T-Dec</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F6.sf3">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="A2.F6.sf3.1" style="width:130.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F6.sf3.1.g1" src="x10.png" width="830"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Many-to-One, LCS</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="A2.F6.2" style="width:353.1pt;height:369.7pt;vertical-align:-369.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.2pt,0.0pt) scale(0.875,0.875) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F6.sf4">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="A2.F6.sf4.1" style="width:130.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F6.sf4.1.g1" src="x11.png" width="830"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>One-to-Many, T-Enc</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F6.sf5">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="A2.F6.sf5.1" style="width:130.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F6.sf5.1.g1" src="x12.png" width="830"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(e) </span>One-to-Many, S-Enc-T-Dec</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F6.sf6">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_top" id="A2.F6.sf6.1" style="width:130.1pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F6.sf6.1.g1" src="x13.png" width="830"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(f) </span>One-to-Many, LCS</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
The KDE of T-SNE reduced averaged encoder output in many-to-one and one-to-many directions.
</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="A2.F7.3" style="width:211.9pt;height:352.6pt;vertical-align:-352.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-112.5pt,0.0pt) scale(0.485,0.485) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.1.1">
<div class="ltx_block ltx_minipage ltx_align_top" id="A2.F7.1.1.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F7.1.1.1.g1" src="x14.png" width="830"/>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.2.2">
<div class="ltx_block ltx_minipage ltx_align_top" id="A2.F7.2.2.1" style="width:433.6pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="622" id="A2.F7.2.2.1.g1" src="x15.png" width="830"/>
</div>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
Translation performance of deeper encoder on the supervised and zero-shot test set of denoised OPUS-100.
</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Selection of <math alttext="k" class="ltx_Math" display="inline" id="A3.1.m1.1"><semantics id="A3.1.m1.1b"><mi id="A3.1.m1.1.1" xref="A3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.1.m1.1c"><ci id="A3.1.m1.1.1.cmml" xref="A3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="A3.1.m1.1e">italic_k</annotation></semantics></math> in deeper encoder</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In this section, we supply the experiments of exploration on hyperparameter <math alttext="k" class="ltx_Math" display="inline" id="A3.p1.1.m1.1"><semantics id="A3.p1.1.m1.1a"><mi id="A3.p1.1.m1.1.1" xref="A3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p1.1.m1.1b"><ci id="A3.p1.1.m1.1.1.cmml" xref="A3.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p1.1.m1.1d">italic_k</annotation></semantics></math> in 24- and 48-layer encoders on the noise and denoise OPUS-100 datasets.
We report the results in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.T10" title="Table 10 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.2">As illustrated by Tab.<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.T10" title="Table 10 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">10</span></a>, the optimal selection of <math alttext="k" class="ltx_Math" display="inline" id="A3.p2.1.m1.1"><semantics id="A3.p2.1.m1.1a"><mi id="A3.p2.1.m1.1.1" xref="A3.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p2.1.m1.1b"><ci id="A3.p2.1.m1.1.1.cmml" xref="A3.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p2.1.m1.1d">italic_k</annotation></semantics></math> in the 24-layer encoder is 5 and 6 in the 48-layer encoder on zero-shot translation, while keeping similar performance on supervised translation.
The better range of selection of <math alttext="k" class="ltx_Math" display="inline" id="A3.p2.2.m2.1"><semantics id="A3.p2.2.m2.1a"><mi id="A3.p2.2.m2.1.1" xref="A3.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.p2.2.m2.1b"><ci id="A3.p2.2.m2.1.1.cmml" xref="A3.p2.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.p2.2.m2.1d">italic_k</annotation></semantics></math> is the nearby integers of 15% of the encoder depth.</p>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1">Further, we also evaluate the performance of T-Enc, S-Enc-T-Dec, and LCS strategies on the denoised OPUS-100 dataset.
Compared to the noise version, although the T-Enc could roughly yield better performance of zero-shot translation with more encoder layers, LCS could perform better than it, demonstrating that LCS is effective in boosting the performance of zero-shot translation.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>T-SNE Visualization</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">To detect the language-specific of the encoder output representation, we sample 2000 samples from the MultiUN test set<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>Each sentence has the corresponding translation in every language.</span></span></span>, each with four languages, <em class="ltx_emph ltx_font_italic" id="A4.p1.1.1">i.e.</em>, Arabic (Ar), English (En), Russian (Ru), and Chinese(Zh).
We retrieve the encoder output representation in T-Enc, S-Enc-T-Dec, and LCS strategies on many-to-one and one-to-many directions<span class="ltx_note ltx_role_footnote" id="footnote17"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span>Our models are trained in many-to-many direction</span></span></span>, and visualize them by T-SNE <cite class="ltx_cite ltx_citemacro_citep">(VanÂ der Maaten and Hinton, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib28" title="">2008</a>)</cite>, following <cite class="ltx_cite ltx_citemacro_citet">Pan etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#bib.bib24" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.F6" title="Figure 6 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>, in the many-to-one direction (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.F6.sf1" title="In Figure 6 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6(a)</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.F6.sf2" title="In Figure 6 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6(b)</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.F6.sf3" title="In Figure 6 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6(c)</span></a>), the contour lines of three strategies tend to overlap, while the contour lines of T-Enc have the highest degree to overlap with each other, and the contour lines of S-Enc-T-Dec have the lowest degree.
The overlap results suggest that the language-specific of the encoder output representation is target-language-specific and the overlap ranking of the three strategy contour lines is consistent with the cosine similarity ranking in <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#S3.F2" title="Figure 2 â€£ 3.4 Language Indication in the Encoder â€£ 3 Probing Off-Target issue in MNMT â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.
However, in the one-to-many direction of S-Enc-T-Dec (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.02876v2#A2.F6.sf5" title="In Figure 6 â€£ Appendix B Impact of the Language Tag Placement â€£ LCS: A Language Converter Strategy for Zero-Shot Neural Machine Translation"><span class="ltx_text ltx_ref_tag">6(e)</span></a>), the contour lines nearly perfectly overlap with each other, suggesting that the encoder output representation of S-Enc-T-Dec tends to be source-specific rather than target-language-specific with the indication from the source language tag in deep encoder layers.
In summary, the encoder output representation of the T-Enc and LCS strategy tends to be target-language-specific, while the representation of the S-Enc-T-Dec strategy tends to be source-language-specific.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jun  6 03:13:56 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
