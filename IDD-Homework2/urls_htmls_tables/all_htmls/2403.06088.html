<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.06088] Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models</title><meta property="og:description" content="In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for s…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.06088">

<!--Generated on Fri Apr  5 13:40:35 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> <img src="/html/2403.06088/assets/x1.png" id="id1.1.g1" class="ltx_graphics ltx_img_square" width="5" height="5" alt="[Uncaptioned image]"> Esmaeil Seraj 
<br class="ltx_break">Research and Advanced Engineering
<br class="ltx_break">2101 Village Rd, Room 3112
<br class="ltx_break">Dearborn, MI 48124, USA 
<br class="ltx_break"><span id="id3.3.id1" class="ltx_text ltx_font_typewriter">eseraj@ford.com</span> 
<br class="ltx_break">&amp;<img src="/html/2403.06088/assets/x1.png" id="id2.2.g2" class="ltx_graphics ltx_img_square" width="5" height="5" alt="[Uncaptioned image]"> Walter Talamonti 
<br class="ltx_break">Research and Advanced Engineering
<br class="ltx_break">2101 Village Rd, Room 3112
<br class="ltx_break">Dearborn, MI 48124, USA 
<br class="ltx_break"><span id="id4.4.id2" class="ltx_text ltx_font_typewriter">wtalamo1@ford.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Corresponding author &lt;email: esmaeil.seraj09@gmail.com&gt;.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">In the burgeoning field of intelligent transportation systems, enhancing vehicle-driver interaction through facial attribute recognition, such as facial expression, eye gaze, age, etc., is of paramount importance for safety, personalization, and overall user experience. However, the scarcity of comprehensive large-scale, real-world datasets poses a significant challenge for training robust multi-task models. Existing literature often overlooks the potential of synthetic datasets and the comparative efficacy of state-of-the-art vision foundation models in such constrained settings. This paper addresses these gaps by investigating the utility of synthetic datasets for training complex multi-task models that recognize facial attributes of passengers of a vehicle, such as gaze plane, age, and facial expression. Utilizing transfer learning techniques with both pre-trained Vision Transformer (ViT) and Residual Network (ResNet) models, we explore various training and adaptation methods to optimize performance, particularly when data availability is limited. We provide extensive post-evaluation analysis, investigating the effects of synthetic data distributions on model performance in in-distribution data and out-of-distribution inference. Our study unveils counter-intuitive findings, notably the superior performance of ResNet over ViTs in our specific multi-task context, which is attributed to the mismatch in model complexity relative to task complexity. Our results highlight the challenges and opportunities for enhancing the use of synthetic data and vision foundation models in practical applications.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.4" class="ltx_p"><em id="p1.4.1" class="ltx_emph ltx_font_bold ltx_font_italic">Keywords</em> Vision Foundation Models  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Facial Attribute Recognition  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Multi-Task Learning  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Synthetic Data  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
Intelligent Transportation Systems</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">The advancement of intelligent transportation systems has opened new avenues for enhancing the interaction between vehicles and their drivers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. One such promising area is the use of in-cabin cameras to recognize facial attributes of the driver and passengers, thereby improving safety measures, personalizing user experiences, and facilitating more natural human-vehicle interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">However, the field faces several challenges that limit its potential. Existing literature primarily focuses on using real-world datasets for training models, which are often expensive and time-consuming to collect and can raise privacy concerns (i.e., collecting personal data for training models) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Moreover, there is a lack of comprehensive studies that explore the efficacy of using synthetic datasets and pre-trained vision foundation models for in-vehicle facial recognition tasks. These gaps in the literature present a missed opportunity for leveraging alternative, possibly more efficient, methods for model training and evaluation such as the utility of synthetic data and pre-trained foundation models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Existing machine learning methods for synthetic data generation span a variety of techniques, most notably Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, as well as Diffisuion Models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These methods have been widely used to create high-fidelity, realistic synthetic datasets that can mimic the complexities of real-world data. The generated synthetic data serves as a valuable resource for training and validating machine learning models, especially in scenarios where collecting real-world data is challenging or costly, such as healthcare domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Utilizing pre-trained models when working with limited synthetic data can offer significant advantages, such as reducing the training time and computational resources required <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Moreover, these models often come with learned features that can enhance the model’s generalization capabilities, thereby improving performance on both in-distribution and out-of-distribution tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.06088/assets/vision_foundation_model.png" id="S1.F1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="176" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of a vision foundation model for in-vehicle perception and intelligence. Data is collected and used to train a deep neural network model. The trained model then generates a high-dimensional feature space representing the input data which can be used for any downstream task. To adapt the learned foundation model for downstream task, adaptation methods are needed.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">The use of synthetic data and pre-trained models introduces its own set of challenges. Synthetic datasets, while easier to generate, may not perfectly replicate the complexities of real-world data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Similarly, pre-trained models like Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and Deep Residual Networks (ResNets) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> come with their own limitations, such as robust adaptation, the risk of overfitting due to unbalanced model complexity with respect to task complexity, and the computational costs associated with their complexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. These challenges necessitate a thorough investigation to ascertain the utility of such approaches in the context of multi-task facial attribute recognition in vehicles.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">Motivated by these challenges and gaps, this paper aims to investigate the utility of synthetic datasets for training robust multi-task models capable of recognizing various facial attributes. We also explore the applicability of pre-trained vision foundation models (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), specifically ViTs and ResNets, for this task. Additionally, we explore and test various adaptation methods such as linear probing, prefix tuning, and full fine tuning as well as different training techniques such as curriculum learning to improve model performance. We also perform several ablation studies and experiments on the effects of different preprocessing steps on model performances. Finally, we provide extensive post-evaluation analysis, investigating the effects of synthetic data distribution on model performance in in-distribution and out-of-distribution data. The goal is to provide a comprehensive analysis that not only evaluates the performance of these models but also delves into the nuances of training techniques and data quality.</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p">In this work, we collect synthetic datasets from three major data generation companies. Herein, to protect proprietary and confidential information, we will refer to the three synthetic datasets as SynthA, SynthB, and SynthC datasets <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text" style="color:#FF0000;">Please note that, while the real dataset names are removed, to achieve reproducibility, we intend to release anonymized samples of each dataset in a public Github which will accompany the manuscript upon acceptance.</span></span></span></span> (see Section <a href="#S5.SS1" title="5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> for more details about each dataset). We employ various training and adaptation techniques to train multi-task models empowered by existing vision foundation models. We particularly focus on building a perception model capable of understanding several facial attributes, such as human gaze, age, and facial expression (Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), of the vehicle driver or passengers. Such modular system which is built on synthetic data and existing vision foundation models can be adapted to any desired downstream task (i.e., tasks can readily be added or removed), enabling in-vehicle intelligence and enhanced vehicle-driver interaction. We delve into the intricacies of model architecture, layer configurations, and training regimes to provide a nuanced understanding of how these foundation models can be effectively adapted for automotive applications with available synthetic datasets. We evaluate the performance of these models using both in-distribution and out-of-distribution datasets. Our empirical evaluation focuses on a range of metrics, including but not limited to, data and label distributions, model performance, and data quality. Our key contributions and findings are as follows:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A comprehensive study on the utility of synthetically generated datasets for training multi-task facial attribute recognition models and building perception models for in-vehicle intelligence.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">An in-depth analysis of the power, flexibility and performance of the pre-trained vision foundation models, revealing counter-intuitive findings such as the superior performance of ResNet over ViTs in our specific setting due to unbalanced model and task complexities.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Exploring several training and adaptation techniques for vision foundation models in order to improve model performance, particularly in scenarios with limited data availability.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i4.p1.1" class="ltx_p">Post-evaluation inference and data distribution analysis and providing critical insights into the limitations and potential improvements for using synthetic data in real-world applications.</p>
</div>
</li>
</ol>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2403.06088/assets/example_incabin_ai_3img.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="183" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example of in-vehicle perception and intelligence via multi-task facial attribute recognition on synthetically generated data. Figure demonstrates an in-cabin perception system capable of understanding several facial attributes, such as gaze, age, and facial expression of the driver and passengers. Such modular system which is built on readily generated synthetic data and existing vision foundation models can be adapted to any desired downstream task, enabling an enhanced vehicle-driver interaction and passenger experience.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Prior Work and Literature Review</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">The field of in-vehicle facial recognition has garnered significant attention in recent years, driven by the increasing integration of intelligent systems into automotive applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This section provides an comprehensive overview of the existing literature, focusing on four main areas: intelligent and connected vehicles, driver-vehicle interactions, in-vehicle facial recognition technologies, and the use of vision foundation models in automotive settings. We also identify gaps in the current research landscape that our work aims to address.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Intelligent and Connected Vehicles</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">In recent years, the field of intelligent and connected vehicles has garnered significant attention, serving as a cornerstone for the advancement of modern transportation systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. These vehicles are equipped with a myriad of sensors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> and communication technologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> that enable them to perceive their environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, make decisions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, control and maneuver <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, and communicate with other vehicles and infrastructure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>. The concept of Vehicle-to-Everything (V2X) communication has been extensively studied to facilitate real-time data exchange, enhancing road safety and traffic efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>. Machine Learning (ML) and Computer Vision (CV) techniques have been employed for various in-vehicle perception tasks, such as driver monitoring <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, and path planning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Driver-Vehicle Interactions</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">In the realm of driver-vehicle interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, a plethora of research has been conducted to explore various facets of this complex relationship, with the overarching goal of enhancing safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, and user experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>. Studies have delved into the use of advanced Human-Machine Interfaces (HMIs), such as touchscreens, voice commands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, and even gesture recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, to facilitate seamless communication between the driver and the vehicle. Additionally, there has been a growing interest in leveraging machine learning algorithms to understand and predict driver behavior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, including attention level <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>, emotional state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>, and intent<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>, <a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>. These predictive models often utilize data from in-car sensors, cameras, and other IoT devices to provide real-time feedback or even automated interventions, such as adaptive cruise control adjustments or lane-keeping assistance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. However, much of the existing work has relied on traditional data collection methods and single-task learning models, overlooking the potential of multi-task learning frameworks and synthetic datasets, which are the focus of our paper. Our research aims to extend the current understanding by exploring the efficacy of multi-task facial attribute recognition models in enhancing driver-vehicle interactions, particularly when trained on synthetic data.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>In-Vehicle Facial Recognition</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">In-vehicle facial recognition technologies have primarily been developed to enhance safety and user experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. These systems often employ a variety of machine learning algorithms, including Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, to detect and analyze facial attributes such as gaze direction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>, <a href="#bib.bib97" title="" class="ltx_ref">97</a>, <a href="#bib.bib98" title="" class="ltx_ref">98</a>, <a href="#bib.bib99" title="" class="ltx_ref">99</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib100" title="" class="ltx_ref">100</a>, <a href="#bib.bib101" title="" class="ltx_ref">101</a>, <a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>, age <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>, <a href="#bib.bib104" title="" class="ltx_ref">104</a>, <a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, and emotional state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>, <a href="#bib.bib107" title="" class="ltx_ref">107</a>, <a href="#bib.bib108" title="" class="ltx_ref">108</a>, <a href="#bib.bib109" title="" class="ltx_ref">109</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>. The majority of these studies rely on real-world datasets, which are labor-intensive and costly to collect. Moreover, these datasets often suffer from issues of imbalance and lack of diversity, which can lead to biased or less robust models.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">The existing literature largely overlooks the potential of synthetic datasets for training ML and CV models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The focus has been on traditional data collection methods, such as in-car cameras and mobile devices, which may not be scalable or cost-effective for broader applications and could raise privacy concerns (i.e., collecting personal data for training models) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Our work aims to fill this gap by investigating the utility of synthetic datasets for training multi-task facial recognition models in vehicles. We also explore the use of advanced transfer learning and adaptation techniques to improve the generalizability and robustness of these models, empowered by existing pre-trained foundation models. The incorporation of synthetic data not only alleviates the need for extensive real-world data collection but also provides a controlled environment to simulate various lighting conditions, angles, and occlusions that are critical for in-vehicle applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Overall, our work contributes to the growing body of research that advocates for a more flexible and scalable approach to data collection and model training in the realm of in-vehicle facial recognition.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Vision Foundation Models for Automotive Applications</h3>

<div id="S2.SS4.p1" class="ltx_para ltx_noindent">
<p id="S2.SS4.p1.1" class="ltx_p">Vision foundation models like Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>, ResNets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, and more recently, Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, have been employed in various automotive applications, ranging from object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to lane tracking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, and even driver monitoring systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. These models offer robust performance but often require substantial computational resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>, <a href="#bib.bib111" title="" class="ltx_ref">111</a>, <a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite>, making them less ideal for real-time, in-vehicle applications. The computational burden is further exacerbated when these models are deployed in embedded systems with limited processing capabilities, which are common in automotive settings.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para ltx_noindent">
<p id="S2.SS4.p2.1" class="ltx_p">A limitation in the current literature is the lack of comprehensive studies that explore the trade-offs between different types of vision foundation models, especially in the context of multi-task learning for in-vehicle applications. This gap is significant because each type of foundation model has its own set of advantages and limitations that could be more or less suitable for specific tasks. For instance, CNNs are generally good for spatial hierarchies but may struggle with long-range dependencies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>, <a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>, <a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>, <a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>, whereas ViTs excel in capturing such dependencies but might be computationally more intensive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>, <a href="#bib.bib122" title="" class="ltx_ref">122</a>, <a href="#bib.bib123" title="" class="ltx_ref">123</a>, <a href="#bib.bib124" title="" class="ltx_ref">124</a>, <a href="#bib.bib125" title="" class="ltx_ref">125</a>, <a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite>. Furthermore, there is minimal exploration of training techniques that could optimize these models for better performance and efficiency, such as fine-tuning, transfer learning, and data augmentation strategies tailored for automotive scenarios.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para ltx_noindent">
<p id="S2.SS4.p3.1" class="ltx_p">Our work addresses these limitations by evaluating the performance of pre-trained ResNets and ViTs in a multi-task learning framework, while also exploring various training and adaptation techniques. We delve into the intricacies of model architecture, layer configurations, and training regimes to provide a nuanced understanding of how these foundation models can be effectively adapted for automotive applications. Additionally, we investigate the impact of using synthetic data to train these models, offering insights into how the quality and distribution of training data can affect both in-distribution and out-of-distribution performance. Overall, our research contributes to a more holistic understanding of the role and optimization of vision foundation models in the rapidly evolving field of automotive intelligence, particularly when paired with limited amount, synthetically generated datasets.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">This section provides a comprehensive background on the key concepts and methodologies that underpin our research. We delve into multi-task learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>, transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>, and vision foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, elucidating the mathematical foundations and practical implications of each.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Multi-Task Learning</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.6" class="ltx_p">Multi-Task Learning (MTL) is a learning paradigm where a single model is trained to perform multiple tasks simultaneously <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>, <a href="#bib.bib130" title="" class="ltx_ref">130</a>, <a href="#bib.bib131" title="" class="ltx_ref">131</a>, <a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite>. The primary advantage of MTL is that it allows the model to leverage shared representations across tasks, often leading to improved generalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>, <a href="#bib.bib134" title="" class="ltx_ref">134</a>, <a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite>. The general loss function for an MTL model can be formulated as in Equation <a href="#S3.E1" title="In 3.1 Multi-Task Learning ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, where <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{i}(\theta)" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.2" xref="S3.SS1.p1.1.m1.1.2.cmml"><msub id="S3.SS1.p1.1.m1.1.2.2" xref="S3.SS1.p1.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.2.2.2" xref="S3.SS1.p1.1.m1.1.2.2.2.cmml">ℒ</mi><mi id="S3.SS1.p1.1.m1.1.2.2.3" xref="S3.SS1.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.2.1" xref="S3.SS1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS1.p1.1.m1.1.2.3.2" xref="S3.SS1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.1.2.3.2.1" xref="S3.SS1.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.1.2.3.2.2" xref="S3.SS1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.2"><times id="S3.SS1.p1.1.m1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.2.1"></times><apply id="S3.SS1.p1.1.m1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.2.2.1.cmml" xref="S3.SS1.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.2.2.2.cmml" xref="S3.SS1.p1.1.m1.1.2.2.2">ℒ</ci><ci id="S3.SS1.p1.1.m1.1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{L}_{i}(\theta)</annotation></semantics></math> is the loss for task <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">i</annotation></semantics></math>, <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="w_{i}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">w</mi><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑤</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">w_{i}</annotation></semantics></math> is the weight for task <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">i</annotation></semantics></math>, <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">\theta</annotation></semantics></math> are the model parameters, and <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">N</annotation></semantics></math> is the number of tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>.</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\mathcal{L}(\theta)=\sum_{i=1}^{N}w_{i}\mathcal{L}_{i}(\theta)" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><mrow id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.2.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.2.1" xref="S3.E1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.2.3.2.1" xref="S3.E1.m1.2.3.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.2.3.2.3.2.2" xref="S3.E1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.2.3.1" xref="S3.E1.m1.2.3.1.cmml">=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><munderover id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml"><mo movablelimits="false" id="S3.E1.m1.2.3.3.1.2.2" xref="S3.E1.m1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.3.3.1.2.3" xref="S3.E1.m1.2.3.3.1.2.3.cmml"><mi id="S3.E1.m1.2.3.3.1.2.3.2" xref="S3.E1.m1.2.3.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.2.3.3.1.2.3.1" xref="S3.E1.m1.2.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.3.3.1.2.3.3" xref="S3.E1.m1.2.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.2.3.3.1.3" xref="S3.E1.m1.2.3.3.1.3.cmml">N</mi></munderover><mrow id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml"><msub id="S3.E1.m1.2.3.3.2.2" xref="S3.E1.m1.2.3.3.2.2.cmml"><mi id="S3.E1.m1.2.3.3.2.2.2" xref="S3.E1.m1.2.3.3.2.2.2.cmml">w</mi><mi id="S3.E1.m1.2.3.3.2.2.3" xref="S3.E1.m1.2.3.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.2.1" xref="S3.E1.m1.2.3.3.2.1.cmml">​</mo><msub id="S3.E1.m1.2.3.3.2.3" xref="S3.E1.m1.2.3.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.2.3.3.2.3.2" xref="S3.E1.m1.2.3.3.2.3.2.cmml">ℒ</mi><mi id="S3.E1.m1.2.3.3.2.3.3" xref="S3.E1.m1.2.3.3.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.3.3.2.1a" xref="S3.E1.m1.2.3.3.2.1.cmml">​</mo><mrow id="S3.E1.m1.2.3.3.2.4.2" xref="S3.E1.m1.2.3.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.3.3.2.4.2.1" xref="S3.E1.m1.2.3.3.2.cmml">(</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">θ</mi><mo stretchy="false" id="S3.E1.m1.2.3.3.2.4.2.2" xref="S3.E1.m1.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><eq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></eq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><times id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2.1"></times><ci id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2.2">ℒ</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝜃</ci></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><apply id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.1.1.cmml" xref="S3.E1.m1.2.3.3.1">superscript</csymbol><apply id="S3.E1.m1.2.3.3.1.2.cmml" xref="S3.E1.m1.2.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.1.2.1.cmml" xref="S3.E1.m1.2.3.3.1">subscript</csymbol><sum id="S3.E1.m1.2.3.3.1.2.2.cmml" xref="S3.E1.m1.2.3.3.1.2.2"></sum><apply id="S3.E1.m1.2.3.3.1.2.3.cmml" xref="S3.E1.m1.2.3.3.1.2.3"><eq id="S3.E1.m1.2.3.3.1.2.3.1.cmml" xref="S3.E1.m1.2.3.3.1.2.3.1"></eq><ci id="S3.E1.m1.2.3.3.1.2.3.2.cmml" xref="S3.E1.m1.2.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E1.m1.2.3.3.1.2.3.3.cmml" xref="S3.E1.m1.2.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.2.3.3.1.3.cmml" xref="S3.E1.m1.2.3.3.1.3">𝑁</ci></apply><apply id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2"><times id="S3.E1.m1.2.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.2.1"></times><apply id="S3.E1.m1.2.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.2.1.cmml" xref="S3.E1.m1.2.3.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.3.3.2.2.2.cmml" xref="S3.E1.m1.2.3.3.2.2.2">𝑤</ci><ci id="S3.E1.m1.2.3.3.2.2.3.cmml" xref="S3.E1.m1.2.3.3.2.2.3">𝑖</ci></apply><apply id="S3.E1.m1.2.3.3.2.3.cmml" xref="S3.E1.m1.2.3.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.2.3.1.cmml" xref="S3.E1.m1.2.3.3.2.3">subscript</csymbol><ci id="S3.E1.m1.2.3.3.2.3.2.cmml" xref="S3.E1.m1.2.3.3.2.3.2">ℒ</ci><ci id="S3.E1.m1.2.3.3.2.3.3.cmml" xref="S3.E1.m1.2.3.3.2.3.3">𝑖</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝜃</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\mathcal{L}(\theta)=\sum_{i=1}^{N}w_{i}\mathcal{L}_{i}(\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p">In the context of our research, MTL is particularly relevant for training a single model on top of existing powerful vision foundation models to recognize multiple downstream tasks, i.e., facial attributes, thereby reducing the computational load and increasing the efficiency of in-vehicle systems.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transfer Learning</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.5" class="ltx_p">Transfer Learning (TL) is a machine learning technique where a model developed for a particular task is adapted for a second related task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>, <a href="#bib.bib136" title="" class="ltx_ref">136</a>, <a href="#bib.bib137" title="" class="ltx_ref">137</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>. In the context of neural networks, this often involves fine-tuning a pre-trained model on a new dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>, <a href="#bib.bib140" title="" class="ltx_ref">140</a>, <a href="#bib.bib141" title="" class="ltx_ref">141</a>, <a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite>. The loss function for transfer learning can be expressed as in Equation <a href="#S3.E2" title="In 3.2 Transfer Learning ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}_{\text{source}}(\theta)" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><msub id="S3.SS2.p1.1.m1.1.2.2" xref="S3.SS2.p1.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.1.m1.1.2.2.2" xref="S3.SS2.p1.1.m1.1.2.2.2.cmml">ℒ</mi><mtext id="S3.SS2.p1.1.m1.1.2.2.3" xref="S3.SS2.p1.1.m1.1.2.2.3a.cmml">source</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.1.m1.1.2.1" xref="S3.SS2.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS2.p1.1.m1.1.2.3.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.p1.1.m1.1.2.3.2.1" xref="S3.SS2.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">θ</mi><mo stretchy="false" id="S3.SS2.p1.1.m1.1.2.3.2.2" xref="S3.SS2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><times id="S3.SS2.p1.1.m1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.1"></times><apply id="S3.SS2.p1.1.m1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2">ℒ</ci><ci id="S3.SS2.p1.1.m1.1.2.2.3a.cmml" xref="S3.SS2.p1.1.m1.1.2.2.3"><mtext mathsize="70%" id="S3.SS2.p1.1.m1.1.2.2.3.cmml" xref="S3.SS2.p1.1.m1.1.2.2.3">source</mtext></ci></apply><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\mathcal{L}_{\text{source}}(\theta)</annotation></semantics></math> is the loss on the source task, <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\text{target}}(\theta^{\prime})" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><msub id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1.3.2" xref="S3.SS2.p1.2.m2.1.1.3.2.cmml">ℒ</mi><mtext id="S3.SS2.p1.2.m2.1.1.3.3" xref="S3.SS2.p1.2.m2.1.1.3.3a.cmml">target</mtext></msub><mo lspace="0em" rspace="0em" id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS2.p1.2.m2.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">(</mo><msup id="S3.SS2.p1.2.m2.1.1.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.1.1.1.2" xref="S3.SS2.p1.2.m2.1.1.1.1.1.2.cmml">θ</mi><mo id="S3.SS2.p1.2.m2.1.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.SS2.p1.2.m2.1.1.1.1.3" xref="S3.SS2.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><times id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2"></times><apply id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.3.1.cmml" xref="S3.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.3.2.cmml" xref="S3.SS2.p1.2.m2.1.1.3.2">ℒ</ci><ci id="S3.SS2.p1.2.m2.1.1.3.3a.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3"><mtext mathsize="70%" id="S3.SS2.p1.2.m2.1.1.3.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3.3">target</mtext></ci></apply><apply id="S3.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.2">𝜃</ci><ci id="S3.SS2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.1.1.1.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathcal{L}_{\text{target}}(\theta^{\prime})</annotation></semantics></math> is the loss on the target task, <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="\theta" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\theta</annotation></semantics></math> and <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="\theta^{\prime}" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><msup id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">θ</mi><mo id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝜃</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">\theta^{\prime}</annotation></semantics></math> are the model parameters for the source and target tasks respectively, and <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">\lambda</annotation></semantics></math> is a regularization term <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite>.</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.4" class="ltx_Math" alttext="\mathcal{L}(\theta,\theta^{\prime})=\mathcal{L}_{\text{source}}(\theta)+\lambda\mathcal{L}_{\text{target}}(\theta^{\prime})" display="block"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.3.3.1.3" xref="S3.E2.m1.3.3.1.3.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.2.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">θ</mi><mo id="S3.E2.m1.3.3.1.1.1.3" xref="S3.E2.m1.3.3.1.1.2.cmml">,</mo><msup id="S3.E2.m1.3.3.1.1.1.1" xref="S3.E2.m1.3.3.1.1.1.1.cmml"><mi id="S3.E2.m1.3.3.1.1.1.1.2" xref="S3.E2.m1.3.3.1.1.1.1.2.cmml">θ</mi><mo id="S3.E2.m1.3.3.1.1.1.1.3" xref="S3.E2.m1.3.3.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.E2.m1.3.3.1.1.1.4" xref="S3.E2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml">=</mo><mrow id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><mrow id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml"><msub id="S3.E2.m1.4.4.2.3.2" xref="S3.E2.m1.4.4.2.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.2.3.2.2" xref="S3.E2.m1.4.4.2.3.2.2.cmml">ℒ</mi><mtext id="S3.E2.m1.4.4.2.3.2.3" xref="S3.E2.m1.4.4.2.3.2.3a.cmml">source</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.3.1" xref="S3.E2.m1.4.4.2.3.1.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.3.3.2" xref="S3.E2.m1.4.4.2.3.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.3.3.2.1" xref="S3.E2.m1.4.4.2.3.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">θ</mi><mo stretchy="false" id="S3.E2.m1.4.4.2.3.3.2.2" xref="S3.E2.m1.4.4.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml">+</mo><mrow id="S3.E2.m1.4.4.2.1" xref="S3.E2.m1.4.4.2.1.cmml"><mi id="S3.E2.m1.4.4.2.1.3" xref="S3.E2.m1.4.4.2.1.3.cmml">λ</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.1.2" xref="S3.E2.m1.4.4.2.1.2.cmml">​</mo><msub id="S3.E2.m1.4.4.2.1.4" xref="S3.E2.m1.4.4.2.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.4.4.2.1.4.2" xref="S3.E2.m1.4.4.2.1.4.2.cmml">ℒ</mi><mtext id="S3.E2.m1.4.4.2.1.4.3" xref="S3.E2.m1.4.4.2.1.4.3a.cmml">target</mtext></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.4.4.2.1.2a" xref="S3.E2.m1.4.4.2.1.2.cmml">​</mo><mrow id="S3.E2.m1.4.4.2.1.1.1" xref="S3.E2.m1.4.4.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m1.4.4.2.1.1.1.2" xref="S3.E2.m1.4.4.2.1.1.1.1.cmml">(</mo><msup id="S3.E2.m1.4.4.2.1.1.1.1" xref="S3.E2.m1.4.4.2.1.1.1.1.cmml"><mi id="S3.E2.m1.4.4.2.1.1.1.1.2" xref="S3.E2.m1.4.4.2.1.1.1.1.2.cmml">θ</mi><mo id="S3.E2.m1.4.4.2.1.1.1.1.3" xref="S3.E2.m1.4.4.2.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S3.E2.m1.4.4.2.1.1.1.3" xref="S3.E2.m1.4.4.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4"><eq id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"></eq><apply id="S3.E2.m1.3.3.1.cmml" xref="S3.E2.m1.3.3.1"><times id="S3.E2.m1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.2"></times><ci id="S3.E2.m1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.3">ℒ</ci><interval closure="open" id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝜃</ci><apply id="S3.E2.m1.3.3.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1.1">superscript</csymbol><ci id="S3.E2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.1.1.2">𝜃</ci><ci id="S3.E2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.1.1.3">′</ci></apply></interval></apply><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><plus id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2"></plus><apply id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3"><times id="S3.E2.m1.4.4.2.3.1.cmml" xref="S3.E2.m1.4.4.2.3.1"></times><apply id="S3.E2.m1.4.4.2.3.2.cmml" xref="S3.E2.m1.4.4.2.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.3.2.1.cmml" xref="S3.E2.m1.4.4.2.3.2">subscript</csymbol><ci id="S3.E2.m1.4.4.2.3.2.2.cmml" xref="S3.E2.m1.4.4.2.3.2.2">ℒ</ci><ci id="S3.E2.m1.4.4.2.3.2.3a.cmml" xref="S3.E2.m1.4.4.2.3.2.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.2.3.2.3.cmml" xref="S3.E2.m1.4.4.2.3.2.3">source</mtext></ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝜃</ci></apply><apply id="S3.E2.m1.4.4.2.1.cmml" xref="S3.E2.m1.4.4.2.1"><times id="S3.E2.m1.4.4.2.1.2.cmml" xref="S3.E2.m1.4.4.2.1.2"></times><ci id="S3.E2.m1.4.4.2.1.3.cmml" xref="S3.E2.m1.4.4.2.1.3">𝜆</ci><apply id="S3.E2.m1.4.4.2.1.4.cmml" xref="S3.E2.m1.4.4.2.1.4"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.1.4.1.cmml" xref="S3.E2.m1.4.4.2.1.4">subscript</csymbol><ci id="S3.E2.m1.4.4.2.1.4.2.cmml" xref="S3.E2.m1.4.4.2.1.4.2">ℒ</ci><ci id="S3.E2.m1.4.4.2.1.4.3a.cmml" xref="S3.E2.m1.4.4.2.1.4.3"><mtext mathsize="70%" id="S3.E2.m1.4.4.2.1.4.3.cmml" xref="S3.E2.m1.4.4.2.1.4.3">target</mtext></ci></apply><apply id="S3.E2.m1.4.4.2.1.1.1.1.cmml" xref="S3.E2.m1.4.4.2.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.1.1.1.1.1.cmml" xref="S3.E2.m1.4.4.2.1.1.1">superscript</csymbol><ci id="S3.E2.m1.4.4.2.1.1.1.1.2.cmml" xref="S3.E2.m1.4.4.2.1.1.1.1.2">𝜃</ci><ci id="S3.E2.m1.4.4.2.1.1.1.1.3.cmml" xref="S3.E2.m1.4.4.2.1.1.1.1.3">′</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\mathcal{L}(\theta,\theta^{\prime})=\mathcal{L}_{\text{source}}(\theta)+\lambda\mathcal{L}_{\text{target}}(\theta^{\prime})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p">Transfer learning is pivotal in our work for adapting existing pre-trained vision foundation models, such as ViTs and ResNets, in order to exploit their power and flexibility for our specific multi-task learning problem, thereby saving time and computational resources.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Vision Foundation Models</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">Foundation models serve as a cornerstone in the machine learning landscape, offering pre-trained architectures that can be fine-tuned for a wide array of specific tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. These models are trained on large, diverse datasets, enabling them to capture intricate patterns and relationships in the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. When specialized for computer vision tasks, these foundation models are referred to as vision foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>, <a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite>. They have been instrumental in advancing the field of computer vision, offering robust performance across a range of tasks from object detection to semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>, <a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>. The adaptability and generalizability of these models make them a valuable asset in the development of specialized computer vision applications.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">In the context of our research, vision foundation models play a pivotal role in achieving high-performance multi-task learning for in-vehicle facial attribute recognition. Leveraging pre-trained vision foundation models such as ResNet and ViTs allows us to bypass the need for extensive data collection and training from scratch. This is particularly beneficial given the resource constraints and the need for real-time processing in automotive applications. By fine-tuning these models on our specific tasks via advanced adaptation techniques (Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), we can achieve enhanced performance while significantly reducing the time and computational resources required for model development. In the subsequent sections, we will provide a succinct overview of two foundational models integral to our research: Residual Networks (ResNet) and Vision Transformers (ViT). ResNet models are the most prevalent foundation models and have established themselves as a cornerstone in the realm of computer vision tasks, while ViTs represent the latest advancements in architectural design, garnering significant attention for their exceptional computational efficacy and performance metrics.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Residual Network (ResNet) Model</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">Residual Networks (ResNets) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> are a type of Convolutional Neural Network (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> that include skip connections to allow gradients to flow through the network more easily. This architecture mitigates the vanishing gradient problem, making it possible to train very deep networks effectively.</p>
</div>
<div id="S3.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS1.p2.3" class="ltx_p">The defining feature of ResNets is the incorporation of residual connections, also known as skip connections, that bypass one or more layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. These connections are added back to the output of the stacked layers, forming the final output of the residual block. Mathematically, this can be represented as <math id="S3.SS3.SSS1.p2.1.m1.2" class="ltx_Math" alttext="F(x)=H(x)-x" display="inline"><semantics id="S3.SS3.SSS1.p2.1.m1.2a"><mrow id="S3.SS3.SSS1.p2.1.m1.2.3" xref="S3.SS3.SSS1.p2.1.m1.2.3.cmml"><mrow id="S3.SS3.SSS1.p2.1.m1.2.3.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.2.3.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.1.m1.2.3.2.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.SS3.SSS1.p2.1.m1.2.3.2.3.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p2.1.m1.2.3.2.3.2.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.cmml">(</mo><mi id="S3.SS3.SSS1.p2.1.m1.1.1" xref="S3.SS3.SSS1.p2.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS3.SSS1.p2.1.m1.2.3.2.3.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p2.1.m1.2.3.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS3.SSS1.p2.1.m1.2.3.3" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.cmml"><mrow id="S3.SS3.SSS1.p2.1.m1.2.3.3.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.cmml"><mi id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.1.cmml">​</mo><mrow id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.3.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.3.2.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.cmml">(</mo><mi id="S3.SS3.SSS1.p2.1.m1.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.2.cmml">x</mi><mo stretchy="false" id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.3.2.2" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS1.p2.1.m1.2.3.3.1" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.1.cmml">−</mo><mi id="S3.SS3.SSS1.p2.1.m1.2.3.3.3" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.3.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.1.m1.2b"><apply id="S3.SS3.SSS1.p2.1.m1.2.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3"><eq id="S3.SS3.SSS1.p2.1.m1.2.3.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.1"></eq><apply id="S3.SS3.SSS1.p2.1.m1.2.3.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.2"><times id="S3.SS3.SSS1.p2.1.m1.2.3.2.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.1"></times><ci id="S3.SS3.SSS1.p2.1.m1.2.3.2.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.2.2">𝐹</ci><ci id="S3.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.1.1">𝑥</ci></apply><apply id="S3.SS3.SSS1.p2.1.m1.2.3.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.3"><minus id="S3.SS3.SSS1.p2.1.m1.2.3.3.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.1"></minus><apply id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2"><times id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.1.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.1"></times><ci id="S3.SS3.SSS1.p2.1.m1.2.3.3.2.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.2.2">𝐻</ci><ci id="S3.SS3.SSS1.p2.1.m1.2.2.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.2">𝑥</ci></apply><ci id="S3.SS3.SSS1.p2.1.m1.2.3.3.3.cmml" xref="S3.SS3.SSS1.p2.1.m1.2.3.3.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.1.m1.2c">F(x)=H(x)-x</annotation></semantics></math>, where <math id="S3.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext="F(x)" display="inline"><semantics id="S3.SS3.SSS1.p2.2.m2.1a"><mrow id="S3.SS3.SSS1.p2.2.m2.1.2" xref="S3.SS3.SSS1.p2.2.m2.1.2.cmml"><mi id="S3.SS3.SSS1.p2.2.m2.1.2.2" xref="S3.SS3.SSS1.p2.2.m2.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.2.m2.1.2.1" xref="S3.SS3.SSS1.p2.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS3.SSS1.p2.2.m2.1.2.3.2" xref="S3.SS3.SSS1.p2.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p2.2.m2.1.2.3.2.1" xref="S3.SS3.SSS1.p2.2.m2.1.2.cmml">(</mo><mi id="S3.SS3.SSS1.p2.2.m2.1.1" xref="S3.SS3.SSS1.p2.2.m2.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS3.SSS1.p2.2.m2.1.2.3.2.2" xref="S3.SS3.SSS1.p2.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.2.m2.1b"><apply id="S3.SS3.SSS1.p2.2.m2.1.2.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.2"><times id="S3.SS3.SSS1.p2.2.m2.1.2.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.2.1"></times><ci id="S3.SS3.SSS1.p2.2.m2.1.2.2.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.2.2">𝐹</ci><ci id="S3.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p2.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.2.m2.1c">F(x)</annotation></semantics></math> is the residual mapping to be learned and <math id="S3.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="H(x)" display="inline"><semantics id="S3.SS3.SSS1.p2.3.m3.1a"><mrow id="S3.SS3.SSS1.p2.3.m3.1.2" xref="S3.SS3.SSS1.p2.3.m3.1.2.cmml"><mi id="S3.SS3.SSS1.p2.3.m3.1.2.2" xref="S3.SS3.SSS1.p2.3.m3.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS3.SSS1.p2.3.m3.1.2.1" xref="S3.SS3.SSS1.p2.3.m3.1.2.1.cmml">​</mo><mrow id="S3.SS3.SSS1.p2.3.m3.1.2.3.2" xref="S3.SS3.SSS1.p2.3.m3.1.2.cmml"><mo stretchy="false" id="S3.SS3.SSS1.p2.3.m3.1.2.3.2.1" xref="S3.SS3.SSS1.p2.3.m3.1.2.cmml">(</mo><mi id="S3.SS3.SSS1.p2.3.m3.1.1" xref="S3.SS3.SSS1.p2.3.m3.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS3.SSS1.p2.3.m3.1.2.3.2.2" xref="S3.SS3.SSS1.p2.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p2.3.m3.1b"><apply id="S3.SS3.SSS1.p2.3.m3.1.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.2"><times id="S3.SS3.SSS1.p2.3.m3.1.2.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.2.1"></times><ci id="S3.SS3.SSS1.p2.3.m3.1.2.2.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.2.2">𝐻</ci><ci id="S3.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p2.3.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p2.3.m3.1c">H(x)</annotation></semantics></math> is the original mapping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. The residual connections enable the backpropagation of gradients all the way through the network, mitigating the vanishing gradient problem commonly encountered in deep networks. In terms of spatial feature learning, the residual layers allow ResNets to learn more complex representations by combining both local and global contextual information from different layers, thereby enhancing the network’s ability to recognize intricate patterns in images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Vision Transformer (ViT)</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">Vision Transformers (ViTs) leverage the Transformer architecture, originally designed for natural language processing tasks, for computer vision applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Unlike CNNs, ViTs divide an image into patches and process them in parallel through the Transformer layers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>. This approach allows for more global feature extraction, making ViTs particularly effective for complex vision tasks.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">ViTs are a novel class of models that adapt the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>, originally designed for natural language processing, to the realm of computer vision. The architecture consists of multiple transformer layers, each comprising multi-head self-attention mechanisms and feed-forward neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The attention mechanism is particularly noteworthy for its ability to capture long-range dependencies in the data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite>. In the context of images, this means that ViTs can learn spatial hierarchies and relationships between distant pixels, thereby capturing more global features. The attention mechanism works by computing a weighted sum of all pixels, where the weights are determined by the similarity between the query, key, and value representations of each pixel. To perform these processes, however, an image must be <span id="S3.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_italic">patchified</span> (i.e., cropped and converted into a series of sequential patches). This is similar to turning an image into several word-embeddings in a sentence. This enables the model to focus on different parts of the image adaptively, providing a more nuanced understanding of the spatial features present. The aforementioned process in ViTs are mathematically described in Equations <a href="#S3.E3" title="In 3.3.2 Vision Transformer (ViT) ‣ 3.3 Vision Foundation Models ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-<a href="#S3.E4" title="In 3.3.2 Vision Transformer (ViT) ‣ 3.3 Vision Foundation Models ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="\text{Patch Embedding: }p_{i}=\text{Conv2D}\left(\text{Linear}(x_{i})\right)" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mrow id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mtext id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2a.cmml">Patch Embedding: </mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.3.1" xref="S3.E3.m1.1.1.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml"><mi id="S3.E3.m1.1.1.3.3.2" xref="S3.E3.m1.1.1.3.3.2.cmml">p</mi><mi id="S3.E3.m1.1.1.3.3.3" xref="S3.E3.m1.1.1.3.3.3.cmml">i</mi></msub></mrow><mo id="S3.E3.m1.1.1.2" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3a.cmml">Conv2D</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml"><mtext id="S3.E3.m1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.3a.cmml">Linear</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><times id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.3.2a.cmml" xref="S3.E3.m1.1.1.3.2"><mtext id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">Patch Embedding: </mtext></ci><apply id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.3.3.2">𝑝</ci><ci id="S3.E3.m1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.3.3.3">𝑖</ci></apply></apply><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><times id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3">Conv2D</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.2"></times><ci id="S3.E3.m1.1.1.1.1.1.1.3a.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3"><mtext id="S3.E3.m1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.3">Linear</mtext></ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\text{Patch Embedding: }p_{i}=\text{Conv2D}\left(\text{Linear}(x_{i})\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.4" class="ltx_Math" alttext="\text{Global Feature: }Z=\text{Transformer}(p_{1},p_{2},\ldots,p_{n})" display="block"><semantics id="S3.E4.m1.4a"><mrow id="S3.E4.m1.4.4" xref="S3.E4.m1.4.4.cmml"><mrow id="S3.E4.m1.4.4.5" xref="S3.E4.m1.4.4.5.cmml"><mtext id="S3.E4.m1.4.4.5.2" xref="S3.E4.m1.4.4.5.2a.cmml">Global Feature: </mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.5.1" xref="S3.E4.m1.4.4.5.1.cmml">​</mo><mi id="S3.E4.m1.4.4.5.3" xref="S3.E4.m1.4.4.5.3.cmml">Z</mi></mrow><mo id="S3.E4.m1.4.4.4" xref="S3.E4.m1.4.4.4.cmml">=</mo><mrow id="S3.E4.m1.4.4.3" xref="S3.E4.m1.4.4.3.cmml"><mtext id="S3.E4.m1.4.4.3.5" xref="S3.E4.m1.4.4.3.5a.cmml">Transformer</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.4.4.3.4" xref="S3.E4.m1.4.4.3.4.cmml">​</mo><mrow id="S3.E4.m1.4.4.3.3.3" xref="S3.E4.m1.4.4.3.3.4.cmml"><mo stretchy="false" id="S3.E4.m1.4.4.3.3.3.4" xref="S3.E4.m1.4.4.3.3.4.cmml">(</mo><msub id="S3.E4.m1.2.2.1.1.1.1" xref="S3.E4.m1.2.2.1.1.1.1.cmml"><mi id="S3.E4.m1.2.2.1.1.1.1.2" xref="S3.E4.m1.2.2.1.1.1.1.2.cmml">p</mi><mn id="S3.E4.m1.2.2.1.1.1.1.3" xref="S3.E4.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E4.m1.4.4.3.3.3.5" xref="S3.E4.m1.4.4.3.3.4.cmml">,</mo><msub id="S3.E4.m1.3.3.2.2.2.2" xref="S3.E4.m1.3.3.2.2.2.2.cmml"><mi id="S3.E4.m1.3.3.2.2.2.2.2" xref="S3.E4.m1.3.3.2.2.2.2.2.cmml">p</mi><mn id="S3.E4.m1.3.3.2.2.2.2.3" xref="S3.E4.m1.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.E4.m1.4.4.3.3.3.6" xref="S3.E4.m1.4.4.3.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml">…</mi><mo id="S3.E4.m1.4.4.3.3.3.7" xref="S3.E4.m1.4.4.3.3.4.cmml">,</mo><msub id="S3.E4.m1.4.4.3.3.3.3" xref="S3.E4.m1.4.4.3.3.3.3.cmml"><mi id="S3.E4.m1.4.4.3.3.3.3.2" xref="S3.E4.m1.4.4.3.3.3.3.2.cmml">p</mi><mi id="S3.E4.m1.4.4.3.3.3.3.3" xref="S3.E4.m1.4.4.3.3.3.3.3.cmml">n</mi></msub><mo stretchy="false" id="S3.E4.m1.4.4.3.3.3.8" xref="S3.E4.m1.4.4.3.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.4b"><apply id="S3.E4.m1.4.4.cmml" xref="S3.E4.m1.4.4"><eq id="S3.E4.m1.4.4.4.cmml" xref="S3.E4.m1.4.4.4"></eq><apply id="S3.E4.m1.4.4.5.cmml" xref="S3.E4.m1.4.4.5"><times id="S3.E4.m1.4.4.5.1.cmml" xref="S3.E4.m1.4.4.5.1"></times><ci id="S3.E4.m1.4.4.5.2a.cmml" xref="S3.E4.m1.4.4.5.2"><mtext id="S3.E4.m1.4.4.5.2.cmml" xref="S3.E4.m1.4.4.5.2">Global Feature: </mtext></ci><ci id="S3.E4.m1.4.4.5.3.cmml" xref="S3.E4.m1.4.4.5.3">𝑍</ci></apply><apply id="S3.E4.m1.4.4.3.cmml" xref="S3.E4.m1.4.4.3"><times id="S3.E4.m1.4.4.3.4.cmml" xref="S3.E4.m1.4.4.3.4"></times><ci id="S3.E4.m1.4.4.3.5a.cmml" xref="S3.E4.m1.4.4.3.5"><mtext id="S3.E4.m1.4.4.3.5.cmml" xref="S3.E4.m1.4.4.3.5">Transformer</mtext></ci><vector id="S3.E4.m1.4.4.3.3.4.cmml" xref="S3.E4.m1.4.4.3.3.3"><apply id="S3.E4.m1.2.2.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.1.1.1.1.1.cmml" xref="S3.E4.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.2.2.1.1.1.1.2.cmml" xref="S3.E4.m1.2.2.1.1.1.1.2">𝑝</ci><cn type="integer" id="S3.E4.m1.2.2.1.1.1.1.3.cmml" xref="S3.E4.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="S3.E4.m1.3.3.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4.m1.3.3.2.2.2.2.1.cmml" xref="S3.E4.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S3.E4.m1.3.3.2.2.2.2.2.cmml" xref="S3.E4.m1.3.3.2.2.2.2.2">𝑝</ci><cn type="integer" id="S3.E4.m1.3.3.2.2.2.2.3.cmml" xref="S3.E4.m1.3.3.2.2.2.2.3">2</cn></apply><ci id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1">…</ci><apply id="S3.E4.m1.4.4.3.3.3.3.cmml" xref="S3.E4.m1.4.4.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4.m1.4.4.3.3.3.3.1.cmml" xref="S3.E4.m1.4.4.3.3.3.3">subscript</csymbol><ci id="S3.E4.m1.4.4.3.3.3.3.2.cmml" xref="S3.E4.m1.4.4.3.3.3.3.2">𝑝</ci><ci id="S3.E4.m1.4.4.3.3.3.3.3.cmml" xref="S3.E4.m1.4.4.3.3.3.3.3">𝑛</ci></apply></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.4c">\text{Global Feature: }Z=\text{Transformer}(p_{1},p_{2},\ldots,p_{n})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">This section delineates the comprehensive methodology employed in our research to investigate the utility of synthetic data and pre-trained vision foundation models for multi-task learning in in-vehicle facial attribute recognition. The section is organized into several subsections, each focusing on a critical aspect of our research pipeline. We begin with the preprocessing steps (Section <a href="#S4.SS1" title="4.1 Preprocessing ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>), detailing the procedures for data cleaning, normalization, and preparation. Subsequent subsections delve into the architecture of the multi-task models (Section <a href="#S4.SS2" title="4.2 Multi-Task Model Architecture ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), the adaptation techniques used for transfer learning (Section <a href="#S4.SS3" title="4.3 Adaptation ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>), and the specific training and evaluation strategies (Section <a href="#S4.SS4" title="4.4 Training and Evaluation ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>). The section aims to provide a thorough understanding of the experimental setup, thereby enabling replicability and further exploration.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Preprocessing</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">In our research, we utilized two separate preprocessing pipelines, where the efficacy and performance of each will be rigorously evaluated in the results section. The first pipeline utilizes the original full images captured by in-cabin cameras. The second pipeline, on the other hand, extracts the face of the driver/passengers and uses face-only images for training the models. To extract the human faces in the second pipeline, we tested two prevalent face-extraction pre-trained models: (1) OpenCV’s Internal Haar Cascade pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> and (2) LightFace’s RetinaFace pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite>. Our experiments indicate that the RetinaFace model significantly outperforms the Haar Cascade model across various lighting conditions, head positions, head sizes, and face-background color contrasts. As such, our presented results for face-only data in Section <a href="#S5" title="5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> utilize the RetinaFace model. Both preprocessing pipelines (i.e., full-image data and face-only data) share the following steps:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Normalization</span>: Normalization is used to adjust the data distribution to match that of the pre-trained models. Both ResNet and ViT foundation models employed in our work are trained on the ImageNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite>. Therefore, the distribution of our data is normalized to fit that of the ImageNet dataset.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Downsampling and Resizing</span>: Downsampling and Resizing of images are used to fit the input shape of the pre-trained models. For instance, the pre-trained ViT b_16 model works with input images of size <math id="S4.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml"><mn id="S4.I1.i2.p1.1.m1.1.1.2" xref="S4.I1.i2.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.I1.i2.p1.1.m1.1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.I1.i2.p1.1.m1.1.1.3" xref="S4.I1.i2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><apply id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1"><times id="S4.I1.i2.p1.1.m1.1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.I1.i2.p1.1.m1.1.1.2.cmml" xref="S4.I1.i2.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S4.I1.i2.p1.1.m1.1.1.3.cmml" xref="S4.I1.i2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">224\times 224</annotation></semantics></math>.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.2" class="ltx_p"><span id="S4.I1.i3.p1.2.1" class="ltx_text ltx_font_bold">Scaling</span>: Data scaling to transform the RGB range from <math id="S4.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="0-255" display="inline"><semantics id="S4.I1.i3.p1.1.m1.1a"><mrow id="S4.I1.i3.p1.1.m1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.cmml"><mn id="S4.I1.i3.p1.1.m1.1.1.2" xref="S4.I1.i3.p1.1.m1.1.1.2.cmml">0</mn><mo id="S4.I1.i3.p1.1.m1.1.1.1" xref="S4.I1.i3.p1.1.m1.1.1.1.cmml">−</mo><mn id="S4.I1.i3.p1.1.m1.1.1.3" xref="S4.I1.i3.p1.1.m1.1.1.3.cmml">255</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.1.m1.1b"><apply id="S4.I1.i3.p1.1.m1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1"><minus id="S4.I1.i3.p1.1.m1.1.1.1.cmml" xref="S4.I1.i3.p1.1.m1.1.1.1"></minus><cn type="integer" id="S4.I1.i3.p1.1.m1.1.1.2.cmml" xref="S4.I1.i3.p1.1.m1.1.1.2">0</cn><cn type="integer" id="S4.I1.i3.p1.1.m1.1.1.3.cmml" xref="S4.I1.i3.p1.1.m1.1.1.3">255</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.1.m1.1c">0-255</annotation></semantics></math> to <math id="S4.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="0-1" display="inline"><semantics id="S4.I1.i3.p1.2.m2.1a"><mrow id="S4.I1.i3.p1.2.m2.1.1" xref="S4.I1.i3.p1.2.m2.1.1.cmml"><mn id="S4.I1.i3.p1.2.m2.1.1.2" xref="S4.I1.i3.p1.2.m2.1.1.2.cmml">0</mn><mo id="S4.I1.i3.p1.2.m2.1.1.1" xref="S4.I1.i3.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.I1.i3.p1.2.m2.1.1.3" xref="S4.I1.i3.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i3.p1.2.m2.1b"><apply id="S4.I1.i3.p1.2.m2.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1"><minus id="S4.I1.i3.p1.2.m2.1.1.1.cmml" xref="S4.I1.i3.p1.2.m2.1.1.1"></minus><cn type="integer" id="S4.I1.i3.p1.2.m2.1.1.2.cmml" xref="S4.I1.i3.p1.2.m2.1.1.2">0</cn><cn type="integer" id="S4.I1.i3.p1.2.m2.1.1.3.cmml" xref="S4.I1.i3.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i3.p1.2.m2.1c">0-1</annotation></semantics></math>. This is a common preprocessing step in computer vision models to help achieve numerical stability, faster Convergence, and improved generalization.</p>
</div>
</li>
<li id="S4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S4.I1.i4.p1" class="ltx_para">
<p id="S4.I1.i4.p1.1" class="ltx_p"><span id="S4.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Shuffling</span>: Shuffling of the dataset to introduce randomness and improve generalization.</p>
</div>
</li>
<li id="S4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S4.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.i5.p1.2" class="ltx_p"><span id="S4.I1.i5.p1.2.1" class="ltx_text ltx_font_bold">Splitting</span>: Train-test splitting, allocating <math id="S4.I1.i5.p1.1.m1.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S4.I1.i5.p1.1.m1.1a"><mn id="S4.I1.i5.p1.1.m1.1.1" xref="S4.I1.i5.p1.1.m1.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.1.m1.1b"><cn type="integer" id="S4.I1.i5.p1.1.m1.1.1.cmml" xref="S4.I1.i5.p1.1.m1.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.1.m1.1c">70</annotation></semantics></math>% of the data for training and <math id="S4.I1.i5.p1.2.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.I1.i5.p1.2.m2.1a"><mn id="S4.I1.i5.p1.2.m2.1.1" xref="S4.I1.i5.p1.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i5.p1.2.m2.1b"><cn type="integer" id="S4.I1.i5.p1.2.m2.1.1.cmml" xref="S4.I1.i5.p1.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i5.p1.2.m2.1c">30</annotation></semantics></math>% for testing.</p>
</div>
</li>
</ol>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p">We did not employ any data augmentation techniques. Instead, we conducted experiments on each synthetic dataset separately and also on a combined dataset comprising all three synthetic datasets, i.e., SynthA, SynthB, and SynthC (see Section <a href="#S5.SS1" title="5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>). Additionally, specific preprocessing steps were required for each synthetic dataset, as described below. Further details regarding each dataset, provided labels, and categories are provided in Section <a href="#S5.SS1" title="5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S4.I2.i1.p1.1" class="ltx_p">In the <span id="S4.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">SynthA</span> dataset, the age labels are provided as a numerical value, which is ambiguous and unhelpful for age recognition task. As such, the numerical age labels were transformed into age ranges as follows: <span id="S4.I2.i1.p1.1.2" class="ltx_text ltx_font_typewriter">{‘0-3’, ‘4-12’, ‘13-18’, ‘19-30’, ‘31-50’, ‘50+’}</span>.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S4.I2.i2.p1.1" class="ltx_p">In the <span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">SynthB</span> dataset some images, some labels were missing, despite a human driver was present in the respective image (and vice versa), and therefore, an additional cleaning step was necessary to remove data with problematic labels or images.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="S4.I2.i3.p1.1" class="ltx_p">For the <span id="S4.I2.i3.p1.1.1" class="ltx_text ltx_font_italic">SynthC</span> dataset, we re-annotated the gaze labels, reducing the number of originally provided gaze planes from 17 to seven based on our specific application-dependent interests.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multi-Task Model Architecture</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.2" class="ltx_p">Our multi-task model architecture for facial attribute recognition is shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Multi-Task Model Architecture ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We employ two distinct vision foundation models as the backbone for our multi-task learning architecture: the Vision Transformer (ViT) and the Residual Network (ResNet). Both architectures share a common preprocessing block that performs the operations outlined in the previous subsection <a href="#S4.SS1" title="4.1 Preprocessing ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. However, the ViT model incorporates an additional <span id="S4.SS2.p1.2.1" class="ltx_text ltx_font_italic">Patchification</span> block (i.e., cropping and converting an image into a series of sequential patches). As shown in Equations <a href="#S3.E3" title="In 3.3.2 Vision Transformer (ViT) ‣ 3.3 Vision Foundation Models ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>-<a href="#S3.E4" title="In 3.3.2 Vision Transformer (ViT) ‣ 3.3 Vision Foundation Models ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the patchification block employs a Conv2D layer to transform a <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">224\times 224</annotation></semantics></math> image into a series of <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="16\times 16" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mn id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">16</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><times id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">16</cn><cn type="integer" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">16\times 16</annotation></semantics></math> patches. These patches are subsequently flattened and stacked, serving as the input to the transformer layers in the ViT model. This patchification step is crucial for adapting image data into a format that the transformer architecture can effectively process.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For the ResNet model, the patchification step is omitted, and the output from the preprocessing block directly feeds into the pre-trained model (Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Multi-Task Model Architecture ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We utilized the ViT b_16 and ResNet 18 variants for our experiments. The ViT b_16 model consists of 12 transformer layers, each with multi-head self-attention mechanisms and feed-forward neural networks, as detailed in Subsection <a href="#S3.SS3.SSS2" title="3.3.2 Vision Transformer (ViT) ‣ 3.3 Vision Foundation Models ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>. On the other hand, the ResNet 18 model is composed of 18 layers, including multiple residual blocks that leverage skip connections to facilitate the training of deeper networks, as described in Subsection <a href="#S3.SS3.SSS1" title="3.3.1 Residual Network (ResNet) Model ‣ 3.3 Vision Foundation Models ‣ 3 Background ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>. Each model offers unique advantages: the ViT is renowned for its superior performance in complex vision tasks, while the ResNet provides a more lightweight architecture that is well-suited for edge deployments.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.06088/assets/architecture.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="336" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Our employed multi-task facial attribute recognition architecture via transfer learning from pre-trained vision foundation models. Our research employs two distinct vision foundation models as the backbone for our multi-task learning architecture: the Vision Transformer (ViT) and the Residual Network (ResNet). Both architectures share a common preprocessing block. The patchification block is only applied for the ViT model and is crucial for adapting image data into a format that the transformer architecture can effectively process. The output from these pre-trained models, which represents a high-dimensional feature space, is then directed into three separate task heads. The final layer in each separate task head is a linear layer with number of neurons equal to the number of classes for the respective task, outputting a probability vector for each prediction.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p">As shown in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Multi-Task Model Architecture ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the output from these pre-trained models, which represents a high-dimensional feature space, is then directed into three separate task heads. Each task head is responsible for one of the facial attributes we aim to predict: (1) gaze plane, (2) age, and (3) facial expression. These task heads are structurally identical, consisting of multiple fully-connected layers, dropout layers for regularization, and ReLU activation functions. The final layer in each task head is a linear layer with number of neurons equal to the number of classes for the respective task, outputting a probability vector for each prediction.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.1" class="ltx_p">The architecture is designed to be highly modular, particularly at the task heads. This modularity allows for easy adaptability and experimentation with different tasks without requiring changes to the underlying foundation model. It also facilitates the incorporation of additional tasks in the future, thereby enhancing the model’s utility across a broader range of applications. Full schematic representation of the architecture is provided in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Multi-Task Model Architecture ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> for further clarity.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Adaptation</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.1" class="ltx_p">In order to adapt the pre-trained vision foundation models to our specific multi-task learning problem, we employed three distinct adaptation methods: linear probing, prefix tuning, and full fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>. Each of these methods offers a unique approach to leveraging the learned feature spaces of the pre-trained models for our specific tasks, and they come with their own sets of advantages and disadvantages.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Linear Probing (LP):</span> In this method, the pre-trained vision foundation models are kept frozen during the training process, serving solely as feature extractors. The high-dimensional feature space generated by these models is then fed into the task heads, which are the only components updated via gradient descent. This approach is computationally efficient and straightforward, but its performance is highly dependent on the quality of the pre-trained model. It assumes that the feature space learned during pre-training is sufficiently expressive for the downstream tasks, which may not always be the case <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Prefix Tuning (PT):</span> This method unfreezes a small portion of the pre-trained vision foundation model, allowing it to be updated alongside the task heads during training. For the ViT model, we unfroze the last encoder layer, while for the ResNet model, the last residual block was made trainable. This selective fine-tuning enables the feature space to adapt more closely to the specific tasks at hand. Prefix tuning strikes a balance between computational efficiency and performance, often outperforming linear probing, especially when the dataset size is moderate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.p4.1" class="ltx_p"><span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_bold">Full Fine-Tuning (FFT):</span> In this approach, the entire architecture, including the pre-trained vision foundation model, is updated via gradient descent. The pre-trained models essentially serve as an initialization point or a "warm-start" which facilitates faster convergence. This method is the most computationally intensive but also the most effective when abundant data and computational resources are available. It allows the model to fully adapt to the specific tasks, often resulting in the highest performance metrics among the three methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.p5.1" class="ltx_p">Each of these adaptation methods has its own trade-offs. LP is computationally less demanding but may suffer from performance limitations. PT offers a middle ground, being more adaptive than linear probing while being less resource-intensive than full fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>. FFT, although computationally expensive, often yields the best performance, making it the method of choice when resources are not a constraint. A comparative illustration of these adaptation methods is provided in Figure X for further clarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib150" title="" class="ltx_ref">150</a>, <a href="#bib.bib128" title="" class="ltx_ref">128</a>, <a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite>.</p>
</div>
<div id="S4.SS3.p6" class="ltx_para ltx_noindent">
<p id="S4.SS3.p6.1" class="ltx_p">The efficacy of these adaptation methods, i.e., LP, PT, and FFT, is not solely determined by computational resources or the volume of available data; it is also intricately linked to the data distribution and the complexity of the downstream tasks. For instance, FFT may not always be the optimal choice for out-of-distribution (OOD) data, especially when the pre-trained features are of high quality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite>. In such cases, fine-tuning the entire model can lead to a degradation in performance due to distributional shifts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>]</cite>. On the other hand, the complexity of the downstream tasks also plays a crucial role in the selection of the adaptation method. For relatively simpler tasks, extensive fine-tuning, as in FFT, can result in overfitting, thereby negatively impacting the model’s generalization capabilities. Therefore, the choice of adaptation strategy should be made judiciously, taking into account not just the available resources and data, but also the specific characteristics of the tasks and the data distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib151" title="" class="ltx_ref">151</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Training and Evaluation</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.1" class="ltx_p">The training and evaluation of our multi-task model were conducted in a unified framework, leveraging a composite loss function and multiple evaluation metrics. The optimization process was designed to be both robust and efficient, incorporating various techniques to ensure the model’s performance across different tasks.</p>
</div>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Loss Function</h5>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS0.Px1.p1.4" class="ltx_p">The loss function is a critical component in training neural networks, and in our case, it is a composite function that combines the Cross Entropy Loss (CEL) for each of the three task heads, i.e., gaze, age, and facial expression. Mathematically, the loss function <math id="S4.SS4.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px1.p1.1.m1.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.1.m1.1b"><ci id="S4.SS4.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.1.m1.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.1.m1.1c">\mathcal{L}</annotation></semantics></math> can be represented as in Equation <a href="#S4.E5" title="In Loss Function ‣ 4.4 Training and Evaluation ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, where <math id="S4.SS4.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.2.m2.1a"><msub id="S4.SS4.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1.2.cmml">y</mi><mi id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1.2">𝑦</ci><ci id="S4.SS4.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS0.Px1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.2.m2.1c">y_{i}</annotation></semantics></math> and <math id="S4.SS4.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\hat{y}_{i}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.3.m3.1a"><msub id="S4.SS4.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.cmml"><mover accent="true" id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml"><mi id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.2" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.2.cmml">y</mi><mo id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.1" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><apply id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2"><ci id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.1.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.1">^</ci><ci id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.2.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.2.2">𝑦</ci></apply><ci id="S4.SS4.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS0.Px1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.3.m3.1c">\hat{y}_{i}</annotation></semantics></math> are the true and predicted labels for each task, and <math id="S4.SS4.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\lambda_{i}" display="inline"><semantics id="S4.SS4.SSS0.Px1.p1.4.m4.1a"><msub id="S4.SS4.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.2" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1.2.cmml">λ</mi><mi id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.3" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px1.p1.4.m4.1b"><apply id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1.2">𝜆</ci><ci id="S4.SS4.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S4.SS4.SSS0.Px1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px1.p1.4.m4.1c">\lambda_{i}</annotation></semantics></math> is the weight decay for L2 regularization specific to each task head.:</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.1" class="ltx_Math" alttext="\mathcal{L}=\sum_{i=1}^{3}\left(\text{CrossEntropy}(y_{i},\hat{y}_{i})+\lambda_{i}\cdot\text{L2 regularization term}\right)" display="block"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml">ℒ</mi><mo rspace="0.111em" id="S4.E5.m1.1.1.2" xref="S4.E5.m1.1.1.2.cmml">=</mo><mrow id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.cmml"><munderover id="S4.E5.m1.1.1.1.2" xref="S4.E5.m1.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E5.m1.1.1.1.2.2.2" xref="S4.E5.m1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S4.E5.m1.1.1.1.2.2.3" xref="S4.E5.m1.1.1.1.2.2.3.cmml"><mi id="S4.E5.m1.1.1.1.2.2.3.2" xref="S4.E5.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E5.m1.1.1.1.2.2.3.1" xref="S4.E5.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S4.E5.m1.1.1.1.2.2.3.3" xref="S4.E5.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mn id="S4.E5.m1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.2.3.cmml">3</mn></munderover><mrow id="S4.E5.m1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mo id="S4.E5.m1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.2.cmml"><mtext id="S4.E5.m1.1.1.1.1.1.1.2.4" xref="S4.E5.m1.1.1.1.1.1.1.2.4a.cmml">CrossEntropy</mtext><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.1.1.2.3.cmml">​</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.2.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.3" xref="S4.E5.m1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">y</mi><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S4.E5.m1.1.1.1.1.1.1.2.2.2.4" xref="S4.E5.m1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mover accent="true" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.2" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.2.cmml">y</mi><mo id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.1" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.1.cmml">^</mo></mover><mi id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.3" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.5" xref="S4.E5.m1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.3.cmml">+</mo><mrow id="S4.E5.m1.1.1.1.1.1.1.4" xref="S4.E5.m1.1.1.1.1.1.1.4.cmml"><msub id="S4.E5.m1.1.1.1.1.1.1.4.2" xref="S4.E5.m1.1.1.1.1.1.1.4.2.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.4.2.2" xref="S4.E5.m1.1.1.1.1.1.1.4.2.2.cmml">λ</mi><mi id="S4.E5.m1.1.1.1.1.1.1.4.2.3" xref="S4.E5.m1.1.1.1.1.1.1.4.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E5.m1.1.1.1.1.1.1.4.1" xref="S4.E5.m1.1.1.1.1.1.1.4.1.cmml">⋅</mo><mtext id="S4.E5.m1.1.1.1.1.1.1.4.3" xref="S4.E5.m1.1.1.1.1.1.1.4.3a.cmml">L2 regularization term</mtext></mrow></mrow><mo id="S4.E5.m1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><eq id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1.2"></eq><ci id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3">ℒ</ci><apply id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"><apply id="S4.E5.m1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.2.1.cmml" xref="S4.E5.m1.1.1.1.2">superscript</csymbol><apply id="S4.E5.m1.1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.2.2.1.cmml" xref="S4.E5.m1.1.1.1.2">subscript</csymbol><sum id="S4.E5.m1.1.1.1.2.2.2.cmml" xref="S4.E5.m1.1.1.1.2.2.2"></sum><apply id="S4.E5.m1.1.1.1.2.2.3.cmml" xref="S4.E5.m1.1.1.1.2.2.3"><eq id="S4.E5.m1.1.1.1.2.2.3.1.cmml" xref="S4.E5.m1.1.1.1.2.2.3.1"></eq><ci id="S4.E5.m1.1.1.1.2.2.3.2.cmml" xref="S4.E5.m1.1.1.1.2.2.3.2">𝑖</ci><cn type="integer" id="S4.E5.m1.1.1.1.2.2.3.3.cmml" xref="S4.E5.m1.1.1.1.2.2.3.3">1</cn></apply></apply><cn type="integer" id="S4.E5.m1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.2.3">3</cn></apply><apply id="S4.E5.m1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1"><plus id="S4.E5.m1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.3"></plus><apply id="S4.E5.m1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2"><times id="S4.E5.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.3"></times><ci id="S4.E5.m1.1.1.1.1.1.1.2.4a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.4"><mtext id="S4.E5.m1.1.1.1.1.1.1.2.4.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.4">CrossEntropy</mtext></ci><interval closure="open" id="S4.E5.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2"><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.2">𝑦</ci><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><apply id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2"><ci id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.1">^</ci><ci id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.2.2">𝑦</ci></apply><ci id="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply><apply id="S4.E5.m1.1.1.1.1.1.1.4.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4"><ci id="S4.E5.m1.1.1.1.1.1.1.4.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.1">⋅</ci><apply id="S4.E5.m1.1.1.1.1.1.1.4.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.4.2.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.2">subscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.4.2.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.2.2">𝜆</ci><ci id="S4.E5.m1.1.1.1.1.1.1.4.2.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.2.3">𝑖</ci></apply><ci id="S4.E5.m1.1.1.1.1.1.1.4.3a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.3"><mtext id="S4.E5.m1.1.1.1.1.1.1.4.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.4.3">L2 regularization term</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\mathcal{L}=\sum_{i=1}^{3}\left(\text{CrossEntropy}(y_{i},\hat{y}_{i})+\lambda_{i}\cdot\text{L2 regularization term}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Optimization</h5>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">For the optimization process, we employed the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite>, a widely-used optimization algorithm that combines the advantages of both AdaGrad <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="" class="ltx_ref">154</a>]</cite> and RMSProp <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>. Adam is particularly effective in handling non-convex optimization landscapes, a common challenge in multi-task learning. Its adaptive learning rate capabilities make it a robust choice for our complex model architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref">153</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Metrics</h5>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">Both loss and accuracy metrics were used for evaluation. Specifically, we monitored the total loss and accuracy, as well as task-specific losses and accuracies. The model was trained and evaluated in the same loop over a batch of input data, iterating through multiple epochs. The model with the lowest evaluation loss was saved for inference, obviating the need for early stopping.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Training Techniques</h5>

<div id="S4.SS4.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.SSS0.Px4.p1.2" class="ltx_p">Several specific training techniques were employed to enhance the model’s performance. First, we used learning rate schedules, starting with an initial learning rate of <math id="S4.SS4.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="lr=1\times 10^{-3}" display="inline"><semantics id="S4.SS4.SSS0.Px4.p1.1.m1.1a"><mrow id="S4.SS4.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.cmml"><mrow id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.cmml"><mi id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.1.cmml">​</mo><mi id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3.cmml">r</mi></mrow><mo id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.cmml"><mn id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.1" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.1.cmml">×</mo><msup id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.cmml"><mn id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.2.cmml">10</mn><mrow id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.cmml"><mo id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3a" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.cmml">−</mo><mn id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.2" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.2.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1"><eq id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.1"></eq><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2"><times id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.1"></times><ci id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.2">𝑙</ci><ci id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.2.3">𝑟</ci></apply><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3"><times id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.1"></times><cn type="integer" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.2">1</cn><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3">superscript</csymbol><cn type="integer" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.2">10</cn><apply id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3"><minus id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.1.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3"></minus><cn type="integer" id="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.2.cmml" xref="S4.SS4.SSS0.Px4.p1.1.m1.1.1.3.3.3.2">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.1.m1.1c">lr=1\times 10^{-3}</annotation></semantics></math> and decaying it by a factor of 0.5, with a minimum allowed learning rate of <math id="S4.SS4.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="1\times 10^{-6}" display="inline"><semantics id="S4.SS4.SSS0.Px4.p1.2.m2.1a"><mrow id="S4.SS4.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.cmml"><mn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.cmml"><mo id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3a" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.2" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.2.cmml">6</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px4.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1"><times id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.2">1</cn><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.2.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.2">10</cn><apply id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3"><minus id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS4.SSS0.Px4.p1.2.m2.1.1.3.3.2">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px4.p1.2.m2.1c">1\times 10^{-6}</annotation></semantics></math>. Second, we used empirically chosen loss weights for each task to balance their contributions to the overall loss. Lastly, we experimented with curriculum learning, where task losses were added incrementally based on their evaluation loss. Although this technique did not make it into all our experiments, its impact will be discussed as part of an ablation study in the results Section <a href="#S5" title="5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Empirical Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this comprehensive section, we present an empirical evaluation of our multi-task learning model, focusing on its performance across various synthetic datasets and under different experimental conditions. We initiate the discussion with an in-depth examination of the synthetic datasets used for training and validation (Section <a href="#S5.SS1" title="5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>). Following this, we delve into the model’s performance on these individual datasets as well as a combined dataset, providing learning curves and task-specific metrics for both ViT and ResNet architectures (Section <a href="#S5.SS2" title="5.2 Model Performances ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>). Ablation studies are then presented to assess the impact of different adaptation methods and curriculum learning strategies (Section <a href="#S5.SS3" title="5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>). We also include a section dedicated to the model’s inference performance on out-of-distribution (OOD) data (Section <a href="#S5.SS4" title="5.4 Out-of-Distribution Inference Results ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.4</span></a>). Finally, we offer a post-evaluation study, analyzing the data distributions alongside achieved model performances, and provide insights into the observed results (Section <a href="#S5.SS6" title="5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.6</span></a>). This section aims to provide a thorough understanding of the model’s capabilities and limitations when paired with synthetic data.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2403.06088/assets/SynthesisAI_sample_faces_retina.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Image samples from the SynthA dataset. The RetinaFace pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> has been applied to extract the face bounding box (i.e., green box) of the driver.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Synthetic Datasets</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">In this subsection, we provide an in-depth analysis of the synthetic datasets employed in our study. These datasets were generated by different sources and are designed to simulate various real-world scenarios involving in-cabin cameras capturing drivers and passengers. We focus on three primary datasets: SynthA, SynthB, and SynthC <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span id="footnote2.1" class="ltx_text" style="color:#FF0000;">Please note that er replaced the real names of the synthetic data generation companies due to proprietary an confidential information. However, to achieve reproducibility, we intend to release anonymized samples of each dataset in a public Github which will accompany the manuscript upon acceptance.</span></span></span></span>. Each dataset has its unique characteristics, which we outline below.</p>
</div>
<section id="S5.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SynthA:</h5>

<div id="S5.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS0.Px1.p1.1" class="ltx_p">The SynthA dataset consists of 10,000 images captured by an in-cabin camera focusing on a driver in various driving scenarios, such as different lighting conditions and driver positions. The images in this dataset are frames extracted from short videos, which results in relatively low diversity compared to the other datasets. Samples of the SynthA dataset are demonstrated in Figure <a href="#S5.F4" title="Figure 4 ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section id="S5.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SynthB:</h5>

<div id="S5.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px2.p1.1" class="ltx_p">The SynthB dataset is more extensive, containing 2,999 images (post-cleaning). These images capture multiple passengers and a driver in a wide array of scenarios, including different lighting conditions and positions. The dataset is highly diverse, featuring drivers and passengers of various races and ages with different head- or face-wears. Samples of the SynthB dataset are demonstrated in Figure <a href="#S5.F5" title="Figure 5 ‣ SynthB: ‣ 5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2403.06088/assets/Anyverse_sample_faces_retina.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Image samples from the SynthB dataset. The RetinaFace pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> has been applied to extract the face bounding box (i.e., green box) of the driver.</figcaption>
</figure>
</section>
<section id="S5.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">SynthC:</h5>

<div id="S5.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS0.Px3.p1.1" class="ltx_p">The SynthC dataset is comprised of 1,920 images featuring humans in diverse settings, including both indoor and outdoor environments, during different times of the day. Unlike the other datasets, the subjects in these images are not situated in a vehicle. While the diversity is higher than that of SynthA, it is not as extensive as SynthB. Samples of the SynthC dataset are demonstrated in Figure <a href="#S5.F6" title="Figure 6 ‣ SynthC: ‣ 5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2403.06088/assets/Datagen_sample_faces_retina.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Image samples from the SynthC dataset. The RetinaFace pre-trained model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>, <a href="#bib.bib148" title="" class="ltx_ref">148</a>]</cite> has been applied to extract the face bounding box (i.e., green box) of the driver.</figcaption>
</figure>
</section>
<section id="S5.SS1.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Label Annotations</h5>

<div id="S5.SS1.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS0.Px4.p1.1" class="ltx_p">Each dataset comes with a rich set of labels and annotations. However, for the scope of this study, we are particularly interested in the labels related to facial attributes. These labels are categorized as follows:</p>
</div>
<div id="S5.SS1.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS0.Px4.p2.1" class="ltx_p"><span id="S5.SS1.SSS0.Px4.p2.1.1" class="ltx_text ltx_font_bold">Gaze Labels:</span></p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: 5 planes = <span id="S5.I1.i1.p1.1.2" class="ltx_text ltx_font_typewriter">{‘HUD’, ‘instrument_cluster’, ‘center_console’, ‘driver_front_windshield’, 
<br class="ltx_break">‘middle_front_windshield’}</span></p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: 5 planes = <span id="S5.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">{‘infotainment’, ‘ext_mirror’, ‘int_mirror’, ‘rear’, ‘road’}</span></p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i3.p1.1" class="ltx_p"><span id="S5.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: 17 planes, re-annotated to 7 desired planes = <span id="S5.I1.i3.p1.1.2" class="ltx_text ltx_font_typewriter">{‘center_stack_area’, ‘road_area’, ‘up’, ‘down’, ‘passenger_side’, ‘driver_side’, ‘rearview_mirror’}</span></p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.SSS0.Px4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS0.Px4.p3.1" class="ltx_p"><span id="S5.SS1.SSS0.Px4.p3.1.1" class="ltx_text ltx_font_bold">Age Labels:</span></p>
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i1.p1.1" class="ltx_p"><span id="S5.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: Numerical values, re-annotated to 4 categories = <span id="S5.I2.i1.p1.1.2" class="ltx_text ltx_font_typewriter">{‘13-18’, ‘19-30’, ‘31-50’, ‘50+’}</span></p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i2.p1.1" class="ltx_p"><span id="S5.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: 6 categories = <span id="S5.I2.i2.p1.1.2" class="ltx_text ltx_font_typewriter">{‘0-3’, ‘4-12’, ‘13-18’, ‘19-30’, ‘31-50’, ‘50+’}</span></p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I2.i3.p1.1" class="ltx_p"><span id="S5.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: 3 categories = <span id="S5.I2.i3.p1.1.2" class="ltx_text ltx_font_typewriter">{‘young’, ‘adult’, ‘old’}</span></p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.SSS0.Px4.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS0.Px4.p4.1" class="ltx_p"><span id="S5.SS1.SSS0.Px4.p4.1.1" class="ltx_text ltx_font_bold">Facial Expression Labels:</span></p>
<ul id="S5.I3" class="ltx_itemize">
<li id="S5.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I3.i1.p1.1" class="ltx_p"><span id="S5.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: 3 categories = <span id="S5.I3.i1.p1.1.2" class="ltx_text ltx_font_typewriter">{‘eyes_closed’, ‘compressed’, ‘none’}</span></p>
</div>
</li>
<li id="S5.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I3.i2.p1.1" class="ltx_p"><span id="S5.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: 6 categories = <span id="S5.I3.i2.p1.1.2" class="ltx_text ltx_font_typewriter">{‘happy’, ‘surprised’, ‘angry’, ‘random’, ‘neutral’, ‘sad’}</span></p>
</div>
</li>
<li id="S5.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I3.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I3.i3.p1.1" class="ltx_p"><span id="S5.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: 9 categories = <span id="S5.I3.i3.p1.1.2" class="ltx_text ltx_font_typewriter">{‘happiness’, ‘disgust’, ‘contempt’, ‘fear’, ‘surprise’, ‘none’, ‘anger’, ‘mouth_open’, ‘sadness’}</span></p>
</div>
</li>
</ul>
</div>
<div id="S5.SS1.SSS0.Px4.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.SSS0.Px4.p5.1" class="ltx_p">These labels serve as the ground truth for training and evaluating our multi-task learning model. The diversity and richness of these labels across different datasets provide a robust platform for assessing the model’s performance in various real-world-like scenarios.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2403.06088/assets/faceonly_resnet_all.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="355" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Performance of the pre-trained ResNet vision foundation model on the employed synthetic face-only datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC) for in-vehicle multi-task facial attribute recognition. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the FFT adaptation method.</figcaption>
</figure>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Model Performances</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this subsection, we delve into the performance metrics of the multi-task learning models built on pre-trained Vision Transformer (ViT) and ResNet architectures. We present comprehensive results on the face-only (Section <a href="#S4.SS1" title="4.1 Preprocessing ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>) synthetic data, including learning curves for both training and evaluation phases. These curves encapsulate total loss, accuracy, and task-specific metrics for each synthetic dataset and for both vision foundation model architectures for in-vehicle multi-task facial attribute recognition.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2403.06088/assets/faceonly_vit_all.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="350" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Performance of the pre-trained Vision Transformer (ViT) vision foundation model on the employed synthetic face-only datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC) for in-vehicle multi-task facial attribute recognition. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the prefix tuning adaptation method.</figcaption>
</figure>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Performance on Individual Datasets</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">Figure <a href="#S5.F7" title="Figure 7 ‣ Label Annotations ‣ 5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the performance of the pre-trained ResNet vision foundation model on the employed synthetic face-only datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC) for in-vehicle multi-task facial attribute recognition. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the FFT adaptation method. As our results indicate, the pre-trained ResNet model excels on the SynthA dataset, achieving near-optimal in-distribution performance. We attribute this high level of performance to the dataset’s large volume and relatively low diversity compared to the other datasets used. In contrast, the SynthC dataset, which contains only 1,920 samples—a number generally considered insufficient for computer vision tasks—still yields impressive results when using the pre-trained ResNet model, outperforming the SynthB dataset. It’s worth noting that the facial expression recognition task on the SynthC dataset involves classifying among nine different categories, adding complexity to an already challenging task due to the limited dataset size. Despite these constraints and the data-intensive nature of multi-task learning, our findings underscore the efficacy of transfer learning and the robustness of pre-trained foundation models in such scenarios.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 Model Performances ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the performance of the pre-trained Vision Transformer (ViT) vision foundation model on the employed synthetic face-only datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC) for in-vehicle multi-task facial attribute recognition. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the prefix tuning adaptation method. As evidenced by our results, the pre-trained ViT model also attains near-optimal performance on the SynthA dataset (in-distribution), mirroring the performance trend observed with the ResNet model. However, a comparative analysis of Figures <a href="#S5.F7" title="Figure 7 ‣ Label Annotations ‣ 5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S5.F8" title="Figure 8 ‣ 5.2 Model Performances ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> reveals that the ResNet model outperforms the ViT model in several cases. Specifically, ResNet demonstrates superior task accuracies, enhanced data efficiency, and faster convergence rates, particularly when applied to the SynthC dataset. This result is particularly surprising given the recent successes of ViT architectures in various vision tasks. We hypothesize that the feature space learned by ViT may be overly complex for our specific multi-task problem, whereas the feature space of ResNet appears to be more amenable to the tasks at hand.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Performance on Combined Dataset</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">For the combined dataset, we utilized the face-only data extracted from the original datasets. This resulted in a dataset comprising 14,919 total samples and a more diverse data distribution. Note that, given the disparate label annotations across the original datasets, we re-annotated the labels to create a unified set. The re-annotated labels are as follows:</p>
<ul id="S5.I4" class="ltx_itemize">
<li id="S5.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I4.i1.p1" class="ltx_para">
<p id="S5.I4.i1.p1.1" class="ltx_p"><span id="S5.I4.i1.p1.1.1" class="ltx_text ltx_font_bold">Gaze Labels</span>: 6 planes <span id="S5.I4.i1.p1.1.2" class="ltx_text ltx_font_typewriter">{‘infotainment’, ‘ext_mirror’, ‘int_mirror’, ‘rear’, ‘road’, ‘passenger’}</span></p>
</div>
</li>
<li id="S5.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I4.i2.p1" class="ltx_para">
<p id="S5.I4.i2.p1.1" class="ltx_p"><span id="S5.I4.i2.p1.1.1" class="ltx_text ltx_font_bold">Age Labels</span>: 3 categories <span id="S5.I4.i2.p1.1.2" class="ltx_text ltx_font_typewriter">{‘teen’, ‘adult’, ‘elderly’}</span></p>
</div>
</li>
<li id="S5.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I4.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I4.i3.p1.1" class="ltx_p"><span id="S5.I4.i3.p1.1.1" class="ltx_text ltx_font_bold">Facial Expression Labels</span>: 5 categories <span id="S5.I4.i3.p1.1.2" class="ltx_text ltx_font_typewriter">{‘happy’, ‘surprised’, ‘frown’, ‘neutral’, ‘sad’}</span></p>
</div>
</li>
</ul>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2403.06088/assets/combined_results_all.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Figures (a) and (b) show the performance of the pre-trained Vision Transformer (ViT) and ResNet vision foundation models on the combined face-only datasets, respectively, for in-vehicle multi-task facial attribute recognition. In each figure, the left and right plot demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs.</figcaption>
</figure>
<div id="S5.SS2.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p">Figures (a) and (b) in Fig. <a href="#S5.F9" title="Figure 9 ‣ 5.2.2 Performance on Combined Dataset ‣ 5.2 Model Performances ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> show the performance of the pre-trained ViT and ResNet foundation models on the combined face-only datasets, respectively, for in-vehicle multi-task facial attribute recognition. In each figure, the left and right plot demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. As illustrated in our findings, both the pre-trained ResNet and ViT models exhibit significant performance improvements when trained on the aggregated synthetic dataset, relative to their performance on individual datasets as discussed in Section <a href="#S5.SS2.SSS1" title="5.2.1 Performance on Individual Datasets ‣ 5.2 Model Performances ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>. This uptick in performance can be attributed to the augmented data volume and a more diverse data distribution, which collectively contribute to a more robust and generalizable model compared to when trained on individual datasets. Additionally, once again, it is noteworthy that the pre-trained ResNet model consistently outperforms the ViT model, achieving not only higher accuracy rates but also more rapid convergence. Moreover, the ViT model exhibits a tendency to overfit (increasing validation loss), despite the implementation of several regularization techniques (e.g., loss regularization and dropout layers).</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.p1.1" class="ltx_p">In this section, we present a comprehensive ablation study to dissect the performance of our multi-task models. We focus on two primary aspects: the comparison between full-image and face-only data combinations, and the impact of curriculum learning on improving model performance.</p>
</div>
<section id="S5.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Study 1: Comparing Foundation Models and Preprocessing Steps</h4>

<div id="S5.SS3.SSS1.p1" class="ltx_para">
<p id="S5.SS3.SSS1.p1.1" class="ltx_p">In this ablation study, we aim to rigorously evaluate the performance of the two foundation models used in our study across different preprocessing conditions. Specifically, we compare the efficacy of these models on the full-image individual datasets against the face-only individual datasets (presented in Figures <a href="#S5.F7" title="Figure 7 ‣ Label Annotations ‣ 5.1 Synthetic Datasets ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S5.F8" title="Figure 8 ‣ 5.2 Model Performances ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>). The latter involves an additional face-extraction preprocessing step, as described in Section <a href="#S4.SS1" title="4.1 Preprocessing ‣ 4 Methodology ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. This comparison serves to elucidate the impact of preprocessing techniques alongside the foundation model choice on the multi-task facial attribute recognition tasks, thereby providing insights into the optimal configurations for in-vehicle perception systems.</p>
</div>
<figure id="S5.F10" class="ltx_figure"><img src="/html/2403.06088/assets/fullimage_resnet_all.png" id="S5.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Performance of the pre-trained ResNet vision foundation model on the employed full-image (i.e., no face extraction in preprocessing) synthetic datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC) for in-vehicle multi-task facial attribute recognition. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the FFT adaptation method.</figcaption>
</figure>
<div id="S5.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS1.p2.1" class="ltx_p">Figures <a href="#S5.F10" title="Figure 10 ‣ 5.3.1 Study 1: Comparing Foundation Models and Preprocessing Steps ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> and <a href="#S5.F11" title="Figure 11 ‣ 5.3.1 Study 1: Comparing Foundation Models and Preprocessing Steps ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> demonstrate the performances of the pre-trained ResNet and ViT foundation models on the employed full-image synthetic datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC), respectively. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the FFT adaptation method. In summary, our results indicate that models trained on face-only data generally outperform those trained on full-image data. This suggests that focusing on the region of interest (i.e., the face) can lead to more accurate and reliable predictions. Additionally, we found that the ResNet foundation model, when fully fine-tuned, generally outperforms the ViT model with prefix tuning. This is noteworthy as full fine-tuning of ViT models is computationally expensive and often impractical.</p>
</div>
<figure id="S5.F11" class="ltx_figure"><img src="/html/2403.06088/assets/fullimage_vit_all.png" id="S5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="347" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Performance of the pre-trained sViT vision foundation model on the employed full-image (i.e., no face extraction in preprocessing) synthetic datasets (i.e., (a) SynthA, (b) SynthB, and (c) SynthC) for in-vehicle multi-task facial attribute recognition. Top and bottom row demonstrate the task-specific losses and accuracies, respectively, for both training (shades of red) and validation (shades of blue) over epochs. All results use the PT adaptation method.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>Study 2: Comparing Adaptation Methods</h4>

<div id="S5.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS2.p1.1" class="ltx_p">In this part of the ablation study, we focus on investigating the performance implications of various adaptation methods, i.e., Linear Probing (LP), Prefix Tuning (PT), and Full Fine Tuning (FFT), on each of the three individual synthetic datasets. The objective is to understand how different adaptation strategies affect the model’s ability to generalize and perform well on multi-task facial attribute recognition, particularly when trained on limited synthetic data. This comparative analysis aims to identify the most effective adaptation method for each synthetic dataset, thereby offering valuable insights for model deployment in real-world applications. For this experiment we only use the pre-trained ResNet model as it achieved the best individual and aggregated dataset performances.</p>
</div>
<div id="S5.SS3.SSS2.p2" class="ltx_para">
<p id="S5.SS3.SSS2.p2.1" class="ltx_p">Figure <a href="#S5.F12" title="Figure 12 ‣ 5.3.2 Study 2: Comparing Adaptation Methods ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> presents the task-specific accuracies attained by the pre-trained ResNet model using different adaptation techniques—Linear Probing (LP), Prefix Tuning (PT), and Full Fine Tuning (FFT)—across each of the three individual synthetic datasets. Remarkably, FFT consistently outperforms both LP and PT, a result that is particularly impressive given the limited dataset sizes. This underscores the adaptability and efficacy of pre-trained models in tackling new tasks. As anticipated, LP exhibits the slowest adaptation rate, primarily because only the task-specific heads are fine-tuned, while the foundational layers responsible for feature extraction remain static.</p>
</div>
<figure id="S5.F12" class="ltx_figure"><img src="/html/2403.06088/assets/adaptation_ablations_all.png" id="S5.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="631" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Performance (task-specific accuracies) of various adaptation methods, i.e., Linear Probing (LP), Prefix Tuning (PT), and Full Fine Tuning (FFT), on each of the three individual synthetic datasets.</figcaption>
</figure>
</section>
<section id="S5.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>Ablation Results Summary</h4>

<div id="S5.SS3.SSS3.p1" class="ltx_para">
<p id="S5.SS3.SSS3.p1.1" class="ltx_p">In this section we present two summary tables (Tables <a href="#S5.T2" title="Table 2 ‣ 5.3.3 Ablation Results Summary ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.T1" title="Table 1 ‣ 5.3.3 Ablation Results Summary ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) that encapsulate our overall results and ablation study findings. Tables <a href="#S5.T2" title="Table 2 ‣ 5.3.3 Ablation Results Summary ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S5.T1" title="Table 1 ‣ 5.3.3 Ablation Results Summary ‣ 5.3 Ablation Studies ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> below show the training (T) and validation (V) accuracies for each task (gaze, age, facial expression) and for both face-only and full-image data, respectively.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Ablation Study Results for Face-Only Dataset. The table presents the best training (T) and validation (V) results achieved via the best model.</figcaption>
<div id="S5.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:125.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-62.2pt,18.1pt) scale(0.777134018653533,0.777134018653533) ;">
<table id="S5.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.1.1.1.1" class="ltx_tr">
<th id="S5.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t">Foundation Model</th>
<th id="S5.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Dataset</th>
<th id="S5.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Gaze (T – V)</th>
<th id="S5.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Age (T – V)</th>
<th id="S5.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">Facial Exp. (T – V)</th>
<th id="S5.T1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">Adaptation Method</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.1.1.2.1" class="ltx_tr">
<td id="S5.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt" rowspan="4"><span id="S5.T1.1.1.2.1.1.1" class="ltx_text">ViT</span></td>
<td id="S5.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SynthA</td>
<td id="S5.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 95.0%</td>
<td id="S5.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 98.5%</td>
<td id="S5.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T1.1.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S5.T1.1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.2.1.6.1.1" class="ltx_p" style="width:85.4pt;">Linear Probing</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.3.2" class="ltx_tr">
<td id="S5.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthB</td>
<td id="S5.T1.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 58.5%</td>
<td id="S5.T1.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 69.0%</td>
<td id="S5.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 100% – V: 39.8%</td>
<td id="S5.T1.1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.3.2.5.1.1" class="ltx_p" style="width:85.4pt;">Prefix Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.4.3" class="ltx_tr">
<td id="S5.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthC</td>
<td id="S5.T1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 86.5%</td>
<td id="S5.T1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 76.2%</td>
<td id="S5.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 100% – V: 32.7%</td>
<td id="S5.T1.1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.4.3.5.1.1" class="ltx_p" style="width:85.4pt;">Prefix Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.5.4" class="ltx_tr">
<td id="S5.T1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.5.4.1.1" class="ltx_text ltx_font_bold">Combined</span></td>
<td id="S5.T1.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.5.4.2.1" class="ltx_text ltx_font_bold">T: 100% – V: 85.1%</span></td>
<td id="S5.T1.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T1.1.1.5.4.3.1" class="ltx_text ltx_font_bold">T: 97.5% – V: 94.2%</span></td>
<td id="S5.T1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S5.T1.1.1.5.4.4.1" class="ltx_text ltx_font_bold">T: 99.5% – V: 82.0%</span></td>
<td id="S5.T1.1.1.5.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.1.5.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.5.4.5.1.1" class="ltx_p" style="width:85.4pt;"><span id="S5.T1.1.1.5.4.5.1.1.1" class="ltx_text ltx_font_bold">Prefix Tuning</span></span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.6.5" class="ltx_tr">
<td id="S5.T1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_tt" rowspan="4"><span id="S5.T1.1.1.6.5.1.1" class="ltx_text">ResNet</span></td>
<td id="S5.T1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SynthA</td>
<td id="S5.T1.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 98%</td>
<td id="S5.T1.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T1.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T1.1.1.6.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S5.T1.1.1.6.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.6.5.6.1.1" class="ltx_p" style="width:85.4pt;">Full Fine Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.7.6" class="ltx_tr">
<td id="S5.T1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthB</td>
<td id="S5.T1.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 64.5%</td>
<td id="S5.T1.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 70.0%</td>
<td id="S5.T1.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 100% – V: 43.7%</td>
<td id="S5.T1.1.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.7.6.5.1.1" class="ltx_p" style="width:85.4pt;">Full Fine Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.8.7" class="ltx_tr">
<td id="S5.T1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthC</td>
<td id="S5.T1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 90.0%</td>
<td id="S5.T1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 85.0%</td>
<td id="S5.T1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 100% – V: 34.2%</td>
<td id="S5.T1.1.1.8.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T1.1.1.8.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.8.7.5.1.1" class="ltx_p" style="width:85.4pt;">Full Fine Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T1.1.1.9.8" class="ltx_tr">
<td id="S5.T1.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.1.9.8.1.1" class="ltx_text ltx_font_bold">Combined</span></td>
<td id="S5.T1.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.1.9.8.2.1" class="ltx_text ltx_font_bold">T: 100% – V: 93.1%</span></td>
<td id="S5.T1.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T1.1.1.9.8.3.1" class="ltx_text ltx_font_bold">T: 100% – V: 95.5%</span></td>
<td id="S5.T1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t"><span id="S5.T1.1.1.9.8.4.1" class="ltx_text ltx_font_bold">T: 100% – V: 86.0%</span></td>
<td id="S5.T1.1.1.9.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T1.1.1.9.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T1.1.1.9.8.5.1.1" class="ltx_p" style="width:85.4pt;"><span id="S5.T1.1.1.9.8.5.1.1.1" class="ltx_text ltx_font_bold">Full Fine Tuning</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation Study Results for Full-Image Dataset. The table presents the best training (T) and validation (V) results achieved via the best model.</figcaption>
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:99.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-57.0pt,13.1pt) scale(0.791837854039757,0.791837854039757) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_rr ltx_border_t">Foundation Model</th>
<th id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Dataset</th>
<th id="S5.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Gaze (T – V)</th>
<th id="S5.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Age (T – V)</th>
<th id="S5.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t">Facial Exp. (T – V)</th>
<th id="S5.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">Adaptation Method</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.2.1" class="ltx_tr">
<td id="S5.T2.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_rr ltx_border_tt" rowspan="3"><span id="S5.T2.1.1.2.1.1.1" class="ltx_text">ViT</span></td>
<td id="S5.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SynthA</td>
<td id="S5.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 97.8%</td>
<td id="S5.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T2.1.1.2.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S5.T2.1.1.2.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.2.1.6.1.1" class="ltx_p" style="width:85.4pt;">Linear Probing</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.1.3.2" class="ltx_tr">
<td id="S5.T2.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthB</td>
<td id="S5.T2.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 43.5%</td>
<td id="S5.T2.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 64.2%</td>
<td id="S5.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 100% – V: 25.0%</td>
<td id="S5.T2.1.1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.1.1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.3.2.5.1.1" class="ltx_p" style="width:85.4pt;">Prefix Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.1.4.3" class="ltx_tr">
<td id="S5.T2.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthC</td>
<td id="S5.T2.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 98.1% – V: 61.1%</td>
<td id="S5.T2.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 85.0%</td>
<td id="S5.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 61.0% – V: 19.9%</td>
<td id="S5.T2.1.1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.1.1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.4.3.5.1.1" class="ltx_p" style="width:85.4pt;">Prefix Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.1.5.4" class="ltx_tr">
<td id="S5.T2.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_rr ltx_border_tt" rowspan="3"><span id="S5.T2.1.1.5.4.1.1" class="ltx_text">ResNet</span></td>
<td id="S5.T2.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">SynthA</td>
<td id="S5.T2.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 99%</td>
<td id="S5.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T2.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">T: 100% – V: 100%</td>
<td id="S5.T2.1.1.5.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt">
<span id="S5.T2.1.1.5.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.5.4.6.1.1" class="ltx_p" style="width:85.4pt;">Full Fine Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.1.6.5" class="ltx_tr">
<td id="S5.T2.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SynthB</td>
<td id="S5.T2.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 55.5%</td>
<td id="S5.T2.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">T: 100% – V: 67.2%</td>
<td id="S5.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">T: 100% – V: 26.5%</td>
<td id="S5.T2.1.1.6.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.1.1.6.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.6.5.5.1.1" class="ltx_p" style="width:85.4pt;">Full Fine Tuning</span>
</span>
</td>
</tr>
<tr id="S5.T2.1.1.7.6" class="ltx_tr">
<td id="S5.T2.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">SynthC</td>
<td id="S5.T2.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">T: 100% – V: 87.1%</td>
<td id="S5.T2.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">T: 100% – V: 91.1%</td>
<td id="S5.T2.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">T: 100% – V: 18.5%</td>
<td id="S5.T2.1.1.7.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.1.1.7.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.7.6.5.1.1" class="ltx_p" style="width:85.4pt;">Full Fine Tuning</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4 </span>Study 3: Training Techniques (Curriculum Learning)</h4>

<div id="S5.SS3.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS3.SSS4.p1.1" class="ltx_p">In addition to the above comparisons, we also evaluated the impact of curriculum learning on model performance. In the context of MTL, curriculum learning serves as a strategic framework for model training that aims to improve the efficiency of the learning process. Traditional MTL often involves training a model on multiple tasks simultaneously, treating all tasks as equally important. However, curriculum learning introduces a pedagogical approach, akin to human learning, where tasks are organized in a sequence from simpler to more complex. Our findings suggest that while curriculum learning does not significantly improve the overall performance, it does lead to minor faster convergence.</p>
</div>
</section>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Out-of-Distribution Inference Results</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Out-of-Distribution (OOD) inference serves as a pivotal evaluation metric for assessing the generalization capability of machine learning models, particularly in real-world scenarios where the model is likely to encounter data that diverges from the training set. This form of evaluation is especially crucial for synthetic datasets, which often excel in in-distribution performance but may falter when exposed to real-world, diverse data. In this subsection, we present an exhaustive analysis of the OOD performance of our best-performing models, i.e., multi-task facial attribute recognition model built on pre-trained ResNet foundation model, trained on aggregated face-only synthetic data and adapted via the FFT adaptation technique. specifically focusing on their ability to generalize to the Kaggle UTKFace dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>]</cite>, a dataset that is significantly different in nature from the synthetic datasets used for training.</p>
</div>
<figure id="S5.F13" class="ltx_figure"><img src="/html/2403.06088/assets/OOD_Test.png" id="S5.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Out-of-Distribution (OOD) inference performance of our best model (i.e., multi-task facial attribute recognition model built on pre-trained ResNet foundation model, trained on aggregated face-only synthetic data and adapted via the FFT adaptation technique) obtained from the Kaggle UTKFace dataset (only ground truth age labels are provided by the dataset). Empirically, this model achieves an average accuracy of approximately 50% across tasks on the assessed OOD data. These results underscore the remarkable potential of leveraging existing foundation models and synthetic data to construct robust and capable models.</figcaption>
</figure>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.p2.1" class="ltx_p">OOD inference was rigorously performed on the Kaggle UTKFace dataset, a dataset that was intentionally kept separate and was not seen by the model during any phase of the training process. This approach allows us to critically evaluate how well the model can generalize to new, unseen data, thereby providing a robust measure of its real-world applicability. We performed several OOD experiments specific to each of the datasets separately in order to characterize model performance with respect to data distribution and volume. In summary:</p>
<ol id="S5.I5" class="ltx_enumerate">
<li id="S5.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I5.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i1.p1.1" class="ltx_p"><span id="S5.I5.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: This dataset proved to be “too synthetic" for real-world applications. The models trained on this dataset, while achieving remarkable near-optimal performance on in-distribution data, performed very poorly on OOD tasks, averaging less than 10% accuracy (empirically) across all tasks. This suggests that while the dataset may be suitable for in-distribution tasks, it is not robust enough for real-world applications.</p>
</div>
</li>
<li id="S5.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I5.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i2.p1.1" class="ltx_p"><span id="S5.I5.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: Models trained on this dataset performed better in OOD tasks compared to those trained on SynthA. However, the overall performance was still noticeably suboptimal, likely due to the simultaneous low sample size and wide data distribution.</p>
</div>
</li>
<li id="S5.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I5.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I5.i3.p1.1" class="ltx_p"><span id="S5.I5.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: Despite having the lowest sample size (1920 samples), models trained on this dataset outperformed the others in OOD tasks, averaging around 35% accuracy (empirically). This suggests that SynthC provides a more balanced and robust training set for real-world applications.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS4.p3" class="ltx_para ltx_noindent">
<p id="S5.SS4.p3.1" class="ltx_p">Finally, we also performed an OOD experiment on the models trained via the aggregated dataset. Combining the three datasets increases data diversity and data count which in theory should improve the performance in OOD inference. This is exactly what we observe in practice. Figure <a href="#S5.F13" title="Figure 13 ‣ 5.4 Out-of-Distribution Inference Results ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> shows the OOD inference performance of our best model (i.e., multi-task facial attribute recognition model built on pre-trained ResNet foundation model, trained on aggregated face-only synthetic data and adapted via the FFT adaptation technique) obtained from the Kaggle UTKFace dataset (only ground truth age labels are provided by the dataset). Empirically, this model achieves an average accuracy of approximately 50% across tasks on the assessed OOD data. These results underscore the remarkable potential of leveraging existing foundation models and synthetic data to construct robust and capable models.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para ltx_noindent">
<p id="S5.SS4.p4.1" class="ltx_p">Nevertheless, to shed further light onto these OOD performances, a post-evaluation data distribution analysis is necessary (Section <a href="#S5.SS6" title="5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.6</span></a>). For instance, we hypothesize that the aggregated dataset has a relatively bad label distribution given the re-annotation process that was required to merge the data samples. This bad distribution is almost unavoidable due to different nature of the three datasets.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Discussion</h3>

<div id="S5.SS5.p1" class="ltx_para ltx_noindent">
<p id="S5.SS5.p1.1" class="ltx_p">The synthetic nature of the data used for training has profound implications on the model’s OOD performance, a critical factor for the deployment of machine learning models in real-world scenarios. Synthetic data, while highly controlled and easily customizable, often lacks the inherent variability and noise present in real-world data. This discrepancy can lead to models that perform exceptionally well on in-distribution data but falter when faced with real-world, diverse data (e.e., the SynthA dataset). Therefore, for synthetic data to be genuinely useful in real-world applications, it must be designed to be as diverse as possible, capturing a wide range of scenarios, conditions, and edge cases. Introducing controlled noise into the synthetic data can also be beneficial, as it can simulate the kind of data irregularities that a model is likely to encounter in a real-world setting.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para ltx_noindent">
<p id="S5.SS5.p2.1" class="ltx_p">In light of these challenges, the use of advanced techniques becomes indispensable for improving OOD performance. In this study, we employed a variety of such techniques, including data augmentation, fine-tuning and regularization methods, as well as transfer learning, adaptation, and robust evaluation metrics, to ensure that the models are not just memorizing the training data but are learning to generalize across different distributions. These advanced techniques serve as a compensatory mechanism for the limitations of synthetic data, enabling the model to bridge the gap between synthetic and real-world data. By doing so, we created multi-task models that are not only high-performing but also robust and reliable when deployed in real-world applications, thereby addressing one of the most significant challenges in the utilization of synthetic data for machine learning.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Post-Evaluation Analysis: Data Distributions and Similarity</h3>

<div id="S5.SS6.p1" class="ltx_para ltx_noindent">
<p id="S5.SS6.p1.1" class="ltx_p">In this subsection, we delve into the data distributions of the synthetic datasets used for training the models. We investigate both image and label distributions to understand the characteristics of each dataset. This analysis aims to shed light on the observed model performances (in-distribution and out-of-distribution) and to provide insights into the limitations and potentials of each dataset.</p>
</div>
<section id="S5.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.1 </span>Visualizing Data Distributions and Similarity Metrics</h4>

<div id="S5.SS6.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS6.SSS1.p1.1" class="ltx_p">We begin our analysis by presenting the data distributions in a structured manner. To achieve this, we first flatten the images, transforming them from multi-dimensional arrays into one-dimensional vectors. This step is crucial for simplifying the computational complexity involved in similarity calculations. We then compute two distinct metrics of similarity: Euclidean and Cosine similarities. These metrics serve different purposes; while Euclidean similarity measures the <span id="S5.SS6.SSS1.p1.1.1" class="ltx_text ltx_font_italic">straight-line</span> distance between two points in the feature space, Cosine similarity assesses the angle between two vectors, providing insights into their directional relationship. To make these similarity measures comparable and to facilitate their interpretation, we then normalize these similarity matrices.</p>
</div>
<div id="S5.SS6.SSS1.p2" class="ltx_para">
<p id="S5.SS6.SSS1.p2.1" class="ltx_p">To further visualize the data distributions, we employ t-SNE (t-Distributed Stochastic Neighbor Embedding) for dimensionality reduction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref">157</a>]</cite>. Each image, now represented as a flattened vector, is mapped to a point in a 2D or 3D space. These scatter plots are then color-coded based on the previously computed and normalized similarity matrices, providing a multi-faceted view of data relationships. It is worth noting that t-SNE is sensitive to the scale of the data. This sensitivity necessitates appropriate data preprocessing steps, such as normalization or standardization, to ensure that the t-SNE algorithm accurately captures the underlying data structure. Figures <a href="#S5.F14.sf1" title="In Figure 14 ‣ 5.6.1 Visualizing Data Distributions and Similarity Metrics ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(a)</span></a>-<a href="#S5.F14.sf3" title="In Figure 14 ‣ 5.6.1 Visualizing Data Distributions and Similarity Metrics ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(c)</span></a> present the data distribution (equal portions) of the three employed synthetic datasets. Each point in space represents an RGB image where the distances and color-maps are computed via t-SNE and the similarity metrics as described above. Refer to Section <a href="#S5.SS6.SSS2" title="5.6.2 Understanding and Analyzing the Data Distribution Scatter Plots ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.6.2</span></a> for a road-map regarding understanding and interpreting the scatter plots. The visualizations serve as a qualitative tool for exploring the relationships between images in the datasets. By examining clusters, outliers, and color mapping, and by comparing different similarity measures, we can gain valuable insights into the underlying structure of the data. These insights can guide further quantitative analysis, feature engineering, or model selection.</p>
</div>
<figure id="S5.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.06088/assets/Synthesis_data_dist_1000.png" id="S5.F14.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="499" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>SynthA Data Distribution</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.06088/assets/Anyverse_data_distribution.png" id="S5.F14.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="499" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>SynthB Data Distribution</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S5.F14.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2403.06088/assets/Datagen_data_distribution.png" id="S5.F14.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="499" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>SynthC Data Distribution</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Data distribution (equal portions) of the three employed synthetic datasets. Each point in space represents an RGB image. Distances are computed via t-SNE algorithm and the color-maps represent two similarity metrics: (1) Euclidean similarity and (2) Cosine similarity. Both 2D and 3D representations are provided for better clarity. Refer to Section <a href="#S5.SS6.SSS2" title="5.6.2 Understanding and Analyzing the Data Distribution Scatter Plots ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.6.2</span></a> for details regarding understanding and interpreting the scatter plots.</figcaption>
</figure>
</section>
<section id="S5.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.2 </span>Understanding and Analyzing the Data Distribution Scatter Plots</h4>

<div id="S5.SS6.SSS2.p1" class="ltx_para ltx_noindent">
<ul id="S5.I6" class="ltx_itemize">
<li id="S5.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I6.i1.p1" class="ltx_para">
<p id="S5.I6.i1.p1.1" class="ltx_p"><span id="S5.I6.i1.p1.1.1" class="ltx_text ltx_font_bold">Axes (<math id="S5.I6.i1.p1.1.1.m1.3" class="ltx_Math" alttext="x,y,z" display="inline"><semantics id="S5.I6.i1.p1.1.1.m1.3a"><mrow id="S5.I6.i1.p1.1.1.m1.3.4.2" xref="S5.I6.i1.p1.1.1.m1.3.4.1.cmml"><mi id="S5.I6.i1.p1.1.1.m1.1.1" xref="S5.I6.i1.p1.1.1.m1.1.1.cmml">x</mi><mo id="S5.I6.i1.p1.1.1.m1.3.4.2.1" xref="S5.I6.i1.p1.1.1.m1.3.4.1.cmml">,</mo><mi id="S5.I6.i1.p1.1.1.m1.2.2" xref="S5.I6.i1.p1.1.1.m1.2.2.cmml">y</mi><mo id="S5.I6.i1.p1.1.1.m1.3.4.2.2" xref="S5.I6.i1.p1.1.1.m1.3.4.1.cmml">,</mo><mi id="S5.I6.i1.p1.1.1.m1.3.3" xref="S5.I6.i1.p1.1.1.m1.3.3.cmml">z</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.I6.i1.p1.1.1.m1.3b"><list id="S5.I6.i1.p1.1.1.m1.3.4.1.cmml" xref="S5.I6.i1.p1.1.1.m1.3.4.2"><ci id="S5.I6.i1.p1.1.1.m1.1.1.cmml" xref="S5.I6.i1.p1.1.1.m1.1.1">𝑥</ci><ci id="S5.I6.i1.p1.1.1.m1.2.2.cmml" xref="S5.I6.i1.p1.1.1.m1.2.2">𝑦</ci><ci id="S5.I6.i1.p1.1.1.m1.3.3.cmml" xref="S5.I6.i1.p1.1.1.m1.3.3">𝑧</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S5.I6.i1.p1.1.1.m1.3c">x,y,z</annotation></semantics></math>):</span> The axes represent the three main components obtained from the t-SNE algorithm. These components capture the most significant patterns in the data.</p>
</div>
</li>
<li id="S5.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I6.i2.p1" class="ltx_para">
<p id="S5.I6.i2.p1.1" class="ltx_p"><span id="S5.I6.i2.p1.1.1" class="ltx_text ltx_font_bold">Color Mapping:</span> Colors represent either Euclidean or Cosine similarity. Lower Euclidean values and higher Cosine values indicate greater similarity.</p>
</div>
</li>
<li id="S5.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I6.i3.p1" class="ltx_para">
<p id="S5.I6.i3.p1.1" class="ltx_p"><span id="S5.I6.i3.p1.1.1" class="ltx_text ltx_font_bold">Cluster Identification and Color Interpretation:</span> By looking for clusters of points in the 2D/3D spaces, we identify groups of similar images. We examine the color coding in conjunction with the spatial positioning and clustering behavior of the points.</p>
</div>
</li>
<li id="S5.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I6.i4.p1" class="ltx_para">
<p id="S5.I6.i4.p1.1" class="ltx_p"><span id="S5.I6.i4.p1.1.1" class="ltx_text ltx_font_bold">Compare Euclidean vs. Cosine Similarity:</span> Differences between these plots could provide insights into the nature of the images.</p>
</div>
</li>
<li id="S5.I6.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I6.i5.p1" class="ltx_para ltx_noindent">
<p id="S5.I6.i5.p1.1" class="ltx_p"><span id="S5.I6.i5.p1.1.1" class="ltx_text ltx_font_bold">Outlier Detection:</span> Points far away from others could represent unique or anomalous images.</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS6.SSS2.p2" class="ltx_para">
<p id="S5.SS6.SSS2.p2.1" class="ltx_p">For label distributions, we count the number of labels for each task and represent this distribution as pie charts. This visualization helps in quickly grasping the balance or imbalance in the dataset for each task, which is essential for understanding model performance later on. Figure <a href="#S5.F15" title="Figure 15 ‣ 5.6.2 Understanding and Analyzing the Data Distribution Scatter Plots ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> shows the label distributions across all three employed synthetic datastes, i.e., SynthA, SynthB, and SynthC, as well as the combined dataset. See Section  <a href="#S5.SS6.SSS3" title="5.6.3 Summary of Data and Label Distribution Observations ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.6.3</span></a> for a summary of the data and label distribution observations.</p>
</div>
<figure id="S5.F15" class="ltx_figure"><img src="/html/2403.06088/assets/x2.png" id="S5.F15.g1" class="ltx_graphics ltx_centering ltx_img_square" width="664" height="678" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Label distributions across all three employed synthetic datastes, i.e., SynthA, SynthB, and SynthC, as well as the combined dataset.</figcaption>
</figure>
</section>
<section id="S5.SS6.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.3 </span>Summary of Data and Label Distribution Observations</h4>

<section id="S5.SS6.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data Distribution Observations:</h5>

<div id="S5.SS6.SSS3.Px1.p1" class="ltx_para ltx_noindent">
<ol id="S5.I7" class="ltx_enumerate">
<li id="S5.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I7.i1.p1" class="ltx_para">
<p id="S5.I7.i1.p1.1" class="ltx_p"><span id="S5.I7.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: Exhibits clear clustering behavior according to t-SNE, with each cluster having a distinct color (similarity metric).</p>
</div>
</li>
<li id="S5.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I7.i2.p1" class="ltx_para">
<p id="S5.I7.i2.p1.1" class="ltx_p"><span id="S5.I7.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: Displays minor clustering and is highly dispersed in the feature space. The colors do not align with the clusters and vary significantly across the data distribution.</p>
</div>
</li>
<li id="S5.I7.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I7.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I7.i3.p1.1" class="ltx_p"><span id="S5.I7.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: Similar to SynthB in terms of minor clustering but is less dispersed. The colors are generally similar for all data samples, unlike SynthB.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS6.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Label Distribution Observations:</h5>

<div id="S5.SS6.SSS3.Px2.p1" class="ltx_para ltx_noindent">
<ol id="S5.I8" class="ltx_enumerate">
<li id="S5.I8.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I8.i1.p1" class="ltx_para">
<p id="S5.I8.i1.p1.1" class="ltx_p"><span id="S5.I8.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: The label distribution in this dataset is an example of a bad and unrealistic distribution for building a generalisable model. In two out of the three tasks (gaze and age), not only the label distribution is non-uniform, but also, more than half of the data have a similar label. This makes the high in-distribution performance of the model “fake", as the model can only predict one label all the time and achieve 50% or more accuracy.</p>
</div>
</li>
<li id="S5.I8.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I8.i2.p1" class="ltx_para">
<p id="S5.I8.i2.p1.1" class="ltx_p"><span id="S5.I8.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: Although the label distributions in this dataset is more uniform compared to SynthA, we still observe a large portion of the data labeled as <span id="S5.I8.i2.p1.1.2" class="ltx_text ltx_font_typewriter">‘19-30’</span> (i.e., 68%) for the age classification task.</p>
</div>
</li>
<li id="S5.I8.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I8.i3.p1" class="ltx_para">
<p id="S5.I8.i3.p1.1" class="ltx_p"><span id="S5.I8.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: This dataset has the most uniform label distribution among the three synthetic datasets. Also, note that this dataset has most categories in two of the downstream classification tasks (gaze and facial expression).</p>
</div>
</li>
<li id="S5.I8.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I8.i4.p1" class="ltx_para ltx_noindent">
<p id="S5.I8.i4.p1.1" class="ltx_p"><span id="S5.I8.i4.p1.1.1" class="ltx_text ltx_font_bold">Combined</span>: The aggregated dataset has a also a bad label distribution given the re-annotation process that was required to merge the data samples. This bad distribution is almost unavoidable due to different nature of the three datasets.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S5.SS6.SSS3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Connections and Reasoning</h5>

<div id="S5.SS6.SSS3.Px3.p1" class="ltx_para ltx_noindent">
<ol id="S5.I9" class="ltx_enumerate">
<li id="S5.I9.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I9.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I9.i1.p1.1" class="ltx_p"><span id="S5.I9.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA</span>: The clear clustering behavior in SynthA likely contributes to its excellent in-distribution performance. The model can easily learn the distinct clusters, leading to high accuracy. However, this also makes the model less generalizable, explaining its poor out-of-distribution performance.</p>
</div>
</li>
<li id="S5.I9.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I9.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I9.i2.p1.1" class="ltx_p"><span id="S5.I9.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB</span>: The lack of clear clustering and the high dispersion in the feature space make it challenging for the model to learn meaningful patterns, leading to mediocre in-distribution performance. The varying similarity metrics across the data distribution further complicate the learning process, resulting in poor out-of-distribution performance.</p>
</div>
</li>
<li id="S5.I9.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I9.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I9.i3.p1.1" class="ltx_p"><span id="S5.I9.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC</span>: The minor clustering and less dispersion in SynthC make it easier for the model to generalize, which is reflected in its decent in-distribution and out-of-distribution performance. The more uniform similarity metrics across the data samples likely aid in this generalization.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS6.SSS3.Px3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS6.SSS3.Px3.p2.1" class="ltx_p">In summary, the data distribution characteristics directly influence the model’s ability to learn and generalize, which is evident from the performance observations on the three datasets.</p>
</div>
</section>
</section>
<section id="S5.SS6.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.6.4 </span>Summary of Post-Evaluation Analysis Findings</h4>

<div id="S5.SS6.SSS4.p1" class="ltx_para ltx_noindent">
<p id="S5.SS6.SSS4.p1.1" class="ltx_p">In this section, we distill the essential findings from our extensive experiments and post-evaluation analyses. These key findings serve as a concise summary that encapsulates the strengths and weaknesses of each dataset in terms of their utility for training robust machine learning models, particularly in the context of out-of-distribution performance.</p>
<ol id="S5.I10" class="ltx_enumerate">
<li id="S5.I10.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I10.i1.p1" class="ltx_para ltx_noindent">
<p id="S5.I10.i1.p1.1" class="ltx_p"><span id="S5.I10.i1.p1.1.1" class="ltx_text ltx_font_bold">SynthA:</span> This dataset is almost “<span id="S5.I10.i1.p1.1.2" class="ltx_text ltx_font_italic">too synthetic</span>", which is evident from its near-perfect in-distribution performance. However, this perfection becomes its Achilles’ heel when it comes to OOD performance. The models trained on this dataset struggle to generalize to real-world data, as evidenced by their poor OOD performance. The synthetic nature of the data seems to lack the diversity and noise inherent in real-world scenarios (Figure <a href="#S5.F14.sf1" title="In Figure 14 ‣ 5.6.1 Visualizing Data Distributions and Similarity Metrics ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14(a)</span></a>), making it less suitable for training models that need to operate effectively in less controlled environments.</p>
</div>
</li>
<li id="S5.I10.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I10.i2.p1" class="ltx_para ltx_noindent">
<p id="S5.I10.i2.p1.1" class="ltx_p"><span id="S5.I10.i2.p1.1.1" class="ltx_text ltx_font_bold">SynthB:</span> This dataset stands out for its wide distribution, encompassing a broad range of features and characteristics. However, its utility is significantly hampered by its low sample size. Given that machine learning models, especially multi-task foundation models like the ones we are working with, are data-hungry, the low sample size becomes a limiting factor. The wide distribution coupled with a low sample size results in a dataset that is not representative enough for training robust models. This leads to poor generalization and suboptimal performance in both in-distribution and OOD scenarios.</p>
</div>
</li>
<li id="S5.I10.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I10.i3.p1" class="ltx_para ltx_noindent">
<p id="S5.I10.i3.p1.1" class="ltx_p"><span id="S5.I10.i3.p1.1.1" class="ltx_text ltx_font_bold">SynthC:</span> Among the datasets examined, SynthC offers a more balanced distribution of features. It strikes a middle ground between the overly synthetic nature of SynthA and the wide but sparse distribution of SynthB (the more uniform similarity metrics across the data samples). This balance makes it a promising candidate for training more robust models that can generalize well to unseen data. While its sample size is still very low, the quality and distribution of the data make up for it to some extent, as evidenced by its relatively better in-distribution and OOD performance.</p>
</div>
</li>
<li id="S5.I10.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I10.i4.p1" class="ltx_para ltx_noindent">
<p id="S5.I10.i4.p1.1" class="ltx_p"><span id="S5.I10.i4.p1.1.1" class="ltx_text ltx_font_bold">Combined:</span> Combining the three datasets increases data diversity and data count which in theory and practice improved the performance for both in-distribution data and OOD inference. This OOD results was observed despite the inevitable bad label distribution in aggregated data, which highlights the importance of data diversity and high data counts. To make the OOD performance even better using the combined dataset, we would need to increase counts for labels that have a low count (Figure <a href="#S5.F15" title="Figure 15 ‣ 5.6.2 Understanding and Analyzing the Data Distribution Scatter Plots ‣ 5.6 Post-Evaluation Analysis: Data Distributions and Similarity ‣ 5 Empirical Evaluation ‣ Towards In-Vehicle Multi-Task Facial Attribute Recognition: Investigating Synthetic Data and Vision Foundation Models" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>).</p>
</div>
</li>
</ol>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this study, we have rigorously evaluated the performance of Vision Transformer (ViT) and ResNet foundation models on multi-task vision problems using synthetic datasets. We particularly focused on building robust and generalisable perception models in-vehicle multi-task facial attribute recognition. Our comprehensive experiments encompassed various aspects such as model architectures, data types, adaptation methods, and out-of-distribution performance analysis. Our best performing model was comprised of a multi-task facial attribute recognition model built on pre-trained ResNet foundation model, trained on aggregated face-only synthetic data and adapted via the FFT adaptation technique. Our results indicated the impressive capability of foundation models and transfer learning in learning multiple complex perception tasks, even when train on limited amount of synthetic data. The findings of this research demonstrate a great potential in benefiting from synthetic data where models were able to learn complex computer vision tasks within a few training epochs via employing transfer learning from existing vision foundation models. The results also revealed that while synthetic data can be highly effective for in-distribution tasks, they pose challenges for generalization to real-world and out-of-distribution scenarios. We also delved into the data distributions of the synthetic datasets, employing both image and label distributions to understand their characteristics. Through visualizations and quantitative analyses, we gained valuable insights into the limitations and potentials of each dataset.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Key Research Takeaways:</h3>

<div id="S6.SS1.p1" class="ltx_para ltx_noindent">
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">In-Vehicle Perception and Intelligence:</span> The significance of in-vehicle perception and intelligence cannot be overstated, especially when it comes to multi-task facial attribute recognition. This technology is pivotal for enhancing the safety and personalized experience of passengers, thereby making autonomous and semi-autonomous vehicles more reliable and user-friendly.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">High-Quality Real-World Data:</span> To achieve the level of precision required for in-vehicle perception, it is crucial to have access to high-quality and well-distributed real-world data. The quality of the data directly influences the model’s ability to generalize and perform reliably in diverse conditions, thereby reinforcing the importance of the first point.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Data:</span> In the absence of sufficient real-world data, synthetic data serves as a viable alternative. However, the utility of synthetic data is not just a stopgap but a strategic asset that can simulate various edge cases, thereby aiding in the robustness of in-vehicle perception systems.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S6.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Vision Foundation Models:</span> Vision foundation models become particularly important when high-quality real-world data is scarce. These pre-trained models can be fine-tuned and adapted to perform specific tasks related to in-vehicle perception, thereby offering a quicker and more efficient route to achieving the goals set out in the first point.</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S6.I1.i5.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i5.p1.1" class="ltx_p"><span id="S6.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Realistic Synthetic Data:</span> While synthetic data is beneficial, it needs to closely mimic real-world conditions in terms of distributions, noisiness, etc., to be truly effective. Data that is “<span id="S6.I1.i5.p1.1.2" class="ltx_text ltx_font_italic">too synthetic</span>" can lead to poor out-of-distribution performance, undermining the real-world applicability of models designed for in-vehicle perception.</p>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S6.I1.i6.p1" class="ltx_para ltx_noindent">
<p id="S6.I1.i6.p1.1" class="ltx_p"><span id="S6.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Future Research:</span> Conducting research like ours is vital for the future of the automotive industry and technology. It not only addresses immediate challenges in multi-task facial attribute recognition but also lays the groundwork for more advanced in-vehicle perception systems, thereby contributing to the broader goals of automotive safety and personalization.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Limitations and Future Work</h3>

<div id="S6.SS2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS2.p1.1" class="ltx_p">Despite the comprehensive nature of this study, there are limitations that warrant attention. First, the synthetic nature of the datasets used may not fully capture the complexities and variabilities of real-world data, thereby affecting the generalizability of the models. Second, the study was constrained by the sample sizes of the synthetic datasets, which could impact the robustness of the models and the reliability of the results. Future work should focus on incorporating more diverse and larger datasets to validate the findings of this study. Additionally, exploring other advanced techniques and architectures could provide further insights into improving both in-distribution and out-of-distribution performances. Finally, by employing model compression techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib158" title="" class="ltx_ref">158</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> such as network quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>, and model pruning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>, and hardware-aware design <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib159" title="" class="ltx_ref">159</a>]</cite> we aim to further realize the potentials of such systems and models for in-vehicle perception and intelligence and real-world deployment.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. J. Fagnant and K. Kockelman, “Preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Transportation Research Part A: Policy and Practice</em>, vol. 77, pp. 167–181, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. K. Tyagi and N. Sreenath, “Autonomous vehicles and intelligent transportation systems—a framework of intelligent vehicles,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Intelligent Transportation Systems: Theory and Practice</em>.   Springer, 2022, pp. 75–98.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
M. Da Lio, F. Biral, E. Bertolazzi, M. Galvani, P. Bosetti, D. Windridge, A. Saroldi, and F. Tango, “Artificial co-drivers as a universal enabling technology for future intelligent vehicles and transportation systems,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on intelligent transportation systems</em>, vol. 16, no. 1, pp. 244–263, 2014.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Li, C. Olaverri-Monreal, X. Qu, C. S. Wu, S. E. Li, H. Taghavifar, Y. Xing, and S. Li, “Driver behavior in intelligent transportation systems [guest editorial],” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Transportation Systems Magazine</em>, vol. 14, no. 3, pp. 7–9, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
M. Natarajan, E. Seraj, B. Altundas, R. Paleja, S. Ye, L. Chen, R. Jensen, K. C. Chang, and M. Gombolay, “Human-robot teaming: Grand challenges,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Current Robotics Reports</em>, pp. 1–20, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Torstensson, T. H. Bui, D. Lindström, C. Englund, and B. Duran, “In-vehicle driver and passenger activity recognition,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">37th Annual Swedish Symposium on Image Analysis (SSBA 2019), Gothenburg, Sweden, March 19-20, 2019</em>, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Kashevnik, I. Lashkov, D. Ryumin, and A. Karpov, “Smartphone-based driver support in vehicle cabin: Human-computer interaction interface,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Interactive Collaborative Robotics: 4th International Conference, ICR 2019, Istanbul, Turkey, August 20–25, 2019, Proceedings 4</em>.   Springer, 2019, pp. 129–138.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Sasidharan and V. Kanagarajan, “Vehicle cabin safety alert system,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">2015 International Conference on Computer Communication and Informatics (ICCCI)</em>.   IEEE, 2015, pp. 1–4.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
A. Kashevnik, A. Ponomarev, and A. Krasov, “Human-computer threats classification in intelligent transportation systems,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">2020 26th Conference of Open Innovations Association (FRUCT)</em>.   IEEE, 2020, pp. 151–157.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Fernandez and T. Ito, “Driver classification for intelligent transportation systems using fuzzy logic,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2016 IEEE 19th international conference on intelligent transportation systems (ITSC)</em>.   IEEE, 2016, pp. 1212–1216.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
E. Seraj, A. Silva, and M. Gombolay, “Safe coordination of human-robot firefighting teams,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.06847</em>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
E. Namazi, R. N. Holthe-Berg, C. S. Lofsberg, and J. Li, “Using vehicle-mounted camera to collect information for managing mixed traffic,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2019 15th International Conference on Signal-Image Technology &amp; Internet-Based Systems (SITIS)</em>.   IEEE, 2019, pp. 222–230.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Chen, Y. Fang, H. Sheng, I. Masaki, B. Horn, and Z. Xiong, “Real-time vehicle status perception without frame-based segmentation for smart camera network,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2018 4th International Conference on Universal Village (UV)</em>.   IEEE, 2018, pp. 1–6.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C. Ergenc and L. Yifei, “A review of art and real world applications of intelligent perception systems,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in Intelligent Systems and Technologies</em>, pp. 076–086, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Abufadda and K. Mansour, “A survey of synthetic data generation for machine learning,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">2021 22nd international arab conference on information technology (ACIT)</em>.   IEEE, 2021, pp. 1–7.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Lu, H. Wang, and W. Wei, “Machine learning for synthetic data generation: a review,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.04062</em>, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
S. I. Nikolenko, <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Synthetic data for deep learning</em>.   Springer, 2021, vol. 174.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo, “Vector quantized diffusion model for text-to-image synthesis,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 10 696–10 706.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, “Diffusion models in vision: A survey,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J. Ho, C. Saharia, W. Chan, D. J. Fleet, M. Norouzi, and T. Salimans, “Cascaded diffusion models for high fidelity image generation,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">The Journal of Machine Learning Research</em>, vol. 23, no. 1, pp. 2249–2281, 2022.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
R. J. Chen, M. Y. Lu, T. Y. Chen, D. F. Williamson, and F. Mahmood, “Synthetic data in machine learning for medicine and healthcare,” <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Nature Biomedical Engineering</em>, vol. 5, no. 6, pp. 493–497, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Dahmen and D. Cook, “Synsys: A synthetic data generation system for healthcare applications,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 19, no. 5, p. 1181, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
D. Talwar, S. Guruswamy, N. Ravipati, and M. Eirinaki, “Evaluating validity of synthetic data in perception tasks for autonomous vehicles,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference On Artificial Intelligence Testing (AITest)</em>.   IEEE, 2020, pp. 73–80.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Y. Yuan and M. Sester, “Comap: A synthetic dataset for collective multi-agent perception of autonomous driving,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</em>, vol. 43, pp. 255–263, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are features in deep neural networks?” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 27, 2014.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. Müller, and W. Samek, “On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">PloS one</em>, vol. 10, no. 7, p. e0130140, 2015.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Shorten and T. M. Khoshgoftaar, “A survey on image data augmentation for deep learning,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Journal of big data</em>, vol. 6, no. 1, pp. 1–48, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
E. J. Topol, “High-performance medicine: the convergence of human and artificial intelligence,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Nature medicine</em>, vol. 25, no. 1, pp. 44–56, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition at scale,” 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, “Transformers in vision: A survey,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ACM computing surveys (CSUR)</em>, vol. 54, no. 10s, pp. 1–41, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A survey on vision transformer,” <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 45, no. 1, pp. 87–110, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. G. Muhammad Shafiq, “Deep residual learning for image recognition: A survey,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 2022. [Online]. Available: <a target="_blank" href="https://doi.org/10.3390/APP12188972" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.3390/APP12188972</a>

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “On the opportunities and risks of foundation models,” <em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07258</em>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
G. Mai, C. Cundy, K. Choi, Y. Hu, N. Lao, and S. Ermon, “Towards a foundation model for geospatial artificial intelligence (vision paper),” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 30th International Conference on Advances in Geographic Information Systems</em>, 2022, pp. 1–4.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
W. Wang, J. Dai, Z. Chen, Z. Huang, Z. Li, X. Zhu, X. Hu, T. Lu, L. Lu, H. Li <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Internimage: Exploring large-scale vision foundation models with deformable convolutions,” in <em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 14 408–14 419.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P. K. Murali, M. Kaboli, and R. Dahiya, “Intelligent in-vehicle interaction technologies,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Advanced Intelligent Systems</em>, vol. 4, no. 2, p. 2100122, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
J. Lu, X. Zhang, X. Yang, I. Unwala <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Intelligent in-vehicle safety and security monitoring system with face recognition,” in <em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)</em>.   IEEE, 2019, pp. 225–229.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
P. Archana, P. Divyabharathi, S. Balaji, N. Kumareshan, P. Veeramanikandan, S. Naitik, S. M. Rafi, P. V. Nandankar, and G. Manikandan, “Face recognition based vehicle starter using machine learning,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Measurement: Sensors</em>, vol. 24, p. 100575, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
D. Yang, K. Jiang, D. Zhao, C. Yu, Z. Cao, S. Xie, Z. Xiao, X. Jiao, S. Wang, and K. Zhang, “Intelligent and connected vehicles: Current status and future perspectives,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Science China Technological Sciences</em>, vol. 61, pp. 1446–1471, 2018.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
C. Mingyang, H. Heye, X. Qing, W. Jianqiang, T. SEKIGUCHI, G. Lu, and L. Keqiang, “Survey of intelligent and connected vehicle technologies: Architectures, functions and applications,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Journal of Tsinghua University (Science and Technology)</em>, vol. 62, no. 3, pp. 493–508, 2022.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J. Liu and J. Liu, “Intelligent and connected vehicles: Current situation, future directions, and challenges,” <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Standards Magazine</em>, vol. 2, no. 3, pp. 59–65, 2018.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
E. Seraj, “Enhancing teamwork in multi-robot systems: Embodied intelligence via model- and data-driven approaches,” Ph.D. dissertation, Georgia Institute of Technology, 2023.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
W. Chu, Q. Wuniri, X. Du, Q. Xiong, T. Huang, and K. Li, “Cloud control system architectures, technologies and applications on intelligent and connected vehicles: a review,” <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Chinese Journal of Mechanical Engineering</em>, vol. 34, no. 1, pp. 1–23, 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y. Xun, Y. Sun, and J. Liu, “An experimental study towards driver identification for intelligent and connected vehicles,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ICC 2019-2019 IEEE International Conference on Communications (ICC)</em>.   IEEE, 2019, pp. 1–6.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
E. Seraj, “Embodied team intelligence in multi-robot systems,” in <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems</em>, 2022, pp. 1869–1871.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
E. Seraj, Z. Wang, R. Paleja, D. Martin, M. Sklar, A. Patel, and M. Gombolay, “Learning efficient diverse communication for cooperative heterogeneous teaming,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21st international conference on autonomous agents and multiagent systems</em>, 2022, pp. 1173–1182.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
E. Seraj, “Embodied, intelligent communication for multi-agent cooperation,” in <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 37, no. 13, 2023, pp. 16 135–16 136.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
L. Du, W. Chen, J. Ji, Z. Pei, B. Tong, H. Zheng <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A novel intelligent approach to lane-change behavior prediction for intelligent and connected vehicles,” <em id="bib.bib49.2.2" class="ltx_emph ltx_font_italic">Computational Intelligence and Neuroscience</em>, vol. 2022, 2022.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
L. Du, J. Ji, Z. Pei, H. Zheng, S. Fu, H. Kong, and W. Chen, “Improved detection method for traffic signs in real scenes applied in intelligent and connected vehicles,” <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IET Intelligent Transport Systems</em>, vol. 14, no. 12, pp. 1555–1564, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
L. Du, W. Chen, S. Fu, H. Kong, C. Li, and Z. Pei, “Real-time detection of vehicle and traffic light for intelligent and connected vehicles based on yolov3 network,” in <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">2019 5th International Conference on Transportation Information and Safety (ICTIS)</em>.   IEEE, 2019, pp. 388–392.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
W. Zhou, L. Yang, T. Ying, J. Yuan, and Y. Yang, “Velocity prediction of intelligent and connected vehicles for a traffic light distance on the urban road,” <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 20, no. 11, pp. 4119–4133, 2018.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
B. Okumura, M. R. James, Y. Kanzawa, M. Derry, K. Sakai, T. Nishi, and D. Prokhorov, “Challenges in perception and decision making for intelligent automotive vehicles: A case study,” <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Vehicles</em>, vol. 1, no. 1, pp. 20–32, 2016.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
E. Seraj, Z. Wang, R. Paleja, M. Sklar, A. Patel, and M. Gombolay, “Heterogeneous graph attention networks for learning diverse communication,” <em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.09568</em>, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
S. G. Konan, E. Seraj, and M. Gombolay, “Contrastive decision transformers,” in <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning</em>.   PMLR, 2023, pp. 2159–2169.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
L. Hu, X. Zhou, X. Zhang, F. Wang, Q. Li, and W. Wu, “A review on key challenges in intelligent vehicles: Safety and driver-oriented features,” <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">IET Intelligent Transport Systems</em>, vol. 15, no. 9, pp. 1093–1105, 2021.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
E. Seraj, X. Wu, and M. Gombolay, “Firecommander: An interactive, probabilistic multi-agent environment for heterogeneous robot teams,” <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.00165</em>, 2020.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Q. Xu, M. Cai, K. Li, B. Xu, J. Wang, and X. Wu, “Coordinated formation control for intelligent and connected vehicles in multiple traffic scenarios,” <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">IET Intelligent Transport Systems</em>, vol. 15, no. 1, pp. 159–173, 2021.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
E. Seraj and M. Gombolay, “Coordinated control of uavs for human-centered active sensing of wildfires,” in <em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">2020 American control conference (ACC)</em>.   IEEE, 2020, pp. 1845–1852.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
D. Xiaoping, L. Dongxin, L. Shen, W. Qiqige, and C. Wenbo, “Coordinated control algorithm at non-recurrent freeway bottlenecks for intelligent and connected vehicles,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp. 51 621–51 633, 2020.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
E. Seraj, L. Chen, and M. C. Gombolay, “A hierarchical coordination framework for joint perception-action tasks in composite robot teams,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Robotics</em>, vol. 38, no. 1, pp. 139–158, 2021.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
E. Seraj, V. Azimi, C. Abdallah, S. Hutchinson, and M. Gombolay, “Adaptive leader-follower control for multi-robot teams with uncertain network structure,” in <em id="bib.bib62.1.1" class="ltx_emph ltx_font_italic">2021 American control conference (ACC)</em>.   IEEE, 2021, pp. 1088–1094.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
D. Jia and D. Ngoduy, “Enhanced cooperative car-following traffic model with the combination of v2v and v2i communication,” <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Transportation Research Part B: Methodological</em>, vol. 90, pp. 172–191, 2016.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
K. C. Dey, A. Rayamajhi, M. Chowdhury, P. Bhavsar, and J. Martin, “Vehicle-to-vehicle (v2v) and vehicle-to-infrastructure (v2i) communication in a heterogeneous wireless network–performance evaluation,” <em id="bib.bib64.1.1" class="ltx_emph ltx_font_italic">Transportation Research Part C: Emerging Technologies</em>, vol. 68, pp. 168–184, 2016.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
J. Santa, A. F. Gómez-Skarmeta, and M. Sánchez-Artigas, “Architecture and evaluation of a unified v2v and v2i communication system based on cellular networks,” <em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">Computer Communications</em>, vol. 31, no. 12, pp. 2850–2861, 2008.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
S. G. Konan, E. Seraj, and M. Gombolay, “Iterated reasoning with mutual information in cooperative and byzantine decentralized teaming,” in <em id="bib.bib66.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
E. Seraj, J. Y. Xiong, M. L. Schrum, and M. Gombolay, “Mixed-initiative multiagent apprenticeship learning for human training of robot teams,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
L. Pimentel, R. Paleja, Z. Wang, E. Seraj, J. E. Pagan, and M. Gombolay, “Scaling multi-agent reinforcement learning via state upsampling,” in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">RSS 2022 Workshop on Scaling Robot Learning</em>, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Z. Tan, N. Dai, Y. Su, R. Zhang, Y. Li, D. Wu, and S. Li, “Human–machine interaction in intelligent and connected vehicles: a review of status quo, issues, and opportunities,” <em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23, no. 9, pp. 13 954–13 975, 2021.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
S. Cafiso, A. Di Graziano, and G. Pappalardo, “In-vehicle stereo vision system for identification of traffic conflicts between bus and pedestrian,” <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">Journal of traffic and transportation engineering (English edition)</em>, vol. 4, no. 1, pp. 3–13, 2017.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
R. Dinakaran, L. Zhang, and R. Jiang, “In-vehicle object detection in the wild for driverless vehicles,” in <em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Developments of Artificial Intelligence Technologies in Computation and Robotics: Proceedings of the 14th International FLINS Conference (FLINS 2020)</em>.   World Scientific, 2020, pp. 1139–1147.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
T. Huang and S. Russell, “Object identification: A bayesian analysis with application to traffic surveillance,” <em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence</em>, vol. 103, no. 1-2, pp. 77–93, 1998.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
G. Qi, H. Wang, M. Haner, C. Weng, S. Chen, and Z. Zhu, “Convolutional neural network based detection and judgement of environmental obstacle in vehicle operation,” <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">CAAI Transactions on Intelligence Technology</em>, vol. 4, no. 2, pp. 80–91, 2019.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
S. Aradi, “Survey of deep reinforcement learning for motion planning of autonomous vehicles,” <em id="bib.bib74.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23, no. 2, pp. 740–759, 2020.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
D. Yudin, A. Skrynnik, A. Krishtopik, I. Belkin, and A. Panov, “Object detection with deep neural networks for reinforcement learning in the task of autonomous vehicles path planning at the intersection,” <em id="bib.bib75.1.1" class="ltx_emph ltx_font_italic">Optical Memory and Neural Networks</em>, vol. 28, pp. 283–295, 2019.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
R. Xie, Z. Meng, L. Wang, H. Li, K. Wang, and Z. Wu, “Unmanned aerial vehicle path planning algorithm based on deep reinforcement learning in large-scale and dynamic environments,” <em id="bib.bib76.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 9, pp. 24 884–24 900, 2021.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
C. J. Normark, “Personalizable vehicle user interfaces for better user experience,” <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Advances in Affective and Pleasurable Design</em>, p. 169, 2021.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
H. Du, S. Teng, H. Chen, J. Ma, X. Wang, C. Gou, B. Li, S. Ma, Q. Miao, X. Na <em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Chat with chatgpt on intelligent vehicles: An ieee tiv perspective,” <em id="bib.bib78.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Vehicles</em>, 2023.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
J. Watson, E. Duff, C. Fletcher, J. McVeigh-Schultz, J. Stein, and S. S. Fisher, “Ambient storytelling for vehicle-driver interaction.”

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
K. Uvarov and A. Ponomarev, “Maintaining vehicle driver’s state using personalized interventions,” in <em id="bib.bib80.1.1" class="ltx_emph ltx_font_italic">2022 31st Conference of Open Innovations Association (FRUCT)</em>.   IEEE, 2022, pp. 347–354.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
M. Hasenjäger and H. Wersing, “Personalization in advanced driver assistance systems and autonomous vehicles: A review,” in <em id="bib.bib81.1.1" class="ltx_emph ltx_font_italic">2017 ieee 20th international conference on intelligent transportation systems (itsc)</em>.   IEEE, 2017, pp. 1–7.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
J. Xu, J. Chen, and Z. Liu, “Research on active interaction and user experience of community intelligent vehicle system,” in <em id="bib.bib82.1.1" class="ltx_emph ltx_font_italic">2021 International Symposium on Artificial Intelligence and its Application on Media (ISAIAM)</em>.   IEEE, 2021, pp. 43–50.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Y. Jacob, S. Manitsaris, F. Moutarde, G. Lele, and L. Pradere, “Hand gesture recognition for driver vehicle interaction,” in <em id="bib.bib83.1.1" class="ltx_emph ltx_font_italic">IEEE Computer Society Workshop on Observing and understanding hands in action (Hands 2015) of 28th IEEE conf. on Computer Vision and Pattern Recognition (CVPR’2015)</em>, 2015.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
C. Pickering, “Gesture recognition driver controls,” <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Computing and Control Engineering</em>, vol. 16, no. 1, pp. 26–27, 2005.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
U. Agrawal, S. Giripunje, and P. Bajaj, “Emotion and gesture recognition with soft computing tool for drivers assistance system in human centered transportation,” in <em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">2013 IEEE International Conference on Systems, Man, and Cybernetics</em>.   IEEE, 2013, pp. 4612–4616.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
A. Doshi and M. M. Trivedi, “Tactical driver behavior prediction and intent inference: A review,” in <em id="bib.bib86.1.1" class="ltx_emph ltx_font_italic">2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2011, pp. 1892–1897.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
H.-B. Kang, “Various approaches for driver and driving behavior monitoring: A review,” in <em id="bib.bib87.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer Vision Workshops</em>, 2013, pp. 616–623.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
D. Yang, X. Li, X. Dai, R. Zhang, L. Qi, W. Zhang, and Z. Jiang, “All in one network for driver attention monitoring,” in <em id="bib.bib88.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 2258–2262.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Z. Hu, Y. Zhang, Q. Li, and C. Lv, “A novel heterogeneous network for modeling driver attention with multi-level visual content,” <em id="bib.bib89.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on intelligent transportation systems</em>, vol. 23, no. 12, pp. 24 343–24 354, 2022.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Y. Rong, N.-R. Kassautzki, W. Fuhl, and E. Kasneci, “Where and what: Driver attention-based object detection,” <em id="bib.bib90.1.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Human-Computer Interaction</em>, vol. 6, no. ETRA, pp. 1–22, 2022.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
A. Zyner, S. Worrall, J. Ward, and E. Nebot, “Long short term memory for driver intent prediction,” in <em id="bib.bib91.1.1" class="ltx_emph ltx_font_italic">2017 IEEE Intelligent Vehicles Symposium (IV)</em>.   IEEE, 2017, pp. 1484–1489.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
B. Morris, A. Doshi, and M. Trivedi, “Lane change intent prediction for driver assistance: On-road design and evaluation,” in <em id="bib.bib92.1.1" class="ltx_emph ltx_font_italic">2011 IEEE Intelligent Vehicles Symposium (IV)</em>.   IEEE, 2011, pp. 895–901.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
D. J. Phillips, T. A. Wheeler, and M. J. Kochenderfer, “Generalizable intention prediction of human drivers at intersections,” in <em id="bib.bib93.1.1" class="ltx_emph ltx_font_italic">2017 IEEE intelligent vehicles symposium (IV)</em>.   IEEE, 2017, pp. 1665–1670.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, “A survey of convolutional neural networks: analysis, applications, and prospects,” <em id="bib.bib94.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks and learning systems</em>, 2021.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
K. O’Shea and R. Nash, “An introduction to convolutional neural networks,” <em id="bib.bib95.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1511.08458</em>, 2015.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
F. Vicente, Z. Huang, X. Xiong, F. De la Torre, W. Zhang, and D. Levi, “Driver gaze tracking and eyes off the road detection system,” <em id="bib.bib96.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 16, no. 4, pp. 2014–2027, 2015.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
H. S. Yoon, N. R. Baek, N. Q. Truong, and K. R. Park, “Driver gaze detection based on deep residual networks using the combined single image of dual near-infrared cameras,” <em id="bib.bib97.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 7, pp. 93 448–93 461, 2019.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
R. A. Naqvi, M. Arsalan, G. Batchuluun, H. S. Yoon, and K. R. Park, “Deep learning-based gaze detection system for automobile drivers using a nir camera sensor,” <em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 18, no. 2, p. 456, 2018.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
L. Fletcher and A. Zelinsky, “Driver inattention detection based on eye gaze—road event correlation,” <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">The international journal of robotics research</em>, vol. 28, no. 6, pp. 774–801, 2009.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
S. M. Shah, Z. Sun, K. Zaman, A. Hussain, M. Shoaib, and L. Pei, “A driver gaze estimation method based on deep learning,” <em id="bib.bib100.1.1" class="ltx_emph ltx_font_italic">Sensors</em>, vol. 22, no. 10, p. 3959, 2022.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
L. Fridman, P. Langhans, J. Lee, and B. Reimer, “Driver gaze region estimation without use of eye movement,” <em id="bib.bib101.1.1" class="ltx_emph ltx_font_italic">IEEE Intelligent Systems</em>, vol. 31, no. 3, pp. 49–56, 2016.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
S. Dari, N. Kadrileev, and E. Hüllermeier, “A neural network-based driver gaze classification system with vehicle signals,” in <em id="bib.bib102.1.1" class="ltx_emph ltx_font_italic">2020 International Joint Conference on Neural Networks (IJCNN)</em>.   IEEE, 2020, pp. 1–7.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
X. Wang, R. Guo, and C. Kambhamettu, “Deeply-learned feature for age estimation,” in <em id="bib.bib103.1.1" class="ltx_emph ltx_font_italic">2015 IEEE Winter Conference on Applications of Computer Vision</em>.   IEEE, 2015, pp. 534–541.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
S. Ashiqur Rahman, P. Giacobbi, L. Pyles, C. Mullett, G. Doretto, and D. A. Adjeroh, “Deep learning for biological age estimation,” <em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Briefings in bioinformatics</em>, vol. 22, no. 2, pp. 1767–1781, 2021.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
J. H. Lee and K. G. Kim, “Applying deep learning in medical images: the case of bone age estimation,” <em id="bib.bib105.1.1" class="ltx_emph ltx_font_italic">Healthcare informatics research</em>, vol. 24, no. 1, pp. 86–92, 2018.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
G. Du, Z. Wang, B. Gao, S. Mumtaz, K. M. Abualnaja, and C. Du, “A convolution bidirectional long short-term memory neural network for driver emotion recognition,” <em id="bib.bib106.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 22, no. 7, pp. 4570–4578, 2020.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
S. Zepf, J. Hernandez, A. Schmitt, W. Minker, and R. W. Picard, “Driver emotion recognition for intelligent vehicles: A survey,” <em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">ACM Computing Surveys (CSUR)</em>, vol. 53, no. 3, pp. 1–30, 2020.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
W. Li, G. Zeng, J. Zhang, Y. Xu, Y. Xing, R. Zhou, G. Guo, Y. Shen, D. Cao, and F.-Y. Wang, “Cogemonet: A cognitive-feature-augmented driver emotion recognition model for smart cockpit,” <em id="bib.bib108.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Computational Social Systems</em>, vol. 9, no. 3, pp. 667–678, 2021.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
H. Xiao, W. Li, G. Zeng, Y. Wu, J. Xue, J. Zhang, C. Li, and G. Guo, “On-road driver emotion recognition using facial expression,” <em id="bib.bib109.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, vol. 12, no. 2, p. 807, 2022.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
C. Park, Y. Jeong, M. Cho, and J. Park, “Fast point transformer,” in <em id="bib.bib110.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2022, pp. 16 949–16 958.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, “Video swin transformer,” in <em id="bib.bib111.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 3202–3211.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in <em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 10 012–10 022.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, “Long-term recurrent convolutional networks for visual recognition and description,” in <em id="bib.bib113.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, pp. 2625–2634.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em id="bib.bib114.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional sequence to sequence learning,” in <em id="bib.bib115.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2017, pp. 1243–1252.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
H. Hosseini, B. Xiao, M. Jaiswal, and R. Poovendran, “On the limitation of convolutional neural networks in recognizing negative images,” in <em id="bib.bib116.1.1" class="ltx_emph ltx_font_italic">2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)</em>.   IEEE, 2017, pp. 352–358.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
R. Yamashita, M. Nishio, R. K. G. Do, and K. Togashi, “Convolutional neural networks: an overview and application in radiology,” <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Insights into imaging</em>, vol. 9, pp. 611–629, 2018.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
C. Nebauer, “Evaluation of convolutional neural networks for visual recognition,” <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on neural networks</em>, vol. 9, no. 4, pp. 685–696, 1998.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
B. Li, Y. Shi, Z. Qi, and Z. Chen, “A survey on semantic segmentation,” in <em id="bib.bib119.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Data Mining Workshops (ICDMW)</em>.   IEEE, 2018, pp. 1233–1240.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
B. Wang, Y. Lei, T. Yan, N. Li, and L. Guo, “Recurrent convolutional neural network: A new framework for remaining useful life prediction of machinery,” <em id="bib.bib120.1.1" class="ltx_emph ltx_font_italic">Neurocomputing</em>, vol. 379, pp. 117–129, 2020.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
K. Islam, “Recent advances in vision transformer: A survey and outlook of recent work,” <em id="bib.bib121.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.01536</em>, 2022.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay, J. Feng, and S. Yan, “Tokens-to-token vit: Training vision transformers from scratch on imagenet,” in <em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 558–567.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
H. Thisanke, C. Deshan, K. Chamith, S. Seneviratne, R. Vidanaarachchi, and D. Herath, “Semantic segmentation using vision transformers: A survey,” <em id="bib.bib123.1.1" class="ltx_emph ltx_font_italic">Engineering Applications of Artificial Intelligence</em>, vol. 126, p. 106669, 2023.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Z. Long, Z. Meng, G. A. Camarasa, and R. McCreadie, “Lacvit: A label-aware contrastive training framework for vision transformers,” <em id="bib.bib124.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.18013</em>, 2023.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
Y. Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool, “Localvit: Bringing locality to vision transformers,” <em id="bib.bib125.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.05707</em>, 2021.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
H. Xu, Q. Xu, F. Cong, J. Kang, C. Han, Z. Liu, A. Madabhushi, and C. Lu, “Vision transformers for computational histopathology,” <em id="bib.bib126.1.1" class="ltx_emph ltx_font_italic">IEEE Reviews in Biomedical Engineering</em>, 2023.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Y. Zhang and Q. Yang, “An overview of multi-task learning,” <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">National Science Review</em>, vol. 5, no. 1, pp. 30–43, 2018.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
K. Weiss, T. M. Khoshgoftaar, and D. Wang, “A survey of transfer learning,” <em id="bib.bib128.1.1" class="ltx_emph ltx_font_italic">Journal of Big data</em>, vol. 3, no. 1, pp. 1–40, 2016.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
L. Torrey and J. Shavlik, “Transfer learning,” in <em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">Handbook of research on machine learning applications and trends: algorithms, methods, and techniques</em>.   IGI global, 2010, pp. 242–264.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
S. Ruder, “An overview of multi-task learning in deep neural networks,” <em id="bib.bib130.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.05098</em>, 2017.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Y. Zhang and Q. Yang, “A survey on multi-task learning,” <em id="bib.bib131.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</em>, vol. 34, no. 12, pp. 5586–5609, 2021.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
K.-H. Thung and C.-Y. Wee, “A brief review on multi-task learning,” <em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">Multimedia Tools and Applications</em>, vol. 77, pp. 29 705–29 725, 2018.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
T. Evgeniou and M. Pontil, “Regularized multi–task learning,” in <em id="bib.bib133.1.1" class="ltx_emph ltx_font_italic">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</em>, 2004, pp. 109–117.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
T. Standley, A. Zamir, D. Chen, L. Guibas, J. Malik, and S. Savarese, “Which tasks should be learned together in multi-task learning?” in <em id="bib.bib134.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>.   PMLR, 2020, pp. 9120–9132.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
D.-K. Nguyen and T. Okatani, “Multi-task learning of hierarchical vision-language representation,” in <em id="bib.bib135.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 10 492–10 501.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He, “A comprehensive survey on transfer learning,” <em id="bib.bib136.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE</em>, vol. 109, no. 1, pp. 43–76, 2020.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
S. Niu, Y. Liu, J. Wang, and H. Song, “A decade survey of transfer learning (2010–2020),” <em id="bib.bib137.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Artificial Intelligence</em>, vol. 1, no. 2, pp. 151–166, 2020.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em id="bib.bib138.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on knowledge and data engineering</em>, vol. 22, no. 10, pp. 1345–1359, 2009.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Zhang, W. Dai, and S. J. Pan, <em id="bib.bib139.1.1" class="ltx_emph ltx_font_italic">Transfer learning</em>.   Cambridge University Press, 2020.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
X. Li, Y. Grandvalet, F. Davoine, J. Cheng, Y. Cui, H. Zhang, S. Belongie, Y.-H. Tsai, and M.-H. Yang, “Transfer learning in computer vision tasks: Remember where you come from,” <em id="bib.bib140.1.1" class="ltx_emph ltx_font_italic">Image and Vision Computing</em>, vol. 93, p. 103853, 2020.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
B.-X. Wu, C.-G. Yang, and J.-P. Zhong, “Research on transfer learning of vision-based gesture recognition,” <em id="bib.bib141.1.1" class="ltx_emph ltx_font_italic">International Journal of Automation and Computing</em>, vol. 18, pp. 422–431, 2021.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
K. Gopalakrishnan, S. K. Khaitan, A. Choudhary, and A. Agrawal, “Deep convolutional neural networks with transfer learning for computer vision-based data-driven pavement distress detection,” <em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">Construction and building materials</em>, vol. 157, pp. 322–330, 2017.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
M. Awais, M. Naseer, S. Khan, R. M. Anwer, H. Cholakkal, M. Shah, M.-H. Yang, and F. S. Khan, “Foundational models defining a new era in vision: A survey and outlook,” <em id="bib.bib143.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.13721</em>, 2023.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
C. Zhang, S. Zheng, C. Li, Y. Qiao, T. Kang, X. Shan, C. Zhang, C. Qin, F. Rameau, S.-H. Bae <em id="bib.bib144.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A survey on segment anything model (sam): Vision foundation model meets prompt engineering,” <em id="bib.bib144.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.06211</em>, 2023.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language models for vision tasks: A survey,” <em id="bib.bib145.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.00685</em>, 2023.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
L. Cuimei, Q. Zhiliang, J. Nan, and W. Jianhua, “Human face detection algorithm via haar cascade classifier combined with three additional classifiers,” in <em id="bib.bib146.1.1" class="ltx_emph ltx_font_italic">2017 13th IEEE International Conference on Electronic Measurement &amp; Instruments (ICEMI)</em>.   IEEE, 2017, pp. 483–487.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
S. I. Serengil and A. Ozpinar, “Lightface: A hybrid deep face recognition framework,” in <em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">2020 Innovations in Intelligent Systems and Applications Conference (ASYU)</em>.   IEEE, 2020, pp. 23–27. [Online]. Available: <a target="_blank" href="https://doi.org/10.1109/ASYU50717.2020.9259802" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ASYU50717.2020.9259802</a>

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
——, “Hyperextended lightface: A facial attribute analysis framework,” in <em id="bib.bib148.1.1" class="ltx_emph ltx_font_italic">2021 International Conference on Engineering and Emerging Technologies (ICEET)</em>.   IEEE, 2021, pp. 1–4. [Online]. Available: <a target="_blank" href="https://doi.org/10.1109/ICEET53442.2021.9659697" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICEET53442.2021.9659697</a>

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in <em id="bib.bib149.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, and C. Liu, “A survey on deep transfer learning,” in <em id="bib.bib150.1.1" class="ltx_emph ltx_font_italic">Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27</em>.   Springer, 2018, pp. 270–279.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for generation,” <em id="bib.bib151.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00190</em>, 2021.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
A. Kumar, A. Raghunathan, R. Jones, T. Ma, and P. Liang, “Fine-tuning can distort pretrained features and underperform out-of-distribution,” <em id="bib.bib152.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.10054</em>, 2022.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” <em id="bib.bib153.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1412.6980</em>, 2014.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and stochastic optimization.” <em id="bib.bib154.1.1" class="ltx_emph ltx_font_italic">Journal of machine learning research</em>, vol. 12, no. 7, 2011.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
S. Ruder, “An overview of gradient descent optimization algorithms,” <em id="bib.bib155.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.04747</em>, 2016.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Z. Zhang, Y. Song, and H. Qi, “Age progression/regression by conditional adversarial autoencoder,” in <em id="bib.bib156.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 5810–5818.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
M. C. Cieslak, A. M. Castelfranco, V. Roncalli, P. H. Lenz, and D. K. Hartline, “t-distributed stochastic neighbor embedding (t-sne): A tool for eco-physiological transcriptomic analysis,” <em id="bib.bib157.1.1" class="ltx_emph ltx_font_italic">Marine genomics</em>, vol. 51, p. 100723, 2020.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
F. Karimzadeh, “Hardware-friendly model compression techniques for deep learning accelerators,” Ph.D. dissertation, Georgia Institute of Technology, 2022.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
F. Karimzadeh and A. Raychowdhury, “Towards cim-friendly and energy-efficient dnn accelerator via bit-level sparsity,” in <em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">2022 IFIP/IEEE 30th International Conference on Very Large Scale Integration (VLSI-SoC)</em>.   IEEE, 2022, pp. 1–2.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
F. Karimzadeh, J.-H. Yoon, and A. Raychowdhury, “Bits-net: Bit-sparse deep neural network for energy-efficient rram-based compute-in-memory,” <em id="bib.bib160.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems I: Regular Papers</em>, vol. 69, no. 5, pp. 1952–1961, 2022.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
F. Karimzadeh, N. Cao, B. Crafton, J. Romberg, and A. Raychowdhury, “A hardware-friendly approach towards sparse neural networks based on lfsr-generated pseudo-random sequences,” <em id="bib.bib161.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems I: Regular Papers</em>, vol. 68, no. 2, pp. 751–764, 2020.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
F. Karimzadeh and A. Raychowdhury, “Memory and energy efficient method toward sparse neural network using lfsr indexing,” in <em id="bib.bib162.1.1" class="ltx_emph ltx_font_italic">2020 IFIP/IEEE 28th International Conference on Very Large Scale Integration (VLSI-SOC)</em>.   IEEE, 2020, pp. 206–207.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
F. Karimzadeh, N. Cao, B. Crafton, J. Romberg, and A. Raychowdhury, “Hardware-aware pruning of dnns using lfsr-generated pseudo-random indices,” in <em id="bib.bib163.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Symposium on Circuits and Systems (ISCAS)</em>.   IEEE, 2020, pp. 1–5.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
F. Karimzadeh and A. Raychowdhury, “Towards energy efficient dnn accelerator via sparsified gradual knowledge distillation,” in <em id="bib.bib164.1.1" class="ltx_emph ltx_font_italic">2022 IFIP/IEEE 30th International Conference on Very Large Scale Integration (VLSI-SoC)</em>.   IEEE, 2022, pp. 1–6.

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="David S. Hippocampus, Elias D. Striatum"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="First keyword, Second keyword, More"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="q-bio.NC, q-bio.QM"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="A template for the arxiv style"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.06087" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.06088" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.06088">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.06088" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.06089" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 13:40:35 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
