<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Boosting Healthcare LLMs Through Retrieved Context</title>
<!--Generated on Mon Sep 23 15:27:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15127v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S1" title="In Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S2" title="In Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3" title="In Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.SS1" title="In 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Retrieval Components</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.SS2" title="In 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.SS3" title="In 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Models and Compute</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4" title="In Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Retrieval Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS1" title="In 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>SC-CoT Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS2" title="In 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Medprompt Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS3" title="In 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Proposed Scheme</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS4" title="In 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>State-of-the-art Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS5" title="In 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Private Comparison</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5" title="In Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Open-Ended Answer Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS1" title="In 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>OpenMedprompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS2" title="In 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Dataset and Database Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS3" title="In 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS4" title="In 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Results and Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S6" title="In Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S6.SS1" title="In 6. Conclusions ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S6.SS2" title="In 6. Conclusions ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Carbon Footprint</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S6.SS3" title="In 6. Conclusions ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Ethical Considerations</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Boosting Healthcare LLMs Through Retrieved Context</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jordi Bayarri-Planas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0009-0005-1968-3467" title="ORCID identifier">0009-0005-1968-3467</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Barcelona Supercomputing Center (BSC)</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Barcelona</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jordi.bayarri@bsc.es">jordi.bayarri@bsc.es</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ashwin Kumar Gururajan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-9246-4552" title="ORCID identifier">0000-0002-9246-4552</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Barcelona Supercomputing Center (BSC)</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Barcelona</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ashwin.gururajan@bsc.es">ashwin.gururajan@bsc.es</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dario Garcia-Gasulla
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-6732-5641" title="ORCID identifier">0000-0001-6732-5641</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Barcelona Supercomputing Center (BSC)</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Barcelona</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">Spain</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dario.garcia@bsc.es">dario.garcia@bsc.es</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, and yet, their factual inaccuracies and hallucinations limits their application, particularly in critical domains like healthcare. Context retrieval methods, by introducing relevant information as input, have emerged as a crucial approach for enhancing LLM factuality and reliability. This study explores the boundaries of context retrieval methods within the healthcare domain, optimizing their components and benchmarking their performance against open and closed alternatives. Our findings reveal how open LLMs, when augmented with an optimized retrieval system, can achieve performance comparable to the biggest private solutions on established healthcare benchmarks (multiple-choice question answering). Recognizing the lack of realism of including the possible answers within the question (a setup only found in medical exams), and after assessing a strong LLM performance degradation in the absence of those options, we extend the context retrieval system in that direction. In particular, we propose OpenMedPrompt a pipeline that improves the generation of more reliable open-ended answers, moving this technology closer to practical application.</p>
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; ; </span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) are the default solution for most text-related tasks. However, a critical concern remains: their factual accuracy<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We consider reasoning to be beyond a text task</span></span></span>, a limitation inherent to their generative nature. LLMs are not designed to retrieve precise information, but rather to generate plausible text based on learned patterns. A popular approach to enhance the factuality of LLMs is to contextualize them by biasing their output through relevant input tokens. This ranges from simple prompting techniques, such as ”Let’s think step by step,” to more sophisticated Retrieval Augmented Generation (RAG) systems. Indeed, integrating context retrieval systems can significantly boost the performance and reliability of LLMs.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the domain of medical multi-choice question-answering (MCQA), the current state-of-the-art is dominated by private models like GPT-4 and MedPalm-2. According to popular evaluation methods, open models significantly lag behind, limiting their practical use. However, a comprehensive assessment must include context retrieval systems and consider more realistic evaluation methods. This study addresses two key research questions: First, how competitive can open LLMs be in healthcare when enhanced with optimized context retrieval systems? Second, how can these systems be extended beyond the limited domain of multi-choice QA?</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the first question, this work explores the limits of model boosting through context retrieval by optimizing its components. Open models of various sizes are then enhanced with this optimized system and compared with private solutions. For the second question, we propose an extension of the context retrieval system to generate open-ended (OE) answers, increasing the quality of responses not biased by possible answers.</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S1.I1.i1.p1.1.1">C1</span>: Guidelines for an optimized context retrieval setup</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_typewriter" id="S1.I1.i2.p1.1.1">C2</span>: Updated benchmark on open/private LLMs for healthcare MCQA</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S1.I1.i3.p1.1.1">C4</span>: Novel context retrieval design for OE answer generation</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_typewriter" id="S1.I1.i4.p1.1.1">C4</span>: Automated library for model boosting</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Addressing the challenge of LLM factuality has spurred extensive research. Initial attempts to improve it centered on harnessing the inherent In-Context Learning (ICL) abilities of LLMs <cite class="ltx_cite ltx_citemacro_citep">(et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib6" title="">2020</a>)</cite>, allowing them to adapt to new tasks with minimal examples and without specific training. This paved the way for the development of sophisticated prompting techniques designed to elicit more accurate and reasoned responses. Chain of Thought (CoT) prompting <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib33" title="">2023</a>)</cite> guides LLMs through intermediate reasoning steps, enhancing their performance on complex tasks. In contrast, Self-Consistency (SC) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib32" title="">2023b</a>)</cite> exploits the stochastic nature of LLMs, producing and comparing several outcomes for the same input, before producing a unified answer. Both are combined in Self-Consistency Chain of Thought (SC-CoT). In a similar fashion, tree of Thought (ToT) <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib34" title="">2023</a>)</cite> builds a search space across generated steps towards the answer. At each step several follow-up options are generated, evaluated, and selected, building a pruned tree in the process, from which the final answer is extracted. Reflection methods  <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib26" title="">2023</a>; Renze and Guven, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib24" title="">2024</a>)</cite> employ the same generation model as a critique model to iteratively critique the intermediate output and then generate an improved output based on this feedback.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Recognizing the limitations of relying solely on internal knowledge, researchers turned to external knowledge integration through prompting techniques, ultimately leading to the emergence of Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib14" title="">2021</a>)</cite>. RAG systems retrieve and integrate relevant information from external knowledge bases, significantly enhancing LLM performance by biasing responses with factual data. In the healthcare domain, few-shot, CoT and SC are recurrently used to improve factuality <cite class="ltx_cite ltx_citemacro_citep">(Liévin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib16" title="">2024</a>; Pal and Sankarasubbu, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib22" title="">2024</a>; Savage et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib25" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib30" title="">2024</a>)</cite>, and a combination of those was proposed in Medprompt <cite class="ltx_cite ltx_citemacro_citep">(Nori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib20" title="">2023</a>)</cite>, a context retrieval system designed for medical MCQA that achieves state-of-the-art results with GPT-4. While Medprompt has been adapted for open-source models like  <cite class="ltx_cite ltx_citemacro_citep">(Maharjan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib17" title="">2024</a>)</cite>, a thorough investigation into the optimal configuration of its components (<span class="ltx_text ltx_font_italic" id="S2.p2.1.1">e.g., </span>DBs, embeddings) remains an open area of research.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Despite recent advances, a significant performance gap persists between large private models and their open-weight counterparts <cite class="ltx_cite ltx_citemacro_citep">(Liévin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib16" title="">2024</a>; Hager et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib9" title="">2024</a>)</cite>. However, the rapid evolution of accessible LLMs <cite class="ltx_cite ltx_citemacro_citep">(Maharjan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib17" title="">2024</a>; et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib4" title="">2024a</a>)</cite>, coupled with the potential of optimized RAG systems, suggests that this gap may be narrowing. The ability to leverage medium-sized open models with optimized RAG, potentially achieving performance comparable to larger closed-source alternatives, holds significant promise for reducing adoption costs and increasing accessibility.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">At this point, it is worth noting the limitations of MCQA benchmarks. While providing a valuable assessment tool (given its reliable ground truth), it represents a simplified and often unrealistic setup <cite class="ltx_cite ltx_citemacro_citep">(Hager et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib9" title="">2024</a>)</cite> only found in professional medicine exams. Indeed, when considering real-world clinical scenarios, we should not expect the question to include a list of possible answers. That is, models ought to generate open-ended answers. While CoT mechanisms have also been applied to the task of open-ended answers in QAs, contributions in that regard are scarce and non-exhaustive <cite class="ltx_cite ltx_citemacro_citep">(Liévin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib16" title="">2024</a>; Savage et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib25" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section details the methodology employed to optimize a context retrieval system and evaluate its performance. We first outline the core components of the context retrieval system under investigation (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.SS1" title="3.1. Retrieval Components ‣ 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">3.1</span></a>), followed by a description of the benchmark datasets used for evaluation (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.SS2" title="3.2. Datasets ‣ 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">3.2</span></a>), and conclude with the specific models and computational resources employed in the study (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.SS3" title="3.3. Models and Compute ‣ 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Let us start with the retrieval system architecture, which is based on the Medprompt design. Figure  <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.F1" title="Figure 1 ‣ 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates this question-answering system, highlighting the key components that will be explored and optimized in this work.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S3.F1.g1" src="extracted/5873777/architecture.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Components of a question-answering system based on context retrieval for LLMs.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Retrieval Components</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this work, we consider the role and impact of the following components:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Choice shuffling</span>. This technique involves randomly shuffling the order of the answer choices presented in multiple-choice questions to mitigate position bias <cite class="ltx_cite ltx_citemacro_citep">(Nori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib20" title="">2023</a>; Liévin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib16" title="">2024</a>)</cite>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Number of ensembles</span>. Refers to the number of independently generated responses produced by the LLM for a given question. These responses are then aggregated, to arrive at the final answer (through techniques such as majority voting). The effect of this component is experimented with the SC-CoT technique.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Database.</span> This component represents the external knowledge source used to contextualize and bias the model. We experiment with two distinct database approaches: utilizing the validation set of the datasets to generate examples in execution time as seen in the original Medprompt methodology, and constructing custom databases derived from the training sets.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Embedding model</span>. The embedding model is crucial for retrieving relevant information from the database. It transforms both the input question and the database entries into numerical vector representations, allowing for similarity comparisons. We evaluate various embedding models, considering factors like dimensionality (e.g., 768 vs. 4096) and domain specificity (e.g., general purpose vs. healthcare-specific).
</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Reranking model</span>. This optional component aims to refine the initial retrieval results by re-ranking the candidate QA’s retrieved from the database based on their relevance to the input question. We employ the MedCPT-Cross-Encoder <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib13" title="">2023</a>)</cite>, a specialized medical reranking model trained on a large corpus of biomedical literature.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Beyond these core components, we also explore the impact of hyperparameters such as temperature and the number of few-shot examples, though their impact is less pronounced.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Datasets</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To evaluate the performance of our optimized system, we employ four widely recognized medical Multiple-Choice Question Answering (MCQA) datasets:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">MedQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib12" title="">2021</a>)</cite>: Consists of 1,273 multiple-choice questions in the format of the US Medical License Exam (USMLE).</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">MedMCQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Pal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib23" title="">2022</a>)</cite>: Large-scale multiple-choice question answering dataset with questions from Indian medical school entrance exams. We use the validation set which is composed of 4,183 questions, as the answers of the test set are private.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">CareQA</span> <cite class="ltx_cite ltx_citemacro_citep">(Ganzabal and Pérez, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib7" title="">2024</a>)</cite>: Multiple-choice question answering dataset based on the access exam for Spanish Specialised Healthcare Training. It consists of 5,621 questions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i4.p1.1.1">MMLU</span> <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib10" title="">2021</a>)</cite>: MMLU is a multitask benchmark suite of 57 different datasets spanning domains across STEM. From all these tasks we use the medical related: <span class="ltx_text ltx_font_italic" id="S3.I2.i4.p1.1.2">anatomy, clinical knowledge, college biology, college medicine, medical genetics </span>and <span class="ltx_text ltx_font_italic" id="S3.I2.i4.p1.1.3">professional medicine</span>, which account for a total of 1,089 questions.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">These datasets collectively provide a comprehensive and diverse evaluation platform for assessing the performance of our system across different multi-choice medical question styles and sources.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Models and Compute</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The main model used in our experiments is Llama3-Aloe-8B-Alpha <cite class="ltx_cite ltx_citemacro_citep">(Gururajan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib8" title="">2024</a>)</cite>, a state-of-the-art open-source LLM specifically fine-tuned for the healthcare domain. With 8 billion parameters, this model builds upon the Meta Llama-3 architecture, leveraging a curated combination of high-quality medical data, synthetic data, and targeted alignment via DPO and Red-teaming datasets. It offers a compelling balance between performance and computational cost.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">All experiments are conducted on a single compute node equipped with 4x NVIDIA H100 (64GB) GPUs. We utilize a single GPU for smaller models (¡10B parameters) and employ tensor parallelism across multiple GPUs for larger models (¿20B parameters). To assess the environmental impact of our experiments, we monitor the node’s power consumption and extrapolate these measurements to estimate the carbon footprint of each experiment, utilizing the latest <math alttext="CO_{2}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">C</mi><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><msub id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mi id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2.cmml">O</mi><mn id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><times id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></times><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝐶</ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">𝑂</ci><cn id="S3.SS3.p2.1.m1.1.1.3.3.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">CO_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> emissions ratio provided by the European Union.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Furthermore, we developed a custom software repository <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/HPAI-BSC/prompt_engine" title="">https://github.com/HPAI-BSC/prompt_engine</a></span></span></span> to facilitate this research, which we release open-source to benefit the broader research community. This repository provides a streamlined framework for evaluating various prompt strategies and optimizing context retrieval systems for diverse LLMs. Detailed documentation and an online tutorial are provided to facilitate its adoption.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Retrieval Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section details the experiments conducted to evaluate the impact of different Context Retrieval (CR) components on the performance of Large Language Models (LLMs) in answering healthcare questions. We begin by establishing a baseline performance without any context retrieval (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS1" title="4.1. SC-CoT Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">4.1</span></a>), followed by a systematic investigation of individual components within the Self-Consistency with Chain-of-Thought (SC-CoT) framework. We then study the impact of adding external knowledge sources and the various components in the Medprompt architecture (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS2" title="4.2. Medprompt Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Based on these findings, we propose an optimized CR configuration (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS3" title="4.3. Proposed Scheme ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">4.3</span></a>) and compare its performance against state-of-the-art models (§<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.SS4" title="4.4. State-of-the-art Comparison ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>SC-CoT Experiments</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T1" title="Table 1 ‣ 4.1. SC-CoT Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">1</span></a> presents the baseline performance of Llama3-Aloe-8B-Alpha, our primary evaluation model, on four benchmark datasets using zero-shot next token prediction, CoT, and SC-CoT. This serves as a reference point for evaluating the subsequent improvements achieved through the integration of various components.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">CareQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">MedMCQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">MedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">MMLU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1.1">Zero-shot</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">67.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">58.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">62.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">72.76</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.3.2.1.1">CoT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">65.11</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">55.10</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">64.26</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">72.93</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.3.1.1">SC-CoT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2">67.64</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">56.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4">64.81</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5">73.68</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Baseline results of <span class="ltx_text ltx_font_italic" id="S4.T1.3.1">Llama3-Aloe-8B-Alpha</span> using zero-shot next token prediction, CoT, and SC-CoT with 5 ensembles.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">We investigate the impact of choice shuffling and a variable number of ensembles. The impact of <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">choice shuffling</span>, is among the most well-documented phenomenon <cite class="ltx_cite ltx_citemacro_citep">(Liévin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib16" title="">2024</a>)</cite>, as LLMs answering MCQs seem to be biased towards the first response (<span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">e.g., </span>”A”). Table  <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T2" title="Table 2 ‣ 4.1. SC-CoT Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">2</span></a> demonstrates the consistent benefits of incorporating choice shuffling within the SC-CoT framework, yielding improvements across all datasets and ensemble configurations. Based on these results, all subsequent experiments utilize choice shuffling to ensure unbiased evaluation.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.1.1">N</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.2.1">CareQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.3.1">MedMCQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.4.1">MedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T2.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1.5.1">MMLU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.2.1.1.1">5</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.2">67.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.3">56.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.4">64.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.2.1.5">73.68</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.2.1.1">5 + CS</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.2.2">+0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.2.3">+2.13</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.2.4">+0.24</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.3.2.5">+1.88</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.4.3.1.1">20</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.2">68.89</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.3">56.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.4">64.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.4.3.5">73.79</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T2.3.5.4.1.1">20 + CS</span></th>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.4.2">+1.08</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.4.3">+2.53</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.4.4">+3.53</td>
<td class="ltx_td ltx_align_center" id="S4.T2.3.5.4.5">+3.75</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Accuracy change when adding choice shuffling (CS) to the baseline with SC-CoT. <math alttext="N" class="ltx_Math" display="inline" id="S4.T2.2.m1.1"><semantics id="S4.T2.2.m1.1b"><mi id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><ci id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.m1.1e">italic_N</annotation></semantics></math> represents the number of ensembles of the SC component.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Next, we consider the second SC-CoT factor, the <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.1">number of ensembles</span> used in the self-consistency process. This choice has a significant effect, not only on task performance but also on computational cost and footprint. Such trade-off can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T3" title="Table 3 ‣ 4.1. SC-CoT Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">3</span></a>, where performance gains as footprint consistently grow up to 5-6% and 1.76Kg of <math alttext="CO_{2}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1.1"><semantics id="S4.SS1.p3.1.m1.1a"><mrow id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml"><mi id="S4.SS1.p3.1.m1.1.1.2" xref="S4.SS1.p3.1.m1.1.1.2.cmml">C</mi><mo id="S4.SS1.p3.1.m1.1.1.1" xref="S4.SS1.p3.1.m1.1.1.1.cmml">⁢</mo><msub id="S4.SS1.p3.1.m1.1.1.3" xref="S4.SS1.p3.1.m1.1.1.3.cmml"><mi id="S4.SS1.p3.1.m1.1.1.3.2" xref="S4.SS1.p3.1.m1.1.1.3.2.cmml">O</mi><mn id="S4.SS1.p3.1.m1.1.1.3.3" xref="S4.SS1.p3.1.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><apply id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1"><times id="S4.SS1.p3.1.m1.1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1.1"></times><ci id="S4.SS1.p3.1.m1.1.1.2.cmml" xref="S4.SS1.p3.1.m1.1.1.2">𝐶</ci><apply id="S4.SS1.p3.1.m1.1.1.3.cmml" xref="S4.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS1.p3.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS1.p3.1.m1.1.1.3.2">𝑂</ci><cn id="S4.SS1.p3.1.m1.1.1.3.3.cmml" type="integer" xref="S4.SS1.p3.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">CO_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p3.1.m1.1d">italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> respectively. The first five ensembles provide around 3.5% accuracy gains, while another five adds half of that. Gains beyond that are marginal, requiring up to twenty-five ensembles (fifteen more) to gain another 1%. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.F2" title="Figure 2 ‣ 4.1. SC-CoT Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">2</span></a> visually depicts the trend of diminishing returns.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">The recommended number of ensembles depends on the criticality of the task, and the available resources. In our experimentation, and unless otherwise specified, all SC experiments will be conducted using 5 ensembles.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1">N</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1">CareQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.1">MedMCQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.1">MedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.6.1">MMLU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T3.1.1.1"><math alttext="CO_{2}" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mi id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">C</mi><mo id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1.cmml">⁢</mo><msub id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml"><mi id="S4.T3.1.1.1.m1.1.1.3.2" xref="S4.T3.1.1.1.m1.1.1.3.2.cmml">O</mi><mn id="S4.T3.1.1.1.m1.1.1.3.3" xref="S4.T3.1.1.1.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1"></times><ci id="S4.T3.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.m1.1.1.2">𝐶</ci><apply id="S4.T3.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.1.1.1.m1.1.1.3.1.cmml" xref="S4.T3.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.1.1.1.m1.1.1.3.2.cmml" xref="S4.T3.1.1.1.m1.1.1.3.2">𝑂</ci><cn id="S4.T3.1.1.1.m1.1.1.3.3.cmml" type="integer" xref="S4.T3.1.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">CO_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.1.2.1.1">1</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">65.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">55.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4">64.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5">72.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.6">0.08 Kg</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.3.2.1.1">3</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">+1.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">+1.58</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4">+3.30</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5">+0.32</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.6">0.20 Kg</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.1.1">5</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2">+3.36</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3">+3.56</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.4">+4.40</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.5">+2.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.6">0.33 Kg</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.5.4.1.1">10</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.2">+5.84</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.3">+4.78</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.4">+4.24</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.5">+4.51</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.6">0.69 Kg</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.6.5.1.1">15</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.2">+6.26</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.3">+4.57</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.4">+4.08</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.5">+5.27</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.6">1.02 Kg</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.7.6.1.1">20</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.2">+6.42</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.3">+5.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.4">+4.90</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.5">+5.34</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.6">1.38 Kg</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.8.7.1.1">25</span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.2">+6.56</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.3">+5.33</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.4">+5.18</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.5">+5.59</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.6">1.76 Kg</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Accuracy change of SC-CoT with a variable number of ensembles, when compared to the baseline without SC (N=1). <math alttext="CO_{2}" class="ltx_Math" display="inline" id="S4.T3.3.m1.1"><semantics id="S4.T3.3.m1.1b"><mrow id="S4.T3.3.m1.1.1" xref="S4.T3.3.m1.1.1.cmml"><mi id="S4.T3.3.m1.1.1.2" xref="S4.T3.3.m1.1.1.2.cmml">C</mi><mo id="S4.T3.3.m1.1.1.1" xref="S4.T3.3.m1.1.1.1.cmml">⁢</mo><msub id="S4.T3.3.m1.1.1.3" xref="S4.T3.3.m1.1.1.3.cmml"><mi id="S4.T3.3.m1.1.1.3.2" xref="S4.T3.3.m1.1.1.3.2.cmml">O</mi><mn id="S4.T3.3.m1.1.1.3.3" xref="S4.T3.3.m1.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.m1.1c"><apply id="S4.T3.3.m1.1.1.cmml" xref="S4.T3.3.m1.1.1"><times id="S4.T3.3.m1.1.1.1.cmml" xref="S4.T3.3.m1.1.1.1"></times><ci id="S4.T3.3.m1.1.1.2.cmml" xref="S4.T3.3.m1.1.1.2">𝐶</ci><apply id="S4.T3.3.m1.1.1.3.cmml" xref="S4.T3.3.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.3.m1.1.1.3.1.cmml" xref="S4.T3.3.m1.1.1.3">subscript</csymbol><ci id="S4.T3.3.m1.1.1.3.2.cmml" xref="S4.T3.3.m1.1.1.3.2">𝑂</ci><cn id="S4.T3.3.m1.1.1.3.3.cmml" type="integer" xref="S4.T3.3.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.m1.1d">CO_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.m1.1e">italic_C italic_O start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> indicates the associated footprint.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="296" id="S4.F2.g1" src="extracted/5873777/ensembles_trend.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Trends on the accuracy gain obtained by using an increasing number of ensembles (horizontal axis) in a SC-CoT setup.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Medprompt Experiments</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">At this point, we extend the SC-CoT scheme with retrieval components, exploiting external data sources. This will include the main elements included in Medprompt: An <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">embedding</span> model, a <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">database</span>, and <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.3">reranker</span> model. The <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4">embedding</span> model encodes both input and database items before computing their similarity scores. For this component, we consider four different models, ranging in embedding size and in number of parameters. We also consider whether they have been specialized in the healthcare domain or not. These properties are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T4" title="Table 4 ‣ 4.2. Medprompt Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">4</span></a>, and their performance in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T5" title="Table 5 ‣ 4.2. Medprompt Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">5</span></a>. Results show all models achieve comparable performance in most datasets, with no embedding model clearly outperforming the rest. As a result, we select the healthcare-specific, cheaper model PubMedBERT to be the embedding model for our future experiments.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.2.1" style="font-size:90%;">Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.3.1" style="font-size:90%;">Emb. size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.4.1" style="font-size:90%;">Params.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.2.1.1.1" style="font-size:90%;">PubMedBERT <cite class="ltx_cite ltx_citemacro_citep">(Mezzetti, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib19" title="">2023</a>)</cite></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2"><span class="ltx_text" id="S4.T4.1.2.1.2.1" style="font-size:90%;">Medical</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3"><span class="ltx_text" id="S4.T4.1.2.1.3.1" style="font-size:90%;">768</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4"><span class="ltx_text" id="S4.T4.1.2.1.4.1" style="font-size:90%;">109M</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.3.2.1.1" style="font-size:90%;">MedCPT <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib13" title="">2023</a>)</cite></span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2"><span class="ltx_text" id="S4.T4.1.3.2.2.1" style="font-size:90%;">Medical</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.3"><span class="ltx_text" id="S4.T4.1.3.2.3.1" style="font-size:90%;">768</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.4"><span class="ltx_text" id="S4.T4.1.3.2.4.1" style="font-size:90%;">109M</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.4.3.1.1" style="font-size:90%;">UAE-Large-V1 <cite class="ltx_cite ltx_citemacro_citep">(Li and Li, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib15" title="">2023</a>)</cite></span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.2"><span class="ltx_text" id="S4.T4.1.4.3.2.1" style="font-size:90%;">General</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.3"><span class="ltx_text" id="S4.T4.1.4.3.3.1" style="font-size:90%;">1024</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.4"><span class="ltx_text" id="S4.T4.1.4.3.4.1" style="font-size:90%;">335M</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.5.4.1.1" style="font-size:90%;">SFR-Mistral <cite class="ltx_cite ltx_citemacro_citep">(Meng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib18" title="">2024</a>)</cite></span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.2"><span class="ltx_text" id="S4.T4.1.5.4.2.1" style="font-size:90%;">General</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.3"><span class="ltx_text" id="S4.T4.1.5.4.3.1" style="font-size:90%;">4096</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.4"><span class="ltx_text" id="S4.T4.1.5.4.4.1" style="font-size:90%;">7B</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Properties of the models used to embed the questions.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.1">Embedding</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.2.1">CareQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.3.1">MedMCQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.4.1">MedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T5.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.5.1">MMLU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T5.1.2.1.1">PubMedBERT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.2">68.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.3">59.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.4">69.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.2.1.5">75.55</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.3.2.1">MedCPT</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.2">68.81</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.3">59.29</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.4">67.16</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.3.2.5">75.44</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.4.3.1">UAE-Large-V1</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.2">68.08</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.3">59.53</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.4">69.05</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.4.3.5">76.70</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T5.1.5.4.1">SFR-Mistral</th>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.2">68.61</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.3">60.60</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.4">70.15</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.5.4.5">73.33</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>Accuracy for each embedding model on each dataset. We set 5 as the number of ensembles and few-shot examples, choice shuffling is activated. CoT database in use.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The second retrieval component is the <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">database</span>. That is the documental source from which pieces of text are extracted and introduced into the model prompt for contextualization. Both the quality and the diversity of those databases have a large impact on performance. To test this hypothesis we test two setups. First, a smaller database, composed by the validation set of each dataset. For CareQA (which lacks a validation split) we use the MedMCQA validation split as database. The second setup includes a larger and augmented database. Instead of the validation split, we use the training splits of MedMCQA  (180K QA pairs, also used for CareQA and MMLU) and MedQA (10K QA pairs).</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">In Medprompt, these samples are enhanced using CoT. In addition, we consider ToT as well. In both cases, samples are enhanced by prompting an LLM chosen for its instruction-following capability. For CoT, we prompted the Mixtral-8x7B <cite class="ltx_cite ltx_citemacro_citep">(AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib3" title="">2024</a>)</cite> model with the question, the possible answers, and the correct choice, and then asked to analyse each option individually, to explain the answer through detailed reasoning, and to end with a re-identification of the selected option (which is tested for validity). For ToT, we followed the same approach but used Llama-3.1-70B-Instruct to generate the answers. We adapted the original ToT prompt to simulate three logical experts collaborating to answer the question. The size of both databases is exactly the same.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">While the MedMCQA and MedQA experiments study the impact of size and quality in databases, the experiments on CareQA and MMLU add a factor of generalization (by using a DB from a different source). Results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T6" title="Table 6 ‣ 4.2. Medprompt Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">6</span></a>. In three out of four cases, the synthetically enhanced data improves the performance of the retrieval system. Even when the database comes from a different source, the extended database contributes to increase accuracy. Between CoT and ToT, the first outperforms the second in three out of four datasets. All further experiments will make use of the CoT extended databases.</p>
</div>
<figure class="ltx_table" id="S4.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1.1">Database</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.2.1">MedMCQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.3.1">MedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.4.1">CareQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T6.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.5.1">MMLU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T6.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.2.1.1.1">Validation</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.2">68.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.3">59.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.4">69.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T6.1.2.1.5">75.55</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.3.2.1.1">Train+CoT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.2">+7.15</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.3">-1.26</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.4">+0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.3.2.5">+3.23</td>
</tr>
<tr class="ltx_tr" id="S4.T6.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T6.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T6.1.4.3.1.1">Train+ToT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.2">-1.83</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.3">+4.40</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.4">+0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T6.1.4.3.5">+1.88</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6. </span>Accuracy change when extending the database through CoT and ToT (MedMCQA and MedQA train splits). Also when using a database extended with CoT/ToT coming from a different source (CareQA and MMLU use MedMCQA train split as database).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">The final component we study is the use of a <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.1">reranker</span> model. The role of the reranker is to sort the most similar items retrieved from the database, which yields performance boosts in certain domains <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib31" title="">2023a</a>)</cite>. To that end, we use the MedCPT-Cross-Encoder model. That is a contrastively pre-trained transformer, tuned on PubMed information retrieval. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T7" title="Table 7 ‣ 4.2. Medprompt Experiments ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">7</span></a> shows the inconsistent performance gains achieved with the reranker, leading us to exclude it from further experimentation due to its added computational cost.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.1.1.1">
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.1.1">CareQA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.2.1">MedMCQA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.3.1">MedQA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T7.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.4.1">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.2.2.1">-0.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.2.2.2">+0.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.2.2.3">+1.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T7.1.2.2.4">-1.35</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7. </span>Accuracy change when adding the reranker, sorting the samples retrieved from the database.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Proposed Scheme</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Based on our empirical findings, we propose an optimized context retrieval configuration that follows the Medprompt (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S3.F1" title="Figure 1 ‣ 3. Methods ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">1</span></a>) scheme. The main component removed is the reranker model. The proposed setup utilizes the following:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Choice Shuffling</span>: Enabled to mitigate position bias.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Number of Ensembles</span>: 5 as the default, as this provides the best trade-off between performance gains and computational cost associated. 20 for benchmarking.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Database</span>: CoT-augmented training sets, providing the most comprehensive and effective knowledge source.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Embedding Model</span>: PubMedBERT, a small and efficient healthcare-specific embedding model, is chosen.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i5.p1.1.1">Reranking Model</span>: Excluded due to its inconsistent performance gains and added computational overhead.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">This setup is the one provided by default in the software library released in association with this work.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>State-of-the-art Comparison</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In this section, we benchmark the performance of our optimized CR system against a diverse set of open-source state-of-the-art general-purpose and healthcare-specific LLMs, including models of varying sizes and architectures.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Aloe-8B</span>: A fine-tuned version of Llama3 8B for the healthcare domain, tuned with multi-choice QA data. The retrieval system was optimized for this model, its results may be biased.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Llama-3.1-8B/70B</span> <cite class="ltx_cite ltx_citemacro_citep">(et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib4" title="">2024a</a>)</cite>: Latest models released by Meta in 2024. Instruct versions are tuned using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Qwen-2-7B/72B</span> <cite class="ltx_cite ltx_citemacro_citep">(et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib5" title="">2024b</a>)</cite>: Instructed versions of the new series of Qwen large language models.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i4.p1.1.1">Mistral-7B-Instruct-v0.3</span> <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib11" title="">2023</a>)</cite>: Third version of the 7B model developed by Mistral AI.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i5.p1.1.1">Gemma-2-27B-it</span> <cite class="ltx_cite ltx_citemacro_citep">(Team and et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib28" title="">2024</a>)</cite>: Instruct tuned version of the larger model from the Gemma family, which is a family of state-of-the-art open models from Google.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i6.p1">
<p class="ltx_p" id="S4.I2.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i6.p1.1.1">Yi-1.5B-34B-Chat-16K</span> <cite class="ltx_cite ltx_citemacro_citep">(AI et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib2" title="">2024</a>)</cite>: The Yi series of models are a family of LLMs trained from scratch by 01.AI. Yi-1.5 is an upgraded version of Yi, continuously pre-trained on top of it. The chat version is the instruct-tuned version.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">For each model in this list, we conduct the same evaluation, with and without the context retrieval setup. Results, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T8" title="Table 8 ‣ 4.4. State-of-the-art Comparison ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">8</span></a>, indicate a unanimous boost in performance in all datasets and models. The gains are generalized but non-homogeneous. These are higher on less-performing models, clearly influenced by both the smaller model size and the larger room for improvement.</p>
</div>
<figure class="ltx_table" id="S4.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T8.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.1.1">Model</span></th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.2.1">CareQA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.3.1">MedMCQA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.4.1">MedQA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.5.1">MMLU</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.2.2.1.1">Mistral-7B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.2.2.2">60.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.2.2.3">48.22</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.2.2.4">52.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.2.2.5">66.05</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.3.3.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.3.3.2">+0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.3.3.3">+12.24</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.3.3.4">+9.98</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.3.3.5">+8.55</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.4.4.1.1">Qwen2-7B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.4.4.2">68.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.4.4.3">55.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.4.4.4">57.03</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.4.4.5">73.41</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.5.5.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.5.5.2">+1.28</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.5.5.3">+8.27</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.5.5.4">+4.71</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.5.5.5">+3.57</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.6.6.1.1">Aloe-8B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.6.6.2">67.57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.6.6.3">58.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.6.6.4">62.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.6.6.5">72.76</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.7.7.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.7.7.2">+4.11</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.7.7.3">+8.86</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.7.7.4">+9.98</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.7.7.5">+6.81</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.8.8.1.1">Llama-3.1-8B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.8.8.2">70.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.8.8.3">59.31</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.8.8.4">63.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.8.8.5">75.73</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.9.9.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.9.9.2">+3.36</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.9.9.3">+8.85</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.9.9.4">+8.88</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.9.9.5">+5.45</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.10.10.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.10.10.1.1">Gemma-2-27B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.10.10.2">78.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.10.10.3">61.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.10.10.4">66.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.10.10.5">81.70</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.11.11.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.11.11.2">+0.05</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.11.11.3">+8.77</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.11.11.4">+2.75</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.11.11.5">+2.19</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.12.12.1.1">Yi-1.5-34B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.12.12.2">73.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.12.12.3">57.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.12.12.4">61.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.12.12.5">78.69</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.13.13.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.13.13.2">+2.33</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.13.13.3">+8.68</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.13.13.4">+10.37</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.13.13.5">+4.12</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.14.14.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.14.14.1.1">Qwen2-72B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.14.14.2">83.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.14.14.3">69.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.14.14.4">76.83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.14.14.5">86.51</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.15.15.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.15.15.2">+2.12</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.15.15.3">+4.57</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.15.15.4">+2.44</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.15.15.5">+2.41</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.16.16.1"><span class="ltx_text ltx_font_bold" id="S4.T8.1.16.16.1.1">Llama-3.1-70B</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.16.16.2">83.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.16.16.3">72.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.16.16.4">79.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.16.16.5">87.21</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T8.1.17.17.1">with CR</th>
<td class="ltx_td ltx_align_center" id="S4.T8.1.17.17.2">+4.07</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.17.17.3">+4.40</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.17.17.4">+4.79</td>
<td class="ltx_td ltx_align_center" id="S4.T8.1.17.17.5">+3.46</td>
</tr>
<tr class="ltx_tr" id="S4.T8.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T8.1.18.18.1">
<span class="ltx_text ltx_font_bold" id="S4.T8.1.18.18.1.1">Avg.</span> with CR</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.18.18.2">+2.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.18.18.3">+8.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.18.18.4">+6.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T8.1.18.18.5">+4.87</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8. </span>Accuracy of LLMs with and without the context retrieval components proposed, when evaluated on multi-choice medical QA.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">Performance improvement seems to be highly dependent on the model family. Llama-based models (Llama3-Aloe 8B, Llama 3.1 8B/70B) seem to benefit particularly from context retrieval systems, while other models enjoy lower gains (<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.1">e.g., </span>Qwen2 72B). LLM performance on a retrieval system is in fact being consistently affected by training policies. There are also differences in gains across benchmarks, with some obtaining consistently higher benefits from context retrieval than others (<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.2">e.g., </span>MedMCQA). That is a remarkable difference taking into account the huge task similarities shared by all four datasets (<span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.3">i.e., </span>multiple-choice medical question answering). This points towards the importance of data sources, and the challenges of generalization.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Overall, the performance gain provided by the context retrieval scheme is highly valuable, as it reduces the costs of having highly reliable healthcare systems. It shows a well-tuned system based on small LLMs reaching the accuracy levels of much bigger models.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Private Comparison</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">To contextualize our results further, we include performance data reported for prominent private models, not been reproduced by this work. The open models under study include:</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<ul class="ltx_itemize" id="S4.I3">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.1">GPT-4</span> <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib21" title="">2024</a>; Nori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib20" title="">2023</a>)</cite>: Developed by OpenAI, with an undisclosed number of parameters but estimated to be at least in the hundreds of billions. We report GPT-4 results on these benchmarks with Medprompt.</p>
</div>
</li>
<li class="ltx_item" id="S4.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I3.i2.p1">
<p class="ltx_p" id="S4.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i2.p1.1.1">MedPalm-2</span> <cite class="ltx_cite ltx_citemacro_citep">(Singhal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib27" title="">2023</a>)</cite>: Developed by Google. We report MedPalm-2, likely based on the Palm-2 model, with Ensemble refinement as a prompting technique.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T9" title="Table 9 ‣ 4.5. Private Comparison ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">9</span></a> presents a consolidated leaderboard, sorted by average performance across available datasets, by unifying the results shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S4.T8" title="Table 8 ‣ 4.4. State-of-the-art Comparison ‣ 4. Retrieval Experiments ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">8</span></a> with those reported for private models. This comparison reveals that our optimized CR configuration not only boosts the accuracy of open-source models but also enables them to achieve performance levels comparable to much larger private models. In particular, when augmented with our CR system, the Llama-3.1-70B and Qwen-2-72B models demonstrate competitive performance with Google’s MedPalm-2 or OpenAI’s GPT4.</p>
</div>
<figure class="ltx_table" id="S4.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T9.1.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row" id="S4.T9.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T9.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.2.1">RAG</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.3.1">MedMCQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.4.1">MedQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T9.1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.5.1">MMLU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T9.1.1.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.1.1.6.1">Avg.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.1.2.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="S4.T9.1.2.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.2.1.1.1">GPT-4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T9.1.2.1.2" style="padding-left:3.0pt;padding-right:3.0pt;">MP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.2.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.2.1.3.1">79.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.2.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.2.1.4.1">90.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T9.1.2.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.2.1.5.1">94.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T9.1.2.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.2.1.6.1">87.83</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T9.1.3.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.3.2.1.1">Llama-3.1-70B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T9.1.3.2.2" style="padding-left:3.0pt;padding-right:3.0pt;">CR</th>
<td class="ltx_td ltx_align_center" id="S4.T9.1.3.2.3" style="padding-left:3.0pt;padding-right:3.0pt;">76.57</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.3.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">84.60</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.1.3.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">90.67</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.3.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">83.94</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.4.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T9.1.4.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.4.3.1.1">MedPalm-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T9.1.4.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">ER</th>
<td class="ltx_td ltx_align_center" id="S4.T9.1.4.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.4.3.3.1">72.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.4.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.4.3.4.1">85.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.1.4.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.4.3.5.1">89.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.4.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.4.3.6.1">82.36</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.5.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T9.1.5.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.5.4.1.1">Qwen2-72B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T9.1.5.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">CR</th>
<td class="ltx_td ltx_align_center" id="S4.T9.1.5.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">73.89</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.5.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">79.26</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.1.5.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">88.92</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.5.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">80.69</td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.6.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T9.1.6.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.6.5.1.1">GPT-4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T9.1.6.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">5S</th>
<td class="ltx_td ltx_align_center" id="S4.T9.1.6.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.6.5.3.1">72.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.6.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.6.5.4.1">81.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.1.6.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.6.5.5.1">87.40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.6.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.6.5.6.1">80.40</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.7.6">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T9.1.7.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.7.6.1.1">MedPalm-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T9.1.7.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">5S</th>
<td class="ltx_td ltx_align_center" id="S4.T9.1.7.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.7.6.3.1">71.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.7.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.7.6.4.1">79.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.1.7.6.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.7.6.5.1">87.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.7.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T9.1.7.6.6.1">79.60</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.1.8.7">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="S4.T9.1.8.7.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T9.1.8.7.1.1">Gemma-2-27B</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T9.1.8.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">CR</th>
<td class="ltx_td ltx_align_center" id="S4.T9.1.8.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">70.38</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.8.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">69.68</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T9.1.8.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">83.89</td>
<td class="ltx_td ltx_align_center" id="S4.T9.1.8.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">74.65</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9. </span>Accuracy of top performing models in medical MCQA, sorted by average performance. MP: Medprompt. CR: Context Retrieval. ER: Ensemble Refinement (Google’s custom prompt technique). 5S: Five-shot Underlined values are reported by others <cite class="ltx_cite ltx_citemacro_citep">(Nori et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib20" title="">2023</a>; Singhal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib27" title="">2023</a>)</cite>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Open-Ended Answer Generation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">While multiple-choice question answering (MCQA) benchmarks have been valuable in evaluating Large Language Models (LLMs) for medical applications, they fail to fully capture the complexities of real-world clinical scenarios. In practice, healthcare professionals often need to formulate comprehensive answers without pre-defined options. This necessitates a shift towards open-ended question-answering capabilities in medical AI systems.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our preliminary analysis revealed a significant performance gap when transitioning from multiple-choice to open-ended formats. Table  <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.T10" title="Table 10 ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">10</span></a> illustrates this disparity, showing a substantial decrease in accuracy of 10% for the Llama-3.1-8B-Instruct model on the MedQA dataset when transitioning from multiple choice questions (MCQs) to open-ended (OE) questions. Surprisingly, the incorporation of Chain-of-Thought (CoT) and Tree-of-Thought (ToT) reasoning did not improve performance for open-ended questions, contrary to their effectiveness in other domains.</p>
</div>
<figure class="ltx_table" id="S5.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T10.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S5.T10.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T10.1.1.1.1.1">K</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T10.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T10.1.1.1.2.1">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T10.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T10.1.1.1.3.1">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T10.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T10.1.2.1.1">0</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T10.1.2.1.2">Multiple-choice</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T10.1.2.1.3">63.71</td>
</tr>
<tr class="ltx_tr" id="S5.T10.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T10.1.3.2.1">0</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T10.1.3.2.2">Open-Ended</td>
<td class="ltx_td ltx_align_center" id="S5.T10.1.3.2.3">53.34</td>
</tr>
<tr class="ltx_tr" id="S5.T10.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T10.1.4.3.1">5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T10.1.4.3.2">CoT Open-Ended</td>
<td class="ltx_td ltx_align_center" id="S5.T10.1.4.3.3">52.40</td>
</tr>
<tr class="ltx_tr" id="S5.T10.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T10.1.5.4.1">5</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T10.1.5.4.2">ToT Open-Ended</td>
<td class="ltx_td ltx_align_center" id="S5.T10.1.5.4.3">51.93</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10. </span>Baseline results of Llama-3.1-8B-Instruct for the MedQA dataset</figcaption>
</figure>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">To address this gap, we propose OpenMedprompt, an extension of our optimized retrieval system specifically designed for open-ended medical question answering. We introduce the methodology in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS1" title="5.1. OpenMedprompt ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">5.1</span></a>, our modifications for datasets and databases for OE generation in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS2" title="5.2. Dataset and Database Construction ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">5.2</span></a>, the evaluation procedure in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS3" title="5.3. Experiments ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">5.3</span></a> and the results and discussion in §<a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.SS4" title="5.4. Results and Discussion ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">5.4</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>OpenMedprompt</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">OpenMedprompt adapts the Medprompt architecture for open-ended question answering. We replace the MCQA database with an OE-QA database and remove components specific to multiple-choice formats. We propose two strategies for consensus-building and answer refinement:</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.3"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.3.1">OpenMedprompt with Ensemble Refining (OM-ER)</span>:
This strategy leverages the diversity of multiple generated answers to produce a refined and more accurate final response. It involves generating <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.1d">italic_N</annotation></semantics></math> initial answers with randomized temperature and top_p parameters, incorporating <math alttext="K" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mi id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><ci id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_K</annotation></semantics></math> relevant examples from the database into the prompt. Then, the LLM synthesizes these <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p2.3.m3.1"><semantics id="S5.SS1.p2.3.m3.1a"><mi id="S5.SS1.p2.3.m3.1.1" xref="S5.SS1.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.3.m3.1b"><ci id="S5.SS1.p2.3.m3.1.1.cmml" xref="S5.SS1.p2.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.3.m3.1d">italic_N</annotation></semantics></math> answers into a single, refined response.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.2.1">OpenMedprompt with Self-reflection(OM-SR)</span> <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib26" title="">2023</a>; Renze and Guven, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib24" title="">2024</a>)</cite>:
This strategy employs a feedback loop to improve the generated answer. It begins by generating an initial answer using the <math alttext="K" class="ltx_Math" display="inline" id="S5.SS1.p3.1.m1.1"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.1.m1.1d">italic_K</annotation></semantics></math> most similar examples from the database. Then, it performs <math alttext="N" class="ltx_Math" display="inline" id="S5.SS1.p3.2.m2.1"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p3.2.m2.1d">italic_N</annotation></semantics></math> iterations of self-reflection, where the model generates feedback on its previous response and produces an improved answer based on this feedback. We integrate attribute scores from ArmoRM-Llama3-8B <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib29" title="">[n. d.]</a>)</cite>, a reward model along with the critique model’s reflection as an external feedback to guide answer generation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">The ArmoRM-Llama3-8B reward model provides reward scores across 19 attributes. Out of the nineteen total, we give the generation model only the following scores <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.1">ultrafeedback-truthfulness</span>, <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.2">ultrafeedback-honesty</span>,<span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.3">ultrafeedback-helpfullness</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.4">prometheus-score</span> which have a good correlation with correct responses.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.F3" title="Figure 3 ‣ 5.2. Dataset and Database Construction ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.F4" title="Figure 4 ‣ 5.2. Dataset and Database Construction ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">4</span></a> illustrate the architecture of OM-ER and OM-SR, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Dataset and Database Construction</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We utilized the MedQA dataset, which is particularly suitable for this task as it contains medical questions that can be answered without providing multiple-choice options, with minimal or no rephrasing of the original question required, making it ideal for testing our proposed framework.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To create a robust context retrieval system, we construct two distinct databases using the MedQA training set. We use LLaMA-3.1-70B-Instruct to generate CoT and ToT answers for each question in the training set. Both databases are carefully curated to ensure the accuracy of the generated responses and their alignment with the ground truth answers.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="211" id="S5.F3.g1" src="extracted/5873777/openmedprompt_merge.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Components of the OpenMedprompt with Ensemble Refining (OM-ER) system.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S5.F4.g1" src="extracted/5873777/openmedprompt_arch.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Components of the OpenMedprompt with Self-reflection(OM-SR) system.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Experiments</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We evaluate the performance of OM-ER and OM-SR using an automated LLM-based judge (LLaMA-3.1-70B-Instruct). This judge assesses the correctness of the generated open-ended answers by comparing them to the ground truth answers.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.T11" title="Table 11 ‣ 5.3. Experiments ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">11</span></a> presents the results of our experiments of OM-ER using the CoT and ToT databases. We see that both CoT and ToT databases show improvement over the baseline open-ended performance. ToT generally outperforms CoT, with the best result (60.02%) achieved using 20 ensembles.</p>
</div>
<figure class="ltx_table" id="S5.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T11.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T11.3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T11.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T11.3.1.1.1.1">N</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T11.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T11.3.1.1.2.1">CoT-Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T11.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T11.3.1.1.3.1">ToT-Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T11.3.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T11.3.2.1.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T11.3.2.1.2">56.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T11.3.2.1.3">57.82</td>
</tr>
<tr class="ltx_tr" id="S5.T11.3.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T11.3.3.2.1">5</th>
<td class="ltx_td ltx_align_center" id="S5.T11.3.3.2.2">56.40</td>
<td class="ltx_td ltx_align_center" id="S5.T11.3.3.2.3">56.40</td>
</tr>
<tr class="ltx_tr" id="S5.T11.3.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T11.3.4.3.1">7</th>
<td class="ltx_td ltx_align_center" id="S5.T11.3.4.3.2">56.87</td>
<td class="ltx_td ltx_align_center" id="S5.T11.3.4.3.3">57.58</td>
</tr>
<tr class="ltx_tr" id="S5.T11.3.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T11.3.5.4.1">10</th>
<td class="ltx_td ltx_align_center" id="S5.T11.3.5.4.2">56.72</td>
<td class="ltx_td ltx_align_center" id="S5.T11.3.5.4.3">59.54</td>
</tr>
<tr class="ltx_tr" id="S5.T11.3.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T11.3.6.5.1">15</th>
<td class="ltx_td ltx_align_center" id="S5.T11.3.6.5.2">59.31</td>
<td class="ltx_td ltx_align_center" id="S5.T11.3.6.5.3">59.00</td>
</tr>
<tr class="ltx_tr" id="S5.T11.3.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T11.3.7.6.1">20</th>
<td class="ltx_td ltx_align_center" id="S5.T11.3.7.6.2">58.76</td>
<td class="ltx_td ltx_align_center" id="S5.T11.3.7.6.3">60.02</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11. </span>OM-ER accuracy results of Llama-3.1-8B-Instruct in MedQA dataset using the CoT and ToT database. <math alttext="N" class="ltx_Math" display="inline" id="S5.T11.2.m1.1"><semantics id="S5.T11.2.m1.1b"><mi id="S5.T11.2.m1.1.1" xref="S5.T11.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T11.2.m1.1c"><ci id="S5.T11.2.m1.1.1.cmml" xref="S5.T11.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T11.2.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S5.T11.2.m1.1e">italic_N</annotation></semantics></math> represents the number of ensembles.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Table  <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.T12" title="Table 12 ‣ 5.3. Experiments ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">12</span></a> shows the results for OM-SR with the CoT and ToT databases. We see that the OM-SR system shows more consistent improvement over the baseline compared to the OM-ER system. The CoT database performance peaks at N=15 (60.88%), outperforming ToT in this configuration. The ToT database on the other hand shows less variation across different N values, suggesting more stable performance.</p>
</div>
<figure class="ltx_table" id="S5.T12">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T12.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T12.3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T12.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T12.3.1.1.1.1">N</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T12.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T12.3.1.1.2.1">CoT-Accuracy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T12.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T12.3.1.1.3.1">ToT-Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T12.3.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T12.3.2.1.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T12.3.2.1.2">55.93</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T12.3.2.1.3">57.50</td>
</tr>
<tr class="ltx_tr" id="S5.T12.3.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T12.3.3.2.1">5</th>
<td class="ltx_td ltx_align_center" id="S5.T12.3.3.2.2">57.34</td>
<td class="ltx_td ltx_align_center" id="S5.T12.3.3.2.3">57.89</td>
</tr>
<tr class="ltx_tr" id="S5.T12.3.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T12.3.4.3.1">7</th>
<td class="ltx_td ltx_align_center" id="S5.T12.3.4.3.2">59.07</td>
<td class="ltx_td ltx_align_center" id="S5.T12.3.4.3.3">58.21</td>
</tr>
<tr class="ltx_tr" id="S5.T12.3.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T12.3.5.4.1">10</th>
<td class="ltx_td ltx_align_center" id="S5.T12.3.5.4.2">60.25</td>
<td class="ltx_td ltx_align_center" id="S5.T12.3.5.4.3">60.02</td>
</tr>
<tr class="ltx_tr" id="S5.T12.3.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T12.3.6.5.1">15</th>
<td class="ltx_td ltx_align_center" id="S5.T12.3.6.5.2">60.88</td>
<td class="ltx_td ltx_align_center" id="S5.T12.3.6.5.3">58.52</td>
</tr>
<tr class="ltx_tr" id="S5.T12.3.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T12.3.7.6.1">20</th>
<td class="ltx_td ltx_align_center" id="S5.T12.3.7.6.2">58.68</td>
<td class="ltx_td ltx_align_center" id="S5.T12.3.7.6.3">58.68</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12. </span>OM-SR accuracy results of Llama-3.1-8B-Instruct in MedQA dataset using the CoT and ToT database. <math alttext="N" class="ltx_Math" display="inline" id="S5.T12.2.m1.1"><semantics id="S5.T12.2.m1.1b"><mi id="S5.T12.2.m1.1.1" xref="S5.T12.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.T12.2.m1.1c"><ci id="S5.T12.2.m1.1.1.cmml" xref="S5.T12.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T12.2.m1.1d">N</annotation><annotation encoding="application/x-llamapun" id="S5.T12.2.m1.1e">italic_N</annotation></semantics></math> represents the number of refinement iterations.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S5.F5.g1" src="extracted/5873777/plot_k.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>OM-ER and OM-SR accuracy when changing the number of few-shots (K) with a fixed number of ensembles (5).</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">While we expect performance to increase when the number of few shots increases we notice that in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.F5" title="Figure 5 ‣ 5.3. Experiments ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">5</span></a> this isn’t the case for OM-ER ToT. This could be attributed to the effective context length of the LLM. ToT responses are very verbose and when increasing the number of few-shots the context size significantly grows making it difficult for the LLM to reason over this large context.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="355" id="S5.F6.g1" src="extracted/5873777/plot_n.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>OM-ER and OM-SR accuracy when changing the number of ensembles (N) with a fixed number of few-shots (5).</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1">We also see that the increase in the number of ensembles doesn’t consistently improve the accuracy in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#S5.F6" title="Figure 6 ‣ 5.3. Experiments ‣ 5. Open-Ended Answer Generation ‣ Boosting Healthcare LLMs Through Retrieved Context"><span class="ltx_text ltx_ref_tag">6</span></a>. OM-ER benefits from ensemble sizes up to 20 while for the OM-SR method ensemble size of 15 provides the best performance for CoT while ensemble size 10 provides the best performance for ToT.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Results and Discussion</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Our results demonstrate the effectiveness of OpenMedprompt in improving open-ended answer generation accuracy in the medical domain. Both OM-ER and OM-SR contribute to performance gains compared to the baseline. The choice of database and the number of retrieved examples also play a significant role in performance.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Specifically, we observe that OM-SR generally outperforms OM-ER across most configurations. This suggests that the iterative feedback loop and incorporation of reward model scores provide a more effective mechanism for refining the generated answers. The choice of database (CoT vs. ToT) also has an impact on the performance differently for each approach. OM-ER benefits more from the ToT database, while OM-SR shows stronger results with the CoT database at higher iteration counts. OM-SR often achieves good performance with fewer iterations compared to the number of ensembles required for OM-ER to reach similar accuracy levels. However, OM-ER might be preferred when speed and simplicity are prioritized, or when exploring diverse perspectives is beneficial.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Choosing the Right Configuration:</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Accuracy Priority</span>: Choose OM-SR, particularly with higher iteration counts (<math alttext="N\geq 10" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><mrow id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml"><mi id="S5.I1.i1.p1.1.m1.1.1.2" xref="S5.I1.i1.p1.1.m1.1.1.2.cmml">N</mi><mo id="S5.I1.i1.p1.1.m1.1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.1.cmml">≥</mo><mn id="S5.I1.i1.p1.1.m1.1.1.3" xref="S5.I1.i1.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><apply id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1"><geq id="S5.I1.i1.p1.1.m1.1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1.1"></geq><ci id="S5.I1.i1.p1.1.m1.1.1.2.cmml" xref="S5.I1.i1.p1.1.m1.1.1.2">𝑁</ci><cn id="S5.I1.i1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.I1.i1.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">N\geq 10</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">italic_N ≥ 10</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Complex Reasoning</span>: OM-SR’s self-reflection mechanism may be more effective for questions requiring intricate logical steps.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Speed and Simplicity</span>: OM-ER with moderate ensemble sizes (<math alttext="N=5-10" class="ltx_Math" display="inline" id="S5.I1.i3.p1.1.m1.1"><semantics id="S5.I1.i3.p1.1.m1.1a"><mrow id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml"><mi id="S5.I1.i3.p1.1.m1.1.1.2" xref="S5.I1.i3.p1.1.m1.1.1.2.cmml">N</mi><mo id="S5.I1.i3.p1.1.m1.1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S5.I1.i3.p1.1.m1.1.1.3" xref="S5.I1.i3.p1.1.m1.1.1.3.cmml"><mn id="S5.I1.i3.p1.1.m1.1.1.3.2" xref="S5.I1.i3.p1.1.m1.1.1.3.2.cmml">5</mn><mo id="S5.I1.i3.p1.1.m1.1.1.3.1" xref="S5.I1.i3.p1.1.m1.1.1.3.1.cmml">−</mo><mn id="S5.I1.i3.p1.1.m1.1.1.3.3" xref="S5.I1.i3.p1.1.m1.1.1.3.3.cmml">10</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><apply id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1"><eq id="S5.I1.i3.p1.1.m1.1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1.1"></eq><ci id="S5.I1.i3.p1.1.m1.1.1.2.cmml" xref="S5.I1.i3.p1.1.m1.1.1.2">𝑁</ci><apply id="S5.I1.i3.p1.1.m1.1.1.3.cmml" xref="S5.I1.i3.p1.1.m1.1.1.3"><minus id="S5.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1.3.1"></minus><cn id="S5.I1.i3.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S5.I1.i3.p1.1.m1.1.1.3.2">5</cn><cn id="S5.I1.i3.p1.1.m1.1.1.3.3.cmml" type="integer" xref="S5.I1.i3.p1.1.m1.1.1.3.3">10</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">N=5-10</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.p1.1.m1.1d">italic_N = 5 - 10</annotation></semantics></math>) offers a good balance of improved accuracy and computational efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Diverse Perspectives</span>: OM-ER inherently generates multiple initial answers, which can be valuable when diversity of thought is important.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">Task</span>: The OM-ER approach is not suitable when large output texts are expected, as models start running out of effective context windows when merging the responses to create a unified answer.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work underscores the significant potential of augmenting Large Language Models (LLMs) with context retrieval systems to enhance their accuracy and reliability in the healthcare domain. Our exploration of Self-Consistency with Chain-of-Thought (SC-CoT) components revealed substantial gains through choice shuffling and an optimal number of ensembles, striking a balance between performance and computational cost. Further investigation into the Medprompt architecture highlighted the effectiveness of small, healthcare-specific embedding models and the value of enriching databases with Chain-of-Thought augmented examples. Conversely, the inclusion of a reranking model was found to be computationally expensive with inconsistent benefits, leading us to recommend against its use.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our optimized context retrieval configuration, when applied to a diverse set of open-source LLMs, consistently boosted performance across multiple medical question-answering benchmarks. Notably, smaller models experienced the most significant improvements, showcasing the ability of well-tuned retrieval systems to bridge the performance gap between smaller open models and larger private alternatives. This finding has profound implications for democratizing access to high-performing healthcare AI systems, reducing reliance on resource-intensive large models. Moreover, our results demonstrate that open LLMs augmented with our optimized CR system can achieve accuracy comparable to, and in some cases surpassing, state-of-the-art private solutions like MedPalm-2 and GPT-4.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">Recognizing the limitations of multiple-choice question answering (MCQA) in mirroring real-world clinical scenarios, we extended our approach to develop OpenMedprompt, a novel framework for open-ended medical question answering. Two distinct strategies, OpenMedprompt with Ensemble Refining (OM-ER) and OpenMedprompt with Self-Reflection (OM-SR), were introduced and evaluated, revealing their effectiveness in improving open-ended answer generation accuracy. OM-SR, with its iterative feedback loop and integration of reward model scores, generally outperformed OM-ER, suggesting the potential of self-reflection mechanisms for complex medical reasoning.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">By releasing our custom software repository as an open-source resource, we aim to empower the research community to further explore and refine these techniques, contributing to the development of more accurate, accessible, and impactful LLM applications in healthcare.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Future Work</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Several avenues for future research remain, encompassing both the optimization of retrieval-augmented generation for multiple-choice questions and the further development of OpenMedprompt for open-ended answer generation.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Expanding Retrieval System Capabilities:
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Dynamic Retrieval:</span> Exploring dynamic retrieval techniques that adapt the number of retrieved examples based on the complexity of the question, potentially improving efficiency and accuracy.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.2">Multi-Database Integration:</span> Investigating the integration of multiple knowledge sources, such as medical ontologies or clinical guidelines, to enrich the retrieval database and enhance the LLM’s understanding of complex medical concepts.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.3">Cross-Lingual Retrieval:</span> Adapting the retrieval system to support multiple languages, facilitating broader access to medical information and enabling cross-lingual medical question answering.
Enhancing OpenMedprompt:
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.4">Hybrid Approaches:</span> Exploring combinations of OM-ER and OM-SR to leverage the strengths of both methods, potentially leading to a more robust and adaptable system for open-ended medical question answering.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.5">Advanced Reward Models:</span> Investigating more sophisticated reward models tailored specifically to medical knowledge evaluation, capturing nuanced aspects of medical reasoning, factual accuracy, and clinical relevance.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.6">Prompt Engineering:</span> Fine-tuning prompt structures for both initial answer generation and refinement stages, incorporating specific instructions, constraints, or contextual information to guide the LLM towards more accurate and comprehensive responses.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.7">Larger Models:</span> Evaluating the performance of OpenMedprompt with more powerful language models to assess scalability and explore the potential for further accuracy gains.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.8">Domain Adaptation:</span> Extending OpenMedprompt to other specialized domains beyond medicine, such as law or engineering, focusing on tailoring the retrieval database and reward models to the specific knowledge requirements of each domain.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Carbon Footprint</h3>
<figure class="ltx_table" id="S6.T13">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T13.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T13.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S6.T13.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T13.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T13.1.1.1.2.1">Time</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T13.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T13.1.1.1.3.1">Power consumption</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S6.T13.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T13.1.1.1.4.1">Footprint</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T13.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S6.T13.1.2.1.1">Medprompt</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T13.1.2.1.2">652.90</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T13.1.2.1.3">828.47</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T13.1.2.1.4">169.84</td>
</tr>
<tr class="ltx_tr" id="S6.T13.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S6.T13.1.3.2.1">OpenMedprompt</th>
<td class="ltx_td ltx_align_center" id="S6.T13.1.3.2.2">195.40</td>
<td class="ltx_td ltx_align_center" id="S6.T13.1.3.2.3">134.01</td>
<td class="ltx_td ltx_align_center" id="S6.T13.1.3.2.4">27.47</td>
</tr>
<tr class="ltx_tr" id="S6.T13.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T13.1.4.3.1">Total</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.1.4.3.2">848.30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.1.4.3.3">962.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T13.1.4.3.4">197.31</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13. </span>Computational cost of the experiments conducted in this study. Time is expressed in hours, power consumption in KWh and carbon footprint in kg/CO2.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Ethical Considerations</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">This work seeks to increase the factuality and reliability of LLM-based systems. However, any work that deals with generative models like LLMs should be done in awareness of the several ethical limitations that the technology entails. This includes the computational footprint (which is considered in this work), as well as the factuality (which is the main target of this work). Limitations related to the potential for impersonation, self-diagnose and other non-approved uses, are further discussed in publications related with healthcare model releases (<span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.1">e.g., </span> <cite class="ltx_cite ltx_citemacro_citep">(Gururajan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.15127v1#bib.bib8" title="">2024</a>)</cite>).
</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work has been partly funded by the AI4Europe projects from the European Union’s Horizon 2020 programme (Grant Agreements Nº951911 and Nº101070000), and by a SGR-GRE grant from the Generalitat de Catalunya (code 2021 SGR 01187).

</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
01. AI, :, and Alex Young et al. 2024.

</span>
<span class="ltx_bibblock">Yi: Open Foundation Models by 01.AI.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.04652 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.04652" title="">https://arxiv.org/abs/2403.04652</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI (2024)</span>
<span class="ltx_bibblock">
Mistral AI. 2024.

</span>
<span class="ltx_bibblock">Mixtral of Experts.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mixtral-of-experts/" title="">https://mistral.ai/news/mixtral-of-experts/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2024a)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey et al. 2024a.

</span>
<span class="ltx_bibblock">The Llama 3 Herd of Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2407.21783 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.21783" title="">https://arxiv.org/abs/2407.21783</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2024b)</span>
<span class="ltx_bibblock">
An Yang et al. 2024b.

</span>
<span class="ltx_bibblock">Qwen2 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2407.10671</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown et al. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.14165 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.14165" title="">https://arxiv.org/abs/2005.14165</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganzabal and Pérez (2024)</span>
<span class="ltx_bibblock">
Lucia Urcelay Ganzabal and Pablo Bernabeu Pérez. 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">careqa</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/HPAI-BSC/CareQA" title="">https://huggingface.co/datasets/HPAI-BSC/CareQA</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururajan et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, and Ulises Cortés Dario Garcia-Gasulla. 2024.

</span>
<span class="ltx_bibblock">Aloe: A Family of Fine-tuned Open Healthcare LLMs.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.01886 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.01886" title="">https://arxiv.org/abs/2405.01886</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hager et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Paul Hager, Friederike Jungmann, Robbie Holland, Kunal Bhagat, Inga Hubrecht, Manuel Knauer, Jakob Vielhauer, Marcus Makowski, Rickmer Braren, Georgios Kaissis, et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Evaluation and mitigation of the limitations of large language models in clinical decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">Nature medicine</em> (2024), 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2009.03300 [cs.CY]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2009.03300" title="">https://arxiv.org/abs/2009.03300</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.06825 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06825" title="">https://arxiv.org/abs/2310.06825</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Applied Sciences</em> 11, 14 (2021), 6421.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and Zhiyong Lu. 2023.

</span>
<span class="ltx_bibblock">MedCPT: Contrastive Pre-trained Transformers with large-scale PubMed search logs for zero-shot biomedical information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Bioinformatics</em> 39, 11 (2023), btad651.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.11401 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.11401" title="">https://arxiv.org/abs/2005.11401</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Li (2023)</span>
<span class="ltx_bibblock">
Xianming Li and Jing Li. 2023.

</span>
<span class="ltx_bibblock">AnglE-optimized Text Embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">preprint arXiv:2309.12871</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liévin et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. 2024.

</span>
<span class="ltx_bibblock">Can large language models reason about medical questions?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Patterns</em> 5, 3 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maharjan et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, and Ritankar Das. 2024.

</span>
<span class="ltx_bibblock">OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Scientific Reports</em> 14, 1 (2024), 14156.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024.

</span>
<span class="ltx_bibblock">SFR-Embedding-Mistral:Enhance Text Retrieval with Transfer Learning.

</span>
<span class="ltx_bibblock">Salesforce AI Research Blog.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.salesforceairesearch.com/sfr-embedded-mistral/" title="">https://blog.salesforceairesearch.com/sfr-embedded-mistral/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mezzetti (2023)</span>
<span class="ltx_bibblock">
David Mezzetti. 2023.

</span>
<span class="ltx_bibblock">Embeddings for Medical Literature.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://medium.com/neuml/embeddings-for-medical-literature" title="">https://medium.com/neuml/embeddings-for-medical-literature</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nori et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. 2023.

</span>
<span class="ltx_bibblock">Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2311.16452 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2311.16452" title="">https://arxiv.org/abs/2311.16452</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2024)</span>
<span class="ltx_bibblock">
OpenAI. 2024.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08774" title="">https://arxiv.org/abs/2303.08774</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal and Sankarasubbu (2024)</span>
<span class="ltx_bibblock">
Ankit Pal and Malaikannan Sankarasubbu. 2024.

</span>
<span class="ltx_bibblock">Gemini goes to med school: exploring the capabilities of multimodal large language models on medical challenge problems &amp; hallucinations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2402.07023</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022.

</span>
<span class="ltx_bibblock">MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2203.14371 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.14371" title="">https://arxiv.org/abs/2203.14371</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Renze and Guven (2024)</span>
<span class="ltx_bibblock">
Matthew Renze and Erhan Guven. 2024.

</span>
<span class="ltx_bibblock">Self-Reflection in LLM Agents: Effects on Problem-Solving Performance.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2405.06682 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2405.06682" title="">https://arxiv.org/abs/2405.06682</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savage et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Thomas Savage, Ashwin Nayak, Robert Gallo, Ekanath Rangan, and Jonathan H. Chen. 2024.

</span>
<span class="ltx_bibblock">Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">npj Digital Medicine</em> 7, 1 (2024), 20.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41746-024-01010-1" title="">https://doi.org/10.1038/s41746-024-01010-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.

</span>
<span class="ltx_bibblock">Reflexion: Language Agents with Verbal Reinforcement Learning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.11366 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.11366" title="">https://arxiv.org/abs/2303.11366</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and
Vivek Natarajan. 2023.

</span>
<span class="ltx_bibblock">Towards Expert-Level Medical Question Answering with Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.09617 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.09617" title="">https://arxiv.org/abs/2305.09617</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team and et al. (2024)</span>
<span class="ltx_bibblock">
Gemma Team and Morgane Riviere et al. 2024.

</span>
<span class="ltx_bibblock">Gemma 2: Improving Open Language Models at a Practical Size.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2408.00118 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2408.00118" title="">https://arxiv.org/abs/2408.00118</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. [n. d.].

</span>
<span class="ltx_bibblock">Interpretable Preferences via Multi-Objective Reward Modeling and Mixture-of-Experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:2406.12845</em> ([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Li Wang, Xi Chen, XiangWen Deng, Hao Wen, MingKe You, WeiZhi Liu, Qi Li, and Jian Li. 2024.

</span>
<span class="ltx_bibblock">Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">npj Digital Medicine</em> 7, 1 (2024), 41.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41746-024-01029-4" title="">https://doi.org/10.1038/s41746-024-01029-4</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xiaodan Wang, Lei Li, Zhixu Li, Xuwu Wang, Xiangru Zhu, Chengyu Wang, Jun Huang, and Yanghua Xiao. 2023a.

</span>
<span class="ltx_bibblock">Agree: Aligning cross-modal entities for image-text retrieval upon vision-language pre-trained models. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining</em>. 456–464.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b.

</span>
<span class="ltx_bibblock">Self-Consistency Improves Chain of Thought Reasoning in Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2203.11171 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2203.11171" title="">https://arxiv.org/abs/2203.11171</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2201.11903 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11903" title="">https://arxiv.org/abs/2201.11903</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.

</span>
<span class="ltx_bibblock">Tree of Thoughts: Deliberate Problem Solving with Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.10601 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.10601" title="">https://arxiv.org/abs/2305.10601</a>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 15:27:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
