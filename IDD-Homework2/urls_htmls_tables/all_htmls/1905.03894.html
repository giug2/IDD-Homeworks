<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1905.03894] Ship classification from overhead imagery using synthetic data and domain adaptation</title><meta property="og:description" content="In this paper, we revisit the problem of classifying ships (maritime vessels) detected from overhead imagery.
Despite the last decade of research on this very important and pertinent problem, it remains largely unsolve…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ship classification from overhead imagery using synthetic data and domain adaptation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Ship classification from overhead imagery using synthetic data and domain adaptation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1905.03894">

<!--Generated on Mon Feb 26 19:36:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Ship classification from overhead imagery 
<br class="ltx_break">using synthetic data and domain adaptation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chris M. Ward, Josh Harguess, Cameron Hilton 
<br class="ltx_break">Space and Naval Warfare Systems Center, Pacific (SSC PAC) 
<br class="ltx_break">San Diego, CA 92152, United States 
<br class="ltx_break">Emails: {cward, harguess, bhilton}@spawar.navy.mil
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">In this paper, we revisit the problem of classifying ships (maritime vessels) detected from overhead imagery.
Despite the last decade of research on this very important and pertinent problem, it remains largely unsolved.
One of the major issues with the detection and classification of ships and other objects in the maritime domain
is the lack of substantial ground truth data needed to train state-of-the-art machine learning algorithms. We address this issue by building a large (200k) synthetic image dataset using the Unity gaming engine
and 3D ship models. We demonstrate that with the use of synthetic data, classification performance increases
dramatically, particularly when there are very few annotated images used in training.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, a great deal of progress has been made in research areas within computer vision, particularly those with respect to object classification.
This has partly been due to the advancement of deep learning approaches in convolutional neural network architectures and the availability of large annotated datasets. However, real-world applications, such as ship classification from overhead imagery, have been slow to adopt these methods due to the lack of annotated data and the difficulty of real-world image conditions such as varying lighting conditions, view angles, and atmospheric effects.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Earlier work in ship classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> relied on classical approaches to object recognition, such as feature extraction using subspace projection and a support vector machine (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for classification. These approaches provided limited success on the practical problem of ship classification within a four-class problem (barge, cargo, container, tanker), but nevertheless underscored the importance and difficulty of the problem.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We revisit this work with a fresh look at the problem given recent advancements, particularly in two main areas. First, recent machine learning and computer vision architectures, such as Residual Networks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> paired with transfer learning, have reduced the requirement of large annotated datasets in the target problem area due to their power, flexibility, and ability to learn generalized features from large datasets such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Second, we build a new synthetic imagery database based on ship models taken from the same classes as the earlier work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. These synthetic images may be used to supplement the real-world image training and validation sets as shown in recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. In this paper, we test the ability of ResNet-34<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to be fine-tuned to real-world images of overhead ship imagery as well as the impact of utilizing synthetic imagery in the ship classification problem.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The paper is organized as follows: In Section <a href="#S2" title="II Methodology ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, we introduce our approach to the ship classification problem as well as the baseline approaches from previous work. Section <a href="#S3" title="III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> covers the datasets created and used, the details of the experimental setup, and the experimental results. We provide a discussion on the results in Section <a href="#S4" title="IV Discussion ‣ III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and conclude the paper in Section <a href="#S5" title="V Conclusion ‣ III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We assume a typical two-stage pipeline for the overall goal of vessel classification, which includes both ship
detection and classification. First, the ship or vessel of interest has been detected with an appropriate anomaly detection method or a ship detector trained on overhead imagery. Then, the resulting detection candidates are then fed to the second stage for classification, which is the focus of this work.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Machine learning algorithms are trained on our synthetic image data and then fine-tuned to real-world data taken from overhead images. We use previous work in ship classification as a baseline for these algorithms. Details of the methodology are provided below.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">The primary goal of our experimentation is to understand the utility of synthetic data in deep-learning-based classification of maritime vessels. We fine-tune a neural network with both synthetic imagery and real imagery and compare its performance to a variety of classic object recognition methods. We chose to use ResNet-34 as a basis for our deep learning experimentation, due to its high performance in classification tasks, as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. While other models available at the time of our experimentation demonstrate higher classification accuracy, ResNet-34 strikes a balance between accuracy and the number of operations required for a single forward pass<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. In each of our ResNet-34 experiments, we initialize the model with pre-computed ImageNet<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> weights. We repeated the following experiments on each of the data splits in Table <a href="#S3.SS2" title="III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>:</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">ResNet-34 fine-tuned with BCCT200</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In order to establish baseline performance of a convolutional neural network (CNN) on BCCT200, we performed conventional transfer-learning and fine-tuning of a CNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. ResNet-34 was initialized with pre-computed ImageNet weights and then end-to-end fine-tuned and tested on each of our defined BCCT200 data splits.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">ResNet-34 Fine-tuned with BCCT-Synth </span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this experiment we evaluate a CNN that is never exposed to real-world overhead imagery prior to test time; we test a ResNet-34 trained and validated only on synthetic vessel images against a set of real vessel images. We initialized ResNet-34 with pre-computed ImageNet weights and then end-to-end fine-tuned on BCCT-Synth until a validation accuracy of <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="99.99\%" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">99.99</mn><mo id="S2.SS2.p1.1.m1.1.1.1" xref="S2.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">99.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">99.99\%</annotation></semantics></math> was achieved. The network was then tested against each defined set of BCCT200 test images.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">ResNet-34 Synthetically-Tuned with Domain Adaptation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">This experiment extends the previous experiment. We initialized ResNet-34 with our precomputed BCCT-Synth weights and then end-to-end fine-tune on BCCT200 training image sets. We refer to this final fine-tuning phase as “domain adaptation.” The domain-adapted model was then tested against each defined set of BCCT200 test images.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Classic Object Recognition Methods</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">As a baseline to the above approaches, we revisit the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. While there are many feature extraction and classification methods utilized in that work, we will focus our baselines in this paper on the higher performing methods. For feature extraction, Hierarchical Multi-scale Local Binary Pattern (HMLBP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, Multi-linear Principal Components Analysis (MPCA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and Histograms of Oriented Gradients (HOG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> will be used. Our baseline classifiers will be the Support Vector Machine (SVM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and Sparse Representation-based Classification (SRC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. For baseline comparisons, we will use the following combinations of feature extraction and classification methods:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">HMLBP + SVM</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">MPCA + SVM</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">HOG + SRC</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">For more details into these approaches and the parameters used for our experimentation, please see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimentation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we introduce the datasets used in the experiments, the setup of the experiments, and the experimental results.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.4.1.1" class="ltx_text">III-A</span>1 </span>BCCT200 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">We will use the previously cited Barge, Cargo, Container, Tanker (BCCT200) dataset that consists of real-world overhead imagery chips of the aforementioned vessel classes, with each class consisting of <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mn id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">200</annotation></semantics></math> images. Specifically, we utilize the BCC200-resize subset of the dataset where the images are rotated, cropped, aligned and resized. Example images of each class in BCCT200 are shown in Figure <a href="#S3.F1" title="Figure 1 ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.4.1.1" class="ltx_text">III-A</span>2 </span>BCCT-Synth dataset</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.2" class="ltx_p">This synthetic dataset is comprised of <math id="S3.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="200k" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.1a"><mrow id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS2.p1.1.m1.1.1.2" xref="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml">200</mn><mo lspace="0em" rspace="0em" id="S3.SS1.SSS2.p1.1.m1.1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml">​</mo><mi id="S3.SS1.SSS2.p1.1.m1.1.1.3" xref="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.1b"><apply id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1"><times id="S3.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.2">200</cn><ci id="S3.SS1.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.1c">200k</annotation></semantics></math> labeled images of the same four generic vessel classes as BCCT200, captured from <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="15" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mn id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">15</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><cn type="integer" id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">15</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">15</annotation></semantics></math> virtual overhead sensors. BCCT-Synth features imagery with varying lighting conditions, as well as varying observation angles in order to capture off-nadir instances of vessels at sea. We also introduce real-world conditions to the data, such as varying weather, sea states, and clouds. Image diversity was increased by the addition of “class modifiers,” including: vessel wake, secondary vessels, and fenders. Example images of each class in BCCT-Synth are shown in Figure <a href="#S3.F2" title="Figure 2 ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Based on previous work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we used the Unity game engine as our modeling environment. After rendering, we process the Unity-generated RGB images in order to simulate the panchromatic spectrum of BCCT200. We performed histogram specification by setting the shape of the R,G,B channel histograms to match the spectral response of a panchromatic sensor. Our method of histogram matching consisted of redistribution of energy in the blue channel, and nonlinear stretch operations on red and green channels. We then performed grayscale-conversion resulting in a pseudo-panchromatic image set.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Training and Test Data Splits</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To understand the effects of data sparsity on various object recognition approaches, we test each method against five data split ratios. The split ratios, shown in Table <a href="#S3.SS2" title="III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>, indicate the percentage of training images and percentage of test images used during experimentation, respectively.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Additionally, in our experiments with ResNet-34, we used a <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">20</mn><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">20\%</annotation></semantics></math> holdout for validation data with the exception of the 1/99 split, where a single image was used to train, and a single image was used in validation.</p>
</div>
<figure id="S3.SS2.tab1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>BCCT200 training/test data split ratios and the number of images in each set. </figcaption>
<div id="S3.SS2.tab1.1" class="ltx_sectional-block">
<div id="S3.SS2.tab1.1.p1" class="ltx_para">
<span id="S3.SS2.tab1.1.p1.1" class="ltx_ERROR ltx_centering undefined">{tabu*}</span>
<p id="S3.SS2.tab1.1.p1.2" class="ltx_p ltx_align_center"><span id="S3.SS2.tab1.1.p1.2.1" class="ltx_text" style="font-size:90%;">to 0.48— X[c] — X[c] — X[c] —
Data Split % (Training / Test)  Number of Training images  Number of Test images
<br class="ltx_break">80/20  160  40
<br class="ltx_break">50/50  100  100
<br class="ltx_break">20/80  40  160
<br class="ltx_break">5/95  10  190
<br class="ltx_break">1/99  2  198
<br class="ltx_break"></span></p>
<p id="S3.SS2.tab1.1.p1.3" class="ltx_p ltx_align_center"><span id="S3.SS2.tab1.1.p1.3.1" class="ltx_text" style="font-size:90%;">To assess how our models generalize to an independent data set, each object recognition method was evaluated on five data shuffles. In Table <a href="#S3.SS3" title="III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a> we report the average performance across the five shuffled sets.</span></p>
</div>
<figure id="S3.F1" class="ltx_figure ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F0.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/barge.jpeg" id="S3.F0.sf1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="188" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(a) </span>Barge </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F0.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/cargo.jpeg" id="S3.F0.sf2.g1" class="ltx_graphics ltx_img_portrait" width="94" height="188" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(b) </span>Cargo </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F0.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/container.jpeg" id="S3.F0.sf3.g1" class="ltx_graphics ltx_img_portrait" width="94" height="188" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(c) </span>Container </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F0.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/tanker.jpeg" id="S3.F0.sf4.g1" class="ltx_graphics ltx_img_portrait" width="94" height="188" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(d) </span>Tanker </figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example images from the Barge, Cargo, Container, Tanker (BCCT200) dataset.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure ltx_align_center">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/barge_synth.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_img_portrait" width="90" height="180" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(a) </span>Synthetic Barge </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/cargo_synth.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_img_portrait" width="90" height="180" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(b) </span>Synthetic Cargo </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/container_synth.png" id="S3.F1.sf3.g1" class="ltx_graphics ltx_img_portrait" width="90" height="180" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(c) </span>Synthetic Container </figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/1905.03894/assets/figures/tanker_synth.png" id="S3.F1.sf4.g1" class="ltx_graphics ltx_img_portrait" width="90" height="180" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(d) </span>Synthetic Tanker </figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example images from the Synthetic Barge, Cargo, Container, Tanker (BCCT-Synth) dataset, which is created from 3D ship models within the Unity environment. The Synthetic Barge class features image modifiers like adjacent utility vessels and fenders (Figure <a href="#S3.F1.sf1" title="In Figure 2 ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1a</span></a>). The Synthetic Cargo, Container, and Tanker classes feature a wake modifier shown in Figure <a href="#S3.F1.sf4" title="In Figure 2 ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1d</span></a>. </figcaption>
</figure>
<section id="S3.SS3" class="ltx_subsection ltx_centering">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Experimental Results</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">The average test accuracy of 5-fold cross-validated results for each classification method under test are shown in Table </span><a href="#S3.SS3" title="III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a><span id="S3.SS3.p1.1.2" class="ltx_text" style="font-size:90%;"> and visualized in Figure </span><a href="#S3.F3" title="Figure 3 ‣ III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.SS3.p1.1.3" class="ltx_text" style="font-size:90%;">. ResNet-34, fine-tuned on synthetic imagery and adapted to real imagery, outperformed other tested methods in nearly every case.</span></p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#S5" title="V Conclusion ‣ III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">V</span></a><span id="S3.SS3.p2.1.2" class="ltx_text" style="font-size:90%;"> shows the improvement in classification accuracy attributed to the use of synthetic imagery in the training process. With respect to the BCCT200 dataset, we observed an increasing benefit from synthetic imagery as real training data grew more sparse. As shown in Table </span><a href="#S5" title="V Conclusion ‣ III-C Experimental Results ‣ III-B Training and Test Data Splits ‣ III Experimentation ‣ Ship classification from overhead imagery using synthetic data and domain adaptation" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">V</span></a><span id="S3.SS3.p2.1.3" class="ltx_text" style="font-size:90%;">, synthetic imagery yielded the largest performance boost when availability of real data was minimal, yielding a </span><math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="10.73" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mn mathsize="90%" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">10.73</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><cn type="float" id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">10.73</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">10.73</annotation></semantics></math><span id="S3.SS3.p2.1.4" class="ltx_text" style="font-size:90%;">% gain in classification accuracy.</span></p>
</div>
<figure id="S3.SS3.tab1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Average of 5-fold cross-validated results for each object recognition method under test. </figcaption>
<div id="S3.SS3.tab1.3" class="ltx_block">
<table id="S3.SS3.tab1.3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.SS3.tab1.3.1.1" class="ltx_tr">
<td id="S3.SS3.tab1.3.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">o 0.98— X[c] — X[c] — X[c] — X[c] — X[c] —X[c] — X[c] —
Data Split % (Training / Test)</span></td>
<td id="S3.SS3.tab1.3.1.1.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.1.2.1" class="ltx_text" style="font-size:90%;">HMLBP+SVM</span></td>
<td id="S3.SS3.tab1.3.1.1.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.1.3.1" class="ltx_text" style="font-size:90%;">MPCA+SVM</span></td>
<td id="S3.SS3.tab1.3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.1.4.1" class="ltx_text" style="font-size:90%;">HOG+SRC</span></td>
<td id="S3.SS3.tab1.3.1.1.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.1.5.1" class="ltx_text" style="font-size:90%;">ResNet-34 tuned on BCCT-Synth</span></td>
<td id="S3.SS3.tab1.3.1.1.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.1.6.1" class="ltx_text" style="font-size:90%;">ResNet-34 tuned on BCCT200</span></td>
<td id="S3.SS3.tab1.3.1.1.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;">
<span id="S3.SS3.tab1.3.1.1.7.1" class="ltx_text" style="font-size:90%;">ResNet-34 tuned on BCCT-Synth + BCCT200</span>
<br class="ltx_break">
</td>
</tr>
<tr id="S3.SS3.tab1.3.1.2" class="ltx_tr">
<td id="S3.SS3.tab1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.1.1" class="ltx_text" style="font-size:90%;">80/20</span></td>
<td id="S3.SS3.tab1.3.1.2.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.2.1" class="ltx_text" style="font-size:90%;">0.8500</span></td>
<td id="S3.SS3.tab1.3.1.2.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.3.1" class="ltx_text" style="font-size:90%;">0.82875</span></td>
<td id="S3.SS3.tab1.3.1.2.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.4.1" class="ltx_text" style="font-size:90%;">0.8150</span></td>
<td id="S3.SS3.tab1.3.1.2.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.5.1" class="ltx_text" style="font-size:90%;">0.5888</span></td>
<td id="S3.SS3.tab1.3.1.2.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.9688</span></td>
<td id="S3.SS3.tab1.3.1.2.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.2.7.1" class="ltx_text" style="font-size:90%;">0.9663</span></td>
</tr>
<tr id="S3.SS3.tab1.3.1.3" class="ltx_tr">
<td id="S3.SS3.tab1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.1.1" class="ltx_text" style="font-size:90%;">50/50</span></td>
<td id="S3.SS3.tab1.3.1.3.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.2.1" class="ltx_text" style="font-size:90%;">0.8145</span></td>
<td id="S3.SS3.tab1.3.1.3.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.3.1" class="ltx_text" style="font-size:90%;">0.7950</span></td>
<td id="S3.SS3.tab1.3.1.3.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.4.1" class="ltx_text" style="font-size:90%;">0.7840</span></td>
<td id="S3.SS3.tab1.3.1.3.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.5.1" class="ltx_text" style="font-size:90%;">0.5820</span></td>
<td id="S3.SS3.tab1.3.1.3.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.6.1" class="ltx_text" style="font-size:90%;">0.9525</span></td>
<td id="S3.SS3.tab1.3.1.3.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.3.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.9595</span></td>
</tr>
<tr id="S3.SS3.tab1.3.1.4" class="ltx_tr">
<td id="S3.SS3.tab1.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.1.1" class="ltx_text" style="font-size:90%;">20/80</span></td>
<td id="S3.SS3.tab1.3.1.4.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.2.1" class="ltx_text" style="font-size:90%;">0.7059</span></td>
<td id="S3.SS3.tab1.3.1.4.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.3.1" class="ltx_text" style="font-size:90%;">0.7375</span></td>
<td id="S3.SS3.tab1.3.1.4.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.4.1" class="ltx_text" style="font-size:90%;">0.7328</span></td>
<td id="S3.SS3.tab1.3.1.4.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.5.1" class="ltx_text" style="font-size:90%;">0.5856</span></td>
<td id="S3.SS3.tab1.3.1.4.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.6.1" class="ltx_text" style="font-size:90%;">0.9353</span></td>
<td id="S3.SS3.tab1.3.1.4.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.4.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.9469</span></td>
</tr>
<tr id="S3.SS3.tab1.3.1.5" class="ltx_tr">
<td id="S3.SS3.tab1.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.1.1" class="ltx_text" style="font-size:90%;">5/95</span></td>
<td id="S3.SS3.tab1.3.1.5.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.2.1" class="ltx_text" style="font-size:90%;">0.5653</span></td>
<td id="S3.SS3.tab1.3.1.5.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.3.1" class="ltx_text" style="font-size:90%;">0.5884</span></td>
<td id="S3.SS3.tab1.3.1.5.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.4.1" class="ltx_text" style="font-size:90%;">0.6458</span></td>
<td id="S3.SS3.tab1.3.1.5.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.5.1" class="ltx_text" style="font-size:90%;">0.5897</span></td>
<td id="S3.SS3.tab1.3.1.5.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.6.1" class="ltx_text" style="font-size:90%;">0.8092</span></td>
<td id="S3.SS3.tab1.3.1.5.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.5.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.8730</span></td>
</tr>
<tr id="S3.SS3.tab1.3.1.6" class="ltx_tr">
<td id="S3.SS3.tab1.3.1.6.1" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_bb ltx_border_bb ltx_border_bb ltx_border_t" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.1.1" class="ltx_text" style="font-size:90%;">1/99</span></td>
<td id="S3.SS3.tab1.3.1.6.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.2.1" class="ltx_text" style="font-size:90%;">0.4841</span></td>
<td id="S3.SS3.tab1.3.1.6.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.3.1" class="ltx_text" style="font-size:90%;">0.5051</span></td>
<td id="S3.SS3.tab1.3.1.6.4" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.4.1" class="ltx_text" style="font-size:90%;">0.5035</span></td>
<td id="S3.SS3.tab1.3.1.6.5" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.5.1" class="ltx_text" style="font-size:90%;">0.5920</span></td>
<td id="S3.SS3.tab1.3.1.6.6" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.6.1" class="ltx_text" style="font-size:90%;">0.5508</span></td>
<td id="S3.SS3.tab1.3.1.6.7" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:1.8pt;padding-bottom:1.8pt;"><span id="S3.SS3.tab1.3.1.6.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.6580</span></td>
</tr>
</table>
<figure id="S3.F3" class="ltx_figure ltx_align_center">
<figure id="S3.F2.sf1" class="ltx_figure ltx_align_center"><img src="/html/1905.03894/assets/figures/accuracy_option1.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="864" height="510" alt="Refer to caption">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Classification accuracy of each tested method vs. data splits</figcaption>
</figure>
<section id="S4" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">The results of training ResNet-34 on both synthetic data and varying levels of real data demonstrably suggests that synthetic data can be readily used to improve classification results of a convolutional neural network, particularly in applications where real data is extremely sparse; however, additional testing with datasets and multiple models is needed. Future experimentation on a dataset more challenging to ResNet-34 may show greater performance divergence between real and synthetically trained models.</span></p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text" style="font-size:90%;">Perhaps the greatest utility of synthetic data exists in problems where real data is not available - problem spaces where data collection is prohibitively challenging, or where object and events occur rarely and spontaneously in the wild. How might we increase the performance of a model trained on purely synthetic imagery? Recent work in computer vision implies that Generative Adversarial Networks</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S4.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.p2.1.4" class="ltx_text" style="font-size:90%;"> have a great deal to offer as a means of domain adaptation</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S4.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.p2.1.7" class="ltx_text" style="font-size:90%;">, but in a scenario with extremely scarce data, or a multimodal target distribution, training an adversarial network may present a circular problem. We don’t fully understand the disparity between real and synthetic data, and extending work in image quality metrics and statistical imagery analysis may provide critical insights for advancing domain adaptation techniques.</span></p>
</div>
</section>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">Given sufficient training data, CNNs outperform classical object recognition methods. However, as training data becomes increasingly sparse, the performance of object recognition models, including CNNs, suffers. We can leverage synthetically-generated images to bolster the performance achievements of CNNs when training data is limited. The power of synthetic data paired with a deep learning model is most evident in a complete absence of real training data - our experimentation shows that a deep learning classifier trained purely on synthetic imagery can still achieve performance levels higher than traditional object recognition methods trained on real data.</span></p>
</div>
<figure id="S5.tab1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Impact of using synthetic imagery w/ domain adaptation</figcaption>
</figure>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text" style="font-size:90%;">Accuracy Increase 
80/20  0.03%
50/50  0.70%
20/80  1.16%
5/95  6.34%
1/99  </span><span id="S5.p2.1.2" class="ltx_text ltx_font_bold" style="font-size:90%;">10.73%</span><span id="S5.p2.1.3" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
J. Harguess, S. Parameswaran, K. Rainey, and J. Stastny, “Vessel
classification in overhead satellite imagery using learned dictionaries,” in
</span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SPIE Optical Engineering+ Applications</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">.   International Society for Optics and Photonics, 2012, pp.
84 992F–84 992F.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
K. Rainey and J. Stastny, “Object recognition in ocean imagery using feature
selection and compressive sensing,” in </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Imagery Pattern
Recognition Workshop (AIPR), 2011 IEEE</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2011, pp. 1–6.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
S. Parameswaran and K. Rainey, “Vessel classification in overhead satellite
imagery using weighted” bag of visual words”,” in </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Automatic target
recognition XXV</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, vol. 9476.   International Society for Optics and Photonics, 2015, p. 947609.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
J. Harguess and K. Rainey, “Are face recognition methods useful for
classifying ships?” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Imagery Pattern Recognition Workshop
(AIPR), 2011 IEEE</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2011, pp.
1–7.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
B. Schölkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett, “New
support vector algorithms,” </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural computation</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, vol. 12, no. 5, pp.
1207–1245, 2000.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1512.03385</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and
Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2009, pp. 248–255.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To,
E. Cameracci, S. Boochoon, and S. Birchfield, “Training deep networks with
synthetic data: Bridging the reality gap by domain randomization,”
</span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.06516</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
C. M. Ward, J. Harguess, and A. G. Corelli, “Leveraging synthetic imagery for
collision-at-sea avoidance,” in </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Geospatial Informatics, Motion
Imagery, and Network Analytics VIII</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, vol. 10645.   International Society for Optics and Photonics, 2018, p. 1064507.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
A. Canziani, A. Paszke, and E. Culurciello, “An analysis of deep neural
network models for practical applications,” </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint
arXiv:1605.07678</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
M. Huh, P. Agrawal, and A. A. Efros, “What makes imagenet good for transfer
learning?” </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1608.08614</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Z. Guo, L. Zhang, D. Zhang, and X. Mou, “Hierarchical multiscale lbp for face
and palmprint recognition,” in </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Image Processing (ICIP), 2010 17th IEEE
International Conference on</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">.   IEEE,
2010, pp. 4521–4524.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
T. Ahonen, A. Hadid, and M. Pietikäinen, “Face recognition with local
binary patterns,” in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European conference on computer vision</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">.   Springer, 2004, pp. 469–481.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
H. Gao, X. Gu, T. Yu, L. Liu, Y. Sun, Y. Xie, and Q. Liu, “Validation of the
calibration coefficient of the gaofen-1 pms sensor using the landsat 8 oli,”
</span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote Sensing</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">, vol. 8, no. 2, p. 132, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
H. Lu, K. N. Plataniotis, and A. N. Venetsanopoulos, “Mpca: Multilinear
principal component analysis of tensor objects,” </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on
Neural Networks</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, vol. 19, no. 1, pp. 18–39, 2008.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
P. N. Belhumeur, J. P. Hespanha, and D. J. Kriegman, “Eigenfaces vs.
fisherfaces: Recognition using class specific linear projection,” Yale
University New Haven United States, Tech. Rep., 1997.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
N. Dalal and B. Triggs, “Histograms of oriented gradients for human
detection,” in </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition, 2005. CVPR
2005. IEEE Computer Society Conference on</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, vol. 1.   IEEE, 2005, pp. 886–893.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust face
recognition via sparse representation,” </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE transactions on pattern
analysis and machine intelligence</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">, vol. 31, no. 2, pp. 210–227, 2009.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, and Y. Bengio, “Generative adversarial nets,” in
</span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, 2014, pp.
2672–2680.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised
pixel-level domain adaptation with generative adversarial networks,” in
</span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">,
vol. 1, no. 2, 2017, p. 7.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa, “Generate to
adapt: Aligning domains using generative adversarial networks,” </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv
e-prints, abs/1704.01705</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
</ul>
</section>
</div>
</figure>
</section>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1905.03892" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1905.03894" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1905.03894">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1905.03894" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1905.03895" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Feb 26 19:36:33 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
