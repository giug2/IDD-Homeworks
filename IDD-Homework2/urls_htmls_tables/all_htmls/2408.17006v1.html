<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.17006] Retrieval-Augmented Natural Language Reasoning for Explainable Visual Question Answering</title><meta property="og:description" content="Visual Question Answering with Natural Language Explanation (VQA-NLE) task is challenging due to its high demand for reasoning-based inference.
Recent VQA-NLE studies focus on enhancing model networks
to amplify the mo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Retrieval-Augmented Natural Language Reasoning for Explainable Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Retrieval-Augmented Natural Language Reasoning for Explainable Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.17006">

<!--Generated on Thu Sep  5 15:54:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Retrieval-Augmented Natural Language Reasoning for Explainable Visual Question Answering</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering with Natural Language Explanation (VQA-NLE) task is challenging due to its high demand for reasoning-based inference.
Recent VQA-NLE studies focus on enhancing model networks
to amplify the model’s reasoning capability but this approach is resource-consuming and unstable.
In this work, we introduce a new VQA-NLE model, ReRe (Retrieval-augmented natural language Reasoning), using leverage retrieval information from the memory to aid in generating accurate answers and persuasive explanations without relying on complex networks and extra datasets.
ReRe is an encoder-decoder architecture model using a pre-trained clip vision encoder and a pre-trained GPT-2 language model as a decoder.
Cross-attention layers are added in the GPT-2 for processing retrieval feature.
ReRe outperforms previous methods in VQA accuracy and explanation score and shows improvement in NLE with more persuasive, reliability.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With significant advancements in deep learning models, there has been notable progress in vision-language tasks such as image captioning and visual question answering (VQA).
To substantiate the performance improvements in vision-language (VL) tasks, explanation has become crucial.
The importance of natural language explanations (NLE) is further emphasized, particularly for applying vision-language tasks based on principles of truth, correctness, and understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, NLE for vision and VL tasks remains a challenging task due to its high demand for reasoning-based inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
It requires both an understanding of the image and a higher-level reasoning ability beyond VQA to prove the answers.
With a lack of reasoning, models generate explanations that are non-relate to the predicted answers or are completely wrong.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent NLE studies focus on enhancing model networks to amplify the model’s reasoning capability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This approach is ideal for obtaining high reasoning ability without relying on large-scale model architecture and data, efficiently producing explanations through their unique logical processes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.
However, These unique logical processes generally consist of many reasoning steps and are often recursive.
Constructing such complex networks is resource-consuming and in some cases, results get worse over steps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we introduce a new VQA-NLE model, ReRe (<span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Re</span>trieval-augmented natural language <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">Re</span>asoning), using leverage retrieval information from the memory to aid in generating accurate answers and persuasive explanations without relying on complex networks and extra datasets.
Recent research has demonstrated significant results by applying retrieval augmentation to various vision-language tasks such as video question answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
These studies show that by providing semantic features from retrieval augmentation to the model, the model performance can be enhanced.
Inspired by this approach, we design a retrieval augmentation to the NLE task, specifically for the VQA-NLE task.
Our model uses a pre-trained CLIP vision encoder and a pre-trained GPT-2 language model as a decoder.
A new layer is added in GPT-2 to perform cross-attention over the encoded retrieval features, which is an extremely simple way to gain reasoning from retrieval features.
After retrieval of the memory database, semantic retrieval features are extracted by averaging the encoded sample feature.
These retrieval features are then inputted with the image feature and question feature encoded with the clip vision encoder.
ReRe generates the answer and explanation from the given image and question with the aid of retrieval features.
Compared with other methods, ReRe shows improvement in explanation with more persuasive, reliability.
</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">VQA-NLE:</span>
VQA has firstly proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> that answering questions about the given real-world images.
Since then, many approaches have been proposed on VQA task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
To pursue explainable VQA with reasoning process, NLE task has been proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Textual explanation of classification decision is generated for end-user, which is different from lower-level explanations that apply visualization technologies<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> proposed VQA-X datasets and PJ-X model.
VQA-X gives a rational explanation of visual question answering tasks.
PJ-X consists of an answering model and a multimodal explanation model, in which the predicted answer of the answering model is used to generate textual justifications in the explanation model.
e-ug <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> model is also separated from VL-model (UNITER <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>) that predicts answers and pre-trained language model (GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>) to generate explanation.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> suggests a model architecture that generates text explanation by GPT-2’s backbone architecture.
NLX-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is a unified model that simultaneously generates answers and explanations. By unifying the VL model and explanation model in one, their answer and explanation are more correlated.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Recently, <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="S_{3}C" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><msub id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml"><mi id="S2.p2.1.m1.1.1.2.2" xref="S2.p2.1.m1.1.1.2.2.cmml">S</mi><mn id="S2.p2.1.m1.1.1.2.3" xref="S2.p2.1.m1.1.1.2.3.cmml">3</mn></msub><mo lspace="0em" rspace="0em" id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><times id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></times><apply id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.2.1.cmml" xref="S2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.2.cmml" xref="S2.p2.1.m1.1.1.2.2">𝑆</ci><cn type="integer" id="S2.p2.1.m1.1.1.2.3.cmml" xref="S2.p2.1.m1.1.1.2.3">3</cn></apply><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">S_{3}C</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> used self-critical learning networks to improve the model’s self-interpretability.
MCLE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> improves NLE ability by chain-of-thought strategy in generating explanation and multi-level contrastive learning network.
ReVisE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> introduces recursive networks where the generated explanation is utilized for next-step explanation generation.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Retrieval Augmentation:</span>
Retrieval augmentation has gained attention in natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and also various multimodal tasks such as image and video captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and VQA<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Despite of achievement of retrieval augmentation in many tasks, there is no attempt in NLE tasks. To the best of our knowledge, this is the first study to design retrieval augmentation for NLE tasks.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<p id="S2.F1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.F1.1.1" class="ltx_text"><img src="/html/2408.17006/assets/Figure1-1.png" id="S2.F1.1.1.g1" class="ltx_graphics ltx_img_square" width="350" height="322" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.3.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Examples of retrieval in our method. (a) The question types between the query (left) and retrieval (right) samples are the same, but the images contain different semantic information, (b) the images contain the same semantic information, but the contents of the questions are different.
(c) is an ideal case where both the image and the question contents have the same semantics.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to retrieve related samples from a memory database and properly utilize them to enhance the performance of VQA and NLE.
In subsection 3.1, we introduce how to retrieve informative samples from memory database. Our definition of an informative sample is a sample with the same question type and similar semantic information. Fig.1 shows examples of retrieval in our method.
In subsection 3.2, we introduce details of ReRe’s architecture which processes input image, question, and retrieval features.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Retrieval Method</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The VQA-NLE task is to generate an answer from the question based on visual information from the image, along with an explanation that justifies the answer.
To help the model generate more accurate and informative answers and explanations, we build a memory database that consists of images, questions, answers, and explanations from the training dataset.
To retrieve a sample, we use a question and an image of the input query which are used as a key for searching and retrieving answers and explanations from similar cases.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">First, we measure the similarity score between the query question (<math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="Q_{q}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">Q</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑄</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">Q_{q}</annotation></semantics></math>) and the sample question (<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="Q_{s}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">Q</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝑄</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">Q_{s}</annotation></semantics></math>) using cosine similarity, and we also measure the similarity score between the query image (<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="I_{q}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><msub id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">I</mi><mi id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝐼</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">I_{q}</annotation></semantics></math>) and the sample explanation (<math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="E_{s}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">E</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝐸</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">E_{s}</annotation></semantics></math>).
To measure cosine similarity between text-text and text-image, we encoded text and images into feature representation using a pre-trained CLIP model.
Note that the CLIP model, trained with text and images in the same embedding space, can be used to measure similarity between multimodal features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Text-text cosine similarity gets higher when sentence structure is similar and identical words are used.
By measuring the similarity between the query question and the sample questions, we can retrieve samples that have the same question type (sentence structure is similar) and deal with the same domain (identical words are used).
Also, we measure the similarity between the query image and the sample explanation in order to retrieve a sample that contains semantic information relevant to the problem we want to solve.
The query’s semantic information is contained in the image and the retrieved sample’s semantic information is mainly extracted in the explanation.
By comparison with the retrieved sample’s image and explanation, explanation information contains necessary image information in specific question situations.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">To calculate the final similarity score between the query and the samples in the memory database, these two scores are combined as</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="Score_{Retrieval}=cos(Q_{q},Q_{s})+cos(I_{q},E_{s})." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.6.cmml"><mi id="S3.E1.m1.1.1.1.1.6.2" xref="S3.E1.m1.1.1.1.1.6.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.1" xref="S3.E1.m1.1.1.1.1.6.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.3" xref="S3.E1.m1.1.1.1.1.6.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.1a" xref="S3.E1.m1.1.1.1.1.6.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.4" xref="S3.E1.m1.1.1.1.1.6.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.1b" xref="S3.E1.m1.1.1.1.1.6.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.5" xref="S3.E1.m1.1.1.1.1.6.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.1c" xref="S3.E1.m1.1.1.1.1.6.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.6.6" xref="S3.E1.m1.1.1.1.1.6.6.cmml"><mi id="S3.E1.m1.1.1.1.1.6.6.2" xref="S3.E1.m1.1.1.1.1.6.6.2.cmml">e</mi><mrow id="S3.E1.m1.1.1.1.1.6.6.3" xref="S3.E1.m1.1.1.1.1.6.6.3.cmml"><mi id="S3.E1.m1.1.1.1.1.6.6.3.2" xref="S3.E1.m1.1.1.1.1.6.6.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.3" xref="S3.E1.m1.1.1.1.1.6.6.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1a" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.4" xref="S3.E1.m1.1.1.1.1.6.6.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1b" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.5" xref="S3.E1.m1.1.1.1.1.6.6.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1c" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.6" xref="S3.E1.m1.1.1.1.1.6.6.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1d" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.7" xref="S3.E1.m1.1.1.1.1.6.6.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1e" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.8" xref="S3.E1.m1.1.1.1.1.6.6.3.8.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1f" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.9" xref="S3.E1.m1.1.1.1.1.6.6.3.9.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.6.3.1g" xref="S3.E1.m1.1.1.1.1.6.6.3.1.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.6.6.3.10" xref="S3.E1.m1.1.1.1.1.6.6.3.10.cmml">l</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml"><mrow id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.4" xref="S3.E1.m1.1.1.1.1.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.2.2.5" xref="S3.E1.m1.1.1.1.1.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.3a" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.2.2.6" xref="S3.E1.m1.1.1.1.1.2.2.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.2.3b" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">Q</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S3.E1.m1.1.1.1.1.2.2.2.2.4" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.cmml">Q</mi><mi id="S3.E1.m1.1.1.1.1.2.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml">s</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.2.2.2.2.5" xref="S3.E1.m1.1.1.1.1.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.4.5" xref="S3.E1.m1.1.1.1.1.4.5.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.4.4" xref="S3.E1.m1.1.1.1.1.4.4.cmml"><mi id="S3.E1.m1.1.1.1.1.4.4.4" xref="S3.E1.m1.1.1.1.1.4.4.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.4.3" xref="S3.E1.m1.1.1.1.1.4.4.3.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.4.4.5" xref="S3.E1.m1.1.1.1.1.4.4.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.4.3a" xref="S3.E1.m1.1.1.1.1.4.4.3.cmml">​</mo><mi id="S3.E1.m1.1.1.1.1.4.4.6" xref="S3.E1.m1.1.1.1.1.4.4.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.4.3b" xref="S3.E1.m1.1.1.1.1.4.4.3.cmml">​</mo><mrow id="S3.E1.m1.1.1.1.1.4.4.2.2" xref="S3.E1.m1.1.1.1.1.4.4.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.4.4.2.2.3" xref="S3.E1.m1.1.1.1.1.4.4.2.3.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.3.3.1.1.1" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.1.1.1.2" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1.2.cmml">I</mi><mi id="S3.E1.m1.1.1.1.1.3.3.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1.3.cmml">q</mi></msub><mo id="S3.E1.m1.1.1.1.1.4.4.2.2.4" xref="S3.E1.m1.1.1.1.1.4.4.2.3.cmml">,</mo><msub id="S3.E1.m1.1.1.1.1.4.4.2.2.2" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.4.4.2.2.2.2" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2.2.cmml">E</mi><mi id="S3.E1.m1.1.1.1.1.4.4.2.2.2.3" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2.3.cmml">s</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.4.4.2.2.5" xref="S3.E1.m1.1.1.1.1.4.4.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5"></eq><apply id="S3.E1.m1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.6"><times id="S3.E1.m1.1.1.1.1.6.1.cmml" xref="S3.E1.m1.1.1.1.1.6.1"></times><ci id="S3.E1.m1.1.1.1.1.6.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2">𝑆</ci><ci id="S3.E1.m1.1.1.1.1.6.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.6.4.cmml" xref="S3.E1.m1.1.1.1.1.6.4">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.6.5.cmml" xref="S3.E1.m1.1.1.1.1.6.5">𝑟</ci><apply id="S3.E1.m1.1.1.1.1.6.6.cmml" xref="S3.E1.m1.1.1.1.1.6.6"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.6.6.1.cmml" xref="S3.E1.m1.1.1.1.1.6.6">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.6.6.2.cmml" xref="S3.E1.m1.1.1.1.1.6.6.2">𝑒</ci><apply id="S3.E1.m1.1.1.1.1.6.6.3.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3"><times id="S3.E1.m1.1.1.1.1.6.6.3.1.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.1"></times><ci id="S3.E1.m1.1.1.1.1.6.6.3.2.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.2">𝑅</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.3.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.4.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.4">𝑡</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.5.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.5">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.6.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.6">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.7.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.7">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.8.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.8">𝑣</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.9.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.9">𝑎</ci><ci id="S3.E1.m1.1.1.1.1.6.6.3.10.cmml" xref="S3.E1.m1.1.1.1.1.6.6.3.10">𝑙</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"><plus id="S3.E1.m1.1.1.1.1.4.5.cmml" xref="S3.E1.m1.1.1.1.1.4.5"></plus><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2"><times id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"></times><ci id="S3.E1.m1.1.1.1.1.2.2.4.cmml" xref="S3.E1.m1.1.1.1.1.2.2.4">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.2.2.5.cmml" xref="S3.E1.m1.1.1.1.1.2.2.5">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.2.2.6.cmml" xref="S3.E1.m1.1.1.1.1.2.2.6">𝑠</ci><interval closure="open" id="S3.E1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2"><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">𝑄</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">𝑞</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.2">𝑄</ci><ci id="S3.E1.m1.1.1.1.1.2.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2.2.2.3">𝑠</ci></apply></interval></apply><apply id="S3.E1.m1.1.1.1.1.4.4.cmml" xref="S3.E1.m1.1.1.1.1.4.4"><times id="S3.E1.m1.1.1.1.1.4.4.3.cmml" xref="S3.E1.m1.1.1.1.1.4.4.3"></times><ci id="S3.E1.m1.1.1.1.1.4.4.4.cmml" xref="S3.E1.m1.1.1.1.1.4.4.4">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.4.4.5.cmml" xref="S3.E1.m1.1.1.1.1.4.4.5">𝑜</ci><ci id="S3.E1.m1.1.1.1.1.4.4.6.cmml" xref="S3.E1.m1.1.1.1.1.4.4.6">𝑠</ci><interval closure="open" id="S3.E1.m1.1.1.1.1.4.4.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.4.2.2"><apply id="S3.E1.m1.1.1.1.1.3.3.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1.2">𝐼</ci><ci id="S3.E1.m1.1.1.1.1.3.3.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1.1.1.3">𝑞</ci></apply><apply id="S3.E1.m1.1.1.1.1.4.4.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.4.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.4.4.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2.2">𝐸</ci><ci id="S3.E1.m1.1.1.1.1.4.4.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.4.2.2.2.3">𝑠</ci></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">Score_{Retrieval}=cos(Q_{q},Q_{s})+cos(I_{q},E_{s}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p5.1" class="ltx_p">After measuring the similarity score for all samples in the memory database with respect to the given query, our method retrieves the top-K samples based on their similarity scores.
In our work, we retrieve 10 samples (K=10) for each query.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/2408.17006/assets/x1.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_square" width="269" height="222" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Overall architecture of the proposed method. </figcaption>
</figure>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p">After retrieving, we utilize answers and explanations of retrieval samples.
These K pairs of answers and explanations are encoded into features using the CLIP model, and the answer features and explanation features are each averaged to assist the model’s reasoning.
By averaging K number of answers and explanations feature, it shows the effect that each feature representations concentrate on necessary representation.
Averaging features could refine noisy effects and reduce the computation complexity when they are used in the language model.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Features Encoding:</span>
ReRe consists of an encoder-decoder architecture as depicted in Fig.2.
In the encoder part, a pre-trained CLIP model is used to encode the input image and retrieval text into feature representations.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The retrieval information to be inputted into the model from the retrieved samples consists of the answer and explanation.
The answer from the retrieved sample can serve as a hint to accurately answer the given query question.
Additionally, providing an explanation can help the model provide a logical justification for the answer it has given.
</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Features Cross Attention:</span>
ReRe’s decoder part consists of pre-trained distilled GPT-2 and newly added cross-attention layers to deal with image, question, and retrieval features.
The original GPT’s block structure consists of a self-attention layer and a cross-attention layer.
In ReRe, the basic structure’s self-attention layer processes the question text, and the cross-attention layer processes the image.
The newly added cross-attention layer follows the cross-attention layer that processes the image, and it handles retrieval features in that part.
In detail, hidden states obtained from the question feature embedding after passing through self-attention are cross-attention with the image features, output semantic cross-modal features that contain unified information about image and question<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
These semantic cross-modal features are then cross-attention with the retrieval features, and this process is repeated in every language model block.
Through these sequential attention computation procedures, the final answer to the query question is output based on the retrieval information.
This structure can incorporate retrieval information while preserving the general language capabilities of the original language model.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Answer Explanation Generation:</span>
Our model is a unified model that generates both the answer and explanation simultaneously. The model generates sentences based on retrieval-based inference, and these sentences are structured as “<span id="S3.SS2.p4.1.2" class="ltx_text">the answer is [answer] because [explanation]</span>”.
Generating both the answer and explanation simultaneously in this way can increase the logical coherence between the answer and explanation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text ltx_font_bold">Dataset:</span>
Our experiments are conducted using VQA-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which is widely used in the VQA-NLE task.
VQA-X is human annotated multimodal explanations for visual question answering.
It consists of 28K images, 33K Q/A pairs, and 42K explanations.
Out of the 33K Q/A pairs, 2.9K are used for training, 1.4K for validation, and 1.9K for testing.
The questions in VQA-X are composed of problems where answers and explanations need to be based on visual information, such as “Is this banana ripe?”.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Retrieval Memory Database:</span>
Our retrieval memory database is set up of VQA-X data.
During the training process, retrieval is conducted using only the training data from VQA-X in the Retrieval Memory Database.
During inference, the Database is constructed using both the training and validation data from VQA-X.
This utilization could set retrieval data to the fullest extent at the training and validation stage without any data leakage.
The retrieval samples in the Memory Database consist of images, question-answer pairs, and explanations, the same as the original VQA-X data configuration.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Training:</span>
The weights of the newly added cross-attention layers to the pre-trained distilled GPT-2 model were initialized randomly.
The Clip encoder is used only for extracting feature representations and is excluded from training.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics:</span>
We computed scores for prediction using automatic natural language generation (NLG) metrics including N-gram-based metrics BLEU-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, as well as SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> and BERTScore <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which focus more on the semantic information of explanations.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_transformed_outer" style="width:358.2pt;height:137.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.6pt,12.1pt) scale(0.85,0.85) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">B4</td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">M</td>
<td id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">R</td>
<td id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_left ltx_border_t">C</td>
<td id="S4.T1.1.1.2.1.6" class="ltx_td ltx_align_left ltx_border_t">S</td>
<td id="S4.T1.1.1.2.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">BS</td>
<td id="S4.T1.1.1.2.1.8" class="ltx_td ltx_align_left ltx_border_t">Acc</td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">PJ-X <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_t">22.7</td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_t">19.7</td>
<td id="S4.T1.1.1.3.2.4" class="ltx_td ltx_align_left ltx_border_t">46.0</td>
<td id="S4.T1.1.1.3.2.5" class="ltx_td ltx_align_left ltx_border_t">82.7</td>
<td id="S4.T1.1.1.3.2.6" class="ltx_td ltx_align_left ltx_border_t">17.1</td>
<td id="S4.T1.1.1.3.2.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">84.6</td>
<td id="S4.T1.1.1.3.2.8" class="ltx_td ltx_align_left ltx_border_t">76.4</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">FME <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</th>
<td id="S4.T1.1.1.4.3.2" class="ltx_td ltx_align_left">23.1</td>
<td id="S4.T1.1.1.4.3.3" class="ltx_td ltx_align_left">20.4</td>
<td id="S4.T1.1.1.4.3.4" class="ltx_td ltx_align_left">47.1</td>
<td id="S4.T1.1.1.4.3.5" class="ltx_td ltx_align_left">87.0</td>
<td id="S4.T1.1.1.4.3.6" class="ltx_td ltx_align_left">18.4</td>
<td id="S4.T1.1.1.4.3.7" class="ltx_td ltx_align_left ltx_border_r">85.2</td>
<td id="S4.T1.1.1.4.3.8" class="ltx_td ltx_align_left">75.5</td>
</tr>
<tr id="S4.T1.1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</th>
<td id="S4.T1.1.1.5.4.2" class="ltx_td ltx_align_left">17.4</td>
<td id="S4.T1.1.1.5.4.3" class="ltx_td ltx_align_left">19.2</td>
<td id="S4.T1.1.1.5.4.4" class="ltx_td ltx_align_left">42.1</td>
<td id="S4.T1.1.1.5.4.5" class="ltx_td ltx_align_left">52.5</td>
<td id="S4.T1.1.1.5.4.6" class="ltx_td ltx_align_left">15.8</td>
<td id="S4.T1.1.1.5.4.7" class="ltx_td ltx_align_left ltx_border_r">85.7</td>
<td id="S4.T1.1.1.5.4.8" class="ltx_td ltx_align_left">68.6</td>
</tr>
<tr id="S4.T1.1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">e-UG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>
</th>
<td id="S4.T1.1.1.6.5.2" class="ltx_td ltx_align_left">23.2</td>
<td id="S4.T1.1.1.6.5.3" class="ltx_td ltx_align_left">22.1</td>
<td id="S4.T1.1.1.6.5.4" class="ltx_td ltx_align_left">45.7</td>
<td id="S4.T1.1.1.6.5.5" class="ltx_td ltx_align_left">49.9</td>
<td id="S4.T1.1.1.6.5.6" class="ltx_td ltx_align_left">20.1</td>
<td id="S4.T1.1.1.6.5.7" class="ltx_td ltx_align_left ltx_border_r">87.0</td>
<td id="S4.T1.1.1.6.5.8" class="ltx_td ltx_align_left">80.5</td>
</tr>
<tr id="S4.T1.1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">NLX-GPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>
</th>
<td id="S4.T1.1.1.7.6.2" class="ltx_td ltx_align_left">28.5</td>
<td id="S4.T1.1.1.7.6.3" class="ltx_td ltx_align_left">23.1</td>
<td id="S4.T1.1.1.7.6.4" class="ltx_td ltx_align_left">51.5</td>
<td id="S4.T1.1.1.7.6.5" class="ltx_td ltx_align_left">110.6</td>
<td id="S4.T1.1.1.7.6.6" class="ltx_td ltx_align_left">22.1</td>
<td id="S4.T1.1.1.7.6.7" class="ltx_td ltx_align_left ltx_border_r">86.9</td>
<td id="S4.T1.1.1.7.6.8" class="ltx_td ltx_align_left">83.1</td>
</tr>
<tr id="S4.T1.1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">ReVisE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</th>
<td id="S4.T1.1.1.8.7.2" class="ltx_td ltx_align_left">28.2</td>
<td id="S4.T1.1.1.8.7.3" class="ltx_td ltx_align_left">23.2</td>
<td id="S4.T1.1.1.8.7.4" class="ltx_td ltx_align_left">51.8</td>
<td id="S4.T1.1.1.8.7.5" class="ltx_td ltx_align_left">108.9</td>
<td id="S4.T1.1.1.8.7.6" class="ltx_td ltx_align_left">22.6</td>
<td id="S4.T1.1.1.8.7.7" class="ltx_td ltx_align_left ltx_border_r">88.1</td>
<td id="S4.T1.1.1.8.7.8" class="ltx_td ltx_align_center">_</td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">ReRe<sub id="S4.T1.1.1.1.1.1.1" class="ltx_sub"><span id="S4.T1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">I</span></sub></span></th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">28.7</span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">23.4</span></td>
<td id="S4.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">52.0</span></td>
<td id="S4.T1.1.1.1.5" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">111.7</span></td>
<td id="S4.T1.1.1.1.6" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.6.1" class="ltx_text ltx_font_bold">22.7</span></td>
<td id="S4.T1.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S4.T1.1.1.1.7.1" class="ltx_text ltx_font_bold">90.1</span></td>
<td id="S4.T1.1.1.1.8" class="ltx_td ltx_align_left ltx_border_t"><span id="S4.T1.1.1.1.8.1" class="ltx_text ltx_font_bold">83.0</span></td>
</tr>
<tr id="S4.T1.1.1.9.8" class="ltx_tr">
<th id="S4.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S4.T1.1.1.9.8.1.1" class="ltx_text ltx_font_bold">ReRe</span></th>
<td id="S4.T1.1.1.9.8.2" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.1.9.8.2.1" class="ltx_text ltx_font_bold">29.2</span></td>
<td id="S4.T1.1.1.9.8.3" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.1.9.8.3.1" class="ltx_text ltx_font_bold">23.4</span></td>
<td id="S4.T1.1.1.9.8.4" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.1.9.8.4.1" class="ltx_text ltx_font_bold">52.1</span></td>
<td id="S4.T1.1.1.9.8.5" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.1.9.8.5.1" class="ltx_text ltx_font_bold">113.4</span></td>
<td id="S4.T1.1.1.9.8.6" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.1.9.8.6.1" class="ltx_text ltx_font_bold">22.7</span></td>
<td id="S4.T1.1.1.9.8.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S4.T1.1.1.9.8.7.1" class="ltx_text ltx_font_bold">90.2</span></td>
<td id="S4.T1.1.1.9.8.8" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T1.1.1.9.8.8.1" class="ltx_text ltx_font_bold">83.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.10.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Filtered Scores comparison with the state-of-the-art model on the VQA-X datasets. Filtered scores only consider the samples that have correct answers. B4, M, R, C, S, BS, Acc are short for BLEU-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, SPICE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, BERTSCORE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, answer accuracy.
ReRe<sub id="S4.T1.11.2" class="ltx_sub"><span id="S4.T1.11.2.1" class="ltx_text ltx_font_italic">I</span></sub> denotes the result of measuring retrieval score with a cosine similarity of image-image(<math id="S4.T1.6.m2.2" class="ltx_Math" alttext="cos(I_{q},I_{s})" display="inline"><semantics id="S4.T1.6.m2.2b"><mrow id="S4.T1.6.m2.2.2" xref="S4.T1.6.m2.2.2.cmml"><mi id="S4.T1.6.m2.2.2.4" xref="S4.T1.6.m2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.m2.2.2.3" xref="S4.T1.6.m2.2.2.3.cmml">​</mo><mi id="S4.T1.6.m2.2.2.5" xref="S4.T1.6.m2.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.m2.2.2.3b" xref="S4.T1.6.m2.2.2.3.cmml">​</mo><mi id="S4.T1.6.m2.2.2.6" xref="S4.T1.6.m2.2.2.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T1.6.m2.2.2.3c" xref="S4.T1.6.m2.2.2.3.cmml">​</mo><mrow id="S4.T1.6.m2.2.2.2.2" xref="S4.T1.6.m2.2.2.2.3.cmml"><mo stretchy="false" id="S4.T1.6.m2.2.2.2.2.3" xref="S4.T1.6.m2.2.2.2.3.cmml">(</mo><msub id="S4.T1.6.m2.1.1.1.1.1" xref="S4.T1.6.m2.1.1.1.1.1.cmml"><mi id="S4.T1.6.m2.1.1.1.1.1.2" xref="S4.T1.6.m2.1.1.1.1.1.2.cmml">I</mi><mi id="S4.T1.6.m2.1.1.1.1.1.3" xref="S4.T1.6.m2.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S4.T1.6.m2.2.2.2.2.4" xref="S4.T1.6.m2.2.2.2.3.cmml">,</mo><msub id="S4.T1.6.m2.2.2.2.2.2" xref="S4.T1.6.m2.2.2.2.2.2.cmml"><mi id="S4.T1.6.m2.2.2.2.2.2.2" xref="S4.T1.6.m2.2.2.2.2.2.2.cmml">I</mi><mi id="S4.T1.6.m2.2.2.2.2.2.3" xref="S4.T1.6.m2.2.2.2.2.2.3.cmml">s</mi></msub><mo stretchy="false" id="S4.T1.6.m2.2.2.2.2.5" xref="S4.T1.6.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.6.m2.2c"><apply id="S4.T1.6.m2.2.2.cmml" xref="S4.T1.6.m2.2.2"><times id="S4.T1.6.m2.2.2.3.cmml" xref="S4.T1.6.m2.2.2.3"></times><ci id="S4.T1.6.m2.2.2.4.cmml" xref="S4.T1.6.m2.2.2.4">𝑐</ci><ci id="S4.T1.6.m2.2.2.5.cmml" xref="S4.T1.6.m2.2.2.5">𝑜</ci><ci id="S4.T1.6.m2.2.2.6.cmml" xref="S4.T1.6.m2.2.2.6">𝑠</ci><interval closure="open" id="S4.T1.6.m2.2.2.2.3.cmml" xref="S4.T1.6.m2.2.2.2.2"><apply id="S4.T1.6.m2.1.1.1.1.1.cmml" xref="S4.T1.6.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.T1.6.m2.1.1.1.1.1.1.cmml" xref="S4.T1.6.m2.1.1.1.1.1">subscript</csymbol><ci id="S4.T1.6.m2.1.1.1.1.1.2.cmml" xref="S4.T1.6.m2.1.1.1.1.1.2">𝐼</ci><ci id="S4.T1.6.m2.1.1.1.1.1.3.cmml" xref="S4.T1.6.m2.1.1.1.1.1.3">𝑞</ci></apply><apply id="S4.T1.6.m2.2.2.2.2.2.cmml" xref="S4.T1.6.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.T1.6.m2.2.2.2.2.2.1.cmml" xref="S4.T1.6.m2.2.2.2.2.2">subscript</csymbol><ci id="S4.T1.6.m2.2.2.2.2.2.2.cmml" xref="S4.T1.6.m2.2.2.2.2.2.2">𝐼</ci><ci id="S4.T1.6.m2.2.2.2.2.2.3.cmml" xref="S4.T1.6.m2.2.2.2.2.2.3">𝑠</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.m2.2d">cos(I_{q},I_{s})</annotation></semantics></math>), instead of the purposed method <math id="S4.T1.7.m3.2" class="ltx_Math" alttext="cos(I_{q},E_{s})" display="inline"><semantics id="S4.T1.7.m3.2b"><mrow id="S4.T1.7.m3.2.2" xref="S4.T1.7.m3.2.2.cmml"><mi id="S4.T1.7.m3.2.2.4" xref="S4.T1.7.m3.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T1.7.m3.2.2.3" xref="S4.T1.7.m3.2.2.3.cmml">​</mo><mi id="S4.T1.7.m3.2.2.5" xref="S4.T1.7.m3.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.T1.7.m3.2.2.3b" xref="S4.T1.7.m3.2.2.3.cmml">​</mo><mi id="S4.T1.7.m3.2.2.6" xref="S4.T1.7.m3.2.2.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.T1.7.m3.2.2.3c" xref="S4.T1.7.m3.2.2.3.cmml">​</mo><mrow id="S4.T1.7.m3.2.2.2.2" xref="S4.T1.7.m3.2.2.2.3.cmml"><mo stretchy="false" id="S4.T1.7.m3.2.2.2.2.3" xref="S4.T1.7.m3.2.2.2.3.cmml">(</mo><msub id="S4.T1.7.m3.1.1.1.1.1" xref="S4.T1.7.m3.1.1.1.1.1.cmml"><mi id="S4.T1.7.m3.1.1.1.1.1.2" xref="S4.T1.7.m3.1.1.1.1.1.2.cmml">I</mi><mi id="S4.T1.7.m3.1.1.1.1.1.3" xref="S4.T1.7.m3.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S4.T1.7.m3.2.2.2.2.4" xref="S4.T1.7.m3.2.2.2.3.cmml">,</mo><msub id="S4.T1.7.m3.2.2.2.2.2" xref="S4.T1.7.m3.2.2.2.2.2.cmml"><mi id="S4.T1.7.m3.2.2.2.2.2.2" xref="S4.T1.7.m3.2.2.2.2.2.2.cmml">E</mi><mi id="S4.T1.7.m3.2.2.2.2.2.3" xref="S4.T1.7.m3.2.2.2.2.2.3.cmml">s</mi></msub><mo stretchy="false" id="S4.T1.7.m3.2.2.2.2.5" xref="S4.T1.7.m3.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.7.m3.2c"><apply id="S4.T1.7.m3.2.2.cmml" xref="S4.T1.7.m3.2.2"><times id="S4.T1.7.m3.2.2.3.cmml" xref="S4.T1.7.m3.2.2.3"></times><ci id="S4.T1.7.m3.2.2.4.cmml" xref="S4.T1.7.m3.2.2.4">𝑐</ci><ci id="S4.T1.7.m3.2.2.5.cmml" xref="S4.T1.7.m3.2.2.5">𝑜</ci><ci id="S4.T1.7.m3.2.2.6.cmml" xref="S4.T1.7.m3.2.2.6">𝑠</ci><interval closure="open" id="S4.T1.7.m3.2.2.2.3.cmml" xref="S4.T1.7.m3.2.2.2.2"><apply id="S4.T1.7.m3.1.1.1.1.1.cmml" xref="S4.T1.7.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.T1.7.m3.1.1.1.1.1.1.cmml" xref="S4.T1.7.m3.1.1.1.1.1">subscript</csymbol><ci id="S4.T1.7.m3.1.1.1.1.1.2.cmml" xref="S4.T1.7.m3.1.1.1.1.1.2">𝐼</ci><ci id="S4.T1.7.m3.1.1.1.1.1.3.cmml" xref="S4.T1.7.m3.1.1.1.1.1.3">𝑞</ci></apply><apply id="S4.T1.7.m3.2.2.2.2.2.cmml" xref="S4.T1.7.m3.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.T1.7.m3.2.2.2.2.2.1.cmml" xref="S4.T1.7.m3.2.2.2.2.2">subscript</csymbol><ci id="S4.T1.7.m3.2.2.2.2.2.2.cmml" xref="S4.T1.7.m3.2.2.2.2.2.2">𝐸</ci><ci id="S4.T1.7.m3.2.2.2.2.2.3.cmml" xref="S4.T1.7.m3.2.2.2.2.2.3">𝑠</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.m3.2d">cos(I_{q},E_{s})</annotation></semantics></math>.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.2" class="ltx_inline-block ltx_transformed_outer" style="width:241.7pt;height:62.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.1pt,4.7pt) scale(0.87,0.87) ;">
<table id="S4.T2.2.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.2.3.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t"></th>
<th id="S4.T2.2.2.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">B4</th>
<th id="S4.T2.2.2.3.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">M</th>
<th id="S4.T2.2.2.3.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">R</th>
<th id="S4.T2.2.2.3.1.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">C</th>
<th id="S4.T2.2.2.3.1.6" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">S</th>
<th id="S4.T2.2.2.3.1.7" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">BS</th>
<th id="S4.T2.2.2.3.1.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">ACC</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.2.4.1" class="ltx_tr">
<th id="S4.T2.2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">ReRe</th>
<td id="S4.T2.2.2.4.1.2" class="ltx_td ltx_align_left ltx_border_t">29.2</td>
<td id="S4.T2.2.2.4.1.3" class="ltx_td ltx_align_left ltx_border_t">23.4</td>
<td id="S4.T2.2.2.4.1.4" class="ltx_td ltx_align_left ltx_border_t">52.1</td>
<td id="S4.T2.2.2.4.1.5" class="ltx_td ltx_align_left ltx_border_t">113.4</td>
<td id="S4.T2.2.2.4.1.6" class="ltx_td ltx_align_left ltx_border_t">22.7</td>
<td id="S4.T2.2.2.4.1.7" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">90.2</td>
<td id="S4.T2.2.2.4.1.8" class="ltx_td ltx_align_left ltx_border_t">83.7</td>
</tr>
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="Oracle_{e}" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.1.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1a" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.4" xref="S4.T2.1.1.1.1.m1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1b" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.5" xref="S4.T2.1.1.1.1.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1c" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.1.1.1.1.m1.1.1.6" xref="S4.T2.1.1.1.1.m1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.1.m1.1.1.1d" xref="S4.T2.1.1.1.1.m1.1.1.1.cmml">​</mo><msub id="S4.T2.1.1.1.1.m1.1.1.7" xref="S4.T2.1.1.1.1.m1.1.1.7.cmml"><mi id="S4.T2.1.1.1.1.m1.1.1.7.2" xref="S4.T2.1.1.1.1.m1.1.1.7.2.cmml">e</mi><mi id="S4.T2.1.1.1.1.m1.1.1.7.3" xref="S4.T2.1.1.1.1.m1.1.1.7.3.cmml">e</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.1"></times><ci id="S4.T2.1.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.2">𝑂</ci><ci id="S4.T2.1.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.3">𝑟</ci><ci id="S4.T2.1.1.1.1.m1.1.1.4.cmml" xref="S4.T2.1.1.1.1.m1.1.1.4">𝑎</ci><ci id="S4.T2.1.1.1.1.m1.1.1.5.cmml" xref="S4.T2.1.1.1.1.m1.1.1.5">𝑐</ci><ci id="S4.T2.1.1.1.1.m1.1.1.6.cmml" xref="S4.T2.1.1.1.1.m1.1.1.6">𝑙</ci><apply id="S4.T2.1.1.1.1.m1.1.1.7.cmml" xref="S4.T2.1.1.1.1.m1.1.1.7"><csymbol cd="ambiguous" id="S4.T2.1.1.1.1.m1.1.1.7.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1.7">subscript</csymbol><ci id="S4.T2.1.1.1.1.m1.1.1.7.2.cmml" xref="S4.T2.1.1.1.1.m1.1.1.7.2">𝑒</ci><ci id="S4.T2.1.1.1.1.m1.1.1.7.3.cmml" xref="S4.T2.1.1.1.1.m1.1.1.7.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">Oracle_{e}</annotation></semantics></math></th>
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">36.4</span></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">27.9</span></td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">58.2</span></td>
<td id="S4.T2.1.1.1.5" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_bold">142.1</span></td>
<td id="S4.T2.1.1.1.6" class="ltx_td ltx_align_left"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_bold">27.1</span></td>
<td id="S4.T2.1.1.1.7" class="ltx_td ltx_align_left ltx_border_r"><span id="S4.T2.1.1.1.7.1" class="ltx_text ltx_font_bold">90.85</span></td>
<td id="S4.T2.1.1.1.8" class="ltx_td ltx_align_left">83.84</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="Oracle_{ae}" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mrow id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.2" xref="S4.T2.2.2.2.1.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.3" xref="S4.T2.2.2.2.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1a" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.4" xref="S4.T2.2.2.2.1.m1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1b" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.5" xref="S4.T2.2.2.2.1.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1c" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.6" xref="S4.T2.2.2.2.1.m1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.1d" xref="S4.T2.2.2.2.1.m1.1.1.1.cmml">​</mo><msub id="S4.T2.2.2.2.1.m1.1.1.7" xref="S4.T2.2.2.2.1.m1.1.1.7.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.7.2" xref="S4.T2.2.2.2.1.m1.1.1.7.2.cmml">e</mi><mrow id="S4.T2.2.2.2.1.m1.1.1.7.3" xref="S4.T2.2.2.2.1.m1.1.1.7.3.cmml"><mi id="S4.T2.2.2.2.1.m1.1.1.7.3.2" xref="S4.T2.2.2.2.1.m1.1.1.7.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.2.2.2.1.m1.1.1.7.3.1" xref="S4.T2.2.2.2.1.m1.1.1.7.3.1.cmml">​</mo><mi id="S4.T2.2.2.2.1.m1.1.1.7.3.3" xref="S4.T2.2.2.2.1.m1.1.1.7.3.3.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><apply id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1"><times id="S4.T2.2.2.2.1.m1.1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.1"></times><ci id="S4.T2.2.2.2.1.m1.1.1.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.2">𝑂</ci><ci id="S4.T2.2.2.2.1.m1.1.1.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.3">𝑟</ci><ci id="S4.T2.2.2.2.1.m1.1.1.4.cmml" xref="S4.T2.2.2.2.1.m1.1.1.4">𝑎</ci><ci id="S4.T2.2.2.2.1.m1.1.1.5.cmml" xref="S4.T2.2.2.2.1.m1.1.1.5">𝑐</ci><ci id="S4.T2.2.2.2.1.m1.1.1.6.cmml" xref="S4.T2.2.2.2.1.m1.1.1.6">𝑙</ci><apply id="S4.T2.2.2.2.1.m1.1.1.7.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7"><csymbol cd="ambiguous" id="S4.T2.2.2.2.1.m1.1.1.7.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7">subscript</csymbol><ci id="S4.T2.2.2.2.1.m1.1.1.7.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7.2">𝑒</ci><apply id="S4.T2.2.2.2.1.m1.1.1.7.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7.3"><times id="S4.T2.2.2.2.1.m1.1.1.7.3.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7.3.1"></times><ci id="S4.T2.2.2.2.1.m1.1.1.7.3.2.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7.3.2">𝑎</ci><ci id="S4.T2.2.2.2.1.m1.1.1.7.3.3.cmml" xref="S4.T2.2.2.2.1.m1.1.1.7.3.3">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">Oracle_{ae}</annotation></semantics></math></th>
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_b">30.8</td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_left ltx_border_b">24.3</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_left ltx_border_b">52.6</td>
<td id="S4.T2.2.2.2.5" class="ltx_td ltx_align_left ltx_border_b">118.9</td>
<td id="S4.T2.2.2.2.6" class="ltx_td ltx_align_left ltx_border_b">24.0</td>
<td id="S4.T2.2.2.2.7" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">90.43</td>
<td id="S4.T2.2.2.2.8" class="ltx_td ltx_align_left ltx_border_b"><span id="S4.T2.2.2.2.8.1" class="ltx_text ltx_font_bold">94.10</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.12.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Filtered score of oracle test.
<math id="S4.T2.7.m1.1" class="ltx_Math" alttext="Oracle_{e}" display="inline"><semantics id="S4.T2.7.m1.1b"><mrow id="S4.T2.7.m1.1.1" xref="S4.T2.7.m1.1.1.cmml"><mi id="S4.T2.7.m1.1.1.2" xref="S4.T2.7.m1.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T2.7.m1.1.1.1" xref="S4.T2.7.m1.1.1.1.cmml">​</mo><mi id="S4.T2.7.m1.1.1.3" xref="S4.T2.7.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.7.m1.1.1.1b" xref="S4.T2.7.m1.1.1.1.cmml">​</mo><mi id="S4.T2.7.m1.1.1.4" xref="S4.T2.7.m1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.7.m1.1.1.1c" xref="S4.T2.7.m1.1.1.1.cmml">​</mo><mi id="S4.T2.7.m1.1.1.5" xref="S4.T2.7.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.7.m1.1.1.1d" xref="S4.T2.7.m1.1.1.1.cmml">​</mo><mi id="S4.T2.7.m1.1.1.6" xref="S4.T2.7.m1.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.7.m1.1.1.1e" xref="S4.T2.7.m1.1.1.1.cmml">​</mo><msub id="S4.T2.7.m1.1.1.7" xref="S4.T2.7.m1.1.1.7.cmml"><mi id="S4.T2.7.m1.1.1.7.2" xref="S4.T2.7.m1.1.1.7.2.cmml">e</mi><mi id="S4.T2.7.m1.1.1.7.3" xref="S4.T2.7.m1.1.1.7.3.cmml">e</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.m1.1c"><apply id="S4.T2.7.m1.1.1.cmml" xref="S4.T2.7.m1.1.1"><times id="S4.T2.7.m1.1.1.1.cmml" xref="S4.T2.7.m1.1.1.1"></times><ci id="S4.T2.7.m1.1.1.2.cmml" xref="S4.T2.7.m1.1.1.2">𝑂</ci><ci id="S4.T2.7.m1.1.1.3.cmml" xref="S4.T2.7.m1.1.1.3">𝑟</ci><ci id="S4.T2.7.m1.1.1.4.cmml" xref="S4.T2.7.m1.1.1.4">𝑎</ci><ci id="S4.T2.7.m1.1.1.5.cmml" xref="S4.T2.7.m1.1.1.5">𝑐</ci><ci id="S4.T2.7.m1.1.1.6.cmml" xref="S4.T2.7.m1.1.1.6">𝑙</ci><apply id="S4.T2.7.m1.1.1.7.cmml" xref="S4.T2.7.m1.1.1.7"><csymbol cd="ambiguous" id="S4.T2.7.m1.1.1.7.1.cmml" xref="S4.T2.7.m1.1.1.7">subscript</csymbol><ci id="S4.T2.7.m1.1.1.7.2.cmml" xref="S4.T2.7.m1.1.1.7.2">𝑒</ci><ci id="S4.T2.7.m1.1.1.7.3.cmml" xref="S4.T2.7.m1.1.1.7.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.m1.1d">Oracle_{e}</annotation></semantics></math> using only answer feature and <math id="S4.T2.8.m2.1" class="ltx_Math" alttext="Oracle_{ae}" display="inline"><semantics id="S4.T2.8.m2.1b"><mrow id="S4.T2.8.m2.1.1" xref="S4.T2.8.m2.1.1.cmml"><mi id="S4.T2.8.m2.1.1.2" xref="S4.T2.8.m2.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T2.8.m2.1.1.1" xref="S4.T2.8.m2.1.1.1.cmml">​</mo><mi id="S4.T2.8.m2.1.1.3" xref="S4.T2.8.m2.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.8.m2.1.1.1b" xref="S4.T2.8.m2.1.1.1.cmml">​</mo><mi id="S4.T2.8.m2.1.1.4" xref="S4.T2.8.m2.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.8.m2.1.1.1c" xref="S4.T2.8.m2.1.1.1.cmml">​</mo><mi id="S4.T2.8.m2.1.1.5" xref="S4.T2.8.m2.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.8.m2.1.1.1d" xref="S4.T2.8.m2.1.1.1.cmml">​</mo><mi id="S4.T2.8.m2.1.1.6" xref="S4.T2.8.m2.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.8.m2.1.1.1e" xref="S4.T2.8.m2.1.1.1.cmml">​</mo><msub id="S4.T2.8.m2.1.1.7" xref="S4.T2.8.m2.1.1.7.cmml"><mi id="S4.T2.8.m2.1.1.7.2" xref="S4.T2.8.m2.1.1.7.2.cmml">e</mi><mrow id="S4.T2.8.m2.1.1.7.3" xref="S4.T2.8.m2.1.1.7.3.cmml"><mi id="S4.T2.8.m2.1.1.7.3.2" xref="S4.T2.8.m2.1.1.7.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.8.m2.1.1.7.3.1" xref="S4.T2.8.m2.1.1.7.3.1.cmml">​</mo><mi id="S4.T2.8.m2.1.1.7.3.3" xref="S4.T2.8.m2.1.1.7.3.3.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.m2.1c"><apply id="S4.T2.8.m2.1.1.cmml" xref="S4.T2.8.m2.1.1"><times id="S4.T2.8.m2.1.1.1.cmml" xref="S4.T2.8.m2.1.1.1"></times><ci id="S4.T2.8.m2.1.1.2.cmml" xref="S4.T2.8.m2.1.1.2">𝑂</ci><ci id="S4.T2.8.m2.1.1.3.cmml" xref="S4.T2.8.m2.1.1.3">𝑟</ci><ci id="S4.T2.8.m2.1.1.4.cmml" xref="S4.T2.8.m2.1.1.4">𝑎</ci><ci id="S4.T2.8.m2.1.1.5.cmml" xref="S4.T2.8.m2.1.1.5">𝑐</ci><ci id="S4.T2.8.m2.1.1.6.cmml" xref="S4.T2.8.m2.1.1.6">𝑙</ci><apply id="S4.T2.8.m2.1.1.7.cmml" xref="S4.T2.8.m2.1.1.7"><csymbol cd="ambiguous" id="S4.T2.8.m2.1.1.7.1.cmml" xref="S4.T2.8.m2.1.1.7">subscript</csymbol><ci id="S4.T2.8.m2.1.1.7.2.cmml" xref="S4.T2.8.m2.1.1.7.2">𝑒</ci><apply id="S4.T2.8.m2.1.1.7.3.cmml" xref="S4.T2.8.m2.1.1.7.3"><times id="S4.T2.8.m2.1.1.7.3.1.cmml" xref="S4.T2.8.m2.1.1.7.3.1"></times><ci id="S4.T2.8.m2.1.1.7.3.2.cmml" xref="S4.T2.8.m2.1.1.7.3.2">𝑎</ci><ci id="S4.T2.8.m2.1.1.7.3.3.cmml" xref="S4.T2.8.m2.1.1.7.3.3">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.m2.1d">Oracle_{ae}</annotation></semantics></math> use answer and explanation features.
<math id="S4.T2.9.m3.1" class="ltx_Math" alttext="Oracle_{e}" display="inline"><semantics id="S4.T2.9.m3.1b"><mrow id="S4.T2.9.m3.1.1" xref="S4.T2.9.m3.1.1.cmml"><mi id="S4.T2.9.m3.1.1.2" xref="S4.T2.9.m3.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T2.9.m3.1.1.1" xref="S4.T2.9.m3.1.1.1.cmml">​</mo><mi id="S4.T2.9.m3.1.1.3" xref="S4.T2.9.m3.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.9.m3.1.1.1b" xref="S4.T2.9.m3.1.1.1.cmml">​</mo><mi id="S4.T2.9.m3.1.1.4" xref="S4.T2.9.m3.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.9.m3.1.1.1c" xref="S4.T2.9.m3.1.1.1.cmml">​</mo><mi id="S4.T2.9.m3.1.1.5" xref="S4.T2.9.m3.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.9.m3.1.1.1d" xref="S4.T2.9.m3.1.1.1.cmml">​</mo><mi id="S4.T2.9.m3.1.1.6" xref="S4.T2.9.m3.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.9.m3.1.1.1e" xref="S4.T2.9.m3.1.1.1.cmml">​</mo><msub id="S4.T2.9.m3.1.1.7" xref="S4.T2.9.m3.1.1.7.cmml"><mi id="S4.T2.9.m3.1.1.7.2" xref="S4.T2.9.m3.1.1.7.2.cmml">e</mi><mi id="S4.T2.9.m3.1.1.7.3" xref="S4.T2.9.m3.1.1.7.3.cmml">e</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.m3.1c"><apply id="S4.T2.9.m3.1.1.cmml" xref="S4.T2.9.m3.1.1"><times id="S4.T2.9.m3.1.1.1.cmml" xref="S4.T2.9.m3.1.1.1"></times><ci id="S4.T2.9.m3.1.1.2.cmml" xref="S4.T2.9.m3.1.1.2">𝑂</ci><ci id="S4.T2.9.m3.1.1.3.cmml" xref="S4.T2.9.m3.1.1.3">𝑟</ci><ci id="S4.T2.9.m3.1.1.4.cmml" xref="S4.T2.9.m3.1.1.4">𝑎</ci><ci id="S4.T2.9.m3.1.1.5.cmml" xref="S4.T2.9.m3.1.1.5">𝑐</ci><ci id="S4.T2.9.m3.1.1.6.cmml" xref="S4.T2.9.m3.1.1.6">𝑙</ci><apply id="S4.T2.9.m3.1.1.7.cmml" xref="S4.T2.9.m3.1.1.7"><csymbol cd="ambiguous" id="S4.T2.9.m3.1.1.7.1.cmml" xref="S4.T2.9.m3.1.1.7">subscript</csymbol><ci id="S4.T2.9.m3.1.1.7.2.cmml" xref="S4.T2.9.m3.1.1.7.2">𝑒</ci><ci id="S4.T2.9.m3.1.1.7.3.cmml" xref="S4.T2.9.m3.1.1.7.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.m3.1d">Oracle_{e}</annotation></semantics></math> and <math id="S4.T2.10.m4.1" class="ltx_Math" alttext="Oracle_{ae}" display="inline"><semantics id="S4.T2.10.m4.1b"><mrow id="S4.T2.10.m4.1.1" xref="S4.T2.10.m4.1.1.cmml"><mi id="S4.T2.10.m4.1.1.2" xref="S4.T2.10.m4.1.1.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T2.10.m4.1.1.1" xref="S4.T2.10.m4.1.1.1.cmml">​</mo><mi id="S4.T2.10.m4.1.1.3" xref="S4.T2.10.m4.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.T2.10.m4.1.1.1b" xref="S4.T2.10.m4.1.1.1.cmml">​</mo><mi id="S4.T2.10.m4.1.1.4" xref="S4.T2.10.m4.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.10.m4.1.1.1c" xref="S4.T2.10.m4.1.1.1.cmml">​</mo><mi id="S4.T2.10.m4.1.1.5" xref="S4.T2.10.m4.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.T2.10.m4.1.1.1d" xref="S4.T2.10.m4.1.1.1.cmml">​</mo><mi id="S4.T2.10.m4.1.1.6" xref="S4.T2.10.m4.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.T2.10.m4.1.1.1e" xref="S4.T2.10.m4.1.1.1.cmml">​</mo><msub id="S4.T2.10.m4.1.1.7" xref="S4.T2.10.m4.1.1.7.cmml"><mi id="S4.T2.10.m4.1.1.7.2" xref="S4.T2.10.m4.1.1.7.2.cmml">e</mi><mrow id="S4.T2.10.m4.1.1.7.3" xref="S4.T2.10.m4.1.1.7.3.cmml"><mi id="S4.T2.10.m4.1.1.7.3.2" xref="S4.T2.10.m4.1.1.7.3.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.T2.10.m4.1.1.7.3.1" xref="S4.T2.10.m4.1.1.7.3.1.cmml">​</mo><mi id="S4.T2.10.m4.1.1.7.3.3" xref="S4.T2.10.m4.1.1.7.3.3.cmml">e</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.m4.1c"><apply id="S4.T2.10.m4.1.1.cmml" xref="S4.T2.10.m4.1.1"><times id="S4.T2.10.m4.1.1.1.cmml" xref="S4.T2.10.m4.1.1.1"></times><ci id="S4.T2.10.m4.1.1.2.cmml" xref="S4.T2.10.m4.1.1.2">𝑂</ci><ci id="S4.T2.10.m4.1.1.3.cmml" xref="S4.T2.10.m4.1.1.3">𝑟</ci><ci id="S4.T2.10.m4.1.1.4.cmml" xref="S4.T2.10.m4.1.1.4">𝑎</ci><ci id="S4.T2.10.m4.1.1.5.cmml" xref="S4.T2.10.m4.1.1.5">𝑐</ci><ci id="S4.T2.10.m4.1.1.6.cmml" xref="S4.T2.10.m4.1.1.6">𝑙</ci><apply id="S4.T2.10.m4.1.1.7.cmml" xref="S4.T2.10.m4.1.1.7"><csymbol cd="ambiguous" id="S4.T2.10.m4.1.1.7.1.cmml" xref="S4.T2.10.m4.1.1.7">subscript</csymbol><ci id="S4.T2.10.m4.1.1.7.2.cmml" xref="S4.T2.10.m4.1.1.7.2">𝑒</ci><apply id="S4.T2.10.m4.1.1.7.3.cmml" xref="S4.T2.10.m4.1.1.7.3"><times id="S4.T2.10.m4.1.1.7.3.1.cmml" xref="S4.T2.10.m4.1.1.7.3.1"></times><ci id="S4.T2.10.m4.1.1.7.3.2.cmml" xref="S4.T2.10.m4.1.1.7.3.2">𝑎</ci><ci id="S4.T2.10.m4.1.1.7.3.3.cmml" xref="S4.T2.10.m4.1.1.7.3.3">𝑒</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.m4.1d">Oracle_{ae}</annotation></semantics></math> show outstanding explanation score and accuracy in line with our intuition.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Automatic Evaluation</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In Table 1, we present the performance scores compared to state-of-the-art models in the filtered version.
The scoring method includes unfiltered scores, which measure all predictions regardless of whether they are correct or not, and filtered scores, which measure only the predictions that match the correct answers.
In VQA-NLE, generating a good explanation based on accurate answers is important, and providing a good explanation for incorrect answers is meaningless.
Therefore, filtered scores are given more consideration.
Follow the <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, VQA accuracy is measured as correct when the predicted answers are within the expected answers. Experimental results show that measuring the similarity between the query’s image and the sample’s explanation shows higher performance than measuring image-image similarity for memory retrieval.
Compared to recent state-of-the-art models, OURS shows a performance improvement of 2<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\sim</annotation></semantics></math>3% in the metric measured by explanation score.
Through these results, we can confirm that retrieval information helps generate more accurate answers and higher-quality explanations.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F3.1.1" class="ltx_text"><img src="/html/2408.17006/assets/fig.png" id="S4.F3.1.1.g1" class="ltx_graphics ltx_img_square" width="350" height="285" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.3.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>Example of retrieval sample and model generated prediction compare to ground truth.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Oracle Test</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">From Table 2, we can see how much the performance of our model can be improved when the ideal retrieval samples are retrieved.
Ideal retrieval is retrieved from a memory database, using cosine similarity of ground truth answer, explanation with sample’s answer, and explanation.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We conducted an Oracle test considering two cases: one using only the answer feature for input retrieval features and the other using both the answer and explanation features.
The result of the Oracle test shows that in line with our intuition, the explanation score significantly improved with the aid of the ideal sample’s explanation feature.
When the ideal answer feature is given with the explanation feature, Accuracy is raised to 94.10.
These results demonstrate that a simple structure of adding a cross-attention block in the LM block is sufficient for the model to gain reasoning from retrieval features.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we propose applying the retrieval augmentation method to the VQA-NLE task.
We define appropriate retrieval in terms of question type and semantic consistency.
For retrieving appropriate samples, we utilize cosine similarity on feature combinations.
The retrieved features are processed by the cross-attention in the GPT-2 language model.
Through these processes, ReRe generates answers and explanations simultaneously to aid with the retrieval feature.
ReRe shows improvement in accuracy and explanation score on VQA-X. It will be interesting future work to explore larger memory with better similarity matching to further improve the performance of the VQA-NLE task.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel,

</span>
<span class="ltx_bibblock">“Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2018, pp. 3674–3683.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Yuchao Feng, Wei Hua, and Yuxiang Sun,

</span>
<span class="ltx_bibblock">“Nle-dm: Natural-language explanations for decision making of autonomous driving based on semantic scene understanding,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</span>, 2023.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Ana Marasović, Chandra Bhagavatula, Jae Sung Park, Ronan Le Bras, Noah A Smith, and Yejin Choi,

</span>
<span class="ltx_bibblock">“Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Findings of EMNLP</span>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chengen Lai, Shengli Song, Shiqi Meng, Jingyang Li, Sitong Yan, and Guangneng Hu,

</span>
<span class="ltx_bibblock">“Towards more faithful natural language explanation using multi-level contrastive learning in vqa,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 2024, vol. 38, pp. 2849–2857.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Wei Suo, Mengyang Sun, Weisong Liu, Yiqi Gao, Peng Wang, Yanning Zhang, and Qi Wu,

</span>
<span class="ltx_bibblock">“S3c: Semi-supervised vqa natural language explanation via self-critical learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, 2023, pp. 2646–2656.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Jiaxin Ge, Sanjay Subramanian, Trevor Darrell, and Boyi Li,

</span>
<span class="ltx_bibblock">“From wrong to right: A recursive approach towards vision-language explanation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Empirical Methods in Natural Language Processing (EMNLP)</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan,

</span>
<span class="ltx_bibblock">“Learn to explain: Multimodal reasoning via thought chains for science question answering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 2507–2521, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Junting Pan, Ziyi Lin, Yuying Ge, Xiatian Zhu, Renrui Zhang, Yi Wang, Yu Qiao, and Hongsheng Li,

</span>
<span class="ltx_bibblock">“Retrieving-to-answer: Zero-shot video question answering with frozen large language models,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, 2023, pp. 272–283.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Rita Ramos, Bruno Martins, Desmond Elliott, and Yova Kementchedjhieva,

</span>
<span class="ltx_bibblock">“Smallcap: lightweight image captioning prompted with retrieval augmentation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, 2023, pp. 2840–2849.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara,

</span>
<span class="ltx_bibblock">“Retrieval-augmented transformer for image captioning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the 19th international conference on content-based multimedia indexing</span>, 2022, pp. 1–7.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz,

</span>
<span class="ltx_bibblock">“A multi-world approach to question answering about real-world scenes based on uncertain input,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 27, 2014.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang,

</span>
<span class="ltx_bibblock">“Bottom-up and top-down attention for image captioning and visual question answering,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2018, pp. 6077–6086.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Chao Ma, Chunhua Shen, Anthony Dick, Qi Wu, Peng Wang, Anton Van den Hengel, and Ian Reid,

</span>
<span class="ltx_bibblock">“Visual question answering with memory-augmented networks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2018, pp. 6975–6984.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell,

</span>
<span class="ltx_bibblock">“Generating visual explanations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016, pp. 3–19.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra,

</span>
<span class="ltx_bibblock">“Grad-cam: Visual explanations from deep networks via gradient-based localization,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2017, pp. 618–626.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach,

</span>
<span class="ltx_bibblock">“Multimodal explanations: Justifying decisions and pointing to the evidence,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2018, pp. 8779–8788.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Maxime Kayser, Oana-Maria Camburu, Leonard Salewski, Cornelius Emde, Virginie Do, Zeynep Akata, and Thomas Lukasiewicz,

</span>
<span class="ltx_bibblock">“e-vil: A dataset and benchmark for natural language explanations in vision-language tasks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2021, pp. 1244–1254.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu,

</span>
<span class="ltx_bibblock">“Uniter: Universal image-text representation learning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2020, pp. 104–120.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.,

</span>
<span class="ltx_bibblock">“Language models are unsupervised multitask learners,”

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, vol. 1, no. 8, pp. 9, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Fawaz Sammani, Tanmoy Mukherjee, and Nikos Deligiannis,

</span>
<span class="ltx_bibblock">“Nlx-gpt: A model for natural language explanations in vision and vision-language tasks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, 2022, pp. 8322–8332.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Chenxi Whitehouse, Tillman Weyde, and Pranava Madhyastha,

</span>
<span class="ltx_bibblock">“Towards a unified model for generating answers and explanations in visual question answering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.10799</span>, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.,

</span>
<span class="ltx_bibblock">“Improving language models by retrieving from trillions of tokens,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>. PMLR, 2022, pp. 2206–2240.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang,

</span>
<span class="ltx_bibblock">“Retrieval augmented language model pre-training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>. PMLR, 2020, pp. 3929–3938.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.,

</span>
<span class="ltx_bibblock">“Retrieval-augmented generation for knowledge-intensive nlp tasks,”

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, vol. 33, pp. 9459–9474, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, and Seong Tae Kim,

</span>
<span class="ltx_bibblock">“Do you remember? dense video captioning with cross-modal memory retrieval,”

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.07610</span>, 2024.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.,

</span>
<span class="ltx_bibblock">“Learning transferable visual models from natural language supervision,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>. PMLR, 2021, pp. 8748–8763.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu,

</span>
<span class="ltx_bibblock">“Bleu: a method for automatic evaluation of machine translation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ACL</span>, 2002, pp. 311–318.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie,

</span>
<span class="ltx_bibblock">“Meteor: An automatic metric for mt evaluation with improved correlation with human judgments,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</span>, 2005, pp. 65–72.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Chin-Yew Lin,

</span>
<span class="ltx_bibblock">“Rouge: A package for automatic evaluation of summaries,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Text summarization branches out</span>, 2004, pp. 74–81.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh,

</span>
<span class="ltx_bibblock">“Cider: Consensus-based image description evaluation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CVPR</span>, 2015, pp. 4566–4575.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould,

</span>
<span class="ltx_bibblock">“Spice: Semantic propositional image caption evaluation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">ECCV</span>, 2016, pp. 382–398.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi,

</span>
<span class="ltx_bibblock">“Bertscore: Evaluating text generation with bert,”

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">ICLR</span>, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Jialin Wu and Raymond J Mooney,

</span>
<span class="ltx_bibblock">“Faithful multimodal explanation for visual question answering,”

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">ACL BlackboxNLP workshop</span>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Qian Yang, Yunxin Li, Baotian Hu, Lin Ma, Yuxin Ding, and Min Zhang,

</span>
<span class="ltx_bibblock">“Chunk-aware alignment and lexical constraint for visual entailment with natural language explanations,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of the 30th ACM International Conference on Multimedia</span>, 2022, pp. 3587–3597.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.17005" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.17006" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.17006">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.17006" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.17007" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 15:54:20 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
