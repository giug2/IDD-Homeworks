<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PyMarian: Fast Neural Machine Translation and Evaluation in Python</title>
<!--Generated on Thu Aug 15 01:34:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.11853v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S1" title="In PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S2" title="In PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>PyMarian API</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S2.SS1" title="In 2 PyMarian API ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Translator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S2.SS2" title="In 2 PyMarian API ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Trainer</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3" title="In PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Fast MT Evaluation in PyMarian</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.SS1" title="In 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Evaluator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.SS2" title="In 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.SS3" title="In 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Benchmarks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S4" title="In PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Example applications</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S4.SS1" title="In 4 Example applications ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Jupyter notebook</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S4.SS2" title="In 4 Example applications ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>OPUS-MT models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S4.SS3" title="In 4 Example applications ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Web app demo</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S5" title="In PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S6" title="In PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Summary</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">PyMarian: Fast Neural Machine Translation and Evaluation in Python</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thamme Gowda<sup class="ltx_sup" id="id9.8.id1">1</sup>     Roman Grundkiewicz<sup class="ltx_sup" id="id10.9.id2">1</sup>     Elijah Rippeth<sup class="ltx_sup" id="id11.10.id3">2</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.2">Matt Post<sup class="ltx_sup" id="id5.5.2.1"><span class="ltx_text ltx_font_medium" id="id5.5.2.1.1">1</span></sup>      Marcin Junczys-Dowmunt<sup class="ltx_sup" id="id5.5.2.2"><span class="ltx_text ltx_font_medium" id="id5.5.2.2.1">1</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id12.11.id4">1</sup> Microsoft Translator 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id13.12.id5">{thammegowda,rogrundk,mattpost,marcinjd}@microsoft.com</span>
<br class="ltx_break"/><sup class="ltx_sup" id="id14.13.id6">2</sup> University of Maryland 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id15.14.id7">erip@cs.umd.edu</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id8.1">The deep learning language of choice these days is Python; measured by factors such as available libraries and technical support, it is hard to beat.
At the same time, software written in lower-level programming languages like C++ retain advantages in speed.
We describe a Python interface to Marian NMT, a C++-based training and inference toolkit for sequence-to-sequence models, focusing on machine translation.
This interface enables models trained with Marian to be connected to the rich, wide range of tools available in Python.
A highlight of the interface is the ability to compute state-of-the-art COMET metrics from Python but using Marian’s inference engine, with a speedup factor of up to 7.8<math alttext="\times" class="ltx_Math" display="inline" id="id8.1.m1.1"><semantics id="id8.1.m1.1a"><mo id="id8.1.m1.1.1" xref="id8.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id8.1.m1.1b"><times id="id8.1.m1.1.1.cmml" xref="id8.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id8.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id8.1.m1.1d">×</annotation></semantics></math> the existing implementations.
We also briefly spotlight a number of other integrations, including Jupyter notebooks, connection with prebuilt models, and a web app interface provided with the package.
PyMarian is available in PyPI via <span class="ltx_text ltx_font_typewriter" id="id8.1.1">pip install pymarian</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.7">
<p class="ltx_p" id="p1.7.8"><span class="ltx_text ltx_font_bold" id="p1.7.8.1">PyMarian: Fast Neural Machine Translation and Evaluation in Python</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.7.7" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.7.7.7" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.7.7.7.7">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.3.3.3.3.3">
<span class="ltx_td ltx_align_center" id="p1.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="p1.3.3.3.3.3.3.3">Thamme Gowda<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.1"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.1.1">1</span></sup>     Roman Grundkiewicz<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.2"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.2.1">1</span></sup>     Elijah Rippeth<sup class="ltx_sup" id="p1.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_medium" id="p1.3.3.3.3.3.3.3.3.1">2</span></sup></span></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.2"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.2.2">Matt Post<sup class="ltx_sup" id="p1.5.5.5.5.5.2.2.1"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.2.2.1.1">1</span></sup>      Marcin Junczys-Dowmunt<sup class="ltx_sup" id="p1.5.5.5.5.5.2.2.2"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.2.2.2.1">1</span></sup></span></span></span>
<span class="ltx_tr" id="p1.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.1"><sup class="ltx_sup" id="p1.6.6.6.6.6.1.1">1</sup> Microsoft Translator</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.8.1">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.8.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.1.1">{thammegowda,rogrundk,mattpost,marcinjd}@microsoft.com</span></span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.1"><sup class="ltx_sup" id="p1.7.7.7.7.7.1.1">2</sup> University of Maryland</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.9.2">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.9.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.9.2.1.1">erip@cs.umd.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Marian NMT<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://marian-nmt.github.io" title="">https://marian-nmt.github.io</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib4" title="">2018a</a>)</cite> was one of the earliest training and inference toolkits for sequence-to-sequence-based machine translation.
Originally written under the name <code class="ltx_verbatim ltx_font_typewriter" id="S1.p1.1.1">amun</code> and providing fast inference for Groundhog-trained models,<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pascanur/GroundHog" title="">https://github.com/pascanur/GroundHog</a></span></span></span> it was quickly built up to also provide speedy, reliable multi-GPU and multi-node training of Transformer models, along with many other features.
It has been widely used in commercial production settings <cite class="ltx_cite ltx_citemacro_cite">Junczys-Dowmunt et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib5" title="">2018b</a>)</cite>, for academic and industrial research, for the distribution of pre-trained models <cite class="ltx_cite ltx_citemacro_cite">Tiedemann and Thottingal (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib17" title="">2020a</a>)</cite>, and as the basis for extremely fast in-browser translation <cite class="ltx_cite ltx_citemacro_cite">Bogoychev et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib1" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Many of these features were enabled by its efficient C++-backend, but it must be admitted that this dependency is also a barrier to many researchers, who increasingly work with Python.
This paper describes a new set of Python bindings that have been added to Marian.
Written using Pybind11, these bindings are available as a <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.1">pip</span>-installable Python package via the Python Package Index<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pypi.org/project/pymarian" title="">https://pypi.org/project/pymarian</a></span></span></span> or can be installed from Marian’s source.
We then describe several features and applications facilitated by PyMarian:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.1">Inference and training</em> (§ <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S2" title="2 PyMarian API ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">2</span></a>).
It is easy to load Marian-trained models and send data through them for translation.
This also makes it easy to translate with publicly-available models, and to plug them into other Python codebases.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.1">Fast evaluation</em> (§ <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3" title="3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">3</span></a>).
Model-based metrics such as COMET and BLEURT have demonstrated their superiority, but their provided toolsets make them slow to compute.
We provide <span class="ltx_text ltx_font_typewriter" id="S1.I1.i2.p1.1.2">pymarian-eval</span>, which makes use of converted models, packaged in a Python CLI interface.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.1">Example applications</em> (§ <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S4" title="4 Example applications ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">4</span></a>).
We demonstrate the versatility of <span class="ltx_text ltx_font_typewriter" id="S1.I1.i3.p1.1.2">pymarian</span> with a number of examples including a web-based demonstration framework.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S1.p2.2">A particular focus of the paper is in benchmarking popular COMET models reimplemented in Marian and available through PyMarian (§ <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.SS3" title="3.3 Benchmarks ‣ 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">3.3</span></a>), which run significantly faster than in their native implementations, providing up to 7.8x speedup in a multi-GPU setting.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>PyMarian API</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">PyMarian offers <span class="ltx_text ltx_font_typewriter" id="S2.p1.1.1">pymarian</span> Python package containing convenient high level APIs.
We use Pybind11<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pybind/pybind11" title="">https://github.com/pybind/pybind11</a></span></span></span> to bind the Python calls to Marian C++ APIs.
<span class="ltx_text ltx_font_typewriter" id="S2.p1.1.2">pymarian</span> uses the same configuration system as Marian, however makes it Pythonic by offering keyword-argument (i.e., <span class="ltx_text ltx_font_italic" id="S2.p1.1.3">**kwargs</span>).</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">At the package’s top level, we have three classes: <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.1">Translator</span>, <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.2">Trainer</span> and <span class="ltx_text ltx_font_typewriter" id="S2.p2.1.3">Evaluator</span>. First two are described in this section, while the evaluator is presented in details later in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3" title="3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Translator</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The Python API for decoding Marian models with beam search is provided by <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p1.1.1">Translator</span> class.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<span class="ltx_ERROR undefined" id="S2.SS1.p2.1">{minted}</span>
<p class="ltx_p" id="S2.SS1.p2.2">[xleftmargin=0.2em]python
from pymarian import Translator
mt = Translator(
models="model.ende.npz",
vocabs=["vocab.spm", "vocab.spm"]
)
hyp = mt.translate("Hello world!")
print(hyp) # "Hallo Welt!"</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">It offers the same hyperparameters and functionalities as the translation service in C++, such as:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Translation speed optimization with custom beam search sizes (<span class="ltx_text ltx_font_typewriter" id="S2.I1.i1.p1.1.1">beam_size</span>), batch organization (<span class="ltx_text ltx_font_typewriter" id="S2.I1.i1.p1.1.2">mini_batch</span>, <span class="ltx_text ltx_font_typewriter" id="S2.I1.i1.p1.1.3">mini_batch_sort</span>), and <span class="ltx_text ltx_font_typewriter" id="S2.I1.i1.p1.1.4">fp16</span>;</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><math alttext="n" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><mi id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><ci id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_n</annotation></semantics></math>-best lists translation (<span class="ltx_text ltx_font_typewriter" id="S2.I1.i2.p1.1.1">n_best=True</span>);</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Word alignments (e.g., <span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.p1.1.1">alignment="hard"</span>) and word-level scores (<span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.p1.1.2">word_scores=True</span>) when more detailed subword-level information is needed (<span class="ltx_text ltx_font_typewriter" id="S2.I1.i3.p1.1.3">no_spm_encode=True</span>);</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1">Noised sampling from full distribution and top-K sampling with custom temperatures (e.g., <span class="ltx_text ltx_font_typewriter" id="S2.I1.i4.p1.1.1">output_sampling="topk 100 0.1"</span>);</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i5.p1">
<p class="ltx_p" id="S2.I1.i5.p1.1">Force-decoding of given target language prefixes (<span class="ltx_text ltx_font_typewriter" id="S2.I1.i5.p1.1.1">force_decode=True</span>).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Trainer</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Python API for training models supported in Marian toolkit is provided by the <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.1">Trainer</span> class.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<span class="ltx_ERROR undefined" id="S2.SS2.p2.1">{minted}</span>
<p class="ltx_p" id="S2.SS2.p2.2">[ xleftmargin=0.2em]python
from pymarian import Trainer
args = 
"type": "transformer",
"model": "model.npz",
"train_sets": ["train.en", "train.de"],
"vocabs": ["vocab.spm", "vocab.spm"],

trainer = Trainer(**args)
trainer.train()</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Complete examples are available in Marian’s source code in <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p3.1.1">src/python/tests/regression</span>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Fast MT Evaluation in PyMarian</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The Marian NMT had been a toolkit for translation and language modeling with the emphasis on speed.
With the recent revision of Marian toolkit, we have implemented evaluation metrics, for both training and fast inferencing, while retaining its emphasis on speed.
In addition, we have also enabled evaluator APIs in Python module, via a class named <span class="ltx_text ltx_font_typewriter" id="S3.p1.1.1">Evaluator</span>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluator</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">Evaluator</span> supports scoring MT hypothesis with either
source, or reference, or both.
Generally, evaluators are classified into reference-free (quality estimation) and reference-based types.
We provide implementations of both types.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<span class="ltx_ERROR undefined" id="S3.SS1.p2.1">{minted}</span>
<p class="ltx_p" id="S3.SS1.p2.2">[xleftmargin=0.2em]python
from pathlib import Path
from pymarian import Evaluator</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">evaluator = Evaluator.new(
model_file="marian.model.bin",
vocab_file="vocab.spm",
like="comet-qe", quiet=True,
fp16=False, cpu_threads=4)</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">srcs = [’Hello’, ’Howdy’]
mts = [’Howdy’, ’Hello’]
lines = (f’st͡’
for s,t in zip(srcs, mts))
scores = evaluator.evaluate(lines)
for score in scores:
print(f’score:.4f’)</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Metrics</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Along with providing implementation for Evaluator, we also provide checkpoints for some of the popular MT metrics, such as COMETs and BLEURT.
Since the checkpoint file format of the existing metrics are incompatible with Marian toolkit, we have converted them to the required format and released on Huggingface.<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/models" title="">https://huggingface.co/models</a></span></span></span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.T1" title="Table 1 ‣ 3.2 Metrics ‣ 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">1</span></a> shows the available models and their IDs on HuggingFace hub.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Fields</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1">Reference</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.4.1">HuggingFace ID</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.2.1.1">bleurt-20</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.2.1.2">T, R</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.2.1.3"><cite class="ltx_cite ltx_citemacro_citet">Sellam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib15" title="">2020</a>)</cite></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.2.1.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/marian-nmt/bleurt-20" title="">marian-nmt/bleurt-20</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.1">wmt20-comet-da</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.2">S, T, R</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.3"><cite class="ltx_cite ltx_citemacro_citet">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib13" title="">2020</a>)</cite></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.3.2.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt20-comet-da-marian" title="">unbabel/wmt20-comet-da-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.1">wmt20-comet-qe-da</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.3.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt20-comet-qe-da-marian" title="">unbabel/wmt20-comet-qe-da-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.1">wmt20-comet-qe-da-v2</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.4.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.4.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt20-comet-qe-da-v2-marian" title="">unbabel/wmt20-comet-qe-da-v2-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.1">wmt21-comet-da</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.2">S, T, R</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5.3"><cite class="ltx_cite ltx_citemacro_citet">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib11" title="">2021</a>)</cite></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.5.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt21-comet-da-marian" title="">unbabel/wmt21-comet-da-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.6.1">wmt21-comet-qe-da</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.6.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.6.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.6.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt21-comet-qe-da-marian" title="">unbabel/wmt21-comet-qe-da-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.7.1">wmt21-comet-qe-mqm</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.7.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.7.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.7.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt21-comet-qe-mqm-marian" title="">unbabel/wmt21-comet-qe-mqm-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.8">
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.8.1">wmt22-comet-da</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.8.2">S, T, R</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.8.3"><cite class="ltx_cite ltx_citemacro_citet">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib10" title="">2022a</a>)</cite></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.8.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt22-comet-da-marian" title="">unbabel/wmt22-comet-da-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.9">
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.9.1">wmt22-cometkiwi-da</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.9.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.9.3"><cite class="ltx_cite ltx_citemacro_citet">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib14" title="">2022b</a>)</cite></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.9.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt22-cometkiwi-da-marian" title="">unbabel/wmt22-cometkiwi-da-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.10">
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.10.1">wmt23-cometkiwi-da-xl</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.10.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.10.3"><cite class="ltx_cite ltx_citemacro_citet">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib12" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.10.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt23-cometkiwi-da-xl-marian" title="">unbabel/wmt23-cometkiwi-da-xl-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.11">
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.11.1">wmt23-cometkiwi-da-xxl</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.11.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.11.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.11.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/unbabel/wmt23-cometkiwi-da-xxl-marian" title="">unbabel/wmt23-cometkiwi-da-xxl-marian</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.12">
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.12.1">cometoid22-wmt21</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.12.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.12.3"><cite class="ltx_cite ltx_citemacro_citet">Gowda et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib2" title="">2023</a>)</cite></td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.12.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/marian-nmt/cometoid22-wmt21" title="">marian-nmt/cometoid22-wmt21</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.13">
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.13.1">cometoid22-wmt22</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.13.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.13.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.13.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/marian-nmt/cometoid22-wmt22" title="">marian-nmt/cometoid22-wmt22</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.14">
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.14.1">cometoid22-wmt23</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.14.2">S, T</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.14.3">"</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.14.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/marian-nmt/cometoid22-wmt23" title="">marian-nmt/cometoid22-wmt23</a></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.15">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.16.15.1">chrfoid-wmt23</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.16.15.2">S, T</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.16.15.3">"</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.16.15.4"><a class="ltx_ref ltx_href ltx_font_typewriter" href="https://huggingface.co/marian-nmt/chrfoid-wmt23" title="">marian-nmt/chrfoid-wmt23</a></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>List of metrics supported in <span class="ltx_text ltx_font_typewriter" id="S3.T1.8.1">pymarian</span>, their required fields, reference, and HuggingFace model IDs.
Fields S, T, and R are <em class="ltx_emph ltx_font_italic" id="S3.T1.9.2">source</em>, <em class="ltx_emph ltx_font_italic" id="S3.T1.10.3">translation</em> (also variously called the <em class="ltx_emph ltx_font_italic" id="S3.T1.11.4">candidate</em> or <em class="ltx_emph ltx_font_italic" id="S3.T1.12.5">hypothesis</em>), and <em class="ltx_emph ltx_font_italic" id="S3.T1.13.6">reference</em>, respectively.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Using the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.1">Evaluator</span> API, we have developed a command-line utility named <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p2.1.2">pymarian-eval</span>, which internally takes care of downloading models from HuggingFace model hub and caching them locally.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We provide <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.1">-a|----average</span> option for obtaining the system level score only (<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.2">-a only</span>), segment level scores only (<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.3">-a skip</span>), or both where average is appended (<span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.1.4">-a append</span>). For example,
<span class="ltx_ERROR undefined" id="S3.SS2.p3.1.5">{minted}</span>shell
pymarian-eval -m wmt22-cometkiwi-da  -s src.txt -t mt.txt -a only</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The current toolkits that originally implement the popular metrics consume higher memory and time for loading the checkpoints than necessary.
This is increasingly problematic as metric checkpoint files are getting bigger over the years. The format used by Marian is optimized for faster loading with minimal memory overhead.
We present the model loading time and memory utilization in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.T2" title="Table 2 ‣ 3.2 Metrics ‣ 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">2</span></a>.
For instance, consider <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.1">wmt23-cometkiwi-da-xl</span>, whose checkpoint file is 13.9GB.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><span class="ltx_text ltx_font_typewriter" id="footnote6.1">wmt23-cometkiwi-da-xxl</span> is 42.9GB and we were unable to load it on the GPUs used for benchmarks in this paper (32GB V100).</span></span></span>
The <span class="ltx_text ltx_font_italic" id="S3.SS2.p4.1.2">original</span> tool (comet-score) takes 27GB RAM and 530 seconds to warmup on 8 GPUs, where as <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p4.1.3">pymarian-eval</span> achieves the same in half the RAM and only 12 seconds.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="6" id="S3.T2.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">Time (seconds)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T2.1.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Memory (MB)</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_column" id="S3.T2.1.2.2.1" style="padding-top:1pt;padding-bottom:1pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="3" id="S3.T2.1.2.2.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.2.1">1 GPU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="3" id="S3.T2.1.2.2.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.3.1">8 GPUs</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="S3.T2.1.2.2.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.4.1">1 GPU</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="S3.T2.1.2.2.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.2.2.5.1">8 GPUs</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S3.T2.1.3.3.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.1.1">Model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.2.1">Orig</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.3.1">Ours</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.4.1">Speedup</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.5.1">Orig</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.6" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.6.1">Ours</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.3.3.7" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.7.1">Speedup</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.8" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.8.1">Orig</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.9" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.9.1">Ours</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.10" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.10.1">Orig</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.3.3.11" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.3.11.1">Ours</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.4.1.1" style="padding-top:1pt;padding-bottom:1pt;">bleurt-20</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.2" style="padding-top:1pt;padding-bottom:1pt;">23.7</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.3" style="padding-top:1pt;padding-bottom:1pt;">3.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.4" style="padding-top:1pt;padding-bottom:1pt;">7.9x</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.5" style="padding-top:1pt;padding-bottom:1pt;">NA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.6" style="padding-top:1pt;padding-bottom:1pt;">8.4</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T2.1.4.1.7" style="padding-top:1pt;padding-bottom:1pt;">NA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.8" style="padding-top:1pt;padding-bottom:1pt;">6,606</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.9" style="padding-top:1pt;padding-bottom:1pt;">2,640</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.10" style="padding-top:1pt;padding-bottom:1pt;">NA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T2.1.4.1.11" style="padding-top:1pt;padding-bottom:1pt;">3,455</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.2">
<td class="ltx_td ltx_align_left" id="S3.T2.1.5.2.1" style="padding-top:1pt;padding-bottom:1pt;">wmt20-comet-da</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.2" style="padding-top:1pt;padding-bottom:1pt;">37.0</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.3" style="padding-top:1pt;padding-bottom:1pt;">4.6</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.4" style="padding-top:1pt;padding-bottom:1pt;">8.0x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.5" style="padding-top:1pt;padding-bottom:1pt;">193.8</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.6" style="padding-top:1pt;padding-bottom:1pt;">9.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T2.1.5.2.7" style="padding-top:1pt;padding-bottom:1pt;">19.9x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.8" style="padding-top:1pt;padding-bottom:1pt;">5,387</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.9" style="padding-top:1pt;padding-bottom:1pt;">2,782</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.10" style="padding-top:1pt;padding-bottom:1pt;">5,388</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.5.2.11" style="padding-top:1pt;padding-bottom:1pt;">3,598</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.6.3">
<td class="ltx_td ltx_align_left" id="S3.T2.1.6.3.1" style="padding-top:1pt;padding-bottom:1pt;">wmt20-comet-qe-da</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.2" style="padding-top:1pt;padding-bottom:1pt;">32.6</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.3" style="padding-top:1pt;padding-bottom:1pt;">3.8</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.4" style="padding-top:1pt;padding-bottom:1pt;">8.6x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.5" style="padding-top:1pt;padding-bottom:1pt;">197.3</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.6" style="padding-top:1pt;padding-bottom:1pt;">8.9</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T2.1.6.3.7" style="padding-top:1pt;padding-bottom:1pt;">22.1x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.8" style="padding-top:1pt;padding-bottom:1pt;">5,276</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.9" style="padding-top:1pt;padding-bottom:1pt;">2,682</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.10" style="padding-top:1pt;padding-bottom:1pt;">5,278</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.6.3.11" style="padding-top:1pt;padding-bottom:1pt;">3,499</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.7.4">
<td class="ltx_td ltx_align_left" id="S3.T2.1.7.4.1" style="padding-top:1pt;padding-bottom:1pt;">wmt22-comet-da</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.2" style="padding-top:1pt;padding-bottom:1pt;">37.9</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.3" style="padding-top:1pt;padding-bottom:1pt;">4.5</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.4" style="padding-top:1pt;padding-bottom:1pt;">8.5x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.5" style="padding-top:1pt;padding-bottom:1pt;">193.5</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.6" style="padding-top:1pt;padding-bottom:1pt;">9.7</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T2.1.7.4.7" style="padding-top:1pt;padding-bottom:1pt;">20.0x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.8" style="padding-top:1pt;padding-bottom:1pt;">5,365</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.9" style="padding-top:1pt;padding-bottom:1pt;">2,786</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.10" style="padding-top:1pt;padding-bottom:1pt;">5,364</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.7.4.11" style="padding-top:1pt;padding-bottom:1pt;">3,603</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.8.5">
<td class="ltx_td ltx_align_left" id="S3.T2.1.8.5.1" style="padding-top:1pt;padding-bottom:1pt;">wmt22-cometkiwi-da</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.2" style="padding-top:1pt;padding-bottom:1pt;">33.9</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.3" style="padding-top:1pt;padding-bottom:1pt;">3.3</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.4" style="padding-top:1pt;padding-bottom:1pt;">10.2x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.5" style="padding-top:1pt;padding-bottom:1pt;">199.1</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.6" style="padding-top:1pt;padding-bottom:1pt;">8.8</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T2.1.8.5.7" style="padding-top:1pt;padding-bottom:1pt;">22.7x</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.8" style="padding-top:1pt;padding-bottom:1pt;">5,244</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.9" style="padding-top:1pt;padding-bottom:1pt;">2,623</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.10" style="padding-top:1pt;padding-bottom:1pt;">5,246</td>
<td class="ltx_td ltx_align_right" id="S3.T2.1.8.5.11" style="padding-top:1pt;padding-bottom:1pt;">3,438</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.9.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.9.6.1" style="padding-top:1pt;padding-bottom:1pt;">wmt23-cometkiwi-da-xl</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.2" style="padding-top:1pt;padding-bottom:1pt;">108.5</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.3" style="padding-top:1pt;padding-bottom:1pt;">7.5</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.4" style="padding-top:1pt;padding-bottom:1pt;">14.4x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.5" style="padding-top:1pt;padding-bottom:1pt;">530.2</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.6" style="padding-top:1pt;padding-bottom:1pt;">12.1</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T2.1.9.6.7" style="padding-top:1pt;padding-bottom:1pt;">43.9x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.8" style="padding-top:1pt;padding-bottom:1pt;">27,554</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.9" style="padding-top:1pt;padding-bottom:1pt;">13,815</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.10" style="padding-top:1pt;padding-bottom:1pt;">27,554</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T2.1.9.6.11" style="padding-top:1pt;padding-bottom:1pt;">14,631</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Model load time (seconds) and memory (megabytes) taken to initialize the models and score a single example.
Marian and <span class="ltx_text ltx_font_typewriter" id="S3.T2.3.1">pymarian</span> use memory-mapped files, which enable faster loading than original implementation.
Numbers are the average of three runs. </figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.47">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.47.48.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T3.47.48.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S3.T3.47.48.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.47.48.1.2.1">Time (seconds)</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T3.47.48.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.47.48.1.3.1">Speedup</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.47.49.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.47.49.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.1.1">Metric</span></th>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.2"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.2.1">Original</span></td>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.3"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.3.1">Marian</span></td>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.4"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.4.1">PyM</span></td>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.5"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.5.1">PyM FP16</span></td>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.6"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.6.1">Marian</span></td>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.7"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.7.1">PyM</span></td>
<td class="ltx_td ltx_align_right" id="S3.T3.47.49.2.8"><span class="ltx_text ltx_font_bold" id="S3.T3.47.49.2.8.1">PyM FP16</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.47.50.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="8" id="S3.T3.47.50.3.1"><span class="ltx_text ltx_font_italic" id="S3.T3.47.50.3.1.1">1 GPU</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.4.4.5">bleurt-20</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.1.1.1">2312<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.m1.1.1" xref="S3.T3.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T3.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.m1.1d">±</annotation></semantics></math>2.2</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.2.2.2">635<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.2.2.2.m1.1"><semantics id="S3.T3.2.2.2.m1.1a"><mo id="S3.T3.2.2.2.m1.1.1" xref="S3.T3.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.m1.1b"><csymbol cd="latexml" id="S3.T3.2.2.2.m1.1.1.cmml" xref="S3.T3.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.m1.1d">±</annotation></semantics></math>0.3</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.3.3.3">656<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.3.3.3.m1.1"><semantics id="S3.T3.3.3.3.m1.1a"><mo id="S3.T3.3.3.3.m1.1.1" xref="S3.T3.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.m1.1b"><csymbol cd="latexml" id="S3.T3.3.3.3.m1.1.1.cmml" xref="S3.T3.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.m1.1d">±</annotation></semantics></math>0.3</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.4.4.4">467<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.4.4.4.m1.1"><semantics id="S3.T3.4.4.4.m1.1a"><mo id="S3.T3.4.4.4.m1.1.1" xref="S3.T3.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.m1.1b"><csymbol cd="latexml" id="S3.T3.4.4.4.m1.1.1.cmml" xref="S3.T3.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.4.m1.1d">±</annotation></semantics></math>0.6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.4.4.6">3.6x</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.4.4.7">3.5x</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.4.4.8">4.9x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.8.8.5">wmt20-comet-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.5.5.1">3988<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.5.5.1.m1.1"><semantics id="S3.T3.5.5.1.m1.1a"><mo id="S3.T3.5.5.1.m1.1.1" xref="S3.T3.5.5.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.1.m1.1b"><csymbol cd="latexml" id="S3.T3.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.5.5.1.m1.1d">±</annotation></semantics></math>0.8</td>
<td class="ltx_td ltx_align_right" id="S3.T3.6.6.2">954<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.6.6.2.m1.1"><semantics id="S3.T3.6.6.2.m1.1a"><mo id="S3.T3.6.6.2.m1.1.1" xref="S3.T3.6.6.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.2.m1.1b"><csymbol cd="latexml" id="S3.T3.6.6.2.m1.1.1.cmml" xref="S3.T3.6.6.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.6.6.2.m1.1d">±</annotation></semantics></math>1.0</td>
<td class="ltx_td ltx_align_right" id="S3.T3.7.7.3">968<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.7.7.3.m1.1"><semantics id="S3.T3.7.7.3.m1.1a"><mo id="S3.T3.7.7.3.m1.1.1" xref="S3.T3.7.7.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.3.m1.1b"><csymbol cd="latexml" id="S3.T3.7.7.3.m1.1.1.cmml" xref="S3.T3.7.7.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.7.3.m1.1d">±</annotation></semantics></math>4.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.8.8.4">783<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.8.8.4.m1.1"><semantics id="S3.T3.8.8.4.m1.1a"><mo id="S3.T3.8.8.4.m1.1.1" xref="S3.T3.8.8.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.4.m1.1b"><csymbol cd="latexml" id="S3.T3.8.8.4.m1.1.1.cmml" xref="S3.T3.8.8.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.8.4.m1.1d">±</annotation></semantics></math>5.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.8.8.6">4.2x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.8.8.7">4.1x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.8.8.8">5.1x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.12.12.5">wmt20-comet-qe-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.9.9.1">2529<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.9.9.1.m1.1"><semantics id="S3.T3.9.9.1.m1.1a"><mo id="S3.T3.9.9.1.m1.1.1" xref="S3.T3.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.1.m1.1b"><csymbol cd="latexml" id="S3.T3.9.9.1.m1.1.1.cmml" xref="S3.T3.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.9.9.1.m1.1d">±</annotation></semantics></math>0.4</td>
<td class="ltx_td ltx_align_right" id="S3.T3.10.10.2">608<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.10.10.2.m1.1"><semantics id="S3.T3.10.10.2.m1.1a"><mo id="S3.T3.10.10.2.m1.1.1" xref="S3.T3.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.2.m1.1b"><csymbol cd="latexml" id="S3.T3.10.10.2.m1.1.1.cmml" xref="S3.T3.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.10.10.2.m1.1d">±</annotation></semantics></math>3.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.11.11.3">623<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.11.11.3.m1.1"><semantics id="S3.T3.11.11.3.m1.1a"><mo id="S3.T3.11.11.3.m1.1.1" xref="S3.T3.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.11.11.3.m1.1b"><csymbol cd="latexml" id="S3.T3.11.11.3.m1.1.1.cmml" xref="S3.T3.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.11.11.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.11.11.3.m1.1d">±</annotation></semantics></math>3.6</td>
<td class="ltx_td ltx_align_right" id="S3.T3.12.12.4">501<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.12.12.4.m1.1"><semantics id="S3.T3.12.12.4.m1.1a"><mo id="S3.T3.12.12.4.m1.1.1" xref="S3.T3.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.12.12.4.m1.1b"><csymbol cd="latexml" id="S3.T3.12.12.4.m1.1.1.cmml" xref="S3.T3.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.12.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.12.12.4.m1.1d">±</annotation></semantics></math>0.3</td>
<td class="ltx_td ltx_align_right" id="S3.T3.12.12.6">4.2x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.12.12.7">4.1x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.12.12.8">5.0x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.16.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.16.16.5">wmt22-comet-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.13.13.1">3772<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.13.13.1.m1.1"><semantics id="S3.T3.13.13.1.m1.1a"><mo id="S3.T3.13.13.1.m1.1.1" xref="S3.T3.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.13.13.1.m1.1b"><csymbol cd="latexml" id="S3.T3.13.13.1.m1.1.1.cmml" xref="S3.T3.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.13.13.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.13.13.1.m1.1d">±</annotation></semantics></math>1.3</td>
<td class="ltx_td ltx_align_right" id="S3.T3.14.14.2">858<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.14.14.2.m1.1"><semantics id="S3.T3.14.14.2.m1.1a"><mo id="S3.T3.14.14.2.m1.1.1" xref="S3.T3.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.14.14.2.m1.1b"><csymbol cd="latexml" id="S3.T3.14.14.2.m1.1.1.cmml" xref="S3.T3.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.14.14.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.14.14.2.m1.1d">±</annotation></semantics></math>4.6</td>
<td class="ltx_td ltx_align_right" id="S3.T3.15.15.3">884<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.15.15.3.m1.1"><semantics id="S3.T3.15.15.3.m1.1a"><mo id="S3.T3.15.15.3.m1.1.1" xref="S3.T3.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.15.15.3.m1.1b"><csymbol cd="latexml" id="S3.T3.15.15.3.m1.1.1.cmml" xref="S3.T3.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.15.15.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.15.15.3.m1.1d">±</annotation></semantics></math>4.5</td>
<td class="ltx_td ltx_align_right" id="S3.T3.16.16.4">676<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.16.16.4.m1.1"><semantics id="S3.T3.16.16.4.m1.1a"><mo id="S3.T3.16.16.4.m1.1.1" xref="S3.T3.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.16.16.4.m1.1b"><csymbol cd="latexml" id="S3.T3.16.16.4.m1.1.1.cmml" xref="S3.T3.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.16.16.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.16.16.4.m1.1d">±</annotation></semantics></math>0.8</td>
<td class="ltx_td ltx_align_right" id="S3.T3.16.16.6">4.4x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.16.16.7">4.3x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.16.16.8">5.6x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.20.20.5">wmt22-cometkiwi-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.17.17.1">2357<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.17.17.1.m1.1"><semantics id="S3.T3.17.17.1.m1.1a"><mo id="S3.T3.17.17.1.m1.1.1" xref="S3.T3.17.17.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.17.17.1.m1.1b"><csymbol cd="latexml" id="S3.T3.17.17.1.m1.1.1.cmml" xref="S3.T3.17.17.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.17.17.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.17.17.1.m1.1d">±</annotation></semantics></math>2.0</td>
<td class="ltx_td ltx_align_right" id="S3.T3.18.18.2">419<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.18.18.2.m1.1"><semantics id="S3.T3.18.18.2.m1.1a"><mo id="S3.T3.18.18.2.m1.1.1" xref="S3.T3.18.18.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.18.18.2.m1.1b"><csymbol cd="latexml" id="S3.T3.18.18.2.m1.1.1.cmml" xref="S3.T3.18.18.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.18.18.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.18.18.2.m1.1d">±</annotation></semantics></math>0.4</td>
<td class="ltx_td ltx_align_right" id="S3.T3.19.19.3">437<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.19.19.3.m1.1"><semantics id="S3.T3.19.19.3.m1.1a"><mo id="S3.T3.19.19.3.m1.1.1" xref="S3.T3.19.19.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.19.19.3.m1.1b"><csymbol cd="latexml" id="S3.T3.19.19.3.m1.1.1.cmml" xref="S3.T3.19.19.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.19.19.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.19.19.3.m1.1d">±</annotation></semantics></math>1.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.20.20.4">327<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.20.20.4.m1.1"><semantics id="S3.T3.20.20.4.m1.1a"><mo id="S3.T3.20.20.4.m1.1.1" xref="S3.T3.20.20.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.20.20.4.m1.1b"><csymbol cd="latexml" id="S3.T3.20.20.4.m1.1.1.cmml" xref="S3.T3.20.20.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.20.20.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.20.20.4.m1.1d">±</annotation></semantics></math>1.0</td>
<td class="ltx_td ltx_align_right" id="S3.T3.20.20.6">5.6x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.20.20.7">5.4x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.20.20.8">7.2x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.24.24.5">wmt23-cometkiwi-da-xl</th>
<td class="ltx_td ltx_align_right" id="S3.T3.21.21.1">17252<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.21.21.1.m1.1"><semantics id="S3.T3.21.21.1.m1.1a"><mo id="S3.T3.21.21.1.m1.1.1" xref="S3.T3.21.21.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.21.21.1.m1.1b"><csymbol cd="latexml" id="S3.T3.21.21.1.m1.1.1.cmml" xref="S3.T3.21.21.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.21.21.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.21.21.1.m1.1d">±</annotation></semantics></math>0.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.22.22.2">3405<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.22.22.2.m1.1"><semantics id="S3.T3.22.22.2.m1.1a"><mo id="S3.T3.22.22.2.m1.1.1" xref="S3.T3.22.22.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.22.22.2.m1.1b"><csymbol cd="latexml" id="S3.T3.22.22.2.m1.1.1.cmml" xref="S3.T3.22.22.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.22.22.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.22.22.2.m1.1d">±</annotation></semantics></math>4.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.23.23.3">3480<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.23.23.3.m1.1"><semantics id="S3.T3.23.23.3.m1.1a"><mo id="S3.T3.23.23.3.m1.1.1" xref="S3.T3.23.23.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.23.23.3.m1.1b"><csymbol cd="latexml" id="S3.T3.23.23.3.m1.1.1.cmml" xref="S3.T3.23.23.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.23.23.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.23.23.3.m1.1d">±</annotation></semantics></math>3.9</td>
<td class="ltx_td ltx_align_right" id="S3.T3.24.24.4">1949<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.24.24.4.m1.1"><semantics id="S3.T3.24.24.4.m1.1a"><mo id="S3.T3.24.24.4.m1.1.1" xref="S3.T3.24.24.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.24.24.4.m1.1b"><csymbol cd="latexml" id="S3.T3.24.24.4.m1.1.1.cmml" xref="S3.T3.24.24.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.24.24.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.24.24.4.m1.1d">±</annotation></semantics></math>3.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.24.24.6">5.1x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.24.24.7">5.0x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.24.24.8">8.8x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.47.51.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="8" id="S3.T3.47.51.4.1"><span class="ltx_text ltx_font_italic" id="S3.T3.47.51.4.1.1">8 GPUs</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.27.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.27.27.4">bleurt-20</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.27.27.5">NA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.25.25.1">85<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.25.25.1.m1.1"><semantics id="S3.T3.25.25.1.m1.1a"><mo id="S3.T3.25.25.1.m1.1.1" xref="S3.T3.25.25.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.25.25.1.m1.1b"><csymbol cd="latexml" id="S3.T3.25.25.1.m1.1.1.cmml" xref="S3.T3.25.25.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.25.25.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.25.25.1.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.26.26.2">99<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.26.26.2.m1.1"><semantics id="S3.T3.26.26.2.m1.1a"><mo id="S3.T3.26.26.2.m1.1.1" xref="S3.T3.26.26.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.26.26.2.m1.1b"><csymbol cd="latexml" id="S3.T3.26.26.2.m1.1.1.cmml" xref="S3.T3.26.26.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.26.26.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.26.26.2.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.27.27.3">76<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.27.27.3.m1.1"><semantics id="S3.T3.27.27.3.m1.1a"><mo id="S3.T3.27.27.3.m1.1.1" xref="S3.T3.27.27.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.27.27.3.m1.1b"><csymbol cd="latexml" id="S3.T3.27.27.3.m1.1.1.cmml" xref="S3.T3.27.27.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.27.27.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.27.27.3.m1.1d">±</annotation></semantics></math>0.4</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.27.27.6">NA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.27.27.7">NA</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T3.27.27.8">NA</td>
</tr>
<tr class="ltx_tr" id="S3.T3.31.31">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.31.31.5">wmt20-comet-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.28.28.1">926<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.28.28.1.m1.1"><semantics id="S3.T3.28.28.1.m1.1a"><mo id="S3.T3.28.28.1.m1.1.1" xref="S3.T3.28.28.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.28.28.1.m1.1b"><csymbol cd="latexml" id="S3.T3.28.28.1.m1.1.1.cmml" xref="S3.T3.28.28.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.28.28.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.28.28.1.m1.1d">±</annotation></semantics></math>1.0</td>
<td class="ltx_td ltx_align_right" id="S3.T3.29.29.2">125<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.29.29.2.m1.1"><semantics id="S3.T3.29.29.2.m1.1a"><mo id="S3.T3.29.29.2.m1.1.1" xref="S3.T3.29.29.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.29.29.2.m1.1b"><csymbol cd="latexml" id="S3.T3.29.29.2.m1.1.1.cmml" xref="S3.T3.29.29.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.29.29.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.29.29.2.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.30.30.3">146<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.30.30.3.m1.1"><semantics id="S3.T3.30.30.3.m1.1a"><mo id="S3.T3.30.30.3.m1.1.1" xref="S3.T3.30.30.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.30.30.3.m1.1b"><csymbol cd="latexml" id="S3.T3.30.30.3.m1.1.1.cmml" xref="S3.T3.30.30.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.30.30.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.30.30.3.m1.1d">±</annotation></semantics></math>0.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.31.31.4">124<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.31.31.4.m1.1"><semantics id="S3.T3.31.31.4.m1.1a"><mo id="S3.T3.31.31.4.m1.1.1" xref="S3.T3.31.31.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.31.31.4.m1.1b"><csymbol cd="latexml" id="S3.T3.31.31.4.m1.1.1.cmml" xref="S3.T3.31.31.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.31.31.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.31.31.4.m1.1d">±</annotation></semantics></math>1.0</td>
<td class="ltx_td ltx_align_right" id="S3.T3.31.31.6">7.4x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.31.31.7">6.3x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.31.31.8">7.5x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.35.35">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.35.35.5">wmt20-comet-qe-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.32.32.1">622<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.32.32.1.m1.1"><semantics id="S3.T3.32.32.1.m1.1a"><mo id="S3.T3.32.32.1.m1.1.1" xref="S3.T3.32.32.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.32.32.1.m1.1b"><csymbol cd="latexml" id="S3.T3.32.32.1.m1.1.1.cmml" xref="S3.T3.32.32.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.32.32.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.32.32.1.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.33.33.2">82<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.33.33.2.m1.1"><semantics id="S3.T3.33.33.2.m1.1a"><mo id="S3.T3.33.33.2.m1.1.1" xref="S3.T3.33.33.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.33.33.2.m1.1b"><csymbol cd="latexml" id="S3.T3.33.33.2.m1.1.1.cmml" xref="S3.T3.33.33.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.33.33.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.33.33.2.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.34.34.3">95<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.34.34.3.m1.1"><semantics id="S3.T3.34.34.3.m1.1a"><mo id="S3.T3.34.34.3.m1.1.1" xref="S3.T3.34.34.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.34.34.3.m1.1b"><csymbol cd="latexml" id="S3.T3.34.34.3.m1.1.1.cmml" xref="S3.T3.34.34.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.34.34.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.34.34.3.m1.1d">±</annotation></semantics></math>0.2</td>
<td class="ltx_td ltx_align_right" id="S3.T3.35.35.4">81<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.35.35.4.m1.1"><semantics id="S3.T3.35.35.4.m1.1a"><mo id="S3.T3.35.35.4.m1.1.1" xref="S3.T3.35.35.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.35.35.4.m1.1b"><csymbol cd="latexml" id="S3.T3.35.35.4.m1.1.1.cmml" xref="S3.T3.35.35.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.35.35.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.35.35.4.m1.1d">±</annotation></semantics></math>0.2</td>
<td class="ltx_td ltx_align_right" id="S3.T3.35.35.6">7.6x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.35.35.7">6.5x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.35.35.8">7.7x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.39.39">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.39.39.5">wmt22-comet-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.36.36.1">896<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.36.36.1.m1.1"><semantics id="S3.T3.36.36.1.m1.1a"><mo id="S3.T3.36.36.1.m1.1.1" xref="S3.T3.36.36.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.36.36.1.m1.1b"><csymbol cd="latexml" id="S3.T3.36.36.1.m1.1.1.cmml" xref="S3.T3.36.36.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.36.36.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.36.36.1.m1.1d">±</annotation></semantics></math>0.8</td>
<td class="ltx_td ltx_align_right" id="S3.T3.37.37.2">114<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.37.37.2.m1.1"><semantics id="S3.T3.37.37.2.m1.1a"><mo id="S3.T3.37.37.2.m1.1.1" xref="S3.T3.37.37.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.37.37.2.m1.1b"><csymbol cd="latexml" id="S3.T3.37.37.2.m1.1.1.cmml" xref="S3.T3.37.37.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.37.37.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.37.37.2.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.38.38.3">135<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.38.38.3.m1.1"><semantics id="S3.T3.38.38.3.m1.1a"><mo id="S3.T3.38.38.3.m1.1.1" xref="S3.T3.38.38.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.38.38.3.m1.1b"><csymbol cd="latexml" id="S3.T3.38.38.3.m1.1.1.cmml" xref="S3.T3.38.38.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.38.38.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.38.38.3.m1.1d">±</annotation></semantics></math>0.3</td>
<td class="ltx_td ltx_align_right" id="S3.T3.39.39.4">111<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.39.39.4.m1.1"><semantics id="S3.T3.39.39.4.m1.1a"><mo id="S3.T3.39.39.4.m1.1.1" xref="S3.T3.39.39.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.39.39.4.m1.1b"><csymbol cd="latexml" id="S3.T3.39.39.4.m1.1.1.cmml" xref="S3.T3.39.39.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.39.39.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.39.39.4.m1.1d">±</annotation></semantics></math>0.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.39.39.6">7.8x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.39.39.7">6.6x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.39.39.8">8.1x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.43.43">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.43.43.5">wmt22-cometkiwi-da</th>
<td class="ltx_td ltx_align_right" id="S3.T3.40.40.1">562<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.40.40.1.m1.1"><semantics id="S3.T3.40.40.1.m1.1a"><mo id="S3.T3.40.40.1.m1.1.1" xref="S3.T3.40.40.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.40.40.1.m1.1b"><csymbol cd="latexml" id="S3.T3.40.40.1.m1.1.1.cmml" xref="S3.T3.40.40.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.40.40.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.40.40.1.m1.1d">±</annotation></semantics></math>0.7</td>
<td class="ltx_td ltx_align_right" id="S3.T3.41.41.2">59<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.41.41.2.m1.1"><semantics id="S3.T3.41.41.2.m1.1a"><mo id="S3.T3.41.41.2.m1.1.1" xref="S3.T3.41.41.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.41.41.2.m1.1b"><csymbol cd="latexml" id="S3.T3.41.41.2.m1.1.1.cmml" xref="S3.T3.41.41.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.41.41.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.41.41.2.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.42.42.3">72<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.42.42.3.m1.1"><semantics id="S3.T3.42.42.3.m1.1a"><mo id="S3.T3.42.42.3.m1.1.1" xref="S3.T3.42.42.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.42.42.3.m1.1b"><csymbol cd="latexml" id="S3.T3.42.42.3.m1.1.1.cmml" xref="S3.T3.42.42.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.42.42.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.42.42.3.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.43.43.4">58<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.43.43.4.m1.1"><semantics id="S3.T3.43.43.4.m1.1a"><mo id="S3.T3.43.43.4.m1.1.1" xref="S3.T3.43.43.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.43.43.4.m1.1b"><csymbol cd="latexml" id="S3.T3.43.43.4.m1.1.1.cmml" xref="S3.T3.43.43.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.43.43.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.43.43.4.m1.1d">±</annotation></semantics></math>0.1</td>
<td class="ltx_td ltx_align_right" id="S3.T3.43.43.6">9.5x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.43.43.7">7.8x</td>
<td class="ltx_td ltx_align_right" id="S3.T3.43.43.8">9.6x</td>
</tr>
<tr class="ltx_tr" id="S3.T3.47.47">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.47.47.5">wmt23-cometkiwi-da-xl</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.44.44.1">3288<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.44.44.1.m1.1"><semantics id="S3.T3.44.44.1.m1.1a"><mo id="S3.T3.44.44.1.m1.1.1" xref="S3.T3.44.44.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.44.44.1.m1.1b"><csymbol cd="latexml" id="S3.T3.44.44.1.m1.1.1.cmml" xref="S3.T3.44.44.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.44.44.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.44.44.1.m1.1d">±</annotation></semantics></math>1.8</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.45.45.2">662<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.45.45.2.m1.1"><semantics id="S3.T3.45.45.2.m1.1a"><mo id="S3.T3.45.45.2.m1.1.1" xref="S3.T3.45.45.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.45.45.2.m1.1b"><csymbol cd="latexml" id="S3.T3.45.45.2.m1.1.1.cmml" xref="S3.T3.45.45.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.45.45.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.45.45.2.m1.1d">±</annotation></semantics></math>2.6</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.46.46.3">862<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.46.46.3.m1.1"><semantics id="S3.T3.46.46.3.m1.1a"><mo id="S3.T3.46.46.3.m1.1.1" xref="S3.T3.46.46.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.46.46.3.m1.1b"><csymbol cd="latexml" id="S3.T3.46.46.3.m1.1.1.cmml" xref="S3.T3.46.46.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.46.46.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.46.46.3.m1.1d">±</annotation></semantics></math>13.3</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.47.47.4">258<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T3.47.47.4.m1.1"><semantics id="S3.T3.47.47.4.m1.1a"><mo id="S3.T3.47.47.4.m1.1.1" xref="S3.T3.47.47.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T3.47.47.4.m1.1b"><csymbol cd="latexml" id="S3.T3.47.47.4.m1.1.1.cmml" xref="S3.T3.47.47.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.47.47.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T3.47.47.4.m1.1d">±</annotation></semantics></math>0.7</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.47.47.6">5.0x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.47.47.7">3.8x</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T3.47.47.8">12.7x</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Time taken (seconds) to score the benchmark datasets having 364,200 examples, and the speedup of our implementation with respect to the original.
Numbers are the average of three runs on one and eight GPUs. PyM is short for PyMarian.
The column with FP16 is half-precision, and the rest are full-precision (32-bit).</figcaption>
</figure>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T4.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3" id="S3.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.2.1">Score</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.3.1">Error</span></th>
</tr>
<tr class="ltx_tr" id="S3.T4.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S3.T4.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.2.1.1">Metric</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r" id="S3.T4.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.2.2.1">Original</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r" id="S3.T4.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.2.3.1">Marian FP32</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r" id="S3.T4.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.2.4.1">Marian FP16</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r" id="S3.T4.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.2.5.1">Marian FP32</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column" id="S3.T4.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S3.T4.1.2.2.6.1">Marian FP16</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T4.1.3.1.1">bleurt-20</th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T4.1.3.1.2">0.7255</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T4.1.3.1.3">0.7252</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T4.1.3.1.4">0.7211</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T4.1.3.1.5">0.0003</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T4.1.3.1.6">0.0044</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T4.1.4.2.1">wmt20-comet-da</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.4.2.2">0.5721</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.4.2.3">0.5720</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.4.2.4">0.5716</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.4.2.5">0.0001</td>
<td class="ltx_td ltx_align_right" id="S3.T4.1.4.2.6">0.0005</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T4.1.5.3.1">wmt20-comet-qe-da</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.5.3.2">0.1933</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.5.3.3">0.1932</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.5.3.4">0.1924</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.5.3.5">0.0001</td>
<td class="ltx_td ltx_align_right" id="S3.T4.1.5.3.6">0.0009</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T4.1.6.4.1">wmt22-comet-da</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.6.4.2">0.8462</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.6.4.3">0.8461</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.6.4.4">0.8427</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.6.4.5">0.0000</td>
<td class="ltx_td ltx_align_right" id="S3.T4.1.6.4.6">0.0034</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T4.1.7.5.1">wmt22-cometkiwi-da</th>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.7.5.2">0.7984</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.7.5.3">0.7984</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.7.5.4">0.7981</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T4.1.7.5.5">0.0000</td>
<td class="ltx_td ltx_align_right" id="S3.T4.1.7.5.6">0.0003</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T4.1.8.6.1">wmt23-cometkiwi-da-xl</th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T4.1.8.6.2">0.6840</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T4.1.8.6.3">0.6839</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T4.1.8.6.4">0.6862</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T4.1.8.6.5">0.0001</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T4.1.8.6.6">0.0023</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The average scores produced by the original implementation and ours.
The columns named ‘Error’ are the absolute difference between the average of scores from the original and our implementations.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Benchmarks</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">A concern with new implementations is the risk of producing incompatible results.
We therefore compare our model conversion and implementations carefully so as to ensure that <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.1.1">pymarian-eval</span> produces the same results.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Our benchmark setup is as follows:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Dataset: WMT23 General Translation submissions; we combine all systems for all languages pairs, which results in a total of 364,200 examples.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">COMETs original implementation: unbabel-comet v2.2.2; transititive dependencies: torch v2.4.0, pytorch-lightning v2.3.3, transformers v4.43.3</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">BLEURT original implementation is installed from source repository<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google-research/bleurt/tree/cebe7e6f" title="">https://github.com/google-research/bleurt/tree/cebe7e6f</a></span></span></span>; transititive dependencies: tensorflow v2.17.0</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Marian v1.12.31, compiled with GCC v11.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Python v3.10.12, Ubuntu 22.04.3, on Intel(R) Xeon(R) Platinum 8168 CPU @ 2.70GHz</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1">GPU: 8x Nvidia Tesla V100 (32GB); Driver v525.105.17, CUDA v12.3</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i7.p1">
<p class="ltx_p" id="S3.I1.i7.p1.1">Batch size is 128, except for wmt23-cometkiwi-xl, the largest batch size that worked are: 64 for eight GPUs and 32 for one GPU.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.T3" title="Table 3 ‣ 3.2 Metrics ‣ 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">3</span></a>, we report the time taken by original toolkits (Pytorch based comet-score and Tensorflow based bluert) and our implementation.
For ours, we report Marian (binary produced by C++), and <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">pymarian-eval</span>(with float32 and float16 precisions).
In addition, we also present the average of segment scores, and error, i.e., the absolute difference between the scores produced by the original and ours.
The scores and derived errors for our implementation remain consistent regardless of whether the C++ implementation is invoked via the command line binary (marian evaluate) or through the Python bindings wrapper (pymarian-eval). Additionally, the scores are identical whether the benchmarks are conducted on a single GPU or parallelized across multiple GPUs.
We avoid repetition, and instead present only the values for full-precision (FP32) and half-precision (FP16).
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S3.T4" title="Table 4 ‣ 3.2 Metrics ‣ 3 Fast MT Evaluation in PyMarian ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">4</span></a>, ours yield the same scores as the original, with minor discrepancies attributable to floating-point calculations.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">In addition to providing significantly faster processing times, <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p4.1.1">pymarian-eval</span> provides a flexible CLI tool with a natural POSIX interface (e.g., STDIN/STDOUT, use of TSV formats).
This allows it to integrate well with other tools, such as SacreBLEU’s test-set downloading capabilities<cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib9" title="">2018</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Example applications</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">A Python API makes it simple to incorporate Marian models into the many Python-native settings that researchers are accustomed to.
In this section we illustrate example use cases and applications of PyMarian, demonstrating its versatility.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Jupyter notebook</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">PyMarian makes it easy to use Marian-trained models in interactive sessions such as Jupyter Notebook-like<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jupyter.org" title="">https://jupyter.org</a></span></span></span> environments.
We provide an example notebook for translation, training, and evaluation via Google Colab at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://colab.research.google.com/drive/1Lg_W5K2nLtvaKfLuHjc-LAajenI_SGL3" title="">https://colab.research.google.com/drive/1Lg_W5K2nLtvaKfLuHjc-LAajenI_SGL3</a></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>OPUS-MT models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Over the years, Marian NMT has been widely adopted by the community to train and release open-sourced machine translation systems.
One of the largest projects developing such resources is OPUS-MT, which offers over 1,000 pre-trained models <cite class="ltx_cite ltx_citemacro_cite">Tiedemann and Thottingal (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib18" title="">2020b</a>); Tiedemann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib16" title="">2023</a>)</cite>.
PyMarian provides a seamless interface to decode with these existing Marian-trained models.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Web app demo</h3>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="625" id="S4.F1.g1" src="extracted/5792748/demo.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>PyMarian web demo with two outputs and diff between them.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.1">pymarian</span> permits easy connection from Marian models to Python’s visualization libraries.
We incorporate a Flask-based web server that can display a range of models side by side.<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/marian-nmt/pymarian-webapp" title="">https://github.com/marian-nmt/pymarian-webapp</a></span></span></span>
It supports loading of models from local disk (type “base”) or connecting to Microsoft’s API (type “mtapi”).
<span class="ltx_ERROR undefined" id="S4.SS3.p1.1.2">{minted}</span>yaml
translators:
en-de-research:
type: base
name: research
model: /path/to/marian.npz
vocab: /path/to/vocab.spm
en-de-prod:
type: mtapi
name: prod
subscription-key: redacted
source-language: en
target-language: de

Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#S4.F1" title="Figure 1 ‣ 4.3 Web app demo ‣ 4 Example applications ‣ PyMarian: Fast Neural Machine Translation and Evaluation in Python"><span class="ltx_text ltx_ref_tag">1</span></a> provides an example of this interface.
Due to the flexibility of Python, extending the model to support other types is simple.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">A wide range of Python toolkits exist for training and inference for the “classical” (i.e., not LLM-based) sequence-to-sequence approach to machine translation.
One of the most popular is Meta’s Fairseq <cite class="ltx_cite ltx_citemacro_cite">Ott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib7" title="">2019</a>)</cite>, which supports a wide range of training and inference features, including multi-GPU and multi-node training.
Amazon’s Sockeye <cite class="ltx_cite ltx_citemacro_cite">Hieber et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib3" title="">2022</a>)</cite> is another option; while it has fewer features than fairseq, it is known for its strong software engineering practices and flexibility.
Both of these toolkits are based on Pytorch <cite class="ltx_cite ltx_citemacro_cite">Paszke et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib8" title="">2019</a>)</cite>, and support research and production use cases.
Sockeye has recently (as of June 7, 2024) been end-of-lifed.<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/awslabs/sockeye/commit/e42fbb30be9bca1f5073f092b687966636370092" title="">https://github.com/awslabs/sockeye/commit/e42fbb30be9bca1f5073f092b687966636370092</a></span></span></span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">A significant amount of research and development activity takes place using HuggingFace’s popular <code class="ltx_verbatim ltx_font_typewriter" id="S5.p2.1.1">transformers</code> package.
Work in this area tends to be much more research-focused, however, which means that software-engineering practices and speed are sacrificed in favor of rapid development.
HuggingFace also provides a data store for a huge range of datasets and models.
VLLM is a recent project that provides fast, production-oriented inference for HuggingFace models <cite class="ltx_cite ltx_citemacro_cite">Kwon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib6" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">There is support for loading Marian models in HuggingFace, largely provided by <cite class="ltx_cite ltx_citemacro_cite">Tiedemann and Thottingal (<a class="ltx_ref" href="https://arxiv.org/html/2408.11853v1#bib.bib17" title="">2020a</a>)</cite>.
However, not all Marian model features are supported.
<span class="ltx_text ltx_font_typewriter" id="S5.p3.1.1">pymarian</span> provides Python-based access to any Marian model, with C++ inference speeds.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Summary</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We have introduced <span class="ltx_text ltx_font_typewriter" id="S6.p1.1.1">pymarian</span>, a set of Python bindings that export Marian’s fast training and inference capabilities to Python settings, without requiring any model conversion into much slower frameworks.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">These bindings enable a range of integrations with Python—the preferred toolkit for research in NLP and MT—making available Marian’s high training and inference speeds.
In particular, it enables <span class="ltx_text ltx_font_typewriter" id="S6.p2.1.1">pymarian-eval</span>, an implementation of COMET and BLEURT models yielding speedups as high as
7.8x (for wmt22-cometkiwi-da) on eight GPUs, and never less than <math alttext="3.5" class="ltx_Math" display="inline" id="S6.p2.1.m1.1"><semantics id="S6.p2.1.m1.1a"><mn id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">3.5</mn><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><cn id="S6.p2.1.m1.1.1.cmml" type="float" xref="S6.p2.1.m1.1.1">3.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">3.5</annotation><annotation encoding="application/x-llamapun" id="S6.p2.1.m1.1d">3.5</annotation></semantics></math>x.
<span class="ltx_text ltx_font_typewriter" id="S6.p2.1.2">pymarian-eval</span> is also significantly faster at loading models, and up to 44x (for wmt23-cometkiwi-da-xl) on eight GPUs.
These models are made available on Hugginface and are seamlessly downloaded at runtime.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">PyMarian aims to enhance the accessibility and usability of Marian NMT and publicly available machine translation models trained with the toolkit.
The primary limitation of PyMarian is that it is designed specifically for Marian-trained models, which may restrict its flexibility for users who wish to integrate models trained using other frameworks or custom architectures.
Additionally, we have implemented only the most popular evaluation metrics, such as COMET and BLEURT, which may not encompass all the evaluation metrics required for specific research or application needs.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">COMET-Kiwi models require users to accept a custom license and terms of use.
To ensure that the license is preserved in the Marian-trained versions, we collaborated with the original authors.
They now host our models exclusively on HuggingFace, where users must accept the same license before downloading.
The availability of these models is subject to their decisions.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">The reported benchmarks are based on specific hardware and software settings and may not fully capture the variability in real-world scenarios.
Despite the optimizations, running MT evaluation metrics can be resource-intensive, requiring significant computational power.
This limitation may pose challenges for users with limited access to high-performance computing resources.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">Finally, as with any open-source project, the long-term maintenance and support of PyMarian depend on the community’s contributions and engagement.
Ensuring the project’s sustainability requires continuous collaboration and support from the community.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bogoychev et al. (2021)</span>
<span class="ltx_bibblock">
Nikolay Bogoychev, Jelmer Van der Linde, and Kenneth Heafield. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-demo.20" title="">TranslateLocally: Blazing-fast translation running on the local CPU</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 168–174, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gowda et al. (2023)</span>
<span class="ltx_bibblock">
Thamme Gowda, Tom Kocmi, and Marcin Junczys-Dowmunt. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.62" title="">Cometoid: Distilling strong reference-based machine translation metrics into Even stronger quality estimation metrics</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 751–755, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hieber et al. (2022)</span>
<span class="ltx_bibblock">
Felix Hieber, Michael Denkowski, Tobias Domhan, Barbara Darques Barros, Celina Dong Ye, Xing Niu, Cuong Hoang, Ke Tran, Benjamin Hsu, Maria Nadejde, Surafel Lakew, Prashant Mathur, Anna Currey, and Marcello Federico. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.05851" title="">Sockeye 3: Fast neural machine translation with pytorch</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt et al. (2018a)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, André F. T. Martins, and Alexandra Birch. 2018a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P18-4020" title="">Marian: Fast neural machine translation in C++</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of ACL 2018, System Demonstrations</em>, pages 116–121, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Junczys-Dowmunt et al. (2018b)</span>
<span class="ltx_bibblock">
Marcin Junczys-Dowmunt, Kenneth Heafield, Hieu Hoang, Roman Grundkiewicz, and Anthony Aue. 2018b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-2716" title="">Marian: Cost-effective high-quality neural machine translation in C++</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</em>, pages 129–135, Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3600006.3613165" title="">Efficient memory management for large language model serving with pagedattention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</em>, SOSP ’23, page 611–626, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/N19-4009" title="">fairseq: A fast, extensible toolkit for sequence modeling</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</em>, pages 48–53, Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke et al. (2019)</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf" title="">Pytorch: An imperative style, high-performance deep learning library</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in Neural Information Processing Systems</em>, volume 32. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6319" title="">A call for clarity in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186–191, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2022a)</span>
<span class="ltx_bibblock">
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2021)</span>
<span class="ltx_bibblock">
Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan van Stigt, Craig Stewart, Pedro Ramos, Taisiya Glushkova, André F. T. Martins, and Alon Lavie. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.111" title="">Are references really needed? unbabel-IST 2021 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 1030–1040, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2023)</span>
<span class="ltx_bibblock">
Ricardo Rei, Nuno M. Guerreiro, JosÃ© Pombal, Daan van Stigt, Marcos Treviso, Luisa Coheur, José G. C. de Souza, and André Martins. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.73" title="">Scaling up CometKiwi: Unbabel-IST 2023 submission for the quality estimation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 841–848, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.101" title="">Unbabel’s participation in the WMT20 metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 911–920, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2022b)</span>
<span class="ltx_bibblock">
Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.60" title="">CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.704" title="">BLEURT: Learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 7881–7892, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann et al. (2023)</span>
<span class="ltx_bibblock">
Jörg Tiedemann, Mikko Aulamo, Daria Bakshandaeva, Michele Boggia, Stig-Arne Grönroos, Tommi Nieminen, Alessandro Raganato  Yves Scherrer, Raul Vazquez, and Sami Virpioja. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s10579-023-09704-w" title="">Democratizing neural machine translation with OPUS-MT</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Language Resources and Evaluation</em>, (58):713–755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal (2020a)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Santhosh Thottingal. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.61" title="">OPUS-MT – building open translation services for the world</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 479–480, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal (2020b)</span>
<span class="ltx_bibblock">
Jörg Tiedemann and Santhosh Thottingal. 2020b.

</span>
<span class="ltx_bibblock">OPUS-MT — Building open translation services for the World.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</em>, Lisbon, Portugal.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 15 01:34:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
