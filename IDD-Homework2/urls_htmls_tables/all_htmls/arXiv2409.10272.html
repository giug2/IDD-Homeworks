<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data</title>
<!--Generated on Sun Sep 22 15:41:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
human annotation,  expert annotators,  remote sensing,  segmentation,  object detection,  error types,  precision,  recall.
" lang="en" name="keywords"/>
<base href="/html/2409.10272v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S1" title="In Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S2" title="In Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S2.SS1" title="In II Related work ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Training Sets and Human Annotators in RS</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S2.SS2" title="In II Related work ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Annotation Strategy</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S2.SS3" title="In II Related work ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Task Conditions - Level of Imbalanced Data</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S2.SS4" title="In II Related work ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Annotators Expertise</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3" title="In Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Methods</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS1" title="In III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Participants</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS2" title="In III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS2.SSS1" title="In III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Annotation strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS2.SSS2" title="In III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Task Conditions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS2.SSS3" title="In III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>3 </span>Prior Experience</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS3" title="In III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Experimental Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS3.SSS1" title="In III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>1 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS3.SSS2" title="In III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>2 </span>Performance Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS3.SSS3" title="In III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>3 </span>Spatial Data Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.SS3.SSS4" title="In III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span>4 </span>Statistical Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4" title="In Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.SS1" title="In IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Object Detection versus Segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.SS2" title="In IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Differences in Error Types</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.SS3" title="In IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Performance Comparisons</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.SS4" title="In IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Impact of Prior Experience</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5" title="In Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.SS1" title="In V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Higher Performance in OD Compared with Segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.SS2" title="In V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Consistent Bias in Error Types</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.SS3" title="In V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Annotator‚Äôs Performance Across Strategies and Conditions</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.SS3.SSS1" title="In V-C Annotator‚Äôs Performance Across Strategies and Conditions ‚Ä£ V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>1 </span>Majority Vote Outperforms Reviewing Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.SS3.SSS2" title="In V-C Annotator‚Äôs Performance Across Strategies and Conditions ‚Ä£ V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>2 </span>The effect of imbalanced data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.SS4" title="In V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">No Advantage in Performance for Experts</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S6" title="In Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Roni¬†Blushtein-Livnon<span class="ltx_text" id="id5.1.id1" style="position:relative; bottom:2.2pt;"><span class="ltx_ERROR undefined" id="id5.1.id1.1">\orcidlink</span>0000-0002-3493-4894</span>,
Tal¬†Svoray<span class="ltx_text" id="id6.2.id2" style="position:relative; bottom:2.2pt;"><span class="ltx_ERROR undefined" id="id6.2.id2.1">\orcidlink</span>0000-0003-2243-8532</span>,
and¬†Michael Dorman<span class="ltx_text" id="id7.3.id3" style="position:relative; bottom:2.2pt;"><span class="ltx_ERROR undefined" id="id7.3.id3.1">\orcidlink</span>0000-0001-6450-8047</span>
</span><span class="ltx_author_notes">R. Blushtein-Livnon and M. Dorman are with the Department of Environmental, Geoinformatics and Urban Planning Sciences, Ben-Gurion University of the Negev, Israel (e-mail: livnon@bgu.ac.il; dorman@bgu.ac.il).T. Svoray is with the Department of Environmental, Geoinformatics and Urban Planning Sciences, and the Department of Psychology, Ben-Gurion University of the Negev, Israel (e-mail: tsvoray@bgu.ac.il).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">This study introduces a laboratory experiment designed to assess the influence of annotation strategies, levels of imbalanced data, and prior experience, on the performance of human annotators. The experiment focuses on labeling aerial imagery, using ArcGIS Pro, to detect and segment small-scale PVs, selected as a case study for rectangular objects. The experiment is conducted using images with a pixel size of 0.15<math alttext="m" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mi id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">italic_m</annotation></semantics></math>, involving both expert and non-expert participants, across different setup strategies and target-background ratio datasets. Our findings indicate that annotators generally perform more effectively in object detection than in segmentation tasks. A marked tendency to commit more Type II errors (False Negatives, i.e., undetected objects) than Type I errors (False Positives, i.e. falsely detecting objects that do not exist) was observed across all experimental setups and conditions, suggesting a consistent bias in annotation processes. Performance was better in tasks with higher target-background ratios (i.e., more objects per unit area). Prior experience did not significantly impact performance and may, in some cases, even lead to overestimation in segmentation. These results provide evidence that annotators are relatively cautious and tend to identify objects only when they are confident about them, prioritizing underestimation over overestimation. Annotators‚Äô performance is also influenced by object scarcity, showing a decline in areas with extremely imbalanced datasets and a low ratio of target-to-background. These findings may enhance annotation strategies for remote sensing research while efficient human annotators are crucial in an era characterized by growing demands for high-quality training data to improve segmentation and detection models.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
human annotation, expert annotators, remote sensing, segmentation, object detection, error types, precision, recall.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Availability of reliable training data is a prime requirement for computer vision tasks such as automatic object detection (OD) and segmentation. Even large generic models, such as the <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">Segment Anything Model (SAM)</span>, that promise zero-shot capabilities, require problem-specific data to adapt to the particular problem being studied, often through methods such as transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib1" title="">1</a>]</cite>. Specifically, training sets are much needed for many remote sensing (RS) applications for earth observation missions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib2" title="">2</a>]</cite>. Despite recent developments in automatic, or semi-automatic, architectures to create training datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib3" title="">3</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib4" title="">4</a>]</cite>, and the availability of historical datasets for reuse, human annotators are still identified as the major source for the generation of training sets for computer vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib5" title="">5</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib1" title="">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib6" title="">6</a>]</cite>. This is also true in other artificial intelligence fields such as natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib7" title="">7</a>]</cite>. The need for data annotation (or labeling) by humans is particularly actual to measure performance of classifiers on quantitative tasks such as density measurement of rooftops. As machine learning (ML) usage increases dramatically <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib8" title="">8</a>]</cite> for various purposes, so is the need for human-annotated gold standards <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib9" title="">9</a>]</cite>. This requirement increases due to the emerging usage of crowdsourcing procedures and the ongoing development of large training sets by multiple nonexpert annotators <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib10" title="">10</a>]</cite>, which increase motivations to advance knowledge on how to increase labeling efficiency.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Human annotators vary in their skills, capabilities, and the approaches they use to achieve optimal annotation. Some annotators may have years of relevant experience while others may be laymen. Moreover, the approach in which the annotation process is conducted may also affect the outcome quality, for instance, whether the final annotation is based on a group of annotators and how each annotator influences the final result. Annotators can also be influenced by the object, or the task, characteristics. In RS applications, annotation by humans can be particularly challenging for various reasons, such as objects within the same category appearing differently in different images or regions within the same image. For example, a solar panel, which may be assumed a simple rectangular object, can be oriented in multiple directions toward the sun, affected by internal and external shading, and can vary in color and size. Consequently, it is difficult to expect that two random human annotators will produce the same results. Such differences may have substantial implications for training and prediction processes. Previous Earth observation studies have demonstrated that variations in training sets can impact training and validation accuracy. For instance, by using synthetic golden standards through a simulation process of real ground data, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib11" title="">11</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib12" title="">12</a>]</cite> demonstrate that the direction and magnitude of accuracy metric mis-estimation were a function of prevalence, size, and nature of imperfections in the reference standard. Namely, ground data that introduced errors and biases led to an incorrect underestimation of the model‚Äôs performance. While studies based on synthetic models have significant implications for understanding ground data accuracy, they are limited to simulated human behavior. This means they do not capture the actual biases, limitations, and advantages of human actions but instead generalize them. Findings based on simulations can reflect generic patterns but do not provide particular evidence of actual human actions and behaviors. Understanding the latter is crucial and requires further exploration, as will be demonstrated below.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">aim</span> is to assess performance of human annotators in two key computer-vision tasks: object detection and segmentation of RS data. Specifically, we tackle the following four <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">operative objectives</span>:
(1) To compare the performance of human annotators in OD versus segmentation tasks;
(2) To analyze distinctions between Type I errors (False Positives) and Type II errors (False Negatives) in annotations across different task conditions (sparse-target task versus dense-target task) and annotation setup strategies (individual versus group; and independent versus dependent group setup processes);
(3) To compare annotators‚Äô performances under various task conditions and setup strategies.
(4) To examine the impact of prior experience in RS data interpretation, digitization, and annotation on the annotation performance.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To achieve these objectives, we conducted an experiment on photovoltaic solar panels (PV), as a case study for rectangular objects with varied appearances, in a semiarid environment. The experiment involved 24 human participants and was carried out in a geoinformatics laboratory. The findings enhance the development of training sets by humans and improve our understanding of how annotators operate, ultimately leading to better selection and guidance of human annotators.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Training Sets and Human Annotators in RS</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Image segmentation is broadly referred to the technique of partitioning an image into segments labeled based on their characteristics. Pixel-level labeling, known as semantic segmentation, is an essential computer vision and RS technique for, e.g., crop cover analysis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib13" title="">13</a>]</cite>, land-use mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib14" title="">14</a>]</cite>, and environmental monitoring<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib15" title="">15</a>]</cite>. Among various semantic segmentation methodologies, Machine Learning has gained increasing usage over the past decades, with a notable rise in using Deep Learning in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib16" title="">16</a>]</cite>. Consequently, labeling has gained interest by many researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib17" title="">17</a>]</cite>. Labeling processes rely on training sets that are accurately labeled using trustworthy sources and then used to train models for predictions. The quality of training sets is crucial, as they influence the performance of machine- and deep-learning models. Thus, improving training processes to extract high-quality training sets is much sought after. While efforts were made to generate simulated training sets using machines or to enhance human annotation through automated algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib18" title="">18</a>]</cite>, most current image segmentation models still rely on human input (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib6" title="">6</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib1" title="">1</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Annotations by humans can be utilized at the beginning of the training process, creating a high-quality dataset that serves as a reference for model‚Äôs training and later being expanded automatically <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib1" title="">1</a>]</cite>; or, at the final stage for monitoring objects/pixels predicted by models with low confidence and high loss. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib19" title="">19</a>]</cite> evaluated annotations quality using data from human annotators who were asked to correct model segmentation with corrective clicks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Annotators can be students, professionals, and even random individuals participating in crowdsourcing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib20" title="">20</a>]</cite>, whereas the latter especially are characterized by varying quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib21" title="">21</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib22" title="">22</a>]</cite> and sometimes may even reduce overall performance. The latters are usually referred to as <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">malicious</span> workers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib23" title="">23</a>]</cite>. Human annotators are among the most commonly used sources in RS data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib24" title="">24</a>]</cite>, which often lacks sufficient labeled training sources<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib25" title="">25</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib26" title="">26</a>]</cite>. The demand for reliable data from humans has led to an ongoing effort to develop semi-automated processes for generating RS annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib27" title="">27</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib25" title="">25</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib28" title="">28</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib29" title="">29</a>]</cite>. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib30" title="">30</a>]</cite> proposed a framework for annotating RS images using humans for initial annotation only, which then serves as a basis for automatic annotation and reduces the efforts required for full annotation. Namely, even in generated annotations, reliance on human annotations at the initial stage is still unavoidable and will probably be inevitable in the near future, while human annotators‚Äô quality clearly impacts machine-generated training sets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib26" title="">26</a>]</cite>. Human annotation performance can suffer biases and we consider here three of them. The <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.2">first</span> is the annotation strategy, namely how the final annotation is obtained. <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.3">Second</span> is the nature of the task, in particular targets prevalence. The <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.4">third</span> bias is the diversity of human annotators‚Äô characteristics, e.g., their prior experience.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Previous studies did not examine the impact of these biases on human annotation quality and such an investigation would improve understanding of training set generation and lead to more informed recommendations on how to best activate human annotators in creating training sets. This is critical for RS data analysis, because it is more challenging to annotate RS data than simple ground photos due to their complexity in interpretation, richness, diversity, and intricacy of the information they contain, and the vast presence of objects with similar appearances <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">Human annotators quality was investigated, primarily comparing cognitive abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib31" title="">31</a>]</cite>, demographic characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib32" title="">32</a>]</cite>, tactics of annotation process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib33" title="">33</a>]</cite>, and their reliability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib23" title="">23</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib34" title="">34</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib35" title="">35</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib10" title="">10</a>]</cite>. However, these studies have focused on human annotation of standard photographs rather than RS imagery. Only a few studies addressed annotation quality of the latter. Among these, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib36" title="">36</a>]</cite> has found that more educated annotators performed better at interpreting land-cover types from RS imagery. Manual interpretation accuracy improved with training, access to ultra-high-resolution images as supplementary data, and the annotators‚Äô familiarity with the study area. The performance of a CNN model, when trained on a small sample dataset, was inferior to that of manual interpretation. Group consistency also proved a reliable indicator of the samples‚Äô quality. However, Wang et al. indeed assessed the accuracy and strategy of individual annotators but not of groups and also did not address the common imbalanced data problem, which is typical to RS tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib37" title="">37</a>]</cite>. These two issues, among others, are addressed here.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Annotation Strategy</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Annotating RS data is time-consuming and, therefore, labor-intensive, expensive, and can be error-prone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib38" title="">38</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib17" title="">17</a>]</cite>. It is, therefore, crucial to select and plan the most useful annotation strategy to ensure the highest performance. Among those available to the researcher are the <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">individual</span> and <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">group</span> strategies. The individual strategy is the simple activation of a single annotator who is requested to identify an object and delineate its borders according to her own consideration. Such an approach does not mix the knowledge of several individuals and is highly dependent on each annotator‚Äôs skills and expertise. In the group approach, a team of annotators combine knowledge and detect or segment objects. This is typically accomplished using a majority vote decision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib39" title="">39</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib24" title="">24</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib40" title="">40</a>]</cite>, where, in binary tasks, individual votes are compared, and the object (in classification tasks) or pixel (in segmentation task) is included in the final annotation if the majority of annotators have marked it. In multi-class tasks, the most frequent label among the annotations (i.e., the majority) is selected as the final label for that object/pixel. This approach was used mainly when non-experts made the pool of annotators and the group compensated for errors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib21" title="">21</a>]</cite>. Another group strategy is to assign a greater weight to annotators with higher skills or prior experience. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib36" title="">36</a>]</cite> proposed an annotation framework featuring a pre-annotation test of the annotators to estimate their ability to distinguish between classes. The framework assesses each annotator‚Äôs labeling proficiency, providing a prior label quality estimate, whereas a subsequent quality evaluation process uses this prior quality as a weight, giving higher weights to annotators who perform better in difficult classes. This approach may increase likelihood for correct labels to prevail, even when they are fewer in number.
Another group strategy is the sequential approach, where the annotation process is performed in progressive stages: the first annotator labels or delineates the objects, and subsequent annotators review the process one after the other <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib20" title="">20</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Task Conditions - Level of Imbalanced Data</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">High-class imbalance is naturally inherent in many real-world applications, e.g., medical diagnosis, natural disaster prediction, fraud detection, etc. Imbalanced data problems are also very frequent in RS problems, especially when detecting specific objects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib17" title="">17</a>]</cite>, and in both cases in which only two classes are present in the considered data set and in multiclass cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib41" title="">41</a>]</cite>. In deep learning applications in RS, this issue was extensively studied e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib42" title="">42</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib43" title="">43</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib44" title="">44</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib45" title="">45</a>]</cite>, since highly imbalanced data poses added difficulty, as most models exhibit bias towards the majority class while neglecting underrepresented ones, therefore leading to biased predictions and increased Type I errors (false positives). Imbalance can also slow down learning processes, as the model struggles to learn effectively from limited examples of minority classes and abundance of examples from the majority group, ultimately reducing overall accuracy and generalizability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib46" title="">46</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib47" title="">47</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib48" title="">48</a>]</cite>.
A key question in human annotators‚Äô performance is how imbalanced data affects their ability to identify objects and the tendency to over- or under-estimate their prevalence. In an imbalanced dataset, class distribution is not uniform, and in a binary problem e.g., detection and segmentation of PV cells, the majority of data belongs to one class (non-PV cell), and only a small part belongs to the other (PV cells), a problem often referred to as a needle in a haystack. Thus, the study of large regions with a sparse appearance of a specific target can lead to a decrease in the annotator‚Äôs performance by decreasing the opportunity to identify the object by increasing the search time of large, sometimes monotone areas that may be tiring to search for a specific object, and by increasing the likelihood of labeling targets where they are not present (false positive). Furthermore, in dense areas, various phases of the same entities can be viewed by human annotators in a small and condensed area, the annotator can become more familiar with the object, exposed to similar targets in proximity, that is, within the same field of view, allowing for comparison between targets, and therefore more easily identify them. Human annotators can use the surrounding context to make informed decisions about ambiguous or partially obscured objects. For example, in an urban environment, the annotator can identify a solar panel when it is located on the roof center because she saw it in many other cases in a similar spot or with the same deployment pattern. In dense areas, this ability may help accurately annotate objects that may be closely located together. Despite their presumed potential effect described above, these biases of the target-background ratio of an RS imagery on human annotation quality have not been investigated so far.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Annotators Expertise</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">The ability to digitize and label objects correctly in RS data may differ between individuals, as with any other tasks, and especially complex tasks, that humans apply <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib18" title="">18</a>]</cite>. Differences in capabilities may be related to spatial cognition of the annotator and her ability to process information about different environments and spaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib49" title="">49</a>]</cite>. Spatial cognition is influenced by various personal characteristics that are relevant to annotation tasks, such as wayfinding ability, life stage, gender, and prior experience <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib50" title="">50</a>]</cite>. Individuals are also prone to spatial distortions, including alignment and rotation biases, distance and direction errors, and structural and semantic biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib51" title="">51</a>]</cite>. While it is beyond the scope of this paper to explore the various aspects of spatial cognition that affect human annotators, we focused on experience and, more specifically, on the difference between experts and non-experts in their performance and their tendency to make mistakes.
This category was chosen because human annotation skills have gained significant attention in the growing field of crowdsourcing. Previous studies indicate that agencies often employ non-experts, who are available at a lower cost, reserving experts for tasks that are particularly complex and justify higher annotation expenses. Segmentation annotations of medical images, for example, suffer from a scarcity of expert annotators, and annotations in that field show considerable variability, which is further influenced by the annotator‚Äôs level of expertise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib52" title="">52</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib21" title="">21</a>]</cite>. This raises a key question for annotation projects: which type of annotator is cost-effective to employ? Additionally, the differences between these two groups affect how crowdsourced data should be managed (see the section above on annotation strategy). A critical question arises regarding how this practice impacts the quality of training datasets created by non-experts. This conundrum was also observed in NLP annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib53" title="">53</a>]</cite>.
For example, the study by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib9" title="">9</a>]</cite> demonstrated that contrary to the common belief that ‚Äùexperts are better‚Äù, experts did not consistently produced higher-quality annotations compared with non-experts. While lay users might make mistakes due to a lack of knowledge, experts possessing sufficient knowledge may sometimes misinterpret implicit data, a tendency not observed in lay users. Conversely, research in RS indicates that generating high-quality labels for RS data is a complex task that demands previous knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib26" title="">26</a>]</cite>. Additionally, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib30" title="">30</a>]</cite> found that expert annotators are often necessary for accurately identifying pixels at object boundaries and in ambiguous regions within urban landscapes using RS data. Similar findings were observed in studies examining the impact of expertise on human annotation of medical images, and strategies for properly weighing the annotator‚Äôs level of expertise have been developed in this field<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib54" title="">54</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib21" title="">21</a>]</cite>. The work of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib36" title="">36</a>]</cite> showed that annotation accuracy increased after individual annotators were trained and provided with supporting and diverse background material on the problem. However, no study has examined the difference in annotation quality between experts and non-experts in RS imagery and the weight that should be given to these experts in determining the final results when working with a group of annotators using the commonly-used majority vote strategy.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.1">The above four subsections describe distinct challenges of data annotation by humans, and the large need for high-quality training sets in various RS fields of image segmentation. The current paper aims to bridge the three research gaps and suggests an experiment that provides knowledge on the difference in performance of human annotators, focusing on comparisons between experimental setups, task conditions, and variations in annotators‚Äô expertise. According to our best knowledge, such an experiment has not been published yet.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Methods</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Participants</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Overall 24 students from the Department of Environmental, Geoinformatics and Urban Planning Sciences at Ben-Gurion University of the Negev, have participated in the experiment. This sample size aligns with commonly accepted standards of human annotators‚Äô experiments in RS works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib55" title="">55</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib22" title="">22</a>]</cite>. All students are graduates of the course ‚ÄôIntroduction to Geoinformatics‚Äô, during which they have gained limited experience in using various types of RS imagery. The course includes a two-hour class how to perform digitization and further usage of ArcGIS Pro digitizing tools for a single home exercise. So the majority of the participants have a very limited experience that can be acquired by guided learning of a few hours in every agency. Prior to the experiment, the participants were asked to complete a questionnaire detailing their previous experience in interpreting RS imagery, conducting digitization processes, and performing annotation tasks. Among the 24 participants, six experts were identified, with an average prior experience of 22 months in professional annotation projects in the public or private sector. These six participants are denoted hereafter as the experts. The participants‚Äô ages ranged from 20 to 30, with 56% being male and 44% female. All participants were first- or second-year undergraduate students.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The participants were asked to identify all solar panels appearing in the aerial image (OD task) and to delineate the solar panel boundaries with maximum accuracy (segmentation task). In addition, they were asked to rate each identified object according to three levels of confidence in the correctness of their identification. Before the experiment, they received a detailed briefing on the nature of the target (small-scale solar panels), its common and less common appearances, and the challenges involved in its detection and delineation (such as shading, low contrast, resembling objects, and adjacent objects, see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F1" title="Figure 1 ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="809" id="S3.F1.g1" src="extracted/5871508/challenges.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Challenges of small-scale PVs detection from RS images: A - Low contrast of ground-based panels with their surroundings (on the left), compared with a high contrast of rooftop panels (on the right). B - Presence of adjacent objects near ground panels make them difficult to detect. C - Shading of the panel makes it difficult to distinguish it from the target. D - The target appears in varying RGB values, making it difficult to identify. E ‚Äì resemblance to other objects: a small shade structure with a similar size and color to a solar panel. F - a striped tarpaulin sheet resembling a solar panel.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.5.1.1">III-B</span>1 </span>Annotation strategy</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The participants were divided into two equal size groups: one group performed an <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">independent annotation process</span>, while the other followed a <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.2">dependent annotation process</span>. The participants in the independent process were asked to perform the annotation tasks independently of other annotators. These participants were randomly assigned to teams of three annotators. The final annotation for each team was determined based on the majority vote principle: for the evaluation of OD performance, an object (panel) labeled by two or more annotators was included in the final annotation. For segmentation evaluation, a pixel marked by two or more annotators was included in the final annotation. The participants in the dependent annotation process were divided into teams of three annotators, where the first conducted a full annotation, the second reviewed the detection and segmentation, and the third reviewed the detection and segmentation after the previous reviewer (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>A). A final annotation was obtained after the second review. Further comparison was made between the annotation performance of individuals, where their annotation served as the final annotation for performance evaluation, and groups, consisting of the final annotation of both dependent and independent teams.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.5.1.1">III-B</span>2 </span>Task Conditions</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To examine tasks with different levels of imbalanced data, namely different target-background ratios, the participants were given two aerial image segments, each containing the same number of targets but differing in total area. For the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">dense-target task</span>, a segment with a total area of 0.15 km¬≤ and a high density of objects was selected. For the <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.2">sparse-target task</span>, a segment with a total area of 4 km¬≤ was chosen (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>B).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS3.5.1.1">III-B</span>3 </span>Prior Experience</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">In addition to the performance comparison between individual expert and non-expert annotators, we designed a setup in which the expert annotator was given double the weight when voting on the objects to be included in the final annotation (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>C). This setup was applied to teams conducting an independent annotation process.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="659" id="S3.F2.g1" src="extracted/5871508/setup.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Experimental setup overview: A ‚Äì Strategy setup: Individuals annotator (a1) versus groups of 3 annotators (a2). Within the groups: Independent Process (a2 on the left) - Each annotator creates an annotation separately. The final annotation is determined by majority vote. An object marked by at least 2 annotators will be included in the final annotation; Dependent Process (a2 on the right) - The first annotator passes the annotation to a reviewer who corrects it and passes the corrected product to a second reviewer, who finalizes the annotation. B ‚Äì Dense-target task versus sparse-target task. Each task contains the same number of targets spread over different area sizes for varying target-background imbalance. C ‚Äì Expert-weighted setup: assigning double weight to the expert annotator in the group compared to the non-expert annotators. This setup is compared to an unweighted setup (a2 left panel).</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Experimental Analysis</span>
</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS1.5.1.1">III-C</span>1 </span>Data</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">RS image: An aerial photograph of the northern Negev of Israel (centered at 30¬∞40' N, 34¬∞50' E) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib56" title="">56</a>]</cite> from 2020, with a resolution of 0.15 meters, was selected for the experiment. In this area, over 1,300 clusters of Bedouin settlements are scattered across more than 600 square kilometers. This population is disconnected from the national electricity grid and is therefore characterized by high adoption rates of small household solar panels, located both on the ground and on rooftops, positioned at various angles and locations.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">Golden standard dataset: The performance evaluation of the annotators was conducted by comparison to a gold standard. The gold standard was established through a two-step process. In the first step, the aerial image segments were annotated by an expert in the interpretation and annotation of RS images and reviewed by another annotator. In the second step, another independent review of the annotation was carried out by four RS experts who are deeply familiar with the study area. The gold standard was accepted only after achieving a complete consensus among the experts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS2.5.1.1">III-C</span>2 </span>Performance Evaluation Metrics</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Across all setups and tasks, a comparison using a confusion matrix was made between the final annotations of each annotator/team and the gold standard.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.1">A confusion matrix</span> is a table that provides a detailed summary of a labeling performance by cross-referencing annotated and actual labels, illustrating the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), thereby serving as the basis for calculating evaluation metrics, such as accuracy, precision, and recall (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F3" title="Figure 3 ‚Ä£ III-C2 Performance Evaluation Metrics ‚Ä£ III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">3</span></a>). Examples of the performed annotations and annotation errors are presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F4" title="Figure 4 ‚Ä£ III-C2 Performance Evaluation Metrics ‚Ä£ III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">4</span></a>. The detection and delineation of solar panels is an imbalanced data problem because the target-background ratio can be extremely low. Therefore, the <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.2">accuracy</span> metric is not suitable for performance evaluation due to the accuracy paradox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib57" title="">57</a>]</cite>. The performance evaluation focused on two metrics that are appropriate for imbalanced problems: <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.3">Pecision</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS2.p2.1.4">recall</span>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="608" id="S3.F3.g1" src="extracted/5871508/confution_matrix.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Confusion matrix and performance metrics in OD and segmentation: A - Components of confusion matrix for performance evaluations. On the left: Matrix components for OD. An object is defined as a True Positive if it has at least 60% overlap with ground truth. The matrix components quantify the number of annotated objects (panels) in each category. On the right: Matrix components for segmentation. The components represent the number of annotated pixels in each category. B ‚Äì on the left: A confusion matrix with a tool to represent all combinations between ground truth and annotation in binary classification. On the right: Performance metrics derived from the confusion matrix. <span class="ltx_text ltx_font_italic" id="S3.F3.3.1">Precision</span> measures the ratio of correctly identified panels to all annotated panels, representing the accuracy of positive annotations; <span class="ltx_text ltx_font_italic" id="S3.F3.4.2">Recall</span> measures the ratio of correctly identified to all actual panels, representing the model‚Äôs ability to identify all relevant instances.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S3.F4.g1" src="extracted/5871508/annotation_examples.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Top row: Examples from the annotation task. Annotators were asked to identify and segment solar panels. Bottom row: Examples of annotations (red rectangle); A ‚Äì an unannotated panel (FN object); B ‚Äì wrong detection (FP object), where the annotated object is a sun-heated boiler; C ‚Äì under-segmentation (FN pixels), with the panel not fully annotated; D ‚Äì over-segmentation (FP pixels), where the annotation includes the shadow of the panel.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">Precision quantifies the proportion of correctly identified target instances out of all instances identified as targets, reflecting the annotator‚Äôs ability to minimize false positives and ensure the relevance of its annotations.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S3.E1X.2.1.1.1">Precision</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{TP}{TP+FP}" class="ltx_Math" display="inline" id="S3.E1X.3.2.2.m1.1"><semantics id="S3.E1X.3.2.2.m1.1a"><mrow id="S3.E1X.3.2.2.m1.1.1" xref="S3.E1X.3.2.2.m1.1.1.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.2" xref="S3.E1X.3.2.2.m1.1.1.2.cmml"></mi><mo id="S3.E1X.3.2.2.m1.1.1.1" xref="S3.E1X.3.2.2.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E1X.3.2.2.m1.1.1.3" xref="S3.E1X.3.2.2.m1.1.1.3.cmml"><mfrac id="S3.E1X.3.2.2.m1.1.1.3a" xref="S3.E1X.3.2.2.m1.1.1.3.cmml"><mrow id="S3.E1X.3.2.2.m1.1.1.3.2" xref="S3.E1X.3.2.2.m1.1.1.3.2.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.3.2.2" xref="S3.E1X.3.2.2.m1.1.1.3.2.2.cmml">T</mi><mo id="S3.E1X.3.2.2.m1.1.1.3.2.1" xref="S3.E1X.3.2.2.m1.1.1.3.2.1.cmml">‚Å¢</mo><mi id="S3.E1X.3.2.2.m1.1.1.3.2.3" xref="S3.E1X.3.2.2.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S3.E1X.3.2.2.m1.1.1.3.3" xref="S3.E1X.3.2.2.m1.1.1.3.3.cmml"><mrow id="S3.E1X.3.2.2.m1.1.1.3.3.2" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.3.3.2.2" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.2.cmml">T</mi><mo id="S3.E1X.3.2.2.m1.1.1.3.3.2.1" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.1.cmml">‚Å¢</mo><mi id="S3.E1X.3.2.2.m1.1.1.3.3.2.3" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S3.E1X.3.2.2.m1.1.1.3.3.1" xref="S3.E1X.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E1X.3.2.2.m1.1.1.3.3.3" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.cmml"><mi id="S3.E1X.3.2.2.m1.1.1.3.3.3.2" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S3.E1X.3.2.2.m1.1.1.3.3.3.1" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E1X.3.2.2.m1.1.1.3.3.3.3" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.3.cmml">P</mi></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.3.2.2.m1.1b"><apply id="S3.E1X.3.2.2.m1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1"><eq id="S3.E1X.3.2.2.m1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S3.E1X.3.2.2.m1.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.2">absent</csymbol><apply id="S3.E1X.3.2.2.m1.1.1.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3"><divide id="S3.E1X.3.2.2.m1.1.1.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3"></divide><apply id="S3.E1X.3.2.2.m1.1.1.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.2"><times id="S3.E1X.3.2.2.m1.1.1.3.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.2.1"></times><ci id="S3.E1X.3.2.2.m1.1.1.3.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.2.2">ùëá</ci><ci id="S3.E1X.3.2.2.m1.1.1.3.2.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.2.3">ùëÉ</ci></apply><apply id="S3.E1X.3.2.2.m1.1.1.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3"><plus id="S3.E1X.3.2.2.m1.1.1.3.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.1"></plus><apply id="S3.E1X.3.2.2.m1.1.1.3.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.2"><times id="S3.E1X.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S3.E1X.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.2">ùëá</ci><ci id="S3.E1X.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.2.3">ùëÉ</ci></apply><apply id="S3.E1X.3.2.2.m1.1.1.3.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.3"><times id="S3.E1X.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.1"></times><ci id="S3.E1X.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.2">ùêπ</ci><ci id="S3.E1X.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S3.E1X.3.2.2.m1.1.1.3.3.3.3">ùëÉ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.3.2.2.m1.1c">\displaystyle=\frac{TP}{TP+FP}</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.3.2.2.m1.1d">= divide start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_P end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p5">
<p class="ltx_p" id="S3.SS3.SSS2.p5.1">The complement of precision is the false positive rate (Type I errors), which reflects the likelihood that a positive annotation made by the annotator is actually incorrect.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p6">
<p class="ltx_p" id="S3.SS3.SSS2.p6.1">Recall measures the proportion of actual positive instances correctly identified by the annotator, capturing its ability to detect all relevant cases (true positives) and minimizing false negatives.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p7">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S3.E2X.2.1.1.1">Recall</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{TP}{TP+FN}" class="ltx_Math" display="inline" id="S3.E2X.3.2.2.m1.1"><semantics id="S3.E2X.3.2.2.m1.1a"><mrow id="S3.E2X.3.2.2.m1.1.1" xref="S3.E2X.3.2.2.m1.1.1.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.2" xref="S3.E2X.3.2.2.m1.1.1.2.cmml"></mi><mo id="S3.E2X.3.2.2.m1.1.1.1" xref="S3.E2X.3.2.2.m1.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E2X.3.2.2.m1.1.1.3" xref="S3.E2X.3.2.2.m1.1.1.3.cmml"><mfrac id="S3.E2X.3.2.2.m1.1.1.3a" xref="S3.E2X.3.2.2.m1.1.1.3.cmml"><mrow id="S3.E2X.3.2.2.m1.1.1.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.2.2.cmml">T</mi><mo id="S3.E2X.3.2.2.m1.1.1.3.2.1" xref="S3.E2X.3.2.2.m1.1.1.3.2.1.cmml">‚Å¢</mo><mi id="S3.E2X.3.2.2.m1.1.1.3.2.3" xref="S3.E2X.3.2.2.m1.1.1.3.2.3.cmml">P</mi></mrow><mrow id="S3.E2X.3.2.2.m1.1.1.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.3.cmml"><mrow id="S3.E2X.3.2.2.m1.1.1.3.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.3.2.2" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.2.cmml">T</mi><mo id="S3.E2X.3.2.2.m1.1.1.3.3.2.1" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.1.cmml">‚Å¢</mo><mi id="S3.E2X.3.2.2.m1.1.1.3.3.2.3" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.3.cmml">P</mi></mrow><mo id="S3.E2X.3.2.2.m1.1.1.3.3.1" xref="S3.E2X.3.2.2.m1.1.1.3.3.1.cmml">+</mo><mrow id="S3.E2X.3.2.2.m1.1.1.3.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.cmml"><mi id="S3.E2X.3.2.2.m1.1.1.3.3.3.2" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.2.cmml">F</mi><mo id="S3.E2X.3.2.2.m1.1.1.3.3.3.1" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.1.cmml">‚Å¢</mo><mi id="S3.E2X.3.2.2.m1.1.1.3.3.3.3" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.3.cmml">N</mi></mrow></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.3.2.2.m1.1b"><apply id="S3.E2X.3.2.2.m1.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1"><eq id="S3.E2X.3.2.2.m1.1.1.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.1.1.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.2">absent</csymbol><apply id="S3.E2X.3.2.2.m1.1.1.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3"><divide id="S3.E2X.3.2.2.m1.1.1.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3"></divide><apply id="S3.E2X.3.2.2.m1.1.1.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2"><times id="S3.E2X.3.2.2.m1.1.1.3.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.1"></times><ci id="S3.E2X.3.2.2.m1.1.1.3.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.2">ùëá</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.2.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.2.3">ùëÉ</ci></apply><apply id="S3.E2X.3.2.2.m1.1.1.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3"><plus id="S3.E2X.3.2.2.m1.1.1.3.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.1"></plus><apply id="S3.E2X.3.2.2.m1.1.1.3.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.2"><times id="S3.E2X.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S3.E2X.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.2">ùëá</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.2.3">ùëÉ</ci></apply><apply id="S3.E2X.3.2.2.m1.1.1.3.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.3"><times id="S3.E2X.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.1"></times><ci id="S3.E2X.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.2">ùêπ</ci><ci id="S3.E2X.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S3.E2X.3.2.2.m1.1.1.3.3.3.3">ùëÅ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.3.2.2.m1.1c">\displaystyle=\frac{TP}{TP+FN}</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.3.2.2.m1.1d">= divide start_ARG italic_T italic_P end_ARG start_ARG italic_T italic_P + italic_F italic_N end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p8">
<p class="ltx_p" id="S3.SS3.SSS2.p8.1">The complement of recall is the false negative rate (Type II errors), which indicates the proportion of positive instances that were missed by the annotator, highlighting the likelihood of failing to identify a true positive.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p9">
<p class="ltx_p" id="S3.SS3.SSS2.p9.1">The F1 score is the harmonic mean of precision and recall and provides a balanced view of performance, especially when there is a trade-off between precision and recall, making it particularly suitable for evaluating performance where one type of error is not more significant than the other.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p10">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath ltx_font_italic" id="S3.E3X.2.1.1.1">F1 Score</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=2\times\frac{\text{Precision}\times\text{Recall}}{\text{%
Precision}+\text{Recall}}" class="ltx_Math" display="inline" id="S3.E3X.3.2.2.m1.1"><semantics id="S3.E3X.3.2.2.m1.1a"><mrow id="S3.E3X.3.2.2.m1.1.1" xref="S3.E3X.3.2.2.m1.1.1.cmml"><mi id="S3.E3X.3.2.2.m1.1.1.2" xref="S3.E3X.3.2.2.m1.1.1.2.cmml"></mi><mo id="S3.E3X.3.2.2.m1.1.1.1" xref="S3.E3X.3.2.2.m1.1.1.1.cmml">=</mo><mrow id="S3.E3X.3.2.2.m1.1.1.3" xref="S3.E3X.3.2.2.m1.1.1.3.cmml"><mn id="S3.E3X.3.2.2.m1.1.1.3.2" xref="S3.E3X.3.2.2.m1.1.1.3.2.cmml">2</mn><mo id="S3.E3X.3.2.2.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.E3X.3.2.2.m1.1.1.3.1.cmml">√ó</mo><mstyle displaystyle="true" id="S3.E3X.3.2.2.m1.1.1.3.3" xref="S3.E3X.3.2.2.m1.1.1.3.3.cmml"><mfrac id="S3.E3X.3.2.2.m1.1.1.3.3a" xref="S3.E3X.3.2.2.m1.1.1.3.3.cmml"><mrow id="S3.E3X.3.2.2.m1.1.1.3.3.2" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.cmml"><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.2.2" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.2a.cmml">Precision</mtext><mo id="S3.E3X.3.2.2.m1.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.1.cmml">√ó</mo><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.2.3" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.3a.cmml">Recall</mtext></mrow><mrow id="S3.E3X.3.2.2.m1.1.1.3.3.3" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.cmml"><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.3.2" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.2a.cmml">Precision</mtext><mo id="S3.E3X.3.2.2.m1.1.1.3.3.3.1" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.1.cmml">+</mo><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.3.3" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.3a.cmml">Recall</mtext></mrow></mfrac></mstyle></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.3.2.2.m1.1b"><apply id="S3.E3X.3.2.2.m1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1"><eq id="S3.E3X.3.2.2.m1.1.1.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.1"></eq><csymbol cd="latexml" id="S3.E3X.3.2.2.m1.1.1.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.2">absent</csymbol><apply id="S3.E3X.3.2.2.m1.1.1.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.3"><times id="S3.E3X.3.2.2.m1.1.1.3.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.1"></times><cn id="S3.E3X.3.2.2.m1.1.1.3.2.cmml" type="integer" xref="S3.E3X.3.2.2.m1.1.1.3.2">2</cn><apply id="S3.E3X.3.2.2.m1.1.1.3.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3"><divide id="S3.E3X.3.2.2.m1.1.1.3.3.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3"></divide><apply id="S3.E3X.3.2.2.m1.1.1.3.3.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.2"><times id="S3.E3X.3.2.2.m1.1.1.3.3.2.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.1"></times><ci id="S3.E3X.3.2.2.m1.1.1.3.3.2.2a.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.2"><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.2.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.2">Precision</mtext></ci><ci id="S3.E3X.3.2.2.m1.1.1.3.3.2.3a.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.3"><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.2.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.2.3">Recall</mtext></ci></apply><apply id="S3.E3X.3.2.2.m1.1.1.3.3.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.3"><plus id="S3.E3X.3.2.2.m1.1.1.3.3.3.1.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.1"></plus><ci id="S3.E3X.3.2.2.m1.1.1.3.3.3.2a.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.2"><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.3.2.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.2">Precision</mtext></ci><ci id="S3.E3X.3.2.2.m1.1.1.3.3.3.3a.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.3"><mtext id="S3.E3X.3.2.2.m1.1.1.3.3.3.3.cmml" xref="S3.E3X.3.2.2.m1.1.1.3.3.3.3">Recall</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.3.2.2.m1.1c">\displaystyle=2\times\frac{\text{Precision}\times\text{Recall}}{\text{%
Precision}+\text{Recall}}</annotation><annotation encoding="application/x-llamapun" id="S3.E3X.3.2.2.m1.1d">= 2 √ó divide start_ARG Precision √ó Recall end_ARG start_ARG Precision + Recall end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS3.5.1.1">III-C</span>3 </span>Spatial Data Processing</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.2">When combining the polygonal layers of participants of a given group (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>) into their final prediction, the intersections of at least two participants were considered. In the expert-based setup, expert participants were given double votes, i.e., their polygonal layer was duplicated (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>C). Predicted ‚Äùpanels‚Äù with areas less than 1 <math alttext="m^{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.1.m1.1"><semantics id="S3.SS3.SSS3.p1.1.m1.1a"><msup id="S3.SS3.SSS3.p1.1.m1.1.1" xref="S3.SS3.SSS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS3.p1.1.m1.1.1.2" xref="S3.SS3.SSS3.p1.1.m1.1.1.2.cmml">m</mi><mn id="S3.SS3.SSS3.p1.1.m1.1.1.3" xref="S3.SS3.SSS3.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.1.m1.1b"><apply id="S3.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1.2">ùëö</ci><cn id="S3.SS3.SSS3.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS3.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.1.m1.1c">m^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.1.m1.1d">italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math> were discarded. Then, agreement metrics between the combined participant layer and the reference (‚Äùgold standard‚Äù) layer were calculated. In the OD approach (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F3" title="Figure 3 ‚Ä£ III-C2 Performance Evaluation Metrics ‚Ä£ III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">3</span></a>A), we considered polygon count, where group polygons covering more than 60% of a reference polygon they intersect with are considered TP, while the remaining ‚Äùunmatched‚Äù group and reference polygons are considered FP and FN, respectively. In the segmentation approach, we considered the area (in <math alttext="m^{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.2.m2.1"><semantics id="S3.SS3.SSS3.p1.2.m2.1a"><msup id="S3.SS3.SSS3.p1.2.m2.1.1" xref="S3.SS3.SSS3.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS3.p1.2.m2.1.1.2" xref="S3.SS3.SSS3.p1.2.m2.1.1.2.cmml">m</mi><mn id="S3.SS3.SSS3.p1.2.m2.1.1.3" xref="S3.SS3.SSS3.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.2.m2.1b"><apply id="S3.SS3.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS3.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS3.SSS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS3.p1.2.m2.1.1.2">ùëö</ci><cn id="S3.SS3.SSS3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS3.SSS3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.2.m2.1c">m^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.2.m2.1d">italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>) of overlap (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F3" title="Figure 3 ‚Ä£ III-C2 Performance Evaluation Metrics ‚Ä£ III-C Experimental Analysis ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">3</span></a>B), rather than polygon count. The process was repeated for the different participant groupings, tasks, and expert treatments (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>), while the reference layer remained fixed. Spatial data processing was done in R version 4.4.1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib58" title="">58</a>]</cite> and package <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS3.p1.2.1">sf<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_upright" id="S3.SS3.SSS3.p1.2.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib59" title="">59</a><span class="ltx_text ltx_font_upright" id="S3.SS3.SSS3.p1.2.1.2.2">]</span></cite></span>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS3.SSS4.5.1.1">III-C</span>4 </span>Statistical Analysis</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">Performance comparison across all setups and tasks was conducted using a T-test for differences in means of independent samples. A preliminary test for homoscedasticity was performed using the F-ratio to determine the method for variance estimation. For a visual representation of the results of each comparison, we used a box and whisker plot that summarizes the distribution of a dataset and its central tendency.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Results</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To address the four operative objectives (OBJ) outlined at the end of the Introduction, we formulated the following eight research questions (RQ). Each RQ is assigned with one of the four BJs.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_italic" id="S4.p2.1.1">OBJ1:</span> To compare the performance of human annotators in OD versus segmentation tasks.</p>
<ul class="ltx_itemize" id="S4.p2.2">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">RQ1:</span> Do annotators achieve higher recall, precision, and F1 scores in OD compared with segmentation?</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.p2.3"><span class="ltx_text ltx_font_italic" id="S4.p2.3.1">OBJ2:</span> To analyze distinctions between Type I errors (False Positives) and Type II errors (False Negatives) in annotations <span class="ltx_text ltx_font_italic" id="S4.p2.3.2">across</span> annotation setup strategies and task conditions.</p>
<ul class="ltx_itemize" id="S4.p2.4">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">RQ2:</span> Is there a difference between precision (indicating <span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.2">correct</span> detection) and recall (indicating <span class="ltx_text ltx_font_italic" id="S4.I2.i1.p1.1.3">complete</span> detection) in both OD and segmentation tasks when comparing individual with group-based annotation setup?</p>
</div>
<div class="ltx_para" id="S4.I2.i1.p2">
<p class="ltx_p" id="S4.I2.i1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p2.1.1">RQ3:</span> Is there a difference between precision and recall in both OD and segmentation tasks when comparing independent group annotation (based on majority voting) with dependent group annotation (involving sequential reviewing)?</p>
</div>
<div class="ltx_para" id="S4.I2.i1.p3">
<p class="ltx_p" id="S4.I2.i1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p3.1.1">RQ4:</span> Is there a difference between precision and recall in tasks where target-background ratio is higher (with objects clumped in small areas) compared with tasks having lower target-background ratios (with objects dispersed over wide areas)?</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_italic" id="S4.p3.1.1">OBJ3</span>: To compare annotators‚Äô performance across annotation setup strategies and varying task conditions.</p>
<ul class="ltx_itemize" id="S4.p3.2">
<li class="ltx_item" id="S4.I3.i1" style="list-style-type:none;">
<div class="ltx_para" id="S4.I3.i1.p1">
<p class="ltx_p" id="S4.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p1.1.1">RQ5:</span> Is there a difference, both in precision and recall, for OD and segmentation tasks, when comparing an independent to a dependent setup?</p>
</div>
<div class="ltx_para" id="S4.I3.i1.p2">
<p class="ltx_p" id="S4.I3.i1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.I3.i1.p2.1.1">RQ6:</span> Is there a difference, both in precision and recall in OD and segmentation tasks, between a task with a low target-background ratio and a task with a high target-background ratio?</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_italic" id="S4.p4.1.1">OBJ4</span>: To examine the impact of prior experience in RS data interpretation, digitization, and annotation on annotation performance.</p>
</div>
<div class="ltx_para" id="S4.p5">
<ul class="ltx_itemize" id="S4.p5.1">
<li class="ltx_item" id="S4.I4.i1" style="list-style-type:none;">
<div class="ltx_para" id="S4.I4.i1.p1">
<p class="ltx_p" id="S4.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i1.p1.1.1">RQ7:</span> Is there a difference in OD and segmentation precision, recall, and F1 between expert and non-expert annotators in an individual annotation setup?</p>
</div>
<div class="ltx_para" id="S4.I4.i1.p2">
<p class="ltx_p" id="S4.I4.i1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.I4.i1.p2.1.1">RQ8:</span> Do the precision, recall, and F1 scores in OD and segmentation tasks improve when expert annotations are given double-weighting in an independent annotation process?</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="498" id="S4.F5.g1" src="extracted/5871508/od_seg.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of evaluation metrics between OD and segmentation tasks. All evaluation metrics are higher in OD compared with segmentation, indicating superior performance in identification compared with accurate delineation. Note that differences between the tasks are more pronounced in precision.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Object Detection versus Segmentation</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To address RQ1, the evaluation metrics: precision, recall, and F1 score, were compared between OD and segmentation tasks, without distinguishing between task conditions or experimental setups. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.F5" title="Figure 5 ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">5</span></a>, performance in OD is higher across all three metrics compared with segmentation. However, differences in precision are more pronounced (0.13, p-value for mean differences &lt;0.000) compared with recall (0.04, p-value for mean differences &lt;0.009). The difference in overall average performance (F1) between the tasks is 0.08 (p-value for mean differences &lt;0.000).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Differences in Error Types</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To achieve the second objective, differences between precision and Recall were analyzed. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.F6" title="Figure 6 ‚Ä£ IV-B Differences in Error Types ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">6</span></a>, in the OD task, precision scores are higher than recall scores across both individual and group annotation setups (RQ2) (differences of 0.05 and 0.07, respectively), independent and dependent annotation processes (RQ3) differences of 0.06 and 0.10, respectively), and dense-target as well as in sparse-target tasks (RQ4) (differences of 0.06 and 0.09, respectively). In all scenarios, the differences in average evaluation metrics are statistically significant (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T1" title="TABLE I ‚Ä£ IV-B Differences in Error Types ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">I</span></a>) indicating higher rates of Type II compared with Type I errors. In the segmentation task, no significant differences were found between the two evaluation metrics in any of the comparisons, except in the independent and dependent setups, where in the latter, recall results surpass precision (difference of 0.09), indicating a higher rate of Type I errors within the dependent setup and a tendency to overestimate the delineation.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="804" id="S4.F6.g1" src="extracted/5871508/precision_recall.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Comparison of Precision and Recall Results. In OD, across all setups and tasks, precision is higher than recall. In segmentation, the results are mixed: in the weighted-expert setup and the dependent annotation process, recall is higher than precision. In the independent annotation process, precision results are higher than recall results. The t-test results for mean differences are highlighted in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T1" title="TABLE I ‚Ä£ IV-B Differences in Error Types ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">I</span></a>.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_border_t" colspan="2" id="S4.T1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">OD</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.1.1.3" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.3.1">
<span class="ltx_p" id="S4.T1.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1.1.1">Segment.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.2.1" rowspan="2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.1.1">Task conditions</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.2.2" style="padding-top:1pt;padding-bottom:1pt;">Dense</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.2.3" style="padding-top:1pt;padding-bottom:1pt;">0.0001***</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.2.2.4" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.2.4.1">
<span class="ltx_p" id="S4.T1.1.2.2.4.1.1">0.4548</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.3.1" style="padding-top:1pt;padding-bottom:1pt;">Sparse</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.3.2" style="padding-top:1pt;padding-bottom:1pt;">0.0038**</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.3.3.3" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.3.3.3.1">
<span class="ltx_p" id="S4.T1.1.3.3.3.1.1">0.1997</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.4.1" rowspan="4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.4.1.1">Setup strategy</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.4.2" style="padding-top:1pt;padding-bottom:1pt;">Individuals</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.4.3" style="padding-top:1pt;padding-bottom:1pt;">0.004*</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T1.1.4.4.4" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.4.4.1">
<span class="ltx_p" id="S4.T1.1.4.4.4.1.1">0.0765</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.5.1" style="padding-top:1pt;padding-bottom:1pt;">Groups</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.5.2" style="padding-top:1pt;padding-bottom:1pt;">0.001**</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.5.5.3" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.5.3.1">
<span class="ltx_p" id="S4.T1.1.5.5.3.1.1">0.2907</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.6.1" style="padding-top:1pt;padding-bottom:1pt;">Independent</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.6.2" style="padding-top:1pt;padding-bottom:1pt;">0.0018**</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.6.6.3" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.6.6.3.1">
<span class="ltx_p" id="S4.T1.1.6.6.3.1.1">0.0144**</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.7.1" style="padding-top:1pt;padding-bottom:1pt;">Dependent</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.7.2" style="padding-top:1pt;padding-bottom:1pt;">0.0434*</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T1.1.7.7.3" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.7.7.3.1">
<span class="ltx_p" id="S4.T1.1.7.7.3.1.1">0.036*</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T1.1.8.8.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.8.8.1.1">Expertise effect</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T1.1.8.8.2" style="padding-top:1pt;padding-bottom:1pt;">Expert-weighted setup</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S4.T1.1.8.8.3" style="padding-top:1pt;padding-bottom:1pt;">0.0111*</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" id="S4.T1.1.8.8.4" style="width:37.0pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.8.8.4.1">
<span class="ltx_p" id="S4.T1.1.8.8.4.1.1">0.0241*</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>P-values of t-tests for differences in mean precision and recall across setup strategies, task conditions, and expert-weighted setup. Significant differences between precision and recall were observed in all OD setups and tasks. In the case of segmentation, significant distinctions between precision and recall were found both in the independent and dependent annotation processes, and in the expert-weighted setup.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Performance Comparisons</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To achieve the third objective, a comparison was made between the evaluation metrics across various strategy setups (RQ5) and task conditions (RQ6), both in OD and segmentation tasks. As indicated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.F7" title="Figure 7 ‚Ä£ IV-C Performance Comparisons ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">7</span></a>, higher precision was achieved in both OD and segmentation in group setups compared with individual annotators setups (difference of 0.035 and 0.015, respectively). Similar results achieved in the independent annotation process compared with the dependent process (difference of 0.03 and 0.12, respectively), and in tasks with a high target-background ratio (dense-target) compared with tasks with a low target-background ratio (sparse-target) (difference of 0.04 and 0.06, respectively), with a lower rate of Type I errors in these scenarios. In all these cases, the superiority of precision over recall was statistically significant (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T2" title="TABLE II ‚Ä£ IV-C Performance Comparisons ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">II</span></a>). Note the large difference in precision scores between the independent/dependent setups in the segmentation task, which stems both from a below average rate of Type I errors in the independent setup and an above average rate of Type I errors in the dependent setup.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In the OD task, no significant differences in recall were found across any of these scenarios, with the rate of Type II errors remaining consistent across all setups and task conditions. In segmentation, however, significant differences in recall were found between the dependent and independent setups, where the dependent setup reduced the rate of Type II errors compared with the independent setup and decreased the extent of under-segmentation in target delineations.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S4.F7.g1" src="extracted/5871508/performance.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Performance metrics comparison across task setups and conditions: top row ‚Äì OD; bottom row ‚Äì segmentation. In OD, precision is higher in the task of clumped objects compared with the sparse objects; among groups compared with single annotators; and in the independent annotation process compared with the dependent process. No differences were found between unweighted and weighted expert setups. Additionally, no significant differences in recall were observed across all setups and tasks (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T2" title="TABLE II ‚Ä£ IV-C Performance Comparisons ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">II</span></a> for t-test results). In the case of segmentation, significant differences were found across all task setups and conditions, with higher precision in task of clumped objects, among groups, and in the case of independent annotation process. Note that the weighted expert setup achieved lower precision compared with unweighted setup. Differences in recall were found only between single annotators and groups, with groups achieving higher scores.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="2" id="S4.T2.1.1.1.1" rowspan="2" style="width:56.9pt;padding-top:1pt;padding-bottom:1pt;"></th>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_r ltx_border_t" colspan="3" id="S4.T2.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Object detection</span></td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="S4.T2.1.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Segmentation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.2.2.1" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.1.1">
<span class="ltx_p" id="S4.T2.1.2.2.1.1.1">Precision</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.2.2.2" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.2.1">
<span class="ltx_p" id="S4.T2.1.2.2.2.1.1">Recall</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.1.2.2.3" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.3.1">
<span class="ltx_p" id="S4.T2.1.2.2.3.1.1">F1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.2.2.4" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.4.1">
<span class="ltx_p" id="S4.T2.1.2.2.4.1.1">Precision</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.2.2.5" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.5.1">
<span class="ltx_p" id="S4.T2.1.2.2.5.1.1">Recall</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.2.2.6" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.2.2.6.1">
<span class="ltx_p" id="S4.T2.1.2.2.6.1.1">F1</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.3.3.1" style="width:56.9pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.1.1">
<span class="ltx_p" id="S4.T2.1.3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.3.1.1.1.1">Task conditions</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.3.3.2" style="width:170.7pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.2.1">
<span class="ltx_p" id="S4.T2.1.3.3.2.1.1">Dense versus Sparse</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.3.3.3" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.3.1">
<span class="ltx_p" id="S4.T2.1.3.3.3.1.1">0.0434*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.3.3.4" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.4.1">
<span class="ltx_p" id="S4.T2.1.3.3.4.1.1">0.3847</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.1.3.3.5" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.5.1">
<span class="ltx_p" id="S4.T2.1.3.3.5.1.1">0.1223</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.3.3.6" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.6.1">
<span class="ltx_p" id="S4.T2.1.3.3.6.1.1">0.0323*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.3.3.7" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.7.1">
<span class="ltx_p" id="S4.T2.1.3.3.7.1.1">0.2359</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.3.3.8" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.3.3.8.1">
<span class="ltx_p" id="S4.T2.1.3.3.8.1.1">0.106</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.4">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.4.4.1" rowspan="2" style="width:56.9pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.1.1">
<span class="ltx_p" id="S4.T2.1.4.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.4.1.1.1.1">Setup strategy</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.4.4.2" style="width:170.7pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.2.1">
<span class="ltx_p" id="S4.T2.1.4.4.2.1.1">Individuals versus Groups</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.4.4.3" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.3.1">
<span class="ltx_p" id="S4.T2.1.4.4.3.1.1">0.001**</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.4.4.4" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.4.1">
<span class="ltx_p" id="S4.T2.1.4.4.4.1.1">0.391</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.1.4.4.5" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.5.1">
<span class="ltx_p" id="S4.T2.1.4.4.5.1.1">0.042*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.4.4.6" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.6.1">
<span class="ltx_p" id="S4.T2.1.4.4.6.1.1">0.02*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.4.4.7" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.7.1">
<span class="ltx_p" id="S4.T2.1.4.4.7.1.1">0.041*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.4.4.8" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.4.4.8.1">
<span class="ltx_p" id="S4.T2.1.4.4.8.1.1">0.01**</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.5">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r" id="S4.T2.1.5.5.1" style="width:170.7pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.1.1">
<span class="ltx_p" id="S4.T2.1.5.5.1.1.1">Independent versus Dependent</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T2.1.5.5.2" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.2.1">
<span class="ltx_p" id="S4.T2.1.5.5.2.1.1">0.0476*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T2.1.5.5.3" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.3.1">
<span class="ltx_p" id="S4.T2.1.5.5.3.1.1">0.4109</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r" id="S4.T2.1.5.5.4" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.4.1">
<span class="ltx_p" id="S4.T2.1.5.5.4.1.1">0.262</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T2.1.5.5.5" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.5.1">
<span class="ltx_p" id="S4.T2.1.5.5.5.1.1">0.0312*</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T2.1.5.5.6" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.6.1">
<span class="ltx_p" id="S4.T2.1.5.5.6.1.1">0.0819</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S4.T2.1.5.5.7" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.5.5.7.1">
<span class="ltx_p" id="S4.T2.1.5.5.7.1.1">0.1485</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.6">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.1.6.6.1" rowspan="2" style="width:56.9pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.1.1">
<span class="ltx_p" id="S4.T2.1.6.6.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.6.1.1.1.1">Expertise effect</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.6.6.2" style="width:170.7pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.2.1">
<span class="ltx_p" id="S4.T2.1.6.6.2.1.1">Experts versus Non-expert (Individuals)</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.6.6.3" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.3.1">
<span class="ltx_p" id="S4.T2.1.6.6.3.1.1">0.442</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.6.6.4" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.4.1">
<span class="ltx_p" id="S4.T2.1.6.6.4.1.1">0.208</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_r ltx_border_t" id="S4.T2.1.6.6.5" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.5.1">
<span class="ltx_p" id="S4.T2.1.6.6.5.1.1">0.323</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.6.6.6" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.6.1">
<span class="ltx_p" id="S4.T2.1.6.6.6.1.1">0.47</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.6.6.7" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.7.1">
<span class="ltx_p" id="S4.T2.1.6.6.7.1.1">0.48</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S4.T2.1.6.6.8" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.6.6.8.1">
<span class="ltx_p" id="S4.T2.1.6.6.8.1.1">0.451</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.7">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T2.1.7.7.1" style="width:170.7pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.1.1">
<span class="ltx_p" id="S4.T2.1.7.7.1.1.1">Weighted-expert setup versus Unweighted setup</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S4.T2.1.7.7.2" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.2.1">
<span class="ltx_p" id="S4.T2.1.7.7.2.1.1">0.1035</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S4.T2.1.7.7.3" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.3.1">
<span class="ltx_p" id="S4.T2.1.7.7.3.1.1">0.145</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_r" id="S4.T2.1.7.7.4" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.4.1">
<span class="ltx_p" id="S4.T2.1.7.7.4.1.1">0.2275</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S4.T2.1.7.7.5" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.5.1">
<span class="ltx_p" id="S4.T2.1.7.7.5.1.1">0.0069**</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S4.T2.1.7.7.6" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.6.1">
<span class="ltx_p" id="S4.T2.1.7.7.6.1.1">0.0711</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" id="S4.T2.1.7.7.7" style="width:31.3pt;padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.7.7.7.1">
<span class="ltx_p" id="S4.T2.1.7.7.7.1.1">0.0288</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>P-values from t-tests showing differences in performance metrics across strategy setups, task conditions, and expertise. Significant differences in precision are observed between task conditions and strategy setups in both OD and segmentation ‚Äî distinguishing between sparse- versus dense-target tasks, between individual annotators versus groups, and between independent versus dependent annotation processes. Significant differences in recall were also noted in segmentation between individuals and groups.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Impact of Prior Experience</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The impact of prior experience in interpretation, digitization, and annotation of RS tasks on annotation performance (OBJ4) was examined in two ways: <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.1">First</span>, a comparison was made between the evaluation metrics of individual expert annotators (annotators with 22 months experience in average) versus those of annotators without prior experience (non-experts) (RQ7). <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.1.2">Second</span>, we investigated whether the results of group annotations were improved when the expert‚Äôs annotations were given a double weight in the majority vote decision compared with non-expert group members (RQ8).</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">A comparison of the performance of expert/non-expert individual annotators reveals that there are no significant differences between the two groups across all evaluation metrics, both in OD and segmentation tasks (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T2" title="TABLE II ‚Ä£ IV-C Performance Comparisons ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">II</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.F8" title="Figure 8 ‚Ä£ IV-D Impact of Prior Experience ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">8</span></a>). As observed in other scenarios, precision is significantly higher than recall for both experts and non-experts (P-values for the t-test of mean differences are 0.0235 and 0.0162, respectively).</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.F8" title="Figure 8 ‚Ä£ IV-D Impact of Prior Experience ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">8</span></a>, in the OD task, a double weighting assigned with expert annotators did not impact annotation performance and did not reduce error rates (no significant difference in the evaluation metrics‚Äîsee Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T2" title="TABLE II ‚Ä£ IV-C Performance Comparisons ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">II</span></a>). In the segmentation task, there is a significant difference in precision (0.10 in favor of the unweighted setup), with the weighted setup leading to lower performance, increasing Type I errors, indicating a tendency toward overestimation in target delineation.
In addition, notice that also in the weighted setup, a significant difference was observed between precision and recall (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.F9" title="Figure 9 ‚Ä£ IV-D Impact of Prior Experience ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">9</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T1" title="TABLE I ‚Ä£ IV-B Differences in Error Types ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">I</span></a>). However, in the OD task, precision scores are higher than recall (difference of 0.05), consistent with the other setups and conditions tested, whereas, in the segmentation task, precision scores are lower than recall (difference of 0.08) and point to the experts‚Äô propensity for target-size overestimation.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="508" id="S4.F8.g1" src="extracted/5871508/performance_exp.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Performance comparison for impact of expertise assessment: On the left - evaluation metrics of individual non-experts versus experts. No significant difference was found in annotation performance in either OD or segmentation tasks. On the right - evaluation metrics for the unweighted setup versus the weighted setup. A significant difference was found in precision, with the weighted setup showing lower performance compared to the unweighted setup.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="540" id="S4.F9.g1" src="extracted/5871508/p_r_exp.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Comparison of Precision and Recall Results between Experts and Non-Experts. Left panel: Results for individual non-experts and experts. Right panel: Expert-weighted setup vs. unweighted setup. In OD, there is a clear difference between the evaluation metrics, with precision being notably higher than recall. In segmentation, no significant differences were observed, except in the weighted setup, where precision is lower than recall, indicating a tendency toward over-estimation in target delineation.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Higher Performance in OD Compared with Segmentation</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">A significant difference was observed in the annotator‚Äôs performance with greater success in OD than in segmentation, both in terms of accuracy (precision) and completeness (recall). Lower performance in segmentation was reported also by studies examining annotation quality in standard ground photography. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib19" title="">19</a>]</cite> showed an average accuracy of 75% in the boundary accuracy of various objects from the COCO dataset (one of the largest image corpus for computer vision tasks training) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib6" title="">6</a>]</cite>. The higher average performance of 83% we observed could be attributed to the skill of the participants, who are geoinformatics students and may have an affinity towards RS tasks more than the general population has to a random ground photo.
The difference in performance in OD versus segmentation can be explained by the following: segmentation is a more complex task that requires higher cognitive demands, which inevitably results in lower performance. For example <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib60" title="">60</a>]</cite> found that in visual processing, OD involves a general pattern matching by the brain, while delineation requires detailed and precise boundary trace, which may increase cognitive load. Note that segmentation errors indicate the overall pixels that were over- and underestimated, and are not necessarily related to the number of detected objects. Note that the dataset panel size histogram is normally distributed.
Another independent reason can be the fact that segmentation requires different skills than OD, which are rooted in a different type of intelligence than demanded for OD. As <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib61" title="">61</a>]</cite> distinguishes, in his seminal work on types and sub-types of intelligence, Spatial Intelligence-primarily required for OD-is closely related but a distinct concept from Visual-Perceptual Intelligence required for accurately delineating detailed polygons. Given the representative sample in our experiment (Kolmogorov-Smirnov Test, p-value&gt;0.05), it is possible that prevalence of individuals with the first type of intelligence is higher than those with the second type.
To the best of our knowledge, there are no existing RS studies that compare the performance between the two key computer vision tasks ‚Äî OD and segmentation ‚Äî even though human annotators in RS tasks are usually required to perform both simultaneously. The importance of this finding lies in understanding the need to coordinate between the annotator‚Äôs personal skills and the task requirements. By aligning the right task‚Äôs requirements, we can achieve faster, more accurate, and higher-quality results. It could also be useful to conduct an initial evaluation of the potential annotators for RS tasks using validated tests that assess the level of spatial intelligence, such as <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">Raven‚Äôs Progressive Matrices</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib62" title="">62</a>]</cite> or <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">Mental Rotation Test</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib63" title="">63</a>]</cite> for batter matching between the annotator‚Äôs competencies and the task.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Alongside, the emergence of large segmentation models, such as <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">Segment Anything Model</span> by Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib1" title="">1</a>]</cite>, which allow defining user prompts such as raw masks, creates the possibility for using lower-quality annotation by humans that can be improved by machine segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib64" title="">64</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Consistent Bias in Error Types</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We found a pronounced tendency by the participants to commit more Type II errors (False Negatives) than Type I errors (False Positives). This finding, reported for the first time for RS tasks, observed across all experimental setups and task conditions, as reflected in the significant difference between precision and recall, clearly favoring the former (see Table <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S4.T1" title="TABLE I ‚Ä£ IV-B Differences in Error Types ‚Ä£ IV Results ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">I</span></a>). This indicates that human annotators perform better in correctly identifying objects or delineating their boundaries (with very few active errors in identifying a non-object as an object), but they are more prone to miss objects of interest. This bias aligns with the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">Prospect theory</span>, formulated by Nobel laureate Kahneman and his co-author Tversky <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib65" title="">65</a>]</cite>. The prospect theory suggests, among other assertions, that humans are biased to prevent losses more than they strive to achieve gains. This bias has been coined by Kahneman and Tversky as the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">Loss Aversion bias</span> and was validated by numerous empirical studies in the behavioral sciences since the 1980s (for example <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib66" title="">66</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib67" title="">67</a>]</cite>). More detailed, the cognitive tendency in decision-making under uncertainty, where aversion to losses outweighs the appeal of equivalent gains, may lead individuals to prioritize avoiding losses over potential rewards, as losses are perceived as more significant and distressing than gains are satisfying. The value function v(x), that dictates the aversion bias, as described by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib65" title="">65</a>]</cite>, is defined as follows:</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S5.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="v(x)=\begin{cases}x^{\alpha}&amp;\text{if }x\geq 0,\\
-\lambda(-x)^{\alpha}&amp;\text{if }x&lt;0,\end{cases}" class="ltx_Math" display="block" id="S5.Ex1.m1.5"><semantics id="S5.Ex1.m1.5a"><mrow id="S5.Ex1.m1.5.6" xref="S5.Ex1.m1.5.6.cmml"><mrow id="S5.Ex1.m1.5.6.2" xref="S5.Ex1.m1.5.6.2.cmml"><mi id="S5.Ex1.m1.5.6.2.2" xref="S5.Ex1.m1.5.6.2.2.cmml">v</mi><mo id="S5.Ex1.m1.5.6.2.1" xref="S5.Ex1.m1.5.6.2.1.cmml">‚Å¢</mo><mrow id="S5.Ex1.m1.5.6.2.3.2" xref="S5.Ex1.m1.5.6.2.cmml"><mo id="S5.Ex1.m1.5.6.2.3.2.1" stretchy="false" xref="S5.Ex1.m1.5.6.2.cmml">(</mo><mi id="S5.Ex1.m1.5.5" xref="S5.Ex1.m1.5.5.cmml">x</mi><mo id="S5.Ex1.m1.5.6.2.3.2.2" stretchy="false" xref="S5.Ex1.m1.5.6.2.cmml">)</mo></mrow></mrow><mo id="S5.Ex1.m1.5.6.1" xref="S5.Ex1.m1.5.6.1.cmml">=</mo><mrow id="S5.Ex1.m1.4.4" xref="S5.Ex1.m1.5.6.3.1.cmml"><mo id="S5.Ex1.m1.4.4.5" xref="S5.Ex1.m1.5.6.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S5.Ex1.m1.4.4.4" rowspacing="0pt" xref="S5.Ex1.m1.5.6.3.1.cmml"><mtr id="S5.Ex1.m1.4.4.4a" xref="S5.Ex1.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.Ex1.m1.4.4.4b" xref="S5.Ex1.m1.5.6.3.1.cmml"><msup id="S5.Ex1.m1.1.1.1.1.1.1" xref="S5.Ex1.m1.1.1.1.1.1.1.cmml"><mi id="S5.Ex1.m1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S5.Ex1.m1.1.1.1.1.1.1.3" xref="S5.Ex1.m1.1.1.1.1.1.1.3.cmml">Œ±</mi></msup></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.Ex1.m1.4.4.4c" xref="S5.Ex1.m1.5.6.3.1.cmml"><mrow id="S5.Ex1.m1.2.2.2.2.2.1.1" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S5.Ex1.m1.2.2.2.2.2.1.1.1" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.cmml"><mtext id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.2" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.2a.cmml">if¬†</mtext><mo id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.1" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.1.cmml">‚Å¢</mo><mi id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.3" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.3.cmml">x</mi></mrow><mo id="S5.Ex1.m1.2.2.2.2.2.1.1.1.1" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.1.cmml">‚â•</mo><mn id="S5.Ex1.m1.2.2.2.2.2.1.1.1.3" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.3.cmml">0</mn></mrow><mo id="S5.Ex1.m1.2.2.2.2.2.1.1.2" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.cmml">,</mo></mrow></mtd></mtr><mtr id="S5.Ex1.m1.4.4.4d" xref="S5.Ex1.m1.5.6.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S5.Ex1.m1.4.4.4e" xref="S5.Ex1.m1.5.6.3.1.cmml"><mrow id="S5.Ex1.m1.3.3.3.3.1.1" xref="S5.Ex1.m1.3.3.3.3.1.1.cmml"><mo id="S5.Ex1.m1.3.3.3.3.1.1a" xref="S5.Ex1.m1.3.3.3.3.1.1.cmml">‚àí</mo><mrow id="S5.Ex1.m1.3.3.3.3.1.1.1" xref="S5.Ex1.m1.3.3.3.3.1.1.1.cmml"><mi id="S5.Ex1.m1.3.3.3.3.1.1.1.3" xref="S5.Ex1.m1.3.3.3.3.1.1.1.3.cmml">Œª</mi><mo id="S5.Ex1.m1.3.3.3.3.1.1.1.2" xref="S5.Ex1.m1.3.3.3.3.1.1.1.2.cmml">‚Å¢</mo><msup id="S5.Ex1.m1.3.3.3.3.1.1.1.1" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.cmml"><mrow id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml"><mo id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml"><mo id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1a" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml">‚àí</mo><mi id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.2" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.2.cmml">x</mi></mrow><mo id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.Ex1.m1.3.3.3.3.1.1.1.1.3" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.3.cmml">Œ±</mi></msup></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S5.Ex1.m1.4.4.4f" xref="S5.Ex1.m1.5.6.3.1.cmml"><mrow id="S5.Ex1.m1.4.4.4.4.2.1.1" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S5.Ex1.m1.4.4.4.4.2.1.1.1" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.cmml"><mrow id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.cmml"><mtext id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.2" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.2a.cmml">if¬†</mtext><mo id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.1" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.1.cmml">‚Å¢</mo><mi id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.3" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.3.cmml">x</mi></mrow><mo id="S5.Ex1.m1.4.4.4.4.2.1.1.1.1" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.1.cmml">&lt;</mo><mn id="S5.Ex1.m1.4.4.4.4.2.1.1.1.3" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.3.cmml">0</mn></mrow><mo id="S5.Ex1.m1.4.4.4.4.2.1.1.2" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.cmml">,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.5b"><apply id="S5.Ex1.m1.5.6.cmml" xref="S5.Ex1.m1.5.6"><eq id="S5.Ex1.m1.5.6.1.cmml" xref="S5.Ex1.m1.5.6.1"></eq><apply id="S5.Ex1.m1.5.6.2.cmml" xref="S5.Ex1.m1.5.6.2"><times id="S5.Ex1.m1.5.6.2.1.cmml" xref="S5.Ex1.m1.5.6.2.1"></times><ci id="S5.Ex1.m1.5.6.2.2.cmml" xref="S5.Ex1.m1.5.6.2.2">ùë£</ci><ci id="S5.Ex1.m1.5.5.cmml" xref="S5.Ex1.m1.5.5">ùë•</ci></apply><apply id="S5.Ex1.m1.5.6.3.1.cmml" xref="S5.Ex1.m1.4.4"><csymbol cd="latexml" id="S5.Ex1.m1.5.6.3.1.1.cmml" xref="S5.Ex1.m1.4.4.5">cases</csymbol><apply id="S5.Ex1.m1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S5.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.2">ùë•</ci><ci id="S5.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.1.1.1.1.3">ùõº</ci></apply><apply id="S5.Ex1.m1.2.2.2.2.2.1.1.1.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1"><geq id="S5.Ex1.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.1"></geq><apply id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2"><times id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.1.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.1"></times><ci id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.2a.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.2"><mtext id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.2.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.2">if¬†</mtext></ci><ci id="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.3.cmml" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.2.3">ùë•</ci></apply><cn id="S5.Ex1.m1.2.2.2.2.2.1.1.1.3.cmml" type="integer" xref="S5.Ex1.m1.2.2.2.2.2.1.1.1.3">0</cn></apply><apply id="S5.Ex1.m1.3.3.3.3.1.1.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1"><minus id="S5.Ex1.m1.3.3.3.3.1.1.2.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1"></minus><apply id="S5.Ex1.m1.3.3.3.3.1.1.1.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1"><times id="S5.Ex1.m1.3.3.3.3.1.1.1.2.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.2"></times><ci id="S5.Ex1.m1.3.3.3.3.1.1.1.3.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.3">ùúÜ</ci><apply id="S5.Ex1.m1.3.3.3.3.1.1.1.1.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S5.Ex1.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1">superscript</csymbol><apply id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1"><minus id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1"></minus><ci id="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.1.1.1.2">ùë•</ci></apply><ci id="S5.Ex1.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S5.Ex1.m1.3.3.3.3.1.1.1.1.3">ùõº</ci></apply></apply></apply><apply id="S5.Ex1.m1.4.4.4.4.2.1.1.1.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1"><lt id="S5.Ex1.m1.4.4.4.4.2.1.1.1.1.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.1"></lt><apply id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2"><times id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.1.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.1"></times><ci id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.2a.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.2"><mtext id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.2.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.2">if¬†</mtext></ci><ci id="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.3.cmml" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.2.3">ùë•</ci></apply><cn id="S5.Ex1.m1.4.4.4.4.2.1.1.1.3.cmml" type="integer" xref="S5.Ex1.m1.4.4.4.4.2.1.1.1.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.5c">v(x)=\begin{cases}x^{\alpha}&amp;\text{if }x\geq 0,\\
-\lambda(-x)^{\alpha}&amp;\text{if }x&lt;0,\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S5.Ex1.m1.5d">italic_v ( italic_x ) = { start_ROW start_CELL italic_x start_POSTSUPERSCRIPT italic_Œ± end_POSTSUPERSCRIPT end_CELL start_CELL if italic_x ‚â• 0 , end_CELL end_ROW start_ROW start_CELL - italic_Œª ( - italic_x ) start_POSTSUPERSCRIPT italic_Œ± end_POSTSUPERSCRIPT end_CELL start_CELL if italic_x &lt; 0 , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">where:</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><math alttext="x" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><mi id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><ci id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1">ùë•</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">italic_x</annotation></semantics></math> represents the change in value (gain or loss).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.3"><math alttext="\alpha" class="ltx_Math" display="inline" id="S5.I1.i2.p1.1.m1.1"><semantics id="S5.I1.i2.p1.1.m1.1a"><mi id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">Œ±</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><ci id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1">ùõº</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.1.m1.1d">italic_Œ±</annotation></semantics></math> is a parameter typically in the range <math alttext="0&lt;\alpha\leq 1" class="ltx_Math" display="inline" id="S5.I1.i2.p1.2.m2.1"><semantics id="S5.I1.i2.p1.2.m2.1a"><mrow id="S5.I1.i2.p1.2.m2.1.1" xref="S5.I1.i2.p1.2.m2.1.1.cmml"><mn id="S5.I1.i2.p1.2.m2.1.1.2" xref="S5.I1.i2.p1.2.m2.1.1.2.cmml">0</mn><mo id="S5.I1.i2.p1.2.m2.1.1.3" xref="S5.I1.i2.p1.2.m2.1.1.3.cmml">&lt;</mo><mi id="S5.I1.i2.p1.2.m2.1.1.4" xref="S5.I1.i2.p1.2.m2.1.1.4.cmml">Œ±</mi><mo id="S5.I1.i2.p1.2.m2.1.1.5" xref="S5.I1.i2.p1.2.m2.1.1.5.cmml">‚â§</mo><mn id="S5.I1.i2.p1.2.m2.1.1.6" xref="S5.I1.i2.p1.2.m2.1.1.6.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.2.m2.1b"><apply id="S5.I1.i2.p1.2.m2.1.1.cmml" xref="S5.I1.i2.p1.2.m2.1.1"><and id="S5.I1.i2.p1.2.m2.1.1a.cmml" xref="S5.I1.i2.p1.2.m2.1.1"></and><apply id="S5.I1.i2.p1.2.m2.1.1b.cmml" xref="S5.I1.i2.p1.2.m2.1.1"><lt id="S5.I1.i2.p1.2.m2.1.1.3.cmml" xref="S5.I1.i2.p1.2.m2.1.1.3"></lt><cn id="S5.I1.i2.p1.2.m2.1.1.2.cmml" type="integer" xref="S5.I1.i2.p1.2.m2.1.1.2">0</cn><ci id="S5.I1.i2.p1.2.m2.1.1.4.cmml" xref="S5.I1.i2.p1.2.m2.1.1.4">ùõº</ci></apply><apply id="S5.I1.i2.p1.2.m2.1.1c.cmml" xref="S5.I1.i2.p1.2.m2.1.1"><leq id="S5.I1.i2.p1.2.m2.1.1.5.cmml" xref="S5.I1.i2.p1.2.m2.1.1.5"></leq><share href="https://arxiv.org/html/2409.10272v2#S5.I1.i2.p1.2.m2.1.1.4.cmml" id="S5.I1.i2.p1.2.m2.1.1d.cmml" xref="S5.I1.i2.p1.2.m2.1.1"></share><cn id="S5.I1.i2.p1.2.m2.1.1.6.cmml" type="integer" xref="S5.I1.i2.p1.2.m2.1.1.6">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.2.m2.1c">0&lt;\alpha\leq 1</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.2.m2.1d">0 &lt; italic_Œ± ‚â§ 1</annotation></semantics></math>, reflecting sensitivity to gains and losses (commonly <math alttext="\alpha\approx 0.88" class="ltx_Math" display="inline" id="S5.I1.i2.p1.3.m3.1"><semantics id="S5.I1.i2.p1.3.m3.1a"><mrow id="S5.I1.i2.p1.3.m3.1.1" xref="S5.I1.i2.p1.3.m3.1.1.cmml"><mi id="S5.I1.i2.p1.3.m3.1.1.2" xref="S5.I1.i2.p1.3.m3.1.1.2.cmml">Œ±</mi><mo id="S5.I1.i2.p1.3.m3.1.1.1" xref="S5.I1.i2.p1.3.m3.1.1.1.cmml">‚âà</mo><mn id="S5.I1.i2.p1.3.m3.1.1.3" xref="S5.I1.i2.p1.3.m3.1.1.3.cmml">0.88</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.3.m3.1b"><apply id="S5.I1.i2.p1.3.m3.1.1.cmml" xref="S5.I1.i2.p1.3.m3.1.1"><approx id="S5.I1.i2.p1.3.m3.1.1.1.cmml" xref="S5.I1.i2.p1.3.m3.1.1.1"></approx><ci id="S5.I1.i2.p1.3.m3.1.1.2.cmml" xref="S5.I1.i2.p1.3.m3.1.1.2">ùõº</ci><cn id="S5.I1.i2.p1.3.m3.1.1.3.cmml" type="float" xref="S5.I1.i2.p1.3.m3.1.1.3">0.88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.3.m3.1c">\alpha\approx 0.88</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.3.m3.1d">italic_Œ± ‚âà 0.88</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><math alttext="\lambda" class="ltx_Math" display="inline" id="S5.I1.i3.p1.1.m1.1"><semantics id="S5.I1.i3.p1.1.m1.1a"><mi id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><ci id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.p1.1.m1.1d">italic_Œª</annotation></semantics></math> is the loss aversion coefficient</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.3">The upper part of the expression refers to gains evaluation, and the bottom shows the evaluation of losses. The loss aversion coefficient (<math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS2.p5.1.m1.1"><semantics id="S5.SS2.p5.1.m1.1a"><mi id="S5.SS2.p5.1.m1.1.1" xref="S5.SS2.p5.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.1.m1.1b"><ci id="S5.SS2.p5.1.m1.1.1.cmml" xref="S5.SS2.p5.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p5.1.m1.1d">italic_Œª</annotation></semantics></math>) captures the degree of loss aversion, with <math alttext="\lambda&gt;1" class="ltx_Math" display="inline" id="S5.SS2.p5.2.m2.1"><semantics id="S5.SS2.p5.2.m2.1a"><mrow id="S5.SS2.p5.2.m2.1.1" xref="S5.SS2.p5.2.m2.1.1.cmml"><mi id="S5.SS2.p5.2.m2.1.1.2" xref="S5.SS2.p5.2.m2.1.1.2.cmml">Œª</mi><mo id="S5.SS2.p5.2.m2.1.1.1" xref="S5.SS2.p5.2.m2.1.1.1.cmml">&gt;</mo><mn id="S5.SS2.p5.2.m2.1.1.3" xref="S5.SS2.p5.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.2.m2.1b"><apply id="S5.SS2.p5.2.m2.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1"><gt id="S5.SS2.p5.2.m2.1.1.1.cmml" xref="S5.SS2.p5.2.m2.1.1.1"></gt><ci id="S5.SS2.p5.2.m2.1.1.2.cmml" xref="S5.SS2.p5.2.m2.1.1.2">ùúÜ</ci><cn id="S5.SS2.p5.2.m2.1.1.3.cmml" type="integer" xref="S5.SS2.p5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.2.m2.1c">\lambda&gt;1</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p5.2.m2.1d">italic_Œª &gt; 1</annotation></semantics></math> indicating that losses ‚Äùhurt‚Äù more than equivalent gains feel good. Empirical studies often find <math alttext="\lambda" class="ltx_Math" display="inline" id="S5.SS2.p5.3.m3.1"><semantics id="S5.SS2.p5.3.m3.1a"><mi id="S5.SS2.p5.3.m3.1.1" xref="S5.SS2.p5.3.m3.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p5.3.m3.1b"><ci id="S5.SS2.p5.3.m3.1.1.cmml" xref="S5.SS2.p5.3.m3.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p5.3.m3.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p5.3.m3.1d">italic_Œª</annotation></semantics></math> values between 2 and 2.5 (see, for example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib68" title="">68</a>]</cite>).</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1">In the case of using data created by human annotators in RS, the loss aversion bias suggests that annotators are likely to refrain from marking an object they are uncertain about, thus risking a loss (missing a true object), rather than marking it and gaining an additional correct identification. This assertion aligns with our data on confidence levels in the correctness of identification reported by the participants during the experiment (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S5.F10" title="Figure 10 ‚Ä£ V-B Consistent Bias in Error Types ‚Ä£ V Discussion ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">10</span></a>). The number of objects marked with high confidence is five-fold higher than those marked with medium or low confidence and accounts for 84% of all labeled objects. This observation indicates a pronounced tendency to favor objects that the annotators are confident that they are indeed solar panels.</p>
</div>
<div class="ltx_para" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1">In the segmentation task, significant differences in errors were found only in three cases, all in group setups: the dependent process; the independent process; and the expert-weighted. The reason for that may be because the cognitive mechanisms required for segmentation are fundamentally different than in OD, as was explained earlier in the Discussion Section. In OD, the participant considers the object as a generalized entity and inquires whether it is a panel or not. In the level of an object, the loss aversion bias is pertinent. In segmentation task, however, other skills of the participant are involved, such as exactness, ability to discern color variations, and comprehensive visual perception of the object. Therefore, an underestimation in delineation, that yields an exclusion of pixels, may not be caused by lost aversion. This may also be the reason why, in the dependent setup, the difference in errors flips: there is more overestimation than underestimation. This flip may reflect a tendency not to discern the boundaries of an object and to include shadows or parts of adjacent similar objects.</p>
</div>
<div class="ltx_para" id="S5.SS2.p8">
<p class="ltx_p" id="S5.SS2.p8.1">We suggest balancing this inherent bias observed here by designing an annotation setup that provides positive incentives for taking risks and marking objects with lower confidence levels in their validity. Another option is to guide the annotators to mark objects with medium or even low certainty, and these objects can be reevaluated in a subsequent stage by others. Such frameworks could reduce underestimation and lead to overall improved performance.</p>
</div>
<figure class="ltx_figure" id="S5.F10">
<p class="ltx_p ltx_align_center" id="S5.F10.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="S5.F10.1.1" style="border-color: #000000;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="577" id="S5.F10.1.1.g1" src="extracted/5871508/confidence.jpg" width="509"/></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Percentage of detected objects by reported confidence level: annotators were asked to rate the detected objects into three confidence levels indicating their certainty in object detected as a solar panel. Note that the vast majority of detected objects (84%) were rated with high confidence level, while only 6% on average were rated with a low confidence level.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Annotator‚Äôs Performance Across Strategies and Conditions</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS1.5.1.1">V-C</span>1 </span>Majority Vote Outperforms Reviewing Process</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">We compared two annotation setups (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#S3.F2" title="Figure 2 ‚Ä£ III-B3 Prior Experience ‚Ä£ III-B Experimental Setup ‚Ä£ III Methods ‚Ä£ Performance of Human Annotators in Object Detection and Segmentation of Remotely Sensed Data"><span class="ltx_text ltx_ref_tag">2</span></a>: (1) individual and group-based annotations; and (2) an independent group setup, where the final annotation is determined by a majority vote of independent annotators, and a dependent group setup, where a single annotator is followed by a sequential double control procedure by other annotators.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1">We found that the group setup outperformed the individual setup (reduction of 4% and 1% in Type I errors in the group setup in OD task and segmentation task, respectively), and the independent setup achieved significantly higher results than the dependent setup, most remarkably in the segmentation task (reduction of 3% and 12% in Type I errors in the group setup in OD task and segmentation task, respectively).
While it is not surprising that teams outperform individuals, regarding the group setup, our findings align with Occam‚Äôs Razor, suggesting that the simple, straightforward majority vote strategy, harnessing the wisdom of the crowd, is more effective in achieving accurate annotations than the process carried out under double control procedure.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p3">
<p class="ltx_p" id="S5.SS3.SSS1.p3.1">The increase in Type I errors in the dependent setup can be explained by a tendency towards overestimation by reviewers. The reviewer‚Äôs task differs from that of the annotator because they receive a rather high-quality product (F1 score of 0.89 in OD task and 0.79 in segmentation for individual annotators) that they need to refine. However, the reviewer may be biased by the previous annotation, influencing their decision-making. For example, in an effort to justify their role, the reviewer may tend to mark objects they are less confident about, which could potentially lead to increased overestimation in the final annotations. The reviewer may also hesitate to remove labels made by a previous annotator.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p4">
<p class="ltx_p" id="S5.SS3.SSS1.p4.1">Based on these findings, we recommend prioritizing a team setup of independent annotators who perform the same task in parallel. Increasing the number of annotators in the team is likely to enhance the reliability of the final outcome.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p5">
<p class="ltx_p" id="S5.SS3.SSS1.p5.1">It is important to note that no difference was observed in Type II errors between the group setups, namely, the independent process did not succeed in reducing the rate of Type II errors compared with the dependent process. This is likely because even a group of annotators fails to identify obscured objects that are difficult to detect due to low contrast, shading, resemblance, or close adjacency to other objects, etc.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS2.5.1.1">V-C</span>2 </span>The effect of imbalanced data</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">We examined the impact of the target-background ratio on the performance of the annotators. In both OD and segmentation, The results indicate significantly higher performance in more balanced tasks, with target spread over a smaller area (higher target-background ratio).</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1">We suggest the following explanations for the observed differences in performance. First, annotating large regions with sparsely distributed targets may reduce performance by increasing the search time across vast, often monotonous areas, which can be tiring for annotators, and lead to missed detection (false negative). Conversely, searching through extensive background regions can increase the statistical likelihood of false positives. Our results support the latter explanation, as they showed that in the imbalanced conditions, the rate of Type I errors was higher by 3.5% compared with the more balanced task. Lastly, in more balanced tasks, where multiple instances of the same entities are visible within a small and condensed space, annotators can become more familiar with the objects and make comparisons within the same field of view, thereby improving their ability to use the surrounding context and correctly identify targets.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p3">
<p class="ltx_p" id="S5.SS3.SSS2.p3.1">Based on our findings - for tasks with a low target-background ratio, we recommend dividing the space into smaller, more manageable tasks, which can enhance annotation performance by increasing the target-background ratio.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">No Advantage in Performance for Experts</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We examined whether there are performance differences between annotators without specific annotation training and those with an average of 22 experience months in RS image interpretation and annotation. In both setups ‚Äî comparing individual experts to non-experts and giving experts double weight in the majority vote strategy ‚Äî no significant differences were found between the annotator‚Äôs performances, and additionally they both have more type II errors than type I.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Moreover, in the segmentation task, assigning double weight to experts even increased the overall overestimation. This observation can be explained by overconfidence among experts, which refers to their excessive confidence in their ability to detect subtle features. Consequently, they may identify and segment, in our case, more areas as part of the object than necessary, ultimately resulting in over-segmentation. This tendency of experts toward overconfidence has been validated in numerous behavioral sciences studies investigating decision-making in uncertainty, including those of <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib69" title="">69</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib70" title="">70</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10272v2#bib.bib71" title="">71</a>]</cite>, which explore cognitive biases among experts. Lower performances among experts could be also attributed to their tendency to perform these tasks more automatically due to their broad span of practice. This automation can lead to a devotion of less attention or conscious thought to the task, potentially impacting the accuracy of the annotation.
We found no advantage in preferring expert annotators over non-expert annotators in RS tasks of object detection and segmentation. Comprehensive training and familiarity with aerial imagery and basic digitization tools are sufficient to yield high performance in these tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study presents an experiment evaluating the performance of human annotators in RS segmentation and OD tasks, with a particular focus on examining differences in error types across various strategy setups and task conditions. The results indicate that human annotators generally perform better in OD than in segmentation tasks, with a pronounced tendency to commit more Type II errors (False Negatives) than Type I errors (False Positives) across all experimental setups and task conditions. This finding suggests a stronger tendency toward under- rather than over-estimation. This trend is evident in OD but is less pronounced in segmentation, possibly due to the differing cognitive demands of the two tasks. Annotators‚Äô accuracy in correctly identifying objects is higher in a setup involving majority voting (independent process) rather than setup integrating double-check review (dependent process) as well as in group setups compared with individual setups, and in tasks where objects are closely grouped together (dense-target task), as opposed to tasks where objects are dispersed over a wide area (sparse-target task). However, there is a minor difference in the <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">complete</span> identification of all actual objects across various setups and task conditions, indicating a similar proportion of Type II errors. Additionally, in our case, experts were not found to improve the quality of annotations, and assigning double weight to annotators with higher expertise does not enhance performance in either correct identification or complete detection of actual objects. In segmentation tasks, weighting expert contributions even increased the number of mapped pixels, leading to an overestimation of the target boundary.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank The Israel Science Foundation (ISF), grant number 299/23, for supporting the project. We thank the annotators who participated in the project and the ethics committee for reviewing and approving the experiment.
Special thanks are extended to Oded Rotem from the Department of Software and Information Systems Engineering, whose insightful analysis of the YOLO model‚Äôs errors inspired us to examine the performance of human annotators.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A.¬†Kirillov, E.¬†Mintun, N.¬†Ravi, H.¬†Mao, C.¬†Rolland, L.¬†Gustafson, T.¬†Xiao, S.¬†Whitehead, A.¬†C. Berg, W.-Y. Lo, P.¬†Doll√°r, and R.¬†Girshick, ‚ÄúSegment anything,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">2023 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023, pp. 3992‚Äì4003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H.¬†Chen, J.¬†Song, and N.¬†Yokoya, ‚ÄúChange detection between optical remote sensing imagery and map data via segment anything model (sam),‚Äù 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.09019" title="">https://arxiv.org/abs/2401.09019</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z.¬†Zhou, J.¬†Shin, L.¬†Zhang, S.¬†Gurudu, M.¬†Gotway, and J.¬†Liang, ‚ÄúFine-tuning convolutional neural networks for biomedical image analysis: Actively and incrementally,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017, pp. 4761‚Äì4772.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H.¬†Huang, K.¬†Hild, Y.¬†Wang, M.¬†K√∂rner, and H.¬†Mayer, ‚ÄúData on demand: Automatic generation of customized datasets for the training of building detection in remote sensing imagery,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2024 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</em>.¬†¬†¬†IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.¬†Sun, A.¬†Shrivastava, S.¬†Singh, and A.¬†Gupta, ‚ÄúRevisiting unreasonable effectiveness of data in deep learning era,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2017 IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp. 843‚Äì852.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M.¬†Maire, S.¬†Belongie, J.¬†Hays, P.¬†Perona, D.¬†Ramanan, P.¬†Doll√°r, and C.¬†L. Zitnick, ‚ÄúMicrosoft coco: Common objects in context,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>.¬†¬†¬†Springer, 2014, pp. 740‚Äì755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P.¬†Stenetorp, S.¬†Pyysalo, G.¬†Topiƒá, T.¬†Ohta, S.¬†Ananiadou, and J.¬†Tsujii, ‚ÄúBrat: a web-based tool for nlp-assisted text annotation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics</em>, 2012, pp. 102‚Äì107.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K.¬†Sharifani and M.¬†Amini, ‚ÄúMachine learning and deep learning: A review of methods and applications,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">World Information Technology and Engineering Journal</em>, vol.¬†10, no.¬†07, pp. 3897‚Äì3904, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L.¬†Aroyo and C.¬†Welty, ‚ÄúTruth is a lie: Crowd truth and the seven myths of human annotation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">AI Magazine</em>, vol.¬†36, no.¬†1, pp. 15‚Äì24, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
J.¬†Lu, W.¬†Li, Q.¬†Wang, and Y.¬†Zhang, ‚ÄúResearch on data quality control of crowdsourcing annotation: A survey,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)</em>.¬†¬†¬†IEEE, 2020, pp. 201‚Äì208.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
G.¬†M. Foody, ‚ÄúGround truth in classification accuracy assessment: Myth and reality,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Geomatics</em>, vol.¬†4, no.¬†1, pp. 81‚Äì90, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2673-7418/4/1/5" title="">https://www.mdpi.com/2673-7418/4/1/5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúChallenges in the real world use of classification accuracy metrics: From recall and precision to the matthews correlation coefficient,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">PLOS ONE</em>, vol.¬†18, no.¬†10, pp. 1‚Äì27, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1371/journal.pone.0291908" title="">https://doi.org/10.1371/journal.pone.0291908</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J.¬†K. Jadhav and R.¬†P. Singh, ‚ÄúAutomatic semantic segmentation and classification of remote sensing data for agriculture,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Mathematical Models in Engineering</em>, vol.¬†4, no.¬†2, pp. 112‚Äì137, jun 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.21595/mme.2018.19840" title="">https://doi.org/10.21595/mme.2018.19840</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
F.¬†Fang, X.¬†Yuan, L.¬†Wang, Y.¬†Liu, and Z.¬†Luo, ‚ÄúUrban land-use classification from photographs,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">IEEE Geoscience and Remote Sensing Letters</em>, vol.¬†15, no.¬†12, pp. 1927‚Äì1931, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
X.¬†Yuan and V.¬†Sarma, ‚ÄúAutomatic urban water-body detection and segmentation from sparse alsm data via spatially constrained model-driven clustering,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE Geoscience and Remote Sensing Letters</em>, vol.¬†8, no.¬†1, pp. 73‚Äì77, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y.¬†Mo, Y.¬†Wu, X.¬†Yang, F.¬†Liu, and Y.¬†Liao, ‚ÄúReview the state-of-the-art technologies of semantic segmentation based on deep learning,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Neurocomputing</em>, vol. 493, pp. 626‚Äì646, 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0925231222000054" title="">https://www.sciencedirect.com/science/article/pii/S0925231222000054</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X.¬†Yuan, J.¬†Shi, and L.¬†Gu, ‚ÄúA review of deep learning methods for semantic segmentation of remote sensing imagery,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Expert Systems with Applications</em>, vol. 169, p. 114417, 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0957417420310836" title="">https://www.sciencedirect.com/science/article/pii/S0957417420310836</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
G.-S. Xia, Z.¬†Wang, C.¬†Xiong, and L.¬†Zhang, ‚ÄúAccurate annotation of remote sensing images via active spectral clustering with little expert knowledge,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Remote Sensing</em>, vol.¬†7, no.¬†11, pp. 15‚Äâ014‚Äì15‚Äâ045, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R.¬†Benenson, S.¬†Popov, and V.¬†Ferrari, ‚ÄúLarge-scale interactive object segmentation with human annotators,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
J.¬†Lu, W.¬†Li, Q.¬†Wang, and Y.¬†Zhang, ‚ÄúResearch on data quality control of crowdsourcing annotation: A survey,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">2020 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)</em>, 2020, pp. 201‚Äì208.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L.¬†Zhang, R.¬†Tanno, M.¬†Xu, Y.¬†Huang, K.¬†Bronik, C.¬†Jin, J.¬†Jacob, Y.¬†Zheng, L.¬†Shao, O.¬†Ciccarelli <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et¬†al.</em>, ‚ÄúLearning from multiple annotators for medical image segmentation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">Pattern Recognition</em>, vol. 138, p. 109400, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B.¬†Hobley, M.¬†Mackiewicz, J.¬†Bremner, T.¬†Dolphin, and R.¬†Arosio, ‚ÄúCrowdsourcing experiment and fully convolutional neural networks for coastal remote sensing of seagrass and macro-algae,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C.¬†Qiu, A.¬†Squicciarini, D.¬†R. Khare, B.¬†Carminati, and J.¬†Caverlee, ‚ÄúCrowdeval: A cost-efficient strategy to evaluate crowdsourced worker‚Äôs reliability,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems</em>, 2018, pp. 1486‚Äì1494.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
E.¬†Saralioglu and O.¬†Gungor, ‚ÄúCrowdsourcing in remote sensing: A review of applications and future directions,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE Geoscience and Remote Sensing Magazine</em>, vol.¬†8, no.¬†4, pp. 89‚Äì110, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H.¬†Li, X.¬†Dou, C.¬†Tao, Z.¬†Wu, J.¬†Chen, J.¬†Peng, M.¬†Deng, and L.¬†Zhao, ‚ÄúRsi-cb: A large-scale remote sensing image classification benchmark using crowdsourced data,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Sensors</em>, vol.¬†20, no.¬†6, 2020. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1424-8220/20/6/1594" title="">https://www.mdpi.com/1424-8220/20/6/1594</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
X.¬†Wang, L.¬†Chen, T.¬†Ban, D.¬†Lyu, Y.¬†Guan, X.¬†Wu, X.¬†Zhou, and H.¬†Chen, ‚ÄúAccurate label refinement from multiannotator of remote sensing data,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol.¬†61, pp. 1‚Äì13, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
B.¬†C. Benato, J.¬†F. Gomes, A.¬†C. Telea, and A.¬†X. Falc√£o, ‚ÄúSemi-automatic data annotation guided by feature space projection,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Pattern Recognition</em>, vol. 109, p. 107612, 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0031320320304155" title="">https://www.sciencedirect.com/science/article/pii/S0031320320304155</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J.¬†Li, L.¬†Meng, B.¬†Yang, C.¬†Tao, L.¬†Li, and W.¬†Zhang, ‚ÄúLabelrs: An automated toolbox to make deep learning samples from remote sensing images,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Remote Sensing</em>, vol.¬†13, no.¬†11, 2021. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2072-4292/13/11/2064" title="">https://www.mdpi.com/2072-4292/13/11/2064</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
T.¬†Yamada, N.¬†Yokoya, T.¬†Tadono, and A.¬†Iwasaki, ‚ÄúLand cover mapping without human annotation,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium</em>, 2019, pp. 5956‚Äì5959.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Y.¬†Hua, D.¬†Marcos, L.¬†Mou, X.¬†X. Zhu, and D.¬†Tuia, ‚ÄúSemantic segmentation of remote sensing images with sparse annotations,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE Geoscience and Remote Sensing Letters</em>, vol.¬†19, pp. 1‚Äì5, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M.¬†Ivasic-Kos, I.¬†Ipsic, and S.¬†Ribaric, ‚ÄúA knowledge-based multi-layered image annotation system,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Expert Systems with Applications</em>, vol.¬†42, no.¬†24, pp. 9539‚Äì9553, 2015. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S095741741500528X" title="">https://www.sciencedirect.com/science/article/pii/S095741741500528X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
D.¬†Gr√ºhn and S.¬†Scheibe, ‚ÄúAge-related differences in valence and arousal ratings of pictures from the international affective picture system (iaps): Do ratings become more extreme with age?‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Behavior research methods</em>, vol.¬†40, pp. 512‚Äì521, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
J.¬†Chen, D.¬†Wang, I.¬†Xie, and Q.¬†Lu, ‚ÄúImage annotation tactics: transitions, strategies and efficiency,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Information Processing &amp; Management</em>, vol.¬†54, no.¬†6, pp. 985‚Äì1001, 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0306457318300451" title="">https://www.sciencedirect.com/science/article/pii/S0306457318300451</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J.¬†Bragg, Mausam, and D.¬†S. Weld, ‚ÄúOptimal testing for crowd workers,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems</em>, 2016, pp. 966‚Äì974.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
C.¬†G. Northcutt, A.¬†Athalye, and J.¬†Mueller, ‚ÄúPervasive label errors in test sets destabilize machine learning benchmarks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2103.14749</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y.¬†Wang, C.¬†Li, X.¬†Liu, H.¬†Li, Z.¬†Yao, and Y.¬†Zhao, ‚ÄúHow well do the volunteers label land cover types in manual interpretation of remote sensing imagery?‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">International Journal of Digital Earth</em>, vol.¬†17, no.¬†1, p. 2347443, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z.¬†Zhou, C.¬†Zheng, X.¬†Liu, Y.¬†Tian, X.¬†Chen, X.¬†Chen, and Z.¬†Dong, ‚ÄúA dynamic effective class balanced approach for remote sensing imagery semantic segmentation of imbalanced data,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Remote Sensing</em>, vol.¬†15, no.¬†7, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2072-4292/15/7/1768" title="">https://www.mdpi.com/2072-4292/15/7/1768</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
G.-S. Xia, Z.¬†Wang, C.¬†Xiong, and L.¬†Zhang, ‚ÄúAccurate annotation of remote sensing images via active spectral clustering with little expert knowledge,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Remote Sensing</em>, vol.¬†7, no.¬†11, pp. 15‚Äâ014‚Äì15‚Äâ045, 2015. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/2072-4292/7/11/15014" title="">https://www.mdpi.com/2072-4292/7/11/15014</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
V.¬†C. Raykar, S.¬†Yu, L.¬†H. Zhao, G.¬†H. Valadez, C.¬†Florin, L.¬†Bogoni, and L.¬†Moy, ‚ÄúLearning from crowds.‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Journal of machine learning research</em>, vol.¬†11, no.¬†4, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A.¬†M. Davani, M.¬†D√≠az, and V.¬†Prabhakaran, ‚ÄúDealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Transactions of the Association for Computational Linguistics</em>, vol.¬†10, pp. 92‚Äì110, 01 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00449" title="">https://doi.org/10.1162/tacl_a_00449</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
L.¬†Bruzzone and S.¬†Serpico, ‚ÄúClassification of imbalanced remote-sensing data by neural networks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Pattern Recognition Letters</em>, vol.¬†18, no.¬†11, pp. 1323‚Äì1328, 1997. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0167865597001098" title="">https://www.sciencedirect.com/science/article/pii/S0167865597001098</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M.¬†Buda, A.¬†Maki, and M.¬†A. Mazurowski, ‚ÄúA systematic study of the class imbalance problem in convolutional neural networks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Neural networks</em>, vol. 106, pp. 249‚Äì259, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
J.¬†M. Johnson and T.¬†M. Khoshgoftaar, ‚ÄúThe effects of data sampling with deep learning and highly imbalanced big data,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Information Systems Frontiers</em>, vol.¬†22, no.¬†5, pp. 1113‚Äì1131, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
I.¬†Valova, C.¬†Harris, T.¬†Mai, and N.¬†Gueorguieva, ‚ÄúOptimization of convolutional neural networks for imbalanced set classification,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Procedia Computer Science</em>, vol. 176, pp. 660‚Äì669, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Y.¬†Hua, D.¬†Marcos, L.¬†Mou, X.¬†X. Zhu, and D.¬†Tuia, ‚ÄúSemantic segmentation of remote sensing images with sparse annotations,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IEEE Geoscience and Remote Sensing Letters</em>, vol.¬†19, pp. 1‚Äì5, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
J.¬†M. Johnson and T.¬†M. Khoshgoftaar, ‚ÄúSurvey on deep learning with class imbalance,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Journal of Big Data</em>, vol.¬†6, no.¬†1, Mar. 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1186/s40537-019-0192-5" title="">http://dx.doi.org/10.1186/s40537-019-0192-5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
R.¬†A. Bauder and T.¬†M. Khoshgoftaar, ‚ÄúThe effects of varying class distribution on learner behavior for medicare fraud detection with imbalanced big data,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Health Information Science and Systems</em>, vol.¬†6, no.¬†1, Sep. 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s13755-018-0051-3" title="">http://dx.doi.org/10.1007/s13755-018-0051-3</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
K.¬†Ghosh, C.¬†Bellinger, R.¬†Corizzo, P.¬†Branco, B.¬†Krawczyk, and N.¬†Japkowicz, ‚ÄúThe class imbalance problem in deep learning,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Machine Learning</em>, vol. 113, no.¬†7, p. 4845√¢‚Ç¨‚Äú4901, Dec. 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/s10994-022-06268-8" title="">http://dx.doi.org/10.1007/s10994-022-06268-8</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
B.¬†Tversky, ‚ÄúDistortions in memory for maps,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Cognative Psycology</em>, vol.¬†13, pp. 407‚Äì433, 1981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
R.¬†Gifford, <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Environmental Psychology</em>.¬†¬†¬†Colville, WA: Optimal Books, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
B.¬†Tversky, ‚ÄúVisuospatial reasoning,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Handbook of reasoning</em>.¬†¬†¬†Cambridge University Press, 2005, pp. 209‚Äì249.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
E.¬†Kats, J.¬†Goldberger, and H.¬†Greenspan, ‚ÄúA soft staple algorithm combined with anatomical knowledge,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Medical Image Computing and Computer Assisted Intervention‚ÄìMICCAI 2019: 22nd International Conference, Shenzhen, China, October 13‚Äì17, 2019, Proceedings, Part III 22</em>.¬†¬†¬†Springer, 2019, pp. 510‚Äì517.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
A.¬†H. Nasution and A.¬†Onan, ‚ÄúChatgpt label: Comparing the quality of human-generated and llm-generated annotations in low-resource language nlp tasks,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">IEEE Access</em>, vol.¬†12, pp. 71‚Äâ876‚Äì71‚Äâ900, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
S.¬†K. Warfield, K.¬†H. Zou, and W.¬†M. Wells, ‚ÄúSimultaneous truth and performance level estimation (staple): An algorithm for the validation of image segmentation,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">IEEE Transactions on Medical Imaging</em>, vol.¬†23, no.¬†7, p. 903 ‚Äì 921, 2004, cited by: 1595; All Open Access, Green Open Access. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.scopus.com/inward/record.uri?eid=2-s2.0-1942438249&amp;doi=10.1109%2fTMI.2004.828354&amp;partnerID=40&amp;md5=bcae7f351cb416c830d46643f86fdf94" title="">https://www.scopus.com/inward/record.uri?eid=2-s2.0-1942438249&amp;doi=10.1109%2fTMI.2004.828354&amp;partnerID=40&amp;md5=bcae7f351cb416c830d46643f86fdf94</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Y.¬†Kim, E.¬†Lee, Y.¬†Lee, and U.¬†Oh, ‚ÄúUnderstanding novice‚Äôs annotation process for 3d semantic segmentation task with human-in-the-loop,‚Äù in <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 29th International Conference on Intelligent User Interfaces</em>, 2024, pp. 444‚Äì454.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
R.¬†Bluestein-Livnon, T.¬†Svoray, D.¬†Michael, and K.¬†Van Der¬†Beek, ‚ÄúEconomic aspects of urban greenness along a dryland rainfall gradient: A time-series analysis,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Urban Forestry &amp; Urban Greening</em>, vol.¬†83, p. 127915, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
F.¬†J. Valverde-Albacete and C.¬†Pel√°ez-Moreno, ‚Äú100% classification accuracy considered harmful: The normalized information transfer factor explains the accuracy paradox,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">PLOS ONE</em>, vol.¬†9, no.¬†1, pp. 1‚Äì10, 01 2014. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1371/journal.pone.0084217" title="">https://doi.org/10.1371/journal.pone.0084217</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
R Core Team, <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">R: A Language and Environment for Statistical Computing</em>, R Foundation for Statistical Computing, Vienna, Austria, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.R-project.org/" title="">https://www.R-project.org/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
E.¬†Pebesma, ‚ÄúSimple Features for R: Standardized Support for Spatial Vector Data,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">The R Journal</em>, vol.¬†10, no.¬†1, pp. 439‚Äì446, 2018. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.32614/RJ-2018-009" title="">https://doi.org/10.32614/RJ-2018-009</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
D.¬†Marr, <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Vision: A computational investigation into the human representation and processing of visual information</em>.¬†¬†¬†MIT press, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
H.¬†E. Gardner, <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Frames of mind: The theory of multiple intelligences</em>.¬†¬†¬†Basic books, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
J.¬†C. Raven and J.¬†H. Court, <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Raven‚Äôs progressive matrices and vocabulary scales</em>.¬†¬†¬†Oxford Psychologists Press Oxford, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
S.¬†G. Vandenberg and A.¬†R. Kuse, ‚ÄúMental rotations, a group test of three-dimensional spatial visualization,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Perceptual and motor skills</em>, vol.¬†47, no.¬†2, pp. 599‚Äì604, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
O.¬†Rafaeli, T.¬†Svoray, R.¬†Blushtein-Livnon, and A.¬†Nahlieli, ‚ÄúPrompt-based segmentation at multiple resolutions and lighting conditions using segment anything model 2,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2408.06970</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
D.¬†Kahneman and A.¬†Tversky, ‚ÄúProspect theory: An analysis of decision under risk,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Econometrica</em>, vol.¬†47, no.¬†2, pp. 263‚Äì291, 1979. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.jstor.org/stable/1914185" title="">http://www.jstor.org/stable/1914185</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M.¬†Abdellaoui, H.¬†Bleichrodt, and C.¬†Paraschiv, ‚ÄúLoss aversion under prospect theory: A parameter-free measurement,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Management Science</em>, vol.¬†53, no.¬†10, p. 1659√¢‚Ç¨‚Äú1674, Oct. 2007. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1287/mnsc.1070.0711" title="">http://dx.doi.org/10.1287/mnsc.1070.0711</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
A.¬†Tversky and D.¬†Kahneman, ‚ÄúLoss aversion in riskless choice: A reference-dependent model,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">The Quarterly Journal of Economics</em>, vol. 106, no.¬†4, pp. 1039‚Äì1061, 1991. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.jstor.org/stable/2937956" title="">http://www.jstor.org/stable/2937956</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúAdvances in prospect theory: Cumulative representation of uncertainty,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Journal of Risk and Uncertainty</em>, vol.¬†5, no.¬†4, p. 297√¢‚Ç¨‚Äú323, Oct. 1992. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/BF00122574" title="">http://dx.doi.org/10.1007/BF00122574</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
‚Äî‚Äî, ‚ÄúJudgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty.‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">science</em>, vol. 185, no. 4157, pp. 1124‚Äì1131, 1974.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J.¬†A. Sniezek and L.¬†M. Van¬†Swol, ‚ÄúTrust, confidence, and expertise in a judge-advisor system,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Organizational behavior and human decision processes</em>, vol.¬†84, no.¬†2, pp. 288‚Äì307, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
D.¬†Griffin and A.¬†Tversky, ‚ÄúThe weighing of evidence and the determinants of confidence,‚Äù <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Cognitive psychology</em>, vol.¬†24, no.¬†3, pp. 411‚Äì435, 1992.

</span>
</li>
</ul>
</section>
<figure class="ltx_float biography" id="id2">
<table class="ltx_tabular" id="id2.1">
<tr class="ltx_tr" id="id2.1.1">
<td class="ltx_td" id="id2.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="id2.1.1.1.g1" src="extracted/5871508/Roni.jpg" width="100"/></td>
<td class="ltx_td" id="id2.1.1.2">
<span class="ltx_inline-block" id="id2.1.1.2.1">
<span class="ltx_p" id="id2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="id2.1.1.2.1.1.1">Roni Blushtein-Livnon</span> 
is a Ph.D. student in the Department of Environmental, Geoinformatics, and Urban Planning Sciences at Ben-Gurion University of the Negev. Her doctoral research involves, among other things, Computer Vision and focuses on deep learning methods for object detection and mapping.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="id3">
<table class="ltx_tabular" id="id3.1">
<tr class="ltx_tr" id="id3.1.1">
<td class="ltx_td" id="id3.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="id3.1.1.1.g1" src="extracted/5871508/tal2.jpg" width="100"/></td>
<td class="ltx_td" id="id3.1.1.2">
<span class="ltx_inline-block" id="id3.1.1.2.1">
<span class="ltx_p" id="id3.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="id3.1.1.2.1.1.1">Tal Svoray</span> 
received a Ph.D. in Radar Remote Sensing of Mediterranean Vegetation from Bar Ilan University, Ramat-Gan, Israel, in 2001.
He is currently a Professor at the Ben-Gurion University of the Negev. His main research interests include object segmentation and detection, remote sensing of soil and vegetation, environmental psychology and geostatistics.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="id4">
<table class="ltx_tabular" id="id4.1">
<tr class="ltx_tr" id="id4.1.1">
<td class="ltx_td" id="id4.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="id4.1.1.1.g1" src="extracted/5871508/michael.jpg" width="97"/></td>
<td class="ltx_td" id="id4.1.1.2">
<span class="ltx_inline-block" id="id4.1.1.2.1">
<span class="ltx_p" id="id4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="id4.1.1.2.1.1.1">Michael Dorman,</span>  Ph.D., is a programmer and lecturer at The Department of Environmental, Geoinformatics and Urban Planning Sciences, Ben-Gurion University of the Negev. He is working with researchers and students to develop computational workflows for spatial analysis, mostly through programming in Python, R, and JavaScript, as well as teaching those participants.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Sep 22 15:41:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
