<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2002.09599] Training Question Answering Models From Synthetic Data</title><meta property="og:description" content="Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Question Answering Models From Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Question Answering Models From Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2002.09599">

<!--Generated on Sat Mar 16 17:59:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Machine Learning,  ICML">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Training Question Answering Models From Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raul Puri
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ryan Spring
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mostofa Patwary
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Shoeybi
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bryan Catanzaro
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the <span id="id1.id1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the <span id="id1.id1.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the <span id="id1.id1.3" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dev set. We further apply our methodology to <span id="id1.id1.4" class="ltx_text ltx_font_smallcaps">SQuAD2.0 </span>and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.T1" class="ltx_table">
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:239.6pt;height:198pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.7pt,19.6pt) scale(0.835,0.835) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Text</th>
<td id="S1.T1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.2.1.1" class="ltx_p" style="width:227.6pt;">Albert Einstein is known for his theories of special relativity and general relativity. He also made important contributions to statistical mechanics, especially his mathematical treatment of Brownian motion, his resolution of the paradox of specific heats, and his connection of fluctuations and dissipation. Despite his reservations about its interpretation, Einstein also made contributions to quantum mechanics and, indirectly, <span id="S1.T1.1.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">quantum field theory</span>, primarily through his theoretical studies of the photon.</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.2.2" class="ltx_tr">
<th id="S1.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">117M</th>
<td id="S1.T1.1.1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.2.2.1.1" class="ltx_p" style="width:227.6pt;">Which two concepts made Einstein’s post on quantum mechanics relevant?</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.3.3" class="ltx_tr">
<th id="S1.T1.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">768M</th>
<td id="S1.T1.1.1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.3.2.1.1" class="ltx_p" style="width:227.6pt;">Albert Einstein also made significant contributions to which field of theory?</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.4.4" class="ltx_tr">
<th id="S1.T1.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">8.3B</th>
<td id="S1.T1.1.1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.4.2.1.1" class="ltx_p" style="width:227.6pt;">Because of his work with the photon, what theory did he indirectly contribute to?</span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1.5.5" class="ltx_tr">
<th id="S1.T1.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Human</th>
<td id="S1.T1.1.1.5.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S1.T1.1.1.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.5.5.2.1.1" class="ltx_p" style="width:227.6pt;">What theory did Einstein have reservations about?</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span> Questions generated by models of increasing capacity with the ground truth answer highlighted in bold. As model size grows, question quality becomes increasingly coherent, complex, and factually relevant.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">One of the limitations of developing models for question answering, or any Deep Learning application for that matter, is the availability and cost of labeled training data. A common approach to alleviate this need is semi-supervised learning, wherein one trains a model on existing data and uses it to label more data for training <cite class="ltx_cite ltx_citemacro_citep">(Zhu, <a href="#bib.bib43" title="" class="ltx_ref">2005</a>; Chapelle et al., <a href="#bib.bib4" title="" class="ltx_ref">2009</a>; Zhu &amp; Goldberg, <a href="#bib.bib42" title="" class="ltx_ref">2009</a>; Kingma et al., <a href="#bib.bib18" title="" class="ltx_ref">2014</a>)</cite>. This technique has demonstrated benefits in recent literature for image classification <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite> and question answering (QA) tasks <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>; Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>. However, the complexities of generating questions and answers in natural language proves challenging for existing methods, with a large gap in quality remaining between synthetic and human-generated data. In this work, we close this gap using only synthetic questions generated from large models. We also show that answer candidate generation is foundational to synthetic question quality.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Consistent with prior work <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>; Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>, we use a 3-step modeling pipeline consisting of unconditional answer extraction from text, question generation, and question filtration. Our approach for training question generators on labeled data uses pretrained GPT-2 decoder models and a next-token-prediction language modeling objective, trained using a concatenation of context, answer, and question tokens. As demonstrated in sections <a href="#S5.SS1" title="5.1 Scaling Question Generation ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> and <a href="#S6.SS1" title="6.1 Question Generation ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>, pretraining large generative transformer models up to 8.3B parameters improves the quality of generated questions. Additionally, we propose an overgenerate and filter approach to further improve question filtration. The quality of questions produced by this pipeline can be assessed quantitatively by finetuning QA models and evaluating results on the <span id="S1.p2.1.1" class="ltx_text ltx_font_smallcaps">SQuAD </span>dataset.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We demonstrate generated questions to be comparable to supervised training with real data. For answerable <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>questions we recover 100.4% of fully supervised EM and F1 scores, when training on purely synthetic questions and answers generated from unlabeled data. Specifically, we achieve scores of 88.4 and 94.1 versus supervised training which achieves 87.7 EM and 94.0 F1. Finetuning the resulting model on real <span id="S1.p3.1.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>data reaches 89.4 EM and 95.1 F1 score, which is higher than any prior BERT-based approach. In Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we show that the generated questions are qualitatively similar to ground truth questions, with quality improving as a function of model size.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Going further, we show that QA models can be successfully trained from fully synthetic data, by running question and answer generation on a corpus generated from an unconditional GPT-2 model. With zero access to human language supervision and only access to models, we achieve an EM of 88.4 and F1 of 93.9. This approach performs comparably to generating questions from real data recovering 100.3% of fully supervised EM and F1 scores.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In sum, we demonstrate that recent advances in language models are capable of generating high quality answerable questions. This can be used in many ways, for example, synthesizing unanswerable questions <cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a href="#bib.bib28" title="" class="ltx_ref">2018</a>)</cite>, boolean questions <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, and complex questions <cite class="ltx_cite ltx_citemacro_citep">(Silver et al., <a href="#bib.bib32" title="" class="ltx_ref">2018</a>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>; Berner et al., <a href="#bib.bib3" title="" class="ltx_ref">2019</a>)</cite>. Outside of data augmentation and training question answering models, high quality question generation can be used to pose human-plausible questions for interpretability or generate intermediate queries for multihop reasoning. Additionally, modeling how humans ask questions can be used to improve search and information retrieval in query-conditional semantic information retrieval.
All these applications require improved question generation capability. In summary, our contributions are as follows:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We demonstrate for the first time that finetuning a model on purely synthetic questions and answers generated from a synthetic corpus, creates a QA model better in <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>EM and F1 scores than one trained from human-labeled data.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We show that by scaling the model size, using better ptretrained models, and leveraging large synthetically generated data, we achieve state of the art results and show 1.7 absolute gain on <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD2.0 </span>EM score compared to prior work using synthetic data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Through detailed ablation studies we identify that the quality of answer generation is fundamental to high fidelity question generation and properly aligning the answer distribution boosts scores by 19.8 EM points.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.10" class="ltx_p">In this work we seek to generate high quality training data for <span id="S2.p1.10.1" class="ltx_text ltx_font_smallcaps">SQuAD </span>style extractive question answering over a given set of documents <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">D</annotation></semantics></math>. This requires us to sample <math id="S2.p1.2.m2.3" class="ltx_Math" alttext="(c,q,a)" display="inline"><semantics id="S2.p1.2.m2.3a"><mrow id="S2.p1.2.m2.3.4.2" xref="S2.p1.2.m2.3.4.1.cmml"><mo stretchy="false" id="S2.p1.2.m2.3.4.2.1" xref="S2.p1.2.m2.3.4.1.cmml">(</mo><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">c</mi><mo id="S2.p1.2.m2.3.4.2.2" xref="S2.p1.2.m2.3.4.1.cmml">,</mo><mi id="S2.p1.2.m2.2.2" xref="S2.p1.2.m2.2.2.cmml">q</mi><mo id="S2.p1.2.m2.3.4.2.3" xref="S2.p1.2.m2.3.4.1.cmml">,</mo><mi id="S2.p1.2.m2.3.3" xref="S2.p1.2.m2.3.3.cmml">a</mi><mo stretchy="false" id="S2.p1.2.m2.3.4.2.4" xref="S2.p1.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.3b"><vector id="S2.p1.2.m2.3.4.1.cmml" xref="S2.p1.2.m2.3.4.2"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">𝑐</ci><ci id="S2.p1.2.m2.2.2.cmml" xref="S2.p1.2.m2.2.2">𝑞</ci><ci id="S2.p1.2.m2.3.3.cmml" xref="S2.p1.2.m2.3.3">𝑎</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.3c">(c,q,a)</annotation></semantics></math> triples for given paragraph contexts <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="c\in D" display="inline"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mi id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml">c</mi><mo id="S2.p1.3.m3.1.1.1" xref="S2.p1.3.m3.1.1.1.cmml">∈</mo><mi id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><in id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1"></in><ci id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2">𝑐</ci><ci id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">c\in D</annotation></semantics></math> according to probability <math id="S2.p1.4.m4.2" class="ltx_Math" alttext="p(q,a|c)" display="inline"><semantics id="S2.p1.4.m4.2a"><mrow id="S2.p1.4.m4.2.2" xref="S2.p1.4.m4.2.2.cmml"><mi id="S2.p1.4.m4.2.2.3" xref="S2.p1.4.m4.2.2.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.2.2.2" xref="S2.p1.4.m4.2.2.2.cmml">​</mo><mrow id="S2.p1.4.m4.2.2.1.1" xref="S2.p1.4.m4.2.2.1.2.cmml"><mo stretchy="false" id="S2.p1.4.m4.2.2.1.1.2" xref="S2.p1.4.m4.2.2.1.2.cmml">(</mo><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">q</mi><mo id="S2.p1.4.m4.2.2.1.1.3" xref="S2.p1.4.m4.2.2.1.2.cmml">,</mo><mrow id="S2.p1.4.m4.2.2.1.1.1" xref="S2.p1.4.m4.2.2.1.1.1.cmml"><mi id="S2.p1.4.m4.2.2.1.1.1.2" xref="S2.p1.4.m4.2.2.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.p1.4.m4.2.2.1.1.1.1" xref="S2.p1.4.m4.2.2.1.1.1.1.cmml">|</mo><mi id="S2.p1.4.m4.2.2.1.1.1.3" xref="S2.p1.4.m4.2.2.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.p1.4.m4.2.2.1.1.4" xref="S2.p1.4.m4.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.2b"><apply id="S2.p1.4.m4.2.2.cmml" xref="S2.p1.4.m4.2.2"><times id="S2.p1.4.m4.2.2.2.cmml" xref="S2.p1.4.m4.2.2.2"></times><ci id="S2.p1.4.m4.2.2.3.cmml" xref="S2.p1.4.m4.2.2.3">𝑝</ci><interval closure="open" id="S2.p1.4.m4.2.2.1.2.cmml" xref="S2.p1.4.m4.2.2.1.1"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">𝑞</ci><apply id="S2.p1.4.m4.2.2.1.1.1.cmml" xref="S2.p1.4.m4.2.2.1.1.1"><csymbol cd="latexml" id="S2.p1.4.m4.2.2.1.1.1.1.cmml" xref="S2.p1.4.m4.2.2.1.1.1.1">conditional</csymbol><ci id="S2.p1.4.m4.2.2.1.1.1.2.cmml" xref="S2.p1.4.m4.2.2.1.1.1.2">𝑎</ci><ci id="S2.p1.4.m4.2.2.1.1.1.3.cmml" xref="S2.p1.4.m4.2.2.1.1.1.3">𝑐</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.2c">p(q,a|c)</annotation></semantics></math>, where <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">q</annotation></semantics></math> is a question resulting in answer <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">a</annotation></semantics></math>, which exists as a contiguous span of text in <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.p1.7.m7.1a"><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">c</annotation></semantics></math>. Leveraging the roundtrip consistency method <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite>, we achieve this by using a three step approach consisting of Answer Generation <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="\hat{a}\sim p(a|c)" display="inline"><semantics id="S2.p1.8.m8.1a"><mrow id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml"><mover accent="true" id="S2.p1.8.m8.1.1.3" xref="S2.p1.8.m8.1.1.3.cmml"><mi id="S2.p1.8.m8.1.1.3.2" xref="S2.p1.8.m8.1.1.3.2.cmml">a</mi><mo id="S2.p1.8.m8.1.1.3.1" xref="S2.p1.8.m8.1.1.3.1.cmml">^</mo></mover><mo id="S2.p1.8.m8.1.1.2" xref="S2.p1.8.m8.1.1.2.cmml">∼</mo><mrow id="S2.p1.8.m8.1.1.1" xref="S2.p1.8.m8.1.1.1.cmml"><mi id="S2.p1.8.m8.1.1.1.3" xref="S2.p1.8.m8.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1.2" xref="S2.p1.8.m8.1.1.1.2.cmml">​</mo><mrow id="S2.p1.8.m8.1.1.1.1.1" xref="S2.p1.8.m8.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.p1.8.m8.1.1.1.1.1.2" xref="S2.p1.8.m8.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.p1.8.m8.1.1.1.1.1.1" xref="S2.p1.8.m8.1.1.1.1.1.1.cmml"><mi id="S2.p1.8.m8.1.1.1.1.1.1.2" xref="S2.p1.8.m8.1.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.p1.8.m8.1.1.1.1.1.1.1" xref="S2.p1.8.m8.1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.p1.8.m8.1.1.1.1.1.1.3" xref="S2.p1.8.m8.1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.p1.8.m8.1.1.1.1.1.3" xref="S2.p1.8.m8.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><apply id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1"><csymbol cd="latexml" id="S2.p1.8.m8.1.1.2.cmml" xref="S2.p1.8.m8.1.1.2">similar-to</csymbol><apply id="S2.p1.8.m8.1.1.3.cmml" xref="S2.p1.8.m8.1.1.3"><ci id="S2.p1.8.m8.1.1.3.1.cmml" xref="S2.p1.8.m8.1.1.3.1">^</ci><ci id="S2.p1.8.m8.1.1.3.2.cmml" xref="S2.p1.8.m8.1.1.3.2">𝑎</ci></apply><apply id="S2.p1.8.m8.1.1.1.cmml" xref="S2.p1.8.m8.1.1.1"><times id="S2.p1.8.m8.1.1.1.2.cmml" xref="S2.p1.8.m8.1.1.1.2"></times><ci id="S2.p1.8.m8.1.1.1.3.cmml" xref="S2.p1.8.m8.1.1.1.3">𝑝</ci><apply id="S2.p1.8.m8.1.1.1.1.1.1.cmml" xref="S2.p1.8.m8.1.1.1.1.1"><csymbol cd="latexml" id="S2.p1.8.m8.1.1.1.1.1.1.1.cmml" xref="S2.p1.8.m8.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.p1.8.m8.1.1.1.1.1.1.2.cmml" xref="S2.p1.8.m8.1.1.1.1.1.1.2">𝑎</ci><ci id="S2.p1.8.m8.1.1.1.1.1.1.3.cmml" xref="S2.p1.8.m8.1.1.1.1.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">\hat{a}\sim p(a|c)</annotation></semantics></math>, Question Generation <math id="S2.p1.9.m9.3" class="ltx_Math" alttext="\hat{q}\sim p(q|\hat{a},c)" display="inline"><semantics id="S2.p1.9.m9.3a"><mrow id="S2.p1.9.m9.3.3" xref="S2.p1.9.m9.3.3.cmml"><mover accent="true" id="S2.p1.9.m9.3.3.3" xref="S2.p1.9.m9.3.3.3.cmml"><mi id="S2.p1.9.m9.3.3.3.2" xref="S2.p1.9.m9.3.3.3.2.cmml">q</mi><mo id="S2.p1.9.m9.3.3.3.1" xref="S2.p1.9.m9.3.3.3.1.cmml">^</mo></mover><mo id="S2.p1.9.m9.3.3.2" xref="S2.p1.9.m9.3.3.2.cmml">∼</mo><mrow id="S2.p1.9.m9.3.3.1" xref="S2.p1.9.m9.3.3.1.cmml"><mi id="S2.p1.9.m9.3.3.1.3" xref="S2.p1.9.m9.3.3.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.3.3.1.2" xref="S2.p1.9.m9.3.3.1.2.cmml">​</mo><mrow id="S2.p1.9.m9.3.3.1.1.1" xref="S2.p1.9.m9.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.p1.9.m9.3.3.1.1.1.2" xref="S2.p1.9.m9.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.p1.9.m9.3.3.1.1.1.1" xref="S2.p1.9.m9.3.3.1.1.1.1.cmml"><mi id="S2.p1.9.m9.3.3.1.1.1.1.2" xref="S2.p1.9.m9.3.3.1.1.1.1.2.cmml">q</mi><mo fence="false" id="S2.p1.9.m9.3.3.1.1.1.1.1" xref="S2.p1.9.m9.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.p1.9.m9.3.3.1.1.1.1.3.2" xref="S2.p1.9.m9.3.3.1.1.1.1.3.1.cmml"><mover accent="true" id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml"><mi id="S2.p1.9.m9.1.1.2" xref="S2.p1.9.m9.1.1.2.cmml">a</mi><mo id="S2.p1.9.m9.1.1.1" xref="S2.p1.9.m9.1.1.1.cmml">^</mo></mover><mo id="S2.p1.9.m9.3.3.1.1.1.1.3.2.1" xref="S2.p1.9.m9.3.3.1.1.1.1.3.1.cmml">,</mo><mi id="S2.p1.9.m9.2.2" xref="S2.p1.9.m9.2.2.cmml">c</mi></mrow></mrow><mo stretchy="false" id="S2.p1.9.m9.3.3.1.1.1.3" xref="S2.p1.9.m9.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.3b"><apply id="S2.p1.9.m9.3.3.cmml" xref="S2.p1.9.m9.3.3"><csymbol cd="latexml" id="S2.p1.9.m9.3.3.2.cmml" xref="S2.p1.9.m9.3.3.2">similar-to</csymbol><apply id="S2.p1.9.m9.3.3.3.cmml" xref="S2.p1.9.m9.3.3.3"><ci id="S2.p1.9.m9.3.3.3.1.cmml" xref="S2.p1.9.m9.3.3.3.1">^</ci><ci id="S2.p1.9.m9.3.3.3.2.cmml" xref="S2.p1.9.m9.3.3.3.2">𝑞</ci></apply><apply id="S2.p1.9.m9.3.3.1.cmml" xref="S2.p1.9.m9.3.3.1"><times id="S2.p1.9.m9.3.3.1.2.cmml" xref="S2.p1.9.m9.3.3.1.2"></times><ci id="S2.p1.9.m9.3.3.1.3.cmml" xref="S2.p1.9.m9.3.3.1.3">𝑝</ci><apply id="S2.p1.9.m9.3.3.1.1.1.1.cmml" xref="S2.p1.9.m9.3.3.1.1.1"><csymbol cd="latexml" id="S2.p1.9.m9.3.3.1.1.1.1.1.cmml" xref="S2.p1.9.m9.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.p1.9.m9.3.3.1.1.1.1.2.cmml" xref="S2.p1.9.m9.3.3.1.1.1.1.2">𝑞</ci><list id="S2.p1.9.m9.3.3.1.1.1.1.3.1.cmml" xref="S2.p1.9.m9.3.3.1.1.1.1.3.2"><apply id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1"><ci id="S2.p1.9.m9.1.1.1.cmml" xref="S2.p1.9.m9.1.1.1">^</ci><ci id="S2.p1.9.m9.1.1.2.cmml" xref="S2.p1.9.m9.1.1.2">𝑎</ci></apply><ci id="S2.p1.9.m9.2.2.cmml" xref="S2.p1.9.m9.2.2">𝑐</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.3c">\hat{q}\sim p(q|\hat{a},c)</annotation></semantics></math>, and Roundtrip Filtration <math id="S2.p1.10.m10.3" class="ltx_Math" alttext="\hat{a}\stackrel{{\scriptstyle?}}{{=}}a^{*}\sim p(a|c,\hat{q})" display="inline"><semantics id="S2.p1.10.m10.3a"><mrow id="S2.p1.10.m10.3.3" xref="S2.p1.10.m10.3.3.cmml"><mover accent="true" id="S2.p1.10.m10.3.3.3" xref="S2.p1.10.m10.3.3.3.cmml"><mi id="S2.p1.10.m10.3.3.3.2" xref="S2.p1.10.m10.3.3.3.2.cmml">a</mi><mo id="S2.p1.10.m10.3.3.3.1" xref="S2.p1.10.m10.3.3.3.1.cmml">^</mo></mover><mover id="S2.p1.10.m10.3.3.4" xref="S2.p1.10.m10.3.3.4.cmml"><mo id="S2.p1.10.m10.3.3.4.2" xref="S2.p1.10.m10.3.3.4.2.cmml">=</mo><mi mathvariant="normal" id="S2.p1.10.m10.3.3.4.3" xref="S2.p1.10.m10.3.3.4.3.cmml">?</mi></mover><msup id="S2.p1.10.m10.3.3.5" xref="S2.p1.10.m10.3.3.5.cmml"><mi id="S2.p1.10.m10.3.3.5.2" xref="S2.p1.10.m10.3.3.5.2.cmml">a</mi><mo id="S2.p1.10.m10.3.3.5.3" xref="S2.p1.10.m10.3.3.5.3.cmml">∗</mo></msup><mo id="S2.p1.10.m10.3.3.6" xref="S2.p1.10.m10.3.3.6.cmml">∼</mo><mrow id="S2.p1.10.m10.3.3.1" xref="S2.p1.10.m10.3.3.1.cmml"><mi id="S2.p1.10.m10.3.3.1.3" xref="S2.p1.10.m10.3.3.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.3.3.1.2" xref="S2.p1.10.m10.3.3.1.2.cmml">​</mo><mrow id="S2.p1.10.m10.3.3.1.1.1" xref="S2.p1.10.m10.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.p1.10.m10.3.3.1.1.1.2" xref="S2.p1.10.m10.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.p1.10.m10.3.3.1.1.1.1" xref="S2.p1.10.m10.3.3.1.1.1.1.cmml"><mi id="S2.p1.10.m10.3.3.1.1.1.1.2" xref="S2.p1.10.m10.3.3.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.p1.10.m10.3.3.1.1.1.1.1" xref="S2.p1.10.m10.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.p1.10.m10.3.3.1.1.1.1.3.2" xref="S2.p1.10.m10.3.3.1.1.1.1.3.1.cmml"><mi id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml">c</mi><mo id="S2.p1.10.m10.3.3.1.1.1.1.3.2.1" xref="S2.p1.10.m10.3.3.1.1.1.1.3.1.cmml">,</mo><mover accent="true" id="S2.p1.10.m10.2.2" xref="S2.p1.10.m10.2.2.cmml"><mi id="S2.p1.10.m10.2.2.2" xref="S2.p1.10.m10.2.2.2.cmml">q</mi><mo id="S2.p1.10.m10.2.2.1" xref="S2.p1.10.m10.2.2.1.cmml">^</mo></mover></mrow></mrow><mo stretchy="false" id="S2.p1.10.m10.3.3.1.1.1.3" xref="S2.p1.10.m10.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.3b"><apply id="S2.p1.10.m10.3.3.cmml" xref="S2.p1.10.m10.3.3"><and id="S2.p1.10.m10.3.3a.cmml" xref="S2.p1.10.m10.3.3"></and><apply id="S2.p1.10.m10.3.3b.cmml" xref="S2.p1.10.m10.3.3"><apply id="S2.p1.10.m10.3.3.4.cmml" xref="S2.p1.10.m10.3.3.4"><csymbol cd="ambiguous" id="S2.p1.10.m10.3.3.4.1.cmml" xref="S2.p1.10.m10.3.3.4">superscript</csymbol><eq id="S2.p1.10.m10.3.3.4.2.cmml" xref="S2.p1.10.m10.3.3.4.2"></eq><ci id="S2.p1.10.m10.3.3.4.3.cmml" xref="S2.p1.10.m10.3.3.4.3">?</ci></apply><apply id="S2.p1.10.m10.3.3.3.cmml" xref="S2.p1.10.m10.3.3.3"><ci id="S2.p1.10.m10.3.3.3.1.cmml" xref="S2.p1.10.m10.3.3.3.1">^</ci><ci id="S2.p1.10.m10.3.3.3.2.cmml" xref="S2.p1.10.m10.3.3.3.2">𝑎</ci></apply><apply id="S2.p1.10.m10.3.3.5.cmml" xref="S2.p1.10.m10.3.3.5"><csymbol cd="ambiguous" id="S2.p1.10.m10.3.3.5.1.cmml" xref="S2.p1.10.m10.3.3.5">superscript</csymbol><ci id="S2.p1.10.m10.3.3.5.2.cmml" xref="S2.p1.10.m10.3.3.5.2">𝑎</ci><times id="S2.p1.10.m10.3.3.5.3.cmml" xref="S2.p1.10.m10.3.3.5.3"></times></apply></apply><apply id="S2.p1.10.m10.3.3c.cmml" xref="S2.p1.10.m10.3.3"><csymbol cd="latexml" id="S2.p1.10.m10.3.3.6.cmml" xref="S2.p1.10.m10.3.3.6">similar-to</csymbol><share href="#S2.p1.10.m10.3.3.5.cmml" id="S2.p1.10.m10.3.3d.cmml" xref="S2.p1.10.m10.3.3"></share><apply id="S2.p1.10.m10.3.3.1.cmml" xref="S2.p1.10.m10.3.3.1"><times id="S2.p1.10.m10.3.3.1.2.cmml" xref="S2.p1.10.m10.3.3.1.2"></times><ci id="S2.p1.10.m10.3.3.1.3.cmml" xref="S2.p1.10.m10.3.3.1.3">𝑝</ci><apply id="S2.p1.10.m10.3.3.1.1.1.1.cmml" xref="S2.p1.10.m10.3.3.1.1.1"><csymbol cd="latexml" id="S2.p1.10.m10.3.3.1.1.1.1.1.cmml" xref="S2.p1.10.m10.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.p1.10.m10.3.3.1.1.1.1.2.cmml" xref="S2.p1.10.m10.3.3.1.1.1.1.2">𝑎</ci><list id="S2.p1.10.m10.3.3.1.1.1.1.3.1.cmml" xref="S2.p1.10.m10.3.3.1.1.1.1.3.2"><ci id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1">𝑐</ci><apply id="S2.p1.10.m10.2.2.cmml" xref="S2.p1.10.m10.2.2"><ci id="S2.p1.10.m10.2.2.1.cmml" xref="S2.p1.10.m10.2.2.1">^</ci><ci id="S2.p1.10.m10.2.2.2.cmml" xref="S2.p1.10.m10.2.2.2">𝑞</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.3c">\hat{a}\stackrel{{\scriptstyle?}}{{=}}a^{*}\sim p(a|c,\hat{q})</annotation></semantics></math>. As illustrated by Algorithm <a href="#alg1" title="Algorithm 1 ‣ 2 Method ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> the synthesized dataset of triples is then used to finetune and train a BERT-based QA model similar to <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<div id="alg1.2" class="ltx_listing ltx_listing">
<div id="alg0.l1" class="ltx_listingline">  1. Sample answer candidates from paragraphs using a BERT model.

</div>
<div id="alg0.l2" class="ltx_listingline">  2. Generate questions from answer candidates and paragraphs using a GPT-2 model.

</div>
<div id="alg0.l3" class="ltx_listingline">  3. Apply a BERT roundtrip consistency model to filter generated question answer pairs.

</div>
<div id="alg0.l4" class="ltx_listingline">  4. Train a BERT QA model using filtered synthetic questions and evaluate on development set.

</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.3.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Pipeline for generating and evaluating synthetic data.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Answer Generation: <math id="S2.SS1.1.m1.1" class="ltx_Math" alttext="\hat{a}\sim p(a|c)" display="inline"><semantics id="S2.SS1.1.m1.1b"><mrow id="S2.SS1.1.m1.1.1" xref="S2.SS1.1.m1.1.1.cmml"><mover accent="true" id="S2.SS1.1.m1.1.1.3" xref="S2.SS1.1.m1.1.1.3.cmml"><mi id="S2.SS1.1.m1.1.1.3.2" xref="S2.SS1.1.m1.1.1.3.2.cmml">a</mi><mo id="S2.SS1.1.m1.1.1.3.1" xref="S2.SS1.1.m1.1.1.3.1.cmml">^</mo></mover><mo id="S2.SS1.1.m1.1.1.2" xref="S2.SS1.1.m1.1.1.2.cmml">∼</mo><mrow id="S2.SS1.1.m1.1.1.1" xref="S2.SS1.1.m1.1.1.1.cmml"><mi id="S2.SS1.1.m1.1.1.1.3" xref="S2.SS1.1.m1.1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.1.m1.1.1.1.2" xref="S2.SS1.1.m1.1.1.1.2.cmml">​</mo><mrow id="S2.SS1.1.m1.1.1.1.1.1" xref="S2.SS1.1.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.1.m1.1.1.1.1.1.2" xref="S2.SS1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.1.m1.1.1.1.1.1.1" xref="S2.SS1.1.m1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.1.m1.1.1.1.1.1.1.2" xref="S2.SS1.1.m1.1.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS1.1.m1.1.1.1.1.1.1.1" xref="S2.SS1.1.m1.1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.1.m1.1.1.1.1.1.1.3" xref="S2.SS1.1.m1.1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS1.1.m1.1.1.1.1.1.3" xref="S2.SS1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.1.m1.1c"><apply id="S2.SS1.1.m1.1.1.cmml" xref="S2.SS1.1.m1.1.1"><csymbol cd="latexml" id="S2.SS1.1.m1.1.1.2.cmml" xref="S2.SS1.1.m1.1.1.2">similar-to</csymbol><apply id="S2.SS1.1.m1.1.1.3.cmml" xref="S2.SS1.1.m1.1.1.3"><ci id="S2.SS1.1.m1.1.1.3.1.cmml" xref="S2.SS1.1.m1.1.1.3.1">^</ci><ci id="S2.SS1.1.m1.1.1.3.2.cmml" xref="S2.SS1.1.m1.1.1.3.2">𝑎</ci></apply><apply id="S2.SS1.1.m1.1.1.1.cmml" xref="S2.SS1.1.m1.1.1.1"><times id="S2.SS1.1.m1.1.1.1.2.cmml" xref="S2.SS1.1.m1.1.1.1.2"></times><ci id="S2.SS1.1.m1.1.1.1.3.cmml" xref="S2.SS1.1.m1.1.1.1.3">𝑝</ci><apply id="S2.SS1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.1.m1.1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.1.m1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.1.m1.1.1.1.1.1.1.2">𝑎</ci><ci id="S2.SS1.1.m1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.1.m1.1.1.1.1.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.1.m1.1d">\hat{a}\sim p(a|c)</annotation></semantics></math>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citep">(Talmor &amp; Berant, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite> empirically showed that a QA model trained on a specific dataset does not necessarily generalize well to other similar QA datasets. For a model to perform well on a specific dataset, we need to match its answer distribution. Our goal is to learn an answer candidate generator <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="p(a|c)" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.1.m1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.1.m1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS1.p1.1.m1.1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS1.p1.1.m1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2"></times><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">𝑝</ci><apply id="S2.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.2">𝑎</ci><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">p(a|c)</annotation></semantics></math>, that acts as a prior for the dataset’s answer distribution. Earlier work <cite class="ltx_cite ltx_citemacro_citep">(Dhingra et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>; Lewis et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> using named entity and noun phrase answer candidates performed best only on those portions of the data distribution. By aligning our answer candidates with the dataset answers through end-to-end model-based approaches, our performance is comparable to the fully-supervised baseline.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.3" class="ltx_p">To achieve this we finetune a BERT-style transformer model with hidden size <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">H</annotation></semantics></math> for extractive span selection. However, unlike BERT finetuning for question answering we omit the question tokens. This yields an unconditional answer extractor model <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="p(a|c)" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">​</mo><mrow id="S2.SS1.p2.2.m2.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.1.1.2" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p2.2.m2.1.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.1.1.1.2" xref="S2.SS1.p2.2.m2.1.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS1.p2.2.m2.1.1.1.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S2.SS1.p2.2.m2.1.1.1.1.1.3" xref="S2.SS1.p2.2.m2.1.1.1.1.1.3.cmml">c</mi></mrow><mo stretchy="false" id="S2.SS1.p2.2.m2.1.1.1.1.3" xref="S2.SS1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><times id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2"></times><ci id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">𝑝</ci><apply id="S2.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S2.SS1.p2.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1.1.2">𝑎</ci><ci id="S2.SS1.p2.2.m2.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.1.1.1.3">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">p(a|c)</annotation></semantics></math> that predicts the start and end of a token span <math id="S2.SS1.p2.3.m3.2" class="ltx_Math" alttext="(s,e)=a" display="inline"><semantics id="S2.SS1.p2.3.m3.2a"><mrow id="S2.SS1.p2.3.m3.2.3" xref="S2.SS1.p2.3.m3.2.3.cmml"><mrow id="S2.SS1.p2.3.m3.2.3.2.2" xref="S2.SS1.p2.3.m3.2.3.2.1.cmml"><mo stretchy="false" id="S2.SS1.p2.3.m3.2.3.2.2.1" xref="S2.SS1.p2.3.m3.2.3.2.1.cmml">(</mo><mi id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">s</mi><mo id="S2.SS1.p2.3.m3.2.3.2.2.2" xref="S2.SS1.p2.3.m3.2.3.2.1.cmml">,</mo><mi id="S2.SS1.p2.3.m3.2.2" xref="S2.SS1.p2.3.m3.2.2.cmml">e</mi><mo stretchy="false" id="S2.SS1.p2.3.m3.2.3.2.2.3" xref="S2.SS1.p2.3.m3.2.3.2.1.cmml">)</mo></mrow><mo id="S2.SS1.p2.3.m3.2.3.1" xref="S2.SS1.p2.3.m3.2.3.1.cmml">=</mo><mi id="S2.SS1.p2.3.m3.2.3.3" xref="S2.SS1.p2.3.m3.2.3.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.2b"><apply id="S2.SS1.p2.3.m3.2.3.cmml" xref="S2.SS1.p2.3.m3.2.3"><eq id="S2.SS1.p2.3.m3.2.3.1.cmml" xref="S2.SS1.p2.3.m3.2.3.1"></eq><interval closure="open" id="S2.SS1.p2.3.m3.2.3.2.1.cmml" xref="S2.SS1.p2.3.m3.2.3.2.2"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">𝑠</ci><ci id="S2.SS1.p2.3.m3.2.2.cmml" xref="S2.SS1.p2.3.m3.2.2">𝑒</ci></interval><ci id="S2.SS1.p2.3.m3.2.3.3.cmml" xref="S2.SS1.p2.3.m3.2.3.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.2c">(s,e)=a</annotation></semantics></math>. Similar to <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite> we used an answer extraction head that models start and end tokens jointly.</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.8" class="ltx_Math" alttext="p(a|c;\theta_{A})=\frac{e^{f(a,c;\theta_{A})}}{\sum_{a^{\prime\prime}}e^{f(a^{\prime\prime},c;\theta_{A})}}" display="block"><semantics id="S2.Ex1.m1.8a"><mrow id="S2.Ex1.m1.8.8" xref="S2.Ex1.m1.8.8.cmml"><mrow id="S2.Ex1.m1.8.8.1" xref="S2.Ex1.m1.8.8.1.cmml"><mi id="S2.Ex1.m1.8.8.1.3" xref="S2.Ex1.m1.8.8.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.8.8.1.2" xref="S2.Ex1.m1.8.8.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.8.8.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.8.8.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.cmml">(</mo><mrow id="S2.Ex1.m1.8.8.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.cmml"><mi id="S2.Ex1.m1.8.8.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.3.cmml">a</mi><mo fence="false" id="S2.Ex1.m1.8.8.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.2.cmml">|</mo><mrow id="S2.Ex1.m1.8.8.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.7.7" xref="S2.Ex1.m1.7.7.cmml">c</mi><mo id="S2.Ex1.m1.8.8.1.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.1.2.cmml">;</mo><msub id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2.cmml">θ</mi><mi id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3.cmml">A</mi></msub></mrow></mrow><mo stretchy="false" id="S2.Ex1.m1.8.8.1.1.1.3" xref="S2.Ex1.m1.8.8.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.8.8.2" xref="S2.Ex1.m1.8.8.2.cmml">=</mo><mfrac id="S2.Ex1.m1.6.6" xref="S2.Ex1.m1.6.6.cmml"><msup id="S2.Ex1.m1.3.3.3" xref="S2.Ex1.m1.3.3.3.cmml"><mi id="S2.Ex1.m1.3.3.3.5" xref="S2.Ex1.m1.3.3.3.5.cmml">e</mi><mrow id="S2.Ex1.m1.3.3.3.3.3" xref="S2.Ex1.m1.3.3.3.3.3.cmml"><mi id="S2.Ex1.m1.3.3.3.3.3.5" xref="S2.Ex1.m1.3.3.3.3.3.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.3.3.3.4" xref="S2.Ex1.m1.3.3.3.3.3.4.cmml">​</mo><mrow id="S2.Ex1.m1.3.3.3.3.3.3.1" xref="S2.Ex1.m1.3.3.3.3.3.3.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.3.3.3.3.1.2" xref="S2.Ex1.m1.3.3.3.3.3.3.2.cmml">(</mo><mi id="S2.Ex1.m1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml">a</mi><mo id="S2.Ex1.m1.3.3.3.3.3.3.1.3" xref="S2.Ex1.m1.3.3.3.3.3.3.2.cmml">,</mo><mi id="S2.Ex1.m1.2.2.2.2.2.2" xref="S2.Ex1.m1.2.2.2.2.2.2.cmml">c</mi><mo id="S2.Ex1.m1.3.3.3.3.3.3.1.4" xref="S2.Ex1.m1.3.3.3.3.3.3.2.cmml">;</mo><msub id="S2.Ex1.m1.3.3.3.3.3.3.1.1" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1.cmml"><mi id="S2.Ex1.m1.3.3.3.3.3.3.1.1.2" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1.2.cmml">θ</mi><mi id="S2.Ex1.m1.3.3.3.3.3.3.1.1.3" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1.3.cmml">A</mi></msub><mo stretchy="false" id="S2.Ex1.m1.3.3.3.3.3.3.1.5" xref="S2.Ex1.m1.3.3.3.3.3.3.2.cmml">)</mo></mrow></mrow></msup><mrow id="S2.Ex1.m1.6.6.6" xref="S2.Ex1.m1.6.6.6.cmml"><msub id="S2.Ex1.m1.6.6.6.4" xref="S2.Ex1.m1.6.6.6.4.cmml"><mo id="S2.Ex1.m1.6.6.6.4.2" xref="S2.Ex1.m1.6.6.6.4.2.cmml">∑</mo><msup id="S2.Ex1.m1.6.6.6.4.3" xref="S2.Ex1.m1.6.6.6.4.3.cmml"><mi id="S2.Ex1.m1.6.6.6.4.3.2" xref="S2.Ex1.m1.6.6.6.4.3.2.cmml">a</mi><mo id="S2.Ex1.m1.6.6.6.4.3.3" xref="S2.Ex1.m1.6.6.6.4.3.3.cmml">′′</mo></msup></msub><msup id="S2.Ex1.m1.6.6.6.5" xref="S2.Ex1.m1.6.6.6.5.cmml"><mi id="S2.Ex1.m1.6.6.6.5.2" xref="S2.Ex1.m1.6.6.6.5.2.cmml">e</mi><mrow id="S2.Ex1.m1.6.6.6.3.3" xref="S2.Ex1.m1.6.6.6.3.3.cmml"><mi id="S2.Ex1.m1.6.6.6.3.3.5" xref="S2.Ex1.m1.6.6.6.3.3.5.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.6.6.6.3.3.4" xref="S2.Ex1.m1.6.6.6.3.3.4.cmml">​</mo><mrow id="S2.Ex1.m1.6.6.6.3.3.3.2" xref="S2.Ex1.m1.6.6.6.3.3.3.3.cmml"><mo stretchy="false" id="S2.Ex1.m1.6.6.6.3.3.3.2.3" xref="S2.Ex1.m1.6.6.6.3.3.3.3.cmml">(</mo><msup id="S2.Ex1.m1.5.5.5.2.2.2.1.1" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1.cmml"><mi id="S2.Ex1.m1.5.5.5.2.2.2.1.1.2" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1.2.cmml">a</mi><mo id="S2.Ex1.m1.5.5.5.2.2.2.1.1.3" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1.3.cmml">′′</mo></msup><mo id="S2.Ex1.m1.6.6.6.3.3.3.2.4" xref="S2.Ex1.m1.6.6.6.3.3.3.3.cmml">,</mo><mi id="S2.Ex1.m1.4.4.4.1.1.1" xref="S2.Ex1.m1.4.4.4.1.1.1.cmml">c</mi><mo id="S2.Ex1.m1.6.6.6.3.3.3.2.5" xref="S2.Ex1.m1.6.6.6.3.3.3.3.cmml">;</mo><msub id="S2.Ex1.m1.6.6.6.3.3.3.2.2" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2.cmml"><mi id="S2.Ex1.m1.6.6.6.3.3.3.2.2.2" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2.2.cmml">θ</mi><mi id="S2.Ex1.m1.6.6.6.3.3.3.2.2.3" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2.3.cmml">A</mi></msub><mo stretchy="false" id="S2.Ex1.m1.6.6.6.3.3.3.2.6" xref="S2.Ex1.m1.6.6.6.3.3.3.3.cmml">)</mo></mrow></mrow></msup></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.8b"><apply id="S2.Ex1.m1.8.8.cmml" xref="S2.Ex1.m1.8.8"><eq id="S2.Ex1.m1.8.8.2.cmml" xref="S2.Ex1.m1.8.8.2"></eq><apply id="S2.Ex1.m1.8.8.1.cmml" xref="S2.Ex1.m1.8.8.1"><times id="S2.Ex1.m1.8.8.1.2.cmml" xref="S2.Ex1.m1.8.8.1.2"></times><ci id="S2.Ex1.m1.8.8.1.3.cmml" xref="S2.Ex1.m1.8.8.1.3">𝑝</ci><apply id="S2.Ex1.m1.8.8.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.8.8.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.2">conditional</csymbol><ci id="S2.Ex1.m1.8.8.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.3">𝑎</ci><list id="S2.Ex1.m1.8.8.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1"><ci id="S2.Ex1.m1.7.7.cmml" xref="S2.Ex1.m1.7.7">𝑐</ci><apply id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.2">𝜃</ci><ci id="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.8.8.1.1.1.1.1.1.1.3">𝐴</ci></apply></list></apply></apply><apply id="S2.Ex1.m1.6.6.cmml" xref="S2.Ex1.m1.6.6"><divide id="S2.Ex1.m1.6.6.7.cmml" xref="S2.Ex1.m1.6.6"></divide><apply id="S2.Ex1.m1.3.3.3.cmml" xref="S2.Ex1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.3.4.cmml" xref="S2.Ex1.m1.3.3.3">superscript</csymbol><ci id="S2.Ex1.m1.3.3.3.5.cmml" xref="S2.Ex1.m1.3.3.3.5">𝑒</ci><apply id="S2.Ex1.m1.3.3.3.3.3.cmml" xref="S2.Ex1.m1.3.3.3.3.3"><times id="S2.Ex1.m1.3.3.3.3.3.4.cmml" xref="S2.Ex1.m1.3.3.3.3.3.4"></times><ci id="S2.Ex1.m1.3.3.3.3.3.5.cmml" xref="S2.Ex1.m1.3.3.3.3.3.5">𝑓</ci><vector id="S2.Ex1.m1.3.3.3.3.3.3.2.cmml" xref="S2.Ex1.m1.3.3.3.3.3.3.1"><ci id="S2.Ex1.m1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1">𝑎</ci><ci id="S2.Ex1.m1.2.2.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2.2.2.2">𝑐</ci><apply id="S2.Ex1.m1.3.3.3.3.3.3.1.1.cmml" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.3.3.3.3.1.1.1.cmml" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1">subscript</csymbol><ci id="S2.Ex1.m1.3.3.3.3.3.3.1.1.2.cmml" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1.2">𝜃</ci><ci id="S2.Ex1.m1.3.3.3.3.3.3.1.1.3.cmml" xref="S2.Ex1.m1.3.3.3.3.3.3.1.1.3">𝐴</ci></apply></vector></apply></apply><apply id="S2.Ex1.m1.6.6.6.cmml" xref="S2.Ex1.m1.6.6.6"><apply id="S2.Ex1.m1.6.6.6.4.cmml" xref="S2.Ex1.m1.6.6.6.4"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.6.4.1.cmml" xref="S2.Ex1.m1.6.6.6.4">subscript</csymbol><sum id="S2.Ex1.m1.6.6.6.4.2.cmml" xref="S2.Ex1.m1.6.6.6.4.2"></sum><apply id="S2.Ex1.m1.6.6.6.4.3.cmml" xref="S2.Ex1.m1.6.6.6.4.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.6.4.3.1.cmml" xref="S2.Ex1.m1.6.6.6.4.3">superscript</csymbol><ci id="S2.Ex1.m1.6.6.6.4.3.2.cmml" xref="S2.Ex1.m1.6.6.6.4.3.2">𝑎</ci><ci id="S2.Ex1.m1.6.6.6.4.3.3.cmml" xref="S2.Ex1.m1.6.6.6.4.3.3">′′</ci></apply></apply><apply id="S2.Ex1.m1.6.6.6.5.cmml" xref="S2.Ex1.m1.6.6.6.5"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.6.5.1.cmml" xref="S2.Ex1.m1.6.6.6.5">superscript</csymbol><ci id="S2.Ex1.m1.6.6.6.5.2.cmml" xref="S2.Ex1.m1.6.6.6.5.2">𝑒</ci><apply id="S2.Ex1.m1.6.6.6.3.3.cmml" xref="S2.Ex1.m1.6.6.6.3.3"><times id="S2.Ex1.m1.6.6.6.3.3.4.cmml" xref="S2.Ex1.m1.6.6.6.3.3.4"></times><ci id="S2.Ex1.m1.6.6.6.3.3.5.cmml" xref="S2.Ex1.m1.6.6.6.3.3.5">𝑓</ci><vector id="S2.Ex1.m1.6.6.6.3.3.3.3.cmml" xref="S2.Ex1.m1.6.6.6.3.3.3.2"><apply id="S2.Ex1.m1.5.5.5.2.2.2.1.1.cmml" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.5.5.5.2.2.2.1.1.1.cmml" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1">superscript</csymbol><ci id="S2.Ex1.m1.5.5.5.2.2.2.1.1.2.cmml" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1.2">𝑎</ci><ci id="S2.Ex1.m1.5.5.5.2.2.2.1.1.3.cmml" xref="S2.Ex1.m1.5.5.5.2.2.2.1.1.3">′′</ci></apply><ci id="S2.Ex1.m1.4.4.4.1.1.1.cmml" xref="S2.Ex1.m1.4.4.4.1.1.1">𝑐</ci><apply id="S2.Ex1.m1.6.6.6.3.3.3.2.2.cmml" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.6.6.6.3.3.3.2.2.1.cmml" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2">subscript</csymbol><ci id="S2.Ex1.m1.6.6.6.3.3.3.2.2.2.cmml" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2.2">𝜃</ci><ci id="S2.Ex1.m1.6.6.6.3.3.3.2.2.3.cmml" xref="S2.Ex1.m1.6.6.6.3.3.3.2.2.3">𝐴</ci></apply></vector></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.8c">p(a|c;\theta_{A})=\frac{e^{f(a,c;\theta_{A})}}{\sum_{a^{\prime\prime}}e^{f(a^{\prime\prime},c;\theta_{A})}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.8" class="ltx_Math" alttext="f(a,c;\theta_{A})=\mbox{MLP}(\mbox{CONCAT}(\mbox{BERT}(c)[s],\mbox{BERT}(c)[e]))" display="block"><semantics id="S2.Ex2.m1.8a"><mrow id="S2.Ex2.m1.8.8" xref="S2.Ex2.m1.8.8.cmml"><mrow id="S2.Ex2.m1.7.7.1" xref="S2.Ex2.m1.7.7.1.cmml"><mi id="S2.Ex2.m1.7.7.1.3" xref="S2.Ex2.m1.7.7.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.7.7.1.2" xref="S2.Ex2.m1.7.7.1.2.cmml">​</mo><mrow id="S2.Ex2.m1.7.7.1.1.1" xref="S2.Ex2.m1.7.7.1.1.2.cmml"><mo stretchy="false" id="S2.Ex2.m1.7.7.1.1.1.2" xref="S2.Ex2.m1.7.7.1.1.2.cmml">(</mo><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">a</mi><mo id="S2.Ex2.m1.7.7.1.1.1.3" xref="S2.Ex2.m1.7.7.1.1.2.cmml">,</mo><mi id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml">c</mi><mo id="S2.Ex2.m1.7.7.1.1.1.4" xref="S2.Ex2.m1.7.7.1.1.2.cmml">;</mo><msub id="S2.Ex2.m1.7.7.1.1.1.1" xref="S2.Ex2.m1.7.7.1.1.1.1.cmml"><mi id="S2.Ex2.m1.7.7.1.1.1.1.2" xref="S2.Ex2.m1.7.7.1.1.1.1.2.cmml">θ</mi><mi id="S2.Ex2.m1.7.7.1.1.1.1.3" xref="S2.Ex2.m1.7.7.1.1.1.1.3.cmml">A</mi></msub><mo stretchy="false" id="S2.Ex2.m1.7.7.1.1.1.5" xref="S2.Ex2.m1.7.7.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.8.8.3" xref="S2.Ex2.m1.8.8.3.cmml">=</mo><mrow id="S2.Ex2.m1.8.8.2" xref="S2.Ex2.m1.8.8.2.cmml"><mtext id="S2.Ex2.m1.8.8.2.3" xref="S2.Ex2.m1.8.8.2.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.8.8.2.2" xref="S2.Ex2.m1.8.8.2.2.cmml">​</mo><mrow id="S2.Ex2.m1.8.8.2.1.1" xref="S2.Ex2.m1.8.8.2.1.1.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.2" xref="S2.Ex2.m1.8.8.2.1.1.1.cmml">(</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1" xref="S2.Ex2.m1.8.8.2.1.1.1.cmml"><mtext id="S2.Ex2.m1.8.8.2.1.1.1.4" xref="S2.Ex2.m1.8.8.2.1.1.1.4a.cmml">CONCAT</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.8.8.2.1.1.1.3" xref="S2.Ex2.m1.8.8.2.1.1.1.3.cmml">​</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.3" xref="S2.Ex2.m1.8.8.2.1.1.1.2.3.cmml">(</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.cmml"><mtext id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.2" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.2a.cmml">BERT</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.1" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.3.2" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.3.2.1" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.cmml">(</mo><mi id="S2.Ex2.m1.3.3" xref="S2.Ex2.m1.3.3.cmml">c</mi><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.3.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.1a" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.1.cmml">​</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.2" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.2.1" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.1.1.cmml">[</mo><mi id="S2.Ex2.m1.4.4" xref="S2.Ex2.m1.4.4.cmml">s</mi><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.1.1.cmml">]</mo></mrow></mrow><mo id="S2.Ex2.m1.8.8.2.1.1.1.2.2.4" xref="S2.Ex2.m1.8.8.2.1.1.1.2.3.cmml">,</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.cmml"><mtext id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.2a.cmml">BERT</mtext><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.1" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.1.cmml">​</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.3.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.cmml"><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.3.2.1" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.cmml">(</mo><mi id="S2.Ex2.m1.5.5" xref="S2.Ex2.m1.5.5.cmml">c</mi><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.3.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.1a" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.1.cmml">​</mo><mrow id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.2.1" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.1.1.cmml">[</mo><mi id="S2.Ex2.m1.6.6" xref="S2.Ex2.m1.6.6.cmml">e</mi><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.2.2" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.1.1.cmml">]</mo></mrow></mrow><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.5" xref="S2.Ex2.m1.8.8.2.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.Ex2.m1.8.8.2.1.1.3" xref="S2.Ex2.m1.8.8.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.8b"><apply id="S2.Ex2.m1.8.8.cmml" xref="S2.Ex2.m1.8.8"><eq id="S2.Ex2.m1.8.8.3.cmml" xref="S2.Ex2.m1.8.8.3"></eq><apply id="S2.Ex2.m1.7.7.1.cmml" xref="S2.Ex2.m1.7.7.1"><times id="S2.Ex2.m1.7.7.1.2.cmml" xref="S2.Ex2.m1.7.7.1.2"></times><ci id="S2.Ex2.m1.7.7.1.3.cmml" xref="S2.Ex2.m1.7.7.1.3">𝑓</ci><vector id="S2.Ex2.m1.7.7.1.1.2.cmml" xref="S2.Ex2.m1.7.7.1.1.1"><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝑎</ci><ci id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2">𝑐</ci><apply id="S2.Ex2.m1.7.7.1.1.1.1.cmml" xref="S2.Ex2.m1.7.7.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.7.7.1.1.1.1.1.cmml" xref="S2.Ex2.m1.7.7.1.1.1.1">subscript</csymbol><ci id="S2.Ex2.m1.7.7.1.1.1.1.2.cmml" xref="S2.Ex2.m1.7.7.1.1.1.1.2">𝜃</ci><ci id="S2.Ex2.m1.7.7.1.1.1.1.3.cmml" xref="S2.Ex2.m1.7.7.1.1.1.1.3">𝐴</ci></apply></vector></apply><apply id="S2.Ex2.m1.8.8.2.cmml" xref="S2.Ex2.m1.8.8.2"><times id="S2.Ex2.m1.8.8.2.2.cmml" xref="S2.Ex2.m1.8.8.2.2"></times><ci id="S2.Ex2.m1.8.8.2.3a.cmml" xref="S2.Ex2.m1.8.8.2.3"><mtext id="S2.Ex2.m1.8.8.2.3.cmml" xref="S2.Ex2.m1.8.8.2.3">MLP</mtext></ci><apply id="S2.Ex2.m1.8.8.2.1.1.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1"><times id="S2.Ex2.m1.8.8.2.1.1.1.3.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.3"></times><ci id="S2.Ex2.m1.8.8.2.1.1.1.4a.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.4"><mtext id="S2.Ex2.m1.8.8.2.1.1.1.4.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.4">CONCAT</mtext></ci><interval closure="open" id="S2.Ex2.m1.8.8.2.1.1.1.2.3.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2"><apply id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1"><times id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.1"></times><ci id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.2a.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.2"><mtext id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.2">BERT</mtext></ci><ci id="S2.Ex2.m1.3.3.cmml" xref="S2.Ex2.m1.3.3">𝑐</ci><apply id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.2"><csymbol cd="latexml" id="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.1.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.1.1.1.4.2.1">delimited-[]</csymbol><ci id="S2.Ex2.m1.4.4.cmml" xref="S2.Ex2.m1.4.4">𝑠</ci></apply></apply><apply id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2"><times id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.1"></times><ci id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.2a.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.2"><mtext id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.2.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.2">BERT</mtext></ci><ci id="S2.Ex2.m1.5.5.cmml" xref="S2.Ex2.m1.5.5">𝑐</ci><apply id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.2"><csymbol cd="latexml" id="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.1.1.cmml" xref="S2.Ex2.m1.8.8.2.1.1.1.2.2.2.4.2.1">delimited-[]</csymbol><ci id="S2.Ex2.m1.6.6.cmml" xref="S2.Ex2.m1.6.6">𝑒</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.8c">f(a,c;\theta_{A})=\mbox{MLP}(\mbox{CONCAT}(\mbox{BERT}(c)[s],\mbox{BERT}(c)[e]))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p2.4" class="ltx_p">We also found that joint modeling performed better than an extraction head that models start and end tokens independently. Lastly, our MLP layer consists of one hidden layer with hidden size <math id="S2.SS1.p2.4.m1.1" class="ltx_Math" alttext="2H" display="inline"><semantics id="S2.SS1.p2.4.m1.1a"><mrow id="S2.SS1.p2.4.m1.1.1" xref="S2.SS1.p2.4.m1.1.1.cmml"><mn id="S2.SS1.p2.4.m1.1.1.2" xref="S2.SS1.p2.4.m1.1.1.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S2.SS1.p2.4.m1.1.1.1" xref="S2.SS1.p2.4.m1.1.1.1.cmml">​</mo><mi id="S2.SS1.p2.4.m1.1.1.3" xref="S2.SS1.p2.4.m1.1.1.3.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m1.1b"><apply id="S2.SS1.p2.4.m1.1.1.cmml" xref="S2.SS1.p2.4.m1.1.1"><times id="S2.SS1.p2.4.m1.1.1.1.cmml" xref="S2.SS1.p2.4.m1.1.1.1"></times><cn type="integer" id="S2.SS1.p2.4.m1.1.1.2.cmml" xref="S2.SS1.p2.4.m1.1.1.2">2</cn><ci id="S2.SS1.p2.4.m1.1.1.3.cmml" xref="S2.SS1.p2.4.m1.1.1.3">𝐻</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m1.1c">2H</annotation></semantics></math>, followed by a ReLU nonlinearity, and a projection from activations to logits.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Question Generation: <math id="S2.SS2.1.m1.3" class="ltx_Math" alttext="\hat{q}\sim p(q|\hat{a},c)" display="inline"><semantics id="S2.SS2.1.m1.3b"><mrow id="S2.SS2.1.m1.3.3" xref="S2.SS2.1.m1.3.3.cmml"><mover accent="true" id="S2.SS2.1.m1.3.3.3" xref="S2.SS2.1.m1.3.3.3.cmml"><mi id="S2.SS2.1.m1.3.3.3.2" xref="S2.SS2.1.m1.3.3.3.2.cmml">q</mi><mo id="S2.SS2.1.m1.3.3.3.1" xref="S2.SS2.1.m1.3.3.3.1.cmml">^</mo></mover><mo id="S2.SS2.1.m1.3.3.2" xref="S2.SS2.1.m1.3.3.2.cmml">∼</mo><mrow id="S2.SS2.1.m1.3.3.1" xref="S2.SS2.1.m1.3.3.1.cmml"><mi id="S2.SS2.1.m1.3.3.1.3" xref="S2.SS2.1.m1.3.3.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS2.1.m1.3.3.1.2" xref="S2.SS2.1.m1.3.3.1.2.cmml">​</mo><mrow id="S2.SS2.1.m1.3.3.1.1.1" xref="S2.SS2.1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.1.m1.3.3.1.1.1.2" xref="S2.SS2.1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.SS2.1.m1.3.3.1.1.1.1" xref="S2.SS2.1.m1.3.3.1.1.1.1.cmml"><mi id="S2.SS2.1.m1.3.3.1.1.1.1.2" xref="S2.SS2.1.m1.3.3.1.1.1.1.2.cmml">q</mi><mo fence="false" id="S2.SS2.1.m1.3.3.1.1.1.1.1" xref="S2.SS2.1.m1.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.SS2.1.m1.3.3.1.1.1.1.3.2" xref="S2.SS2.1.m1.3.3.1.1.1.1.3.1.cmml"><mover accent="true" id="S2.SS2.1.m1.1.1" xref="S2.SS2.1.m1.1.1.cmml"><mi id="S2.SS2.1.m1.1.1.2" xref="S2.SS2.1.m1.1.1.2.cmml">a</mi><mo id="S2.SS2.1.m1.1.1.1" xref="S2.SS2.1.m1.1.1.1.cmml">^</mo></mover><mo id="S2.SS2.1.m1.3.3.1.1.1.1.3.2.1" xref="S2.SS2.1.m1.3.3.1.1.1.1.3.1.cmml">,</mo><mi id="S2.SS2.1.m1.2.2" xref="S2.SS2.1.m1.2.2.cmml">c</mi></mrow></mrow><mo stretchy="false" id="S2.SS2.1.m1.3.3.1.1.1.3" xref="S2.SS2.1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.1.m1.3c"><apply id="S2.SS2.1.m1.3.3.cmml" xref="S2.SS2.1.m1.3.3"><csymbol cd="latexml" id="S2.SS2.1.m1.3.3.2.cmml" xref="S2.SS2.1.m1.3.3.2">similar-to</csymbol><apply id="S2.SS2.1.m1.3.3.3.cmml" xref="S2.SS2.1.m1.3.3.3"><ci id="S2.SS2.1.m1.3.3.3.1.cmml" xref="S2.SS2.1.m1.3.3.3.1">^</ci><ci id="S2.SS2.1.m1.3.3.3.2.cmml" xref="S2.SS2.1.m1.3.3.3.2">𝑞</ci></apply><apply id="S2.SS2.1.m1.3.3.1.cmml" xref="S2.SS2.1.m1.3.3.1"><times id="S2.SS2.1.m1.3.3.1.2.cmml" xref="S2.SS2.1.m1.3.3.1.2"></times><ci id="S2.SS2.1.m1.3.3.1.3.cmml" xref="S2.SS2.1.m1.3.3.1.3">𝑝</ci><apply id="S2.SS2.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS2.1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S2.SS2.1.m1.3.3.1.1.1.1.1.cmml" xref="S2.SS2.1.m1.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.SS2.1.m1.3.3.1.1.1.1.2.cmml" xref="S2.SS2.1.m1.3.3.1.1.1.1.2">𝑞</ci><list id="S2.SS2.1.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.SS2.1.m1.3.3.1.1.1.1.3.2"><apply id="S2.SS2.1.m1.1.1.cmml" xref="S2.SS2.1.m1.1.1"><ci id="S2.SS2.1.m1.1.1.1.cmml" xref="S2.SS2.1.m1.1.1.1">^</ci><ci id="S2.SS2.1.m1.1.1.2.cmml" xref="S2.SS2.1.m1.1.1.2">𝑎</ci></apply><ci id="S2.SS2.1.m1.2.2.cmml" xref="S2.SS2.1.m1.2.2">𝑐</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.1.m1.3d">\hat{q}\sim p(q|\hat{a},c)</annotation></semantics></math>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We develop a conditional question generation model, <math id="S2.SS2.p1.1.m1.3" class="ltx_Math" alttext="p(q|a,c)" display="inline"><semantics id="S2.SS2.p1.1.m1.3a"><mrow id="S2.SS2.p1.1.m1.3.3" xref="S2.SS2.p1.1.m1.3.3.cmml"><mi id="S2.SS2.p1.1.m1.3.3.3" xref="S2.SS2.p1.1.m1.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.3.3.2" xref="S2.SS2.p1.1.m1.3.3.2.cmml">​</mo><mrow id="S2.SS2.p1.1.m1.3.3.1.1" xref="S2.SS2.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS2.p1.1.m1.3.3.1.1.2" xref="S2.SS2.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS2.p1.1.m1.3.3.1.1.1" xref="S2.SS2.p1.1.m1.3.3.1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.3.3.1.1.1.2" xref="S2.SS2.p1.1.m1.3.3.1.1.1.2.cmml">q</mi><mo fence="false" id="S2.SS2.p1.1.m1.3.3.1.1.1.1" xref="S2.SS2.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS2.p1.1.m1.3.3.1.1.1.3.2" xref="S2.SS2.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">a</mi><mo id="S2.SS2.p1.1.m1.3.3.1.1.1.3.2.1" xref="S2.SS2.p1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS2.p1.1.m1.2.2" xref="S2.SS2.p1.1.m1.2.2.cmml">c</mi></mrow></mrow><mo stretchy="false" id="S2.SS2.p1.1.m1.3.3.1.1.3" xref="S2.SS2.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.3b"><apply id="S2.SS2.p1.1.m1.3.3.cmml" xref="S2.SS2.p1.1.m1.3.3"><times id="S2.SS2.p1.1.m1.3.3.2.cmml" xref="S2.SS2.p1.1.m1.3.3.2"></times><ci id="S2.SS2.p1.1.m1.3.3.3.cmml" xref="S2.SS2.p1.1.m1.3.3.3">𝑝</ci><apply id="S2.SS2.p1.1.m1.3.3.1.1.1.cmml" xref="S2.SS2.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S2.SS2.p1.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS2.p1.1.m1.3.3.1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.3.3.1.1.1.2">𝑞</ci><list id="S2.SS2.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S2.SS2.p1.1.m1.3.3.1.1.1.3.2"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝑎</ci><ci id="S2.SS2.p1.1.m1.2.2.cmml" xref="S2.SS2.p1.1.m1.2.2">𝑐</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.3c">p(q|a,c)</annotation></semantics></math> using a pretrained GPT-2 model. As input to our model, we concatenate context tokens, answer tokens, and question tokens into a single sequence, separated by the end of sequence tokens. We use three segment type embeddings to help the GPT-2 decoder model distinguish between different parts of the input. This method of multi-input controlled text generation draws on inspiration from prior work <cite class="ltx_cite ltx_citemacro_citep">(Puri &amp; Catanzaro, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>; Raffel et al., <a href="#bib.bib27" title="" class="ltx_ref">2019</a>; Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>. We also use answer segment type embeddings to highlight the presence of the answer span in the provided context tokens. We trained this question generation model with a left to right next token prediction loss modeled over the entire concatenated sequence. Visualizations of the input representation and training loss can be found in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.2 Question Generation: 𝑞̂∼𝑝⁢(𝑞|{𝑎̂,𝑐}) ‣ 2 Method ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. To sample from our learned model we concatenate the context tokens with the answer tokens and autoregressively sample output question tokens.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To aid our model with generation we employ start and stop word filtration. We prepend ‘<span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">question:</span>’ and append ‘<span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_italic">:question</span>’ tokens to the questions in our training dataset. During inference time, if the model does not sample a sequence containing both the start and stop words we discard the example entirely.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2002.09599/assets/qgen_inp_rep_01.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="491" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Question Generation input representation and language modeling loss. Answer type embeddings highlight the answer’s presence in the text.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Roundtrip Filtration: <math id="S2.SS3.1.m1.3" class="ltx_Math" alttext="\hat{a}\stackrel{{\scriptstyle?}}{{=}}a^{*}\sim p(a|c,\hat{q})" display="inline"><semantics id="S2.SS3.1.m1.3b"><mrow id="S2.SS3.1.m1.3.3" xref="S2.SS3.1.m1.3.3.cmml"><mover accent="true" id="S2.SS3.1.m1.3.3.3" xref="S2.SS3.1.m1.3.3.3.cmml"><mi id="S2.SS3.1.m1.3.3.3.2" xref="S2.SS3.1.m1.3.3.3.2.cmml">a</mi><mo id="S2.SS3.1.m1.3.3.3.1" xref="S2.SS3.1.m1.3.3.3.1.cmml">^</mo></mover><mover id="S2.SS3.1.m1.3.3.4" xref="S2.SS3.1.m1.3.3.4.cmml"><mo id="S2.SS3.1.m1.3.3.4.2" xref="S2.SS3.1.m1.3.3.4.2.cmml">=</mo><mi mathvariant="normal" id="S2.SS3.1.m1.3.3.4.3" xref="S2.SS3.1.m1.3.3.4.3.cmml">?</mi></mover><msup id="S2.SS3.1.m1.3.3.5" xref="S2.SS3.1.m1.3.3.5.cmml"><mi id="S2.SS3.1.m1.3.3.5.2" xref="S2.SS3.1.m1.3.3.5.2.cmml">a</mi><mo id="S2.SS3.1.m1.3.3.5.3" xref="S2.SS3.1.m1.3.3.5.3.cmml">∗</mo></msup><mo id="S2.SS3.1.m1.3.3.6" xref="S2.SS3.1.m1.3.3.6.cmml">∼</mo><mrow id="S2.SS3.1.m1.3.3.1" xref="S2.SS3.1.m1.3.3.1.cmml"><mi id="S2.SS3.1.m1.3.3.1.3" xref="S2.SS3.1.m1.3.3.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS3.1.m1.3.3.1.2" xref="S2.SS3.1.m1.3.3.1.2.cmml">​</mo><mrow id="S2.SS3.1.m1.3.3.1.1.1" xref="S2.SS3.1.m1.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.1.m1.3.3.1.1.1.2" xref="S2.SS3.1.m1.3.3.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.1.m1.3.3.1.1.1.1" xref="S2.SS3.1.m1.3.3.1.1.1.1.cmml"><mi id="S2.SS3.1.m1.3.3.1.1.1.1.2" xref="S2.SS3.1.m1.3.3.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS3.1.m1.3.3.1.1.1.1.1" xref="S2.SS3.1.m1.3.3.1.1.1.1.1.cmml">|</mo><mrow id="S2.SS3.1.m1.3.3.1.1.1.1.3.2" xref="S2.SS3.1.m1.3.3.1.1.1.1.3.1.cmml"><mi id="S2.SS3.1.m1.1.1" xref="S2.SS3.1.m1.1.1.cmml">c</mi><mo id="S2.SS3.1.m1.3.3.1.1.1.1.3.2.1" xref="S2.SS3.1.m1.3.3.1.1.1.1.3.1.cmml">,</mo><mover accent="true" id="S2.SS3.1.m1.2.2" xref="S2.SS3.1.m1.2.2.cmml"><mi id="S2.SS3.1.m1.2.2.2" xref="S2.SS3.1.m1.2.2.2.cmml">q</mi><mo id="S2.SS3.1.m1.2.2.1" xref="S2.SS3.1.m1.2.2.1.cmml">^</mo></mover></mrow></mrow><mo stretchy="false" id="S2.SS3.1.m1.3.3.1.1.1.3" xref="S2.SS3.1.m1.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.1.m1.3c"><apply id="S2.SS3.1.m1.3.3.cmml" xref="S2.SS3.1.m1.3.3"><and id="S2.SS3.1.m1.3.3a.cmml" xref="S2.SS3.1.m1.3.3"></and><apply id="S2.SS3.1.m1.3.3b.cmml" xref="S2.SS3.1.m1.3.3"><apply id="S2.SS3.1.m1.3.3.4.cmml" xref="S2.SS3.1.m1.3.3.4"><csymbol cd="ambiguous" id="S2.SS3.1.m1.3.3.4.1.cmml" xref="S2.SS3.1.m1.3.3.4">superscript</csymbol><eq id="S2.SS3.1.m1.3.3.4.2.cmml" xref="S2.SS3.1.m1.3.3.4.2"></eq><ci id="S2.SS3.1.m1.3.3.4.3.cmml" xref="S2.SS3.1.m1.3.3.4.3">?</ci></apply><apply id="S2.SS3.1.m1.3.3.3.cmml" xref="S2.SS3.1.m1.3.3.3"><ci id="S2.SS3.1.m1.3.3.3.1.cmml" xref="S2.SS3.1.m1.3.3.3.1">^</ci><ci id="S2.SS3.1.m1.3.3.3.2.cmml" xref="S2.SS3.1.m1.3.3.3.2">𝑎</ci></apply><apply id="S2.SS3.1.m1.3.3.5.cmml" xref="S2.SS3.1.m1.3.3.5"><csymbol cd="ambiguous" id="S2.SS3.1.m1.3.3.5.1.cmml" xref="S2.SS3.1.m1.3.3.5">superscript</csymbol><ci id="S2.SS3.1.m1.3.3.5.2.cmml" xref="S2.SS3.1.m1.3.3.5.2">𝑎</ci><times id="S2.SS3.1.m1.3.3.5.3.cmml" xref="S2.SS3.1.m1.3.3.5.3"></times></apply></apply><apply id="S2.SS3.1.m1.3.3c.cmml" xref="S2.SS3.1.m1.3.3"><csymbol cd="latexml" id="S2.SS3.1.m1.3.3.6.cmml" xref="S2.SS3.1.m1.3.3.6">similar-to</csymbol><share href="#S2.SS3.1.m1.3.3.5.cmml" id="S2.SS3.1.m1.3.3d.cmml" xref="S2.SS3.1.m1.3.3"></share><apply id="S2.SS3.1.m1.3.3.1.cmml" xref="S2.SS3.1.m1.3.3.1"><times id="S2.SS3.1.m1.3.3.1.2.cmml" xref="S2.SS3.1.m1.3.3.1.2"></times><ci id="S2.SS3.1.m1.3.3.1.3.cmml" xref="S2.SS3.1.m1.3.3.1.3">𝑝</ci><apply id="S2.SS3.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS3.1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S2.SS3.1.m1.3.3.1.1.1.1.1.cmml" xref="S2.SS3.1.m1.3.3.1.1.1.1.1">conditional</csymbol><ci id="S2.SS3.1.m1.3.3.1.1.1.1.2.cmml" xref="S2.SS3.1.m1.3.3.1.1.1.1.2">𝑎</ci><list id="S2.SS3.1.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.SS3.1.m1.3.3.1.1.1.1.3.2"><ci id="S2.SS3.1.m1.1.1.cmml" xref="S2.SS3.1.m1.1.1">𝑐</ci><apply id="S2.SS3.1.m1.2.2.cmml" xref="S2.SS3.1.m1.2.2"><ci id="S2.SS3.1.m1.2.2.1.cmml" xref="S2.SS3.1.m1.2.2.1">^</ci><ci id="S2.SS3.1.m1.2.2.2.cmml" xref="S2.SS3.1.m1.2.2.2">𝑞</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.1.m1.3d">\hat{a}\stackrel{{\scriptstyle?}}{{=}}a^{*}\sim p(a|c,\hat{q})</annotation></semantics></math>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.5" class="ltx_p">In roundtrip filtration <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite> an extractive question answering model <math id="S2.SS3.p1.1.m1.3" class="ltx_Math" alttext="p(a|c,q)" display="inline"><semantics id="S2.SS3.p1.1.m1.3a"><mrow id="S2.SS3.p1.1.m1.3.3" xref="S2.SS3.p1.1.m1.3.3.cmml"><mi id="S2.SS3.p1.1.m1.3.3.3" xref="S2.SS3.p1.1.m1.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.1.m1.3.3.2" xref="S2.SS3.p1.1.m1.3.3.2.cmml">​</mo><mrow id="S2.SS3.p1.1.m1.3.3.1.1" xref="S2.SS3.p1.1.m1.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.1.m1.3.3.1.1.2" xref="S2.SS3.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.1.m1.3.3.1.1.1" xref="S2.SS3.p1.1.m1.3.3.1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.3.3.1.1.1.2" xref="S2.SS3.p1.1.m1.3.3.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS3.p1.1.m1.3.3.1.1.1.1" xref="S2.SS3.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS3.p1.1.m1.3.3.1.1.1.3.2" xref="S2.SS3.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">c</mi><mo id="S2.SS3.p1.1.m1.3.3.1.1.1.3.2.1" xref="S2.SS3.p1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S2.SS3.p1.1.m1.2.2" xref="S2.SS3.p1.1.m1.2.2.cmml">q</mi></mrow></mrow><mo stretchy="false" id="S2.SS3.p1.1.m1.3.3.1.1.3" xref="S2.SS3.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.3b"><apply id="S2.SS3.p1.1.m1.3.3.cmml" xref="S2.SS3.p1.1.m1.3.3"><times id="S2.SS3.p1.1.m1.3.3.2.cmml" xref="S2.SS3.p1.1.m1.3.3.2"></times><ci id="S2.SS3.p1.1.m1.3.3.3.cmml" xref="S2.SS3.p1.1.m1.3.3.3">𝑝</ci><apply id="S2.SS3.p1.1.m1.3.3.1.1.1.cmml" xref="S2.SS3.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S2.SS3.p1.1.m1.3.3.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.1.m1.3.3.1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.3.3.1.1.1.2">𝑎</ci><list id="S2.SS3.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S2.SS3.p1.1.m1.3.3.1.1.1.3.2"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑐</ci><ci id="S2.SS3.p1.1.m1.2.2.cmml" xref="S2.SS3.p1.1.m1.2.2">𝑞</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.3c">p(a|c,q)</annotation></semantics></math> is trained on the available labeled data. When a new question, answer, and context triple <math id="S2.SS3.p1.2.m2.3" class="ltx_Math" alttext="(c,\hat{q},\hat{a})" display="inline"><semantics id="S2.SS3.p1.2.m2.3a"><mrow id="S2.SS3.p1.2.m2.3.4.2" xref="S2.SS3.p1.2.m2.3.4.1.cmml"><mo stretchy="false" id="S2.SS3.p1.2.m2.3.4.2.1" xref="S2.SS3.p1.2.m2.3.4.1.cmml">(</mo><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">c</mi><mo id="S2.SS3.p1.2.m2.3.4.2.2" xref="S2.SS3.p1.2.m2.3.4.1.cmml">,</mo><mover accent="true" id="S2.SS3.p1.2.m2.2.2" xref="S2.SS3.p1.2.m2.2.2.cmml"><mi id="S2.SS3.p1.2.m2.2.2.2" xref="S2.SS3.p1.2.m2.2.2.2.cmml">q</mi><mo id="S2.SS3.p1.2.m2.2.2.1" xref="S2.SS3.p1.2.m2.2.2.1.cmml">^</mo></mover><mo id="S2.SS3.p1.2.m2.3.4.2.3" xref="S2.SS3.p1.2.m2.3.4.1.cmml">,</mo><mover accent="true" id="S2.SS3.p1.2.m2.3.3" xref="S2.SS3.p1.2.m2.3.3.cmml"><mi id="S2.SS3.p1.2.m2.3.3.2" xref="S2.SS3.p1.2.m2.3.3.2.cmml">a</mi><mo id="S2.SS3.p1.2.m2.3.3.1" xref="S2.SS3.p1.2.m2.3.3.1.cmml">^</mo></mover><mo stretchy="false" id="S2.SS3.p1.2.m2.3.4.2.4" xref="S2.SS3.p1.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.3b"><vector id="S2.SS3.p1.2.m2.3.4.1.cmml" xref="S2.SS3.p1.2.m2.3.4.2"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">𝑐</ci><apply id="S2.SS3.p1.2.m2.2.2.cmml" xref="S2.SS3.p1.2.m2.2.2"><ci id="S2.SS3.p1.2.m2.2.2.1.cmml" xref="S2.SS3.p1.2.m2.2.2.1">^</ci><ci id="S2.SS3.p1.2.m2.2.2.2.cmml" xref="S2.SS3.p1.2.m2.2.2.2">𝑞</ci></apply><apply id="S2.SS3.p1.2.m2.3.3.cmml" xref="S2.SS3.p1.2.m2.3.3"><ci id="S2.SS3.p1.2.m2.3.3.1.cmml" xref="S2.SS3.p1.2.m2.3.3.1">^</ci><ci id="S2.SS3.p1.2.m2.3.3.2.cmml" xref="S2.SS3.p1.2.m2.3.3.2">𝑎</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.3c">(c,\hat{q},\hat{a})</annotation></semantics></math> is generated we apply the QA filtration model <math id="S2.SS3.p1.3.m3.3" class="ltx_Math" alttext="p(a|c,\hat{q})" display="inline"><semantics id="S2.SS3.p1.3.m3.3a"><mrow id="S2.SS3.p1.3.m3.3.3" xref="S2.SS3.p1.3.m3.3.3.cmml"><mi id="S2.SS3.p1.3.m3.3.3.3" xref="S2.SS3.p1.3.m3.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.3.m3.3.3.2" xref="S2.SS3.p1.3.m3.3.3.2.cmml">​</mo><mrow id="S2.SS3.p1.3.m3.3.3.1.1" xref="S2.SS3.p1.3.m3.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.3.m3.3.3.1.1.2" xref="S2.SS3.p1.3.m3.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.3.m3.3.3.1.1.1" xref="S2.SS3.p1.3.m3.3.3.1.1.1.cmml"><mi id="S2.SS3.p1.3.m3.3.3.1.1.1.2" xref="S2.SS3.p1.3.m3.3.3.1.1.1.2.cmml">a</mi><mo fence="false" id="S2.SS3.p1.3.m3.3.3.1.1.1.1" xref="S2.SS3.p1.3.m3.3.3.1.1.1.1.cmml">|</mo><mrow id="S2.SS3.p1.3.m3.3.3.1.1.1.3.2" xref="S2.SS3.p1.3.m3.3.3.1.1.1.3.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">c</mi><mo id="S2.SS3.p1.3.m3.3.3.1.1.1.3.2.1" xref="S2.SS3.p1.3.m3.3.3.1.1.1.3.1.cmml">,</mo><mover accent="true" id="S2.SS3.p1.3.m3.2.2" xref="S2.SS3.p1.3.m3.2.2.cmml"><mi id="S2.SS3.p1.3.m3.2.2.2" xref="S2.SS3.p1.3.m3.2.2.2.cmml">q</mi><mo id="S2.SS3.p1.3.m3.2.2.1" xref="S2.SS3.p1.3.m3.2.2.1.cmml">^</mo></mover></mrow></mrow><mo stretchy="false" id="S2.SS3.p1.3.m3.3.3.1.1.3" xref="S2.SS3.p1.3.m3.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.3b"><apply id="S2.SS3.p1.3.m3.3.3.cmml" xref="S2.SS3.p1.3.m3.3.3"><times id="S2.SS3.p1.3.m3.3.3.2.cmml" xref="S2.SS3.p1.3.m3.3.3.2"></times><ci id="S2.SS3.p1.3.m3.3.3.3.cmml" xref="S2.SS3.p1.3.m3.3.3.3">𝑝</ci><apply id="S2.SS3.p1.3.m3.3.3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.3.3.1.1"><csymbol cd="latexml" id="S2.SS3.p1.3.m3.3.3.1.1.1.1.cmml" xref="S2.SS3.p1.3.m3.3.3.1.1.1.1">conditional</csymbol><ci id="S2.SS3.p1.3.m3.3.3.1.1.1.2.cmml" xref="S2.SS3.p1.3.m3.3.3.1.1.1.2">𝑎</ci><list id="S2.SS3.p1.3.m3.3.3.1.1.1.3.1.cmml" xref="S2.SS3.p1.3.m3.3.3.1.1.1.3.2"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">𝑐</ci><apply id="S2.SS3.p1.3.m3.2.2.cmml" xref="S2.SS3.p1.3.m3.2.2"><ci id="S2.SS3.p1.3.m3.2.2.1.cmml" xref="S2.SS3.p1.3.m3.2.2.1">^</ci><ci id="S2.SS3.p1.3.m3.2.2.2.cmml" xref="S2.SS3.p1.3.m3.2.2.2">𝑞</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.3c">p(a|c,\hat{q})</annotation></semantics></math> to the context and question. The resulting answer <math id="S2.SS3.p1.4.m4.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S2.SS3.p1.4.m4.1a"><msup id="S2.SS3.p1.4.m4.1.1" xref="S2.SS3.p1.4.m4.1.1.cmml"><mi id="S2.SS3.p1.4.m4.1.1.2" xref="S2.SS3.p1.4.m4.1.1.2.cmml">a</mi><mo id="S2.SS3.p1.4.m4.1.1.3" xref="S2.SS3.p1.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.4.m4.1b"><apply id="S2.SS3.p1.4.m4.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.4.m4.1.1.1.cmml" xref="S2.SS3.p1.4.m4.1.1">superscript</csymbol><ci id="S2.SS3.p1.4.m4.1.1.2.cmml" xref="S2.SS3.p1.4.m4.1.1.2">𝑎</ci><times id="S2.SS3.p1.4.m4.1.1.3.cmml" xref="S2.SS3.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.4.m4.1c">a^{*}</annotation></semantics></math> from the model is compared to the answer <math id="S2.SS3.p1.5.m5.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S2.SS3.p1.5.m5.1a"><mover accent="true" id="S2.SS3.p1.5.m5.1.1" xref="S2.SS3.p1.5.m5.1.1.cmml"><mi id="S2.SS3.p1.5.m5.1.1.2" xref="S2.SS3.p1.5.m5.1.1.2.cmml">a</mi><mo id="S2.SS3.p1.5.m5.1.1.1" xref="S2.SS3.p1.5.m5.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.5.m5.1b"><apply id="S2.SS3.p1.5.m5.1.1.cmml" xref="S2.SS3.p1.5.m5.1.1"><ci id="S2.SS3.p1.5.m5.1.1.1.cmml" xref="S2.SS3.p1.5.m5.1.1.1">^</ci><ci id="S2.SS3.p1.5.m5.1.1.2.cmml" xref="S2.SS3.p1.5.m5.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.5.m5.1c">\hat{a}</annotation></semantics></math> from the triple. If the two are equivalent then the question is considered admissible.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In the original work, however, the authors draw attention to the precision of the method. While it does discard invalid questions, several valid questions are discarded as well. To avoid losing valuable pieces of information to train our question answering models we propose generating two questions, instead of one question, for each candidate answer. Roundtrip filtration is then applied to each question individually. If a triple is decided as acceptable then it is kept regardless of whether the other triple is acceptable, leading to a scenario where both can be kept. This method is similar to prior work in overgeneration and reranking of generated questions <cite class="ltx_cite ltx_citemacro_citep">(Heilman &amp; Smith, <a href="#bib.bib15" title="" class="ltx_ref">2010b</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment Setup</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2002.09599/assets/qgen_flowchart.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="287" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Data flow for training and evaluating question generation pipeline.
</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.2" class="ltx_p">For all implementations and training of transformer models we rely on the Megatron-LM codebase <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>. For off-the-shelf weights and implementations of BERT-Large we rely on the HuggingFace’s transformers codebase <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a href="#bib.bib37" title="" class="ltx_ref">2019</a>)</cite>. The GPT-2 models <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> used for question generation were each pretrained on the 174GB corpora used in Megatron-LM: Wikipedia <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, OpenWebText <cite class="ltx_cite ltx_citemacro_citep">(Gokaslan &amp; Cohen, <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>, RealNews <cite class="ltx_cite ltx_citemacro_citep">(Zellers et al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>, and CC-Stories <cite class="ltx_cite ltx_citemacro_citep">(Trinh &amp; Le, <a href="#bib.bib35" title="" class="ltx_ref">2018</a>)</cite>. Unless otherwise noted, our GPT-2 models were trained at a batch size of 512 for 300k iterations with 3k iterations of warmup, Adamw <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov &amp; Hutter, <a href="#bib.bib23" title="" class="ltx_ref">2018</a>)</cite> for optimization, a learning rate of 1.5e-4 decaying linearly to 1e-5, weight decay of 0.01, global gradient norm clipping of <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn type="float" id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">1.0</annotation></semantics></math>, and a normal initialization of <math id="S3.p1.2.m2.2" class="ltx_Math" alttext="\theta\sim\mathcal{N}(0,0.02)" display="inline"><semantics id="S3.p1.2.m2.2a"><mrow id="S3.p1.2.m2.2.3" xref="S3.p1.2.m2.2.3.cmml"><mi id="S3.p1.2.m2.2.3.2" xref="S3.p1.2.m2.2.3.2.cmml">θ</mi><mo id="S3.p1.2.m2.2.3.1" xref="S3.p1.2.m2.2.3.1.cmml">∼</mo><mrow id="S3.p1.2.m2.2.3.3" xref="S3.p1.2.m2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.2.m2.2.3.3.2" xref="S3.p1.2.m2.2.3.3.2.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.p1.2.m2.2.3.3.1" xref="S3.p1.2.m2.2.3.3.1.cmml">​</mo><mrow id="S3.p1.2.m2.2.3.3.3.2" xref="S3.p1.2.m2.2.3.3.3.1.cmml"><mo stretchy="false" id="S3.p1.2.m2.2.3.3.3.2.1" xref="S3.p1.2.m2.2.3.3.3.1.cmml">(</mo><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">0</mn><mo id="S3.p1.2.m2.2.3.3.3.2.2" xref="S3.p1.2.m2.2.3.3.3.1.cmml">,</mo><mn id="S3.p1.2.m2.2.2" xref="S3.p1.2.m2.2.2.cmml">0.02</mn><mo stretchy="false" id="S3.p1.2.m2.2.3.3.3.2.3" xref="S3.p1.2.m2.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.2b"><apply id="S3.p1.2.m2.2.3.cmml" xref="S3.p1.2.m2.2.3"><csymbol cd="latexml" id="S3.p1.2.m2.2.3.1.cmml" xref="S3.p1.2.m2.2.3.1">similar-to</csymbol><ci id="S3.p1.2.m2.2.3.2.cmml" xref="S3.p1.2.m2.2.3.2">𝜃</ci><apply id="S3.p1.2.m2.2.3.3.cmml" xref="S3.p1.2.m2.2.3.3"><times id="S3.p1.2.m2.2.3.3.1.cmml" xref="S3.p1.2.m2.2.3.3.1"></times><ci id="S3.p1.2.m2.2.3.3.2.cmml" xref="S3.p1.2.m2.2.3.3.2">𝒩</ci><interval closure="open" id="S3.p1.2.m2.2.3.3.3.1.cmml" xref="S3.p1.2.m2.2.3.3.3.2"><cn type="integer" id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">0</cn><cn type="float" id="S3.p1.2.m2.2.2.cmml" xref="S3.p1.2.m2.2.2">0.02</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.2c">\theta\sim\mathcal{N}(0,0.02)</annotation></semantics></math>. Finetuning our GPT-2 models we used the same hyperparameters except for a batch size of 32 and a learning rate of 2e-5 decaying to zero over six epochs of finetuning data. For model configurations of hidden size, number of layers, and attention heads, we used the configurations detailed in Megatron-LM.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">To train our BERT models we relied on a pretraining regime similar to ALBERT. We used a n-gram masked language modeling task in conjunction with a sentence order prediction task. Unlike ALBERT we did not utilize weight sharing and we used a GPT-2 style ordering of residual connections and layer normalization. We found this greatly improved stability and allowed us to train significantly larger BERT models than prior work <cite class="ltx_cite ltx_citemacro_citep">(Lan et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> without encountering training instabilities and overfitting. We trained our BERT models with the same hyperparameters as GPT-2 except using learning rate of 1e-4 and a batch size of 1024 over 2 million iterations with 10k iterations of warmup. Finetuning our BERT models for filtration, answer generation, filtration, and question answering was all done with a learning rate of 1e-5 and a cosine decay schedule over 2 epochs of training data. For our 1.2 billion parameter BERT model we used 24 layers, a hidden size of 2048, and 32 attention heads. We refer to our models as BERT-345M and BERT-1.2B and the original BERT model as BERT-Large.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">To train and evaluate the whole question generation pipeline for our ablation studies in sections <a href="#S5" title="5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S6" title="6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we used a data partitioning scheme as detailed in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Experiment Setup ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. A similar data pipeline has been employed in concurrent work of <cite class="ltx_cite ltx_citemacro_citep">(Klein &amp; Nabi, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>. We split the <span id="S3.p3.1.1" class="ltx_text ltx_font_smallcaps">SQuAD </span>training data into equal portions, partitioning the data randomly into two sets of documents. One half of the documents is used to train the answer generator, question generator, and filtration models while the second half of the documents is used to generate synthetic data to finetune a QA model. The finetuned QA model is then evaluated on <span id="S3.p3.1.2" class="ltx_text ltx_font_smallcaps">SQuAD </span>dev set, where the evaluation results are used as a surrogate measure of synthetic data quality. The partitioning of the dataset is done to avoid leakage and overfitting between the data seen at training time and generation time thereby testing the generalization capabilities of our models. Since shuffling is done randomly we repeat this process 5 times with different seeds for every ablation study and report the mean of our results.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Lastly, all our models were trained with mixed precision training <cite class="ltx_cite ltx_citemacro_citep">(Micikevicius et al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite> on NVIDIA V100 GPUs. Pretraining took place on anywhere from 4 to 32 DGX-2H servers for our largest models. Finetuning only required one DGX-1V, except in the case of finetuning the 8.3B parameter question generator which required eight DGX-1Vs.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:252.8pt;height:105.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.0pt,10.4pt) scale(0.835,0.835) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">Text</td>
<td id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">Source</td>
<td id="S4.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">finetune</td>
<td id="S4.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.4.1" class="ltx_text"># Questions</span></td>
<td id="S4.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.5.1" class="ltx_text">EM</span></td>
<td id="S4.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.6.1" class="ltx_text">F1</span></td>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<td id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center">Source</td>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center">Data Size</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center">data</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<td id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.1.1.3.3.1.1" class="ltx_text">Wikipedia</span></td>
<td id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T2.1.1.3.3.2.1" class="ltx_text">638 MB</span></td>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">Synthetic</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_tt">19,925,130</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_tt">88.4</td>
<td id="S4.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_tt">94.1</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<td id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_center">+<span id="S4.T2.1.1.4.4.1.1" class="ltx_text ltx_font_smallcaps">SQuAD</span>
</td>
<td id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center">20,012,729</td>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.4.3.1" class="ltx_text ltx_font_bold">89.4</span></td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.4.4.1" class="ltx_text ltx_font_bold">95.2</span></td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<td id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.1.1.5.5.1.1" class="ltx_text">8.3B GPT-2</span></td>
<td id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T2.1.1.5.5.2.1" class="ltx_text">480 MB</span></td>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">Synthetic</td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">17,400,016</td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">88.4</td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
</tr>
<tr id="S4.T2.1.1.6.6" class="ltx_tr">
<td id="S4.T2.1.1.6.6.1" class="ltx_td ltx_align_center">+<span id="S4.T2.1.1.6.6.1.1" class="ltx_text ltx_font_smallcaps">SQuAD</span>
</td>
<td id="S4.T2.1.1.6.6.2" class="ltx_td ltx_align_center">17,487,615</td>
<td id="S4.T2.1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.6.6.3.1" class="ltx_text ltx_font_bold">89.1</span></td>
<td id="S4.T2.1.1.6.6.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.6.6.4.1" class="ltx_text ltx_font_bold">94.9</span></td>
</tr>
<tr id="S4.T2.1.1.7.7" class="ltx_tr">
<td id="S4.T2.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.7.7.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1</span></td>
<td id="S4.T2.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">14MB</td>
<td id="S4.T2.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t"><span id="S4.T2.1.1.7.7.3.1" class="ltx_text ltx_font_smallcaps">SQuAD</span></td>
<td id="S4.T2.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">87,599</td>
<td id="S4.T2.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">87.7</td>
<td id="S4.T2.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">94.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> Finetuning BERT-345M on synthetic and human-generated data. Using 1.2B parameter models we synthesize question answer pairs from real Wikipedia corpus and synthetic cospus generated from an 8.3B GPT-2 model. Completely synthetic data does better than training with real data. Finetuning with real <span id="S4.T2.3.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>data afterwards further boosts performance. </figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2002.09599/assets/icml_20_data_logscale.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="240" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Effect of labeling data size on downstream <span id="S4.F3.3.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>score. After finetuning BERT-345M models on synthetic data we finetune further on human generated <span id="S4.F3.4.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>data.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we present our results using the best combination of models, algorithms, and parameters. In the following sections, we will perform detailed ablation study and show contributions from each of these choices.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p">We train a 1.2 billion parameter answer generator, question generator, and question filtering model. In these experiments we use the entire <span id="S4.p2.2.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dataset instead of only training on half of the labeled data since we are not doing any model or hyperparameter search. We then use these models to label synthetic data from two sources outside of <span id="S4.p2.2.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>. We first label data from real Wikipedia documents with the overlapping documents from the <span id="S4.p2.2.3" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>training and dev set removed. In parallel we label data from synthetic Wikipedia documents generated by an 8.3B GPT-2 model. This model was first trained with the Megatron-LM codebase for 400k iterations before being finetuned on only Wikipedia documents for 2k iterations. This allows us to generate high quality text from a distribution similar to Wikipedia by using top-<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">p</annotation></semantics></math> (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="p=0.96" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">p</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">0.96</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><eq id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></eq><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝑝</ci><cn type="float" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">0.96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">p=0.96</annotation></semantics></math>) nucleus sampling.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows results when the synthetic data is finetuned on a BERT-345M QA model. We showcase that we are able to recover and surpass the performance of real data by only using synthetic data generated from synthetic corpus. Using real questions synthesized on real Wikipedia data we do even better. Finetuning this model afterwards on the actual <span id="S4.p3.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dataset allows us to achieve a 1.7 and 1.2 point boost to our EM and F1 scores. In Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Results ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we examine the relationship between <span id="S4.p3.1.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>score and the amount of text labeled. We find that the performance of training with purely synthetic data observes a <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\log" display="inline"><semantics id="S4.p3.1.m1.1a"><mi id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">log</mi><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><log id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"></log></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\log</annotation></semantics></math>-linear relationship that begins to saturate at approximately 100 MB of text labeled. However, finetuning these models on labeled <span id="S4.p3.1.3" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>data demonstrates continued improvement even beyond saturation. The performance of these post finetuned models continues to improve even past 500 MB of data labeled.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:259.3pt;height:150.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.6pt,14.9pt) scale(0.835,0.835) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Implementation</th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EM</th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.2.1" class="ltx_tr">
<td id="S4.T3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_tt">BERT-Large <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite>
</td>
<td id="S4.T3.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">78.7</td>
<td id="S4.T3.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">81.9</td>
</tr>
<tr id="S4.T3.1.1.3.2" class="ltx_tr">
<td id="S4.T3.1.1.3.2.1" class="ltx_td ltx_align_left">     + 3M Questions</td>
<td id="S4.T3.1.1.3.2.2" class="ltx_td ltx_align_center">80.1</td>
<td id="S4.T3.1.1.3.2.3" class="ltx_td ltx_align_center">82.8</td>
</tr>
<tr id="S4.T3.1.1.4.3" class="ltx_tr">
<td id="S4.T3.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_t">UniLM <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="S4.T3.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">80.5</td>
<td id="S4.T3.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">83.4</td>
</tr>
<tr id="S4.T3.1.1.5.4" class="ltx_tr">
<td id="S4.T3.1.1.5.4.1" class="ltx_td ltx_align_left">     + 9M Questions</td>
<td id="S4.T3.1.1.5.4.2" class="ltx_td ltx_align_center">84.7</td>
<td id="S4.T3.1.1.5.4.3" class="ltx_td ltx_align_center">87.6</td>
</tr>
<tr id="S4.T3.1.1.6.5" class="ltx_tr">
<td id="S4.T3.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_t">BERT-Large</td>
<td id="S4.T3.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_t">77.4</td>
<td id="S4.T3.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_t">80.6</td>
</tr>
<tr id="S4.T3.1.1.7.6" class="ltx_tr">
<td id="S4.T3.1.1.7.6.1" class="ltx_td ltx_align_left">     + 3M Questions</td>
<td id="S4.T3.1.1.7.6.2" class="ltx_td ltx_align_center">81.6</td>
<td id="S4.T3.1.1.7.6.3" class="ltx_td ltx_align_center">84.5</td>
</tr>
<tr id="S4.T3.1.1.8.7" class="ltx_tr">
<td id="S4.T3.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_t">BERT-345M</td>
<td id="S4.T3.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">84.9</td>
<td id="S4.T3.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">88.2</td>
</tr>
<tr id="S4.T3.1.1.9.8" class="ltx_tr">
<td id="S4.T3.1.1.9.8.1" class="ltx_td ltx_align_left">     + 3M Questions</td>
<td id="S4.T3.1.1.9.8.2" class="ltx_td ltx_align_center">85.8</td>
<td id="S4.T3.1.1.9.8.3" class="ltx_td ltx_align_center">88.6</td>
</tr>
<tr id="S4.T3.1.1.10.9" class="ltx_tr">
<td id="S4.T3.1.1.10.9.1" class="ltx_td ltx_align_left ltx_border_b">  <span id="S4.T3.1.1.10.9.1.1" class="ltx_text ltx_font_bold">   + 8M Questions</span>
</td>
<td id="S4.T3.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.10.9.2.1" class="ltx_text ltx_font_bold">86.4</span></td>
<td id="S4.T3.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.1.10.9.3.1" class="ltx_text ltx_font_bold">89.2</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span> Comparison with prior work.
Improvements in question generation allow for improved <span id="S4.T3.3.1" class="ltx_text ltx_font_smallcaps">SQuAD2.0 </span>score even without generating unanswerable questions.
</figcaption>
</figure>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Comparison with prior work.</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.1" class="ltx_p">To quantify the improvements in question generation quality derived from improvements to language models and our generation techniques we compare our results to the original roundtrip consistency work from <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite>. We generate 3 million questions from real Wikipedia text and finetune the public BERT-Large model on this data. We then finetune the model on the human-generated <span id="S4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD2.0 </span>dataset and evaluate on the dev set. Unlike prior work we do not generate any unanswerable questions, yet we find in Table <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> that our synthetic data approach outperforms the prior work. This is despite our BERT-Large baseline underperforming the numbers reported in <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite> by a full point. We also compare our methods with the state of the art in synthetically trained <span id="S4.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">SQuAD2.0 <cite class="ltx_cite ltx_citemacro_citep"><span id="S4.SS0.SSS0.Px1.p1.1.2.1.1" class="ltx_text ltx_font_upright">(</span>Dong et al.<span id="S4.SS0.SSS0.Px1.p1.1.2.2.2.1.1" class="ltx_text ltx_font_upright">, </span><a href="#bib.bib9" title="" class="ltx_ref">2019</a><span id="S4.SS0.SSS0.Px1.p1.1.2.3.3" class="ltx_text ltx_font_upright">)</span></cite></span> and find that with a similar number of questions we outperform existing methods, and with even more labeled data this trend persists.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Model Scale</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">A central premise of our work is that language models have advanced to the point where they are now able to generate diverse, coherent language and as a result can generate high quality question answering curricula. We show in this section that as we improve pretraining tasks, pretraining scale, and model scale, synthetic data also improves. To show improvements in question generation we track the resulting <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>evaluation score when a BERT-style model is finetuned on the synthetic data. Table <a href="#S5.T4" title="Table 4 ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes the benefits of using larger models for answer generation, question generation, and question filtration. The following subsections ablate this result to show the contributions from scaling individual components of the synthetic data pipeline.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<div id="S5.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:248.2pt;height:75.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.5pt,7.4pt) scale(0.835,0.835) ;">
<table id="S5.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="4">Model Size</th>
<th id="S5.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S5.T4.1.1.1.1.2.1" class="ltx_text"># Questions</span></th>
<th id="S5.T4.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S5.T4.1.1.1.1.3.1" class="ltx_text">EM</span></th>
<th id="S5.T4.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" rowspan="2"><span id="S5.T4.1.1.1.1.4.1" class="ltx_text">F1</span></th>
</tr>
<tr id="S5.T4.1.1.2.2" class="ltx_tr">
<th id="S5.T4.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Answer</th>
<th id="S5.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Question</th>
<th id="S5.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Filter</th>
<th id="S5.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">QA</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.3.1" class="ltx_tr">
<td id="S5.T4.1.1.3.1.1" class="ltx_td ltx_align_center ltx_border_tt">345M</td>
<td id="S5.T4.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_tt">345M</td>
<td id="S5.T4.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_tt">345M</td>
<td id="S5.T4.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">345M</td>
<td id="S5.T4.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_tt">116721</td>
<td id="S5.T4.1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_tt">85.3</td>
<td id="S5.T4.1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_tt">92.0</td>
</tr>
<tr id="S5.T4.1.1.4.2" class="ltx_tr">
<td id="S5.T4.1.1.4.2.1" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.4.2.1.1" class="ltx_text ltx_font_bold">1.2B</span></td>
<td id="S5.T4.1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.4.2.2.1" class="ltx_text ltx_font_bold">1.2B</span></td>
<td id="S5.T4.1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.4.2.3.1" class="ltx_text ltx_font_bold">1.2B</span></td>
<td id="S5.T4.1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.1.1.4.2.4.1" class="ltx_text ltx_font_bold">345M</span></td>
<td id="S5.T4.1.1.4.2.5" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.4.2.5.1" class="ltx_text ltx_font_bold">184992</span></td>
<td id="S5.T4.1.1.4.2.6" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.4.2.6.1" class="ltx_text ltx_font_bold">87.1</span></td>
<td id="S5.T4.1.1.4.2.7" class="ltx_td ltx_align_center"><span id="S5.T4.1.1.4.2.7.1" class="ltx_text ltx_font_bold">93.2</span></td>
</tr>
<tr id="S5.T4.1.1.5.3" class="ltx_tr">
<td id="S5.T4.1.1.5.3.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" colspan="3">Human Generated Data</td>
<td id="S5.T4.1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">345M</td>
<td id="S5.T4.1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">42472</td>
<td id="S5.T4.1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.3</td>
<td id="S5.T4.1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">93.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span> <span id="S5.T4.3.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>performance using synthetic data. Downstream QA models used in all experiments are 345M parameters.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Scaling Question Generation</h3>

<figure id="S5.T5" class="ltx_table">
<div id="S5.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:333.1pt;height:135.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.9pt,13.4pt) scale(0.835,0.835) ;">
<table id="S5.T5.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T5.1.1.1.1" class="ltx_tr">
<th id="S5.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Question Generator</th>
<th id="S5.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"># Questions</th>
<th id="S5.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EM</th>
<th id="S5.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T5.1.1.2.1" class="ltx_tr">
<th id="S5.T5.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">117M</th>
<th id="S5.T5.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">42345</th>
<td id="S5.T5.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">76.6</td>
<td id="S5.T5.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">85.0</td>
</tr>
<tr id="S5.T5.1.1.3.2" class="ltx_tr">
<th id="S5.T5.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">345M <cite class="ltx_cite ltx_citemacro_citep">(Klein &amp; Nabi, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>
</th>
<th id="S5.T5.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">-</th>
<td id="S5.T5.1.1.3.2.3" class="ltx_td ltx_align_center">75.4</td>
<td id="S5.T5.1.1.3.2.4" class="ltx_td ltx_align_center">84.4</td>
</tr>
<tr id="S5.T5.1.1.4.3" class="ltx_tr">
<th id="S5.T5.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">345M (w/ BERT QA model)</th>
<th id="S5.T5.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">42414</th>
<td id="S5.T5.1.1.4.3.3" class="ltx_td ltx_align_center">76.6</td>
<td id="S5.T5.1.1.4.3.4" class="ltx_td ltx_align_center">84.8</td>
</tr>
<tr id="S5.T5.1.1.5.4" class="ltx_tr">
<th id="S5.T5.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">345M</th>
<th id="S5.T5.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">42414</th>
<td id="S5.T5.1.1.5.4.3" class="ltx_td ltx_align_center">80.7</td>
<td id="S5.T5.1.1.5.4.4" class="ltx_td ltx_align_center">88.6</td>
</tr>
<tr id="S5.T5.1.1.6.5" class="ltx_tr">
<th id="S5.T5.1.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">768M</th>
<th id="S5.T5.1.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">42465</th>
<td id="S5.T5.1.1.6.5.3" class="ltx_td ltx_align_center">81.0</td>
<td id="S5.T5.1.1.6.5.4" class="ltx_td ltx_align_center">89.0</td>
</tr>
<tr id="S5.T5.1.1.7.6" class="ltx_tr">
<th id="S5.T5.1.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">1.2B</th>
<th id="S5.T5.1.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">42472</th>
<td id="S5.T5.1.1.7.6.3" class="ltx_td ltx_align_center">83.4</td>
<td id="S5.T5.1.1.7.6.4" class="ltx_td ltx_align_center">90.9</td>
</tr>
<tr id="S5.T5.1.1.8.7" class="ltx_tr">
<th id="S5.T5.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T5.1.1.8.7.1.1" class="ltx_text ltx_font_bold">8.3B</span></th>
<th id="S5.T5.1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T5.1.1.8.7.2.1" class="ltx_text ltx_font_bold">42478</span></th>
<td id="S5.T5.1.1.8.7.3" class="ltx_td ltx_align_center"><span id="S5.T5.1.1.8.7.3.1" class="ltx_text ltx_font_bold">84.9</span></td>
<td id="S5.T5.1.1.8.7.4" class="ltx_td ltx_align_center"><span id="S5.T5.1.1.8.7.4.1" class="ltx_text ltx_font_bold">92.0</span></td>
</tr>
<tr id="S5.T5.1.1.9.8" class="ltx_tr">
<th id="S5.T5.1.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">Human Generated Data</th>
<th id="S5.T5.1.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">42472</th>
<td id="S5.T5.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.3</td>
<td id="S5.T5.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">93.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span> Effect of question generator scale on <span id="S5.T5.3.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>performance. Ground truth answers are used to generate questions without filtration for finetuning.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Question generation plays a critical role in our synthetic data pipeline: it must synthesize linguistically and logically coherent text even if the text does not exist within the provided context. In this section we investigate the relationship between question generator scale and downstream <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>performance. We isolate the quality of question generation by using ground truth answers from the <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dataset to generate questions and finetune a BERT model before evaluating it on the <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dev set. We perform no question filtration in between generation and finetuning. From our experiments in Table <a href="#S5.T5" title="Table 5 ‣ 5.1 Scaling Question Generation ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we find that <span id="S5.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>performance increases monotonically. Additionally, the number of valid samples that pass stopword filtration increase with larger models, indicating bigger models maintain coherency during sampling. For comparisons with prior work we train a question answering model with our ALBERT-style BERT model (BERT-345M) and the original BERT-Large model. <cite class="ltx_cite ltx_citemacro_citep">(Klein &amp; Nabi, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> use a feedback loop to improve the question generator and BERT-Large question answering model. Compared to our work we find that a similarly parameterized set of models achieve equal if not better performance despite using only a single supervised pass through the data and no feedback loop.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Scaling Answer Generation</h3>

<figure id="S5.T6" class="ltx_table">
<div id="S5.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:215.5pt;height:75.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.3pt,7.4pt) scale(0.835,0.835) ;">
<table id="S5.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T6.1.1.1.1" class="ltx_tr">
<th id="S5.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Answer Generator</th>
<th id="S5.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">#Questions</th>
<th id="S5.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EM</th>
<th id="S5.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.2.1" class="ltx_tr">
<th id="S5.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">BERT-Large</th>
<th id="S5.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt">227063</th>
<td id="S5.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">77.7</td>
<td id="S5.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">87.6</td>
</tr>
<tr id="S5.T6.1.1.3.2" class="ltx_tr">
<th id="S5.T6.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">BERT-345M</th>
<th id="S5.T6.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">229297</th>
<td id="S5.T6.1.1.3.2.3" class="ltx_td ltx_align_center">79.1</td>
<td id="S5.T6.1.1.3.2.4" class="ltx_td ltx_align_center">87.9</td>
</tr>
<tr id="S5.T6.1.1.4.3" class="ltx_tr">
<th id="S5.T6.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T6.1.1.4.3.1.1" class="ltx_text ltx_font_bold">BERT-1.2B</span></th>
<th id="S5.T6.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T6.1.1.4.3.2.1" class="ltx_text ltx_font_bold">229067</span></th>
<td id="S5.T6.1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T6.1.1.4.3.3.1" class="ltx_text ltx_font_bold">79.2</span></td>
<td id="S5.T6.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T6.1.1.4.3.4.1" class="ltx_text ltx_font_bold">88.3</span></td>
</tr>
<tr id="S5.T6.1.1.5.4" class="ltx_tr">
<th id="S5.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">Human Generated Answers</th>
<th id="S5.T6.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">42472</th>
<td id="S5.T6.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">83.7</td>
<td id="S5.T6.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">91.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span> Comparison of answer generator pretraining and scale.
Our 1.2 billion parameter question generator is used for generating questions.
</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Answer generation is equally important in our data generation pipeline. Answer generation is the first component of the pipeline and must be precise to avoid compounding errors. For answer generation we use an unconditional extractive BERT model that predicts start and end spans jointly over a given sentence. From each probability distribution we sample the entire nucleus (<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><eq id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></eq><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑝</ci><cn type="float" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">p=0.9</annotation></semantics></math>) or the top-5 spans, choosing whichever is smaller. We arrive at this implementation based on our ablation studies in section <a href="#S6.SS3" title="6.3 Question Filtration ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>. To test the quality of the selected answers we generate questions from our 1.2 billion parameter question generator and finetune a question answering model on the synthesized questions without any filtration. In Table <a href="#S5.T6" title="Table 6 ‣ 5.2 Scaling Answer Generation ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we compare answer generation quality using our two trained models and the original BERT-Large model from <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>. We find that improvements in pretraining data and tasks dramatically improve answer generation quality by 1.4 EM and 0.3 F1 between BERT-Large and our 345 million parameter answer generation model. We find that increasing model scale further to 1.2 billion parameters improves answer generation quality F1 by 0.4 while EM only improves by 0.1.
Although these represent improvements in question quality only achieved by newer models, answer generation seems to be a large bottleneck as we discuss in section <a href="#S6.SS1" title="6.1 Question Generation ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Scaling Question Filtration</h3>

<figure id="S5.T7" class="ltx_table">
<div id="S5.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:206.0pt;height:150.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-20.4pt,14.9pt) scale(0.835,0.835) ;">
<table id="S5.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T7.1.1.1.1" class="ltx_tr">
<th id="S5.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t">Filter Model</th>
<th id="S5.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t"># Questions</th>
<th id="S5.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EM</th>
<th id="S5.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1</th>
</tr>
<tr id="S5.T7.1.1.2.2" class="ltx_tr">
<th id="S5.T7.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" colspan="4">Synthetic Questions + Real Answers</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.3.1" class="ltx_tr">
<th id="S5.T7.1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">BERT-Large</th>
<th id="S5.T7.1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">45888</th>
<td id="S5.T7.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">84.5</td>
<td id="S5.T7.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">91.4</td>
</tr>
<tr id="S5.T7.1.1.4.2" class="ltx_tr">
<th id="S5.T7.1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">BERT-345M</th>
<th id="S5.T7.1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">34341</th>
<td id="S5.T7.1.1.4.2.3" class="ltx_td ltx_align_center">84.2</td>
<td id="S5.T7.1.1.4.2.4" class="ltx_td ltx_align_center">91.4</td>
</tr>
<tr id="S5.T7.1.1.5.3" class="ltx_tr">
<th id="S5.T7.1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T7.1.1.5.3.1.1" class="ltx_text ltx_font_bold">BERT-1.2B</span></th>
<th id="S5.T7.1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T7.1.1.5.3.2.1" class="ltx_text ltx_font_bold">47772</span></th>
<td id="S5.T7.1.1.5.3.3" class="ltx_td ltx_align_center"><span id="S5.T7.1.1.5.3.3.1" class="ltx_text ltx_font_bold">85.6</span></td>
<td id="S5.T7.1.1.5.3.4" class="ltx_td ltx_align_center"><span id="S5.T7.1.1.5.3.4.1" class="ltx_text ltx_font_bold">92.4</span></td>
</tr>
<tr id="S5.T7.1.1.6.4" class="ltx_tr">
<th id="S5.T7.1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="4">Synthetic Questions + Synthetic Answers</th>
</tr>
<tr id="S5.T7.1.1.7.5" class="ltx_tr">
<th id="S5.T7.1.1.7.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">BERT-Large</th>
<th id="S5.T7.1.1.7.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">177712</th>
<td id="S5.T7.1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_t">85.5</td>
<td id="S5.T7.1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_t">91.9</td>
</tr>
<tr id="S5.T7.1.1.8.6" class="ltx_tr">
<th id="S5.T7.1.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">BERT-345M</th>
<th id="S5.T7.1.1.8.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">144322</th>
<td id="S5.T7.1.1.8.6.3" class="ltx_td ltx_align_center">85.9</td>
<td id="S5.T7.1.1.8.6.4" class="ltx_td ltx_align_center">92.5</td>
</tr>
<tr id="S5.T7.1.1.9.7" class="ltx_tr">
<th id="S5.T7.1.1.9.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T7.1.1.9.7.1.1" class="ltx_text ltx_font_bold">BERT-1.2B</span></th>
<th id="S5.T7.1.1.9.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S5.T7.1.1.9.7.2.1" class="ltx_text ltx_font_bold">184992</span></th>
<td id="S5.T7.1.1.9.7.3" class="ltx_td ltx_align_center"><span id="S5.T7.1.1.9.7.3.1" class="ltx_text ltx_font_bold">87.1</span></td>
<td id="S5.T7.1.1.9.7.4" class="ltx_td ltx_align_center"><span id="S5.T7.1.1.9.7.4.1" class="ltx_text ltx_font_bold">93.2</span></td>
</tr>
<tr id="S5.T7.1.1.10.8" class="ltx_tr">
<th id="S5.T7.1.1.10.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">Human Generated Data</th>
<th id="S5.T7.1.1.10.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t">42472</th>
<td id="S5.T7.1.1.10.8.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.3</td>
<td id="S5.T7.1.1.10.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">93.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span> Effect of pretraining and scale on question filtration. Synthetic questions and answers were both generated with 1.2 billion parameter models. Before finetuning, overgeneration and filtration were performed with the models ablated here.
</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We use the 1.2 billion parameter question generator from section <a href="#S5.SS1" title="5.1 Scaling Question Generation ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> to generate questions for filtration. As described in more detail in section <a href="#S6.SS3" title="6.3 Question Filtration ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a> we overgenerate two questions for every answer. We then filter these questions with roundtrip filtration before finetuning a question answering model. In Table <a href="#S5.T7" title="Table 7 ‣ 5.3 Scaling Question Filtration ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we find that our 345 million parameter BERT model modestly outperforms the public BERT-Large model when using synthetic answers to generate questions while our 1.2 billion parameter BERT model further improves on this score by more than a whole point. Interestingly, these results follow an opposite trend compared to the previous section. In the previous section improvements to pretraining scale and tasks made a larger difference on answer generation than increasing model scale. However, here we see the opposite: improvements to pretraining tasks results only in a modest improvement to question filtration, while increasing model size results in much more substantive improvements. We hypothesize that this is due to the larger model’s ability to correctly answer more questions, and therefore allow more valid and high quality samples through to the finetuning phase as indicated by the number of questions generated by the technique.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Modeling Choices</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">While developing our synthetic data generation pipeline we explored several modeling and algorithmic choices before scaling up the model size and data quantity used. We pursued three axis of investigation, ablating choices for each model component of our pipeline at a time. While this analysis does not capture second-order effects that may arise from combining different hyperparameters across studies, we believe that it captures general trends within the space of possible modeling options.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Question Generation</h3>

<figure id="S6.T8" class="ltx_table">
<div id="S6.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:223.8pt;height:75.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.1pt,7.4pt) scale(0.835,0.835) ;">
<table id="S6.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T8.1.1.1.1" class="ltx_tr">
<th id="S6.T8.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Question Generator</th>
<th id="S6.T8.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"># Questions</th>
<th id="S6.T8.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EM</th>
<th id="S6.T8.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T8.1.1.2.1" class="ltx_tr">
<td id="S6.T8.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T8.1.1.2.1.1.1" class="ltx_text ltx_font_bold">345M</span></td>
<td id="S6.T8.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T8.1.1.2.1.2.1" class="ltx_text ltx_font_bold">42414</span></td>
<td id="S6.T8.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T8.1.1.2.1.3.1" class="ltx_text ltx_font_bold">80.7</span></td>
<td id="S6.T8.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S6.T8.1.1.2.1.4.1" class="ltx_text ltx_font_bold">88.6</span></td>
</tr>
<tr id="S6.T8.1.1.3.2" class="ltx_tr">
<td id="S6.T8.1.1.3.2.1" class="ltx_td ltx_align_center">345M (no pretraining)</td>
<td id="S6.T8.1.1.3.2.2" class="ltx_td ltx_align_center">42408</td>
<td id="S6.T8.1.1.3.2.3" class="ltx_td ltx_align_center">42.7</td>
<td id="S6.T8.1.1.3.2.4" class="ltx_td ltx_align_center">51.4</td>
</tr>
<tr id="S6.T8.1.1.4.3" class="ltx_tr">
<td id="S6.T8.1.1.4.3.1" class="ltx_td ltx_align_center">345M (no stopwords)</td>
<td id="S6.T8.1.1.4.3.2" class="ltx_td ltx_align_center">42486</td>
<td id="S6.T8.1.1.4.3.3" class="ltx_td ltx_align_center">75.5</td>
<td id="S6.T8.1.1.4.3.4" class="ltx_td ltx_align_center">84.5</td>
</tr>
<tr id="S6.T8.1.1.5.4" class="ltx_tr">
<td id="S6.T8.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Human Generated Questions</td>
<td id="S6.T8.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">42472</td>
<td id="S6.T8.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.3</td>
<td id="S6.T8.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">93.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span> Effect of question generator modeling choices. Questions are generated from ground truth answers without any filtration.</figcaption>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">To study question generation in isolation we used our 345 million parameter model to generate questions from ground truth <span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>answers. The results of our analysis can be found in Table <a href="#S6.T8" title="Table 8 ‣ 6.1 Question Generation ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. We first investigated the use of pretrained models and found that pretraining our GPT-2 model was crucial for achieving reasonable question generation. We then examined the effect of stopword filtration in our question generator. We found that this provided a substantial boost to EM and F1 scores of 5.2 and 4.1 respectively. The goal of employing this technique was to catch generations that ramble onwards without stopping, or produce end of text prematurely in the middle of a question. On manual inspection we found qualitatively that this technique helped when generating questions on text that featured heavy use of symbols and foreign language. In these cases the model struggled with out of distribution vocabulary, autoregressive sampling degenerated, and no stopword was produced.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Answer Generation</h3>

<figure id="S6.T9" class="ltx_table">
<div id="S6.T9.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:220.5pt;height:90.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.8pt,8.9pt) scale(0.835,0.835) ;">
<table id="S6.T9.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T9.1.1.1.1" class="ltx_tr">
<th id="S6.T9.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Answer Generator</th>
<th id="S6.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"># Questions</th>
<th id="S6.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">EM</th>
<th id="S6.T9.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T9.1.1.2.1" class="ltx_tr">
<td id="S6.T9.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_tt">NER</td>
<td id="S6.T9.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">132729</td>
<td id="S6.T9.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">59.3</td>
<td id="S6.T9.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">70.5</td>
</tr>
<tr id="S6.T9.1.1.3.2" class="ltx_tr">
<td id="S6.T9.1.1.3.2.1" class="ltx_td ltx_align_center">Independent Spans</td>
<td id="S6.T9.1.1.3.2.2" class="ltx_td ltx_align_center">83534</td>
<td id="S6.T9.1.1.3.2.3" class="ltx_td ltx_align_center">77.2</td>
<td id="S6.T9.1.1.3.2.4" class="ltx_td ltx_align_center">87.1</td>
</tr>
<tr id="S6.T9.1.1.4.3" class="ltx_tr">
<td id="S6.T9.1.1.4.3.1" class="ltx_td ltx_align_center"><span id="S6.T9.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Joint Spans</span></td>
<td id="S6.T9.1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S6.T9.1.1.4.3.2.1" class="ltx_text ltx_font_bold">229297</span></td>
<td id="S6.T9.1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S6.T9.1.1.4.3.3.1" class="ltx_text ltx_font_bold">79.1</span></td>
<td id="S6.T9.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S6.T9.1.1.4.3.4.1" class="ltx_text ltx_font_bold">87.9</span></td>
</tr>
<tr id="S6.T9.1.1.5.4" class="ltx_tr">
<td id="S6.T9.1.1.5.4.1" class="ltx_td ltx_align_center">Paragraph-level Joint Spans</td>
<td id="S6.T9.1.1.5.4.2" class="ltx_td ltx_align_center">226672</td>
<td id="S6.T9.1.1.5.4.3" class="ltx_td ltx_align_center">77.3</td>
<td id="S6.T9.1.1.5.4.4" class="ltx_td ltx_align_center">86.9</td>
</tr>
<tr id="S6.T9.1.1.6.5" class="ltx_tr">
<td id="S6.T9.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Human Generated Answers</td>
<td id="S6.T9.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">42472</td>
<td id="S6.T9.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">83.7</td>
<td id="S6.T9.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">91.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span> Effect of answer generator modeling choices. Model based answer generation is performed with BERT-345M and questions are generated using a 1.2B parameter model. No filtration is applied to the generated questions.</figcaption>
</figure>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2002.09599/assets/icml_20_topk.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="239" height="173" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Effect of top-<math id="S6.F4.5.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.F4.5.m1.1b"><mi id="S6.F4.5.m1.1.1" xref="S6.F4.5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.F4.5.m1.1c"><ci id="S6.F4.5.m1.1.1.cmml" xref="S6.F4.5.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.5.m1.1d">k</annotation></semantics></math> answer generation on downstream <span id="S6.F4.10.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>performance. For a particular value of <math id="S6.F4.6.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.F4.6.m2.1b"><mi id="S6.F4.6.m2.1.1" xref="S6.F4.6.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.F4.6.m2.1c"><ci id="S6.F4.6.m2.1.1.cmml" xref="S6.F4.6.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.6.m2.1d">k</annotation></semantics></math> we sample all top-<math id="S6.F4.7.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.F4.7.m3.1b"><mi id="S6.F4.7.m3.1.1" xref="S6.F4.7.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.F4.7.m3.1c"><ci id="S6.F4.7.m3.1.1.cmml" xref="S6.F4.7.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.7.m3.1d">k</annotation></semantics></math> candidate answers (within a nucleus of <math id="S6.F4.8.m4.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S6.F4.8.m4.1b"><mrow id="S6.F4.8.m4.1.1" xref="S6.F4.8.m4.1.1.cmml"><mi id="S6.F4.8.m4.1.1.2" xref="S6.F4.8.m4.1.1.2.cmml">p</mi><mo id="S6.F4.8.m4.1.1.1" xref="S6.F4.8.m4.1.1.1.cmml">=</mo><mn id="S6.F4.8.m4.1.1.3" xref="S6.F4.8.m4.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F4.8.m4.1c"><apply id="S6.F4.8.m4.1.1.cmml" xref="S6.F4.8.m4.1.1"><eq id="S6.F4.8.m4.1.1.1.cmml" xref="S6.F4.8.m4.1.1.1"></eq><ci id="S6.F4.8.m4.1.1.2.cmml" xref="S6.F4.8.m4.1.1.2">𝑝</ci><cn type="float" id="S6.F4.8.m4.1.1.3.cmml" xref="S6.F4.8.m4.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F4.8.m4.1d">p=0.9</annotation></semantics></math>) from a sequence according to a 345M parameter answer generator.</figcaption>
</figure>
<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In our experiments we found answer generation to be a significant bottleneck in performance. In section <a href="#S5.SS2" title="5.2 Scaling Answer Generation ‣ 5 Model Scale ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> we found that scaling up model size allows us to close the gap between human and synthetic training performance. However, these scaling analysis were performed with our best model. In Table <a href="#S6.T9" title="Table 9 ‣ 6.2 Answer Generation ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> we show that the choice of model is critical to closing the gap. Starting with a Named Entity Recognition (NER) model we find that it gets a dismal EM and F1 score. This is due to entities comprising only of <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mo id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><csymbol cd="latexml" id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">\sim</annotation></semantics></math> 50% of the answer distribution for <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>. It’s necessary to use a learned model to model the diverse set of answers present <span id="S6.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>. We then tried to use the most common <span id="S6.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>model, which models the start and end of a span independently, to extract answers from individual sentences. This performed noticeably better, boosting our score to 77.2 EM, despite producing fewer answers than NER extraction. However, upon inspection we found that modeling the span independently resulted in sampling repetitive answers. We then tried using the answer generator from <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite> which models the start and end of a span jointly as a conditional random field. This is the model we ended up choosing as it performed the best with an exact match score of 79.1. Lastly, we also considered jointly modeling answer spans over an entire paragraph instead of a single sentence. However, we found that it performed worse than independent span modeling over sentences.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.6" class="ltx_p">When sampling our answer candidates we used all top-<math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mi id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">k</annotation></semantics></math> answers comprising of the top-<math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><mi id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><ci id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">p</annotation></semantics></math> (<math id="S6.SS2.p2.3.m3.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S6.SS2.p2.3.m3.1a"><mrow id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml"><mi id="S6.SS2.p2.3.m3.1.1.2" xref="S6.SS2.p2.3.m3.1.1.2.cmml">p</mi><mo id="S6.SS2.p2.3.m3.1.1.1" xref="S6.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S6.SS2.p2.3.m3.1.1.3" xref="S6.SS2.p2.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><apply id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1"><eq id="S6.SS2.p2.3.m3.1.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1.1"></eq><ci id="S6.SS2.p2.3.m3.1.1.2.cmml" xref="S6.SS2.p2.3.m3.1.1.2">𝑝</ci><cn type="float" id="S6.SS2.p2.3.m3.1.1.3.cmml" xref="S6.SS2.p2.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">p=0.9</annotation></semantics></math>) nucleus of the distribution. We performed an ablation study to select <math id="S6.SS2.p2.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS2.p2.4.m4.1a"><mi id="S6.SS2.p2.4.m4.1.1" xref="S6.SS2.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.4.m4.1b"><ci id="S6.SS2.p2.4.m4.1.1.cmml" xref="S6.SS2.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.4.m4.1c">k</annotation></semantics></math> as we found that this had a noticeable impact on downstream accuracy. In Figure <a href="#S6.F4" title="Figure 4 ‣ 6.2 Answer Generation ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we found that there was an optimal spot of <math id="S6.SS2.p2.5.m5.1" class="ltx_Math" alttext="k=5" display="inline"><semantics id="S6.SS2.p2.5.m5.1a"><mrow id="S6.SS2.p2.5.m5.1.1" xref="S6.SS2.p2.5.m5.1.1.cmml"><mi id="S6.SS2.p2.5.m5.1.1.2" xref="S6.SS2.p2.5.m5.1.1.2.cmml">k</mi><mo id="S6.SS2.p2.5.m5.1.1.1" xref="S6.SS2.p2.5.m5.1.1.1.cmml">=</mo><mn id="S6.SS2.p2.5.m5.1.1.3" xref="S6.SS2.p2.5.m5.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.5.m5.1b"><apply id="S6.SS2.p2.5.m5.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1"><eq id="S6.SS2.p2.5.m5.1.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1.1"></eq><ci id="S6.SS2.p2.5.m5.1.1.2.cmml" xref="S6.SS2.p2.5.m5.1.1.2">𝑘</ci><cn type="integer" id="S6.SS2.p2.5.m5.1.1.3.cmml" xref="S6.SS2.p2.5.m5.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.5.m5.1c">k=5</annotation></semantics></math> answers per sentence. When generating answers sampled from an entire paragraph we used <math id="S6.SS2.p2.6.m6.1" class="ltx_Math" alttext="k=24" display="inline"><semantics id="S6.SS2.p2.6.m6.1a"><mrow id="S6.SS2.p2.6.m6.1.1" xref="S6.SS2.p2.6.m6.1.1.cmml"><mi id="S6.SS2.p2.6.m6.1.1.2" xref="S6.SS2.p2.6.m6.1.1.2.cmml">k</mi><mo id="S6.SS2.p2.6.m6.1.1.1" xref="S6.SS2.p2.6.m6.1.1.1.cmml">=</mo><mn id="S6.SS2.p2.6.m6.1.1.3" xref="S6.SS2.p2.6.m6.1.1.3.cmml">24</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.6.m6.1b"><apply id="S6.SS2.p2.6.m6.1.1.cmml" xref="S6.SS2.p2.6.m6.1.1"><eq id="S6.SS2.p2.6.m6.1.1.1.cmml" xref="S6.SS2.p2.6.m6.1.1.1"></eq><ci id="S6.SS2.p2.6.m6.1.1.2.cmml" xref="S6.SS2.p2.6.m6.1.1.2">𝑘</ci><cn type="integer" id="S6.SS2.p2.6.m6.1.1.3.cmml" xref="S6.SS2.p2.6.m6.1.1.3">24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.6.m6.1c">k=24</annotation></semantics></math> as we found that there were 4.86 sentences per paragraph on average. In general, answer generation proves to be a bottleneck in our question generation pipeline.
The difficulty in answer generation is that not only must the answers be useful and well-formed, one must solve a one-to-many modeling problem to sample multiple answers from one passage. We believe that this might also be a contributing factor behind the poorer performance of the paragraph-level answer generation.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Question Filtration</h3>

<figure id="S6.T10" class="ltx_table">
<div id="S6.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:255.7pt;height:165.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-25.3pt,16.3pt) scale(0.835,0.835) ;">
<table id="S6.T10.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T10.1.1.1.1" class="ltx_tr">
<th id="S6.T10.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T10.1.1.1.1.1.1" class="ltx_text">Filter Model</span></th>
<th id="S6.T10.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S6.T10.1.1.1.1.2.1" class="ltx_text"># Questions</span></th>
<td id="S6.T10.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2">345M QA</td>
<td id="S6.T10.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="2">Large QA</td>
</tr>
<tr id="S6.T10.1.1.2.2" class="ltx_tr">
<td id="S6.T10.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="S6.T10.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">F1</td>
<td id="S6.T10.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">EM</td>
<td id="S6.T10.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">F1</td>
</tr>
<tr id="S6.T10.1.1.3.3" class="ltx_tr">
<th id="S6.T10.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="6">Synthetic Questions + Real Answers</th>
</tr>
<tr id="S6.T10.1.1.4.4" class="ltx_tr">
<th id="S6.T10.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">None</th>
<th id="S6.T10.1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">42472</th>
<td id="S6.T10.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">83.4</td>
<td id="S6.T10.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.9</td>
<td id="S6.T10.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">79.0</td>
<td id="S6.T10.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">87.0</td>
</tr>
<tr id="S6.T10.1.1.5.5" class="ltx_tr">
<th id="S6.T10.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Roundtrip (RT)</th>
<th id="S6.T10.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">24310</th>
<td id="S6.T10.1.1.5.5.3" class="ltx_td ltx_align_center">84.0</td>
<td id="S6.T10.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">91.3</td>
<td id="S6.T10.1.1.5.5.5" class="ltx_td ltx_align_center">76.5</td>
<td id="S6.T10.1.1.5.5.6" class="ltx_td ltx_align_center">84.4</td>
</tr>
<tr id="S6.T10.1.1.6.6" class="ltx_tr">
<th id="S6.T10.1.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T10.1.1.6.6.1.1" class="ltx_text ltx_font_bold">Overgenerate &amp; RT</span></th>
<th id="S6.T10.1.1.6.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T10.1.1.6.6.2.1" class="ltx_text ltx_font_bold">47772</span></th>
<td id="S6.T10.1.1.6.6.3" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.6.6.3.1" class="ltx_text ltx_font_bold">85.6</span></td>
<td id="S6.T10.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T10.1.1.6.6.4.1" class="ltx_text ltx_font_bold">92.4</span></td>
<td id="S6.T10.1.1.6.6.5" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.6.6.5.1" class="ltx_text ltx_font_bold">81.7</span></td>
<td id="S6.T10.1.1.6.6.6" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.6.6.6.1" class="ltx_text ltx_font_bold">88.7</span></td>
</tr>
<tr id="S6.T10.1.1.7.7" class="ltx_tr">
<th id="S6.T10.1.1.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="6">Synthetic Questions + Synthetic Answers</th>
</tr>
<tr id="S6.T10.1.1.8.8" class="ltx_tr">
<th id="S6.T10.1.1.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">None</th>
<th id="S6.T10.1.1.8.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">229297</th>
<td id="S6.T10.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">79.1</td>
<td id="S6.T10.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">87.9</td>
<td id="S6.T10.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">78.2</td>
<td id="S6.T10.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">86.8</td>
</tr>
<tr id="S6.T10.1.1.9.9" class="ltx_tr">
<th id="S6.T10.1.1.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Roundtrip (RT)</th>
<th id="S6.T10.1.1.9.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">93866</th>
<td id="S6.T10.1.1.9.9.3" class="ltx_td ltx_align_center">86.3</td>
<td id="S6.T10.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">92.7</td>
<td id="S6.T10.1.1.9.9.5" class="ltx_td ltx_align_center">84.1</td>
<td id="S6.T10.1.1.9.9.6" class="ltx_td ltx_align_center">90.5</td>
</tr>
<tr id="S6.T10.1.1.10.10" class="ltx_tr">
<th id="S6.T10.1.1.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T10.1.1.10.10.1.1" class="ltx_text ltx_font_bold">Overgenerate &amp; RT</span></th>
<th id="S6.T10.1.1.10.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S6.T10.1.1.10.10.2.1" class="ltx_text ltx_font_bold">184992</span></th>
<td id="S6.T10.1.1.10.10.3" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.10.10.3.1" class="ltx_text ltx_font_bold">87.1</span></td>
<td id="S6.T10.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S6.T10.1.1.10.10.4.1" class="ltx_text ltx_font_bold">93.2</span></td>
<td id="S6.T10.1.1.10.10.5" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.10.10.5.1" class="ltx_text ltx_font_bold">85.2</span></td>
<td id="S6.T10.1.1.10.10.6" class="ltx_td ltx_align_center"><span id="S6.T10.1.1.10.10.6.1" class="ltx_text ltx_font_bold">91.5</span></td>
</tr>
<tr id="S6.T10.1.1.11.11" class="ltx_tr">
<th id="S6.T10.1.1.11.11.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Human Generated Data</th>
<th id="S6.T10.1.1.11.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">42472</th>
<td id="S6.T10.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">86.3</td>
<td id="S6.T10.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">93.2</td>
<td id="S6.T10.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">82.4</td>
<td id="S6.T10.1.1.11.11.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">89.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span> Effect of filtration modeling choices on questions generated from ground truth and synthetic answers. 1.2 billion parameter models are used for every stage of the generation pipeline. Questions from no filtration are used in the other experiments with a second set of questions generated in overgeneration experiments.</figcaption>
</figure>
<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.4" class="ltx_p">Both question generation and answer generation sometimes produces poor answers. As we show in Table <a href="#S6.T10" title="Table 10 ‣ 6.3 Question Filtration ‣ 6 Modeling Choices ‣ Training Question Answering Models From Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> generating synthetic data from synthetic answers without filtering deteriorates significantly, while roundtrip consistency combats this effect to perform 7.2 EM points better. However, we find that even on questions generated from ground truth answers roundtrip filtering throws away questions associated with perfectly good answers. Throwing away data significantly hurts BERT-Large whose pretrained features are not as robust as our BERT-345M model and require more finetuning data. To combat this we take an approach similar to overgeneration and reranking <cite class="ltx_cite ltx_citemacro_citep">(Heilman &amp; Smith, <a href="#bib.bib15" title="" class="ltx_ref">2010b</a>)</cite> where we generate two questions per answer and feed each into roundtrip filtration independently. We term this overgeneration and filtration. This helps avoid losing important answers in our synthesized training set. To perform overgeneration we sample one question with top-<math id="S6.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S6.SS3.p1.1.m1.1a"><mi id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><ci id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">k</annotation></semantics></math> (<math id="S6.SS3.p1.2.m2.1" class="ltx_Math" alttext="k=40" display="inline"><semantics id="S6.SS3.p1.2.m2.1a"><mrow id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml"><mi id="S6.SS3.p1.2.m2.1.1.2" xref="S6.SS3.p1.2.m2.1.1.2.cmml">k</mi><mo id="S6.SS3.p1.2.m2.1.1.1" xref="S6.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.2.m2.1.1.3" xref="S6.SS3.p1.2.m2.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><apply id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1"><eq id="S6.SS3.p1.2.m2.1.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1.1"></eq><ci id="S6.SS3.p1.2.m2.1.1.2.cmml" xref="S6.SS3.p1.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S6.SS3.p1.2.m2.1.1.3.cmml" xref="S6.SS3.p1.2.m2.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">k=40</annotation></semantics></math>) sampling and one with top-<math id="S6.SS3.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S6.SS3.p1.3.m3.1a"><mi id="S6.SS3.p1.3.m3.1.1" xref="S6.SS3.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.3.m3.1b"><ci id="S6.SS3.p1.3.m3.1.1.cmml" xref="S6.SS3.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">p</annotation></semantics></math> (<math id="S6.SS3.p1.4.m4.1" class="ltx_Math" alttext="p=0.9" display="inline"><semantics id="S6.SS3.p1.4.m4.1a"><mrow id="S6.SS3.p1.4.m4.1.1" xref="S6.SS3.p1.4.m4.1.1.cmml"><mi id="S6.SS3.p1.4.m4.1.1.2" xref="S6.SS3.p1.4.m4.1.1.2.cmml">p</mi><mo id="S6.SS3.p1.4.m4.1.1.1" xref="S6.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="S6.SS3.p1.4.m4.1.1.3" xref="S6.SS3.p1.4.m4.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.4.m4.1b"><apply id="S6.SS3.p1.4.m4.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1"><eq id="S6.SS3.p1.4.m4.1.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1.1"></eq><ci id="S6.SS3.p1.4.m4.1.1.2.cmml" xref="S6.SS3.p1.4.m4.1.1.2">𝑝</ci><cn type="float" id="S6.SS3.p1.4.m4.1.1.3.cmml" xref="S6.SS3.p1.4.m4.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.4.m4.1c">p=0.9</annotation></semantics></math>) nucleus sampling. This leads approximately to a whole point of improvement for our model in both the case with and without ground truth answers, and allows us to surpass training with real <span id="S6.SS3.p1.4.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>data.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Related Work</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">The concept of generating questions for low-resource domains and data augmentation is not new. Early work using rule based question generation <cite class="ltx_cite ltx_citemacro_citep">(Heilman &amp; Smith, <a href="#bib.bib14" title="" class="ltx_ref">2010a</a>)</cite> proposed the idea of over-generating and re-ranking questions with regression models learned over handcrafted linguistic features. With the proliferation of deep neural networks in Natural Language Processing (NLP), question generation advanced to use learned LSTM models <cite class="ltx_cite ltx_citemacro_citep">(Du et al., <a href="#bib.bib10" title="" class="ltx_ref">2017</a>)</cite> on extractive question answering datasets such as <span id="S7.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD </span>. These early works focused primarily on generating questions without explicit extracted answers in the text. However, recent, rapid improvements in language modeling and text generation <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Radford et al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite> have opened the possibility of generating realistic, high-quality answer-aware questions. Answer aware question generation conditions a generative model on context, and a selected answer from the context to generate a question. The current state of the art leverages transformer based language modeling including <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>; Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>; Zhu et al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>; Klein &amp; Nabi, <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>)</cite> uses seq2seq models to generate questions, and then enforce answer consistency on synthetic questions to filter out poorly generatesd questions in a technique called roundtrip consistency. <cite class="ltx_cite ltx_citemacro_citep">(Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> use a unified transformer rather than a seq2seq model to generate QA data in conjunction with roundtrip consistency. They also develop a rule based method for synthesizing unanswerable questions from generated questions. <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite> go a step further to learn a model that can generate unanswerable questions from a given answerable example.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">The process of generating answers for answer-aware question generation in recent literature has primarily leveraged cloze fill-in-the-blank passages to highlight an answer in a given context. Some work uses NER or linguistic parsers to select passages for cloze translation as in <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>; Dhingra et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>. These methods are only able to generate answers for a subset of questions as <span id="S7.p3.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>is only made up of 52% Named Entity Answers. More recent work such as <cite class="ltx_cite ltx_citemacro_citep">(Alberti et al., <a href="#bib.bib1" title="" class="ltx_ref">2019a</a>; Dong et al., <a href="#bib.bib9" title="" class="ltx_ref">2019</a>)</cite> use model based approaches to match the answer distribution of QA datasets and extract more complex answers.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">However, prior work is still lacking in data quality as the resulting QA models are far from the top of the leaderboard. To improve the quality of synthetic data generation and downstream QA models, improving language model quality is crucial. In addition to pretraining task innovation, BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib6" title="" class="ltx_ref">2018</a>)</cite>, RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib22" title="" class="ltx_ref">2019</a>)</cite> and ALBERT <cite class="ltx_cite ltx_citemacro_citep">(Lan et al., <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> have showed that increasing the size of available pretraining data directly improves downstream discriminative task performance. T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>)</cite>, CTRL <cite class="ltx_cite ltx_citemacro_citep">(Keskar et al., <a href="#bib.bib17" title="" class="ltx_ref">2019</a>)</cite>, Megatron-LM <cite class="ltx_cite ltx_citemacro_citep">(Shoeybi et al., <a href="#bib.bib30" title="" class="ltx_ref">2019</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citep">(Puri &amp; Catanzaro, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> have shown that increasing language model scale improves the quality, coherency, and correctness of text generation. The models used in <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a href="#bib.bib27" title="" class="ltx_ref">2019</a>; Keskar et al., <a href="#bib.bib17" title="" class="ltx_ref">2019</a>; Radford et al., <a href="#bib.bib26" title="" class="ltx_ref">2019</a>; Puri &amp; Catanzaro, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> also demonstrate that larger models allow for better control in conditional language generation. Answer generation and question generation require improvements to discriminative and conditional generative modeling, respectively.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p"><span id="S7.p5.1.1" class="ltx_text ltx_font_smallcaps">SQuAD </span>style extractive question answering is not the only form of question answering. There are many other datasets covering a wide range of QA such as multihop <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib39" title="" class="ltx_ref">2018</a>; Welbl et al., <a href="#bib.bib36" title="" class="ltx_ref">2018</a>; Talmor &amp; Berant, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>, Yes-No question <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a href="#bib.bib5" title="" class="ltx_ref">2019</a>)</cite>, trivia questions <cite class="ltx_cite ltx_citemacro_citep">(Joshi et al., <a href="#bib.bib16" title="" class="ltx_ref">2017</a>; Dunn et al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>, analytical questions <cite class="ltx_cite ltx_citemacro_citep">(Dua et al., <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite>, conversational and generative QAs <cite class="ltx_cite ltx_citemacro_citep">(Reddy et al., <a href="#bib.bib29" title="" class="ltx_ref">2019</a>)</cite>, unanswerable questions <cite class="ltx_cite ltx_citemacro_citep">(Rajpurkar et al., <a href="#bib.bib28" title="" class="ltx_ref">2018</a>; Alberti et al., <a href="#bib.bib2" title="" class="ltx_ref">2019b</a>)</cite>, and large multitask question answering datasets <cite class="ltx_cite ltx_citemacro_citep">(Talmor &amp; Berant, <a href="#bib.bib34" title="" class="ltx_ref">2019</a>)</cite>. While these are outside the scope of the current work, the insights developed improving quality for extractive <span id="S7.p5.1.2" class="ltx_text ltx_font_smallcaps">SQuAD </span>questions will aid in generating high quality questions for other datasets.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">We build upon existing work in large scale language modeling and question generation to push the quality of synthetic question generation. With our best models, we generate large question answering datasets from unlabeled Wikipedia documents and finetune a 345 million parameter BERT-style model achieving 88.4 EM score. Finetuning the resulting model on real <span id="S8.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>data further boosts the EM score to 89.4.
This amounts to a 1.7 point improvement over our fully supervised baseline. Finally, we generate synthetic text from a Wikipedia-finetuned GPT-2 model, generate answer candidates and synthetic questions based on those answers, and then train a BERT-Large model to achieve similar question answering accuracy without directly using any real data at all. Doing so required us to scale model size for our answer generators, question generators, and filtration models.
We hope that better synthetic questions will enable new breakthroughs in question answering systems and related natural language tasks.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberti et al. (2019a)</span>
<span class="ltx_bibblock">
Alberti, C., Andor, D., Pitler, E., Devlin, J., and Collins, M.

</span>
<span class="ltx_bibblock">Synthetic qa corpora generation with roundtrip consistency.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.05416</em>, 2019a.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alberti et al. (2019b)</span>
<span class="ltx_bibblock">
Alberti, C., Lee, K., and Collins, M.

</span>
<span class="ltx_bibblock">A bert baseline for the natural questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.08634</em>, 2019b.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Berner et al. (2019)</span>
<span class="ltx_bibblock">
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al.

</span>
<span class="ltx_bibblock">Dota 2 with large scale deep reinforcement learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.06680</em>, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chapelle et al. (2009)</span>
<span class="ltx_bibblock">
Chapelle, O., Scholkopf, B., and Zien, A.

</span>
<span class="ltx_bibblock">Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book
reviews].

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Neural Networks</em>, 20(3):542–542, 2009.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2019)</span>
<span class="ltx_bibblock">
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova,
K.

</span>
<span class="ltx_bibblock">Boolq: Exploring the surprising difficulty of natural yes/no
questions.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.10044</em>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em>, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pp.  4171–4186, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhingra et al. (2018)</span>
<span class="ltx_bibblock">
Dhingra, B., Pruthi, D., and Rajagopal, D.

</span>
<span class="ltx_bibblock">Simple and effective semi-supervised question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.00720</em>, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. (2019)</span>
<span class="ltx_bibblock">
Dong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., Gao, J., Zhou, M.,
and Hon, H.-W.

</span>
<span class="ltx_bibblock">Unified language model pre-training for natural language
understanding and generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.03197</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2017)</span>
<span class="ltx_bibblock">
Du, X., Shao, J., and Cardie, C.

</span>
<span class="ltx_bibblock">Learning to ask: Neural question generation for reading
comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.00106</em>, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et al. (2019)</span>
<span class="ltx_bibblock">
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.

</span>
<span class="ltx_bibblock">Drop: A reading comprehension benchmark requiring discrete reasoning
over paragraphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1903.00161</em>, 2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn et al. (2017)</span>
<span class="ltx_bibblock">
Dunn, M., Sagun, L., Higgins, M., Guney, V. U., Cirik, V., and Cho, K.

</span>
<span class="ltx_bibblock">Searchqa: A new q&amp;a dataset augmented with context from a search
engine.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.05179</em>, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokaslan &amp; Cohen (2019)</span>
<span class="ltx_bibblock">
Gokaslan, A. and Cohen, V.

</span>
<span class="ltx_bibblock">Openwebtext corpus, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heilman &amp; Smith (2010a)</span>
<span class="ltx_bibblock">
Heilman, M. and Smith, N. A.

</span>
<span class="ltx_bibblock">Good question! statistical ranking for question generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics</em>,
pp.  609–617. Association for Computational Linguistics,
2010a.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Heilman &amp; Smith (2010b)</span>
<span class="ltx_bibblock">
Heilman, M. and Smith, N. A.

</span>
<span class="ltx_bibblock">Good question! statistical ranking for question generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Human Language Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for Computational Linguistics</em>,
pp.  609–617. Association for Computational Linguistics,
2010b.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. (2017)</span>
<span class="ltx_bibblock">
Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">Triviaqa: A large scale distantly supervised challenge dataset for
reading comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1705.03551</em>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Keskar et al. (2019)</span>
<span class="ltx_bibblock">
Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R.

</span>
<span class="ltx_bibblock">Ctrl: A conditional transformer language model for controllable
generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.05858</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma et al. (2014)</span>
<span class="ltx_bibblock">
Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M.

</span>
<span class="ltx_bibblock">Semi-supervised learning with deep generative models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, pp. 3581–3589, 2014.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein &amp; Nabi (2019)</span>
<span class="ltx_bibblock">
Klein, T. and Nabi, M.

</span>
<span class="ltx_bibblock">Learning to answer by learning to ask: Getting the best of gpt-2 and
bert worlds.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.02365</em>, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. (2019)</span>
<span class="ltx_bibblock">
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.

</span>
<span class="ltx_bibblock">Albert: A lite bert for self-supervised learning of language
representations.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.11942</em>, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2019)</span>
<span class="ltx_bibblock">
Lewis, P., Denoyer, L., and Riedel, S.

</span>
<span class="ltx_bibblock">Unsupervised question answering by cloze translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.  4896–4910, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1484</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/P19-1484" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/P19-1484</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
Zettlemoyer, L., and Stoyanov, V.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov &amp; Hutter (2018)</span>
<span class="ltx_bibblock">
Loshchilov, I. and Hutter, F.

</span>
<span class="ltx_bibblock">Fixing weight decay regularization in adam, 2018.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=rk6qdGgCZ" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=rk6qdGgCZ</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Micikevicius et al. (2017)</span>
<span class="ltx_bibblock">
Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D.,
Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al.

</span>
<span class="ltx_bibblock">Mixed precision training.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.03740</em>, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puri &amp; Catanzaro (2019)</span>
<span class="ltx_bibblock">
Puri, R. and Catanzaro, B.

</span>
<span class="ltx_bibblock">Zero-shot text classification with generative language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1912.10165</em>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">OpenAI Blog</em>, 1(8), 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2019)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
Y., Li, W., and Liu, P. J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text
transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.10683</em>, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. (2018)</span>
<span class="ltx_bibblock">
Rajpurkar, P., Jia, R., and Liang, P.

</span>
<span class="ltx_bibblock">Know what you don’t know: Unanswerable questions for squad.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.03822</em>, 2018.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy et al. (2019)</span>
<span class="ltx_bibblock">
Reddy, S., Chen, D., and Manning, C. D.

</span>
<span class="ltx_bibblock">Coqa: A conversational question answering challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
7:249–266, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et al. (2019)</span>
<span class="ltx_bibblock">
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
B.

</span>
<span class="ltx_bibblock">Megatron-lm: Training multi-billion parameter language models using
gpu model parallelism.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silver et al. (2017)</span>
<span class="ltx_bibblock">
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al.

</span>
<span class="ltx_bibblock">Mastering the game of go without human knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Nature</em>, 550(7676):354–359, 2017.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silver et al. (2018)</span>
<span class="ltx_bibblock">
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al.

</span>
<span class="ltx_bibblock">A general reinforcement learning algorithm that masters chess, shogi,
and go through self-play.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Science</em>, 362(6419):1140–1144, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor &amp; Berant (2018)</span>
<span class="ltx_bibblock">
Talmor, A. and Berant, J.

</span>
<span class="ltx_bibblock">Repartitioning of the complexwebquestions dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.09623</em>, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor &amp; Berant (2019)</span>
<span class="ltx_bibblock">
Talmor, A. and Berant, J.

</span>
<span class="ltx_bibblock">MultiQA: An empirical investigation of generalization and
transfer in reading comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pp.  4911–4921, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1485</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.aclweb.org/anthology/P19-1485" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.aclweb.org/anthology/P19-1485</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trinh &amp; Le (2018)</span>
<span class="ltx_bibblock">
Trinh, T. H. and Le, Q. V.

</span>
<span class="ltx_bibblock">A simple method for commonsense reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1806.02847</em>, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al. (2018)</span>
<span class="ltx_bibblock">
Welbl, J., Stenetorp, P., and Riedel, S.

</span>
<span class="ltx_bibblock">Constructing datasets for multi-hop reading comprehension across
documents.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
6:287–302, 2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., and Brew, J.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language
processing, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2019)</span>
<span class="ltx_bibblock">
Xie, Q., Hovy, E., Luong, M.-T., and Le, Q. V.

</span>
<span class="ltx_bibblock">Self-training with noisy student improves imagenet classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1911.04252</em>, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and
Manning, C. D.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question
answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.09600</em>, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers et al. (2019)</span>
<span class="ltx_bibblock">
Zellers, R., Holtzman, A., Rashkin, H., Bisk, Y., Farhadi, A., Roesner, F., and
Choi, Y.

</span>
<span class="ltx_bibblock">Defending against neural fake news.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, pp. 9051–9062, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2019)</span>
<span class="ltx_bibblock">
Zhu, H., Dong, L., Wei, F., Wang, W., Qin, B., and Liu, T.

</span>
<span class="ltx_bibblock">Learning to ask unanswerable questions for machine reading
comprehension.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1906.06045</em>, 2019.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu &amp; Goldberg (2009)</span>
<span class="ltx_bibblock">
Zhu, X. and Goldberg, A. B.

</span>
<span class="ltx_bibblock">Introduction to semi-supervised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Synthesis lectures on artificial intelligence and machine
learning</em>, 3(1):1–130, 2009.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu (2005)</span>
<span class="ltx_bibblock">
Zhu, X. J.

</span>
<span class="ltx_bibblock">Semi-supervised learning literature survey.

</span>
<span class="ltx_bibblock">Technical report, University of Wisconsin-Madison Department of
Computer Sciences, 2005.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Samples Generated from Wikipedia Documents</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Below are synthetic question and answering pairs synthesized from real Wikipedia documents. Question and answer generation and filtration were performed by 1.2 billion parameter models finetuned over the entire <span id="A1.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dataset. Generated answer spans are bolded in the text.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p"><span id="A1.p2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;">
<span id="A1.p2.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:203.8pt;">
<span id="A1.p2.1.1.1.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.1.1.1.1" class="ltx_text ltx_font_bold">Question</span>: What indicates there must be data deletion early on in the visual pathway?</span>
<span id="A1.p2.1.1.1.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.1.1.2.1" class="ltx_text ltx_font_bold">Context</span>: Evidence suggests that our visual processing system engages in bottom-up selection. For example, <span id="A1.p2.1.1.1.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.1.1.2.2.1" class="ltx_text ltx_font_bold">inattentional blindness</span></span> suggests that there must be data deletion early on in the visual pathway. This bottom-up approach allows us to respond to unexpected and salient events more quickly and is often directed by attentional selection. This also gives our visual system the property of being goal-directed. Many have suggested that the visual system is able to work efficiently by breaking images down into distinct components. Additionally, it has been argued that the visual system takes advantage of redundancies in inputs in order to transmit as much information as possible while using the fewest resources.</span>
</span></span>

<span id="A1.p2.1.2" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.2.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.2.1.1" class="ltx_text ltx_font_bold">Question</span>: What type of antibiotic is cefalotin?</span>
<span id="A1.p2.1.2.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.2.2.1" class="ltx_text ltx_font_bold">Context</span>: Cefalotin (INN) or cephalothin (USAN) is a <span id="A1.p2.1.2.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.2.2.2.1" class="ltx_text ltx_font_bold">first-generation cephalosporin</span></span> antibiotic. It was the first cephalosporin marketed (1964) and continues to be widely used. It is an intravenously administered agent with a similar antimicrobial spectrum to cefazolin and the oral agent cefalexin. Cefalotin sodium is marketed as Keflin (Lilly) and under other trade names.</span>
</span>

<span id="A1.p2.1.3" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.3.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.3.1.1" class="ltx_text ltx_font_bold">Question</span>: What did “Wanted Dead or Alive” rank on the Billboard Hot 100?</span>
<span id="A1.p2.1.3.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.3.2.1" class="ltx_text ltx_font_bold">Context</span>: “Wanted Dead or Alive” is a song by American rock band Bon Jovi. It is from their 1986 album ”Slippery When Wet”. The song was written by Jon Bon Jovi and Richie Sambora and was released in 1987 as the album’s third single. During a February 20, 2008 encore performance in Detroit, Jon Bon Jovi told the crowd about running into Bob Seger at a Pistons game. As he introduced his song “Wanted Dead or Alive”, he said it was inspired by Seger’s “Turn the Page” hit and called the song the band’s anthem. The song peaked at <span id="A1.p2.1.3.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.3.2.2.1" class="ltx_text ltx_font_bold">#7</span></span> on the “Billboard” Hot 100 chart and #13 on the Mainstream Rock Tracks chart, making it the third single from the album to reach the Top 10 of the Hot 100. As a result, “Slippery When Wet” was the first hard rock/glam metal album to have 3 top 10 hits on the “Billboard” Hot 100.</span>
</span>

<span id="A1.p2.1.4" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.4.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.4.1.1" class="ltx_text ltx_font_bold">Question</span>: Who played the role of Othello in the scene?</span>
<span id="A1.p2.1.4.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.4.2.1" class="ltx_text ltx_font_bold">Context</span>: The book begins when Kostya and his fellow students are waiting for their first lesson with the Director. They are excited and nervous at the prospect of meeting, and are surprised when he tells them that their first exercise is to put on a few scenes from a play. Kostya and two of his friends perform scenes from “Othello”, with <span id="A1.p2.1.4.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.4.2.2.1" class="ltx_text ltx_font_bold">Kostya</span></span> taking the leading role. Afterwards the Director tells them their mistakes.</span>
</span>

<span id="A1.p2.1.5" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.5.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.5.1.1" class="ltx_text ltx_font_bold">Question</span>: Who was Miss United Kingdom in 1997?</span>
<span id="A1.p2.1.5.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.5.2.1" class="ltx_text ltx_font_bold">Context</span>: <span id="A1.p2.1.5.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.5.2.2.1" class="ltx_text ltx_font_bold">Vicki-Lee Walberg</span></span> (born 11 October 1975) is a model who was Miss United Kingdom in 1997, and made the top 10 at the Miss World 1997 pageant. She was the last title holder to advance to the semifinal of the contest. Walberg later went on to work in television and was a ‘Dolly Dealer’ in Bruce Forsyth’s Play Your Cards Right on ITV during its 2002 revival.</span>
</span>

<span id="A1.p2.1.6" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.6.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.6.1.1" class="ltx_text ltx_font_bold">Question</span>: Who broke the Phantom’s mind?</span>
<span id="A1.p2.1.6.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.6.2.1" class="ltx_text ltx_font_bold">Context</span>: In the final episode of the game, it is revealed that Fulbright is in fact deceased, and that the Fulbright seen throughout the game is an international spy known as the Phantom posing as him, as well as the one behind most of the game’s major events. Seven years prior to the game’s events, the Phantom was the catalyst of the UR-1 Incident, having murdered Metis Cykes, Athena’s mother, sabotaged the HAT-1 shuttle, and leaving Simon Blackquill to take the fall for the crime after seemingly incriminating evidence was found to point to Simon as the only suspect. Simon willingly allowed himself to be imprisoned in order to protect Athena and to draw the Phantom out, but Athena suffered severe trauma from the ordeal, having believed for 7 years that she had actually murdered her mother, when in fact she stabbed the Phantom in the hand in self-defense. In the present day, the Phantom attempted to finish their case, murdering Clay Terran and bombing both the HAT-2 shuttle and a courtroom in a desperate attempt to destroy incriminating evidence from the UR-1 incident. The Phantom possesses a unique psychological makeup, showing very little, if any, emotion of any sort, nor any fear. The Phantom also has no sense of self, claiming they do not know what their original gender, face, nationality, or identity even was in the beginning; having taken on so many disguises and identities, the Phantom is an endless void. However, <span id="A1.p2.1.6.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.6.2.2.1" class="ltx_text ltx_font_bold">Phoenix, Apollo, and Athena</span></span> eventually managed to break the emotionless Phantom severely in court, causing them to suffer a severe identity crisis, moments before an unseen sniper rifle takes the Phantom’s life.</span>
</span>

<span id="A1.p2.1.7" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.7.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.7.1.1" class="ltx_text ltx_font_bold">Question</span>: What was the final score for the Tottenham home match against Newcastle United?</span>
<span id="A1.p2.1.7.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.7.2.1" class="ltx_text ltx_font_bold">Context</span>: He scored his first Premier League hat-trick in a 4-0 away win on Boxing Day against Aston Villa. On 5 January 2013, Bale scored in the FA Cup third round fixture against Coventry City as well as assisting Clint Dempsey on both of his goals in a 3-0 win. On 30 January, Bale scored a magnificent solo effort in the 1-1 draw with Norwich City. Bale then scored against West Bromwich Albion in a 1-0 away win on 3 February. Bale then took his goal tally of the season to 15 goals with a brace against Newcastle United in a match which Spurs won <span id="A1.p2.1.7.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.7.2.2.1" class="ltx_text ltx_font_bold">2-1</span></span>. This took Spurs into third place, and strengthened their Champions League ambitions.</span>
</span>

<span id="A1.p2.1.8" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.8.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.8.1.1" class="ltx_text ltx_font_bold">Question</span>: Who was arrested along with Ernst Sekunna?</span>
<span id="A1.p2.1.8.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.8.2.1" class="ltx_text ltx_font_bold">Context</span>: The arrests started in March 1917, with <span id="A1.p2.1.8.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.8.2.2.1" class="ltx_text ltx_font_bold">Chandra Kanta Chakraverty</span></span> “a thin-faced, falsetto-voiced Hindu, a native of Bengal, and a speaker of many languages”, and the German, Ernst Sekunna, being arrested on charges of conspiracy. Most of the others were arrested on April 8, including Franz Bopp, the German Consul General for San Francisco, E. H. von Schack, Deus Dekker and Wilhelm von Brincken. The Indian Nationalists were accused of taking “advantage of American neutrality to plot on American soil against the allies” at “the expense of the laws and hospitality of the United States”. The two men had also taken out trade names to do business as “The Oriental Society”, “The Oriental Kitchen”,and the “Oriental Review”, and purchased of land in an isolated part of New York State.</span>
</span>

<span id="A1.p2.1.9" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A1.p2.1.9.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.9.1.1" class="ltx_text ltx_font_bold">Question</span>: What protected the hulls of the Chiyoda?</span>
<span id="A1.p2.1.9.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A1.p2.1.9.2.1" class="ltx_text ltx_font_bold">Context</span>: “Chiyoda” was a belted cruiser based on a much scaled-down version of the Royal Navy’s. The hull was made of 84 watertight compartments, protected with <span id="A1.p2.1.9.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A1.p2.1.9.2.2.1" class="ltx_text ltx_font_bold">Harvey armor</span></span>. Originally designed to carry 12.6 inch Canet guns, the plan was abandoned due to excessive top weight. Instead, the design was changed so that her main battery consisted of ten QF 4.7 inch /40 naval guns in single mounts, mounted one each in the bow and stern, and four on each side in sponsons. The use of the Elwick quick-firing technology resulted in an increase in the rate of fire by six-fold over previous cruiser designs. Her secondary battery consisted of 14 QF 3 pounder Hotchkiss and three 11-mm, 10-barrel Nordenfelt guns. She was also equipped with three Whitehead torpedo tubes mounted on the main deck. As was standard practice at the time, the prow was reinforced for ramming.</span>
</span></p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Samples Generated from GPT-2 Documents</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Below are synthetic question and answering pairs synthesized from fake Wikipedia documents sampled unconditionally from an 8.3B GPT-2 model. Question and answer generation and filtration were performed by 1.2 billion parameter models finetuned over the entire <span id="A2.p1.1.1" class="ltx_text ltx_font_smallcaps">SQuAD1.1 </span>dataset. Generated answer spans are bolded in the text.

<span id="A2.p1.1.2" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.2.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.2.1.1" class="ltx_text ltx_font_bold">Question</span>: What is a clique in a DAG?</span>
<span id="A2.p1.1.2.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.2.2.1" class="ltx_text ltx_font_bold">Context</span>: The main purpose of the conjecture is to quantify the perfect matchings of the vertices of a graph, in a way that can be related to the number of cliques. A perfect match of two vertices means that if the graph is “cut along the line segment connecting these two vertices”, then the pair of vertices forms an optimal matching. A clique is <span id="A2.p1.1.2.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.2.2.2.1" class="ltx_text ltx_font_bold">a small subgraph</span></span> that contains all but one pair of vertices in the graph and so these perfect matchings form an “array” of cliques with the same size as the original graph, and thus can be described by the same number of cliques.</span>
</span>

<span id="A2.p1.1.3" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.3.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.3.1.1" class="ltx_text ltx_font_bold">Question</span>: What property is the difference between Bis(diphenylphosphino)methane and benz(diphenylphosphino)methane?</span>
<span id="A2.p1.1.3.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.3.2.1" class="ltx_text ltx_font_bold">Context</span>: Bis(diphenylphosphino)methane has been found to be a <span id="A2.p1.1.3.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.3.2.2.1" class="ltx_text ltx_font_bold">sterically hindered</span></span> isomer of benz(diphenylphosphino)methane (CHPH) and therefore it has an oxidation number of 1.</span>
</span>

<span id="A2.p1.1.4" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.4.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.4.1.1" class="ltx_text ltx_font_bold">Question</span>: Who was in charge of the SOE during World War II?</span>
<span id="A2.p1.1.4.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.4.2.1" class="ltx_text ltx_font_bold">Context</span>: By 1939, the Republican cause was being supported by both the Soviet Union and the Third Reich. The SOE, led by <span id="A2.p1.1.4.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.4.2.2.1" class="ltx_text ltx_font_bold">Colonel Hugh Sinclair</span></span>, had been active in the country since 1934, delivering weapons and propaganda material to the Republicans via agents such as future French Resistance leader Francois de La Rocque. This work came to an abrupt end in April 1939, when the Germans invaded the country. Sinclair organised a flight to France, but only about a dozen agents and journalists escaped from the country.</span>
</span>

<span id="A2.p1.1.5" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.5.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.5.1.1" class="ltx_text ltx_font_bold">Question</span>: When did Henry II invade Normandy?</span>
<span id="A2.p1.1.5.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.5.2.1" class="ltx_text ltx_font_bold">Context</span>: During the reign of Louis VII of France, Eleanor was awarded by her husband the County of Anjou. In <span id="A2.p1.1.5.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.5.2.2.1" class="ltx_text ltx_font_bold">1157</span></span>, Henry II of England invaded Normandy to take possession of that duchy, defeating Louis’s troops in the Battle of Brémule. Louis’s grandson and heir, William Adelin, left Anjou for his home in the south of France, where he was crowned at Toulouse on 24 April 1158.</span>
</span>

<span id="A2.p1.1.6" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.6.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.6.1.1" class="ltx_text ltx_font_bold">Question</span>: What does Dick Grayson use as his name?</span>
<span id="A2.p1.1.6.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.6.2.1" class="ltx_text ltx_font_bold">Context</span>: Meanwhile, on his return to the fifth dimension, the leader of the Faceless Ones is killed in the ensuing battle and his daughter is captured. She asks the Faceless Ones for an escape plan and is told that she must first find her father’s ”labyrinth”. The Faceless Ones then freeze her in time and her journey begins. Batman, now imprisoned in Arkham Asylum is visited by Dick Grayson in his new identity of <span id="A2.p1.1.6.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.6.2.2.1" class="ltx_text ltx_font_bold">Nightwing</span></span>. Nightwing informs him that he has broken his parole and is now hunting him. Batman is shocked to discover that Nightwing has come to Arkham because of a deal he made with the Riddler to help him track down some of Batman’s other enemies. Batman is sent by the Joker to assist Nightwing, Deadman, Deathstroke, and Lex Luthor, in tracking down Deadman’s apparent killer. Batman eventually learns that the person who really killed Deadman was his fellow Justice League member, Zauriel. Zauriel is revealed to be a deeply troubled angel-like figure who blames the world for the suffering and death that he has witnessed as he has been with Batman since the death of Damian Wayne. The story arc culminated in a battle in the House of Mystery between the Spectre and Zauriel in an attempt to bring the demon back to Hell. In the end, Batman accepts Zauriel’s invitation to follow him back to the fifth dimension to spare him any further pain and humiliation.</span>
</span>

<span id="A2.p1.1.7" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.7.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.7.1.1" class="ltx_text ltx_font_bold">Question</span>: Who do Jim, Pam, Dwight, Oscar, and Jim’s father, Henry attend the wedding reception for?</span>
<span id="A2.p1.1.7.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.7.2.1" class="ltx_text ltx_font_bold">Context</span>: At the photo shoot, Andy Bernard (Ed Helms) and Erin Hannon (Ellie Kemper) go on a fake zombie honeymoon in the office, having an intimate moment that is interrupted when they encounter a horde of the undead. Michael and Dwight then stop the zombies from approaching Andy and Erin and create a barricade. The horde is scared off, but the building must be sealed off because the zombies have damaged the power generator, resulting in a total loss of power. After the power returns, Jim, Pam, Dwight, Oscar, and Jim’s father, Henry (Brock Peters), begin gathering their families and friends to go to <span id="A2.p1.1.7.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.7.2.2.1" class="ltx_text ltx_font_bold">Erin and Andy</span></span>’s wedding reception in the Scranton branch’s conference room.</span>
</span>

<span id="A2.p1.1.8" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.8.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.8.1.1" class="ltx_text ltx_font_bold">Question</span>: What was the title of 50 Cent’s first album?</span>
<span id="A2.p1.1.8.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.8.2.1" class="ltx_text ltx_font_bold">Context</span>: “I Got Mine” is a song by American rapper 50 Cent from his debut studio album “<span id="A2.p1.1.8.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.8.2.2.1" class="ltx_text ltx_font_bold">Get Rich or Die Tryin’</span></span>” (2003). The song features a guest appearance from fellow New York City rapper Nas, who was also featured on the previous single from “Get Rich or Die Tryin’”, “Hate Me Now”.</span>
</span>

<span id="A2.p1.1.9" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.9.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.9.1.1" class="ltx_text ltx_font_bold">Question</span>: What happens to a star when it bursts into a thermonuclear reaction?</span>
<span id="A2.p1.1.9.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.9.2.1" class="ltx_text ltx_font_bold">Context</span>: When the star explodes, the material is compressed to several hundred times its original size, igniting a thermonuclear reaction. This reaction causes the star to <span id="A2.p1.1.9.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.9.2.2.1" class="ltx_text ltx_font_bold">explode outward</span></span>. The first stage of the supernova explosion is not yet far enough away to reach this red giant stage, so the star is engulfed in a supernova explosion. As the star is heated up by the supernova explosion, the outer layers of the star collapse. The compression that occurred when the shock wave reached the star’s surface begins to occur at the point where the star’s surface meets its core. This core-surface compression heats up and accelerates the remaining core material, producing a shock wave that expands out from the core.</span>
</span>

<span id="A2.p1.1.10" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.10.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.10.1.1" class="ltx_text ltx_font_bold">Question</span>: What style was used in This Wonderful Life’s production?</span>
<span id="A2.p1.1.10.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.10.2.1" class="ltx_text ltx_font_bold">Context</span>: In 2009, Maine College of Art (main campus) presented “This Wonderful Life” as the kick-off production to their 2009/2010 theater season. Director Todd Ziegler created a minimalist approach to the production, relying mostly on the basic premise and atmosphere of the film to create a world. The Main Stage theater was transformed into an <span id="A2.p1.1.10.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.10.2.2.1" class="ltx_text ltx_font_bold">Art Deco</span></span>-esque set with minimal set pieces, provided by Redlich + Feuer Design. This setting was contrasted by the minimalistic approach to lighting, provided by Brian Claypool, that lent the production a somber tone. In keeping with the Art Deco styling, costume design and construction was done entirely by students of the Department of Theater and Dance. The music was provided by the joint choirs of the college and the Maine All State Honor Choir.</span>
</span>

<span id="A2.p1.1.11" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:203.8pt;">
<span id="A2.p1.1.11.1" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.11.1.1" class="ltx_text ltx_font_bold">Question</span>: Which road through the Texas scrublands is a controlled access road?</span>
<span id="A2.p1.1.11.2" class="ltx_p"><span class="ltx_rule" style="width:100%;height:1px;background:black;display:inline-block;"> </span><span id="A2.p1.1.11.2.1" class="ltx_text ltx_font_bold">Context</span>: The western terminus of US 83 is located on the southeast corner of the Texas-New Mexico border at the Van Horn, Texas-Van Horn, Texas city limit line. From the border the highway follows Texas State Highway 116, which crosses US 87 in Van Horn and overlaps US 70. US 83 then crosses US 87 again near Marfa, intersecting US 87 Business and Texas State Highway 292. US 83 continues west from Marfa along Highway 290, a route now called the Trans-Pecos Highway. While <span id="A2.p1.1.11.2.2" class="ltx_text" style="color:#FF0000;"> <span id="A2.p1.1.11.2.2.1" class="ltx_text ltx_font_bold">US 290</span></span> is a controlled-access road, it still has a large number of at-grade intersections, due to the rugged terrain. Between Marfa and Valentine, US 83 travels through the Texas scrubland of the Big Bend.</span>
</span></p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2002.09598" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2002.09599" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2002.09599">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2002.09599" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2002.09600" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 17:59:47 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
