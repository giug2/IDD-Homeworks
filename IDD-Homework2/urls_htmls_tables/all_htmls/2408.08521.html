<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering</title>
<!--Generated on Fri Aug 16 04:23:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.08521v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S1" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Design of MuRAR</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS1" title="In 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Text Answer Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS2" title="In 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Source-Based Multimodal Retrieval</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS2.SSS0.Px1" title="In 2.2 Source-Based Multimodal Retrieval â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Source Attribution.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS2.SSS0.Px2" title="In 2.2 Source-Based Multimodal Retrieval â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Section-Level Multimodal Data Retrieval.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS3" title="In 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Multimodal Answer Refinement</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S3" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>User Interface</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S4" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Data Collection</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S4.SS1" title="In 4 Data Collection â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Multimodal Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S4.SS2" title="In 4 Data Collection â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Multimodal Question Answering Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Human Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.SS1" title="In 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Study Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.SS2" title="In 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Evaluation Schema</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.SS3" title="In 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.SS3.SSS0.Px1" title="In 5.3 Results â€£ 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Psychometric Evaluation Results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.SS3.SSS0.Px2" title="In 5.3 Results â€£ 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Inter-Annotator Agreement.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.SS4" title="In 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Findings</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S6" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1" title="In MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS1" title="In Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Multimodal Scraper Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS2" title="In Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Human Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS2.SSS0.Px1" title="In A.2 Human Evaluation Metrics â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Usefulness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS2.SSS0.Px2" title="In A.2 Human Evaluation Metrics â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Readability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS2.SSS0.Px3" title="In A.2 Human Evaluation Metrics â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Relevance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS2.SSS0.Px4" title="In A.2 Human Evaluation Metrics â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Preference Grading</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS3" title="In Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Additional Human Evaluation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS4" title="In Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Prompts</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS4.SSS0.Px1" title="In A.4 Prompts â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Prompt for Text Answer Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS4.SSS0.Px2" title="In A.4 Prompts â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_title">Prompt for Multimodal Answer Refinement</span></a></li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Zhengyuan Zhu<sup class="ltx_sup" id="id1.1.id1.1"><span class="ltx_text ltx_font_medium" id="id1.1.id1.1.1">1</span></sup></span>, Daniel Lee<sup class="ltx_sup" id="id2.2.id2">2</sup>, Hong Zhang<sup class="ltx_sup" id="id3.3.id3">2</sup>,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id4.4.id4">Sai Sree Harsha<sup class="ltx_sup" id="id4.4.id4.1"><span class="ltx_text ltx_font_medium" id="id4.4.id4.1.1">2</span></sup></span>, <span class="ltx_text ltx_font_bold" id="id5.5.id5">Loic Feujio<sup class="ltx_sup" id="id5.5.id5.1"><span class="ltx_text ltx_font_medium" id="id5.5.id5.1.1">2</span></sup></span>, <span class="ltx_text ltx_font_bold" id="id6.6.id6">Akash Maharaj<sup class="ltx_sup" id="id6.6.id6.1"><span class="ltx_text ltx_font_medium" id="id6.6.id6.1.1">2</span></sup></span>, <span class="ltx_text ltx_font_bold" id="id7.7.id7">Yunyao Li<sup class="ltx_sup" id="id7.7.id7.1"><span class="ltx_text ltx_font_medium" id="id7.7.id7.1.1">2</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7.2"><span class="ltx_text ltx_font_medium" id="id7.7.id7.2.1">1</span></sup></span>University of Texas at Arlington, <sup class="ltx_sup" id="id8.8.id8">2</sup>Adobe
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id9">zhengyuan.zhu@mavs.uta.edu, dlee1@adobe.com, yunyaol@adobe.com </span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id10.id1">Recent advancements in retrieval-augmented generation (RAG) have demonstrated impressive performance in the question-answering (QA) task. However, most previous works predominantly focus on text-based answers. While some studies address multimodal data, they still fall short in generating comprehensive multimodal answers, particularly for explaining concepts or providing step-by-step tutorials on how to accomplish specific goals. This capability is especially valuable for applications such as enterprise chatbots and settings such as customer service and educational systems, where the answers are sourced from multimodal data.
In this paper, we introduce a simple and effective framework named MuRAR (Multimodal Retrieval and Answer Refinement). MuRAR enhances text-based answers by retrieving relevant multimodal data and refining the responses to create coherent multimodal answers. This framework can be easily extended to support multimodal answers in enterprise chatbots with minimal modifications.
Human evaluation results indicate that multimodal answers generated by MuRAR are more useful and readable compared to plain text answers.
A video demo of MuRAR is available at <a class="ltx_ref ltx_href" href="https://youtu.be/ykGRtyVVQpU" title="">https://youtu.be/ykGRtyVVQpU</a>.
<span class="ltx_note ltx_role_todo" id="todox1"><sup class="ltx_note_mark">color=orange</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">color=orange</sup><span class="ltx_note_type">todo: </span><span class="ltx_tag ltx_tag_note">color=orange</span>Record the video once the paper is finished</span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The emergence of large language models (LLMs) such as GPT modelsÂ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib2" title="">2020</a>); OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib7" title="">2023</a>)</cite>, GeminiÂ <cite class="ltx_cite ltx_citemacro_cite">Anil etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib1" title="">2023</a>)</cite>, and LLaMAÂ <cite class="ltx_cite ltx_citemacro_cite">Touvron etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib13" title="">2023</a>)</cite>, along with retrieval-augmented generation (RAG) techniquesÂ <cite class="ltx_cite ltx_citemacro_cite">Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib6" title="">2020</a>); Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib3" title="">2023</a>)</cite>, has significantly transformed the field of question answering (QA) and enhanced the quality of responses provided by AI assistants.
However, the current generation of AI assistants is limited in their ability to provide comprehensive multimodal answers to user questions. This capability is especially important in an enterprise scenario, where answers are sourced from product documentation that is generally multimodal. Here, the presence of images, tables, and videos are often <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">crucial</span> to understanding complex, domain-specific topics. Enhancing AI assistants with multimodal information can therefore greatly improve user understanding and engagementÂ <cite class="ltx_cite ltx_citemacro_cite">Singh etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib11" title="">2021</a>)</cite>, offering significant benefits such as increased productivity, reduced barriers to entry, higher product adoption rates, amplified creativity, and improved user experiences.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Previous workÂ <cite class="ltx_cite ltx_citemacro_cite">Talmor etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib12" title="">2021</a>); Kumar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib5" title="">2020</a>); Joshi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib4" title="">2024</a>)</cite>, has primarily focused on leveraging various techniques to better understand multimodal data to generate plain text answers for a given query. In another scenarioÂ <cite class="ltx_cite ltx_citemacro_cite">Singh etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib11" title="">2021</a>); Wu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib15" title="">2023</a>)</cite>, the output can either be a text answer or a text answer with a retrieved image or video attached at the end. However, the existing solutions fail to adequately address the challenges posed by complex questions that require the answer to illustrate multiple steps to achieve a goal or include multiple multimodal contents.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In summary, the main challenges are: <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">a)</span> How to retrieve the relevant multimodal data that are related and helpful to answer the user questions, and <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">b)</span> How to generate a coherent multimodal answer that integrates the retrieved multimodal data and the text answer snippets.
To address these challenges, we present MuRAR (<span class="ltx_text ltx_font_bold" id="S1.p3.1.3">Mu</span>ltimodal <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">R</span>etrieval and <span class="ltx_text ltx_font_bold" id="S1.p3.1.5">A</span>nswer <span class="ltx_text ltx_font_bold" id="S1.p3.1.6">R</span>efinement), a simple and effective multimodal question-answering framework that can generate a coherent multimodal answer containing the retrieved multimodal data, including images, videos, and tables.
Our framework consists of three main components: text answer generation, source-based multimodal retrieval, and multimodal answer refinement. The text answer generation component retrieves relevant text documents based on the userâ€™s query and generates the initial text answer using an LLM. The source-based multimodal retrieval component aims to retrieve multimodal data that are relevant to the text answer snippets in the initial text answer. Finally, the multimodal answer refinement component prompt LLM to generates the final answer by incorporating the retrieved multimodal data and text answer snippets.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To showcase the effectiveness of the framework, we applied this framework on a collection of multimodal data with a question-answering agent. The modalities include text, images, tables, and videos.
We evaluated the quality of the multimodal answers on a human-annotated dataset of 300 questions and answers. The results demonstrate that the multimodal answers are superior to plain text answers in terms of naturalness and relevance.
Additionally, our framework can be adapted to other enterprise-level AI assistants by collecting topic-specific multimodal data and fine-tuning a topic-specific text retrieval model.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To the best of our knowledge, this is the first work that addresses the problem of generating coherent multimodal answers to user questions.
Our contribution can be summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a multimodal QA framework that can retrieve multimodal data relevant to the userâ€™s question and interests, and generate a coherent multimodal answer containing the retrieved multimodal data. Notably, this framework can be extended to any other enterprise-level AI assistants.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Our evaluation on an enterprise-level dataset demonstrates that the quality of the multimodal answers produced by our framework surpasses the plain text answers.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Design of MuRAR</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="199" id="S2.F1.g1" src="extracted/5794920/Figures/architecture4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The architecture of the MuRAR framework.</figcaption>
</figure><span class="ltx_note ltx_role_todo" id="todox2"><sup class="ltx_note_mark">color=orange</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">color=orange</sup><span class="ltx_note_type">todo: </span><span class="ltx_tag ltx_tag_note">color=orange</span>Yunyao: The figure is not completely accurate. It seems that Step E leads to nothing.</span></span></span>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="476" id="S2.F2.g1" src="extracted/5794920/Figures/UI_9.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The Interface of AI Assistant demonstrating multimodal answers are constructed by combining multimodal data retrieval and answer refinement.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.6">Formally, given a user question <math alttext="q" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_q</annotation></semantics></math> as input and a set of multimodal data <math alttext="\mathcal{D}=\{D_{S},D_{I},D_{T},D_{V}\}" class="ltx_Math" display="inline" id="S2.p1.2.m2.4"><semantics id="S2.p1.2.m2.4a"><mrow id="S2.p1.2.m2.4.4" xref="S2.p1.2.m2.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.4.4.6" xref="S2.p1.2.m2.4.4.6.cmml">ğ’Ÿ</mi><mo id="S2.p1.2.m2.4.4.5" xref="S2.p1.2.m2.4.4.5.cmml">=</mo><mrow id="S2.p1.2.m2.4.4.4.4" xref="S2.p1.2.m2.4.4.4.5.cmml"><mo id="S2.p1.2.m2.4.4.4.4.5" stretchy="false" xref="S2.p1.2.m2.4.4.4.5.cmml">{</mo><msub id="S2.p1.2.m2.1.1.1.1.1" xref="S2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S2.p1.2.m2.1.1.1.1.1.2" xref="S2.p1.2.m2.1.1.1.1.1.2.cmml">D</mi><mi id="S2.p1.2.m2.1.1.1.1.1.3" xref="S2.p1.2.m2.1.1.1.1.1.3.cmml">S</mi></msub><mo id="S2.p1.2.m2.4.4.4.4.6" xref="S2.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S2.p1.2.m2.2.2.2.2.2" xref="S2.p1.2.m2.2.2.2.2.2.cmml"><mi id="S2.p1.2.m2.2.2.2.2.2.2" xref="S2.p1.2.m2.2.2.2.2.2.2.cmml">D</mi><mi id="S2.p1.2.m2.2.2.2.2.2.3" xref="S2.p1.2.m2.2.2.2.2.2.3.cmml">I</mi></msub><mo id="S2.p1.2.m2.4.4.4.4.7" xref="S2.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S2.p1.2.m2.3.3.3.3.3" xref="S2.p1.2.m2.3.3.3.3.3.cmml"><mi id="S2.p1.2.m2.3.3.3.3.3.2" xref="S2.p1.2.m2.3.3.3.3.3.2.cmml">D</mi><mi id="S2.p1.2.m2.3.3.3.3.3.3" xref="S2.p1.2.m2.3.3.3.3.3.3.cmml">T</mi></msub><mo id="S2.p1.2.m2.4.4.4.4.8" xref="S2.p1.2.m2.4.4.4.5.cmml">,</mo><msub id="S2.p1.2.m2.4.4.4.4.4" xref="S2.p1.2.m2.4.4.4.4.4.cmml"><mi id="S2.p1.2.m2.4.4.4.4.4.2" xref="S2.p1.2.m2.4.4.4.4.4.2.cmml">D</mi><mi id="S2.p1.2.m2.4.4.4.4.4.3" xref="S2.p1.2.m2.4.4.4.4.4.3.cmml">V</mi></msub><mo id="S2.p1.2.m2.4.4.4.4.9" stretchy="false" xref="S2.p1.2.m2.4.4.4.5.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.4b"><apply id="S2.p1.2.m2.4.4.cmml" xref="S2.p1.2.m2.4.4"><eq id="S2.p1.2.m2.4.4.5.cmml" xref="S2.p1.2.m2.4.4.5"></eq><ci id="S2.p1.2.m2.4.4.6.cmml" xref="S2.p1.2.m2.4.4.6">ğ’Ÿ</ci><set id="S2.p1.2.m2.4.4.4.5.cmml" xref="S2.p1.2.m2.4.4.4.4"><apply id="S2.p1.2.m2.1.1.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S2.p1.2.m2.1.1.1.1.1.2">ğ·</ci><ci id="S2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S2.p1.2.m2.1.1.1.1.1.3">ğ‘†</ci></apply><apply id="S2.p1.2.m2.2.2.2.2.2.cmml" xref="S2.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.p1.2.m2.2.2.2.2.2.1.cmml" xref="S2.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S2.p1.2.m2.2.2.2.2.2.2.cmml" xref="S2.p1.2.m2.2.2.2.2.2.2">ğ·</ci><ci id="S2.p1.2.m2.2.2.2.2.2.3.cmml" xref="S2.p1.2.m2.2.2.2.2.2.3">ğ¼</ci></apply><apply id="S2.p1.2.m2.3.3.3.3.3.cmml" xref="S2.p1.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.p1.2.m2.3.3.3.3.3.1.cmml" xref="S2.p1.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S2.p1.2.m2.3.3.3.3.3.2.cmml" xref="S2.p1.2.m2.3.3.3.3.3.2">ğ·</ci><ci id="S2.p1.2.m2.3.3.3.3.3.3.cmml" xref="S2.p1.2.m2.3.3.3.3.3.3">ğ‘‡</ci></apply><apply id="S2.p1.2.m2.4.4.4.4.4.cmml" xref="S2.p1.2.m2.4.4.4.4.4"><csymbol cd="ambiguous" id="S2.p1.2.m2.4.4.4.4.4.1.cmml" xref="S2.p1.2.m2.4.4.4.4.4">subscript</csymbol><ci id="S2.p1.2.m2.4.4.4.4.4.2.cmml" xref="S2.p1.2.m2.4.4.4.4.4.2">ğ·</ci><ci id="S2.p1.2.m2.4.4.4.4.4.3.cmml" xref="S2.p1.2.m2.4.4.4.4.4.3">ğ‘‰</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.4c">\mathcal{D}=\{D_{S},D_{I},D_{T},D_{V}\}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.4d">caligraphic_D = { italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT }</annotation></semantics></math>, where <math alttext="\{D_{S},D_{I},D_{T},D_{V}\}" class="ltx_Math" display="inline" id="S2.p1.3.m3.4"><semantics id="S2.p1.3.m3.4a"><mrow id="S2.p1.3.m3.4.4.4" xref="S2.p1.3.m3.4.4.5.cmml"><mo id="S2.p1.3.m3.4.4.4.5" stretchy="false" xref="S2.p1.3.m3.4.4.5.cmml">{</mo><msub id="S2.p1.3.m3.1.1.1.1" xref="S2.p1.3.m3.1.1.1.1.cmml"><mi id="S2.p1.3.m3.1.1.1.1.2" xref="S2.p1.3.m3.1.1.1.1.2.cmml">D</mi><mi id="S2.p1.3.m3.1.1.1.1.3" xref="S2.p1.3.m3.1.1.1.1.3.cmml">S</mi></msub><mo id="S2.p1.3.m3.4.4.4.6" xref="S2.p1.3.m3.4.4.5.cmml">,</mo><msub id="S2.p1.3.m3.2.2.2.2" xref="S2.p1.3.m3.2.2.2.2.cmml"><mi id="S2.p1.3.m3.2.2.2.2.2" xref="S2.p1.3.m3.2.2.2.2.2.cmml">D</mi><mi id="S2.p1.3.m3.2.2.2.2.3" xref="S2.p1.3.m3.2.2.2.2.3.cmml">I</mi></msub><mo id="S2.p1.3.m3.4.4.4.7" xref="S2.p1.3.m3.4.4.5.cmml">,</mo><msub id="S2.p1.3.m3.3.3.3.3" xref="S2.p1.3.m3.3.3.3.3.cmml"><mi id="S2.p1.3.m3.3.3.3.3.2" xref="S2.p1.3.m3.3.3.3.3.2.cmml">D</mi><mi id="S2.p1.3.m3.3.3.3.3.3" xref="S2.p1.3.m3.3.3.3.3.3.cmml">T</mi></msub><mo id="S2.p1.3.m3.4.4.4.8" xref="S2.p1.3.m3.4.4.5.cmml">,</mo><msub id="S2.p1.3.m3.4.4.4.4" xref="S2.p1.3.m3.4.4.4.4.cmml"><mi id="S2.p1.3.m3.4.4.4.4.2" xref="S2.p1.3.m3.4.4.4.4.2.cmml">D</mi><mi id="S2.p1.3.m3.4.4.4.4.3" xref="S2.p1.3.m3.4.4.4.4.3.cmml">V</mi></msub><mo id="S2.p1.3.m3.4.4.4.9" stretchy="false" xref="S2.p1.3.m3.4.4.5.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.4b"><set id="S2.p1.3.m3.4.4.5.cmml" xref="S2.p1.3.m3.4.4.4"><apply id="S2.p1.3.m3.1.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.p1.3.m3.1.1.1.1.2.cmml" xref="S2.p1.3.m3.1.1.1.1.2">ğ·</ci><ci id="S2.p1.3.m3.1.1.1.1.3.cmml" xref="S2.p1.3.m3.1.1.1.1.3">ğ‘†</ci></apply><apply id="S2.p1.3.m3.2.2.2.2.cmml" xref="S2.p1.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.p1.3.m3.2.2.2.2.1.cmml" xref="S2.p1.3.m3.2.2.2.2">subscript</csymbol><ci id="S2.p1.3.m3.2.2.2.2.2.cmml" xref="S2.p1.3.m3.2.2.2.2.2">ğ·</ci><ci id="S2.p1.3.m3.2.2.2.2.3.cmml" xref="S2.p1.3.m3.2.2.2.2.3">ğ¼</ci></apply><apply id="S2.p1.3.m3.3.3.3.3.cmml" xref="S2.p1.3.m3.3.3.3.3"><csymbol cd="ambiguous" id="S2.p1.3.m3.3.3.3.3.1.cmml" xref="S2.p1.3.m3.3.3.3.3">subscript</csymbol><ci id="S2.p1.3.m3.3.3.3.3.2.cmml" xref="S2.p1.3.m3.3.3.3.3.2">ğ·</ci><ci id="S2.p1.3.m3.3.3.3.3.3.cmml" xref="S2.p1.3.m3.3.3.3.3.3">ğ‘‡</ci></apply><apply id="S2.p1.3.m3.4.4.4.4.cmml" xref="S2.p1.3.m3.4.4.4.4"><csymbol cd="ambiguous" id="S2.p1.3.m3.4.4.4.4.1.cmml" xref="S2.p1.3.m3.4.4.4.4">subscript</csymbol><ci id="S2.p1.3.m3.4.4.4.4.2.cmml" xref="S2.p1.3.m3.4.4.4.4.2">ğ·</ci><ci id="S2.p1.3.m3.4.4.4.4.3.cmml" xref="S2.p1.3.m3.4.4.4.4.3">ğ‘‰</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.4c">\{D_{S},D_{I},D_{T},D_{V}\}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.4d">{ italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT }</annotation></semantics></math> denote collections of text document snippet, images, tables, and videos, respectively. The objective is to generate a multimodal answer <math alttext="A_{mm}=F(S,I,T,V)" class="ltx_Math" display="inline" id="S2.p1.4.m4.4"><semantics id="S2.p1.4.m4.4a"><mrow id="S2.p1.4.m4.4.5" xref="S2.p1.4.m4.4.5.cmml"><msub id="S2.p1.4.m4.4.5.2" xref="S2.p1.4.m4.4.5.2.cmml"><mi id="S2.p1.4.m4.4.5.2.2" xref="S2.p1.4.m4.4.5.2.2.cmml">A</mi><mrow id="S2.p1.4.m4.4.5.2.3" xref="S2.p1.4.m4.4.5.2.3.cmml"><mi id="S2.p1.4.m4.4.5.2.3.2" xref="S2.p1.4.m4.4.5.2.3.2.cmml">m</mi><mo id="S2.p1.4.m4.4.5.2.3.1" xref="S2.p1.4.m4.4.5.2.3.1.cmml">â¢</mo><mi id="S2.p1.4.m4.4.5.2.3.3" xref="S2.p1.4.m4.4.5.2.3.3.cmml">m</mi></mrow></msub><mo id="S2.p1.4.m4.4.5.1" xref="S2.p1.4.m4.4.5.1.cmml">=</mo><mrow id="S2.p1.4.m4.4.5.3" xref="S2.p1.4.m4.4.5.3.cmml"><mi id="S2.p1.4.m4.4.5.3.2" xref="S2.p1.4.m4.4.5.3.2.cmml">F</mi><mo id="S2.p1.4.m4.4.5.3.1" xref="S2.p1.4.m4.4.5.3.1.cmml">â¢</mo><mrow id="S2.p1.4.m4.4.5.3.3.2" xref="S2.p1.4.m4.4.5.3.3.1.cmml"><mo id="S2.p1.4.m4.4.5.3.3.2.1" stretchy="false" xref="S2.p1.4.m4.4.5.3.3.1.cmml">(</mo><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">S</mi><mo id="S2.p1.4.m4.4.5.3.3.2.2" xref="S2.p1.4.m4.4.5.3.3.1.cmml">,</mo><mi id="S2.p1.4.m4.2.2" xref="S2.p1.4.m4.2.2.cmml">I</mi><mo id="S2.p1.4.m4.4.5.3.3.2.3" xref="S2.p1.4.m4.4.5.3.3.1.cmml">,</mo><mi id="S2.p1.4.m4.3.3" xref="S2.p1.4.m4.3.3.cmml">T</mi><mo id="S2.p1.4.m4.4.5.3.3.2.4" xref="S2.p1.4.m4.4.5.3.3.1.cmml">,</mo><mi id="S2.p1.4.m4.4.4" xref="S2.p1.4.m4.4.4.cmml">V</mi><mo id="S2.p1.4.m4.4.5.3.3.2.5" stretchy="false" xref="S2.p1.4.m4.4.5.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.4b"><apply id="S2.p1.4.m4.4.5.cmml" xref="S2.p1.4.m4.4.5"><eq id="S2.p1.4.m4.4.5.1.cmml" xref="S2.p1.4.m4.4.5.1"></eq><apply id="S2.p1.4.m4.4.5.2.cmml" xref="S2.p1.4.m4.4.5.2"><csymbol cd="ambiguous" id="S2.p1.4.m4.4.5.2.1.cmml" xref="S2.p1.4.m4.4.5.2">subscript</csymbol><ci id="S2.p1.4.m4.4.5.2.2.cmml" xref="S2.p1.4.m4.4.5.2.2">ğ´</ci><apply id="S2.p1.4.m4.4.5.2.3.cmml" xref="S2.p1.4.m4.4.5.2.3"><times id="S2.p1.4.m4.4.5.2.3.1.cmml" xref="S2.p1.4.m4.4.5.2.3.1"></times><ci id="S2.p1.4.m4.4.5.2.3.2.cmml" xref="S2.p1.4.m4.4.5.2.3.2">ğ‘š</ci><ci id="S2.p1.4.m4.4.5.2.3.3.cmml" xref="S2.p1.4.m4.4.5.2.3.3">ğ‘š</ci></apply></apply><apply id="S2.p1.4.m4.4.5.3.cmml" xref="S2.p1.4.m4.4.5.3"><times id="S2.p1.4.m4.4.5.3.1.cmml" xref="S2.p1.4.m4.4.5.3.1"></times><ci id="S2.p1.4.m4.4.5.3.2.cmml" xref="S2.p1.4.m4.4.5.3.2">ğ¹</ci><vector id="S2.p1.4.m4.4.5.3.3.1.cmml" xref="S2.p1.4.m4.4.5.3.3.2"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ğ‘†</ci><ci id="S2.p1.4.m4.2.2.cmml" xref="S2.p1.4.m4.2.2">ğ¼</ci><ci id="S2.p1.4.m4.3.3.cmml" xref="S2.p1.4.m4.3.3">ğ‘‡</ci><ci id="S2.p1.4.m4.4.4.cmml" xref="S2.p1.4.m4.4.4">ğ‘‰</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.4c">A_{mm}=F(S,I,T,V)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.4d">italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT = italic_F ( italic_S , italic_I , italic_T , italic_V )</annotation></semantics></math>, where <math alttext="F" class="ltx_Math" display="inline" id="S2.p1.5.m5.1"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ¹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">F</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.1d">italic_F</annotation></semantics></math> represents a function that organizes the retrieved multimodal data <math alttext="(S,I,T,V)" class="ltx_Math" display="inline" id="S2.p1.6.m6.4"><semantics id="S2.p1.6.m6.4a"><mrow id="S2.p1.6.m6.4.5.2" xref="S2.p1.6.m6.4.5.1.cmml"><mo id="S2.p1.6.m6.4.5.2.1" stretchy="false" xref="S2.p1.6.m6.4.5.1.cmml">(</mo><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">S</mi><mo id="S2.p1.6.m6.4.5.2.2" xref="S2.p1.6.m6.4.5.1.cmml">,</mo><mi id="S2.p1.6.m6.2.2" xref="S2.p1.6.m6.2.2.cmml">I</mi><mo id="S2.p1.6.m6.4.5.2.3" xref="S2.p1.6.m6.4.5.1.cmml">,</mo><mi id="S2.p1.6.m6.3.3" xref="S2.p1.6.m6.3.3.cmml">T</mi><mo id="S2.p1.6.m6.4.5.2.4" xref="S2.p1.6.m6.4.5.1.cmml">,</mo><mi id="S2.p1.6.m6.4.4" xref="S2.p1.6.m6.4.4.cmml">V</mi><mo id="S2.p1.6.m6.4.5.2.5" stretchy="false" xref="S2.p1.6.m6.4.5.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.4b"><vector id="S2.p1.6.m6.4.5.1.cmml" xref="S2.p1.6.m6.4.5.2"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">ğ‘†</ci><ci id="S2.p1.6.m6.2.2.cmml" xref="S2.p1.6.m6.2.2">ğ¼</ci><ci id="S2.p1.6.m6.3.3.cmml" xref="S2.p1.6.m6.3.3">ğ‘‡</ci><ci id="S2.p1.6.m6.4.4.cmml" xref="S2.p1.6.m6.4.4">ğ‘‰</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.4c">(S,I,T,V)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.6.m6.4d">( italic_S , italic_I , italic_T , italic_V )</annotation></semantics></math> into a coherent and informative answer.
<span class="ltx_note ltx_role_todo" id="todo1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">todo: </span><span class="ltx_tag ltx_tag_note">1</span>we need to clarity how to define S, I, T, V, i.e., a set of retrieved image, etc</span></span></span></p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.12">To achieve high quality <math alttext="A_{mm}" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><msub id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">A</mi><mrow id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml"><mi id="S2.p2.1.m1.1.1.3.2" xref="S2.p2.1.m1.1.1.3.2.cmml">m</mi><mo id="S2.p2.1.m1.1.1.3.1" xref="S2.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S2.p2.1.m1.1.1.3.3" xref="S2.p2.1.m1.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">ğ´</ci><apply id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3"><times id="S2.p2.1.m1.1.1.3.1.cmml" xref="S2.p2.1.m1.1.1.3.1"></times><ci id="S2.p2.1.m1.1.1.3.2.cmml" xref="S2.p2.1.m1.1.1.3.2">ğ‘š</ci><ci id="S2.p2.1.m1.1.1.3.3.cmml" xref="S2.p2.1.m1.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">A_{mm}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math>, we propose MuRAR, as illustrated in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.F1" title="Figure 1 â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>.
The MuRAR framework consists of three main components: text answer generation (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS1" title="2.1 Text Answer Generation â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">2.1</span></a>), source-based multimodal retrieval (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS2" title="2.2 Source-Based Multimodal Retrieval â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">2.2</span></a>), and multimodal answer refinement (SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.SS3" title="2.3 Multimodal Answer Refinement â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">2.3</span></a>).
The text answer generation component uses a RAG approachÂ <cite class="ltx_cite ltx_citemacro_cite">Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib6" title="">2020</a>); Gao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib3" title="">2023</a>)</cite>, first retrieving relevant text document snippets <math alttext="S=\{s_{1},s_{2},...,s_{n}\}\in D_{S}" class="ltx_Math" display="inline" id="S2.p2.2.m2.4"><semantics id="S2.p2.2.m2.4a"><mrow id="S2.p2.2.m2.4.4" xref="S2.p2.2.m2.4.4.cmml"><mi id="S2.p2.2.m2.4.4.5" xref="S2.p2.2.m2.4.4.5.cmml">S</mi><mo id="S2.p2.2.m2.4.4.6" xref="S2.p2.2.m2.4.4.6.cmml">=</mo><mrow id="S2.p2.2.m2.4.4.3.3" xref="S2.p2.2.m2.4.4.3.4.cmml"><mo id="S2.p2.2.m2.4.4.3.3.4" stretchy="false" xref="S2.p2.2.m2.4.4.3.4.cmml">{</mo><msub id="S2.p2.2.m2.2.2.1.1.1" xref="S2.p2.2.m2.2.2.1.1.1.cmml"><mi id="S2.p2.2.m2.2.2.1.1.1.2" xref="S2.p2.2.m2.2.2.1.1.1.2.cmml">s</mi><mn id="S2.p2.2.m2.2.2.1.1.1.3" xref="S2.p2.2.m2.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p2.2.m2.4.4.3.3.5" xref="S2.p2.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.p2.2.m2.3.3.2.2.2" xref="S2.p2.2.m2.3.3.2.2.2.cmml"><mi id="S2.p2.2.m2.3.3.2.2.2.2" xref="S2.p2.2.m2.3.3.2.2.2.2.cmml">s</mi><mn id="S2.p2.2.m2.3.3.2.2.2.3" xref="S2.p2.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p2.2.m2.4.4.3.3.6" xref="S2.p2.2.m2.4.4.3.4.cmml">,</mo><mi id="S2.p2.2.m2.1.1" mathvariant="normal" xref="S2.p2.2.m2.1.1.cmml">â€¦</mi><mo id="S2.p2.2.m2.4.4.3.3.7" xref="S2.p2.2.m2.4.4.3.4.cmml">,</mo><msub id="S2.p2.2.m2.4.4.3.3.3" xref="S2.p2.2.m2.4.4.3.3.3.cmml"><mi id="S2.p2.2.m2.4.4.3.3.3.2" xref="S2.p2.2.m2.4.4.3.3.3.2.cmml">s</mi><mi id="S2.p2.2.m2.4.4.3.3.3.3" xref="S2.p2.2.m2.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S2.p2.2.m2.4.4.3.3.8" stretchy="false" xref="S2.p2.2.m2.4.4.3.4.cmml">}</mo></mrow><mo id="S2.p2.2.m2.4.4.7" xref="S2.p2.2.m2.4.4.7.cmml">âˆˆ</mo><msub id="S2.p2.2.m2.4.4.8" xref="S2.p2.2.m2.4.4.8.cmml"><mi id="S2.p2.2.m2.4.4.8.2" xref="S2.p2.2.m2.4.4.8.2.cmml">D</mi><mi id="S2.p2.2.m2.4.4.8.3" xref="S2.p2.2.m2.4.4.8.3.cmml">S</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.4b"><apply id="S2.p2.2.m2.4.4.cmml" xref="S2.p2.2.m2.4.4"><and id="S2.p2.2.m2.4.4a.cmml" xref="S2.p2.2.m2.4.4"></and><apply id="S2.p2.2.m2.4.4b.cmml" xref="S2.p2.2.m2.4.4"><eq id="S2.p2.2.m2.4.4.6.cmml" xref="S2.p2.2.m2.4.4.6"></eq><ci id="S2.p2.2.m2.4.4.5.cmml" xref="S2.p2.2.m2.4.4.5">ğ‘†</ci><set id="S2.p2.2.m2.4.4.3.4.cmml" xref="S2.p2.2.m2.4.4.3.3"><apply id="S2.p2.2.m2.2.2.1.1.1.cmml" xref="S2.p2.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.2.2.1.1.1.1.cmml" xref="S2.p2.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S2.p2.2.m2.2.2.1.1.1.2.cmml" xref="S2.p2.2.m2.2.2.1.1.1.2">ğ‘ </ci><cn id="S2.p2.2.m2.2.2.1.1.1.3.cmml" type="integer" xref="S2.p2.2.m2.2.2.1.1.1.3">1</cn></apply><apply id="S2.p2.2.m2.3.3.2.2.2.cmml" xref="S2.p2.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p2.2.m2.3.3.2.2.2.1.cmml" xref="S2.p2.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S2.p2.2.m2.3.3.2.2.2.2.cmml" xref="S2.p2.2.m2.3.3.2.2.2.2">ğ‘ </ci><cn id="S2.p2.2.m2.3.3.2.2.2.3.cmml" type="integer" xref="S2.p2.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">â€¦</ci><apply id="S2.p2.2.m2.4.4.3.3.3.cmml" xref="S2.p2.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p2.2.m2.4.4.3.3.3.1.cmml" xref="S2.p2.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S2.p2.2.m2.4.4.3.3.3.2.cmml" xref="S2.p2.2.m2.4.4.3.3.3.2">ğ‘ </ci><ci id="S2.p2.2.m2.4.4.3.3.3.3.cmml" xref="S2.p2.2.m2.4.4.3.3.3.3">ğ‘›</ci></apply></set></apply><apply id="S2.p2.2.m2.4.4c.cmml" xref="S2.p2.2.m2.4.4"><in id="S2.p2.2.m2.4.4.7.cmml" xref="S2.p2.2.m2.4.4.7"></in><share href="https://arxiv.org/html/2408.08521v1#S2.p2.2.m2.4.4.3.cmml" id="S2.p2.2.m2.4.4d.cmml" xref="S2.p2.2.m2.4.4"></share><apply id="S2.p2.2.m2.4.4.8.cmml" xref="S2.p2.2.m2.4.4.8"><csymbol cd="ambiguous" id="S2.p2.2.m2.4.4.8.1.cmml" xref="S2.p2.2.m2.4.4.8">subscript</csymbol><ci id="S2.p2.2.m2.4.4.8.2.cmml" xref="S2.p2.2.m2.4.4.8.2">ğ·</ci><ci id="S2.p2.2.m2.4.4.8.3.cmml" xref="S2.p2.2.m2.4.4.8.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.4c">S=\{s_{1},s_{2},...,s_{n}\}\in D_{S}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.4d">italic_S = { italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } âˆˆ italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> based on user query <math alttext="q" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">italic_q</annotation></semantics></math> and then generating the text answer <math alttext="A_{t}" class="ltx_Math" display="inline" id="S2.p2.4.m4.1"><semantics id="S2.p2.4.m4.1a"><msub id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mi id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">A</mi><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1">subscript</csymbol><ci id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">ğ´</ci><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> by prompting an LLM.
<span class="ltx_note ltx_role_todo" id="todo2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">todo: </span><span class="ltx_tag ltx_tag_note">2</span>briefly explain the source attribution</span></span></span>
Then, we apply source attribution on <math alttext="A_{t}" class="ltx_Math" display="inline" id="S2.p2.5.m5.1"><semantics id="S2.p2.5.m5.1a"><msub id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mi id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">A</mi><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">ğ´</ci><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m5.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to identify the text answer snippets <math alttext="a_{[i,j]}\in A_{t}" class="ltx_Math" display="inline" id="S2.p2.6.m6.2"><semantics id="S2.p2.6.m6.2a"><mrow id="S2.p2.6.m6.2.3" xref="S2.p2.6.m6.2.3.cmml"><msub id="S2.p2.6.m6.2.3.2" xref="S2.p2.6.m6.2.3.2.cmml"><mi id="S2.p2.6.m6.2.3.2.2" xref="S2.p2.6.m6.2.3.2.2.cmml">a</mi><mrow id="S2.p2.6.m6.2.2.2.4" xref="S2.p2.6.m6.2.2.2.3.cmml"><mo id="S2.p2.6.m6.2.2.2.4.1" stretchy="false" xref="S2.p2.6.m6.2.2.2.3.cmml">[</mo><mi id="S2.p2.6.m6.1.1.1.1" xref="S2.p2.6.m6.1.1.1.1.cmml">i</mi><mo id="S2.p2.6.m6.2.2.2.4.2" xref="S2.p2.6.m6.2.2.2.3.cmml">,</mo><mi id="S2.p2.6.m6.2.2.2.2" xref="S2.p2.6.m6.2.2.2.2.cmml">j</mi><mo id="S2.p2.6.m6.2.2.2.4.3" stretchy="false" xref="S2.p2.6.m6.2.2.2.3.cmml">]</mo></mrow></msub><mo id="S2.p2.6.m6.2.3.1" xref="S2.p2.6.m6.2.3.1.cmml">âˆˆ</mo><msub id="S2.p2.6.m6.2.3.3" xref="S2.p2.6.m6.2.3.3.cmml"><mi id="S2.p2.6.m6.2.3.3.2" xref="S2.p2.6.m6.2.3.3.2.cmml">A</mi><mi id="S2.p2.6.m6.2.3.3.3" xref="S2.p2.6.m6.2.3.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.2b"><apply id="S2.p2.6.m6.2.3.cmml" xref="S2.p2.6.m6.2.3"><in id="S2.p2.6.m6.2.3.1.cmml" xref="S2.p2.6.m6.2.3.1"></in><apply id="S2.p2.6.m6.2.3.2.cmml" xref="S2.p2.6.m6.2.3.2"><csymbol cd="ambiguous" id="S2.p2.6.m6.2.3.2.1.cmml" xref="S2.p2.6.m6.2.3.2">subscript</csymbol><ci id="S2.p2.6.m6.2.3.2.2.cmml" xref="S2.p2.6.m6.2.3.2.2">ğ‘</ci><interval closure="closed" id="S2.p2.6.m6.2.2.2.3.cmml" xref="S2.p2.6.m6.2.2.2.4"><ci id="S2.p2.6.m6.1.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1.1">ğ‘–</ci><ci id="S2.p2.6.m6.2.2.2.2.cmml" xref="S2.p2.6.m6.2.2.2.2">ğ‘—</ci></interval></apply><apply id="S2.p2.6.m6.2.3.3.cmml" xref="S2.p2.6.m6.2.3.3"><csymbol cd="ambiguous" id="S2.p2.6.m6.2.3.3.1.cmml" xref="S2.p2.6.m6.2.3.3">subscript</csymbol><ci id="S2.p2.6.m6.2.3.3.2.cmml" xref="S2.p2.6.m6.2.3.3.2">ğ´</ci><ci id="S2.p2.6.m6.2.3.3.3.cmml" xref="S2.p2.6.m6.2.3.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.2c">a_{[i,j]}\in A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m6.2d">italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT âˆˆ italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, where a text answer snippet is a continuous text span in <math alttext="A_{t}" class="ltx_Math" display="inline" id="S2.p2.7.m7.1"><semantics id="S2.p2.7.m7.1a"><msub id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml"><mi id="S2.p2.7.m7.1.1.2" xref="S2.p2.7.m7.1.1.2.cmml">A</mi><mi id="S2.p2.7.m7.1.1.3" xref="S2.p2.7.m7.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><apply id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p2.7.m7.1.1.1.cmml" xref="S2.p2.7.m7.1.1">subscript</csymbol><ci id="S2.p2.7.m7.1.1.2.cmml" xref="S2.p2.7.m7.1.1.2">ğ´</ci><ci id="S2.p2.7.m7.1.1.3.cmml" xref="S2.p2.7.m7.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m7.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.
Next we retrieve multimodal data, namely, <math alttext="I=\{i_{1},i_{2},...,i_{m}\}\in D_{I}" class="ltx_Math" display="inline" id="S2.p2.8.m8.4"><semantics id="S2.p2.8.m8.4a"><mrow id="S2.p2.8.m8.4.4" xref="S2.p2.8.m8.4.4.cmml"><mi id="S2.p2.8.m8.4.4.5" xref="S2.p2.8.m8.4.4.5.cmml">I</mi><mo id="S2.p2.8.m8.4.4.6" xref="S2.p2.8.m8.4.4.6.cmml">=</mo><mrow id="S2.p2.8.m8.4.4.3.3" xref="S2.p2.8.m8.4.4.3.4.cmml"><mo id="S2.p2.8.m8.4.4.3.3.4" stretchy="false" xref="S2.p2.8.m8.4.4.3.4.cmml">{</mo><msub id="S2.p2.8.m8.2.2.1.1.1" xref="S2.p2.8.m8.2.2.1.1.1.cmml"><mi id="S2.p2.8.m8.2.2.1.1.1.2" xref="S2.p2.8.m8.2.2.1.1.1.2.cmml">i</mi><mn id="S2.p2.8.m8.2.2.1.1.1.3" xref="S2.p2.8.m8.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p2.8.m8.4.4.3.3.5" xref="S2.p2.8.m8.4.4.3.4.cmml">,</mo><msub id="S2.p2.8.m8.3.3.2.2.2" xref="S2.p2.8.m8.3.3.2.2.2.cmml"><mi id="S2.p2.8.m8.3.3.2.2.2.2" xref="S2.p2.8.m8.3.3.2.2.2.2.cmml">i</mi><mn id="S2.p2.8.m8.3.3.2.2.2.3" xref="S2.p2.8.m8.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p2.8.m8.4.4.3.3.6" xref="S2.p2.8.m8.4.4.3.4.cmml">,</mo><mi id="S2.p2.8.m8.1.1" mathvariant="normal" xref="S2.p2.8.m8.1.1.cmml">â€¦</mi><mo id="S2.p2.8.m8.4.4.3.3.7" xref="S2.p2.8.m8.4.4.3.4.cmml">,</mo><msub id="S2.p2.8.m8.4.4.3.3.3" xref="S2.p2.8.m8.4.4.3.3.3.cmml"><mi id="S2.p2.8.m8.4.4.3.3.3.2" xref="S2.p2.8.m8.4.4.3.3.3.2.cmml">i</mi><mi id="S2.p2.8.m8.4.4.3.3.3.3" xref="S2.p2.8.m8.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S2.p2.8.m8.4.4.3.3.8" stretchy="false" xref="S2.p2.8.m8.4.4.3.4.cmml">}</mo></mrow><mo id="S2.p2.8.m8.4.4.7" xref="S2.p2.8.m8.4.4.7.cmml">âˆˆ</mo><msub id="S2.p2.8.m8.4.4.8" xref="S2.p2.8.m8.4.4.8.cmml"><mi id="S2.p2.8.m8.4.4.8.2" xref="S2.p2.8.m8.4.4.8.2.cmml">D</mi><mi id="S2.p2.8.m8.4.4.8.3" xref="S2.p2.8.m8.4.4.8.3.cmml">I</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.4b"><apply id="S2.p2.8.m8.4.4.cmml" xref="S2.p2.8.m8.4.4"><and id="S2.p2.8.m8.4.4a.cmml" xref="S2.p2.8.m8.4.4"></and><apply id="S2.p2.8.m8.4.4b.cmml" xref="S2.p2.8.m8.4.4"><eq id="S2.p2.8.m8.4.4.6.cmml" xref="S2.p2.8.m8.4.4.6"></eq><ci id="S2.p2.8.m8.4.4.5.cmml" xref="S2.p2.8.m8.4.4.5">ğ¼</ci><set id="S2.p2.8.m8.4.4.3.4.cmml" xref="S2.p2.8.m8.4.4.3.3"><apply id="S2.p2.8.m8.2.2.1.1.1.cmml" xref="S2.p2.8.m8.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p2.8.m8.2.2.1.1.1.1.cmml" xref="S2.p2.8.m8.2.2.1.1.1">subscript</csymbol><ci id="S2.p2.8.m8.2.2.1.1.1.2.cmml" xref="S2.p2.8.m8.2.2.1.1.1.2">ğ‘–</ci><cn id="S2.p2.8.m8.2.2.1.1.1.3.cmml" type="integer" xref="S2.p2.8.m8.2.2.1.1.1.3">1</cn></apply><apply id="S2.p2.8.m8.3.3.2.2.2.cmml" xref="S2.p2.8.m8.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p2.8.m8.3.3.2.2.2.1.cmml" xref="S2.p2.8.m8.3.3.2.2.2">subscript</csymbol><ci id="S2.p2.8.m8.3.3.2.2.2.2.cmml" xref="S2.p2.8.m8.3.3.2.2.2.2">ğ‘–</ci><cn id="S2.p2.8.m8.3.3.2.2.2.3.cmml" type="integer" xref="S2.p2.8.m8.3.3.2.2.2.3">2</cn></apply><ci id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1">â€¦</ci><apply id="S2.p2.8.m8.4.4.3.3.3.cmml" xref="S2.p2.8.m8.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p2.8.m8.4.4.3.3.3.1.cmml" xref="S2.p2.8.m8.4.4.3.3.3">subscript</csymbol><ci id="S2.p2.8.m8.4.4.3.3.3.2.cmml" xref="S2.p2.8.m8.4.4.3.3.3.2">ğ‘–</ci><ci id="S2.p2.8.m8.4.4.3.3.3.3.cmml" xref="S2.p2.8.m8.4.4.3.3.3.3">ğ‘š</ci></apply></set></apply><apply id="S2.p2.8.m8.4.4c.cmml" xref="S2.p2.8.m8.4.4"><in id="S2.p2.8.m8.4.4.7.cmml" xref="S2.p2.8.m8.4.4.7"></in><share href="https://arxiv.org/html/2408.08521v1#S2.p2.8.m8.4.4.3.cmml" id="S2.p2.8.m8.4.4d.cmml" xref="S2.p2.8.m8.4.4"></share><apply id="S2.p2.8.m8.4.4.8.cmml" xref="S2.p2.8.m8.4.4.8"><csymbol cd="ambiguous" id="S2.p2.8.m8.4.4.8.1.cmml" xref="S2.p2.8.m8.4.4.8">subscript</csymbol><ci id="S2.p2.8.m8.4.4.8.2.cmml" xref="S2.p2.8.m8.4.4.8.2">ğ·</ci><ci id="S2.p2.8.m8.4.4.8.3.cmml" xref="S2.p2.8.m8.4.4.8.3">ğ¼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.4c">I=\{i_{1},i_{2},...,i_{m}\}\in D_{I}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.8.m8.4d">italic_I = { italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_i start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } âˆˆ italic_D start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="T=\{t_{1},t_{2},...,t_{k}\}\in D_{T}" class="ltx_Math" display="inline" id="S2.p2.9.m9.4"><semantics id="S2.p2.9.m9.4a"><mrow id="S2.p2.9.m9.4.4" xref="S2.p2.9.m9.4.4.cmml"><mi id="S2.p2.9.m9.4.4.5" xref="S2.p2.9.m9.4.4.5.cmml">T</mi><mo id="S2.p2.9.m9.4.4.6" xref="S2.p2.9.m9.4.4.6.cmml">=</mo><mrow id="S2.p2.9.m9.4.4.3.3" xref="S2.p2.9.m9.4.4.3.4.cmml"><mo id="S2.p2.9.m9.4.4.3.3.4" stretchy="false" xref="S2.p2.9.m9.4.4.3.4.cmml">{</mo><msub id="S2.p2.9.m9.2.2.1.1.1" xref="S2.p2.9.m9.2.2.1.1.1.cmml"><mi id="S2.p2.9.m9.2.2.1.1.1.2" xref="S2.p2.9.m9.2.2.1.1.1.2.cmml">t</mi><mn id="S2.p2.9.m9.2.2.1.1.1.3" xref="S2.p2.9.m9.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p2.9.m9.4.4.3.3.5" xref="S2.p2.9.m9.4.4.3.4.cmml">,</mo><msub id="S2.p2.9.m9.3.3.2.2.2" xref="S2.p2.9.m9.3.3.2.2.2.cmml"><mi id="S2.p2.9.m9.3.3.2.2.2.2" xref="S2.p2.9.m9.3.3.2.2.2.2.cmml">t</mi><mn id="S2.p2.9.m9.3.3.2.2.2.3" xref="S2.p2.9.m9.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p2.9.m9.4.4.3.3.6" xref="S2.p2.9.m9.4.4.3.4.cmml">,</mo><mi id="S2.p2.9.m9.1.1" mathvariant="normal" xref="S2.p2.9.m9.1.1.cmml">â€¦</mi><mo id="S2.p2.9.m9.4.4.3.3.7" xref="S2.p2.9.m9.4.4.3.4.cmml">,</mo><msub id="S2.p2.9.m9.4.4.3.3.3" xref="S2.p2.9.m9.4.4.3.3.3.cmml"><mi id="S2.p2.9.m9.4.4.3.3.3.2" xref="S2.p2.9.m9.4.4.3.3.3.2.cmml">t</mi><mi id="S2.p2.9.m9.4.4.3.3.3.3" xref="S2.p2.9.m9.4.4.3.3.3.3.cmml">k</mi></msub><mo id="S2.p2.9.m9.4.4.3.3.8" stretchy="false" xref="S2.p2.9.m9.4.4.3.4.cmml">}</mo></mrow><mo id="S2.p2.9.m9.4.4.7" xref="S2.p2.9.m9.4.4.7.cmml">âˆˆ</mo><msub id="S2.p2.9.m9.4.4.8" xref="S2.p2.9.m9.4.4.8.cmml"><mi id="S2.p2.9.m9.4.4.8.2" xref="S2.p2.9.m9.4.4.8.2.cmml">D</mi><mi id="S2.p2.9.m9.4.4.8.3" xref="S2.p2.9.m9.4.4.8.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.4b"><apply id="S2.p2.9.m9.4.4.cmml" xref="S2.p2.9.m9.4.4"><and id="S2.p2.9.m9.4.4a.cmml" xref="S2.p2.9.m9.4.4"></and><apply id="S2.p2.9.m9.4.4b.cmml" xref="S2.p2.9.m9.4.4"><eq id="S2.p2.9.m9.4.4.6.cmml" xref="S2.p2.9.m9.4.4.6"></eq><ci id="S2.p2.9.m9.4.4.5.cmml" xref="S2.p2.9.m9.4.4.5">ğ‘‡</ci><set id="S2.p2.9.m9.4.4.3.4.cmml" xref="S2.p2.9.m9.4.4.3.3"><apply id="S2.p2.9.m9.2.2.1.1.1.cmml" xref="S2.p2.9.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p2.9.m9.2.2.1.1.1.1.cmml" xref="S2.p2.9.m9.2.2.1.1.1">subscript</csymbol><ci id="S2.p2.9.m9.2.2.1.1.1.2.cmml" xref="S2.p2.9.m9.2.2.1.1.1.2">ğ‘¡</ci><cn id="S2.p2.9.m9.2.2.1.1.1.3.cmml" type="integer" xref="S2.p2.9.m9.2.2.1.1.1.3">1</cn></apply><apply id="S2.p2.9.m9.3.3.2.2.2.cmml" xref="S2.p2.9.m9.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p2.9.m9.3.3.2.2.2.1.cmml" xref="S2.p2.9.m9.3.3.2.2.2">subscript</csymbol><ci id="S2.p2.9.m9.3.3.2.2.2.2.cmml" xref="S2.p2.9.m9.3.3.2.2.2.2">ğ‘¡</ci><cn id="S2.p2.9.m9.3.3.2.2.2.3.cmml" type="integer" xref="S2.p2.9.m9.3.3.2.2.2.3">2</cn></apply><ci id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1">â€¦</ci><apply id="S2.p2.9.m9.4.4.3.3.3.cmml" xref="S2.p2.9.m9.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p2.9.m9.4.4.3.3.3.1.cmml" xref="S2.p2.9.m9.4.4.3.3.3">subscript</csymbol><ci id="S2.p2.9.m9.4.4.3.3.3.2.cmml" xref="S2.p2.9.m9.4.4.3.3.3.2">ğ‘¡</ci><ci id="S2.p2.9.m9.4.4.3.3.3.3.cmml" xref="S2.p2.9.m9.4.4.3.3.3.3">ğ‘˜</ci></apply></set></apply><apply id="S2.p2.9.m9.4.4c.cmml" xref="S2.p2.9.m9.4.4"><in id="S2.p2.9.m9.4.4.7.cmml" xref="S2.p2.9.m9.4.4.7"></in><share href="https://arxiv.org/html/2408.08521v1#S2.p2.9.m9.4.4.3.cmml" id="S2.p2.9.m9.4.4d.cmml" xref="S2.p2.9.m9.4.4"></share><apply id="S2.p2.9.m9.4.4.8.cmml" xref="S2.p2.9.m9.4.4.8"><csymbol cd="ambiguous" id="S2.p2.9.m9.4.4.8.1.cmml" xref="S2.p2.9.m9.4.4.8">subscript</csymbol><ci id="S2.p2.9.m9.4.4.8.2.cmml" xref="S2.p2.9.m9.4.4.8.2">ğ·</ci><ci id="S2.p2.9.m9.4.4.8.3.cmml" xref="S2.p2.9.m9.4.4.8.3">ğ‘‡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.4c">T=\{t_{1},t_{2},...,t_{k}\}\in D_{T}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.9.m9.4d">italic_T = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } âˆˆ italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="V=\{v_{1},v_{2},...,v_{l}\}\in D_{V}" class="ltx_Math" display="inline" id="S2.p2.10.m10.4"><semantics id="S2.p2.10.m10.4a"><mrow id="S2.p2.10.m10.4.4" xref="S2.p2.10.m10.4.4.cmml"><mi id="S2.p2.10.m10.4.4.5" xref="S2.p2.10.m10.4.4.5.cmml">V</mi><mo id="S2.p2.10.m10.4.4.6" xref="S2.p2.10.m10.4.4.6.cmml">=</mo><mrow id="S2.p2.10.m10.4.4.3.3" xref="S2.p2.10.m10.4.4.3.4.cmml"><mo id="S2.p2.10.m10.4.4.3.3.4" stretchy="false" xref="S2.p2.10.m10.4.4.3.4.cmml">{</mo><msub id="S2.p2.10.m10.2.2.1.1.1" xref="S2.p2.10.m10.2.2.1.1.1.cmml"><mi id="S2.p2.10.m10.2.2.1.1.1.2" xref="S2.p2.10.m10.2.2.1.1.1.2.cmml">v</mi><mn id="S2.p2.10.m10.2.2.1.1.1.3" xref="S2.p2.10.m10.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.p2.10.m10.4.4.3.3.5" xref="S2.p2.10.m10.4.4.3.4.cmml">,</mo><msub id="S2.p2.10.m10.3.3.2.2.2" xref="S2.p2.10.m10.3.3.2.2.2.cmml"><mi id="S2.p2.10.m10.3.3.2.2.2.2" xref="S2.p2.10.m10.3.3.2.2.2.2.cmml">v</mi><mn id="S2.p2.10.m10.3.3.2.2.2.3" xref="S2.p2.10.m10.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.p2.10.m10.4.4.3.3.6" xref="S2.p2.10.m10.4.4.3.4.cmml">,</mo><mi id="S2.p2.10.m10.1.1" mathvariant="normal" xref="S2.p2.10.m10.1.1.cmml">â€¦</mi><mo id="S2.p2.10.m10.4.4.3.3.7" xref="S2.p2.10.m10.4.4.3.4.cmml">,</mo><msub id="S2.p2.10.m10.4.4.3.3.3" xref="S2.p2.10.m10.4.4.3.3.3.cmml"><mi id="S2.p2.10.m10.4.4.3.3.3.2" xref="S2.p2.10.m10.4.4.3.3.3.2.cmml">v</mi><mi id="S2.p2.10.m10.4.4.3.3.3.3" xref="S2.p2.10.m10.4.4.3.3.3.3.cmml">l</mi></msub><mo id="S2.p2.10.m10.4.4.3.3.8" stretchy="false" xref="S2.p2.10.m10.4.4.3.4.cmml">}</mo></mrow><mo id="S2.p2.10.m10.4.4.7" xref="S2.p2.10.m10.4.4.7.cmml">âˆˆ</mo><msub id="S2.p2.10.m10.4.4.8" xref="S2.p2.10.m10.4.4.8.cmml"><mi id="S2.p2.10.m10.4.4.8.2" xref="S2.p2.10.m10.4.4.8.2.cmml">D</mi><mi id="S2.p2.10.m10.4.4.8.3" xref="S2.p2.10.m10.4.4.8.3.cmml">V</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.4b"><apply id="S2.p2.10.m10.4.4.cmml" xref="S2.p2.10.m10.4.4"><and id="S2.p2.10.m10.4.4a.cmml" xref="S2.p2.10.m10.4.4"></and><apply id="S2.p2.10.m10.4.4b.cmml" xref="S2.p2.10.m10.4.4"><eq id="S2.p2.10.m10.4.4.6.cmml" xref="S2.p2.10.m10.4.4.6"></eq><ci id="S2.p2.10.m10.4.4.5.cmml" xref="S2.p2.10.m10.4.4.5">ğ‘‰</ci><set id="S2.p2.10.m10.4.4.3.4.cmml" xref="S2.p2.10.m10.4.4.3.3"><apply id="S2.p2.10.m10.2.2.1.1.1.cmml" xref="S2.p2.10.m10.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.p2.10.m10.2.2.1.1.1.1.cmml" xref="S2.p2.10.m10.2.2.1.1.1">subscript</csymbol><ci id="S2.p2.10.m10.2.2.1.1.1.2.cmml" xref="S2.p2.10.m10.2.2.1.1.1.2">ğ‘£</ci><cn id="S2.p2.10.m10.2.2.1.1.1.3.cmml" type="integer" xref="S2.p2.10.m10.2.2.1.1.1.3">1</cn></apply><apply id="S2.p2.10.m10.3.3.2.2.2.cmml" xref="S2.p2.10.m10.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.p2.10.m10.3.3.2.2.2.1.cmml" xref="S2.p2.10.m10.3.3.2.2.2">subscript</csymbol><ci id="S2.p2.10.m10.3.3.2.2.2.2.cmml" xref="S2.p2.10.m10.3.3.2.2.2.2">ğ‘£</ci><cn id="S2.p2.10.m10.3.3.2.2.2.3.cmml" type="integer" xref="S2.p2.10.m10.3.3.2.2.2.3">2</cn></apply><ci id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1">â€¦</ci><apply id="S2.p2.10.m10.4.4.3.3.3.cmml" xref="S2.p2.10.m10.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.p2.10.m10.4.4.3.3.3.1.cmml" xref="S2.p2.10.m10.4.4.3.3.3">subscript</csymbol><ci id="S2.p2.10.m10.4.4.3.3.3.2.cmml" xref="S2.p2.10.m10.4.4.3.3.3.2">ğ‘£</ci><ci id="S2.p2.10.m10.4.4.3.3.3.3.cmml" xref="S2.p2.10.m10.4.4.3.3.3.3">ğ‘™</ci></apply></set></apply><apply id="S2.p2.10.m10.4.4c.cmml" xref="S2.p2.10.m10.4.4"><in id="S2.p2.10.m10.4.4.7.cmml" xref="S2.p2.10.m10.4.4.7"></in><share href="https://arxiv.org/html/2408.08521v1#S2.p2.10.m10.4.4.3.cmml" id="S2.p2.10.m10.4.4d.cmml" xref="S2.p2.10.m10.4.4"></share><apply id="S2.p2.10.m10.4.4.8.cmml" xref="S2.p2.10.m10.4.4.8"><csymbol cd="ambiguous" id="S2.p2.10.m10.4.4.8.1.cmml" xref="S2.p2.10.m10.4.4.8">subscript</csymbol><ci id="S2.p2.10.m10.4.4.8.2.cmml" xref="S2.p2.10.m10.4.4.8.2">ğ·</ci><ci id="S2.p2.10.m10.4.4.8.3.cmml" xref="S2.p2.10.m10.4.4.8.3">ğ‘‰</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.4c">V=\{v_{1},v_{2},...,v_{l}\}\in D_{V}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.10.m10.4d">italic_V = { italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT } âˆˆ italic_D start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> that are relevant to the text answer snippets <math alttext="a_{[i,j]}" class="ltx_Math" display="inline" id="S2.p2.11.m11.2"><semantics id="S2.p2.11.m11.2a"><msub id="S2.p2.11.m11.2.3" xref="S2.p2.11.m11.2.3.cmml"><mi id="S2.p2.11.m11.2.3.2" xref="S2.p2.11.m11.2.3.2.cmml">a</mi><mrow id="S2.p2.11.m11.2.2.2.4" xref="S2.p2.11.m11.2.2.2.3.cmml"><mo id="S2.p2.11.m11.2.2.2.4.1" stretchy="false" xref="S2.p2.11.m11.2.2.2.3.cmml">[</mo><mi id="S2.p2.11.m11.1.1.1.1" xref="S2.p2.11.m11.1.1.1.1.cmml">i</mi><mo id="S2.p2.11.m11.2.2.2.4.2" xref="S2.p2.11.m11.2.2.2.3.cmml">,</mo><mi id="S2.p2.11.m11.2.2.2.2" xref="S2.p2.11.m11.2.2.2.2.cmml">j</mi><mo id="S2.p2.11.m11.2.2.2.4.3" stretchy="false" xref="S2.p2.11.m11.2.2.2.3.cmml">]</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p2.11.m11.2b"><apply id="S2.p2.11.m11.2.3.cmml" xref="S2.p2.11.m11.2.3"><csymbol cd="ambiguous" id="S2.p2.11.m11.2.3.1.cmml" xref="S2.p2.11.m11.2.3">subscript</csymbol><ci id="S2.p2.11.m11.2.3.2.cmml" xref="S2.p2.11.m11.2.3.2">ğ‘</ci><interval closure="closed" id="S2.p2.11.m11.2.2.2.3.cmml" xref="S2.p2.11.m11.2.2.2.4"><ci id="S2.p2.11.m11.1.1.1.1.cmml" xref="S2.p2.11.m11.1.1.1.1">ğ‘–</ci><ci id="S2.p2.11.m11.2.2.2.2.cmml" xref="S2.p2.11.m11.2.2.2.2">ğ‘—</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m11.2c">a_{[i,j]}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.11.m11.2d">italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT</annotation></semantics></math>.
Finally, the multimodal answer refinement component generates the final multimodal answer <math alttext="A_{mm}" class="ltx_Math" display="inline" id="S2.p2.12.m12.1"><semantics id="S2.p2.12.m12.1a"><msub id="S2.p2.12.m12.1.1" xref="S2.p2.12.m12.1.1.cmml"><mi id="S2.p2.12.m12.1.1.2" xref="S2.p2.12.m12.1.1.2.cmml">A</mi><mrow id="S2.p2.12.m12.1.1.3" xref="S2.p2.12.m12.1.1.3.cmml"><mi id="S2.p2.12.m12.1.1.3.2" xref="S2.p2.12.m12.1.1.3.2.cmml">m</mi><mo id="S2.p2.12.m12.1.1.3.1" xref="S2.p2.12.m12.1.1.3.1.cmml">â¢</mo><mi id="S2.p2.12.m12.1.1.3.3" xref="S2.p2.12.m12.1.1.3.3.cmml">m</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.p2.12.m12.1b"><apply id="S2.p2.12.m12.1.1.cmml" xref="S2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S2.p2.12.m12.1.1.1.cmml" xref="S2.p2.12.m12.1.1">subscript</csymbol><ci id="S2.p2.12.m12.1.1.2.cmml" xref="S2.p2.12.m12.1.1.2">ğ´</ci><apply id="S2.p2.12.m12.1.1.3.cmml" xref="S2.p2.12.m12.1.1.3"><times id="S2.p2.12.m12.1.1.3.1.cmml" xref="S2.p2.12.m12.1.1.3.1"></times><ci id="S2.p2.12.m12.1.1.3.2.cmml" xref="S2.p2.12.m12.1.1.3.2">ğ‘š</ci><ci id="S2.p2.12.m12.1.1.3.3.cmml" xref="S2.p2.12.m12.1.1.3.3">ğ‘š</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.12.m12.1c">A_{mm}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.12.m12.1d">italic_A start_POSTSUBSCRIPT italic_m italic_m end_POSTSUBSCRIPT</annotation></semantics></math> by incorporating the retrieved multimodal data and text response snippets.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Directly prompting or using techniques such as chain-of-thoughtÂ <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib14" title="">2022</a>)</cite> with LLMs for tasks involving both text and multimodal data is ineffective due to two main reasons. First, the complexity of the task overwhelms the LLM as it needs to determine which data to reference, decide whether to display multimodal data, and figure out where to place it. Additionally, this complexity results in low-quality multimodal answers. To address this, we propose a step-by-step strategy that first generates a pure text answer and then refines it with multimodal data.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text Answer Generation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.4">Our text answer generation follows the RAGÂ <cite class="ltx_cite ltx_citemacro_cite">Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib6" title="">2020</a>)</cite> style method.
Given user queries, we fine-tune a pre-trained text embedding modelÂ <cite class="ltx_cite ltx_citemacro_cite">Reimers and Gurevych (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib10" title="">2019</a>)</cite> on an annotated dataset, and apply this model to embed text snippets which are stored in a vector index.
The index is used to retrieve relevant text document snippets <math alttext="S" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_S</annotation></semantics></math> based on the user query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_q</annotation></semantics></math>, for which the top <math alttext="5" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mn id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><cn id="S2.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S2.SS1.p1.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">5</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">5</annotation></semantics></math> text document snippets are selected.
The LLM is then prompted with the user queries and the retrieved text snippets to generate answer <math alttext="A_{t}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">A</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">ğ´</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, the text prompt can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS4.SSS0.Px1" title="Prompt for Text Answer Generation â€£ A.4 Prompts â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Source-Based Multimodal Retrieval</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The source-based multimodal retrieval component comprises two steps: source attribution and section-level multimodal data retrieval.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Source Attribution.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.8">In the source attribution step, the text answer snippet <math alttext="a_{[i,j]}\in A_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.1.m1.2"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.2a"><mrow id="S2.SS2.SSS0.Px1.p1.1.m1.2.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.cmml"><msub id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.2.cmml">a</mi><mrow id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.4" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml"><mo id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.4.1" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">[</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.4.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.cmml">j</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.4.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml">]</mo></mrow></msub><mo id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.1.cmml">âˆˆ</mo><msub id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.2.cmml">A</mi><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.2b"><apply id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3"><in id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.1"></in><apply id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.2.2">ğ‘</ci><interval closure="closed" id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.4"><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.1.1">ğ‘–</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.2.2">ğ‘—</ci></interval></apply><apply id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.2">ğ´</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.3.3.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.2c">a_{[i,j]}\in A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.1.m1.2d">italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT âˆˆ italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is matched to the corresponding text document snippet <math alttext="s_{i}\in S" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><msub id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">âˆˆ</mo><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1"><in id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.1"></in><apply id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.2.3">ğ‘–</ci></apply><ci id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.2.m2.1c">s_{i}\in S</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.2.m2.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ italic_S</annotation></semantics></math>. Specifically, the text answer <math alttext="A_{t}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS2.SSS0.Px1.p1.3.m3.1a"><msub id="S2.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">A</mi><mi id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.2">ğ´</ci><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.3.m3.1c">A_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.3.m3.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is divided into sentences, and each sentence is compared with text document snippets <math alttext="S" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S2.SS2.SSS0.Px1.p1.4.m4.1a"><mi id="S2.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.4.m4.1b"><ci id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.4.m4.1c">S</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.4.m4.1d">italic_S</annotation></semantics></math> using cosine similarity between their embeddings. For each text answer snippet <math alttext="a_{[i,j]}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.5.m5.2"><semantics id="S2.SS2.SSS0.Px1.p1.5.m5.2a"><msub id="S2.SS2.SSS0.Px1.p1.5.m5.2.3" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.5.m5.2.3.2" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.3.2.cmml">a</mi><mrow id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.4" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml"><mo id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.4.1" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">[</mo><mi id="S2.SS2.SSS0.Px1.p1.5.m5.1.1.1.1" xref="S2.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.cmml">i</mi><mo id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.4.2" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.2" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.cmml">j</mi><mo id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.4.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml">]</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.5.m5.2b"><apply id="S2.SS2.SSS0.Px1.p1.5.m5.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.5.m5.2.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.5.m5.2.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.3.2">ğ‘</ci><interval closure="closed" id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.4"><ci id="S2.SS2.SSS0.Px1.p1.5.m5.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.5.m5.1.1.1.1">ğ‘–</ci><ci id="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.5.m5.2.2.2.2">ğ‘—</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.5.m5.2c">a_{[i,j]}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.5.m5.2d">italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT</annotation></semantics></math>, the text document snippet <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.6.m6.1"><semantics id="S2.SS2.SSS0.Px1.p1.6.m6.1a"><msub id="S2.SS2.SSS0.Px1.p1.6.m6.1.1" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.6.m6.1.1.2" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px1.p1.6.m6.1.1.3" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.6.m6.1b"><apply id="S2.SS2.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.6.m6.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.6.m6.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with the highest cosine similarity score is identified as the source of <math alttext="a_{[i,j]}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.7.m7.2"><semantics id="S2.SS2.SSS0.Px1.p1.7.m7.2a"><msub id="S2.SS2.SSS0.Px1.p1.7.m7.2.3" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.7.m7.2.3.2" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.3.2.cmml">a</mi><mrow id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.4" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.3.cmml"><mo id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.4.1" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.3.cmml">[</mo><mi id="S2.SS2.SSS0.Px1.p1.7.m7.1.1.1.1" xref="S2.SS2.SSS0.Px1.p1.7.m7.1.1.1.1.cmml">i</mi><mo id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.4.2" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.3.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.2" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.2.cmml">j</mi><mo id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.4.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.3.cmml">]</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.7.m7.2b"><apply id="S2.SS2.SSS0.Px1.p1.7.m7.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.7.m7.2.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.7.m7.2.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.3.2">ğ‘</ci><interval closure="closed" id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.4"><ci id="S2.SS2.SSS0.Px1.p1.7.m7.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.7.m7.1.1.1.1">ğ‘–</ci><ci id="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.7.m7.2.2.2.2">ğ‘—</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.7.m7.2c">a_{[i,j]}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.7.m7.2d">italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT</annotation></semantics></math>. We also discard snippets with scores below a predefined threshold.
This step is crucial because the identified text answer snippet <math alttext="a_{[i,j]}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.8.m8.2"><semantics id="S2.SS2.SSS0.Px1.p1.8.m8.2a"><msub id="S2.SS2.SSS0.Px1.p1.8.m8.2.3" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.8.m8.2.3.2" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.3.2.cmml">a</mi><mrow id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.4" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.3.cmml"><mo id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.4.1" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.3.cmml">[</mo><mi id="S2.SS2.SSS0.Px1.p1.8.m8.1.1.1.1" xref="S2.SS2.SSS0.Px1.p1.8.m8.1.1.1.1.cmml">i</mi><mo id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.4.2" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.3.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.2" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.2.cmml">j</mi><mo id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.4.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.3.cmml">]</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.8.m8.2b"><apply id="S2.SS2.SSS0.Px1.p1.8.m8.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.8.m8.2.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.8.m8.2.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.3.2">ğ‘</ci><interval closure="closed" id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.4"><ci id="S2.SS2.SSS0.Px1.p1.8.m8.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.8.m8.1.1.1.1">ğ‘–</ci><ci id="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.8.m8.2.2.2.2">ğ‘—</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.8.m8.2c">a_{[i,j]}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.8.m8.2d">italic_a start_POSTSUBSCRIPT [ italic_i , italic_j ] end_POSTSUBSCRIPT</annotation></semantics></math> serves as the basis for retrieving relevant multimodal data, which will be integrated around the text answer in the final multimodal answer.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Section-Level Multimodal Data Retrieval.</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.3">In the section-level multimodal data retrieval step, we first identify multimodal data located in the same section as the retrieved text document snippet <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.1.m1.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.3.1">i.e.</span>, the multimodal data is located at the same hierarchy level in the original document <math alttext="D_{S}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.2">ğ·</ci><ci id="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.2.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.2.m2.1c">D_{S}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> as the text document snippet <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS2.SSS0.Px2.p1.3.m3.1a"><msub id="S2.SS2.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.3.m3.1b"><apply id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.3.m3.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.3.m3.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. This step aims to reduce the search space and ensure the precision of the retrieval results.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p2.1">Next, we apply the same fine-tuned embedding model used in text document retrieval for multimodal retrieval. To better represent the multimodal data, we include the context surrounding the multimodal data and additional text data as the input for the embedding model. Specifically, for images, we include image captions generated by an LLM; for tables the table content, and for videos the LLM generated video transcript summaries.
The final multimodal responses are selected based on the cosine similarity between the text document snippet <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p2.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.2">ğ‘ </ci><ci id="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p2.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.1.m1.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.1.m1.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the multimodal data embeddings. Only the top multimodal data is selected to ensure relevance. It is worth noting that the same multimodal data could be selected for multiple text answer snippets. To avoid including the same multimodal data in the final answer multiple times, we keep only the one with the highest similarity score.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multimodal Answer Refinement</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Once the relevant multimodal responses are retrieved, the LLM is prompted to generate the final answer.
This process involves providing the LLM with the user question, initial text answer, and initial multimodal response along with their context information.
For each multimodal response, placeholders are added to the initial text answer at the corresponding source locations. Each placeholder includes the multimodal data and its context information, ensuring that the LLM accurately integrates relevant content without generating irrelevant details. Additionally, a few example responses are provided to guide the model in producing higher quality and more consistent outputs.
The final prompt is formulated by concatenating the user question, modified initial text answer, multimodal data, their context information, and example responses. The prompt example can be found in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS4.SSS0.Px2" title="Prompt for Multimodal Answer Refinement â€£ A.4 Prompts â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>User Interface</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We implemented the MuRAR framework and integrated it into a prototype version AI Assistant in Adobe Experience Platform<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://experienceleague.adobe.com/en/docs/experience-platform/ai-assistant/home</span></span></span>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">As shown in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S2.F2" title="Figure 2 â€£ 2 Design of MuRAR â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>, when a user queries AI assistant, for instance, asking â€œWhat is a good tutorial for creating a schema?â€, the process begins with the userâ€™s query (A). The MuRAR first generates a text answer based on the text snippet retrieval (B), and recognizes text answer snippets by source attribution (C).</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Next, MuRAR retrieves the most relevant multimodal data (D1 and D2). In this case, a screenshot of the Schemas Workspace and a video tutorial detailing the process of schema creation are retrieved. Finally, MuRAR integrates the relevant multimodal data into the text answer through multimodal answer refinement (E). This multimodal presentation helps users understand complex instructions more effectively.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data Collection</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We collected two datasets: a multimodal dataset as the source data for implementing the MuRAR framework within the AI assistant application, and a multimodal question-answering dataset for human evaluation.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Multimodal Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The multimodal dataset was curated from 2,173 documentation pages from Adobe Experience LeagueÂ <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_href" href="https://experienceleague.adobe.com/" title="">https://experienceleague.adobe.com/</a></span></span></span>.
The dataset, as shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S4.T1" title="Table 1 â€£ 4.1 Multimodal Dataset â€£ 4 Data Collection â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>, encompasses four primary modalities: text, images, tables, and videos, along with their associated metadata such as contextual text and URLs. The textual data includes both pure text and tabular content, while the visual data consists of images and videos. See the design details in Â <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.SS1" title="A.1 Multimodal Scraper Design â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">A.1</span></a> for more information on data collection.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The text content from the documentation pages was initially extracted and tokenized using the GPT-2 tokenizer, segmenting the document into smaller snippets ranging from 11 to 1,500 tokens, resulting in 18,071 text snippets.
For image data, we extracted image URLs along with the surrounding textual context, defined as the text from the start of the section to the image (pre-image context) and from the image to the end of the section (post-image context). Additionally, GPT-4 was used to generate captions for the images, enhancing the data for multimodal retrieval, which resulted in 6,320 image data pieces.
The tables were extracted in JSON format along with their contextual text, similar to the image data. This included the text before and after the table, leading to a total collection of 2,646 table data pieces. For video content, we extracted video URLs, contextual text, and transcripts. When transcripts were not available, we downloaded the videos and used WhisperÂ <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib9" title="">2023</a>)</cite> to generate transcripts from the audio. GPT-4 was then used to summarize these transcripts, enhancing the data for multimodal retrieval. This process produced 253 videos. For all data types, we also gathered the titles of the pages and the headings of the sections where the data was located, providing a comprehensive context for each multimodal data piece.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Modality</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Metric</span></td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.2.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.2.1.1">Text</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.2.2">Count</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.2.2.3">18,071</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.3.3.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span> Avg content tokens</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.3.3.2">192</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.4.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.4.4.1.1">Image</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.4.4.2">Count</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.4.4.3">6,320</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.5.5.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Avg context tokens</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.5.5.2">238</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.6.6.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Avg caption tokens</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.6.6.2">94</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.7.7.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.7.7.1.1">Video</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.7.7.2">Count</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.7.7.3">253</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.8.8.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Avg context tokens</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.8.8.2">91</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.9.9.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Avg summary tokens</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.9.9.2">33</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.10">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.10.10.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.10.10.1.1">Table</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.10.10.2">Count</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.10.10.3">2,644</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.11.11.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Avg context tokens</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T1.1.11.11.2">160</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T1.1.12.12.1">
<span class="ltx_rule" style="width:0.0pt;height:10.8pt;background:black;display:inline-block;"></span>Avg table tokens</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="S4.T1.1.12.12.2">223</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Multimodal dataset statistics.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Multimodal Question Answering Dataset</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In constructing the multimodal question and answer dataset, we collected 764 questions and applied MuRAR to generate answers. Among these, 306 questions had multimodal answers. To evaluate the quality of these answers, we randomly sampled 150 of the 306 questions for human evaluation.
To evaluate the impact of different backbone LLMs on the performance of multimodal answers, we conducted a comparative study using GPT-3.5 and GPT-4. For each model, we generated 150 text and multimodal answers. This analysis allowed us to assess how the choice of LLM influences the effectiveness and coherence of multimodal responses.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Human Evaluation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">To assess the effectiveness of our multi-modal question answering system, we conducted a comprehensive human evaluation study with two phases: (1) single-model evaluation and (2) pairwise comparison. The single-model psychometric evaluation was designed to measure three key metrics: usefulness, readability, and relevance of the multi-modal outputs. As a pairwise comparison, we included a preference-based ranking to determine the overall user preference between text-only and multi-modal responses. Notice that we do not assess the quality of the text content itself, but rather the core contribution brought by attributing multi-modal outputs to the retrieved answer.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Study Setup</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">We collected a dataset of 300 question-answer pairs, equally split between outputs from GPT-3.5 and GPT-4 models. For each pair, we generated both text-only and multi-modal responses. Eight expert annotators, all with advanced degrees in computer science and experience in NLP, were recruited to evaluate these outputs.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Evaluation Schema</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The annotators were asked to rate each multi-modal output on a 5-point Likert scale (1 being lowest, 5 being highest) for the following metrics, which were adapted from <cite class="ltx_cite ltx_citemacro_cite">Pradeep etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#bib.bib8" title="">2024</a>)</cite>:</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Usefulness:</span> This metric measures the extent to which multi-modal elements contribute to the userâ€™s comprehension of the text content. High scores indicate that the multi-modal output provides valuable additional information, clarifies complex concepts, or illustrates key points in ways that significantly aid understanding.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">Readability:</span> This assesses how well the multi-modal elements are integrated with the text, considering factors such as placement, size, and formatting. High scores indicate seamless integration that enhances the overall reading experience.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">Relevance:</span> This measures how closely the multi-modal elements relate to the content of the text. High scores indicate that the multi-modal output directly supports or illustrates the textual content.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S5.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.1.1">Metric</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.2.1">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.3.1">GPT-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.2.1.4.1">Both GPTs</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.2.1">Usefulness</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.2">3.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.3">3.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.2.4">3.47</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.4.3.1">Readability</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.4.3.2">3.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.4.3.3">3.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.4.3.4">3.63</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.5.4.1">Relevance</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.5.4.2">3.66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.5.4.3">3.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.5.4.4">3.78</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.1.6.5.1">Preference Rate</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.6.5.2">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.6.5.3">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.1.6.5.4">0.86</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Evaluation results for multimodal answers generated by GPT-3.5 and GPT-4.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="S5.F3.g1" src="extracted/5794920/Figures/human_evaluation_bar_chart.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of score counts for evaluation metrics across GPT-3.5, GPT-4, and Both-GPTs.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">After rating the multi-modal outputs, annotators were asked to indicate their overall preference between the text-only version, the multi-modal version, or if they found them equally effective (â€œSameâ€).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Results</h3>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Psychometric Evaluation Results.</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.1">The psychometric evaluation focused on three key aspects: usefulness, readability, and relevance. As shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.T2" title="Table 2 â€£ 5.2 Evaluation Schema â€£ 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>, when examine the results generated by both GPT-3.5 and GPT-4, the usefulness metric achieved an average score of 3.47, while readability and relevance scored 3.63 and 3.78, respectively. These scores, all above the midpoint of the scale as reflected in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#S5.F3" title="Figure 3 â€£ 5.2 Evaluation Schema â€£ 5 Human Evaluation â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>, suggest that our approach performs well in producing useful, readable, and relevant output. Qualitative feedback from annotators further supports this conclusion, indicating that the multi-modal generation provides informative additions to the text, is understandable through its placement, and remains relevant to the associated content. In addition, the average preference rating of 0.86 demonstrates a strong overall preference for our method compared to the text-only alternative. When comparing GPT-3.5 with GPT-4, we found that using GPT-4 as the backbone LLM increased all the metrics, showcasing its superior performance in generating high-quality, multi-modal content within our framework.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Inter-Annotator Agreement.</h4>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.1">We calculated two inter-annotator agreement measures: Krippendorffâ€™s alpha (0.4179, moderate agreement) and Cohenâ€™s kappa between top annotators (0.71, substantial agreement). The lower Krippendorffâ€™s alphas can be explained through the annotator-specific analysis (TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.T5" title="Table 5 â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>), which revealed lower average scores for Annotators 2 (3.0619, SD = 0.5648, 26% of annotations) and 4 (3.3836, SD = 0.253, 5.8% of annotations) compared to others.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Findings</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">We analyzed the errors and mistakes made by the MuRAR framework during our human evaluation. We identified some issues in the multimodal retrieval component. Although precision is ensured thanks to source attribution, it may cause low recall, <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">i.e.</span>, relevant multimodal data to the text answer snippet may not be in the same section as the corresponding text document snippet on a web page. Additionally, during the multimodal answer refinement component, readability can be affected. For instance, multimodal data may contain duplicated information already explained in plain text. This repetition can negatively impact the readability and clarity of the multimodal answer, making it less effective for users.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced MuRAR, a framework that enhances text-based responses with images, tables, and videos. Human evaluations showed that MuRARâ€™s multimodal answers are more useful, readable, and relevant than text-only responses. The system uses text answer generation, source-based multimodal retrieval, and answer refinement to create coherent multimodal answers. Identified areas for improvement include enhancing recall of relevant data and reducing redundancy. Future work will refine these processes and expand the systemâ€™s capabilities for more complex queries.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, AndrewÂ M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, TimothyÂ P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, PaulÂ Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.11805" title="">Gemini: A family of highly capable multimodal models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">CoRR</em>, abs/2312.11805.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2312.10997" title="">Retrieval-augmented generation for large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">CoRR</em>, abs/2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Pankaj Joshi, Aditya Gupta, Pankaj Kumar, and Manas Sisodia. 2024.

</span>
<span class="ltx_bibblock">Robust multi model rag pipeline for documents containing text, table &amp; images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC)</em>, pages 993â€“999. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar etÂ al. (2020)</span>
<span class="ltx_bibblock">
Abhishek Kumar, Trisha Mittal, and Dinesh Manocha. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2004.12238" title="">MCQA: multimodal co-attention based network for question answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">CoRR</em>, abs/2004.12238.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Patrick S.Â H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2303.08774" title="">GPT-4 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">CoRR</em>, abs/2303.08774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ronak Pradeep, Daniel Lee, Ali Mousavi, Jeff Pound, Yisi Sang, Jimmy Lin, Ihab Ilyas, Saloni Potdar, Mostafa Arefiyan, and Yunyao Li. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2408.05948" title="">Convkgyarn: Spinning configurable and scalable conversational knowledge graph qa datasets with large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2023)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v202/radford23a.html" title="">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">Proceedings of Machine Learning Research</em>, pages 28492â€“28518. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1908.10084" title="">Sentence-bert: Sentence embeddings using siamese bert-networks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2021)</span>
<span class="ltx_bibblock">
Hrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and BalajiÂ Vasan Srinivasan. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2021.NAACL-MAIN.418" title="">MIMOQA: multimodal input multimodal output question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021</em>, pages 5317â€“5332. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=ee6W5UgQLa" title="">Multimodalqa: complex question answering over text, tables and images</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">CoRR</em>, abs/2302.13971.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, EdÂ H. Chi, QuocÂ V. Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html" title="">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2309.05519" title="">Next-gpt: Any-to-any multimodal LLM</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">CoRR</em>, abs/2309.05519.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<figure class="ltx_table" id="A1.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.1.1">Annotator</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.2.1">Answers</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.3.1">Usefulness</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.4.1">Readability</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.5.1">Relevance</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T3.1.1.1.6"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.6.1">Preference (Multi-Modal / Text Only / Same)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.2.1.1">No.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.2.1.2">294</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.2.1.3">3.6463</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.2.1.4">3.8367</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.2.1.5">4.0170</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.2.1.6">207 (70.41%) / 41 (13.95%) / 46 (15.65%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.3.2.1">No.2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.3.2.2">237</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.3.2.3">2.8861</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.3.2.4">3.0295</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.3.2.5">3.2700</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.3.2.6">115 (48.52%) / 81 (34.18%) / 41 (17.30%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.4.3.1">No.3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.4.3.2">259</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.4.3.3">3.7452</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.4.3.4">3.8340</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.4.3.5">3.8764</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.4.3.6">205 (79.15%) / 28 (10.81%) / 26 (10.04%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.5.4.1">No.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.5.4.2">53</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.5.4.3">3.0566</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.5.4.4">3.5094</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.5.4.5">3.5849</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.5.4.6">30 (56.60%) / 16 (30.19%) / 7 (13.21%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.6.5.1">No.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.6.5.2">9</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.6.5.3">4.1111</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.6.5.4">4.4444</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.6.5.5">4.4444</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.6.5.6">9 (100.0%) / 0 (0.0%) / 0 (0.0%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.7.6.1">No.6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.7.6.2">22</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.7.6.3">4.0000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.7.6.4">4.0909</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.7.6.5">4.1818</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.7.6.6">19 (86.36%) / 2 (9.09%) / 1 (4.55%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.8.7.1">No.7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="A1.T3.1.8.7.2">24</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.8.7.3">4.2500</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.8.7.4">4.2083</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.8.7.5">4.6250</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.8.7.6">23 (95.83%) / 0 (0.0%) / 0 (0.0%)</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T3.1.9.8.1">No.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T3.1.9.8.2">2</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T3.1.9.8.3">4.0000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T3.1.9.8.4">4.5000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T3.1.9.8.5">4.5000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T3.1.9.8.6">2 (100.0%) / 0 (0.0%) / 0 (0.0%)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Per-annotator average scores and preference.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T4.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T4.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.1.1">Metric</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.2.1">GPT-3.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.3.1">GPT-4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.2.1.4"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.1.4.1">Both GPTs</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.3.2.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.3.2.1.1">Usefulness</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.3.2.2">0.9741</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.3.2.3">0.9059</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.3.2.4">0.9496</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T4.1.4.3.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.4.3.1.1">Readability</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.4.3.2">0.7686</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.4.3.3">0.7059</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.4.3.4">0.7500</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T4.1.5.4.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.5.4.1.1">Relevance</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.5.4.2">0.8576</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.5.4.3">0.8301</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.5.4.4">0.8519</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Standard deviation of evaluation metrics for multimodal answers generated by GPT-3.5 and GPT-4.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.3.4.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A1.T5.3.4.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T5.3.4.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.3.4.1.2.1">Agreement Metric</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.3.3.4"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.4.1">Metric&amp;Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1">K-<math alttext="\alpha" class="ltx_Math" display="inline" id="A1.T5.1.1.1.1.m1.1"><semantics id="A1.T5.1.1.1.1.m1.1a"><mi id="A1.T5.1.1.1.1.m1.1.1" xref="A1.T5.1.1.1.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.1.m1.1b"><ci id="A1.T5.1.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.T5.1.1.1.1.m1.1d">italic_Î±</annotation></semantics></math><sub class="ltx_sub" id="A1.T5.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="A1.T5.1.1.1.1.1.1">normal</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T5.2.2.2.1">K-<math alttext="\alpha" class="ltx_Math" display="inline" id="A1.T5.2.2.2.1.m1.1"><semantics id="A1.T5.2.2.2.1.m1.1a"><mi id="A1.T5.2.2.2.1.m1.1.1" xref="A1.T5.2.2.2.1.m1.1.1.cmml">Î±</mi><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.1.m1.1b"><ci id="A1.T5.2.2.2.1.m1.1.1.cmml" xref="A1.T5.2.2.2.1.m1.1.1">ğ›¼</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.1.m1.1d">italic_Î±</annotation></semantics></math><sub class="ltx_sub" id="A1.T5.2.2.2.1.1"><span class="ltx_text ltx_font_medium" id="A1.T5.2.2.2.1.1.1">combined</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T5.3.3.3.1">C-<math alttext="\kappa" class="ltx_Math" display="inline" id="A1.T5.3.3.3.1.m1.1"><semantics id="A1.T5.3.3.3.1.m1.1a"><mi id="A1.T5.3.3.3.1.m1.1.1" xref="A1.T5.3.3.3.1.m1.1.1.cmml">Îº</mi><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.1.m1.1b"><ci id="A1.T5.3.3.3.1.m1.1.1.cmml" xref="A1.T5.3.3.3.1.m1.1.1">ğœ…</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.1.m1.1c">\kappa</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.1.m1.1d">italic_Îº</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.3.5.2.1">Overall</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.2.2">0.4179</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.2.3">0.3437</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.5.2.4">0.7100</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.3.6.3.1">Usefulness<sub class="ltx_sub" id="A1.T5.3.6.3.1.1">GPT-3.5</sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.3.2">0.4150</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.3.3">0.3468</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.6.3.4">0.6879</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.3.7.4.1">Usefulness<sub class="ltx_sub" id="A1.T5.3.7.4.1.1">GPT-4</sub>
</th>
<td class="ltx_td ltx_align_center" id="A1.T5.3.7.4.2">0.5424</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.7.4.3">0.4993</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.7.4.4">0.7900</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.3.8.5.1">Usefulness<sub class="ltx_sub" id="A1.T5.3.8.5.1.1">all</sub>
</th>
<td class="ltx_td ltx_align_center" id="A1.T5.3.8.5.2">0.4758</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.8.5.3">0.4164</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.8.5.4">0.7383</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.3.9.6.1">Readability<sub class="ltx_sub" id="A1.T5.3.9.6.1.1">GPT-3.5</sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.9.6.2">0.3418</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.9.6.3">0.2147</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.9.6.4">0.6852</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.3.10.7.1">Readability<sub class="ltx_sub" id="A1.T5.3.10.7.1.1">GPT-4</sub>
</th>
<td class="ltx_td ltx_align_center" id="A1.T5.3.10.7.2">0.3187</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.10.7.3">0.2502</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.10.7.4">0.7048</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.3.11.8.1">Readability<sub class="ltx_sub" id="A1.T5.3.11.8.1.1">all</sub>
</th>
<td class="ltx_td ltx_align_center" id="A1.T5.3.11.8.2">0.3424</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.11.8.3">0.2374</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.11.8.4">0.6968</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.12.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.3.12.9.1">Relevance<sub class="ltx_sub" id="A1.T5.3.12.9.1.1">GPT-3.5</sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.12.9.2">0.3369</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.12.9.3">0.2958</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.3.12.9.4">0.6459</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.13.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.3.13.10.1">Relevance<sub class="ltx_sub" id="A1.T5.3.13.10.1.1">GPT-4</sub>
</th>
<td class="ltx_td ltx_align_center" id="A1.T5.3.13.10.2">0.4465</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.13.10.3">0.3664</td>
<td class="ltx_td ltx_align_center" id="A1.T5.3.13.10.4">0.7291</td>
</tr>
<tr class="ltx_tr" id="A1.T5.3.14.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A1.T5.3.14.11.1">Relevance<sub class="ltx_sub" id="A1.T5.3.14.11.1.1">all</sub>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.3.14.11.2">0.3925</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.3.14.11.3">0.3323</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.3.14.11.4">0.6872</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Inter-annotator agreement.</figcaption>
</figure>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Multimodal Scraper Design</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">Our multimodal scraper design collects various fields and metadata from Adobe Experience League for images, videos, and tables. For images, we gather the link to the image and the surrounding context, specifically the text between the previous and current image and between the current and next image. The metadata collected includes the title of the document, the header of each section containing the image, and the URL of the document. For videos, the fields collected include the URL of the video, the context text before the video, and the video transcript. The metadata gathered is similar, including the document title, section headers, and document URL. For tables, we collect the table content in the form of a JSON string, the context text before the table, and the document URL. Additional metadata includes the document title and the header of each section containing the table.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Human Evaluation Metrics</h3>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Usefulness</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">Usefulness measures how much the multi-modal elements contribute to the userâ€™s comprehension of the text content.</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">1 - Not at all useful: Multi-modal elements provide no additional understanding or actively confuse the user.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">2 - Slightly useful: Multi-modal elements offer minimal enhancement to understanding.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">3 - Moderately useful: Multi-modal elements provide some additional clarity or information.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">4 - Very useful: Multi-modal elements significantly enhance understanding of the text.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">5 - Extremely useful: Multi-modal elements are crucial for full comprehension of the text.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Readability</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">Readability assesses how well the multi-modal elements are integrated with the text.</p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1">1 - Severely impairs readability: Multi-modal elements are poorly placed, causing significant disruption to reading flow.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">2 - Somewhat impairs readability: Multi-modal elements are not well-integrated, causing minor disruptions.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">3 - Neutral impact on readability: Multi-modal elements neither enhance nor impair the reading experience.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I2.i4.p1">
<p class="ltx_p" id="A1.I2.i4.p1.1">4 - Enhances readability: Multi-modal elements are well-placed, supporting smooth reading flow.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I2.i5.p1">
<p class="ltx_p" id="A1.I2.i5.p1.1">5 - Significantly enhances readability: Multi-modal elements are perfectly integrated, greatly improving the reading experience.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Relevance</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.1">Relevance measures how closely the multi-modal elements relate to the text content.</p>
<ul class="ltx_itemize" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1">1 - Completely irrelevant: Multi-modal elements have no apparent connection to the text.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1">2 - Mostly irrelevant: Multi-modal elements have only a tenuous connection to the text.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I3.i3.p1">
<p class="ltx_p" id="A1.I3.i3.p1.1">3 - Somewhat relevant: Multi-modal elements relate to the text but not be entirely on-point.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I3.i4.p1">
<p class="ltx_p" id="A1.I3.i4.p1.1">4 - Highly relevant: Multi-modal elements clearly support and illustrate the text content.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I3.i5.p1">
<p class="ltx_p" id="A1.I3.i5.p1.1">5 - Perfectly relevant: Multi-modal elements are essential to the text, providing crucial illustrations or data.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Preference Grading</h4>
<div class="ltx_para" id="A1.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px4.p1.1">Annotators also indicate their overall preference between the text answer and the multimodal answer:</p>
<ul class="ltx_itemize" id="A1.I4">
<li class="ltx_item" id="A1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I4.i1.p1">
<p class="ltx_p" id="A1.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I4.i1.p1.1.1">Text Only</span>: Choose this if you believe the text alone would be more effective without the multi-modal elements.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I4.i2.p1">
<p class="ltx_p" id="A1.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I4.i2.p1.1.1">Multi-Modal</span>: Select this if you think the combination of text and multi-modal elements provides the best experience.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I4.i3.p1">
<p class="ltx_p" id="A1.I4.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I4.i3.p1.1.1">Same</span>: Choose this if you feel text-only and multi-modal versions are equally effective.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Additional Human Evaluation Results</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">Due to space constraints, we include additional human evaluation results in the Appendix. The average score and preference per annotator are presented in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.T3" title="Table 3 â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>. The standard deviation of evaluation metrics for multimodal answers generated by GPT-3.5 and GPT-4 is detailed in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.T4" title="Table 4 â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>. Overall inter-annotator agreement, along with specific agreements for different models and metrics, can be found in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.T5" title="Table 5 â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Prompts</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">Please note that the actual prompts used in the system development differ from the prompts shown below. These are simplified versions that capture the essence of the prompt design.</p>
</div>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Prompt for Text Answer Generation</h4>
<div class="ltx_para" id="A1.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS4.SSS0.Px1.p1.1">The prompt for text answer generation can be found in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.F4" title="Figure 4 â€£ Prompt for Text Answer Generation â€£ A.4 Prompts â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="A1.F4.g1" src="extracted/5794920/Figures/text_answer_prompt.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Prompt for text answer generation.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="A1.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Prompt for Multimodal Answer Refinement</h4>
<div class="ltx_para" id="A1.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS4.SSS0.Px2.p1.1">The prompt for multimodal answer refinement can be found in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2408.08521v1#A1.F5" title="Figure 5 â€£ Prompt for Multimodal Answer Refinement â€£ A.4 Prompts â€£ Appendix A Appendix â€£ MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="208" id="A1.F5.g1" src="extracted/5794920/Figures/multimodal_prompt.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Prompt for multimodal answer refinement.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 16 04:23:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
