<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2202.04309] Vertical Federated Learning: Challenges, Methodologies and Experiments</title><meta property="og:description" content="Recently, federated learning (FL) has emerged as a promising distributed machine learning (ML) technology, owing to the advancing computational and sensing capacities of end-user devices, however with the increasing co…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Vertical Federated Learning: Challenges, Methodologies and Experiments">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Vertical Federated Learning: Challenges, Methodologies and Experiments">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2202.04309">

<!--Generated on Thu Mar  7 18:02:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Vertical FL,  privacy preserving,  communication efficiency,  splitting design.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Vertical Federated Learning: Challenges, Methodologies and Experiments</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kang Wei, 
Jun Li, 
Chuan Ma, 
Ming Ding, 
Sha Wei,
Fan Wu, 
Guihai Chen, 
and
Thilina Ranbaduge
</span><span class="ltx_author_notes">Kang Wei, Jun Li and Chuan Ma are with School of Electrical and Optical Engineering, Nanjing University of Science and Technology, Nanjing 210094, China. Chuan Ma is also with Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education (e-mail: {kang.wei; jun.li; chuan.ma}@njust.edu.cn).Ming Ding and Thilina Ranbaduge are with Data61, CSIRO, NSW 2015, Australia (e-mail: {Ming.Ding; Thilina.Ranbaduge}@data61.csiro.au).Sha Wei is with China Academy of Information and Communication Technology (CAICT), Beijing 100191, China (e-mail: weisha@caict.ac.cn).Fan Wu is with the Shanghai Key Laboratory of Scalable Computing
and Systems, Department of Computer Science and Engineering, Shanghai
Jiao Tong University, Shanghai 200240, China (e-mail: fwu@cs.sjtu.edu.cn).Guihai Chen is with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: gchen@nju.edu.cn).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recently, federated learning (FL) has emerged as a promising distributed machine learning (ML) technology, owing to the advancing computational and sensing capacities of end-user devices, however with the increasing concerns on users’ privacy.
As a special architecture in FL, vertical FL (VFL) is capable of constructing a hyper ML model by embracing sub-models from different clients.
These sub-models are trained locally by vertically partitioned data with distinct attributes.
Therefore, the design of VFL is fundamentally different from that of conventional FL, raising new and unique research issues.
In this paper, we aim to discuss key challenges in VFL with effective solutions, and conduct experiments on real-life datasets to shed light on these issues.
Specifically, we first propose a general framework on VFL, and highlight the key differences between VFL and conventional FL.
Then, we discuss research challenges rooted in VFL systems under four aspects, i.e., security and privacy risks, expensive computation and communication costs, possible structural damage caused by model splitting, and system heterogeneity.
Afterwards, we develop solutions to addressing the aforementioned challenges, and conduct extensive experiments to showcase the effectiveness of our proposed solutions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Vertical FL, privacy preserving, communication efficiency, splitting design.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the emergence of end-user devices, equipped with various sensors and increasingly powerful hardwares, unprecedented amounts of data are generated through day-to-day usage in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
In a concurrent development, machine learning (ML) has revolutionized the ways that information is extracted with groundbreaking successes in areas such as computer vision, natural language processing, voice recognition, etc.
Therefore, there is a high demand for harnessing the data provided by distributed devices or data owners to enrich ML models.
To address this key issue as well as ensuring data privacy and security, many distributed learning systems have been widely exploited to train data models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As a new distributed learning paradigm, federated learning (FL) has recently drawn a lot of attentions by involving training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data locally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
In terms of data partitions, FL can be categorized into three scenarios, i.e., horizontal, vertical and transfer types.
Most of existing works focus on horizontal FL (HFL) that requires all participants possess the same attribute space but different sample spaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
It is suitable for mobile devices and usually involves billions of devices with heterogeneous resources under a complex distributed network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
However, application scenarios of HFL are limited due to practical reasons, such as the confidentiality among companies with a same competing interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Vertical FL (VFL), on the other hand, can avoid this issue by promoting collaborations among non-competing organizations/entities with vertically partitioned data, e.g., a collaboration among a bank, an insurance company, and an e-commerce platform to learn users’ living/shopping behaviors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The success of VFL is primarily owing to its application scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, such as Fedlearner<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/bytedance/fedlearner</span></span></span> in bytedance and Angel PowerFL<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://data.qq.com/powerfl</span></span></span> in Tencent.
Indeed, compared with independent training or HFL, VFL can exploit more/deeper attribute dimensions, and obtain a better learning model.
One of its typical usage scenarios, which is the focus of this paper, is that a few participants (organizations) collaboratively train a model with vertically partitioned data but labels of these attributes are owned by only one participant.
For example, a car insurance company with limited user attributes might want to improve the risk evaluation model by incorporating more attributes from other businesses, e.g., a bank, a taxation office, etc.
The role of the other participants is simply providing additional feature information without directly disclosing their data to other participants, and in return, obtain financial and/or reputational rewards.
In addition, VFL also requires consistently fewer resources from a participating client compared with data sharing directly, enabling lightweight and scalable distributed training solutions.
However, up to now, few efforts have been spent on investigating the core challenges and methodologies of this VFL framework.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2202.04309/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="500" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overall workflow for vertical FL. The classic workflow includes following seven steps: 1) private set intersection; 2) bottom model forward propagation (BM-FP); 3) forward transmission; 4) top model forward propagation (TM-FP); 5) top model backward propagation (TM-BP); 6) backward transmission; 7) bottom model backward propagation (BM-BP). The host is the label owner and the guest is the attribute owner.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Inspired by this research gap, in this article, we investigate the potential challenges as well as methodologies for VFL systems.
Specifically, we first propose a general VFL framework.
Then, we clarify the key differences between VFL and HFL, including data characteristics of participants, exchanged messages and model structures.
Furthermore, we discuss potential unique challenges and possible solutions.
Finally, we conduct extensive experiments using several real-world data sets to evaluate the effectiveness of the proposed solutions.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The remainder of this article is organized as follows.
The next section introduces the basic model of VFL and key differences between VFL and HFL.
Then we illustrate challenges in VFL in Section <a href="#S3" title="III Core Challenges ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, and provide possible solutions to address these challenges in Section <a href="#S4" title="IV Methodologies and Possible Solutions ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.
In Section <a href="#S5" title="V Experiment Results ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, three typical experiments have been conducted to evaluate the proposed solutions.
Finally, conclusions are drawn in Section <a href="#S6" title="VI Conclusion ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Frameworks for VFL</span>
</h3>

<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Key differences between VFL and HFL</figcaption>
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Framework</th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Data characteristics of participants</th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Exchanged messages</th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Model structures</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">VFL</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Same sample space, but different attribute space</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Intermediate outputs and its gradients</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">Flexible and local secret</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">HFL</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Different sample space, but same attribute space</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Global and local model parameters</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Fixed and consistent</td>
</tr>
</tbody>
</table>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The key idea of VFL is to enhance a learning model by utilizing the distributed data with various attributes.
Hence, VFL accepts the vertically partitioned data where participants’ data share the same sample space with different attribute spaces.
As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a general VFL process for each learning epoch includes the following seven key steps:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix1.1.1.m1.1b"><mo id="S2.I1.ix1.1.1.m1.1.1" xref="S2.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix1.1.1.m1.1c"><ci id="S2.I1.ix1.1.1.m1.1.1.cmml" xref="S2.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix1.p1" class="ltx_para">
<p id="S2.I1.ix1.p1.1" class="ltx_p"><span id="S2.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Step 1: Private set intersection.</span>
Before model training, the framework needs to find the common identifiers (IDs) served by all participants (i.e., guest organization and host organization) to align the training data samples, which is called private set intersection (PSI) or secure entity alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
PSI is a secure multiparty protocol which allows multiple participants to find out the common IDs available across their data and nothing else.
Widely adopted PSI techniques include naive hashing, oblivious polynomial evaluation, and oblivious transfer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
</li>
<li id="S2.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix2.1.1.m1.1b"><mo id="S2.I1.ix2.1.1.m1.1.1" xref="S2.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix2.1.1.m1.1c"><ci id="S2.I1.ix2.1.1.m1.1.1.cmml" xref="S2.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix2.p1" class="ltx_para">
<p id="S2.I1.ix2.p1.1" class="ltx_p"><span id="S2.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Step 2: Bottom model forward propagation.</span>
After determining the aligned data samples among all participants, each participant will complete a forward propagation process using local data based on its bottom (local) model.
This forward propagation process is similar to the conventional training except calculating the loss value.</p>
</div>
</li>
<li id="S2.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix3.1.1.m1.1b"><mo id="S2.I1.ix3.1.1.m1.1.1" xref="S2.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix3.1.1.m1.1c"><ci id="S2.I1.ix3.1.1.m1.1.1.cmml" xref="S2.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix3.p1" class="ltx_para">
<p id="S2.I1.ix3.p1.1" class="ltx_p"><span id="S2.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Step 3: Forward output transmission.</span>
Each participant needs to transmit its forward output to the label owner.
Intuitively speaking, the forward output contains intermediate results of local neural networks, which transforms the original attributes into features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Such a transmission process may divulge participants’ privacy information.
Hence, advanced privacy preserving methods, such as differential privacy (DP), should be exploited to address potential privacy risks but will may incur additional communication costs and computation complexities.</p>
</div>
</li>
<li id="S2.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix4.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix4.1.1.m1.1b"><mo id="S2.I1.ix4.1.1.m1.1.1" xref="S2.I1.ix4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix4.1.1.m1.1c"><ci id="S2.I1.ix4.1.1.m1.1.1.cmml" xref="S2.I1.ix4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix4.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix4.p1" class="ltx_para">
<p id="S2.I1.ix4.p1.1" class="ltx_p"><span id="S2.I1.ix4.p1.1.1" class="ltx_text ltx_font_bold">Step 4: Top model forward propagation.</span>
The label owner uses the collected outputs from all participants to calculate the loss function value based on the top model and labels.</p>
</div>
</li>
<li id="S2.I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix5.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix5.1.1.m1.1b"><mo id="S2.I1.ix5.1.1.m1.1.1" xref="S2.I1.ix5.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix5.1.1.m1.1c"><ci id="S2.I1.ix5.1.1.m1.1.1.cmml" xref="S2.I1.ix5.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix5.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix5.p1" class="ltx_para">
<p id="S2.I1.ix5.p1.1" class="ltx_p"><span id="S2.I1.ix5.p1.1.1" class="ltx_text ltx_font_bold">Step 5: Top model backward propagation.</span>
The label owner performs backward propagation and computes two gradients for: 1) model parameters of the top model; and 2) forward outputs from each participant.
Using the gradients of the top model, the label owner can calculate the average gradients for each batch of samples and update its model.</p>
</div>
</li>
<li id="S2.I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix6.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix6.1.1.m1.1b"><mo id="S2.I1.ix6.1.1.m1.1.1" xref="S2.I1.ix6.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix6.1.1.m1.1c"><ci id="S2.I1.ix6.1.1.m1.1.1.cmml" xref="S2.I1.ix6.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix6.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix6.p1" class="ltx_para">
<p id="S2.I1.ix6.p1.1" class="ltx_p"><span id="S2.I1.ix6.p1.1.1" class="ltx_text ltx_font_bold">Step 6: Backward output transmission.</span>
The gradients of forward outputs are sent back to every guest participant.
It can be noticed the required communication cost (transmission bits) is usually much smaller than the ones in <span id="S2.I1.ix6.p1.1.2" class="ltx_text ltx_font_bold">Step 2</span>, because they are gradients instead of intermediate outputs.</p>
</div>
</li>
<li id="S2.I1.ix7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I1.ix7.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I1.ix7.1.1.m1.1b"><mo id="S2.I1.ix7.1.1.m1.1.1" xref="S2.I1.ix7.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.ix7.1.1.m1.1c"><ci id="S2.I1.ix7.1.1.m1.1.1.cmml" xref="S2.I1.ix7.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.ix7.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I1.ix7.p1" class="ltx_para">
<p id="S2.I1.ix7.p1.1" class="ltx_p"><span id="S2.I1.ix7.p1.1.1" class="ltx_text ltx_font_bold">Step 7: Bottom model backward propagation.</span>
Each participant calculates the gradients of its bottom model parameters based on the local data and gradients of the forward outputs from the label owner, and then updates its bottom model.</p>
</div>
</li>
</ul>
<p id="S2.SS1.p1.2" class="ltx_p">We next discuss the differences between VFL and HFL in the following subsection.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Key Differences between VFL and HFL</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this subsection, the differences between VFL and HFL can be summarized under three aspects, i.e., data characteristics, exchanged messages, and model structures, which is shown as follows:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I2.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I2.ix1.1.1.m1.1b"><mo id="S2.I2.ix1.1.1.m1.1.1" xref="S2.I2.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I2.ix1.1.1.m1.1c"><ci id="S2.I2.ix1.1.1.m1.1.1.cmml" xref="S2.I2.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I2.ix1.p1" class="ltx_para">
<p id="S2.I2.ix1.p1.1" class="ltx_p">VFL systems require that all participants possess the same sample space and different attribute spaces, e.g., the collaboration between a bank and an e-commerce platform, but HFL can only be conducted among participants with the same attribute space and different sample spaces.
This inherent difference between VFL and HFL leads to distinctively different neural network structures.
For a concrete example, there could be hundreds or thousands of participants in HFL, while the number of participants in VFL is usually less than five <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
As a result, hot topics of HFL, such as participant selection, do not apply to VFL and unique problems such as PSI protocols can only be studied in the framework of VFL.</p>
</div>
</li>
<li id="S2.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I2.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I2.ix2.1.1.m1.1b"><mo id="S2.I2.ix2.1.1.m1.1.1" xref="S2.I2.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I2.ix2.1.1.m1.1c"><ci id="S2.I2.ix2.1.1.m1.1.1.cmml" xref="S2.I2.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I2.ix2.p1" class="ltx_para">
<p id="S2.I2.ix2.p1.1" class="ltx_p">One of the main characteristics of HFL is that each participant maintains a local model and receives the parameters of the global model periodically.
Thus, the local model of each machine training is identical and complete, and can be independently predicted when making predictions.
However, each participant in VFL possesses a part of a full model, and all participants should finish a training process part by part.
Therefore, exchanged messages among participants in VFL are the intermediate outputs (learning representations) of the local data based on bottom models and their gradients, instead of local model parameters or updates in HFL.</p>
</div>
</li>
<li id="S2.I2.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S2.I2.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S2.I2.ix3.1.1.m1.1b"><mo id="S2.I2.ix3.1.1.m1.1.1" xref="S2.I2.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S2.I2.ix3.1.1.m1.1c"><ci id="S2.I2.ix3.1.1.m1.1.1.cmml" xref="S2.I2.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S2.I2.ix3.p1" class="ltx_para">
<p id="S2.I2.ix3.p1.1" class="ltx_p">The communication cost, and security and privacy risks are much different from the ones in the HFL because of their different structures.
For example, the bottom model structure in VFL for each participant is flexible and a local secret (unknown to others), which is determined by the splitting methods.
In this case, the splitting methods after careful design, need to be suitable for specific models, heterogeneous attribute space, and communication and computing resources.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">To demonstrate key differences between VFL and HFL, we summarize main ideas in Tab. <a href="#S2.T1" title="TABLE I ‣ II-A Frameworks for VFL ‣ II Background ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
In the following section, we introduce the key/core challenges of VFL in detail.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Core Challenges</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we will discuss four of the core challenges associated with solving the distributed learning problem for VFL.
These challenges make the VFL design distinct from other classical problems, such as the centralized learning or HFL.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Security and Privacy Risks</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Research on the privacy and security risks for existing VFL models are insufficient.
In VFL, participants need to obtain the coincident sample space, thus the membership inference attack from other participants may be redundant.
In addition, in VFL, an adversarial participant only controls a part of the federated model, which cannot run independently, and only has access to the gradients of its own bottom model.
However, via analysing exchanged messages, i.e., intermediate outputs and the gradients, participants can infer the clients’ attributes from other participants, such as the label inference attack <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and private data leakage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Protecting the privacy of the labels owned by each participant should be a fundamental requirement in VFL, as the labels might be highly sensitive, e.g., whether a person has a certain type of disease or not.
In some special cases, the gradients from the host organization can also directly leak the label information.
Furthermore, it is also been proven that recovering batch data from the shared gradients is available.
We can also note the attacking methods varies in different data types.
For example, tabular data usually needs embedding before learning, and it is difficult obtain the private information from embedded attributes.
However, it is still a research question whether adversaries can take advantage of the exchanged messages in VFL to recover raw attribute values.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Existing privacy preserving methods including DP, secure multiparty computation (SMC), homomorphic encryption (HE), and their hybrid methods, have been widely adopted in HFL, but few ones have been explored in VFL.
In addition, these methods further need a well designed trade-off in terms of the model performance, the privacy and security level, and the system efficiency.
An unique challenge for VFL is that splitting methods will determine the preprocess of secret attributes, i.e., forward outputs of the bottom model, and then affect the privacy and security level.
For the simple ML models, such as logistic regression (LR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and kernel models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the splitting design is straightforward, but privacy preserving methods are considerable challenging due to the insecure linear process for the input.
If complex neural networks are considered, e.g., the convolutional network, as the bottom model, the intermediate outputs will expose less private information.
This is due to a fact that a complex nonlinear function can naturally enhance the privacy of distributed data.
Understanding and balancing the trade-off of privacy protection, operating efficiency and model performance, both theoretically and empirically, is a considerable challenge in privacy-preserving VFL systems.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">High-cost of Computation and Communication</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Although the raw data is not explicitly shared in the FL setting, resource-limited communication networks are still critical bottlenecks in both HFL and VFL systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Specifically, in VFL, the total computation and communication cost is proportional to the training dataset size.
In other words, the widely adopted batch computation method in HFL cannot be applied to VFL.
When facing a huge amount of data, e.g., billions of advertising data, communication and local computation may be many orders of magnitude due to limited resources, such as hardware capacity, bandwidth, and power.
To fit a limited resources condition in VFL network, it is therefore important to develop computation and communication-efficient methods that reduce the computation complexity and iteratively send the messages or model updates as part of the training process, respectively.
To further reduce computation and communication cost in such a setting, three key aspects can considered 1) pruning the neural network, 2) reducing the size of the transmitted messages by smart compression methods at each training round and 3) selecting the proper attribute splitting design.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Structural Damage of Model Splitting</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">With various datasets, the best corresponding training model is usually carefully designed, but the model splitting process may destroy its specific structure in VFL.
For example, some classic recommendation models for advertising data, e.g., Wide<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mo id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><and id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\&amp;</annotation></semantics></math>Deep and deepFM, need to calculate cosine similarities between user and item attributes to explore their relevances.
However, disparate attribute values and privacy concerns make it difficult to meet the requirement of calculating cosine similarities.
Furthermore, for the much complex but efficient neural network models that contain recurrent and attention layers, e.g., transformer, the splitting design becomes challenging in realizing privacy preserving and communication efficiency.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">System Heterogeneity</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The variability of each participant in storage, hardware (CPU/GPU and memory), network connectivity (5G and wifi), and power (battery level) will incur the system heterogeneity.
Additionally, the network size and systems-related constraints on each participant typically result in asynchronous updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, i.e., a part of participants being active at once.
For example, if one participant possess amounts of attributes but limited transmission capacity, computation frequencies or memory size, it will be difficult to complete the bottom model forward propagation.
These system-level characteristics dramatically pose challenges, such as straggler mitigation and fault tolerance.
Therefore, developed VFL methods must satisfy: 1) anticipating a low amount of participation 2) tolerating heterogeneous hardware, and 3) being robust enough to dropped exchanged messages in one iteration.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Methodologies and Possible Solutions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we analyse the available techniques to address the aforementioned challenges, and propose some possible solutions.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Privacy Preserving Frameworks</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In a VFL system, security and privacy issues usually focus on the exchanged messages, whatever training or serving.
Therefore, it is significant to protect these private information as well as maximizing the training utility.
Some popular techniques, such as DP, SMC and HE, can be adopted for this issue but usually induce other damages to the learning system.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix1.1.1.m1.1b"><mo id="S4.I1.ix1.1.1.m1.1.1" xref="S4.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix1.1.1.m1.1c"><ci id="S4.I1.ix1.1.1.m1.1.1.cmml" xref="S4.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I1.ix1.p1" class="ltx_para">
<p id="S4.I1.ix1.p1.1" class="ltx_p"><span id="S4.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Differential privacy.</span> DP is a popular research direction to enhance the privacy level based on theoretical guarantees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Along with other privacy enhancing techniques, such as data shuffle, driving a tight bound for the privacy loss is beneficial to obtaining a better tarde-off between utility and privacy.
Specifically, for the high efficiency requirement, against the inference attack, DP will be a good selection via perturbing the backward output or local bottom networks as we show in our experiments.</p>
</div>
</li>
<li id="S4.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix2.1.1.m1.1b"><mo id="S4.I1.ix2.1.1.m1.1.1" xref="S4.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix2.1.1.m1.1c"><ci id="S4.I1.ix2.1.1.m1.1.1.cmml" xref="S4.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I1.ix2.p1" class="ltx_para">
<p id="S4.I1.ix2.p1.1" class="ltx_p"><span id="S4.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Secure multi-party computing.</span> SMC is used to design various privacy-preserving protocols by involving encryption and protocol design, which is also worth investigating for VFL systems, especially for the limited communication and computation resources.
Compared with DP, SMC techniques can maintain the original utility, but incurs a higher computational and communicational complexities due to cipher inflation and more information interaction.</p>
</div>
</li>
<li id="S4.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I1.ix3.1.1.m1.1b"><mo id="S4.I1.ix3.1.1.m1.1.1" xref="S4.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I1.ix3.1.1.m1.1c"><ci id="S4.I1.ix3.1.1.m1.1.1.cmml" xref="S4.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I1.ix3.p1" class="ltx_para">
<p id="S4.I1.ix3.p1.1" class="ltx_p"><span id="S4.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Homomorphic encryption.</span> The high complexity is a key bottleneck to consider HE when developing security and privacy methods for federated networks.
Due to the limited number of operations allowed by HE, such as addition and multiplication, thus the nonlinear operation requires some well-designed approximate functions and performance losses are inevitable.
Fortunately, it can be simplified or used to protect the key messages as a part of protocol.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Overall, there always exists a trade-off between privacy, efficiency and utility, in which it is more practical to select appropriate privacy and security settings for specific customer requirements.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Enhanced Communication Schemes</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In this subsection, we introduce three classic schemes to reduce the communication cost in VFL, i.e., transmission compression, model pruning, and data sampling.</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I2.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I2.ix1.1.1.m1.1b"><mo id="S4.I2.ix1.1.1.m1.1.1" xref="S4.I2.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I2.ix1.1.1.m1.1c"><ci id="S4.I2.ix1.1.1.m1.1.1.cmml" xref="S4.I2.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I2.ix1.p1" class="ltx_para">
<p id="S4.I2.ix1.p1.1" class="ltx_p"><span id="S4.I2.ix1.p1.1.1" class="ltx_text ltx_font_bold">Transmission compression.</span> To reduce the communication cost, compressing the transmission messages can be an alternative scheme.
Specifically, for one epoch training, VFL requires two transmission processes, i.e., forward and backward transmissions.
For the forward and backward transmissions, we can observe that a large compression ratio will reduce the communication cost much, but involve a large training error, and then affect the updates of bottom and top models.
Unlike HFL, most of classic compression schemes, e.g., uniform quantization,
are not workable since the gradients of forward outputs are unable to derive during back propagation.
Therefore, it will be a promising direction to design an approximate function to assist the backward propagation in VFL.
It is also significant to develop the relationship between the quantization level and training performance, which can provide a strong guidance to find a satisfied trade-off between transmission and model performance.</p>
</div>
</li>
<li id="S4.I2.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I2.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I2.ix2.1.1.m1.1b"><mo id="S4.I2.ix2.1.1.m1.1.1" xref="S4.I2.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I2.ix2.1.1.m1.1c"><ci id="S4.I2.ix2.1.1.m1.1.1.cmml" xref="S4.I2.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I2.ix2.p1" class="ltx_para">
<p id="S4.I2.ix2.p1.1" class="ltx_p"><span id="S4.I2.ix2.p1.1.1" class="ltx_text ltx_font_bold">Model pruning.</span> Model pruning is one of most popular techniques to reduce the computation and transmission resources.
Specifically, we can drop out some unimportant neurons in the full-connection layer or channels in convolutional layers.
In this way, the computation and transmission cost will be reduced obviously.
Therefore, it is significant to further study on evaluating the importance of different neurons or channels, and then achieve the model pruning design with a performance guarantee.</p>
</div>
</li>
<li id="S4.I2.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I2.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I2.ix3.1.1.m1.1b"><mo id="S4.I2.ix3.1.1.m1.1.1" xref="S4.I2.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I2.ix3.1.1.m1.1c"><ci id="S4.I2.ix3.1.1.m1.1.1.cmml" xref="S4.I2.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I2.ix3.p1" class="ltx_para">
<p id="S4.I2.ix3.p1.1" class="ltx_p"><span id="S4.I2.ix3.p1.1.1" class="ltx_text ltx_font_bold">Data sampling.</span> Data sampling can also be adopted to reduce the communication cost.
In the conventional mini-batch mechanism, all training data will be split into several batches and shuffled, and then VFL would train these batches one by one.
To reduce the communication cost, we can select a part of batches that are important on the model update, via a well designed filtering mechanisms.
Hence, it is a promising direction to design the data selection or combination algorithms to improve the communication efficiency in VFL.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Novel Mechanisms for Asynchronous VFL</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Participants with heterogenous resources may bring out the asynchrony that requires an intelligent allocator or compensation algorithm to improve the training performance.
Asynchronous communication has already been studied at many aspects in HFL for stragglers and heterogeneous latency, such as weight design and update compensation for average methods.
In VFL, each participant possesses a unique attribute set, and its forward outputs are much distinct from others.
If certain participants fall behind at one epoch, using the history information (previous outputs) to train this epoch can be used as an alternative.
In addition, in practical scenarios, participants usually have some overlaps in the attribute space, and these overlaps can be explored to address this challenge.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Splitting Design</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The splitting methods for VFL models are key techniques that affect the training performance, communication and computation allocations as well as privacy risks.</p>
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I3.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I3.ix1.1.1.m1.1b"><mo id="S4.I3.ix1.1.1.m1.1.1" xref="S4.I3.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I3.ix1.1.1.m1.1c"><ci id="S4.I3.ix1.1.1.m1.1.1.cmml" xref="S4.I3.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I3.ix1.p1" class="ltx_para">
<p id="S4.I3.ix1.p1.1" class="ltx_p"><span id="S4.I3.ix1.p1.1.1" class="ltx_text ltx_font_bold">Effects on communication and computation.</span>
For one classic learning model, splitting design will directly influence the allocations of training tasks and transmission cost for participants.
In addition, a simple structure of the bottom model, e.g., the linear computation layer, will incur high privacy risks and call for further privacy protections with additional communication and computation costs.
Overall, a personalized design on the cut/splitting layer for different participants and models is required to ensure computation and communication efficiencies.</p>
</div>
</li>
<li id="S4.I3.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I3.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I3.ix2.1.1.m1.1b"><mo id="S4.I3.ix2.1.1.m1.1.1" xref="S4.I3.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I3.ix2.1.1.m1.1c"><ci id="S4.I3.ix2.1.1.m1.1.1.cmml" xref="S4.I3.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I3.ix2.p1" class="ltx_para">
<p id="S4.I3.ix2.p1.1" class="ltx_p"><span id="S4.I3.ix2.p1.1.1" class="ltx_text ltx_font_bold">Effects on security and privacy.</span>
The splitting rule determines the type of the transmitting messages, which are required to be protected in the learning process.
Involving a privacy-preserving and complexity-acceptability neural network structure for the bottom model may be an alternative.</p>
</div>
</li>
<li id="S4.I3.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S4.I3.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.I3.ix3.1.1.m1.1b"><mo id="S4.I3.ix3.1.1.m1.1.1" xref="S4.I3.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.I3.ix3.1.1.m1.1c"><ci id="S4.I3.ix3.1.1.m1.1.1.cmml" xref="S4.I3.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S4.I3.ix3.p1" class="ltx_para">
<p id="S4.I3.ix3.p1.1" class="ltx_p"><span id="S4.I3.ix3.p1.1.1" class="ltx_text ltx_font_bold">Effects on model performance.</span>
Some well-designed learning structures need to extract the relations among original attributes, such as Wide<math id="S4.I3.ix3.p1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="S4.I3.ix3.p1.1.m1.1a"><mo id="S4.I3.ix3.p1.1.m1.1.1" xref="S4.I3.ix3.p1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S4.I3.ix3.p1.1.m1.1b"><and id="S4.I3.ix3.p1.1.m1.1.1.cmml" xref="S4.I3.ix3.p1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S4.I3.ix3.p1.1.m1.1c">\&amp;</annotation></semantics></math>Deep and DeepFM.
However, VFL requires each participant to keep their raw data locally.
Therefore, we should design the splitting methods while maintaining these specific structures, as well as designing efficient layers or interactive mechanisms to compensate damages caused by the splitting connections.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Experiment Results</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.13" class="ltx_p">In this section, we explain the experiments we conducted to demonstrate the aforementioned issues and discuss some possible solutions.
For each experiment, we first divide the original training data into three parts (three participants) according to the attribute numbers, and conduct the forward and backward propagations as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We evaluate the prototype on the well-known classification dataset Adult and Avazu.
The adult dataset contains <math id="S5.p1.1.m1.2" class="ltx_Math" alttext="48,842" display="inline"><semantics id="S5.p1.1.m1.2a"><mrow id="S5.p1.1.m1.2.3.2" xref="S5.p1.1.m1.2.3.1.cmml"><mn id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">48</mn><mo id="S5.p1.1.m1.2.3.2.1" xref="S5.p1.1.m1.2.3.1.cmml">,</mo><mn id="S5.p1.1.m1.2.2" xref="S5.p1.1.m1.2.2.cmml">842</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.2b"><list id="S5.p1.1.m1.2.3.1.cmml" xref="S5.p1.1.m1.2.3.2"><cn type="integer" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">48</cn><cn type="integer" id="S5.p1.1.m1.2.2.cmml" xref="S5.p1.1.m1.2.2">842</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.2c">48,842</annotation></semantics></math> records of individuals with <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="11" display="inline"><semantics id="S5.p1.2.m2.1a"><mn id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><cn type="integer" id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">11</annotation></semantics></math> attributes from <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="1994" display="inline"><semantics id="S5.p1.3.m3.1a"><mn id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">1994</mn><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><cn type="integer" id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">1994</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">1994</annotation></semantics></math> US Census, and is used to predict if an individual’s annual income exceeds <math id="S5.p1.4.m4.1" class="ltx_Math" alttext="50K" display="inline"><semantics id="S5.p1.4.m4.1a"><mrow id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml"><mn id="S5.p1.4.m4.1.1.2" xref="S5.p1.4.m4.1.1.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S5.p1.4.m4.1.1.1" xref="S5.p1.4.m4.1.1.1.cmml">​</mo><mi id="S5.p1.4.m4.1.1.3" xref="S5.p1.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><apply id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1"><times id="S5.p1.4.m4.1.1.1.cmml" xref="S5.p1.4.m4.1.1.1"></times><cn type="integer" id="S5.p1.4.m4.1.1.2.cmml" xref="S5.p1.4.m4.1.1.2">50</cn><ci id="S5.p1.4.m4.1.1.3.cmml" xref="S5.p1.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">50K</annotation></semantics></math> which can been viewed as a binary classification problem.
The Avazu dataset consists of <math id="S5.p1.5.m5.1" class="ltx_Math" alttext="21" display="inline"><semantics id="S5.p1.5.m5.1a"><mn id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml">21</mn><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><cn type="integer" id="S5.p1.5.m5.1.1.cmml" xref="S5.p1.5.m5.1.1">21</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">21</annotation></semantics></math> attributes including <math id="S5.p1.6.m6.1" class="ltx_Math" alttext="14" display="inline"><semantics id="S5.p1.6.m6.1a"><mn id="S5.p1.6.m6.1.1" xref="S5.p1.6.m6.1.1.cmml">14</mn><annotation-xml encoding="MathML-Content" id="S5.p1.6.m6.1b"><cn type="integer" id="S5.p1.6.m6.1.1.cmml" xref="S5.p1.6.m6.1.1">14</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m6.1c">14</annotation></semantics></math> continuous attributes and <math id="S5.p1.7.m7.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S5.p1.7.m7.1a"><mn id="S5.p1.7.m7.1.1" xref="S5.p1.7.m7.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.p1.7.m7.1b"><cn type="integer" id="S5.p1.7.m7.1.1.cmml" xref="S5.p1.7.m7.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.7.m7.1c">7</annotation></semantics></math> categorical attributes, and targets at predicting whether a mobile ID will be clicked.
In addition, we adopt a classic multilayer perceptron (MLP) network consisting of three hidden layers with <math id="S5.p1.8.m8.1" class="ltx_Math" alttext="48" display="inline"><semantics id="S5.p1.8.m8.1a"><mn id="S5.p1.8.m8.1.1" xref="S5.p1.8.m8.1.1.cmml">48</mn><annotation-xml encoding="MathML-Content" id="S5.p1.8.m8.1b"><cn type="integer" id="S5.p1.8.m8.1.1.cmml" xref="S5.p1.8.m8.1.1">48</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.8.m8.1c">48</annotation></semantics></math>, <math id="S5.p1.9.m9.1" class="ltx_Math" alttext="96" display="inline"><semantics id="S5.p1.9.m9.1a"><mn id="S5.p1.9.m9.1.1" xref="S5.p1.9.m9.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S5.p1.9.m9.1b"><cn type="integer" id="S5.p1.9.m9.1.1.cmml" xref="S5.p1.9.m9.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.9.m9.1c">96</annotation></semantics></math> and <math id="S5.p1.10.m10.1" class="ltx_Math" alttext="196" display="inline"><semantics id="S5.p1.10.m10.1a"><mn id="S5.p1.10.m10.1.1" xref="S5.p1.10.m10.1.1.cmml">196</mn><annotation-xml encoding="MathML-Content" id="S5.p1.10.m10.1b"><cn type="integer" id="S5.p1.10.m10.1.1.cmml" xref="S5.p1.10.m10.1.1">196</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.10.m10.1c">196</annotation></semantics></math> units, respectively.
The second layer is chosen as the cut/splitting layer, where the size of the forward output of each bottom model will be <math id="S5.p1.11.m11.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S5.p1.11.m11.1a"><mn id="S5.p1.11.m11.1.1" xref="S5.p1.11.m11.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S5.p1.11.m11.1b"><cn type="integer" id="S5.p1.11.m11.1.1.cmml" xref="S5.p1.11.m11.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.11.m11.1c">32</annotation></semantics></math> and the input size of the top model will be <math id="S5.p1.12.m12.1" class="ltx_Math" alttext="96" display="inline"><semantics id="S5.p1.12.m12.1a"><mn id="S5.p1.12.m12.1.1" xref="S5.p1.12.m12.1.1.cmml">96</mn><annotation-xml encoding="MathML-Content" id="S5.p1.12.m12.1b"><cn type="integer" id="S5.p1.12.m12.1.1.cmml" xref="S5.p1.12.m12.1.1">96</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.12.m12.1c">96</annotation></semantics></math>.
We adopt the Adam optimizer and the learning rate is set to <math id="S5.p1.13.m13.1" class="ltx_Math" alttext="0.0002" display="inline"><semantics id="S5.p1.13.m13.1a"><mn id="S5.p1.13.m13.1.1" xref="S5.p1.13.m13.1.1.cmml">0.0002</mn><annotation-xml encoding="MathML-Content" id="S5.p1.13.m13.1b"><cn type="float" id="S5.p1.13.m13.1.1.cmml" xref="S5.p1.13.m13.1.1">0.0002</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.13.m13.1c">0.0002</annotation></semantics></math>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">DP assisted VFL</span>
</h3>

<figure id="S5.F2" class="ltx_figure"><img src="/html/2202.04309/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="261" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>AUC with different privacy levels in the VFL framework with three participants (Avazu dataset).</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.3" class="ltx_p">First, we evaluate the relationship between the DP mechanism (being against the inference attack) and the test area under curve (AUC) using the Avazu dataset as discussed in subsection <a href="#S4.SS1" title="IV-A Privacy Preserving Frameworks ‣ IV Methodologies and Possible Solutions ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>.
In this scenario, Gaussian mechanism has been adopted to preserve the participants’ privacy with <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\varepsilon=1" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mrow id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">ε</mi><mo id="S5.SS1.p1.1.m1.1.1.1" xref="S5.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><eq id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1.1"></eq><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">𝜀</ci><cn type="integer" id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\varepsilon=1</annotation></semantics></math>, <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mn id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><cn type="float" id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">1.5</annotation></semantics></math> and <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mn id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><cn type="integer" id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">2</annotation></semantics></math>, where the noise vector is added on the forward output for each sample.
As shown in Fig. <a href="#S5.F2" title="Figure 2 ‣ V-A DP assisted VFL ‣ V Experiment Results ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we can observe that the AUC performance is largely affected by the added noise.
This is due to the fact that the perturbed forward output will make the loss function value biased.
Therefore, the fundamental relationship between the convergence bound and the privacy level needs to be characterized to achieve configurable trade-off requirements.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">Compression empowered Communication Efficiency</span>
</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2202.04309/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="261" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The test AUC with different compression levels in the VFL framework (Adult dataset).</figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2202.04309/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="261" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The test AUC with different approximation functions under the same compression level in the VFL framework (Adult dataset).</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.4" class="ltx_p">To reduce the communication cost, we can compress forward outputs by the well-designed compression method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> as mentioned in subsection <a href="#S4.SS2" title="IV-B Enhanced Communication Schemes ‣ IV Methodologies and Possible Solutions ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>.
In this experiment, we adopt the uniform quantization and the addition approximation for the backward propagation.
Specifically, values in forward outputs will be uniformly divided into <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">N</annotation></semantics></math> buckets (this number is corresponding to the compression level) using the maximum and minimum values as intervals, and then the mean value of each bucket will be utilized to replace the values located in bucket for forward outputs.
The addition approximation means that when the VFL system derives the gradients of forward outputs, we use <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{o}+\boldsymbol{e}" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">𝒐</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">+</mo><mi id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">𝒆</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><plus id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></plus><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝒐</ci><ci id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">𝒆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\boldsymbol{o}+\boldsymbol{e}</annotation></semantics></math> to represent the practical compression function, where <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="\boldsymbol{o}" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">𝒐</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">𝒐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\boldsymbol{o}</annotation></semantics></math> is the forward output vector and <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="\boldsymbol{e}" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mi id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">𝒆</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><ci id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1">𝒆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\boldsymbol{e}</annotation></semantics></math> is the bias caused by the compression.
The AUC performance of different compression levels, as well as the transmission cost for each level, are shown in Fig. <a href="#S5.F3" title="Figure 3 ‣ V-B Compression empowered Communication Efficiency ‣ V Experiment Results ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
We can observe that the transmission cost can be reduced considerably, with a high compressing rate while the AUC performance is largely affected.
This is due to the fact that a low compression rate will result in a large bias compared with the original loss function value.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.9" class="ltx_p">To improve the model performance, we also evaluate the test AUC performance with different approximation functions under the same compression level in Fig. <a href="#S5.F4" title="Figure 4 ‣ V-B Compression empowered Communication Efficiency ‣ V Experiment Results ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The multiply approximation function can be expressed as <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\boldsymbol{h}_{i}\boldsymbol{o}_{i}" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><msub id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2.2" xref="S5.SS2.p2.1.m1.1.1.2.2.cmml">𝒉</mi><mi id="S5.SS2.p2.1.m1.1.1.2.3" xref="S5.SS2.p2.1.m1.1.1.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">​</mo><msub id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml"><mi id="S5.SS2.p2.1.m1.1.1.3.2" xref="S5.SS2.p2.1.m1.1.1.3.2.cmml">𝒐</mi><mi id="S5.SS2.p2.1.m1.1.1.3.3" xref="S5.SS2.p2.1.m1.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><times id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></times><apply id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.2.1.cmml" xref="S5.SS2.p2.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.2.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2.2">𝒉</ci><ci id="S5.SS2.p2.1.m1.1.1.2.3.cmml" xref="S5.SS2.p2.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p2.1.m1.1.1.3.1.cmml" xref="S5.SS2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S5.SS2.p2.1.m1.1.1.3.2.cmml" xref="S5.SS2.p2.1.m1.1.1.3.2">𝒐</ci><ci id="S5.SS2.p2.1.m1.1.1.3.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\boldsymbol{h}_{i}\boldsymbol{o}_{i}</annotation></semantics></math>, where <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="\boldsymbol{o}_{i}" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><msub id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">𝒐</mi><mi id="S5.SS2.p2.2.m2.1.1.3" xref="S5.SS2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">𝒐</ci><ci id="S5.SS2.p2.2.m2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">\boldsymbol{o}_{i}</annotation></semantics></math> is the <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">i</annotation></semantics></math>-th value in <math id="S5.SS2.p2.4.m4.1" class="ltx_Math" alttext="\boldsymbol{o}" display="inline"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">𝒐</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝒐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">\boldsymbol{o}</annotation></semantics></math> and <math id="S5.SS2.p2.5.m5.1" class="ltx_Math" alttext="\boldsymbol{h}_{i}" display="inline"><semantics id="S5.SS2.p2.5.m5.1a"><msub id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml"><mi id="S5.SS2.p2.5.m5.1.1.2" xref="S5.SS2.p2.5.m5.1.1.2.cmml">𝒉</mi><mi id="S5.SS2.p2.5.m5.1.1.3" xref="S5.SS2.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><apply id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS2.p2.5.m5.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS2.p2.5.m5.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.2">𝒉</ci><ci id="S5.SS2.p2.5.m5.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">\boldsymbol{h}_{i}</annotation></semantics></math> is its corresponding scaling factor, and is comprehensible.
The upper-bound approximation function is designed by the form <math id="S5.SS2.p2.6.m6.2" class="ltx_Math" alttext="\log(a\boldsymbol{o}_{i}+b)+c" display="inline"><semantics id="S5.SS2.p2.6.m6.2a"><mrow id="S5.SS2.p2.6.m6.2.2" xref="S5.SS2.p2.6.m6.2.2.cmml"><mrow id="S5.SS2.p2.6.m6.2.2.1.1" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml"><mi id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml">log</mi><mo id="S5.SS2.p2.6.m6.2.2.1.1a" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml">⁡</mo><mrow id="S5.SS2.p2.6.m6.2.2.1.1.1" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml"><mo stretchy="false" id="S5.SS2.p2.6.m6.2.2.1.1.1.2" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml">(</mo><mrow id="S5.SS2.p2.6.m6.2.2.1.1.1.1" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.cmml"><mrow id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.cmml"><mi id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.2" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.1" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.1.cmml">​</mo><msub id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.cmml"><mi id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.2" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.2.cmml">𝒐</mi><mi id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.3" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S5.SS2.p2.6.m6.2.2.1.1.1.1.1" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.1.cmml">+</mo><mi id="S5.SS2.p2.6.m6.2.2.1.1.1.1.3" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.3.cmml">b</mi></mrow><mo stretchy="false" id="S5.SS2.p2.6.m6.2.2.1.1.1.3" xref="S5.SS2.p2.6.m6.2.2.1.2.cmml">)</mo></mrow></mrow><mo id="S5.SS2.p2.6.m6.2.2.2" xref="S5.SS2.p2.6.m6.2.2.2.cmml">+</mo><mi id="S5.SS2.p2.6.m6.2.2.3" xref="S5.SS2.p2.6.m6.2.2.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.2b"><apply id="S5.SS2.p2.6.m6.2.2.cmml" xref="S5.SS2.p2.6.m6.2.2"><plus id="S5.SS2.p2.6.m6.2.2.2.cmml" xref="S5.SS2.p2.6.m6.2.2.2"></plus><apply id="S5.SS2.p2.6.m6.2.2.1.2.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1"><log id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"></log><apply id="S5.SS2.p2.6.m6.2.2.1.1.1.1.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1"><plus id="S5.SS2.p2.6.m6.2.2.1.1.1.1.1.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.1"></plus><apply id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2"><times id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.1.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.1"></times><ci id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.2.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.2">𝑎</ci><apply id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.1.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3">subscript</csymbol><ci id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.2.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.2">𝒐</ci><ci id="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.3.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S5.SS2.p2.6.m6.2.2.1.1.1.1.3.cmml" xref="S5.SS2.p2.6.m6.2.2.1.1.1.1.3">𝑏</ci></apply></apply><ci id="S5.SS2.p2.6.m6.2.2.3.cmml" xref="S5.SS2.p2.6.m6.2.2.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.2c">\log(a\boldsymbol{o}_{i}+b)+c</annotation></semantics></math>, where <math id="S5.SS2.p2.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S5.SS2.p2.7.m7.1a"><mi id="S5.SS2.p2.7.m7.1.1" xref="S5.SS2.p2.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.7.m7.1b"><ci id="S5.SS2.p2.7.m7.1.1.cmml" xref="S5.SS2.p2.7.m7.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.7.m7.1c">a</annotation></semantics></math>, <math id="S5.SS2.p2.8.m8.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S5.SS2.p2.8.m8.1a"><mi id="S5.SS2.p2.8.m8.1.1" xref="S5.SS2.p2.8.m8.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.8.m8.1b"><ci id="S5.SS2.p2.8.m8.1.1.cmml" xref="S5.SS2.p2.8.m8.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.8.m8.1c">b</annotation></semantics></math> and <math id="S5.SS2.p2.9.m9.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S5.SS2.p2.9.m9.1a"><mi id="S5.SS2.p2.9.m9.1.1" xref="S5.SS2.p2.9.m9.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.9.m9.1b"><ci id="S5.SS2.p2.9.m9.1.1.cmml" xref="S5.SS2.p2.9.m9.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.9.m9.1c">c</annotation></semantics></math> can be estimated by the breakpoints in the original compression function.
It can be noticed that the upper-bound and multiplication functions can compensate the bias more caused by the forward compression compared with the addition approximation.
However, it is necessary to explore a more efficient approximation function to improve the model performance under compression.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic">Splitting Design for Resource Allocation</span>
</h3>

<figure id="S5.F5" class="ltx_figure"><img src="/html/2202.04309/assets/x5.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="261" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Communication and computation cost in the VFL framework with three splitting schemes. The <span id="S5.F5.3.1" class="ltx_text ltx_font_bold">left and right bars</span> of each scheme represent the cost of <span id="S5.F5.4.2" class="ltx_text ltx_font_bold">a single guest organization and the host organization</span>, respectively (Avazu dataset).</figcaption>
</figure>
<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">This subsection will evaluate the effects on the performance, and computation and transmission cost for different splitting methods as discussed in subsection <a href="#S4.SS4" title="IV-D Splitting Design ‣ IV Methodologies and Possible Solutions ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span></span></a>.
We adopt three different splitting designs to train a VFL model with the Avazu dataset as follows:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix1.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I1.ix1.1.1.m1.1b"><mo id="S5.I1.ix1.1.1.m1.1.1" xref="S5.I1.ix1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix1.1.1.m1.1c"><ci id="S5.I1.ix1.1.1.m1.1.1.cmml" xref="S5.I1.ix1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix1.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I1.ix1.p1" class="ltx_para">
<p id="S5.I1.ix1.p1.1" class="ltx_p"><span id="S5.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">Scheme 1:</span> splitting the first hidden layer, and thus each guest will output <math id="S5.I1.ix1.p1.1.m1.1" class="ltx_Math" alttext="16" display="inline"><semantics id="S5.I1.ix1.p1.1.m1.1a"><mn id="S5.I1.ix1.p1.1.m1.1.1" xref="S5.I1.ix1.p1.1.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S5.I1.ix1.p1.1.m1.1b"><cn type="integer" id="S5.I1.ix1.p1.1.m1.1.1.cmml" xref="S5.I1.ix1.p1.1.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix1.p1.1.m1.1c">16</annotation></semantics></math> values.</p>
</div>
</li>
<li id="S5.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix2.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I1.ix2.1.1.m1.1b"><mo id="S5.I1.ix2.1.1.m1.1.1" xref="S5.I1.ix2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix2.1.1.m1.1c"><ci id="S5.I1.ix2.1.1.m1.1.1.cmml" xref="S5.I1.ix2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix2.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I1.ix2.p1" class="ltx_para">
<p id="S5.I1.ix2.p1.1" class="ltx_p"><span id="S5.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">Scheme 2:</span> splitting the second hidden layer, and thus each guest will output <math id="S5.I1.ix2.p1.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S5.I1.ix2.p1.1.m1.1a"><mn id="S5.I1.ix2.p1.1.m1.1.1" xref="S5.I1.ix2.p1.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S5.I1.ix2.p1.1.m1.1b"><cn type="integer" id="S5.I1.ix2.p1.1.m1.1.1.cmml" xref="S5.I1.ix2.p1.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix2.p1.1.m1.1c">32</annotation></semantics></math> values.</p>
</div>
</li>
<li id="S5.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.ix3.1.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.I1.ix3.1.1.m1.1b"><mo id="S5.I1.ix3.1.1.m1.1.1" xref="S5.I1.ix3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.I1.ix3.1.1.m1.1c"><ci id="S5.I1.ix3.1.1.m1.1.1.cmml" xref="S5.I1.ix3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix3.1.1.m1.1d">\bullet</annotation></semantics></math></span> 
<div id="S5.I1.ix3.p1" class="ltx_para">
<p id="S5.I1.ix3.p1.1" class="ltx_p"><span id="S5.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">Scheme 3:</span> splitting the third hidden layer, and thus each guest will output <math id="S5.I1.ix3.p1.1.m1.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S5.I1.ix3.p1.1.m1.1a"><mn id="S5.I1.ix3.p1.1.m1.1.1" xref="S5.I1.ix3.p1.1.m1.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S5.I1.ix3.p1.1.m1.1b"><cn type="integer" id="S5.I1.ix3.p1.1.m1.1.1.cmml" xref="S5.I1.ix3.p1.1.m1.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.ix3.p1.1.m1.1c">64</annotation></semantics></math> values.</p>
</div>
</li>
</ul>
<p id="S5.SS3.p1.2" class="ltx_p">We can notice that the test AUC decreases with deeper cutting layers.
The reason is that when the splitting layer is closer to the raw data, more information of the original data can be combined.
However, the simple data preprocess, e.g., a full-connection layer, makes privacy information protection challenging.
From the view of resource consumes, as shown in Fig. <a href="#S5.F5" title="Figure 5 ‣ V-C Splitting Design for Resource Allocation ‣ V Experiment Results ‣ Vertical Federated Learning: Challenges, Methodologies and Experiments" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we can observe that when the splitting layer keeps away from the raw data, the computation cost of the guest organization (attribute owner) will be large and the one of the host organization (label owner) will be few.
We can also note that the transmission cost is determined by the unit size of the splitting layer.
If the splitting layer possesses more units, both participants and label owner need to consume a larger transmission cost.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this article, we have investigated potential challenges and unique issues in VFL from four aspects, i.e., security and privacy risks, expensive computation and communication costs, structural damage, and system heterogeneity.
We have pointed out that the splitting design should be adapted to the model’s specific structure, and it also affects the privacy and security protection as well as computation and communication efficiency of the overall VFL system.
In addition, we have discussed possible solutions for the considered issues one by one in designing VFL systems.
Lastly, we have evaluated the studied issues and solutions using two real-world datasets, e.g., DP assisted VFL, compression empowered communication efficiency, and splitting design for resource allocation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
D. C. Nguyen <em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic">et al.</em>, “Federated learning for internet of things: A
comprehensive survey,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys <math id="bib.bib1.1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="bib.bib1.1.1.m1.1a"><mo id="bib.bib1.1.1.m1.1.1" xref="bib.bib1.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="bib.bib1.1.1.m1.1b"><and id="bib.bib1.1.1.m1.1.1.cmml" xref="bib.bib1.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="bib.bib1.1.1.m1.1c">\&amp;</annotation></semantics></math> Tutorials</em>,
vol. 23, no. 3, pp. 1622–1658, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
K. Wei <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated learning with differential privacy:
Algorithms and performance analysis,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Transactions on
Information Forensics and Security</em>, vol. 15, pp. 3454–3469, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
C. Ma <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “On safeguarding privacy and security in the framework
of federated learning,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 34, no. 4, pp. 242–248,
2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Q. Yang <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated machine learning: Concept and
applications,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology</em>, vol. 10, no. 2, pp. 12:1–12:19, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R. Yu and P. Li, “Toward resource-efficient federated learning in mobile edge
computing,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 35, no. 1, pp. 148–155, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Y. Cheng <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated learning for privacy-preserving AI,”
<em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol. 63, no. 12, pp. 33–36, Nov. 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
F. Fu <em id="bib.bib7.2.1" class="ltx_emph ltx_font_italic">et al.</em>, “VF<sup id="bib.bib7.3.2" class="ltx_sup"><span id="bib.bib7.3.2.1" class="ltx_text ltx_font_italic">2</span></sup>Boost: Very fast vertical federated gradient
boosting for cross-enterprise learning,” in <em id="bib.bib7.4.3" class="ltx_emph ltx_font_italic">Proc. International
Conference on Management of Data (SIGMOD)</em>, Virtual Event, China, Jun. 2021,
pp. 563–576.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
L. Lu and N. Ding, “Multi-party private set intersection in vertical federated
learning,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proc. IEEE International Conference on Trust, Security
and Privacy in Computing and Communications (TrustCom)</em>, Guangzhou, China,
2020, pp. 707–714.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Fu <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Label inference attacks against vertical federated
learning,” in <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">Proc. USENIX Security Symposium (USENIX Security)</em>,
Boston, MA, Aug. 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
X. Jin <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “CAFE: Catastrophic data leakage in vertical
federated learning,” in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Proc. Thirty-fifth Conference on Neural
Information Processing Systems (NeurPIS)</em>, Virtual Event, Dec. 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
B. Gu <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated doubly stochastic kernel learning for
vertically partitioned data,” in <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">Proc. ACM SIGKDD Conference on
Knowledge Discovery and Data Mining (KDD)</em>, Virtual Event, CA, USA, Aug.
2020, pp. 2483–2493.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Singh <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Detailed comparison of communication efficiency of
split learning and federated learning,” <em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic">Arxiv</em>, 2019. [Online].
Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://arxiv.org/abs/1909.09145</span>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Q. Zhang <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Secure bilevel asynchronous vertical federated
learning with backward updating,” in <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Proc. Thirty-Fifth AAAI
Conference on Artificial Intelligence (AAAI)</em>, Virtual Event, Feb. 2021, pp.
10 896–10 904.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Abuadbba <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Can we use split learning on 1d CNN models for
privacy preserving training?” in <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Proc. ACM Asia Conference on
Computer and Communications Security (ASIACCS)</em>, Taipei, Taiwan, Oct. 2020,
pp. 305–318.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Jiang <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “SketchML: Accelerating distributed machine
learning with data sketches,” in <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Proc. International Conference on
Management of Data</em>, Houston, TX, USA, 2018, pp. 1269–1284.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2202.04308" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2202.04309" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2202.04309">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2202.04309" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2202.04310" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 18:02:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
