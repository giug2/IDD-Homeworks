<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation</title>
<!--Generated on Mon Sep 16 16:45:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.10459v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S1" title="In Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Punch-Hole Labeling</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S1.SS1" title="In 1 Punch-Hole Labeling ‣ Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Algorithm</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S1.SS2" title="In 1 Punch-Hole Labeling ‣ Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Advantages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S2" title="In Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminary Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S2.SS1" title="In 2 Preliminary Study ‣ Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Procedure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S2.SS2" title="In 2 Preliminary Study ‣ Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#S2.SS3" title="In 2 Preliminary Study ‣ Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Discussion &amp; Future Work</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.2">\onlineid</span>
<p class="ltx_p" id="p1.1">1105
<span class="ltx_ERROR undefined" id="p1.1.1">\vgtccategory</span>Research Poster
<span class="ltx_ERROR undefined" id="p1.1.2">\vgtcinsertpkg</span>
<span class="ltx_ERROR undefined" id="p1.1.3">\teaser</span>
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="235" id="p1.1.g1" src="extracted/5858592/figures/annotations.png" width="598"/>
<span class="ltx_text ltx_caption ltx_align_center" id="p1.1.4">Comparison of three methods used to extract important areas in charts. <span class="ltx_text ltx_font_bold" id="p1.1.4.1">Importance Annotation</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite> (left): Drawing a box or polygon with a mouse. <span class="ltx_text ltx_font_bold" id="p1.1.4.2">Gaze Tracker</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>]</cite> (middle): Capture the points where the eye stays long. <span class="ltx_text ltx_font_bold" id="p1.1.4.3">Punch-Hole Labeling</span> (right): Extract important patches for proper question answering. The example annotation for Importance Annotation and Punch-Hole labeling is retrieved from the pilot study.</span></p>
</div>
<h1 class="ltx_title ltx_title_document">Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Minsuk Chang <sup class="ltx_sup" id="17.3.1">1</sup>
</span><span class="ltx_author_notes">e-mail: {minsuk, cxiong}@gatech.edu</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Soohyun Lee <sup class="ltx_sup" id="18.3.1">2</sup>
</span><span class="ltx_author_notes">e-mail: {shlee, archo, hj, shpark, jseo}@hcil.snu.ac.kr</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Aeri Cho<sup class="ltx_sup" id="19.3.1"><span class="ltx_text ltx_font_italic" id="19.3.1.1">†</span></sup> <sup class="ltx_sup" id="20.4.2">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Hyeon Jeon<sup class="ltx_sup" id="21.3.1"><span class="ltx_text ltx_font_italic" id="21.3.1.1">†</span></sup> <sup class="ltx_sup" id="22.4.2">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Seokhyeon Park<sup class="ltx_sup" id="23.3.1"><span class="ltx_text ltx_font_italic" id="23.3.1.1">†</span></sup> <sup class="ltx_sup" id="24.4.2">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Cindy Xiong Bearfield<sup class="ltx_sup" id="25.3.1">∗</sup> <sup class="ltx_sup" id="26.4.2">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jinwook Seo<sup class="ltx_sup" id="27.5.1"><span class="ltx_text ltx_font_italic" id="27.5.1.1">†</span></sup> <sup class="ltx_sup" id="28.6.2">2</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><sup class="ltx_sup" id="29.7.2"><span class="ltx_text" id="29.7.2.1" style="font-size:70%;">1</span></sup><span class="ltx_text" id="id16.4.1" style="font-size:70%;">Georgia Institute of Technology  <sup class="ltx_sup" id="id16.4.1.1">2</sup>Seoul National University</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="30.1">We introduce a novel crowdsourcing method for identifying important areas in graphical images through punch-hole labeling. Traditional methods, such as gaze trackers and mouse-based annotations, which generate continuous data, can be impractical in crowdsourcing scenarios. They require many participants, and the outcome data can be noisy. In contrast, our method first segments the graphical image with a grid and drops a portion of the patches (punch holes). Then, we iteratively ask the labeler to validate each annotation with holes, narrowing down the annotation only having the most important area. This approach aims to reduce annotation noise in crowdsourcing by standardizing the annotations while enhancing labeling efficiency and reliability. Preliminary findings from fundamental charts demonstrate that punch-hole labeling can effectively pinpoint critical regions. This also highlights its potential for broader application in visualization research, particularly in studying large-scale users’ graphical perception. Our future work aims to enhance the algorithm to achieve faster labeling speed and prove its utility through large-scale experiments.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Crowdsourcing, visual annotation, importance labeling, punch-hole annotation, graphical image analysis.
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1">Introduction</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">Extracting saliency in graphical images has gained importance for analyzing cognitive activities, such as perceptual bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib1" title="">1</a>]</cite> and subjective importance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib3" title="">3</a>]</cite>. Various researchers use crowdsourcing to retrieve and utilize such human-centric data on a large scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">However, crowd workers often annotate images directly (e.g., bounding boxes, polygons) or indirectly (e.g., gaze capture). While intuitive, this method introduces two main problems. First, labelers may overemphasize certain regions, neglecting other important areas. For example, TurkEyes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite> shows that the extracted importance in a resume is concentrated near the title, overshadowing other sections. Second, free-form annotations (e.g., colored pixels, gaze points) are highly variable, requiring data processing or combining responses from multiple participants. ScannerDeeply <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>]</cite> addressed this by implementing active noise removal in gaze data to counter low accuracy of webcam-based gaze trackers, adding to processing efforts and raising labeling costs.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p" id="p5.1">As a breakthrough, we suggest a novel grid-based annotation to effectively crowdsource the importance or saliency of the graphical image. Punch-hole labeling reduces the continuous annotation task into multiple binary questions, fostering consensus among participants and reducing labeling costs. The process involves dividing the image into small grids and hiding one patch at a time (similar to punching holes) to identify less important areas. If users can answer the given question based on the shown patches, it indirectly indicates that the punched holes lack significance. This approach aims to prevent omitting critical but overlooked regions, minimize unnecessary discrepancies among annotations, and simplify the annotation task for greater time efficiency.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p" id="p6.1">Our preliminary study with two fundamental charts in <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation</span></span> suggests that Punch-hole labeling may achieve faster labeling speed and produce a more reliable importance map than traditional approaches. Our next goal is to enhance the punching algorithm to increase labeling speed and deploy a large-scale experiment in the crowdsourcing platforms.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Punch-Hole Labeling</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Punch-Hole annotation is based on the idea that <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">simplified questions and responses</span> in a crowdsourcing environment can improve both the speed and reliability of labeling tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib2" title="">2</a>]</cite>. Compared to the original annotation task of “<span class="ltx_text ltx_font_italic" id="S1.p1.1.2">Annotate the important area related to this question.</span>”, it is reduced to the binary question of “<span class="ltx_text ltx_font_italic" id="S1.p1.1.3">Can you answer the question based on these patches?</span>”. Our punch-hole algorithm and its advantages follow this principle.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Algorithm</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">We begin by dividing an image into square patches using a predefined grid size and displaying only a subset of these patches to crowd workers. This approach involves sequentially hiding each patch, like punching holes in a piece of paper, and asking whether the remaining visible portions of the image are sufficient to answer the given question. In <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation</span></span>, the remaining areas are essential for answering the questions, compared to unnecessary black patches. For instance, the pie chart’s top-right corner or the bar chart’s middle, which appears black, is unimportant for retrieving the answer.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">This process is iterative, with holes being punched until all visible patches are crucial for the given question. Subsequently, the area’s resolution can be enhanced by reducing the grid size and repeating the process, theoretically achieving pixel-level granularity. Labeling time and patch size present an inverse tradeoff that deployers can adjust based on their budget or quality requirements.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Advantages</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">The potential advantages of punch-hole labeling can be expressed with three key terms:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Show-And-Verify</span> strategy prevents underestimation of specific areas. Punch-hole labeling controls the information presented to users, ensuring no sections are overlooked. Previous methods using active responses like gaze points and polygons <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite> allowed too much freedom, leading to incomplete coverage. For example, users might skim the legend and focus solely on the data, neglecting the legend’s importance. Our approach includes all crucial patches in the final annotation, enhancing quality.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Discrete Responses</span> reduce the number of required participants. Previous methods like gaze trackers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>]</cite> and mouse-based interfaces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite> face issues with merging diverse responses, requiring many participants to suppress noise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite>. Punch-hole annotations, with lower granularity and standardized format, separate controversial or subjective areas from consensus ones. This method achieves the desired granularity after iterations with smaller patch sizes, reducing the impact of outliers and noise.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Task Simplicity</span> increases accessibility. Punch-hole labeling simplifies and standardizes the task, requiring only two buttons (yes/no). Traditional tools require facial position calibration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>]</cite> or precise clicking and brushing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite>, relying heavily on motor skills. This can limit accessibility for elderly users, significant contributors to crowdsourcing. Poor devices can also produce noisy annotations, requiring iterations for accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib5" title="">5</a>]</cite>. The punch-hole algorithm addresses this by focusing on the cognitive task, reducing complexity, and enhancing accessibility.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminary Study</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">As a preliminary study, we tested the effectiveness and efficiency of our punch-hole annotation with a pilot test with two labelers. We analyzed whether our approach could effectively find the important area in the image while reaching a faster labeling speed.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Procedure</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">We started our study by generating the fundamental graphical image for the experiment. Leveraging ChatGPT, we created one pie chart with fruit sales and one bar chart with car sales. Trailing questions were also generated based on the charts’ contents. We then tested our algorithm on a simple web-based labeling tool and compared its results with the box annotation from importance annotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite>. Users were allowed to annotate more than one box for a fair comparison. The time spent to label each annotation was recorded for later analysis.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Results</h3>
<div class="ltx_para" id="S2.SS2.p1">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Extracted Annotations.</span> An example of punch-hole annotation is shown in <span class="ltx_ref ltx_refmacro_autoref ltx_ref_self"><span class="ltx_text ltx_ref_title">Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation</span></span>. Punch-hole annotations include the core area needed to answer the question correctly. In importance annotation, users’ marked areas varied greatly; one marked the whole bar, another only the top. Users also often miss or over-mark legends as important.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Time Analysis.</span> The average labeling speed was 1.32 seconds per annotation candidate and 30.36 seconds per chart. Punch-hole labeling takes a similar time to the 30 seconds reported for importance annotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10459v1#bib.bib4" title="">4</a>]</cite>. By dynamically adjusting the punch-hole shape and refining which patches to hide, we aim to create more precise areas in less time.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Discussion &amp; Future Work</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We have confirmed that punch-hole labeling can extract relative importance from the image within a reasonable time. Our findings also indicate that traditional annotation techniques often miss essential areas and produce highly variable results. Consequently, we aim to advance our punch hole algorithm by: 1) minimizing the number of punch holes to reduce the number of micro-tasks for crowdsourcing workers, and 2) dynamically determining the optimal punch-hole size for each image.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">We also observed that the order in which we punch holes may influence the final output. Randomizing the order of punching and analyzing how the final areas are extracted will be another future direction. To achieve this, we plan to experiment with this approach in a crowdsourcing platform to increase the reliability of the results and conduct a large-scale analysis of the extracted areas, ultimately forming a dataset. We expect that our work will offer researchers focused on human cognition and visual perception the opportunity to crowdsource large-scale annotations, even with limited budgets.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work is supported by NSF awards IIS-2237585 and IIS-2311575.



</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
C. X. Bearfield, L. van Weelden, A. Waytz, and S. Franconeri.

</span>
<span class="ltx_bibblock">Same data, diverging perspectives: The power of visualizations to elicit competing interpretations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(6):2995–3007, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R. M. Borromeo, T. Laurent, and M. Toyama.

</span>
<span class="ltx_bibblock">The influence of crowd type and task complexity on crowdsourced work quality.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 20th International Database Engineering &amp; Applications Symposium</span>, pp. 70–76, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Bylinskii, N. W. Kim, P. O. Donovan, S. Alsheikh, S. Madan, H. Pfister, F. Durand, B. Russell, and A. Hertzmann.

</span>
<span class="ltx_bibblock">Learning visual importance for graphic designs and data visualizations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 30th Annual ACM Symposium on User Interface Software &amp; Technology</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Newman, B. McNamara, C. Fosco, Y. B. Zhang, P. Sukhum, M. Tancik, N. W. Kim, and Z. Bylinskii.

</span>
<span class="ltx_bibblock">Turkeyes: A web-based toolbox for crowdsourcing attention data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Shin, S. Chung, S. Hong, and N. Elmqvist.

</span>
<span class="ltx_bibblock">A scanner deeply: Predicting gaze heatmaps on visualizations using crowdsourced eye movement data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 29(1):396–406, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 16 16:45:49 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
