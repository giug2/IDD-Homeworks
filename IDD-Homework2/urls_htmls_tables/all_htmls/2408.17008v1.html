<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.17008] Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications</title><meta property="og:description" content="With the ubiquitous use of document corpora for question answering, one important aspect which is especially relevant for technical documents is the ability to extract information from tables which are interspersed wit…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.17008">

<!--Generated on Thu Sep  5 15:31:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sujoy Roychowdhury, Sumit Soman, HG Ranjani, Avantika Sharma*, Neeraj Gunda*, Sai Krishna Bala 
<br class="ltx_break">GAIA, Ericsson 
<br class="ltx_break">Bangalore, Karnataka, India 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">{sujoy.roychowdhury, sumit.soman, ranjani.h.g, sai.krishna.bala}@ericsson.com</span>
</span><span class="ltx_author_notes">This work was done during the author’s internship at GAIA, Ericsson.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">With the ubiquitous use of document corpora for question answering, one important aspect which is especially relevant for technical documents is the ability to extract information from tables which are interspersed with text. The major challenge in this is that unlike free-flow text or isolated set of tables, the representation of a table in terms of what is a relevant chunk is not obvious. We conduct a series of experiments examining various representations of tabular data interspersed with text to understand the relative benefits of different representations. We choose a corpus of <math id="id1.1.m1.1" class="ltx_Math" alttext="3^{rd}" display="inline"><semantics id="id1.1.m1.1a"><msup id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">3</mn><mrow id="id1.1.m1.1.1.3" xref="id1.1.m1.1.1.3.cmml"><mi id="id1.1.m1.1.1.3.2" xref="id1.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="id1.1.m1.1.1.3.1" xref="id1.1.m1.1.1.3.1.cmml">​</mo><mi id="id1.1.m1.1.1.3.3" xref="id1.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="ambiguous" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1">superscript</csymbol><cn type="integer" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">3</cn><apply id="id1.1.m1.1.1.3.cmml" xref="id1.1.m1.1.1.3"><times id="id1.1.m1.1.1.3.1.cmml" xref="id1.1.m1.1.1.3.1"></times><ci id="id1.1.m1.1.1.3.2.cmml" xref="id1.1.m1.1.1.3.2">𝑟</ci><ci id="id1.1.m1.1.1.3.3.cmml" xref="id1.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">3^{rd}</annotation></semantics></math> Generation Partnership Project (3GPP) documents since they are heavily interspersed with tables. We create expert curated dataset of question answers to evaluate our approach. We conclude that row level representations with corresponding table header information being included in every cell improves the performance of the retrieval, thus leveraging the structural information present in the tabular data.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.7" class="ltx_p"><em id="p1.7.1" class="ltx_emph ltx_font_bold ltx_font_italic">K</em><span id="p1.7.2" class="ltx_text ltx_font_bold">eywords</span> Question Answering  <math id="p1.1.m1.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation></semantics></math>
Table Representation Learning  <math id="p1.2.m2.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation></semantics></math>
Table QA  <math id="p1.3.m3.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation></semantics></math>
Multi-modal QA  <math id="p1.4.m4.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation></semantics></math>
Retrieval Augmented Generation  <math id="p1.5.m5.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation></semantics></math>
Large Language Models  <math id="p1.6.m6.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation></semantics></math>
Telecom  <math id="p1.7.m7.1" class="ltx_Math" alttext="\cdot" display="inline"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation></semantics></math>
3GPP Standards</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Question Answering (QA) based on retrieval from documents has evoked interest from the research community with recent advances in Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Typically questions can potentially be answered from text present in paragraphs or sections of documents. However, questions anchored on technical documents often involve answers that require comprehending information from tables present in these documents. The tables in documents can be of diverse types: numerical entries, textual data, a combination of both, include symbols, abbreviations, technical specifications or terminology, equations and sometimes even images. Some examples of such tables are shown in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a> and Table <a href="#S3.T2" title="Table 2 ‣ 3 Dataset ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To answer questions related to tables, content from tables in documents must be parsed and represented in a way such that embeddings can be generated. These embeddings in-turn can be used to retrieve most similar chunks to the question embeddings. It is possible that the representations and the choice of embedding models has a bearing on the performance of the QA systems. To the best of our knowledge, a systematic study analysing tabular representations, evaluating on retrieval from technical documents is limited in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">The premise of our work is as follows. Prior work has focused on open-domain table QA with training and evaluation on general datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, domain-specific applications, such as telecom, need technical documents and domain understanding which involve specialized vocabulary.
<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="3^{rd}" display="inline"><semantics id="S1.p3.1.m1.1a"><msup id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mn id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">3</mn><mrow id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml"><mi id="S1.p3.1.m1.1.1.3.2" xref="S1.p3.1.m1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S1.p3.1.m1.1.1.3.1" xref="S1.p3.1.m1.1.1.3.1.cmml">​</mo><mi id="S1.p3.1.m1.1.1.3.3" xref="S1.p3.1.m1.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">3</cn><apply id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3"><times id="S1.p3.1.m1.1.1.3.1.cmml" xref="S1.p3.1.m1.1.1.3.1"></times><ci id="S1.p3.1.m1.1.1.3.2.cmml" xref="S1.p3.1.m1.1.1.3.2">𝑟</ci><ci id="S1.p3.1.m1.1.1.3.3.cmml" xref="S1.p3.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">3^{rd}</annotation></semantics></math> Generation Partnership Project (3GPP) documents, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> are widely accessed by telecommunications specialists in industry. These include several tables that have technical terminology and related specifications, interspersed with (associated) text, figures and equations. It is necessary to extract exact information from the tables to be able to answer questions correctly.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf1.1" class="ltx_p ltx_align_center"><span id="S1.F1.sf1.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2408.17008/assets/icml2024/figs/RepsNew/TableEx.png" id="S1.F1.sf1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="556" height="309" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>A snapshot from a 3GPP document showing how table and text are interspersed</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf2.1" class="ltx_p ltx_align_center"><span id="S1.F1.sf2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2408.17008/assets/icml2024/figs/RepsNew/TableExPipe.jpg" id="S1.F1.sf2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="556" height="206" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Table representation in pipe separated format.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S1.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<p id="S1.F1.sf3.1" class="ltx_p ltx_align_center"><span id="S1.F1.sf3.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="border-color: #000000;"><img src="/html/2408.17008/assets/icml2024/figs/RepsNew/TableExPipeHeader.jpg" id="S1.F1.sf3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="556" height="205" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Table representation in pipe separated format with repeating headers across rows.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A sample table and its representations used for our experiments.</figcaption>
</figure>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">We would like to highlight few important design choices for our study.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The parsed data considered is textual in nature along with tables (including captions); we do not include images and equations in this work.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our study is based on publicly available pre-trained models as our study is to create a benchmark of the retrieval performance based on various representations of tabular information.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p id="S1.I1.i3.p1.1" class="ltx_p">The scope of this study is the retrieval task alone. Current LLMs are able to generate correct answers from contexts containing tables but are dependent on the retrieval output for creating the input context to the LLMs. Hence, it is important to ensure relevant retrievals when the number of tables interspersed with text is large <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p">We highlight that using publicly available embeddings is not only the starting point of any document retrieval / question-answering task, many industrial applications are often limited to using such embeddings because of cost and data constraints. Our study provides clear recommendations which would be helpful even under such constraints.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Research Questions and Contributions</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.1" class="ltx_p">The research questions addressed in this study are as follows:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p"><span id="S1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">RQ1:</span> How is the retrieval performance affected when tables are interspersed with text?</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p"><span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">RQ2:</span> Does granularity (table or row level) of the embedding chunk impact retrieval performance?</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p"><span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">RQ3:</span> Does adding the table header information improve retrieval performance?</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para ltx_noindent">
<p id="S1.I2.i4.p1.1" class="ltx_p"><span id="S1.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">RQ4:</span> Would tabular data representation to demarcate columns modify retrieval accuracies?</p>
</div>
</li>
</ul>
<p id="S1.SS1.p1.2" class="ltx_p">Our contributions from this work are:</p>
<ul id="S1.I3" class="ltx_itemize">
<li id="S1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i1.p1" class="ltx_para">
<p id="S1.I3.i1.p1.1" class="ltx_p">We create a QA dataset such that the answers to questions require information from tables (from 3GPP documents).</p>
</div>
</li>
<li id="S1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i2.p1" class="ltx_para">
<p id="S1.I3.i2.p1.1" class="ltx_p">Conduct a systematic study on the effect of table representation on performance on the retrieval task.</p>
</div>
</li>
<li id="S1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i3.p1" class="ltx_para">
<p id="S1.I3.i3.p1.1" class="ltx_p">We establish that the presence of interspersed text reduces retrieval performance.</p>
</div>
</li>
<li id="S1.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i4.p1" class="ltx_para">
<p id="S1.I3.i4.p1.1" class="ltx_p">We observe that it is better to consider embeddings of rows of tables rather than a single embedding of an entire table. Also, representation introducing table header information along with the tabular content for each cell, prior to the embedding process, improves performance.</p>
</div>
</li>
<li id="S1.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i5.p1" class="ltx_para ltx_noindent">
<p id="S1.I3.i5.p1.1" class="ltx_p">We find publicly available pre-trained embedding models to be competitive in terms of retrieval accuracies in an interspersed text and table setting and the table header being introduced in the cell information.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<p id="S1.SS1.p2.1" class="ltx_p">The rest of this paper is organized as follows. We position our work vis-à-vis that in the literature in Section <a href="#S2" title="2 Literature Review ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For a robust evaluation setting, we choose 3GPP’s technical specification release 18 with a large number of tables across multiple documents (details in Section <a href="#S3" title="3 Dataset ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We create an internal dataset curated by Subject Matter Experts (SMEs) that encompass multiple retrieval modalities from the tables through extractive, aggregation and inferential questions. The experimental setup and results are discussed in Sections <a href="#S4" title="4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5" title="5 Results ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> respectively. Section <a href="#S6" title="6 Conclusions ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes results along with possible directions for future work.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Methods for representation of tabular data have been prevalent in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Prior work such as TaPas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and TAPEX <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> retrieve information from a row, given the correct table. In addition, they have been reported to be unsuitable for larger tables due to token length limitations. Representations amenable for RAG tasks have been evaluated to build an end-to-end table QA system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. An approach for converting tables to text and then using it to augment LLMs has been analyzed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, while knowledge based representations have also been shown to be useful in table QA tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For handling long representations arising out of complex tables, inner table retrieval has been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Identifying relevant parts of tables for QA on large tables has also been evaluated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Our work here complements the literature cited; there are limited works in literature which consider text interspersed with tables, a typical scenario in technical documents.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">As mentioned earlier, we use 3GPP Release Specifications as input data; in particular, we consider 3GPP Release 18 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, which contains consider 392 documents. The data is available in <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">‘.docx’</span> format. Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Dataset ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows a histogram of the distribution of number of rows of a table (excluding header) - we see that most tables have <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><lt id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">&lt;</annotation></semantics></math> 10 rows. This is due to the fact that most tables provide information about certain specifications - the sample image in Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a> is such a table.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:232pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.2pt,-35.0pt) scale(1.4322709147494,1.4322709147494) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Number of documents</td>
<td id="S3.T1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">392</td>
</tr>
<tr id="S3.T1.1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Number of tables</td>
<td id="S3.T1.1.1.2.2.2" class="ltx_td ltx_align_right ltx_border_r">21,824</td>
</tr>
<tr id="S3.T1.1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Number of sentences in free flowing text</td>
<td id="S3.T1.1.1.3.3.2" class="ltx_td ltx_align_right ltx_border_r">948,616</td>
</tr>
<tr id="S3.T1.1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Number of tables from which questions are covered</td>
<td id="S3.T1.1.1.4.4.2" class="ltx_td ltx_align_right ltx_border_r">62</td>
</tr>
<tr id="S3.T1.1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Total Number of Questions</td>
<td id="S3.T1.1.1.5.5.2" class="ltx_td ltx_align_right ltx_border_r">278</td>
</tr>
<tr id="S3.T1.1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Data Extraction Questions</td>
<td id="S3.T1.1.1.6.6.2" class="ltx_td ltx_align_right ltx_border_r">50</td>
</tr>
<tr id="S3.T1.1.1.7.7" class="ltx_tr">
<td id="S3.T1.1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Aggregation Questions</td>
<td id="S3.T1.1.1.7.7.2" class="ltx_td ltx_align_right ltx_border_r">77</td>
</tr>
<tr id="S3.T1.1.1.8.8" class="ltx_tr">
<td id="S3.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">Answers requiring Multiple rows/Columns</td>
<td id="S3.T1.1.1.8.8.2" class="ltx_td ltx_align_right ltx_border_r">51</td>
</tr>
<tr id="S3.T1.1.1.9.9" class="ltx_tr">
<td id="S3.T1.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">Logic based questions</td>
<td id="S3.T1.1.1.9.9.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_r">50</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary statistics of dataset for our experiments.</figcaption>
</figure>
<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:294.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-127.9pt,96.6pt) scale(0.60402273342871,0.60402273342871) ;"><img src="/html/2408.17008/assets/icml2024/figs/others/distributionTables.png" id="S3.F2.1.g1" class="ltx_graphics ltx_img_landscape" width="894" height="675" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Histogram for number of rows (excluding header) in table across corpus</figcaption>
</figure>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">This is a realistic data scenario in contrast to the SciGen dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>; SciGen data comprises of scientific tables containing numerical values and their descriptions. The parsed data considered in this work (using 3GPP specifications) includes title, section headings, text, tables, table of contents, comments; header and footer information are ignored.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">For the tabular retrieval evaluation, we have created an internal dataset of 278 questions manually curated by Subject Matter Experts (SMEs). Statistics of our dataset is in Table <a href="#S3.T1" title="Table 1 ‣ 3 Dataset ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The dataset encompasses multiple retrieval modalities from the tables through extractive, aggregation and inferential questions. A sample example of raw table (Figure <a href="#S1.F1.sf1" title="In Figure 1 ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a>) and it’s parsed formats are shown in Figs. <a href="#S1.F1.sf2" title="In Figure 1 ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a> and <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a> respectively.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">We consider four types of questions (the notation used for these types of questions in the following sections of this paper are indicated in parenthesis with the respective question types):</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Extraction type questions (E)</span> - requires factual information to be extracted from a single cell in a table.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Answers requiring multiple rows and columns (M)</span> - requires factual information to be extracted, but need access to multiple rows/columns from a single table.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Aggregation based questions (A)</span> - requires an aggregation of the results across either a row or column of a single table. Aggregation can involve operations such as count or average.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para ltx_noindent">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Inferential questions (I)</span> - requires drawing inference from multiple rows and columns of a table.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p">Sample questions and their corresponding tables and types are provided in Table <a href="#S3.T2" title="Table 2 ‣ 3 Dataset ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for reference.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.4.5.1" class="ltx_tr">
<th id="S3.T2.4.5.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_t">
<span id="S3.T2.4.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.5.1.1.1.1" class="ltx_p" style="width:346.9pt;"><span id="S3.T2.4.5.1.1.1.1.1" class="ltx_text ltx_font_bold">3GPP Table</span></span>
</span>
</th>
<th id="S3.T2.4.5.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t">
<span id="S3.T2.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.5.1.2.1.1" class="ltx_p" style="width:65.0pt;"><span id="S3.T2.4.5.1.2.1.1.1" class="ltx_text ltx_font_bold">Question</span></span>
</span>
</th>
<th id="S3.T2.4.5.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t">
<span id="S3.T2.4.5.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.5.1.3.1.1" class="ltx_p" style="width:17.3pt;"><span id="S3.T2.4.5.1.3.1.1.1" class="ltx_text ltx_font_bold">Type</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_t" rowspan="2">
<span id="S3.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1" class="ltx_p" style="width:346.9pt;"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text"><img src="/html/2408.17008/assets/icml2024/figs/Tables/T0.png" id="S3.T2.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="327" height="125" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T2.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.2.1.1" class="ltx_p" style="width:65.0pt;">What is the maximum end-to-end latency specified for a UK Home?</span>
</span>
</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.3.1.1" class="ltx_p" style="width:17.3pt;">E</span>
</span>
</td>
</tr>
<tr id="S3.T2.4.6.1" class="ltx_tr">
<td id="S3.T2.4.6.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.4.6.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.6.1.1.1.1" class="ltx_p" style="width:65.0pt;">How many PIN Elements are there in the service area for the UK Home use case?</span>
</span>
</td>
<td id="S3.T2.4.6.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.4.6.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.6.1.2.1.1" class="ltx_p" style="width:17.3pt;">E</span>
</span>
</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_t" rowspan="2">
<span id="S3.T2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.1.1.1" class="ltx_p" style="width:346.9pt;"><span id="S3.T2.2.2.1.1.1.1" class="ltx_text"><img src="/html/2408.17008/assets/icml2024/figs/Tables/T1.png" id="S3.T2.2.2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="399" height="101" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.2.1.1" class="ltx_p" style="width:65.0pt;">What is the average jitter requirement across all modalities?</span>
</span>
</td>
<td id="S3.T2.2.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.2.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.3.1.1" class="ltx_p" style="width:17.3pt;">A</span>
</span>
</td>
</tr>
<tr id="S3.T2.4.7.2" class="ltx_tr">
<td id="S3.T2.4.7.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.4.7.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.7.2.1.1.1" class="ltx_p" style="width:65.0pt;">Is the throughput requirement for video higher than that for haptics?</span>
</span>
</td>
<td id="S3.T2.4.7.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.4.7.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.7.2.2.1.1" class="ltx_p" style="width:17.3pt;">I</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_t" rowspan="3">
<span id="S3.T2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.1.1.1" class="ltx_p" style="width:346.9pt;"><span id="S3.T2.3.3.1.1.1.1" class="ltx_text"><img src="/html/2408.17008/assets/icml2024/figs/Tables/T2.png" id="S3.T2.3.3.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="308" height="184" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T2.3.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.2.1.1" class="ltx_p" style="width:65.0pt;">What is the total number of user applications listed in the table?</span>
</span>
</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.3.1.1" class="ltx_p" style="width:17.3pt;">A</span>
</span>
</td>
</tr>
<tr id="S3.T2.4.8.3" class="ltx_tr">
<td id="S3.T2.4.8.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.4.8.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.8.3.1.1.1" class="ltx_p" style="width:65.0pt;">Identify the user applications with an image recognition latency of less than 5ms.</span>
</span>
</td>
<td id="S3.T2.4.8.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.4.8.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.8.3.2.1.1" class="ltx_p" style="width:17.3pt;">M</span>
</span>
</td>
</tr>
<tr id="S3.T2.4.9.4" class="ltx_tr">
<td id="S3.T2.4.9.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.4.9.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.9.4.1.1.1" class="ltx_p" style="width:65.0pt;">What is the total range of end-to-end latency for all user applications?</span>
</span>
</td>
<td id="S3.T2.4.9.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.4.9.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.9.4.2.1.1" class="ltx_p" style="width:17.3pt;">I</span>
</span>
</td>
</tr>
<tr id="S3.T2.4.4" class="ltx_tr">
<td id="S3.T2.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_t" rowspan="2">
<span id="S3.T2.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.4.1.1.1" class="ltx_p" style="width:346.9pt;"><span id="S3.T2.4.4.1.1.1.1" class="ltx_text"><img src="/html/2408.17008/assets/icml2024/figs/Tables/T3.png" id="S3.T2.4.4.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="471" height="130" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T2.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.4.2.1.1" class="ltx_p" style="width:65.0pt;">Which models have a bi-cubic downsampling model weight size?</span>
</span>
</td>
<td id="S3.T2.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T2.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.4.3.1.1" class="ltx_p" style="width:17.3pt;">M</span>
</span>
</td>
</tr>
<tr id="S3.T2.4.10.5" class="ltx_tr">
<td id="S3.T2.4.10.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T2.4.10.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.10.5.1.1.1" class="ltx_p" style="width:65.0pt;">Which model has a lower model weight size for upsampling, CAR 4x or SR-GAN 4x?</span>
</span>
</td>
<td id="S3.T2.4.10.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T2.4.10.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.4.10.5.2.1.1" class="ltx_p" style="width:17.3pt;">I</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Sample Questions created from 3GPP Tables (shown in the first column). We also have the type of Question in the third column - A is abbreviation for <span id="S3.T2.9.1" class="ltx_text ltx_font_bold">A</span>ggregation Type of Questions, E for <span id="S3.T2.10.2" class="ltx_text ltx_font_bold">E</span>xtraction, I for <span id="S3.T2.11.3" class="ltx_text ltx_font_bold">I</span>nferential and M for those requiring <span id="S3.T2.12.4" class="ltx_text ltx_font_bold">M</span>ultiple rows and columns</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">3GPP <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">‘.docx’</span> documents are parsed, converted to JSON format using the python-docx library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. The JSON representation of a document contains the title, section (and subsection) headings along with free flowing text and tables. The hierarchy and order of the content within the document is preserved in the JSON representation. The parsing process iterates through the elements of the document and identifies sections and tables accordingly. Tables are identified by the <span id="S4.p1.1.2" class="ltx_text ltx_font_italic">tbl</span> tag and can be associated to the sections. Table captions are identified by considering the preceding and subsequent elements in order to determine the most likely caption i.e., if the preceding element is a paragraph and starts with <span id="S4.p1.1.3" class="ltx_text ltx_font_italic">table</span>, it is most likely the caption for the table. Manual inspection of a large number of tables indicates that most tables have a header row. Based on this observation, the first row of any table has been assumed to contain the header information. We are aware there are small number of tables which contain information like document history, list of associated documents - these have been processed as-is and are part of the corpus for the retrieval task. The header and footer information has been ignored.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.17008/assets/icml2024/figs/accs/TablePipe.png" id="S4.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="405" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Chunk Level = Table, Separator= Pipe</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.17008/assets/icml2024/figs/accs/RowPipe.png" id="S4.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Chunk Level= Row, Separator=Pipe</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.17008/assets/icml2024/figs/accs/TableSpace.png" id="S4.F3.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="407" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Chunk Level = Table, Separator=Space</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.17008/assets/icml2024/figs/accs/RowSpace.png" id="S4.F3.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="405" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Chunk Level = Row, Separator=Space</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of top-5 retrieval accuracy (%)different representations for tables. The caption given on the bottom left subplot is common for all subplots.</figcaption>
</figure>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p">We design our experimental setup to answer the research questions posed in Section <a href="#S1.SS1" title="1.1 Research Questions and Contributions ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.1</span></a>.
First, we study the impact of retrieval based on presence of interspersed text as opposed to using tables alone.</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p">Second, whilst comparisons of embeddings between sentence and paragraphs has been studied <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the recommendation for granularity for embeddings of tables is limited in the literature. To this end, we evaluate our embeddings at both a row level chunk and a chunk at the whole table level.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p">Third, we consider how best to leverage the structure of tabular information. In this work, we repeat the column heading in every cell - an example is shown in Figure <a href="#S1.F1.sf3" title="In Figure 1 ‣ 1 Introduction ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(c)</span></a>.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p">Finally, we study if there is an impact of column separators in tabular data representation i.e., does column separators such as space or pipe (<math id="S4.p5.1.m1.1" class="ltx_Math" alttext="|" display="inline"><semantics id="S4.p5.1.m1.1a"><mo fence="false" stretchy="false" id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">|</mo><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">|</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">|</annotation></semantics></math>) helps. The advantage of pipe separation is that it is less likely to occur within the table - however we are not aware of how it would affect embedding representations using publicly available pre-trained models which may have been less exposed to pipe being present in the data.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p">Thus, we have four experimental binary conditions - inclusion (or exclusion) of interspersed text, table vs row level chunking, inclusion (or exclusion) of header in every cell and use of pipe or space as the column separator. We consider a full experimental design of <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="2^{4}=16" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><msup id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml"><mn id="S4.p6.1.m1.1.1.2.2" xref="S4.p6.1.m1.1.1.2.2.cmml">2</mn><mn id="S4.p6.1.m1.1.1.2.3" xref="S4.p6.1.m1.1.1.2.3.cmml">4</mn></msup><mo id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">=</mo><mn id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><eq id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></eq><apply id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.2.1.cmml" xref="S4.p6.1.m1.1.1.2">superscript</csymbol><cn type="integer" id="S4.p6.1.m1.1.1.2.2.cmml" xref="S4.p6.1.m1.1.1.2.2">2</cn><cn type="integer" id="S4.p6.1.m1.1.1.2.3.cmml" xref="S4.p6.1.m1.1.1.2.3">4</cn></apply><cn type="integer" id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">2^{4}=16</annotation></semantics></math> parameters and evaluate the retrieval performance for five pre-trained embedding models, <span id="S4.p6.1.1" class="ltx_text ltx_font_italic">viz.</span>,</p>
</div>
<div id="S4.p7" class="ltx_para ltx_noindent">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">From the sentence transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> library, we consider MPNET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and MiniLM (<span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">all-MiniLM-L6-v2</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. These embeddings have dimensions of 768 and 384 respectively.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para ltx_noindent">
<p id="S4.I1.i2.p1.1" class="ltx_p">From the BAAI family of embedding models, we consider <span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">bge-large-en</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">llm-embedder</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and <span id="S4.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">bge-m3</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> each of 1024 dimensions</p>
</div>
</li>
</ul>
</div>
<div id="S4.p8" class="ltx_para ltx_noindent">
<p id="S4.p8.1" class="ltx_p">Free flowing text (chunked at a sentence level) interspersed with the tables (chunked at row level or table level) are embedded using the models listed above. We use NLTK’s Sentence Tokeniser <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for creating sentences from free flowing text. We note that all the embeddings have unit <math id="S4.p8.1.m1.1" class="ltx_Math" alttext="L_{2}" display="inline"><semantics id="S4.p8.1.m1.1a"><msub id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml"><mi id="S4.p8.1.m1.1.1.2" xref="S4.p8.1.m1.1.1.2.cmml">L</mi><mn id="S4.p8.1.m1.1.1.3" xref="S4.p8.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><apply id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p8.1.m1.1.1.1.cmml" xref="S4.p8.1.m1.1.1">subscript</csymbol><ci id="S4.p8.1.m1.1.1.2.cmml" xref="S4.p8.1.m1.1.1.2">𝐿</ci><cn type="integer" id="S4.p8.1.m1.1.1.3.cmml" xref="S4.p8.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">L_{2}</annotation></semantics></math> norm.</p>
</div>
<div id="S4.p9" class="ltx_para ltx_noindent">
<p id="S4.p9.3" class="ltx_p">For each of the models considered, we compute the cosine similarity between the question embedding and the sentence embeddings of the corpus formed by the free flowing text and tables’ chunks. We retrieve the <math id="S4.p9.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p9.1.m1.1a"><mi id="S4.p9.1.m1.1.1" xref="S4.p9.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p9.1.m1.1b"><ci id="S4.p9.1.m1.1.1.cmml" xref="S4.p9.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.1.m1.1c">k</annotation></semantics></math> chunks (table or sentence) having the highest cosine similarity with the question. If any row, header or caption (corresponding to the correct table) is retrieved in this result, we consider it to be a correct retrieval. Top-<math id="S4.p9.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p9.2.m2.1a"><mi id="S4.p9.2.m2.1.1" xref="S4.p9.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p9.2.m2.1b"><ci id="S4.p9.2.m2.1.1.cmml" xref="S4.p9.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.2.m2.1c">k</annotation></semantics></math> accuracy is the percentage of questions having the correct table association in the <math id="S4.p9.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.p9.3.m3.1a"><mi id="S4.p9.3.m3.1.1" xref="S4.p9.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p9.3.m3.1b"><ci id="S4.p9.3.m3.1.1.cmml" xref="S4.p9.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p9.3.m3.1c">k</annotation></semantics></math> retrieved chunks.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the retriever performance for the 16 table representations described in Section <a href="#S4" title="4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The bars are grouped by the combinations of inclusion/exclusion of header and text (i.e., Repeat Header: No or Yes, Include text: No or Yes). Figures <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> and <a href="#S4.F3.sf2" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a> show the accuracies for pipe-separated representation when the chunking is done at table and row level respectively, while Figures <a href="#S4.F3.sf3" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a> and <a href="#S4.F3.sf4" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(d)</span></a> show the accuracies when space is used as a separator for the respective chunking levels.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">Comparing retrieval accuracies of ‘Include Text: No’ and ‘Include Text: Yes’ groups across all the 4 sub-plots of Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we find that retrieval accuracies reduce with introduction of tables interspersed with text. The ‘Include Text: No’ group can be seen as the best possible result (or empirical upper bound) using only tables as input data. This ia the trend seen across models, and across chunking granularities, and data representation. This is the main insight for <span id="S5.p2.1.1" class="ltx_text ltx_font_bold">RQ1</span>.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">Comparing Figures <a href="#S4.F3.sf1" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a> and <a href="#S4.F3.sf2" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>, we find that chunking at row level gives better performance than chunking at table level. This pattern is also observed between Figures <a href="#S4.F3.sf3" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(c)</span></a> and <a href="#S4.F3.sf4" title="In Figure 3 ‣ 4 Experimental Setup ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(d)</span></a> and across models. This implies that chunking at row level is preferable for improved retrieval accuracies (<span id="S5.p3.1.1" class="ltx_text ltx_font_bold">RQ2</span>).</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p">We also notice that when we have tables interspersed with text, accuracies tagged with ‘Repeat Header: Yes’ are higher than ‘Repeat Header: No’. across most of the models. This establishes that introducing structures in the tables (such as the table header information) can aid in retrieval tasks (<span id="S5.p4.1.1" class="ltx_text ltx_font_bold">RQ3</span>).</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p">In terms of separator, we find that our results do not indicate consistent change in accuracy, though we obtain the highest performance when pipe is used as a separator (<span id="S5.p5.1.1" class="ltx_text ltx_font_bold">RQ4</span>).</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p">For further insights, we consider only results from row level chunks, and tagged ‘Include Text: Yes’, ‘Repeat Header: Yes’. This is because these configurations show improvement in retrieval accuracies.
From Figure <a href="#S5.F4" title="Figure 4 ‣ 5 Results ‣ Evaluation of Table Representations to Answer Questions from Tables in Documents : A Case Study using 3GPP Specifications" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, it can be seen that the drop in performance with inclusion of text is higher for aggregation (A) and multiple row/column retrieval (M) type of questions, highlighting challenges in retrieving relevant portions of text present in multiple chunks. This warrants a more detailed study.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div id="S5.F4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:291.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.5pt,24.3pt) scale(0.857137025771244,0.857137025771244) ;"><img src="/html/2408.17008/assets/icml2024/figs/accs/accs_by_question_type.png" id="S5.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="630" height="470" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance for the best performing representation (row level chunk, repeated header, pipe separator) with and without interspersed text.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">In this study, we chose a 3GPP corpus having a large number of tables interspersed with text. We curate a dataset of various types of question-answers from tables and study retrieval performance of various representations across multiple pre-trained embedding models. We observe that the presence of interspersed text creates a reduction in retrieval performance. More importantly, our experiments lead us to conclude that having the corresponding header information to be repeated in every cell of the table prior to embedding it and choosing a separate embedding for every row is beneficial. This exploitation of the structure of tabular data is critical to obtain best performance even on publicly available embedding models.</p>
</div>
<div id="S6.p2" class="ltx_para ltx_noindent">
<p id="S6.p2.1" class="ltx_p">Our experiments highlight the importance of our work vis-à-vis existing works and datasets that do not consider interspersed text for table retrieval as this is often the case in many practical applications. Although we limit our research to publicly available embeddings, as this is the starting point and often constraints most industrial applications, we expect that the importance of representations would carry through even under domain adaptation techniques.</p>
</div>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p">Our study is limited to a relatively small number of questions because of the fact that a reliable set of useful questions is dependent on SME availability. We wish to expand on our curated dataset over time. This study also can be expanded by following up with domain adaptation techniques - but which may need as a precursor a larger dataset of question answers. Finally, since our approach is generic enough we believe that the observations would hold for other domains too - this is an area of research which can be followed up by us and the community.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2312.10997</span>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Akash Ghosh, Venkata Sahith Bathini, Niloy Ganguly, Pawan Goyal, and Mayank Singh.

</span>
<span class="ltx_bibblock">How robust are the qa models for hybrid scientific tabular data? a study using customized dataset.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</span>, pages 8258–8264, 2024.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Wenting Zhao, Ye Liu, Yao Wan, Yibo Wang, Zhongfen Deng, and S Yu Philip.

</span>
<span class="ltx_bibblock">Localize, retrieve and fuse: A generalized framework for free-form question answering over tables.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings)</span>, pages 1–12, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
3GPP.

</span>
<span class="ltx_bibblock">3GPP release 18.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.3gpp.org/specifications-technologies/releases/release-18" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.3gpp.org/specifications-technologies/releases/release-18</a>, 2022.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-19.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Feifei Pan, Mustafa Canim, Michael Glass, Alfio Gliozzo, and James Hendler.

</span>
<span class="ltx_bibblock">End-to-end table question answering via retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.16714</span>, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Gilbert Badaro, Mohammed Saeed, and Paolo Papotti.

</span>
<span class="ltx_bibblock">Transformers for tabular data representation: A survey of models and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Transactions of the Association for Computational Linguistics</span>, 11:227–249, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jonathan Herzig, Paweł Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisenschlos.

</span>
<span class="ltx_bibblock">TaPas: Weakly supervised table parsing via pre-training.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2004.02349</span>, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou.

</span>
<span class="ltx_bibblock">TAPEX: Table pre-training via learning a neural sql executor.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, et al.

</span>
<span class="ltx_bibblock">Exploring the impact of table-to-text methods on augmenting llm-based question answering with domain hybrid data.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2402.12869</span>, 2024.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Mengkang Hu, Haoyu Dong, Ping Luo, Shi Han, and Dongmei Zhang.

</span>
<span class="ltx_bibblock">Ket-qa: A dataset for knowledge enhanced table question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</span>, pages 9705–9719, 2024.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yujian Liu, Jiabao Ji, Tong Yu, Ryan Rossi, Sungchul Kim, Handong Zhao, Ritwik Sinha, Yang Zhang, and Shiyu Chang.

</span>
<span class="ltx_bibblock">Augment before you try: Knowledge-enhanced table question answering via table expansion.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2401.15555</span>, 2024.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adrià de Gispert, and Gonzalo Iglesias.

</span>
<span class="ltx_bibblock">An inner table retriever for robust table question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span>, pages 9909–9926, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Sohan Patnaik, Heril Changwal, Milan Aggarwal, Sumit Bhatia, Yaman Kumar, and Balaji Krishnamurthy.

</span>
<span class="ltx_bibblock">Cabinet: Content relevance-based noise reduction for table question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">The Twelfth International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Nafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Scigen: a dataset for reasoning-aware text generation from scientific tables.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</span>, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
python-docx Contributors.

</span>
<span class="ltx_bibblock">python-docx documentation.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://python-docx.readthedocs.io/en/latest/index.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://python-docx.readthedocs.io/en/latest/index.html</a>, 2024.

</span>
<span class="ltx_bibblock">Accessed: 2024-05-27.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Ruiqi Li, Xiang Zhao, and Marie-Francine Moens.

</span>
<span class="ltx_bibblock">A brief overview of universal sentence representation methods: A linguistic view.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 55(3):1–42, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kuicai Dong, Aixin Sun, Jung-Jae Kim, and Xiaoli Li.

</span>
<span class="ltx_bibblock">Open information extraction via chunks.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.03299</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Sumit Soman and Sujoy Roychowdhury.

</span>
<span class="ltx_bibblock">Observations on building rag systems for technical documents.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2404.00657</span>, 2024.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych.

</span>
<span class="ltx_bibblock">Sentence-bert: Sentence embeddings using siamese bert-networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span>, pages 3982–3992, 2019.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.

</span>
<span class="ltx_bibblock">Mpnet: Masked and permuted pre-training for language understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 33:16857–16867, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou.

</span>
<span class="ltx_bibblock">Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 33:5776–5788, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.

</span>
<span class="ltx_bibblock">C-pack: Packaged resources to advance general chinese embedding, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie.

</span>
<span class="ltx_bibblock">Retrieve anything to augment large language models, 2023.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.

</span>
<span class="ltx_bibblock">Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Edward Loper and Steven Bird.

</span>
<span class="ltx_bibblock">Nltk: The natural language toolkit.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint cs/0205028</span>, 2002.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.17007" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.17008" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.17008">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.17008" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.17009" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 15:31:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
