<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey of Generative Search and Recommendation in the Era of Large Language Models</title>
<!--Generated on Tue Apr 30 18:30:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Generative Retrieval,  Generative Recommendation,  LLM-based Recommendation,  Model-based IR" lang="en" name="keywords"/>
<base href="/html/2404.16924v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S1" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S2" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Traditional Paradigms</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S2.SS1" title="In 2. Traditional Paradigms ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Machine Learning based Search and Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S2.SS2" title="In 2. Traditional Paradigms ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Deep Learning based Search and Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S3" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Generative Paradigm for Search and Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S3.SS1" title="In 3. Generative Paradigm for Search and Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Scope Clarification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S3.SS2" title="In 3. Generative Paradigm for Search and Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Unified Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Generative Search</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS1" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS2" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Query Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS3" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Document Identifiers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS4" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS5" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS6" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Summary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS7" title="In 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>LLMs for Retrieval beyond Generative Search</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Generative Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS1" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS2" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>User Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS3" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Item Identifiers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS4" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS5" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS6" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Summary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS7" title="In 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>LLMs for Recommendation beyond Generative Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S6" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S6.SS1" title="In 6. Discussion ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Difference Between Generative Search and Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S6.SS2" title="In 6. Discussion ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Open Problems in Generative Search and Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S6.SS3" title="In 6. Discussion ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Envision Next Information-seeking Paradigm: Content Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S7" title="In A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">A Survey of Generative Search and Recommendation in the Era of Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yongqi Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:liyongqi0@gmail.com">liyongqi0@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">The Hong Kong Polytechnic University</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinyu Lin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xylin1028@gmail.com">xylin1028@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">National University of Singapore</span><span class="ltx_text ltx_affiliation_streetaddress" id="id6.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id7.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenjie Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:wenjiewang96@gmail.com">wenjiewang96@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">National University of Singapore</span><span class="ltx_text ltx_affiliation_streetaddress" id="id10.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id11.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fuli Feng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:fulifeng93@gmail.com">fulifeng93@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_streetaddress" id="id14.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id15.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liang Pang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:pangliang@ict.ac.cn">pangliang@ict.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Chinese Academy of Sciences</span><span class="ltx_text ltx_affiliation_streetaddress" id="id18.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id19.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wenjie Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:cswjli@comp.polyu.edu.hk">cswjli@comp.polyu.edu.hk</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">The Hong Kong Polytechnic University</span><span class="ltx_text ltx_affiliation_streetaddress" id="id22.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id23.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id24.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liqiang Nie
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:nieliqiang@gmail.com">nieliqiang@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">Harbin Institute of Technology (Shenzhen)</span><span class="ltx_text ltx_affiliation_streetaddress" id="id26.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id27.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id28.4.id4"></span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiangnan He
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:xiangnanhe@gmail.com">xiangnanhe@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id29.1.id1">University of Science and Technology of China</span><span class="ltx_text ltx_affiliation_streetaddress" id="id30.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id31.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id32.4.id4"></span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tat-Seng Chua
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:dcscts@nus.edu.sg">dcscts@nus.edu.sg</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id33.1.id1">National University of Singapore</span><span class="ltx_text ltx_affiliation_streetaddress" id="id34.2.id2"></span><span class="ltx_text ltx_affiliation_city" id="id35.3.id3"></span><span class="ltx_text ltx_affiliation_country" id="id36.4.id4"></span>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id37.id1">With the information explosion on the Web, search and recommendation are foundational infrastructures to satisfying users’ information needs. As the two sides of the same coin, both revolve around the same core research problem, matching queries with documents or users with items. In the recent few decades, search and recommendation have experienced synchronous technological paradigm shifts, including machine learning-based and deep learning-based paradigms. Recently, the superintelligent generative large language models have sparked a new paradigm in search and recommendation, <span class="ltx_text ltx_font_italic" id="id37.id1.1">i.e.</span>, generative search (retrieval) and recommendation, which aims to address the matching problem in a generative manner. In this paper, we provide a comprehensive survey of the emerging paradigm in information systems and summarize the developments in generative search and recommendation from a unified perspective. Rather than simply categorizing existing works, we abstract a unified framework for the generative paradigm and break down the existing works into different stages within this framework to highlight the strengths and weaknesses. And then, we distinguish generative search and recommendation with their unique challenges, identify open problems and future directions, and envision the next information-seeking paradigm.</p>
</div>
<div class="ltx_keywords">Generative Retrieval, Generative Recommendation, LLM-based Recommendation, Model-based IR
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2024; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Information retrieval</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">With the information explosion on the Web, a fundamental problem in information science gains increasing significance: sifting relevant information from vast pools to meet user needs.
Nowadays, two information access modes — search and recommendation — serve as foundational infrastructures for information delivery.
The objective of search is to retrieve a list of documents (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">e.g., </em>Web documents, Twitter posts, and answers) given the user’s explicit query <cite class="ltx_cite ltx_citemacro_citep">(Kobayashi and Takeda, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib58" title="">2000</a>)</cite>. By contrast, recommendation systems aim to recommend the items (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">e.g., </em>E-commerce products, micro-videos, and news) by implicitly inferring user interest from the user’s profile and historical interactions <cite class="ltx_cite ltx_citemacro_citep">(Adomavicius and Tuzhilin, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib2" title="">2005</a>)</cite>.
Currently, search and recommendation systems are extensively employed across diverse scenarios and domains, including E-commerce, social media, healthcare, and education.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Search and recommendation are the two sides of the same coin <cite class="ltx_cite ltx_citemacro_citep">(Belkin and Croft, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib7" title="">1992</a>)</cite>.
Search is users’ active information retrieval with explicit queries while recommendation is passive information filtering for users.
Despite the distinct objectives, search and recommendation can be unified as the “matching” problem from a technical perspective <cite class="ltx_cite ltx_citemacro_citep">(Garcia-Molina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib33" title="">2011</a>)</cite>.
Search can be formulated as a matching between queries and documents, and recommendation can be considered a matching between users and items.
The common matching property has spurred synchronous technological paradigm shifts in both search and recommendation systems. Specifically,
</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Machine learning-based search and recommendation.</span>
Related works, generalized “learning to match” <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib157" title="">2020</a>)</cite>, learn a matching function using machine learning techniques (<em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.1.2">e.g., </em>learning to ranks <cite class="ltx_cite ltx_citemacro_citep">(Li, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib66" title="">2022</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib90" title="">2009</a>)</cite> and Matrix Factorization <cite class="ltx_cite ltx_citemacro_citep">(Lee and Seung, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib62" title="">2000</a>)</cite>) to estimate the relevance scores on query-document pairs or user-item pairs.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Deep learning-based search and recommendation.</span> With the significant advancements in various neural networks such as CNN <cite class="ltx_cite ltx_citemacro_citep">(LeCun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib60" title="">1998</a>)</cite>, RNN <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib42" title="">1997</a>)</cite>, GNN <cite class="ltx_cite ltx_citemacro_citep">(Scarselli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib119" title="">2008</a>)</cite>, and Transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib139" title="">2017</a>)</cite>, search and recommendation have transitioned into the deep learning-based paradigm. This paradigm leverages the powerful representation ability of deep learning-based methods to encode inputs (<em class="ltx_emph ltx_font_italic" id="S1.I1.i2.p1.1.2">i.e., </em>queries, documents, users, and items) into dense vectors in a latent space <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib56" title="">2020</a>)</cite> and learn the non-linear matching functions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Generative search and recommendation.</span>
With the recent surge of generative large language models (LLMs), a new paradigm emerges in search and recommendation: generative search (retrieval)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Generative retrieval is another frequently used term.</span></span></span> and recommendation. Distinguished from previous discriminative matching paradigms, generative search and recommendation aim to directly generate the target document or item to satisfy users’ information needs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Embracing generative search and recommendation brings new benefits and opportunities for the field of search and recommendation.
In particular,
1) LLMs inherently possess formidable capabilities, such as vast knowledge, semantic understanding, interactive skills, and instruction following. These inherent abilities can be transferred or directly applied to search and recommendation, thereby enhancing information retrieval tasks.
2) The tremendous success of LLMs stems from their generative learning. A profound consideration to apply generative learning to search and recommendation, fundamentally revolutionizing the methods of information retrieval rather than only utilization of LLMs.
3) LLMs-based generative AI applications, such as ChatGPT, are gradually becoming a new gateway for users to access Web content. Developing generative search and recommendation could be better integrated into these generative AI applications.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this survey, we aim to provide a comprehensive review of generative search and recommendation from a technical perspective.
We will not simply list and classify the current works, but instead abstract a unified framework for generative search and recommendation. Within the unified framework, we highlight the objectives of each stage and categorize existing works into different stages from a technical viewpoint. From the unified perspective, we could better highlight their commonalities and present their developments.
In addition, we will delve into deeper discussions, including contrasting generative search and recommendation, identifying open problems of the generative paradigm, and envisioning future information-seeking paradigms.
</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">There are several key points that set our survey apart from existing ones. First and most important, we are the first to summarize and analyze the developments in generation search. Second, compared with existing generation recommendation surveys <cite class="ltx_cite ltx_citemacro_citep">(Deldjoo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib26" title="">2024</a>; Vats et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib140" title="">2024</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib153" title="">2023</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib82" title="">2023a</a>)</cite>, we focus on the fundamental generative paradigms for recommendation rather than incorporating LLM-enhanced discriminative methods. Besides, we examine existing works from a technical perspective, using an abstract framework to categorize generation recommendation works into different stages. Third, we highlight the close relationship between generative search and generation recommendation, where many works in the two areas mutually inform and promote each other. By taking a unified view and comparing the respective developments, we are able to identify potential research directions.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The survey is structured as follows: Chapter 2 summarizes the previous paradigms in search and recommendation. Chapter 3 provides an overview of the generative paradigm for search and recommendation. In Chapters 4 and 5, we delve into the specific details of the generative paradigm in search and recommendation, respectively. Chapter 6 focuses on deep discussions, including the comparison of generative search and recommendation, open problems, and the next information-seeking paradigm. Finally, in Chapter 7, we give a conclusion of this survey.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Traditional Paradigms</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Machine Learning based Search and Recommendation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.2.2"><mo id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">∙</mo><mspace id="S2.SS1.p1.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">Machine learning-based search</span>.
In the era of machine learning, the core problem for search is to learn an effective function to predict the relevance scores between queries and documents. A series of classical works, such as
Regularized Matching in Latent Space (RMLS) <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib154" title="">2013</a>)</cite> and
Supervised Semantic Indexing (SSI) <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib3" title="">2009</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib4" title="">2010</a>)</cite>, explored mapping functions to transform features from queries and documents into a “latent space”. The series of “learning to rank” algorithms <cite class="ltx_cite ltx_citemacro_citep">(Li, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib66" title="">2022</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib90" title="">2009</a>)</cite> were also proposed to develop effective rank losses for machine learning based search methods: the pointwise approaches <cite class="ltx_cite ltx_citemacro_citep">(Chu and Keerthi, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib20" title="">2005</a>; Shashua and Levin, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib120" title="">2002</a>; Nallapati, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib103" title="">2004</a>)</cite> transform ranking into regression or classification on single documents; the pairwise approaches <cite class="ltx_cite ltx_citemacro_citep">(Burges et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib11" title="">2005</a>; Tsai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib136" title="">2007</a>; Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib108" title="">2007</a>)</cite> regard the ranking into classification on the pairs of documents; the listwise approaches <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib13" title="">2007</a>; Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib107" title="">2008</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib147" title="">2018</a>; Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib156" title="">2008</a>; Freund et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib31" title="">2003</a>)</cite> aim to model the ranking problem in a straightforward
fashion and overcome the drawbacks of the aforementioned two approaches by tackling the ranking problem directly.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.2.2"><mo id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">∙</mo><mspace id="S2.SS1.p2.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Machine learning-based recommendation</span>.
In the context of recommender systems, the user-item matching typically leverages Collaborative Filtering (CF), which assumes users with similar interactions (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">e.g., </em>ratings or clicks) share similar preferences on items <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib40" title="">2017</a>)</cite>.
To achieve CF, early effort has been made to develop memory-based methods, which predict the user interactions by memorizing similar user’s or item’s ratings <cite class="ltx_cite ltx_citemacro_citep">(Breese et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib10" title="">1998</a>; Herlocker et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib41" title="">1999</a>; Linden et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib85" title="">2003</a>; Sarwar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib118" title="">2001</a>)</cite>.
Later on, popularized by the Netflix Prize, Matrix Factorization (MF) <cite class="ltx_cite ltx_citemacro_citep">(Rendle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib117" title="">2009</a>)</cite> has emerged as one of the most representative CF approaches.
MF decomposes user-item interactions into latent factors for users and items in a latent space. It then predicts user-item interactions by matching these latent factors through inner product computations.
Following MF, other notable methods have been proposed that also perform matching in the latent space, such as BMF <cite class="ltx_cite ltx_citemacro_citep">(Koren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib59" title="">2009</a>)</cite> and FISM <cite class="ltx_cite ltx_citemacro_citep">(Kabbur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib54" title="">2013</a>)</cite>.
In addition to the CF-based methods, an orthogonal line of research focuses on content-based techniques, aiming to encode user/item features for user-item matching.
Factorization Machine <cite class="ltx_cite ltx_citemacro_citep">(Rendle, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib116" title="">2010</a>)</cite> is a prominent content-based method that represents user and item features as latent factors and models their high-order interactions to match between users and items.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Deep Learning based Search and Recommendation</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.2.2"><mo id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">∙</mo><mspace id="S2.SS2.p1.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">Deep learning-based Search</span>.
Deep learning-based search mainly relies on various neural architectures to represent queries and documents effectively. Feedforward neural networks are the first used to create semantic representations of queries and documents. Deep Structured Semantic Models were proposed to represent queries and documents with deep neural networks <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib50" title="">2013</a>)</cite>. The popular Convolutional Neural Networks are also explored for the application in capturing semantic embeddings <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib122" title="">2014</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib45" title="">2014</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib49" title="">2017</a>; Socher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib124" title="">2013</a>)</cite>. Given the fact that both queries and documents are sequential texts, it is natural
to apply Recurrent Neural Networks to represent the queries and documents <cite class="ltx_cite ltx_citemacro_citep">(Palangi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib104" title="">2016</a>)</cite>. Later, with the rise of the Transformer architecture <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib139" title="">2017</a>)</cite> and pretrained BERT model <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib27" title="">2018</a>)</cite>, the Bert-based dense retrievers show advanced performance in large-scale scenarios <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib56" title="">2020</a>; Qu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib111" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mrow id="S2.SS2.p2.1.m1.1.2.2"><mo id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">∙</mo><mspace id="S2.SS2.p2.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">Deep learning-based recommendation</span>.
As deep neural networks have demonstrated exceptional learning capabilities across various domains, there emerges a trend in leveraging deep learning methodologies to tackle complex interaction patterns for user-item matching in recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib55" title="">2018</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib18" title="">2019</a>)</cite>.
Deep learning-based user-item matching can be broadly classified into two research directions.
One research line focuses on matching function learning, which utilizes deep learning techniques to learn intricate user-item matching function <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib171" title="">2017</a>)</cite>.
Notably, Neural Collaborative Filtering (NCF) <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib40" title="">2017</a>)</cite> leverages Multi-Layer Perceptrons (MLP) to achieve expressive and complicated matching functions.
This approach effectively models noisy implicit feedback data and enhances the recommendation performance.
Another line of work lies in representation learning <cite class="ltx_cite ltx_citemacro_citep">(Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib159" title="">2017</a>)</cite>, which harnesses neural networks to transform user and item features into latent space conducive to matching.
For instance, Bert4Rec <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib125" title="">2019</a>)</cite> employs deep bidirectional self-attention mechanisms to transform user’s historical sequences into latent space for sequential recommendation tasks.
Caser <cite class="ltx_cite ltx_citemacro_citep">(Tang and Wang, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib131" title="">2018</a>)</cite> proposes a convolutional sequence model, which leverages both horizontal and vertical convolutional filters to identify complex historical interaction sequences.
Subsequently, motivated by the graphical structure inherent in user-item interactions, researchers have explored the application of graph neural networks for recommendation tasks.
This research direction has enabled the utilization of high-order neighbor information to enhance the representation of users and items for the user-item matching, as exemplified by NGCF <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib146" title="">2019</a>)</cite> and LightGCN <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib39" title="">2020</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S2.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Comparison of the three paradigms for search and recommendation, <em class="ltx_emph ltx_font_italic" id="S2.F1.2.1">i.e., </em>machine learning-based, deep learning-based, and generative search and recommendation. “rec.” denotes “recommendation”.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Generative Paradigm for Search and Recommendation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first clarify the scope of generative search and recommendation, including comparison with previous paradigms and comparison with other LLM-based methods. And then we abstract a unified framework for generative search and recommendation.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Scope Clarification</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Comparison with previous paradigms</span>. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S2.F1" title="Figure 1 ‣ 2.2. Deep Learning based Search and Recommendation ‣ 2. Traditional Paradigms ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, we present a summary of the three distinct paradigms in search and recommendation. Each paradigm employs different techniques to achieve the same goal of providing relevant documents/items for a given query/user. The machine learning and deep learning paradigms approach the search/recommendation task as a discriminative problem, focusing on calculating similarities between queries/users and documents/items. In contrast, the generative paradigm formulates the task as a generation problem, aiming to directly generate the documents/items based on the queries/users.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We need to address two specific issues for the three paradigms. 1) Deep learning is a subset of machine learning, and neural generative models are a subset of deep learning. While there are sub-concepts within these paradigms, they have clear boundaries and distinct development directions. For instance, with the emergence of deep learning, new methods have been developed to apply neural networks for feature extraction in search and recommendation systems, despite deep learning being a part of machine learning. Similarly, generative models have gained popularity, with numerous works focusing on generative search and recommendation. 2) We use the term “paradigm shift” to indicate potential community interest rather than actual trends. Both generative search and recommendation are relatively new approaches in the research community, and their effectiveness has not been fully verified historically.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Comparison with LLM-based discriminative methods</span>. In this study, we define generative search and generative recommendation as methods that fully accomplish search and recommendation tasks using generative models. This is a specific definition that may exclude certain works. For example, some approaches <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib98" title="">2023</a>; Muennighoff et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib101" title="">2024</a>)</cite> may use generative language models to extract features from queries/users and documents/items. While these works also utilize generative language models, the overall paradigm is not significantly different from previous methods, as they simply replace the original encoder with a generative language model. In our study, we focus on presenting works that completely utilize generative paradigms to accomplish search or recommendation tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Unified Framework</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Benefiting from the simplicity of the generative paradigm, we could summarize the generative search and generative recommendation into a unified framework.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">To accomplish search and recommendation tasks in a generative manner, there are four essential steps:
1) <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Query/User Formulation</span>: This step aims to determine the input of the generative model. For search, complex query formulation is not necessary; for recommendation, user formulation is vital to transform user information into textual sequences.
2) <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.2">Document/Item Identifiers</span>: In practice, directly generating the document or item is almost impossible. Therefore, a short text sequence, known as the identifier, is used to represent the document or item.
3) <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.3">Training</span>: Once the input (query/user formulation) and output (document/item identifiers) of the generative model are determined, training is easily achieved via the generation loss.
4) <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.4">Inference</span>: After the training, the generative model can receive the query/user to predict the document/item identifier, and the document/item identifier can correspond to the document or item.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">While the entire process may seem simple, achieving effective generative search and recommendation is not trivial. Numerous details need to be considered and balanced within the four steps mentioned above. In Sections 4 and 5, we will summarize generative search and generative recommendation methods, emphasizing their focus on specific aspects within the framework.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Generative Search</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Overview</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Generative search endeavors to leverage generative models, specifically generative language models, to complete the conventional search and retrieval process. The target is still the matching between documents and a given query. Different from previous paradigms, generative search aims to directly generate the desired target document when presented with a query.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Query Formulation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In search, users typically express their information needs through textual queries. This is in contrast to generative recommendation, which involves a “user formulation” step to convert user history into a textual sequence. In most retrieval tasks, the textual query can be directly inputted into the generative language model, sometimes with simple prefixes like “query: ”. However, in certain retrieval tasks such as conversational QA and multi-hop QA, the query must be combined with the conversation context <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib75" title="">2023a</a>)</cite> or previous-hop answers <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib63" title="">2022b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Document Identifiers</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Ideally, generative search aims to directly generate the complete target document in response to a given query. However, in practice, this task proves to be extremely challenging for LLMs due to the length and inclusion of irrelevant information in the document’s content. Consequently, current generative search approaches often rely on the use of identifiers to represent documents. These identifiers are concise strings that effectively capture the essence of a document’s content. We summarize the current identifiers in generative search methods, and analyze their advantages and disadvantages as follows.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Numeric ID</span> <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib134" title="">2022</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib148" title="">2022</a>; Zhuang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib180" title="">2022</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib178" title="">2022a</a>; Nadeem et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib102" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib71" title="">2024b</a>)</cite>. Each document in the corpus can be assigned a unique numeric ID, such as “12138”. During the inference stage, the LLM can receive the query as input and generate either a single numeric ID or a list of numeric IDs using beam search. Since each document corresponds to a numeric ID, the predicted numeric ID can represent a retrieved document. However, it is important to note that the numeric ID itself does not have any semantic relation to the content of the document. There are several methods to establish correlations between semantic text and numeric IDs. 1) Document-to-numeric ID training. During the training phase, an LLM can be trained to take a document’s content as input and generate the corresponding numeric ID as the target. This training approach compels the LLM to memorize the relationships between documents and their numeric IDs. Once trained effectively, the LLM is expected to recall the numeric IDs of the documents accurately. 2) Clustered numeric IDs. In  <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib134" title="">2022</a>)</cite>, the authors explored the concept of clustering document embeddings and assigning numeric IDs based on the cluster results. This approach allows similar documents with comparable content to be grouped into the same clusters, resulting in similar numeric IDs for these documents. Unlike randomly assigned numeric IDs, the clustered IDs are determined based on the content of the documents and are consistent with their semantics.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Numeric IDs pose challenges in generative search for the following reasons. 1) Generalization. The inability to generalize is a significant issue for numeric IDs. Previous studies <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib134" title="">2022</a>)</cite> have shown that the Language Model can effectively memorize numeric IDs of passages in the training set. However, when it comes to the test set, the LLM’s performance deteriorates. This can be attributed to the fact that numeric IDs lack semantic meaning, making it difficult for the model to generalize to unseen data. To address this problem, NCI <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib148" title="">2022</a>)</cite> proposed the inclusion of pseudo-queries on test set passages, which helps alleviate the issue. 2) Corpus update. Updating poses a challenge for generative search methods based on numeric IDs. Unlike dense retrieval methods that can easily update passages by modifying the embedding vectors in the corpus, the LLM struggles with updating passages. This is because the LLM stores the passages in its parameters, and accurately editing these parameters is not feasible. In <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib99" title="">2022</a>)</cite>, the authors introduced incremental learning to solve the passage-adding problem partially. 3) Large-scale corpus. Since generative search models must memorize the associations between documents and their numeric IDs, the memorization difficulty increases as the document corpus sizes increase. The work <cite class="ltx_cite ltx_citemacro_citep">(Pradeep et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib106" title="">2023</a>)</cite> explored to scaling up numeric ID based generative search. It is found that while generative search is competitive with state-of-the-art dual encoders on small corpora, scaling to millions of passages remains an important and unsolved challenge.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Identifiers in generative search. </figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.15">
<tr class="ltx_tr" id="S4.T1.15.16">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.15.16.1">Type</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.15.16.2">Semantic</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.15.16.3">Distinctiveness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.15.16.4">Update</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.15.16.5">Training</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.15.16.6">Applicable retrieval scenarios</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.3.3.4">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><times id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mi id="S4.T1.2.2.2.m1.1.1" mathvariant="normal" xref="S4.T1.2.2.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.3"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><times id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.5">
<span class="ltx_text" id="S4.T1.3.3.5.1"></span> <span class="ltx_text" id="S4.T1.3.3.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.3.3.5.2.1">
<span class="ltx_tr" id="S4.T1.3.3.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.3.5.2.1.1.1">document-to-identifier</span></span>
<span class="ltx_tr" id="S4.T1.3.3.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.3.5.2.1.2.1">query-to-identifier</span></span>
</span></span><span class="ltx_text" id="S4.T1.3.3.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.6">Small-scale corpus</td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.6.6.4">Titles</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.4.4.1.m1.1"><semantics id="S4.T1.4.4.1.m1.1a"><mi id="S4.T1.4.4.1.m1.1.1" mathvariant="normal" xref="S4.T1.4.4.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.1.m1.1b"><ci id="S4.T1.4.4.1.m1.1.1.cmml" xref="S4.T1.4.4.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.5.5.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.5.5.2.m1.1"><semantics id="S4.T1.5.5.2.m1.1a"><mo id="S4.T1.5.5.2.m1.1.1" xref="S4.T1.5.5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.2.m1.1b"><times id="S4.T1.5.5.2.m1.1.1.cmml" xref="S4.T1.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.6.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.6.6.3.m1.1"><semantics id="S4.T1.6.6.3.m1.1a"><mi id="S4.T1.6.6.3.m1.1.1" mathvariant="normal" xref="S4.T1.6.6.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.3.m1.1b"><ci id="S4.T1.6.6.3.m1.1.1.cmml" xref="S4.T1.6.6.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.6.5">query-to-identifier</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.6.6">Document-level retrieval</td>
</tr>
<tr class="ltx_tr" id="S4.T1.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.9.9.4">N-grams</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.7.7.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.7.7.1.m1.1"><semantics id="S4.T1.7.7.1.m1.1a"><mi id="S4.T1.7.7.1.m1.1.1" mathvariant="normal" xref="S4.T1.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.1.m1.1b"><ci id="S4.T1.7.7.1.m1.1.1.cmml" xref="S4.T1.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.8.8.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.8.8.2.m1.1"><semantics id="S4.T1.8.8.2.m1.1a"><mo id="S4.T1.8.8.2.m1.1.1" xref="S4.T1.8.8.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.2.m1.1b"><times id="S4.T1.8.8.2.m1.1.1.cmml" xref="S4.T1.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.9.9.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.9.9.3.m1.1"><semantics id="S4.T1.9.9.3.m1.1a"><mi id="S4.T1.9.9.3.m1.1.1" mathvariant="normal" xref="S4.T1.9.9.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.9.9.3.m1.1b"><ci id="S4.T1.9.9.3.m1.1.1.cmml" xref="S4.T1.9.9.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.9.9.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.9.9.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.9.9.5">query-to-identifier</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.9.9.6">Document-level &amp; Passage-level retrieval</td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.12.12.4">Codebook</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.10.10.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.10.10.1.m1.1"><semantics id="S4.T1.10.10.1.m1.1a"><mi id="S4.T1.10.10.1.m1.1.1" mathvariant="normal" xref="S4.T1.10.10.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.10.10.1.m1.1b"><ci id="S4.T1.10.10.1.m1.1.1.cmml" xref="S4.T1.10.10.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.10.10.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.11.11.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.11.11.2.m1.1"><semantics id="S4.T1.11.11.2.m1.1a"><mi id="S4.T1.11.11.2.m1.1.1" mathvariant="normal" xref="S4.T1.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.11.11.2.m1.1b"><ci id="S4.T1.11.11.2.m1.1.1.cmml" xref="S4.T1.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.11.11.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.11.11.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.12.12.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.12.12.3.m1.1"><semantics id="S4.T1.12.12.3.m1.1a"><mi id="S4.T1.12.12.3.m1.1.1" mathvariant="normal" xref="S4.T1.12.12.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.12.12.3.m1.1b"><ci id="S4.T1.12.12.3.m1.1.1.cmml" xref="S4.T1.12.12.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.12.12.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.12.12.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.12.12.5">
<span class="ltx_text" id="S4.T1.12.12.5.1"></span> <span class="ltx_text" id="S4.T1.12.12.5.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.12.12.5.2.1">
<span class="ltx_tr" id="S4.T1.12.12.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.12.12.5.2.1.1.1">codebook training</span></span>
<span class="ltx_tr" id="S4.T1.12.12.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.12.12.5.2.1.2.1">query-to-identifier</span></span>
</span></span><span class="ltx_text" id="S4.T1.12.12.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.12.12.6">Document-level &amp; Passage-level retrieval</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.15">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.15.15.4">Multiview</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.13.13.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.13.13.1.m1.1"><semantics id="S4.T1.13.13.1.m1.1a"><mi id="S4.T1.13.13.1.m1.1.1" mathvariant="normal" xref="S4.T1.13.13.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.13.13.1.m1.1b"><ci id="S4.T1.13.13.1.m1.1.1.cmml" xref="S4.T1.13.13.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.13.13.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.13.13.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.14.14.2"><math alttext="\times" class="ltx_Math" display="inline" id="S4.T1.14.14.2.m1.1"><semantics id="S4.T1.14.14.2.m1.1a"><mo id="S4.T1.14.14.2.m1.1.1" xref="S4.T1.14.14.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T1.14.14.2.m1.1b"><times id="S4.T1.14.14.2.m1.1.1.cmml" xref="S4.T1.14.14.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.14.14.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.T1.14.14.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.15.15.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S4.T1.15.15.3.m1.1"><semantics id="S4.T1.15.15.3.m1.1a"><mi id="S4.T1.15.15.3.m1.1.1" mathvariant="normal" xref="S4.T1.15.15.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S4.T1.15.15.3.m1.1b"><ci id="S4.T1.15.15.3.m1.1.1.cmml" xref="S4.T1.15.15.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.15.15.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S4.T1.15.15.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.15.15.5">query-to-identifier</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.15.15.6">Document-level &amp; Passage-level retrieval</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Document titles</span> <cite class="ltx_cite ltx_citemacro_citep">(De Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib25" title="">2020</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib75" title="">2023a</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib63" title="">2022b</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib16" title="">2022a</a>)</cite>. In certain specific scenarios, documents possess titles that can serve as effective identifiers. For instance, in Wikipedia, each page is assigned a unique title that succinctly summarizes its content. These titles are semantically linked to the documents and establish a one-to-one correspondence, making them ideal identifiers. In 2021, Cao et al. investigated the utilization of titles as identifiers in entity retrieval and document retrieval <cite class="ltx_cite ltx_citemacro_citep">(De Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib25" title="">2020</a>)</cite>. Besides, there are some title-like identifiers, including URLs <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib114" title="">2023b</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib179" title="">2022b</a>; Ziems et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib181" title="">2023</a>)</cite>, keywords <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib64" title="">2023</a>)</cite>, and summaries <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib177" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">However, document titles are only effective in certain retrieval domains and prove ineffective in the following aspects. 1) Firstly, in passage-level retrieval, documents are often divided into smaller retrieval units known as passages. It is hard to design effective passage identifiers even based on the document titles, which makes title-based generative search perform badly in passage retrieval. In 2023, Li et al. employed document titles plus section titles as passage identifiers, but this approach was limited to the Wikipedia corpus <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib75" title="">2023a</a>)</cite>. 2) Secondly, web pages differ from Wikipedia in web retrieval as they lack high-quality titles. These titles may not accurately represent the content of the document, and multiple web pages may share the same title. 3) Additionally, numerous pages do not have any titles at all. These factors contribute to generative search lagging behind traditional retrieval methods.</p>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">N-grams</span> <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib8" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib170" title="">2023c</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib15" title="">2023b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib151" title="">2023e</a>)</cite>. The document itself is semantic but cannot serve as a reliable identifier. This is because not all of its content is necessarily related to the user query, making it difficult for the LLM to generate irrelevant content based solely on the user query. However, inspired by this limitation, the N-grams within the document that are semantically related to the user query could be regarded as potential identifiers. In 2022, the authors in  <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib8" title="">2022</a>)</cite> trained an LLM to generate target N-grams using a user query as input. These N-grams were selected based on the word overlap between the user query and the N-grams. Once trained, the LLM was able to predict relevant N-grams given a query. These predicted N-grams were then transformed into a passage rank list using a heuristic function. The proposed method was evaluated on commonly used datasets, including NQ and TriviaQA, rather than some specifically designed subsets of datasets.</p>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1">However, N-gram identifiers have certain limitations. 1). Firstly, they are not as discriminative as numeric IDs. Unlike numeric IDs, N-grams cannot directly correspond to specific documents on a one-to-one basis. This means that the LLM must rely on a transformation function to convert the predicted N-grams into a document ranking list. In SEAL <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib8" title="">2022</a>)</cite>, for example, the transformation function calculates a document’s score by summarizing the scores of the N-grams it contains. In a way, this transformation function acts as a simple retrieval approach, preventing N-gram-based generative search from achieving end-to-end document retrieval. 2) Secondly, the selection of N-grams in the training phase is a crucial aspect. A document may contain numerous N-grams, and it is necessary to select those that are semantically relevant to the query. However, the number of N-grams to be selected and the method used for selection are adjustable and can vary largely.</p>
</div>
<div class="ltx_para" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p8.1.1">Codebook</span> <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib126" title="">2023b</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib165" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib160" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib167" title="">2024</a>)</cite>. The text codebook, also known as tokens, plays a fundamental role in LLMs. Text is inherently discrete, allowing LLMs to acquire knowledge through tasks like predicting the next token. Some studies  <cite class="ltx_cite ltx_citemacro_citep">(Van Den Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib138" title="">2017</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib61" title="">2022a</a>)</cite> have also focused on developing visual codebooks for discrete images. As mentioned earlier, while a document can be represented as a list of tokens, it is not suitable as an identifier due to its lengthiness. Therefore, an alternative approach is to learn a new codebook specifically for documents, which can represent them more efficiently than natural text tokens. In 2023, Sun et al. proposed a method to learn a codebook for documents in generative search <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib126" title="">2023b</a>)</cite>. The work <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib160" title="">2023</a>)</cite> proposed an end-to-end framework to automatically search best identifiers according to the document’s content.</p>
</div>
<div class="ltx_para" id="S4.SS3.p9">
<p class="ltx_p" id="S4.SS3.p9.1">However, learning a codebook for documents is a complex process. It typically involves encoding documents into dense vectors using an encoder network, discretizing these vectors, and then using a decoder network to reconstruct the original document. The size of the codebook and the length of the sequence required to represent a document must be carefully adjusted. Additionally, compared to titles and n-grams, the codebook lacks interpretability.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Summary of the representative generative search methods.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.1.1">Method</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.2">Identifier</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3">
<span class="ltx_text" id="S4.T2.1.1.3.1"></span> <span class="ltx_text" id="S4.T2.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.3.2.1">
<span class="ltx_tr" id="S4.T2.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.3.2.1.1.1">Backbone</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.1.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4">
<span class="ltx_text" id="S4.T2.1.1.4.1"></span> <span class="ltx_text" id="S4.T2.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.4.2.1">
<span class="ltx_tr" id="S4.T2.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.4.2.1.1.1">Constrained</span></span>
<span class="ltx_tr" id="S4.T2.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.4.2.1.2.1">generation</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.1.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5">Datasets</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.2.1">GENRE <cite class="ltx_cite ltx_citemacro_citep">(De Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib25" title="">2020</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.2">Titles</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.5">KILT</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.3.1">DSI <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib134" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.3">
<span class="ltx_text" id="S4.T2.1.3.3.1"></span> <span class="ltx_text" id="S4.T2.1.3.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.3.3.2.1">
<span class="ltx_tr" id="S4.T2.1.3.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.3.3.2.1.1.1">T5-base, T5-large,</span></span>
<span class="ltx_tr" id="S4.T2.1.3.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.3.3.2.1.2.1">T5-XL, T5-XXL</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.3.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.3.5.1">NQ10K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.3.5.2">NQ100K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.3.5.3">NQ320K</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.4.1">SEAL <cite class="ltx_cite ltx_citemacro_citep">(Bevilacqua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib8" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.2">N-grams</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.4">FM-index</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.5">KILT, <span class="ltx_text ltx_font_italic" id="S4.T2.1.4.5.1">NQ10K</span>, NQ</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.5.1">NCI <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib148" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.2">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.3">
<span class="ltx_text" id="S4.T2.1.5.3.1"></span> <span class="ltx_text" id="S4.T2.1.5.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.5.3.2.1">
<span class="ltx_tr" id="S4.T2.1.5.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.5.3.2.1.1.1">T5-small, T5-base,</span></span>
<span class="ltx_tr" id="S4.T2.1.5.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.5.3.2.1.2.1">T5-large</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.5.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.5.5.1">NQ320K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.5.5.2">Trivia QA (subset)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.6.1">MINDER <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib77" title="">2023c</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.6.2">Multiview</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.6.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.6.4">FM-index</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.6.5">NQ, TriviaQA, MSMARCO</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.7.1">GENRET <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib126" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.7.2">Codebook</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.7.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.7.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.7.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.7.5.1">NQ320K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.7.5.2">MSMARCO</span>, BEIR</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.8.1">TOME <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib114" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.8.2">
<span class="ltx_text" id="S4.T2.1.8.2.1"></span> <span class="ltx_text" id="S4.T2.1.8.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.8.2.2.1">
<span class="ltx_tr" id="S4.T2.1.8.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.8.2.2.1.1.1">Title-like (URLs)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.8.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.8.3">T5-large, T5-XL</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.8.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.8.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.8.5.1">NQ320K</span>, MSMARCO</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.9.1">LTRGR <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib76" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.9.2">Multiview</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.9.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.9.4">FM-index</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.9.5">NQ, TriviaQA, MSMARCO</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.10.1">DGR <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib78" title="">2024e</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.10.2">Multiview</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.10.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.10.4">FM-index</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.10.5">NQ, TriviaQA, MSMARCO, TREC DL</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.11.1">DSI(scaling up) <cite class="ltx_cite ltx_citemacro_citep">(Pradeep et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib106" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.11.2">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.11.3">
<span class="ltx_text" id="S4.T2.1.11.3.1"></span> <span class="ltx_text" id="S4.T2.1.11.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.11.3.2.1">
<span class="ltx_tr" id="S4.T2.1.11.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.11.3.2.1.1.1">T5-base, T5-large,</span></span>
<span class="ltx_tr" id="S4.T2.1.11.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.11.3.2.1.2.1">T5-XL, T5-XXL</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.11.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.11.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.11.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.11.5.1">NQ320K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.11.5.2">Trivia QA (subset)</span>, MSMARCO</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.12.1">DSI-QG <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib180" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.12.2">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.12.3">T5-base, T5-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.12.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.12.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.12.5.1">NQ320K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.12.5.2">XOR QA 100k</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.13.1">DSI++ <cite class="ltx_cite ltx_citemacro_citep">(Mehta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib99" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.13.2">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.13.3">
<span class="ltx_text" id="S4.T2.1.13.3.1"></span> <span class="ltx_text" id="S4.T2.1.13.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.13.3.2.1">
<span class="ltx_tr" id="S4.T2.1.13.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.13.3.2.1.1.1">T5-base, T5-large,</span></span>
<span class="ltx_tr" id="S4.T2.1.13.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.13.3.2.1.2.1">T5-XL</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.13.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.13.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.13.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.13.5.1">NQ-inc</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.13.5.2">MSMARCO-inc</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.14.1">GCoQA <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib75" title="">2023a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.14.2">Titles</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.14.3">
<span class="ltx_text" id="S4.T2.1.14.3.1"></span> <span class="ltx_text" id="S4.T2.1.14.3.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.14.3.2.1">
<span class="ltx_tr" id="S4.T2.1.14.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.14.3.2.1.1.1">T5-small, T5-base,</span></span>
<span class="ltx_tr" id="S4.T2.1.14.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.14.3.2.1.2.1">T5-large, T5-XL</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.14.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.14.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.14.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.14.5.1">TOPIOCQA</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.14.5.2">QRECC</span>,<span class="ltx_text ltx_font_italic" id="S4.T2.1.14.5.3">ORQUAC</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.15.1">GMR <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib63" title="">2022b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.15.2">Titles</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.15.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.15.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.15.5">HotpotQA</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.16.1">CorpusBrain <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib17" title="">2022b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.16.2">Titles</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.16.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.16.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.16.5">KILT</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.17.1">UGR <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib15" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.17.2">N-grams</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.17.3">BART-large</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.17.4">FM-index</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.17.5">KILT</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.18.1">SE-DSI <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib132" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.18.2">
<span class="ltx_text" id="S4.T2.1.18.2.1"></span> <span class="ltx_text" id="S4.T2.1.18.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.18.2.2.1">
<span class="ltx_tr" id="S4.T2.1.18.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.18.2.2.1.1.1">Title-like</span></span>
<span class="ltx_tr" id="S4.T2.1.18.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.18.2.2.1.2.1">(pseudo-queries)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.18.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.18.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.18.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.18.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.18.5.1">NQ100K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.18.5.2">MSMARCO</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.19">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.19.1">GenRRL <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib177" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.19.2">
<span class="ltx_text" id="S4.T2.1.19.2.1"></span> <span class="ltx_text" id="S4.T2.1.19.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.19.2.2.1">
<span class="ltx_tr" id="S4.T2.1.19.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.19.2.2.1.1.1">Title-like (summaries)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.19.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.19.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.19.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.19.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.19.5.1">NQ320K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.19.5.2">MSMARCO</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.20">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.20.1">GLEN <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib64" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.20.2">
<span class="ltx_text" id="S4.T2.1.20.2.1"></span> <span class="ltx_text" id="S4.T2.1.20.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.20.2.2.1">
<span class="ltx_tr" id="S4.T2.1.20.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.20.2.2.1.1.1">Title-like (keywords)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.20.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.20.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.20.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.20.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.20.5.1">NQ320K</span>, MSMARCO, BEIR</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.21">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.21.1">Ultron <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib179" title="">2022b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.21.2">
<span class="ltx_text" id="S4.T2.1.21.2.1"></span> <span class="ltx_text" id="S4.T2.1.21.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.21.2.2.1">
<span class="ltx_tr" id="S4.T2.1.21.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.21.2.2.1.1.1">Title-like (URLs)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.21.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.21.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.21.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.21.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.21.5.1">NQ320K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.21.5.2">MSMARCO</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.22">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.22.1">LLM-URL <cite class="ltx_cite ltx_citemacro_citep">(Ziems et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib181" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.22.2">
<span class="ltx_text" id="S4.T2.1.22.2.1"></span> <span class="ltx_text" id="S4.T2.1.22.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.22.2.2.1">
<span class="ltx_tr" id="S4.T2.1.22.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.22.2.2.1.1.1">Title-like (URLs)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.22.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.22.3">GPT3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.22.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.22.5">NQ, TriviaQA, WebQuestions</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.23">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.23.1">ASI <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib160" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.23.2">
<span class="ltx_text" id="S4.T2.1.23.2.1"></span> <span class="ltx_text" id="S4.T2.1.23.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.23.2.2.1">
<span class="ltx_tr" id="S4.T2.1.23.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.23.2.2.1.1.1">Codebook</span></span>
<span class="ltx_tr" id="S4.T2.1.23.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.23.2.2.1.2.1">(Auto Search)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.23.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.23.3">T5-like</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.23.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.23.5">MSMARCO (3.1M)</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.24">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.24.1">RIPOR <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib165" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.24.2">
<span class="ltx_text" id="S4.T2.1.24.2.1"></span> <span class="ltx_text" id="S4.T2.1.24.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.24.2.2.1">
<span class="ltx_tr" id="S4.T2.1.24.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.24.2.2.1.1.1"><span class="ltx_text" id="S4.T2.1.24.2.2.1.1.1.1"></span> <span class="ltx_text" id="S4.T2.1.24.2.2.1.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.24.2.2.1.1.1.2.1">
<span class="ltx_tr" id="S4.T2.1.24.2.2.1.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.24.2.2.1.1.1.2.1.1.1">Codebook</span></span>
<span class="ltx_tr" id="S4.T2.1.24.2.2.1.1.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.24.2.2.1.1.1.2.1.2.1">(Residual Quantization)</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.24.2.2.1.1.1.3"></span></span></span>
</span></span><span class="ltx_text" id="S4.T2.1.24.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.24.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.24.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.24.5">MSMARCO, TREC DL</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.25">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.25.1">ListGR <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib133" title="">[n. d.]</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.25.2">
<span class="ltx_text" id="S4.T2.1.25.2.1"></span> <span class="ltx_text" id="S4.T2.1.25.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.25.2.2.1">
<span class="ltx_tr" id="S4.T2.1.25.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.25.2.2.1.1.1"><span class="ltx_text" id="S4.T2.1.25.2.2.1.1.1.1"></span> <span class="ltx_text" id="S4.T2.1.25.2.2.1.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.25.2.2.1.1.1.2.1">
<span class="ltx_tr" id="S4.T2.1.25.2.2.1.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.25.2.2.1.1.1.2.1.1.1">Numeric ID</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.25.2.2.1.1.1.3"></span></span></span>
</span></span><span class="ltx_text" id="S4.T2.1.25.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.25.3">T5-base</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.25.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.25.5">
<span class="ltx_text ltx_font_italic" id="S4.T2.1.25.5.1">MSMARCO 100K</span>, <span class="ltx_text ltx_font_italic" id="S4.T2.1.25.5.2">NQ320K</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.26">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.1.26.1">MEVI <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib167" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.26.2">
<span class="ltx_text" id="S4.T2.1.26.2.1"></span> <span class="ltx_text" id="S4.T2.1.26.2.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.26.2.2.1">
<span class="ltx_tr" id="S4.T2.1.26.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.26.2.2.1.1.1"><span class="ltx_text" id="S4.T2.1.26.2.2.1.1.1.1"></span> <span class="ltx_text" id="S4.T2.1.26.2.2.1.1.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.26.2.2.1.1.1.2.1">
<span class="ltx_tr" id="S4.T2.1.26.2.2.1.1.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.26.2.2.1.1.1.2.1.1.1">Codebook</span></span>
</span></span><span class="ltx_text" id="S4.T2.1.26.2.2.1.1.1.3"></span></span></span>
</span></span><span class="ltx_text" id="S4.T2.1.26.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.26.3">T5-large</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.26.4">Trie</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.26.5">MSMARCO, NQ</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S4.SS3.p10">
<p class="ltx_p" id="S4.SS3.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p10.1.1">Multiview identifiers</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib77" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib76" title="">b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib78" title="">2024e</a>)</cite>. The above identifiers are limited in different aspects: numeric IDs require extra memorization steps and are ineffective in the large-scale corpus, while titles and substrings are only pieces of passages and thus lack contextualized information. More importantly, a passage should answer potential queries from different views, but one type of identifier only represents a passage from one perspective. Therefore, a natural idea is to combine different identifiers to exploit their advantages. In 2023, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib77" title="">2023c</a>)</cite> proposed a generative search framework, MINDER, to unify different identities, including titles, N-grams, pseudo queries, and numeric IDs. Any other identifiers could also be included in this framework. The experiments on three common datasets verify the effectiveness and robustness of different retrieval domains benefiting from the multiview identifiers.</p>
</div>
<div class="ltx_para" id="S4.SS3.p11">
<p class="ltx_p" id="S4.SS3.p11.1">Similar to the N-grams, multiview identifiers cannot correspond to documents one-to-one and thus require the transformation function. Besides, in the inference stage, the LLM needs to generate different types of identifiers, decreasing the inference efficiency.</p>
</div>
<div class="ltx_para" id="S4.SS3.p12">
<p class="ltx_p" id="S4.SS3.p12.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p12.1.1">Document identifier summary</span>. To better illustrate the characteristics of different document identifiers, we summarize Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.T1" title="Table 1 ‣ 4.3. Document Identifiers ‣ 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> from the aspects of semantic, distinctiveness, update, training, and applicable retrieval domains. Among the various types of identifiers, the codebook shows great potential in all dimensions. However, it does have a drawback in that it requires a complex training process. The multiview identifier has a simpler training process, but it lacks distinctiveness and requires a transform function from identifiers to documents.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Training</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">In contrast to traditional retrieval methods, the training for generative search is notably simpler. We categorize the training into generative and discriminative training two categories.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S4.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Major milestones in the development of generative search.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Generative training</span>. Once the input and output are determined, the LLM can be trained to predict the next token. There are two main training directions. 1) <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.2">Query-to-identifier</span> training. This involves training the LLM to generate the corresponding identifier for a given query. Most identifier types require this training, with some, such as document titles, N-grams, and multiview identifiers, only needing this query-to-identifier training direction. 2) <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.3">Document-to-identifier</span> training. In this approach, the LLM learns to predict the corresponding identifier when given a document as input. This training is crucial for certain identifiers, such as numeric IDs and the codebook, as they need to align with the semantics of documents, and <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.4">codebook training</span> could be regarded as the special document-to-identifier training. It is noted that not all documents in a search have labels (queries), making it challenging for the LLM to memorize these documents. To address this, some generative search methods <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib180" title="">2022</a>; Pradeep et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib106" title="">2023</a>)</cite> utilize pseudo pairs to expand the training samples and enhance document memorization.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">Discriminative training</span>. Generative search presents a new paradigm for retrieval, as it transforms the original discriminative methods into the generative methods and could train the retrieval model (generative language model) via generation loss. However, the work <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib76" title="">2023b</a>)</cite> has highlighted the importance of discriminative training in generative search. It has been observed that discriminative training can further improve a well-trained generative model. This finding is meaningful for both generative search and the traditional retrieval paradigm. Previous retrieval studies have developed numerous discriminative losses (rank losses) and negative sample mining methods, which is a big treasure for the retrieval field. As the finding illustrates, the previous research approaches could be adjusted to enhance the current generative search. The following works <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib165" title="">2023</a>; Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib133" title="">[n. d.]</a>)</cite> further verified the effectiveness of introducing discriminative training in generative search.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Inference</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">After completing the training process, the generative search model can be used for retrieval purposes.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">Free generation</span>. During the inference stage, the trained LLM is able to predict identifiers based on a user query, similar to the training process. This generation process is free, as the LLM could generate any text without any constraints. These predicted identifiers may correspond directly to specific documents, or they may be determined through a heuristic function based on the type of identifier. This is the unique aspect of generative search, as it allows for direct document retrieval through generation. However, in practical applications, only a few generative approaches <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib160" title="">2023</a>; Ziems et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib181" title="">2023</a>)</cite> actually utilize the free generation method. Because the scope of identifiers is limited, the potential for generation is infinite. This means that the LLM may generate identifiers that could not belong to any document within the corpus.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p3.1.1">Constrained generation</span>. Most generative search approaches employ constrained generation to guarantee the LLM generates valid identifiers. This technique involves post-processing to mask any invalid tokens and only allows the generation of valid tokens that belong to identifiers. To achieve this, special data structures such as Trie and FM_index <cite class="ltx_cite ltx_citemacro_citep">(Ferragina and Manzini, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib30" title="">2000</a>)</cite> are utilized. The FM_index enables the LLM to generate valid tokens from any position within the identifier, while the Trie only supports generation from the first token of an identifier. These data structures play a key role in enabling the LLM to accurately generate valid identifiers.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">The predicted identifiers could correspond directly to specific documents, or they may be determined through a heuristic function based on the type of identifier. Finally, generative search methods could give a final document ranking list in a generative way.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Summary</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p1.1.1">Methods summary</span>. We summarize the current generative search methods in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.T2" title="Table 2 ‣ 4.3. Document Identifiers ‣ 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, from the aspects of identifier, backbone, constrained generation, and datasets. 1) Identifier. We have fully discussed the characteristics and problems of different identifiers in the previous sections and Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.T1" title="Table 1 ‣ 4.3. Document Identifiers ‣ 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>. Different identifiers usually require different training strategies and inference processes. 2) Backbone. Almost all methods employ pretrained language models, like BART <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib65" title="">2019</a>)</cite> and T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib112" title="">2020</a>)</cite>, as the backbone, due to their extensive language knowledge. However, it is also noted that few current approaches apply advanced large foundation models, like ChatGPT and LLaMA. On the one hand, increasing the model size alone may not lead to significant research contributions; on the other hand, closed-source models cannot be adjusted for constrained generation. 3) Constrained generation. Almost all methods adopt the constrained generation to guarantee the valid identifier generation. The multiview identifier and N-gram identifier require the FM_index structure, while others need the Trie structure to facilitate the constrained generation. 4) Datasets are primarily focused on document-level and passage-level retrieval tasks, with some also designed for conversational QA, multi-hop QA, and cross-lingual retrieval tasks. However, due to limitations in identifier types, some methods have had to reformulate certain datasets for evaluation, such as NQ320k, TriviaQA (subset), and MSMARCO (subset). While these reformulated datasets may highlight the advantages of generative search, they may not align with real-world applications. Multiview identifiers based methods <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib77" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib76" title="">b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib78" title="">2024e</a>)</cite> have a significant advantage in general retrieval datasets.</p>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS6.p2.1.1">Timeline summary</span>. We also provide a brief overview of the development of generative search based on the timeline, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.F2" title="Figure 2 ‣ 4.4. Training ‣ 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>. We specifically highlight the works that first introduced a new identifier type or training scheme in generative search. The credit for the first generative search work goes to GENRE <cite class="ltx_cite ltx_citemacro_citep">(De Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib25" title="">2020</a>)</cite>. While GENRE’s primary focus is on entity retrieval rather than document retrieval, it was the first to employ the autoregressive generation paradigm to accomplish the retrieval task and introduce constrained beam search. Starting from 2022, there has been a continuous introduction of new identifier types <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib134" title="">2022</a>; Bevilacqua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib8" title="">2022</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib126" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib77" title="">2023c</a>)</cite>. In 2023, discriminative training <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib76" title="">2023b</a>)</cite> was introduced in generative search, followed by the latest works <cite class="ltx_cite ltx_citemacro_citep">(Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib165" title="">2023</a>; Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib133" title="">[n. d.]</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7. </span>LLMs for Retrieval beyond Generative Search</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">There have been efforts to investigate the potential applications of LLMs in text retrieval beyond generative search. These studies have concentrated on leveraging LLMs to improve existing retrieval pipelines by substituting smaller language models with larger ones. 1) <span class="ltx_text ltx_font_bold" id="S4.SS7.p1.1.1">LLMs for query expansion</span>. Some works <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib144" title="">2023d</a>; Jagerman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib51" title="">2023</a>)</cite> utilized LLMs to expand the queries. For instance, Query2Doc <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib144" title="">2023d</a>)</cite> employs LLMs to generate synthetic documents, which can then be integrated into traditional document retrieval systems to enhance their performance. 2) <span class="ltx_text ltx_font_bold" id="S4.SS7.p1.1.2">LLMs as feature extractors</span>. Dense retrievers are usually conducted based on encoder-only language models, like BERT. Recently, some work <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib98" title="">2023</a>; Muennighoff et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib101" title="">2024</a>)</cite> has focused on leveraging generative large language models to improve document representations. 3) <span class="ltx_text ltx_font_bold" id="S4.SS7.p1.1.3">LLMs based rankers</span>. Rankers should further refine the order of the retrieved candidates to improve output quality, and LLMs have also been applied to refine the ranking of retrieved documents, as demonstrated in studies <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib127" title="">2023c</a>; Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib109" title="">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Generative Recommendation</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Overview</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Since recommendation systems aim to filter out the items that are relevant to the user interests, two crucial components are required to achieve matching in the natural language space, <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">i.e.</span>, user formulation, and item identifiers.
The user formulation and item identifiers are analogous to the query formulation and document identifier in generative search, respectively.
Specifically, as the input for generative model, the user formulation contains diverse information (<span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">e.g.</span>, user’s historical interactions, and user profile) to represent a user in the natural language for modeling the user interests.
To match the user interests in natural language, generative recommendation system will generate item identifier of the relevant items.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>User Formulation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Since there is no specific “query” for each user in recommendation systems, user formulation is a crucial step to represent a user for personalized recommendation.
In generative recommendation systems, the user<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Here, the “user” refers to the input of the generative recommendation task.</span></span></span> is primarily formulated based on four distinct components: the task description, user-associated information, context information, and external knowledge expressed in natural language.
In particular, the user-associated information mainly consists of user’s historical interactions and user profile (<span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">e.g.,</span> age and gender).
Based on the availability of the data, previous work formulates the user by encompassing one or multiple components through pre-defined prompt templates for each user.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Task description</span> <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>)</cite>.
To leverage the strong comprehension ability of the powerful generative models, task description is employed to guide the generative models to accomplish the recommendation task, <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.1.2">i.e., </em>next-item prediction.
For instance, in <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>)</cite>, the task description for movie recommendation is articulated as “Given ten movies that the user watched recently, please recommend a new movie that the user likes to the user.”.
It is noted that the task description can also serve as the prompt template, where various components are inserted into the template to formulate the user.
For example, <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>)</cite> uses “I find the purchase history of …, I wonder what is the next item to recommend to the user. Can you help me decide?” as the task description to instruct the generative models, where the user’s historical interactions will be utilized for user formulation.
In <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib169" title="">2023d</a>)</cite>, “Here is the historical interactions of a user: …, his preferences are as follows: …, please provide recommendations.” is used to guide the generative models, where historical interactions and user preference are incorporated to formulate the user.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">User’s historical interactions</span> <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib57" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib173" title="">2023b</a>)</cite>.
Serving as the implicit user feedback on items, user’s historical interactions play a crucial role in representing user behavior <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib39" title="">2020</a>)</cite> in recommender systems, which implicitly conveys user preference over items.
To present the historical interactions, <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.2">i.e.</span>, the sequence of interacted items, one common practice is to utilize the item ID to form the interaction sequence <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>; Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite>.
Nevertheless, as pointed out by <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib162" title="">2024</a>)</cite>, generative models face limitations in capturing collaborative information while excelling in capturing nuanced item semantics.
Therefore, two categories of work emerge to further improve the recommendation accuracy. 1)
To better leverage the strong semantics understanding of LLMs, a line of work <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib23" title="">2022</a>; Chen, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib19" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib44" title="">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib91" title="">2024a</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib57" title="">2024</a>)</cite> attempts to integrate rich side information of items in historical interactions.
In particular, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib88" title="">2023b</a>; Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib23" title="">2022</a>)</cite> incorporates item descriptions when listing the user’s historical interactions. <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite> utilizes item titles and item attributes to exploit the rich semantics of items for a better understanding of user preference.
2) To strengthen the collaborative information understanding of LLMs, another line of work aims to incorporate ID embeddings of the interacted items for LLMs to understand user behavior.
For example, LLaRA <cite class="ltx_cite ltx_citemacro_citep">(Liao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib80" title="">2023</a>)</cite> additionally includes ID embedding after the title’s token embedding to represent each item in historical interactions.
Moreover, with the advancements in multimodal LLMs, some work <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib91" title="">2024a</a>; Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib35" title="">2023</a>)</cite> attempts to incorporate multimodal feature of the item, <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.3">e.g.,</span> visual features, to complement the textual user’s historical interactions.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">User profile</span> <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib32" title="">2023</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib93" title="">2024</a>)</cite>.
To enhance the user modeling, integrating user profile (<span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.2">e.g.,</span> demographic and preference information about the user) is an effective way to model the user characteristics in recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Uyangoda et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib137" title="">2018</a>)</cite>.
In most cases, user’s demographic information (<span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.3">e.g.,</span> gender) can be obtained directly from the online recommendation platform.
Such user information is then combined with descriptional text, <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.4">e.g.,</span> “User description: female, 25-34, and in sales/marketing” <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib155" title="">2023</a>)</cite>.
For instance, <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib32" title="">2023</a>)</cite> utilizes user’s age and gender to prompt ChatGPT to enhance its comprehension of user characteristics based on encoded prior knowledge.
Although demographic information can implicitly reflect the general preference within specific user groups (<span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.5">e.g.,</span> teenagers),
it can further enrich models’ comprehension of the user by explicitly detailing user’s general preference and current intention during user formulation.
To obtain the explicit general preference and current intention, <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib169" title="">2023d</a>)</cite> proposes to leverage LLMs to infer the user intention and the general preference based on the user’s historical interactions using tailored prompts.
Besides, <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib175" title="">2024</a>)</cite> uses LLMs to summarize user preference based on the user’s historical interactions.
However, acquiring user profiles can be challenging due to user privacy concerns, leading some studies to employ user ID to capture the collaborative information <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>)</cite> or discard user profile for user formulation <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">Context information</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib89" title="">2024b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib162" title="">2024</a>)</cite>.
In addition to user’s historical interactions and profile, environmental context information (e.g., location and time), which can influence user decisions, is also advantageous for models to better match the users with appropriate items.
For example, users may prefer to purchase a coat rather than a T-shirt in winter in clothing recommendation.
Therefore, incorporating context information such as time, can achieve effective user understanding in real-world application scenarios.
For instance, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib89" title="">2024b</a>)</cite> harnesses diagnosis and procedures for medication recommendation while <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib162" title="">2024</a>)</cite> incorporates context information via learnable soft prompt to capture unobserved context signals.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1">External knowledge</span> <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib29" title="">2024</a>)</cite>.
Although generative models have demonstrated promising performance in recommendation tasks based on user-associated information, recent research has explored leveraging external knowledge to enhance the performance of generative recommender models.
To harness structured information from user-item graph, <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib29" title="">2024</a>)</cite> integrates the graph data in natural language and further propagates higher-order neighbor’s information to capture complex relations between users and items.
Additionally, <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib94" title="">2023</a>)</cite> leverages external knowledge from conventional recommender models by incorporating prediction from these models in natural language, showcasing collaborative efforts from both conventional models and generative models.
Some work also incorporates an item candidate set to reduce the searching space of the whole item set, thus alleviating the hallucination problem and improving the accuracy <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib24" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib169" title="">2023d</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Identifiers in generative recommendation.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.12">
<tr class="ltx_tr" id="S5.T3.12.13">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.12.13.1">Type</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.12.13.2">Semantic</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.12.13.3">Distinctiveness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.12.13.4">Update</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.12.13.5">Training</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.3.3.4">Numeric ID</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.1.1.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><times id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.2.2.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.m1.1a"><mi id="S5.T3.2.2.2.m1.1.1" mathvariant="normal" xref="S5.T3.2.2.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.3.3"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.m1.1a"><mo id="S5.T3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><times id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.3.5">user-to-identifier</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.6.6.4">Item’s textual metadata</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.4.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.4.4.1.m1.1"><semantics id="S5.T3.4.4.1.m1.1a"><mi id="S5.T3.4.4.1.m1.1.1" mathvariant="normal" xref="S5.T3.4.4.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.1.m1.1b"><ci id="S5.T3.4.4.1.m1.1.1.cmml" xref="S5.T3.4.4.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.5.5.2"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.5.5.2.m1.1"><semantics id="S5.T3.5.5.2.m1.1a"><mo id="S5.T3.5.5.2.m1.1.1" xref="S5.T3.5.5.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.2.m1.1b"><times id="S5.T3.5.5.2.m1.1.1.cmml" xref="S5.T3.5.5.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.2.m1.1d">×</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.6.6.3.m1.1"><semantics id="S5.T3.6.6.3.m1.1a"><mi id="S5.T3.6.6.3.m1.1.1" mathvariant="normal" xref="S5.T3.6.6.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.3.m1.1b"><ci id="S5.T3.6.6.3.m1.1.1.cmml" xref="S5.T3.6.6.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.5">user-to-identifier</td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.9.9.4">Codebook</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.7.7.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.7.7.1.m1.1"><semantics id="S5.T3.7.7.1.m1.1a"><mi id="S5.T3.7.7.1.m1.1.1" mathvariant="normal" xref="S5.T3.7.7.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.1.m1.1b"><ci id="S5.T3.7.7.1.m1.1.1.cmml" xref="S5.T3.7.7.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.7.7.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.8.8.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.8.8.2.m1.1"><semantics id="S5.T3.8.8.2.m1.1a"><mi id="S5.T3.8.8.2.m1.1.1" mathvariant="normal" xref="S5.T3.8.8.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.2.m1.1b"><ci id="S5.T3.8.8.2.m1.1.1.cmml" xref="S5.T3.8.8.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.8.8.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.9.9.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.9.9.3.m1.1"><semantics id="S5.T3.9.9.3.m1.1a"><mi id="S5.T3.9.9.3.m1.1.1" mathvariant="normal" xref="S5.T3.9.9.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.3.m1.1b"><ci id="S5.T3.9.9.3.m1.1.1.cmml" xref="S5.T3.9.9.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.9.9.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.9.9.5">item-to-identifier, user-to-identifier, auxiliary alignment</td>
</tr>
<tr class="ltx_tr" id="S5.T3.12.12">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T3.12.12.4">Multi-facet</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.10.10.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.10.10.1.m1.1"><semantics id="S5.T3.10.10.1.m1.1a"><mi id="S5.T3.10.10.1.m1.1.1" mathvariant="normal" xref="S5.T3.10.10.1.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.10.10.1.m1.1b"><ci id="S5.T3.10.10.1.m1.1.1.cmml" xref="S5.T3.10.10.1.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.10.10.1.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.10.10.1.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.11.11.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.11.11.2.m1.1"><semantics id="S5.T3.11.11.2.m1.1a"><mi id="S5.T3.11.11.2.m1.1.1" mathvariant="normal" xref="S5.T3.11.11.2.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.2.m1.1b"><ci id="S5.T3.11.11.2.m1.1.1.cmml" xref="S5.T3.11.11.2.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.2.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.11.11.2.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.12.12.3"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.12.12.3.m1.1"><semantics id="S5.T3.12.12.3.m1.1a"><mi id="S5.T3.12.12.3.m1.1.1" mathvariant="normal" xref="S5.T3.12.12.3.m1.1.1.cmml">✓</mi><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.3.m1.1b"><ci id="S5.T3.12.12.3.m1.1.1.cmml" xref="S5.T3.12.12.3.m1.1.1">✓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.3.m1.1c">\checkmark</annotation><annotation encoding="application/x-llamapun" id="S5.T3.12.12.3.m1.1d">✓</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.12.12.5">user-to-identifier</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Item Identifiers</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Similar to generative search, the generative recommender models are expected to generate relevant items given the user formulation. Nevertheless, items in recommendation platforms usually consist of various side information from different modalities, <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">e.g.,</span> thumbnails of the micro-videos, audios of the music, and titles of the news.
As such, the complex data from the items in recommendation necessitates item identifiers to present each item’s characteristics in the language space for generative recommendation.
It is highlighted that a good item identifier should at least meet two criteria as pointed out in <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite>:
1) distinctiveness to emphasize the salient item features learned from user behaviors. And
2) semantics to focus on the utilization of prior knowledge in pre-trained language models, which facilitates strong generalization to cold-start and cross-domain recommendation.
Existing work usually constructs the item identifiers in the following four strategies, meeting different criteria accordingly.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Numeric ID</span> <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>; Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib12" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib150" title="">2024b</a>; Si et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib123" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib14" title="">2023a</a>; Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib46" title="">2023a</a>; Petrov and Macdonald, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib105" title="">2023</a>; Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib166" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib68" title="">2023d</a>; Qiu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib110" title="">2023</a>)</cite>.
Given the widely demonstrated efficacy of numeric IDs for capturing collaborative information in traditional recommendation models <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib173" title="">2023b</a>)</cite>, a straightforward strategy in generative recommendation frameworks is to adopt the strategy of using numeric IDs to represent items <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib79" title="">2024c</a>)</cite>.
However, directly adopting the ID setting in traditional recommender models is infeasible in generative recommender models.
Because traditional recommender models consider each item as an independent “token”, which cannot be further tokenized and strictly refers to one independent embedding.
This requires adding all the independent tokens into the model, which requires 1) large memory to store every item embedding, and 2) sufficient interactions for training the item embedding.
To combat these issues, generative recommender models offer a promising solution by designing the item identifier as a token sequence, where the numeric IDs can be further tokenized and are associated with several token embeddings.
As such, a token sequence as a numeric ID makes it possible to use finite tokens to represent infinite items <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib47" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib69" title="">2023g</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">To effectively represent an item with a numeric ID in a token sequence, previous work explores different strategies for ID assignment <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite>.
In <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>)</cite>, sequential indexing is utilized to capture the collaborative information in an intuitive way.
Specifically, sequential indexing represents the user’s items in chronological order by consecutive numeric IDs, (<span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.1">e.g.,</span> “11138”, “11139”, <math alttext="\dots" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mi id="S5.SS3.p3.1.m1.1.1" mathvariant="normal" xref="S5.SS3.p3.1.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><ci id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">\dots</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">…</annotation></semantics></math>, “11405”), which could capture the co-occurrence of items that are interacted with the same user.
However, such sequential indexing might suffer from the potential data leakage issue <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite> fixes this potential issue and explores other non-trivial indexing methods that incorporate prior information on the items, such as semantics and collaborative knowledge.
By constructing IDs based on item categories in a hierarchical structure, the items belonging to the same categories will have similar IDs and are validated to be effective in generative recommendation.
Similarly, SEATER <cite class="ltx_cite ltx_citemacro_citep">(Si et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib123" title="">2023</a>)</cite> proposes a tree-based hierarchical identifier with numeric IDs, where items with similar interactions will have similar IDs.
In addition, <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite> also attempts to construct IDs based on the item co-occurance matrix, where items that co-occur more times will have more similar IDs, which is assessed to be beneficial in generating appropriate recommendation.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">Despite the effectiveness of distinctive numeric IDs in generative recommendation, it usually lacks semantic information, thus suffering from the cold-start problem <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite> and failing to leverage the world knowledge encoded in the recently emerged powerful generative models, <span class="ltx_text ltx_font_italic" id="S5.SS3.p4.1.1">e.g.,</span> LLMs.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p5.1.1">Item’s textual meta data</span> <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib168" title="">2023a</a>; Lin and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib81" title="">2023</a>; Harte et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib38" title="">2023</a>; Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib161" title="">2023</a>; Di Palma, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib28" title="">2023</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib95" title="">2024b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib86" title="">2023c</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib149" title="">2023c</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib128" title="">2023a</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib163" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib141" title="">2023a</a>)</cite>.
To overcome the absence of semantics in numeric IDs, other work <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib57" title="">2024</a>)</cite> utilizes the item’s textual metadata, <span class="ltx_text ltx_font_italic" id="S5.SS3.p5.1.2">e.g.,</span> item title, leveraging the LLMs’ world knowledge encoded in the parameters to better comprehend the item characteristics based on the semantics in the item’s textual descriptions.
For instance, <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>; Liao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib80" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib172" title="">2021</a>; Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib52" title="">2023b</a>; Wang and Lim, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib143" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib44" title="">2023b</a>)</cite> use movie title, <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib91" title="">2024a</a>; Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib52" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib67" title="">2023h</a>; Chen, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib19" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib87" title="">2023a</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib169" title="">2023d</a>)</cite> use the product name, <cite class="ltx_cite ltx_citemacro_citep">(Zhiyuli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib176" title="">2023</a>)</cite> utilizes book name, <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib72" title="">2023e</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib73" title="">f</a>)</cite> adopt news title, <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib24" title="">2023</a>)</cite> uses song title, <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib130" title="">2024</a>)</cite> employs abstractive text of items, and <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib23" title="">2022</a>)</cite> includes both title and descriptions of online products, as the item identifier for recommendation.
Although leveraging an item’s textual metadata significantly alleviates the cold-start issue <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite>, it is still suboptimal for effective recommendation.
The textual metadata, especially the descriptions could be very lengthy, which would cause out-of-corpus issues, <span class="ltx_text ltx_font_italic" id="S5.SS3.p5.1.3">i.e.,</span> the generated token sequence could not match any valid item identifier.
Although grounding the generated tokens to existing items via distance-based methods is a potential solution to address this problem <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>)</cite>,
it would take us back to deep learning-based recommendation since we need to calculate the matching score between the generated item and each in-corpus item <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib69" title="">2023g</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p6">
<p class="ltx_p" id="S5.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p6.1.1">Codebook</span> <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite>.
To simultaneously leverage the semantics while pursuing a unique short token sequence, <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite> proposes to learn a codebook to construct the item identifier in generative recommendation.
Similar to the codebook for document identifier in generative search, related work in recommendation <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib43" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib43" title="">a</a>)</cite> focuses on developing a codebook specifically for items.
Typically, RQ-VAEs <cite class="ltx_cite ltx_citemacro_citep">(Zeghidour et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib164" title="">2021</a>)</cite> are utilized to learn the codebook, where the input is the item’s semantic representation extracted from a pre-trained language model (<span class="ltx_text ltx_font_italic" id="S5.SS3.p6.1.2">e.g.,</span> LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib135" title="">2023</a>)</cite>) and the output is the generated token sequence.
The overall process of training codebook is similar to that in generative search, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS3" title="4.3. Document Identifiers ‣ 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
Along this line, TIGER <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite> is a representative work that generates an item’s semantic ID based on the item’s textual descriptions via the codebook.
LC-Rec <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib174" title="">2023</a>)</cite> further enhances the generated ID’s representation to align with user preference and semantics of the item’s textual descriptions.</p>
</div>
<div class="ltx_para" id="S5.SS3.p7">
<p class="ltx_p" id="S5.SS3.p7.1">However, while codebook-based identifiers meet both semantics and distinctiveness, they suffer from the misalignment between the semantics correlation and interaction correlation.
Specifically, the codebooks essentially capture the correlation of the item semantics into the identifier, <em class="ltx_emph ltx_font_italic" id="S5.SS3.p7.1.1">i.e., </em>items with similar semantics will have similar identifiers.
The identifier representations will then be optimized to capture interaction correlation by training on recommendation data.
Nevertheless, the items with similar codes might not necessarily have similar interactions, thereby hurting the learning of user behavior.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>Summary of the representative generative recommendation methods.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:433.6pt;height:352.1pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-230.5pt,186.9pt) scale(0.484703503529366,0.484703503529366) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T4.1.1">
<tr class="ltx_tr" id="S5.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Item identifier</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.1.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.2.1">User formulation</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.1.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.3.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.1.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.4.1">Backbone</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.1.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.5.1">Generation</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.1.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.6.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.1.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.7.1">Domain</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.2.1" rowspan="6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.1.1">Numeric ID</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.2.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.2.2.1.1">
<span class="ltx_tr" id="S5.T4.1.1.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.2.2.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.2.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.2.2.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.2.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.3.1">RecSysLLM <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib21" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.2.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.4.1">GLM-10B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.2.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.5.1">Trie</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.2.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.6.1">Sports, Beauty, Toys</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.2.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.2.7.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.1" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.3.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.3.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.3.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.3.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">User profile</span></span>
<span class="ltx_tr" id="S5.T4.1.1.3.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.3.1.1.1.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.3.2.1">P5 <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib34" title="">2022</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.3.3.1">T5-small, T5-Base</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.3.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.3.5.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.3.5.1.1">
<span class="ltx_tr" id="S5.T4.1.1.3.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.3.5.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Sports, Beauty, Toys</span></span>
<span class="ltx_tr" id="S5.T4.1.1.3.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.3.5.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Yelp</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.3.6.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.3.6.1.1">
<span class="ltx_tr" id="S5.T4.1.1.3.6.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.3.6.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">E-commerce</span></span>
<span class="ltx_tr" id="S5.T4.1.1.3.6.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.3.6.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Restaurant</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.4.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.4.1.1">How2index <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.4.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.4.2.1">T5-small</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.4.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.4.3.1">Trie</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.4.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.4.4.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.4.4.1.1">
<span class="ltx_tr" id="S5.T4.1.1.4.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.4.4.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Sports, Beauty</span></span>
<span class="ltx_tr" id="S5.T4.1.1.4.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.4.4.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Yelp</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.4.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.4.5.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.4.5.1.1">
<span class="ltx_tr" id="S5.T4.1.1.4.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.4.5.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">E-commerce</span></span>
<span class="ltx_tr" id="S5.T4.1.1.4.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.4.5.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Restaurant</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.5.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.5.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.5.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.5.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.5.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.5.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.5.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
<span class="ltx_tr" id="S5.T4.1.1.5.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.5.1.1.1.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">Context information</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.5.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.5.2.1">PAP-REC <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib79" title="">2024c</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.5.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.5.3.1">T5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.5.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.5.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.5.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.5.5.1">Beauty, Sports, Toys</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.5.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.5.6.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.6.1" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.6.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.6.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.6.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.6.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.6.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.6.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.6.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.6.2.1">VIP5 <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib35" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.6.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.6.3.1">T5-small</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.6.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.6.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.6.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.6.5.1">Clothing, Sports, Beauty, Toys</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.6.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.6.6.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.7.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.7.1.1">UniMAP <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib152" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.7.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.7.2.1">Redpajama-3B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.7.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.7.3.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.7.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.7.4.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.7.4.1.1">
<span class="ltx_tr" id="S5.T4.1.1.7.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.7.4.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Baby, Beauty, Clothing</span></span>
<span class="ltx_tr" id="S5.T4.1.1.7.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.7.4.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Grocery, Sports, Toys, Office</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.7.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.7.5.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.8.1" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.1.1">Codebook</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.2.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.8.2.1.1">
<span class="ltx_tr" id="S5.T4.1.1.8.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.8.2.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">User profile</span></span>
<span class="ltx_tr" id="S5.T4.1.1.8.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.8.2.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.3.1">TIGER <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.4.1">Transformer-based model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.5.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.6.1">Sports, Beauty, Toys</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.8.7.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.9.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.9.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.9.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.9.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.9.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.9.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.9.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.9.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.9.2.1">LC-Rec <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib174" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.9.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.9.3.1">LLaMA-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.9.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.9.4.1">Trie</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.9.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.9.5.1">Instruments, Arts, Games</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.9.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.9.6.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.10.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.1.1">Multi-facet</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.10.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.2.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.10.2.1.1">
<span class="ltx_tr" id="S5.T4.1.1.10.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.10.2.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.10.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.10.2.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.10.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.3.1">TransRec <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.10.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.4.1">BART-large, LLaMA-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.10.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.5.1">FM-index</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.10.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.6.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.10.6.1.1">
<span class="ltx_tr" id="S5.T4.1.1.10.6.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.10.6.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Beauty, Toys</span></span>
<span class="ltx_tr" id="S5.T4.1.1.10.6.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.10.6.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Yelp</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.10.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.10.7.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.10.7.1.1">
<span class="ltx_tr" id="S5.T4.1.1.10.7.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.10.7.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">E-commerce</span></span>
<span class="ltx_tr" id="S5.T4.1.1.10.7.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.10.7.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Restaurant</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S5.T4.1.1.11.1" rowspan="13" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.11.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.11.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.11.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Item’s textual</span></span>
<span class="ltx_tr" id="S5.T4.1.1.11.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.11.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">metadata</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.11.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.2.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.11.2.1.1">
<span class="ltx_tr" id="S5.T4.1.1.11.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.11.2.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">User profile</span></span>
<span class="ltx_tr" id="S5.T4.1.1.11.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.11.2.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
<span class="ltx_tr" id="S5.T4.1.1.11.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.11.2.1.1.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">Context information</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.11.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.3.1">M6-Rec <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib23" title="">2022</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.11.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.4.1">M6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.11.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.5.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.11.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.6.1">TaoProduct</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.11.7" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.11.7.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.1" rowspan="5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.12.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.12.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.12.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.12.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.12.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.2" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.12.2.1">BIGRec <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.3" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.12.3.1">LLaMa-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.4" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.12.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.5" style="padding-left:3.4pt;padding-right:3.4pt;">Games</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.6" style="padding-left:3.4pt;padding-right:3.4pt;">E-commerce</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.13.1" style="padding-left:3.4pt;padding-right:3.4pt;">MovieLens25M</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.13.2" style="padding-left:3.4pt;padding-right:3.4pt;">Movie</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.14.1" rowspan="3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.14.1.1">LMRecSys <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib172" title="">2021</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.14.2" style="padding-left:3.4pt;padding-right:3.4pt;">BERT-Base, GPT2-Small</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.14.3" rowspan="3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.14.3.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.14.4" rowspan="3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.14.4.1">MovieLens1M</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.14.5" rowspan="3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.14.5.1">Movie</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.15.1" style="padding-left:3.4pt;padding-right:3.4pt;">GPT2-Medium, GPT2-Large</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.16.1" style="padding-left:3.4pt;padding-right:3.4pt;">GPT2-XL</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.17">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.17.1" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.17.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.17.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.17.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.17.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.17.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.17.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
<span class="ltx_tr" id="S5.T4.1.1.17.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.17.1.1.1.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">External knowledge</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.17.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.17.2.1">NIR <cite class="ltx_cite ltx_citemacro_citep">(Wang and Lim, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib143" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.17.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.17.3.1">GPT-3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.17.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.17.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.17.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.17.5.1">MovieLens100K</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.17.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.17.6.1">Movie</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.18.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.18.1.1">RecRanker <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib94" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.18.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.18.2.1">LLaMA2-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.18.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.18.3.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.18.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.18.4.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.18.4.1.1">
<span class="ltx_tr" id="S5.T4.1.1.18.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.18.4.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">MovieLens100K, MovieLens100M</span></span>
<span class="ltx_tr" id="S5.T4.1.1.18.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.18.4.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">BookCrossing</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.18.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.18.5.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.18.5.1.1">
<span class="ltx_tr" id="S5.T4.1.1.18.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.18.5.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Movie</span></span>
<span class="ltx_tr" id="S5.T4.1.1.18.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T4.1.1.18.5.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Book</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.19">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.19.1" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.19.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.19.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.19.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.19.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.19.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.19.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
<span class="ltx_tr" id="S5.T4.1.1.19.1.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.19.1.1.1.3.1" style="padding-left:3.4pt;padding-right:3.4pt;">User profile</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.19.2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.19.2.1">InstructRec <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib169" title="">2023d</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.19.3" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.19.3.1">Flan-T5-XL</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.19.4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.19.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.19.5" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.19.5.1">Games, CDs</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.19.6" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.19.6.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.20">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.1.20.1" rowspan="4" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.20.1.1">
<span class="ltx_tabular ltx_align_middle" id="S5.T4.1.1.20.1.1.1">
<span class="ltx_tr" id="S5.T4.1.1.20.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.20.1.1.1.1.1" style="padding-left:3.4pt;padding-right:3.4pt;">Task description</span></span>
<span class="ltx_tr" id="S5.T4.1.1.20.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_left" id="S5.T4.1.1.20.1.1.1.2.1" style="padding-left:3.4pt;padding-right:3.4pt;">Historical interactions</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.20.2" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.20.2.1">Rec-GPT4V <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib91" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.20.3" style="padding-left:3.4pt;padding-right:3.4pt;">GPT4-V</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.20.4" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.20.4.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.20.5" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.20.5.1">Sports, Clothing, Beauty, Toys</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.20.6" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.20.6.1">E-commerce</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.21.1" style="padding-left:3.4pt;padding-right:3.4pt;">LLaVA-7B, LLaVA-13B</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.22">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.1.22.1" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.22.1.1">DEALRec <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib84" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.1.22.2" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.22.2.1">LLaMA-7B</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.1.1.22.3" rowspan="2" style="padding-left:3.4pt;padding-right:3.4pt;"><span class="ltx_text" id="S5.T4.1.1.22.3.1">Free</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.22.4" style="padding-left:3.4pt;padding-right:3.4pt;">Games, Book</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.22.5" style="padding-left:3.4pt;padding-right:3.4pt;">E-commerce</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.23">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.1.1.23.1" style="padding-left:3.4pt;padding-right:3.4pt;">MicroLens-50K</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.1.1.23.2" style="padding-left:3.4pt;padding-right:3.4pt;">Micro-video</td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS3.p8">
<p class="ltx_p" id="S5.SS3.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p8.1.1">Multi-facet identifier</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite>.
To overcome the issues in previous identifier strategies, the multi-facet identifier is proposed.
Multi-facet identifier aims to pursue both semantics and distinctiveness while mitigating the misalignment between semantics correlation and interaction correlation.
While incorporating semantics (<span class="ltx_text ltx_font_italic" id="S5.SS3.p8.1.2">e.g.,</span> item title) exploits the world knowledge encoded in generative models, utilizing unique numeric IDs ensures distinctiveness for capturing the essential collaborative information.
Additionally, to avoid the lengthy issue of textual metadata, TransRec <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite> allows the generation of the substring of metadata.
The utilization of substring follows the one-to-many corresponding as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S4.SS3" title="4.3. Document Identifiers ‣ 4. Generative Search ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4.3</span></a>, thus might decrease the inference efficiency.</p>
</div>
<div class="ltx_para" id="S5.SS3.p9">
<p class="ltx_p" id="S5.SS3.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p9.1.1">Item identifier summary</span>.
We summarize the characteristics of different types of item identifiers in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.T3" title="Table 3 ‣ 5.2. User Formulation ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> from different aspects, including semantics, distinctiveness, update, and the involved training process.
From the summary, we can find that
1) Incorporating semantics enables better utilization of world knowledge in generative language models and easier identifier update.
This can contribute to improved generalization and practicality in real-world deployments.
2) Codebook and multi-facet identifiers achieve both semantics and distinctiveness, showing the potential to leverage semantics in pre-trained generative language models and learn collaborative information from user-item interactions.
Nevertheless, while codebook requires additional item-to-identifier training and auxiliary alignment to endow the generated identifier with the semantics in language models, multi-facet identifier is naturally advantageous to leverage both numeric ID and descriptions for improved generative recommendation.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Training</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Training the generative recommender models on the recommendation data involves two main steps, <em class="ltx_emph ltx_font_italic" id="S5.SS4.p1.1.1">i.e., </em>textual data construction and model optimization. The textual data construction converts the recommendation data into samples with textual input and output, where the choice of input and output depends on the learning objectives.
While most of the methods can directly construct the textual data based on the pre-defined item identifier, codebook-based methods necessitate the <span class="ltx_text ltx_font_bold" id="S5.SS4.p1.1.2">item-to-identifier</span> training prior to textual data construction.
The item-to-identifier training typically utilizes RQ-VAE to map the item content representation into the quantized code words <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>)</cite> as the item identifier.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">As for model optimization, literature typically utilizes generative training in language modeling <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib65" title="">2019</a>)</cite> to optimize the model.
Specifically, given the constructed samples with the textual input and output, the generative training maximizes the log-likelihood of the target output tokens conditioned on the input.
According to the learning objectives, we divide the generative training into two groups for generative recommendation.
To recommend the item relevant to the user preference, 1) <span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">user-to-identifier</span> training is employed for generative models to learn the matching ability.
For each constructed training sample, the input is the user formulation, and the output is the next-item identifier.
The user-to-identifier training plays a crucial role in the generative recommendation and is utilized across all generative recommendation methods for item retrieval.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">As for the methods that utilize codebooks to learn semantic-aware item identifiers,
they might suffer from the semantic gap between the quantized codewords and the semantics in natural language.
To combat this issue,
2) <span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">auxiliary alignment</span> is additionally used by the methods to strengthen the alignment between the item content and the item identifier.
To achieve the alignment, extra training sample construction is required, which can be broadly divided into two groups.
(i) <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.2">Content-to-identifier</span> or <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.3">identifier-to-content</span>. For each constructed training sample, the input-output pair comprises the identifier and textual content of the same item, each serving interchangeably as input or output.
In addition to the item-wise alignment, (ii) <span class="ltx_text ltx_font_italic" id="S5.SS4.p3.1.4">user-to-content</span> is another strategy to implicitly align the item identifier and the item content by pairing the user formulation with the next-item content <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib174" title="">2023</a>)</cite>.
Despite the effectiveness of various training strategies in adapting the generative models to recommendation tasks, the training costs are usually unaffordable, especially for LLMs such as LLaMA.
To improve training efficiency, recent efforts have focused on model architecture modification <cite class="ltx_cite ltx_citemacro_citep">(Mei and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib100" title="">2023</a>)</cite> and data pruning for LLM-based recommendation <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib84" title="">2024</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib94" title="">2023</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S5.F3.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Major milestones in the development of generative recommendation regarding item identifier.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Inference</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">To achieve item recommendation, the generative models perform generation grounding during the inference stage <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite>.
Given the user formulation in natural language, the generative models first generate the item identifier autoregressively via beam search.
Here, we divide the generation into two types, <em class="ltx_emph ltx_font_italic" id="S5.SS5.p1.1.1">i.e., </em><span class="ltx_text ltx_font_bold" id="S5.SS5.p1.1.2">free generation</span> and <span class="ltx_text ltx_font_bold" id="S5.SS5.p1.1.3">constrained generation</span>.
As for the free generation, for each generation step, the model searches over the whole vocabulary and selects the top-<math alttext="K" class="ltx_Math" display="inline" id="S5.SS5.p1.1.m1.1"><semantics id="S5.SS5.p1.1.m1.1a"><mi id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><ci id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p1.1.m1.1d">italic_K</annotation></semantics></math> tokens with high probability as the subsequent input for the next step’s generation.
However, searching over the whole vocabulary would probably result in out-of-corpus identifier <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib69" title="">2023g</a>; Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite>, making the recommendation invalid.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">To address this problem, early studies utilize exact matching for grounding, which conducts free generation and simply discards the invalid identifier.
Nevertheless, they still have poor accuracy caused by the invalid identifier, especially for the textual metadata-based identifier.
To improve the accuracy, BIGRec <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>)</cite> proposes to ground the generated identifier to the valid items via L2 distance between the generated token sequences’ representations and the item representation.
As such, each generated identifier is ensured to be grounded to valid item identifiers.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">In the same period, constrained generation is also explored in generation grounding <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib21" title="">2023</a>; Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib113" title="">2023</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib174" title="">2023</a>)</cite>.
 <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib21" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib48" title="">2023c</a>)</cite> propose to utilize Trie for constrained generation, where the generated identifier is guaranteed to be valid identifiers.
However, Trie strictly generates the valid identifier from the first token, where the accuracy of the recommendation depends highly on the accuracy of the first several generated tokens.
To combat this issue, TransRec <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite> utilizes FM-index to achieve position-free constrained generation, which allows the generated token from any position of the valid identifier.
The generated valid tokens will then be grounded to the valid identifiers through aggregations from different views.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1">In addition to the typical recommendation that requires valid generation to recommend the existing items to the users,
another research direction capitalizes on the generative capabilities of models to create entirely new item <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib158" title="">2024</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib152" title="">2024</a>; Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib23" title="">2022</a>)</cite>.
For example, <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib158" title="">2024</a>)</cite> generates personalized outfits, which can serve as guidance for fashion factories.
Consequently, within this research line, free generation is employed, enabling recommender systems to fully exploit the generative potential.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6. </span>Summary</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS6.p1.1.1">Methods summary</span>.
We summarize current generative recommendation methods in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.T4" title="Table 4 ‣ 5.3. Item Identifiers ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, from the aspects of user formulation, item identifier, backbone, generation, dataset, and recommendation domain.
1) User formulation. Different components in user formulation have been discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS2" title="5.2. User Formulation ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5.2</span></a>. It is noticed that existing methods usually incorporate task description and user’s historical interactions, while some methods also leverage user profile, context information, and external knowledge as additional components to formulate user.
2) Item identifier. We summarize the characteristics of item identifiers in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.T3" title="Table 3 ‣ 5.2. User Formulation ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>, where each type of identifier meets different criteria of the item identifier as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS3" title="5.3. Item Identifiers ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5.3</span></a>.
In addition, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.T4" title="Table 4 ‣ 5.3. Item Identifiers ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>, we can find that methods that employ LLMs of larger size would usually utilize textual metadata as item identifier.
This is to leverage the rich world knowledge encoded in the LLMs, for a better understanding of user behavior and item characteristics.
With the exploration on various recommendation datasets across different domains, generative recommendation methods show great applicability and generalization ability for real-world applications.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS6.p2.1.1">Timeline summary</span>.
As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS1" title="5.1. Overview ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5.1</span></a>, item identifier is a pivotal component in generative recommendation.
To elucidate the evolution of item identifiers in the context of generative recommendation, we provide a brief timeline to outline significant milestones across four distinct types of identifiers.
For each type of identifier, we highlight the first work and enumerate several subsequent endeavors for further improvements in another dimension such as training strategies.
The earliest effort in generative recommendation is LMRecSys, which employs the generative paradigm for recommendation and utilizes the item’s textual metadata, <em class="ltx_emph ltx_font_italic" id="S5.SS6.p2.1.2">i.e., </em>title, as the item identifier.
In the year 2022, P5 introduces the numeric ID identifier and proposes a unified generative recommendation framework with multi-task training.
Subsequently, the year 2023 has witnessed various investigations of improved numeric ID-based identifiers, such as RecSysLLM with masked language modeling <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib21" title="">2023</a>)</cite>, and SEATER with tree-based numeric IDs <cite class="ltx_cite ltx_citemacro_citep">(Si et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib123" title="">2023</a>)</cite>.
In late 2023, <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite> introduces multi-facet identifier to pursue both semantics and distinctiveness.
It is highlighted that from year 2023 onwards, especially after the birth of ChatGPT, generative recommendation has garnered increased attention.
In this period, there has been extensive research work to enhance generative recommendation with the four different identifier types, including training strategies (<em class="ltx_emph ltx_font_italic" id="S5.SS6.p2.1.3">e.g., </em>GenRec <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib52" title="">2023b</a>)</cite> and BIGRec <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib5" title="">2023a</a>)</cite>), user formulation (<em class="ltx_emph ltx_font_italic" id="S5.SS6.p2.1.4">e.g., </em>InstructRec <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib169" title="">2023d</a>)</cite>), constrained generation (<em class="ltx_emph ltx_font_italic" id="S5.SS6.p2.1.5">e.g., </em>TransRec <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib83" title="">2023b</a>)</cite>), and training efficiency (<em class="ltx_emph ltx_font_italic" id="S5.SS6.p2.1.6">e.g., </em>DEALRec <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib84" title="">2024</a>)</cite>).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7. </span>LLMs for Recommendation beyond Generative Recommendation</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS7.p1.1.1">LLMs as feature extractors</span>.
In addition to the generative recommendation that autoregressively generates the item identifier,
another concurrent line of research work focuses on the utilization of LLMs in data representation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib142" title="">2024a</a>; Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib115" title="">2023a</a>; Lyu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib97" title="">2023</a>)</cite>.
Two approaches are commonly used in this line of work. 1) Utilization of LLMs to obtain augmented features for traditional recommender models <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib155" title="">2023</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib96" title="">2024a</a>; Boz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib9" title="">2024</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib129" title="">2024</a>)</cite>, and
2) incorporation of a linear projector at the last token to predict the probability scores of all items <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib70" title="">2024a</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib37" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib142" title="">2024a</a>)</cite>, which is equivalent to using the LLMs’ hidden states as user representation.
Although item identifier is not explicitly utilized, it is highlighted that this line of work constructs user formulation to obtain the user representation or augmented features.
We also include the related work into the discussion of user formulation in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS2" title="5.2. User Formulation ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS7.p2">
<p class="ltx_p" id="S5.SS7.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS7.p2.1.1">LLMs for CTR tasks</span>.
While extensive effort has been made to explore next-item generation, researchers have also investigated utilizing LLMs for the Click Through Rate (CTR) task.
The CTR task aims to predict the user-item interaction in a pointwise manner <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib24" title="">2023</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S5.SS7.p2.1.2">i.e., </em>the input is the information of the user and the target item.
To leverage LLMs for CTR tasks, existing work usually takes the user formulation and the information of target item as the input, and the target output will be set as “yes” or “no”, for the positive and negative samples, respectively.
During inference, these methods will perform softmax on the two tokens at the output layer, “yes” and “no”, and take the probability score of “yes” as the final prediction score <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib6" title="">2023b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib173" title="">2023b</a>)</cite>.
In this survey, we mainly focus on the next item generation because of its practical promise in industrial recommender systems.
It has the potential to reduce the typical multiple-stage item ranking into one stage, <em class="ltx_emph ltx_font_italic" id="S5.SS7.p2.1.3">i.e., </em>directly generating items to recommend.
As for the LLMs for CTR tasks, we also discuss them in user formulation in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S5.SS2" title="5.2. User Formulation ‣ 5. Generative Recommendation ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Difference Between Generative Search and Recommendation</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">In the preceding sections, we primarily outline the commonalities between generative search and generative recommendation, and summarize a universal framework to present the current works. However, given the distinct nature of their tasks, there are also numerous points of differentiation between the two and unique challenges to address, respectively.</p>
</div>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S6.F4.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Statistics of six representative datasets in search and recommendation. “Int.” denotes “Interactions” and “Rec.” denotes “Recommendation”. The results are based on the LLaMA2 tokenizer <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib135" title="">2023</a>)</cite>. The user formulation for generative recommendation includes task descriptions and the user’s historical interactions with item titles.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Varied input length of generative search and recommendation</span>.
Generative search typically involves short queries and minimal additional processes, while generative recommendation requires a crucial ”user formulation” step. This presents unique challenges for generative recommendation. Firstly, it is necessary to convert user information, such as task descriptions, historical interactions, and user profiles, into a textual sequence that preserves the original information as much as possible. Secondly, the user formulation often involves lengthy inputs. We statistics the average input length for generative search and recommendation on typical datasets, respectively. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S6.F4" title="Figure 4 ‣ 6.1. Difference Between Generative Search and Recommendation ‣ 6. Discussion ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> (a), the input tokens for generative recommendation are significantly greater than those for generative search. The lengthy input in generative recommendation demands significant computing resources for training and poses inference difficulties, particularly with large language models <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib84" title="">2024</a>)</cite>. Consequently, the generative recommendation must confront the challenges posed by the extensive length of the input, which is different from generative search.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Differing interaction density in generative search and recommendation</span>. One important distinction between search and recommendation tasks is the difference in interaction density. In search datasets, only a portion of documents are labeled with queries, while almost all items are interacted with by users (except for cold-start items). According to the statistics in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#S6.F4" title="Figure 4 ‣ 6.1. Difference Between Generative Search and Recommendation ‣ 6. Discussion ‣ A Survey of Generative Search and Recommendation in the Era of Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> (b), each document is associated with less than one query, while items typically have more than ten interactions on average. As previously mentioned, the generative paradigm involves memorizing documents or items in their parameters and then generating them based on input queries or users. In recommendation, the high interaction density ensures that each item can be trained, while most documents are not exposed to the generative search model. Consequently, generative recommendation can easily achieve comparable performance with traditional recommendation methods, whereas generative search struggles with low interaction density. The low interaction density presents unique challenges for generative search.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Different “semantic” meanings in generative search and recommendation</span>.
In search, the relevance between a query and a document is closely tied to their semantic similarity. However, for recommendation, an item’s content is less significant compared to search, while collaborative information holds greater importance. In essence, there are distinct “semantic” implications in search and recommendation, leading to different requirements for identifiers in generative search and recommendation. In generative search, identifiers must accurately represent the document’s content, while in generative recommendation, they should emphasize the item’s collaborative information. For instance, “learning to tokenize” a document’s content is effective in generative search <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib126" title="">2023b</a>)</cite>, but additional collaborative information must be incorporated during the tokenization stage in generative recommendation. This may be an expected research direction to further explore in generative recommendation.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Open Problems in Generative Search and Recommendation</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mrow id="S6.SS2.p1.1.m1.1.2.2"><mo id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS2.p1.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><ci id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">Document and item update in LLMs</span>.
After being trained on query-doc and use-item pairs, the LLM is capable of retrieving documents or recommending items to users by generating the corresponding identifiers. However, this functionality is dependent on the LLM having been trained to memorize the associations between the documents or items and their identifiers. Consequently, the LLM is hard to retrieve or recommend new documents and items that it has not encountered during training. While retraining the LLM on newly added documents or items is a potential solution, it necessitates a significant amount of computing resources. Given the vast number of new documents and items that are introduced daily in search and recommendation systems, it is crucial to develop effective and efficient methods for updating the LLM’s memory in order to ensure optimal generative search and recommendation performance <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib14" title="">2023a</a>; Mehta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib99" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.2.2"><mo id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS2.p2.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Multimodal and cross-modal generative search and recommendation</span>.
Only textual information cannot satisfy users’ various information needs well. To enhance the search experience, it is crucial to incorporate multimodal resources such as images, tables, audio, and videos alongside textual documents. Similarly, recommendations should encompass various types of content, like micro-videos, which necessitate a comprehensive understanding of multimodal elements. Unfortunately, the current generative paradigm predominantly relies on language models and falls short in addressing multimodal information-seeking scenarios. These scenarios include retrieving images, videos, or audio based on a query, performing generative multimodal retrieval by retrieving web pages containing both text and images, and providing multimodal recommendations that incorporate diverse features. Some recent works <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib74" title="">2024d</a>; Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib35" title="">2023</a>; Long et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib92" title="">2024</a>)</cite> have made an initial attempt, and further works are anticipated.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS2.p3.1.m1.1"><semantics id="S6.SS2.p3.1.m1.1a"><mrow id="S6.SS2.p3.1.m1.1.2.2"><mo id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS2.p3.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><ci id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">In-context learning for generative search and recommendation</span>.
One notable advantage of Language Models (LLMs) is their ability to learn in-context. With just a few examples (few-shot) or even no examples (zero-shot) provided in the prompt, LLMs can effectively solve specific tasks without the need for fine-tuning. In-context learning plays a crucial role in enabling LLMs to encompass a wider range of tasks within the same generative paradigm. However, current generative search and recommendation methods still rely on fine-tuning LLMs using domain-specific data, including query-document pairs and user-item pairs. As the size of LLMs increases, the process of fine-tuning becomes computationally expensive. Consequently, the challenge lies in finding ways to incorporate LLMs into search and recommendation domains using zero-shot or few-shot approaches, thereby reducing the need for extensive fine-tuning.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.4"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS2.p4.1.m1.1"><semantics id="S6.SS2.p4.1.m1.1a"><mrow id="S6.SS2.p4.1.m1.1.2.2"><mo id="S6.SS2.p4.1.m1.1.1" xref="S6.SS2.p4.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS2.p4.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.1.m1.1b"><ci id="S6.SS2.p4.1.m1.1.1.cmml" xref="S6.SS2.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS2.p4.4.1">Large-scale recall</span>.
Regardless of whether the objective is to provide users with a list of documents or items, generative search and recommendation methods typically employ beam search in autoregressive decoding to generate such lists instead of just a single result. To illustrate, if the beam search has a size of <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.p4.2.m2.1"><semantics id="S6.SS2.p4.2.m2.1a"><mi id="S6.SS2.p4.2.m2.1.1" xref="S6.SS2.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.2.m2.1b"><ci id="S6.SS2.p4.2.m2.1.1.cmml" xref="S6.SS2.p4.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.2.m2.1d">italic_k</annotation></semantics></math>, generative search and recommendation systems can produce a list containing <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.p4.3.m3.1"><semantics id="S6.SS2.p4.3.m3.1a"><mi id="S6.SS2.p4.3.m3.1.1" xref="S6.SS2.p4.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.3.m3.1b"><ci id="S6.SS2.p4.3.m3.1.1.cmml" xref="S6.SS2.p4.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.3.m3.1d">italic_k</annotation></semantics></math> elements. However, the efficiency of autoregressive generation diminishes as the beam size <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.p4.4.m4.1"><semantics id="S6.SS2.p4.4.m4.1a"><mi id="S6.SS2.p4.4.m4.1.1" xref="S6.SS2.p4.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p4.4.m4.1b"><ci id="S6.SS2.p4.4.m4.1.1.cmml" xref="S6.SS2.p4.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p4.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p4.4.m4.1d">italic_k</annotation></semantics></math> increases. Consequently, current generative search and recommendation systems are unable to achieve large-scale recall due to this limitation. Therefore, it is necessary to explore additional decoding strategies to enhance the performance of generative search and recommendation methods.</p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S6.F5.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Illustration of next information-seeking paradigm, <em class="ltx_emph ltx_font_italic" id="S6.F5.2.1">i.e., </em>content generation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Envision Next Information-seeking Paradigm: Content Generation</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Search and recommendation systems aim to fulfill users’ information needs by retrieving or recommending a list of documents or items from a finite set. In previous discussions, we have explored paradigm shifts in search and recommendation, particularly focusing on the latest generative paradigm. Regardless of whether the approach is based on deep learning or generative learning, the ultimate goal remains the same: matching the available content (documents or items) to users. However, there has been a recent surge in the development of generative AI and models, which has introduced a novel way for users to obtain information from the Web. Instead of merely matching existing content, generative AI models now have the capability to directly generate content. This means that the generated content, such as a document, may not have previously existed on the Web.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS3.p2.1.m1.1"><semantics id="S6.SS3.p2.1.m1.1a"><mrow id="S6.SS3.p2.1.m1.1.2.2"><mo id="S6.SS3.p2.1.m1.1.1" xref="S6.SS3.p2.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS3.p2.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p2.1.m1.1b"><ci id="S6.SS3.p2.1.m1.1.1.cmml" xref="S6.SS3.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p2.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p2.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">Search engines vs generative language models</span>. Search engines, such as Google<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.google.com/" title="">https://www.google.com/</a>.</span></span></span>, are widely used text retrieval applications that can efficiently retrieve relevant web pages from billions of websites based on user queries. Generative language models like ChatGPT <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://chat.openai.com/" title="">https://chat.openai.com/</a>.</span></span></span> and Gemini <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://gemini.google.com/" title="">https://gemini.google.com/</a>.</span></span></span> are trained on vast amounts of web documents to imbibe knowledge into their parameters. These models can then generate responses to user queries based on the instructions provided.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">As a result, users now have the option to input their queries directly into generative language models to seek information. In comparison to search engines, generative language models offer several advantages. 1) They can provide more accurate responses, as they are not limited to presenting an entire web page. 2) They can summarize content from multiple pages, which is particularly useful for complex queries. 3) Additionally, generative language models can understand users’ information needs in multi-turn conversations. However, generative language models do face certain challenges. One such challenge is the issue of information updating. Since these models are not trained on the latest documents, they may not be able to answer queries related to up-to-date information. Consequently, they may lack the corresponding knowledge. Furthermore, generative language models are prone to generating incorrect responses, a phenomenon known as hallucination <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib53" title="">2023a</a>)</cite>. Overall, while generative language models offer unique advantages in information retrieval, they also have limitations, including information updating and hallucinations, that need to be addressed.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS3.p4.1.m1.1"><semantics id="S6.SS3.p4.1.m1.1a"><mrow id="S6.SS3.p4.1.m1.1.2.2"><mo id="S6.SS3.p4.1.m1.1.1" xref="S6.SS3.p4.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS3.p4.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p4.1.m1.1b"><ci id="S6.SS3.p4.1.m1.1.1.cmml" xref="S6.SS3.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p4.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p4.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS3.p4.1.1">Image search vs image generation</span>. People have become accustomed to searching for images on the internet to fulfill their visual requirements. However, the advancements in image generation, such as the GAN <cite class="ltx_cite ltx_citemacro_citep">(Goodfellow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib36" title="">2020</a>)</cite> and diffusion models <cite class="ltx_cite ltx_citemacro_citep">(Croitoru et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib22" title="">2023</a>)</cite>, now enable individuals to directly acquire the desired images through generation rather than relying on web searches. This is especially beneficial for creative endeavors, as image generation has the potential to produce distinctive and unique visuals. Compared to image search, image generation models offer more personalized services and can create unique images based on users’ specific requirements. However, these models are also prone to the issue of generating images that do not adhere to physical rules. This means that the generated images may not accurately represent real-world objects or scenes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p5">
<p class="ltx_p" id="S6.SS3.p5.1"><math alttext="\bullet\quad" class="ltx_Math" display="inline" id="S6.SS3.p5.1.m1.1"><semantics id="S6.SS3.p5.1.m1.1a"><mrow id="S6.SS3.p5.1.m1.1.2.2"><mo id="S6.SS3.p5.1.m1.1.1" xref="S6.SS3.p5.1.m1.1.1.cmml">∙</mo><mspace id="S6.SS3.p5.1.m1.1.2.2.1" width="1.222em"></mspace></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p5.1.m1.1b"><ci id="S6.SS3.p5.1.m1.1.1.cmml" xref="S6.SS3.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p5.1.m1.1c">\bullet\quad</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p5.1.m1.1d">∙</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S6.SS3.p5.1.1">Item generation in recommendation</span>.
Traditional recommendation systems retrieve existing items from item corpus, which may not always meet users’ diverse information needs.
Nevertheless, the boom of generative models offers a promising solution to supplement human-generated content in the traditional recommendation and foster personalized AI-Generated Content (AIGC)
Specifically, item generation in recommendation can be achieved in two approaches.
1) Content repurposing, which edits the existing items tailored to individual user preference for personalized recommendation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib145" title="">2023b</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S6.SS3.p5.1.2">e.g., </em>transfer a micro-video in a cartoon style.
2) Content creation, which generates personalized new items to satisfy user-specific preference and intent <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib23" title="">2022</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib158" title="">2024</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib152" title="">2024</a>; Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16924v1#bib.bib121" title="">2024</a>)</cite>.
By leveraging these two approaches, personalized item generation enhances information seeking by providing personalized open-world content that aligns closely with individual user preferences.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Bridging the semantic gap between two matching entities is a fundamental and challenging issue in search and recommendation. In this survey, we provide an overview of three paradigms aimed at solving this core research problem from a unified perspective. Our focus is primarily on the recent generative paradigm for search and recommendation, known as generative search and recommendation. We provide a comprehensive overview of a unified framework for generative search and recommendation, and discuss the current research contributions within this framework. In addition to categorizing the current works, we offer valuable insights into the generative paradigm, including an analysis of the strengths and weaknesses of various designs, such as different identifiers, training methods, and inference approaches. By adopting a unified view of search and recommendation, we highlight the commonalities and unique challenges of generative search and recommendation. Furthermore, we engage in a thorough discussion about the future outlook of the next information-seeking paradigm and the open problems in this field.</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adomavicius and Tuzhilin (2005)</span>
<span class="ltx_bibblock">
Gediminas Adomavicius and Alexander Tuzhilin. 2005.

</span>
<span class="ltx_bibblock">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE transactions on knowledge and data engineering</em> 17, 6 (2005), 734–749.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and Kilian Weinberger. 2009.

</span>
<span class="ltx_bibblock">Supervised semantic indexing. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 18th ACM conference on Information and knowledge management</em>. 187–196.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and Kilian Weinberger. 2010.

</span>
<span class="ltx_bibblock">Learning to rank with (a lot of) word features.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Information retrieval</em> 13 (2010), 291–314.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023a.

</span>
<span class="ltx_bibblock">A bi-step grounding paradigm for large language models in recommendation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv:2308.08434</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023b.

</span>
<span class="ltx_bibblock">Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">RecSys</em>. ACM, 1007–1014.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Belkin and Croft (1992)</span>
<span class="ltx_bibblock">
Nicholas J Belkin and W Bruce Croft. 1992.

</span>
<span class="ltx_bibblock">Information filtering and information retrieval: Two sides of the same coin?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Commun. ACM</em> 35, 12 (1992), 29–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022.

</span>
<span class="ltx_bibblock">Autoregressive search engines: Generating substrings as document identifiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 31668–31683.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boz et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Artun Boz, Wouter Zorgdrager, Zoe Kotti, Jesse Harte, Panos Louridas, Dietmar Jannach, and Marios Fragkoulis. 2024.

</span>
<span class="ltx_bibblock">Improving Sequential Recommendations with LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">arXiv:2402.01339</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Breese et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (1998)</span>
<span class="ltx_bibblock">
John S Breese, David Heckerman, and Carl Kadie. 1998.

</span>
<span class="ltx_bibblock">Empirical analysis of predictive algorithms for collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">UAI</em>. 43–52.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burges et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005.

</span>
<span class="ltx_bibblock">Learning to rank using gradient descent. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 22nd international conference on Machine learning</em>. 89–96.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuwei Cao, Nikhil Mehta, Xinyang Yi, Raghunandan Keshavan, Lukasz Heldt, Lichan Hong, Ed H Chi, and Maheswaran Sathiamoorthy. 2024.

</span>
<span class="ltx_bibblock">Aligning Large Language Models with Recommendation Knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">arXiv:2404.00245</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007.

</span>
<span class="ltx_bibblock">Learning to rank: from pairwise approach to listwise approach. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Proceedings of the 24th international conference on Machine learning</em>. 129–136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, Yixing Fan, and Xueqi Cheng. 2023a.

</span>
<span class="ltx_bibblock">Continual learning for generative retrieval over dynamic corpora. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>. 306–315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yiqun Liu, Yixing Fan, and Xueqi Cheng. 2023b.

</span>
<span class="ltx_bibblock">A Unified Generative Retriever for Knowledge-Intensive Language Tasks via Prompt Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:2304.14856</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2022a.

</span>
<span class="ltx_bibblock">GERE: Generative evidence retrieval for fact verification. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 2184–2189.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yiqun Liu, Yixing Fan, and Xueqi Cheng. 2022b.

</span>
<span class="ltx_bibblock">CorpusBrain: Pre-train a Generative Retrieval Model for Knowledge-Intensive Language Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 191–200.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xu Chen, Yongfeng Zhang, and Zheng Qin. 2019.

</span>
<span class="ltx_bibblock">Dynamic explainable recommendation based on neural attentive models. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">AAAI</em>, Vol. 33. 53–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2023)</span>
<span class="ltx_bibblock">
Zheng Chen. 2023.

</span>
<span class="ltx_bibblock">PALR: Personalization Aware LLMs for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv:2305.07622</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu and Keerthi (2005)</span>
<span class="ltx_bibblock">
Wei Chu and S Sathiya Keerthi. 2005.

</span>
<span class="ltx_bibblock">New approaches to support vector ordinal regression. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 22nd international conference on Machine learning</em>. 145–152.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, et al<span class="ltx_text" id="bib.bib21.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Leveraging large language models for pre-trained recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.4.1">arXiv:2308.10837</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Croitoru et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. 2023.

</span>
<span class="ltx_bibblock">Diffusion models in vision: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022.

</span>
<span class="ltx_bibblock">M6-rec: Generative pretrained language models are open-ended recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv:2205.08084</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023.

</span>
<span class="ltx_bibblock">Uncovering chatgpt’s capabilities in recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">RecSys</em>. ACM, 1126–1132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Cao et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020.

</span>
<span class="ltx_bibblock">Autoregressive Entity Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deldjoo et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott Sanner, Arnau Ramisa, René Vidal, Maheswaran Sathiamoorthy, Atoosa Kasirzadeh, and Silvia Milano. 2024.

</span>
<span class="ltx_bibblock">A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:2404.00579</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Palma (2023)</span>
<span class="ltx_bibblock">
Dario Di Palma. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented recommender system: Enhancing recommender systems with large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">RecSys</em>. 1369–1373.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yingpeng Du, Ziyan Wang, Zhu Sun, Haoyan Chua, Hongzhi Liu, Zhonghai Wu, Yining Ma, Jie Zhang, and Youchen Sun. 2024.

</span>
<span class="ltx_bibblock">Large Language Model with Graph Convolution for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv:2402.08859</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferragina and Manzini (2000)</span>
<span class="ltx_bibblock">
Paolo Ferragina and Giovanni Manzini. 2000.

</span>
<span class="ltx_bibblock">Opportunistic data structures with applications. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings 41st annual symposium on foundations of computer science</em>. IEEE, 390–398.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freund et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2003)</span>
<span class="ltx_bibblock">
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003.

</span>
<span class="ltx_bibblock">An efficient boosting algorithm for combining preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Journal of machine learning research</em> 4, Nov (2003), 933–969.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023.

</span>
<span class="ltx_bibblock">Chat-rec: Towards interactive and explainable llms-augmented recommender system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv:2303.14524</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia-Molina et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2011)</span>
<span class="ltx_bibblock">
Hector Garcia-Molina, Georgia Koutrika, and Aditya Parameswaran. 2011.

</span>
<span class="ltx_bibblock">Information seeking: convergence of search, recommendations, and advertising.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Commun. ACM</em> 54, 11 (2011), 121–130.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5). In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">RecSys</em>. 299–315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">VIP5: Towards Multimodal Foundation Models for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">EMNLP</em>. 9606–9620.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goodfellow et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Commun. ACM</em> 63, 11 (2020), 139–144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Naicheng Guo, Hongwei Cheng, Qianqiao Liang, Linxun Chen, and Bing Han. 2024.

</span>
<span class="ltx_bibblock">Integrating Large Language Models with Graphical Session-Based Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv:2402.16539</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harte et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023.

</span>
<span class="ltx_bibblock">Leveraging large language models for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">RecSys</em>. ACM, 1096–1102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020.

</span>
<span class="ltx_bibblock">Lightgcn: Simplifying and powering graph convolution network for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">SIGIR</em>. 639–648.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017.

</span>
<span class="ltx_bibblock">Neural Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">WWW</em>. ACM, 173–182.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herlocker et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (1999)</span>
<span class="ltx_bibblock">
Jonathan L Herlocker, Joseph A Konstan, Al Borchers, and John Riedl. 1999.

</span>
<span class="ltx_bibblock">An algorithmic framework for performing collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">SIGIR</em>. ACM, 230–237.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Neural computation</em> 9, 8 (1997), 1735–1780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yupeng Hou, Zhankui He, Julian McAuley, and Wayne Xin Zhao. 2023a.

</span>
<span class="ltx_bibblock">Learning vector-quantized item representation for transferable sequential recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">WWW</em>. 1162–1171.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023b.

</span>
<span class="ltx_bibblock">Large language models are zero-shot rankers for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">arXiv:2305.08845</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014.

</span>
<span class="ltx_bibblock">Convolutional neural network architectures for matching natural language sentences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Advances in neural information processing systems</em> 27 (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, and Yongfeng Zhang. 2023a.

</span>
<span class="ltx_bibblock">Up5: Unbiased foundation model for fairness-aware recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">arXiv:2305.12090</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Wenyue Hua, Lei Li, Shuyuan Xu, Li Chen, and Yongfeng Zhang. 2023b.

</span>
<span class="ltx_bibblock">Tutorial on Large Language Models for Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 1281–1283.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023c.

</span>
<span class="ltx_bibblock">How to Index Item IDs for Recommendation Foundation Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">arXiv:2305.06569</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Jiangping Huang, Shuxin Yao, Chen Lyu, and Donghong Ji. 2017.

</span>
<span class="ltx_bibblock">Multi-granularity neural sentence model for measuring short text similarity. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Database Systems for Advanced Applications: 22nd International Conference, DASFAA 2017, Suzhou, China, March 27-30, 2017, Proceedings, Part I 22</em>. Springer, 439–455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013.

</span>
<span class="ltx_bibblock">Learning deep structured semantic models for web search using clickthrough data. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>. 2333–2338.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jagerman et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023.

</span>
<span class="ltx_bibblock">Query expansion by prompting large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">arXiv preprint arXiv:2305.03653</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, and Yongfeng Zhang. 2023b.

</span>
<span class="ltx_bibblock">Genrec: Large language model for generative recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">arXiv e-prints</em> (2023), arXiv–2307.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023a.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Comput. Surveys</em> 55, 12 (2023), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kabbur et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Santosh Kabbur, Xia Ning, and George Karypis. 2013.

</span>
<span class="ltx_bibblock">Fism: factored item similarity models for top-n recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 659–667.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang and McAuley (2018)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang and Julian McAuley. 2018.

</span>
<span class="ltx_bibblock">Self-attentive sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">ICDM</em>. IEEE, 197–206.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">arXiv preprint arXiv:2004.04906</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Minchul Yang, and Chanyoung Park. 2024.

</span>
<span class="ltx_bibblock">Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">arXiv:2404.11343</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kobayashi and Takeda (2000)</span>
<span class="ltx_bibblock">
Mei Kobayashi and Koichi Takeda. 2000.

</span>
<span class="ltx_bibblock">Information retrieval on the web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ACM computing surveys (CSUR)</em> 32, 2 (2000), 144–173.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koren et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.

</span>
<span class="ltx_bibblock">Matrix factorization techniques for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Computer</em> 42, 8 (2009), 30–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (1998)</span>
<span class="ltx_bibblock">
Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998.

</span>
<span class="ltx_bibblock">Gradient-based learning applied to document recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proc. IEEE</em> 86, 11 (1998), 2278–2324.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. 2022a.

</span>
<span class="ltx_bibblock">Autoregressive image generation using residual quantization. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 11523–11532.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Seung (2000)</span>
<span class="ltx_bibblock">
Daniel Lee and H Sebastian Seung. 2000.

</span>
<span class="ltx_bibblock">Algorithms for non-negative matrix factorization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Advances in neural information processing systems</em> 13 (2000).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022b.

</span>
<span class="ltx_bibblock">Generative multi-hop retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>. 1417–1436.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunkyung Lee, Minjin Choi, and Jongwuk Lee. 2023.

</span>
<span class="ltx_bibblock">GLEN: Generative Retrieval via Lexical Index Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. 7693–7704.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">arXiv preprint arXiv:1910.13461</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li (2022)</span>
<span class="ltx_bibblock">
Hang Li. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Learning to rank for information retrieval and natural language processing</em>.

</span>
<span class="ltx_bibblock">Springer Nature.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2023h)</span>
<span class="ltx_bibblock">
Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023h.

</span>
<span class="ltx_bibblock">GPT4Rec: A generative framework for personalized recommendation and user interests interpretation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">arXiv:2304.03879</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Lei Li, Yongfeng Zhang, and Li Chen. 2023d.

</span>
<span class="ltx_bibblock">Prompt distillation for efficient llm-based recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">CIKM</em>. 1348–1357.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2023g)</span>
<span class="ltx_bibblock">
Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023g.

</span>
<span class="ltx_bibblock">Large language models for generative recommendation: A survey and visionary discussions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">arXiv:2309.01157</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2024a.

</span>
<span class="ltx_bibblock">E4SRec: An elegant effective efficient extensible solution of large language models for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">WWW</em>. ACM.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Xiaoxi Li, Zhicheng Dou, Yujia Zhou, and Fangchao Liu. 2024b.

</span>
<span class="ltx_bibblock">Towards a Unified Language Model for Knowledge-Intensive Tasks Utilizing External Corpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">arXiv preprint arXiv:2402.01176</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. 2023e.

</span>
<span class="ltx_bibblock">PBNR: Prompt-based News Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">arXiv:2304.07862</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2023f)</span>
<span class="ltx_bibblock">
Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. 2023f.

</span>
<span class="ltx_bibblock">A Preliminary Study of ChatGPT on News Recommendation: Personalization, Provider Fairness, Fake News.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">arXiv:2306.10702</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024d.

</span>
<span class="ltx_bibblock">Generative Cross-Modal Retrieval: Memorizing Images in Multimodal Language Models for Retrieval and Beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">arXiv preprint arXiv:2402.10805</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023a.

</span>
<span class="ltx_bibblock">Generative retrieval for conversational question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">Information Processing &amp; Management</em> 60, 5 (2023), 103475.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023b.

</span>
<span class="ltx_bibblock">Learning to rank in generative retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">arXiv preprint arXiv:2306.15222</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023c.

</span>
<span class="ltx_bibblock">Multiview Identifiers Enhanced Generative Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 6636–6648.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Yongqi Li, Zhen Zhang, Wenjie Wang, Liqiang Nie, Wenjie Li, and Tat-Seng Chua. 2024e.

</span>
<span class="ltx_bibblock">Distillation Enhanced Generative Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">arXiv preprint arXiv:2402.10769</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Zelong Li, Jianchao Ji, Yingqiang Ge, Wenyue Hua, and Yongfeng Zhang. 2024c.

</span>
<span class="ltx_bibblock">PAP-REC: Personalized Automatic Prompt for Recommendation Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">arXiv:2402.00284</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock">Llara: Aligning large language models with sequential recommenders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">arXiv:2312.02445</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Zhang (2023)</span>
<span class="ltx_bibblock">
Guo Lin and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general recommender (agr): Early experiments with chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">arXiv:2305.04518</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al<span class="ltx_text" id="bib.bib82.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">How can recommender systems benefit from large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.4.1">arXiv preprint arXiv:2306.05817</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua. 2023b.

</span>
<span class="ltx_bibblock">A multi-facet paradigm to bridge large language model and recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">arXiv:2310.06491</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2024.

</span>
<span class="ltx_bibblock">Data-efficient Fine-tuning for LLM-based Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">SIGIR</em>. ACM.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Linden et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2003)</span>
<span class="ltx_bibblock">
Greg Linden, Brent Smith, and Jeremy York. 2003.

</span>
<span class="ltx_bibblock">Amazon. com recommendations: Item-to-item collaborative filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">IEEE Internet computing</em> 7, 1 (2003), 76–80.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Aonghus Lawlor, Ruihai Dong, and Irene Li. 2023c.

</span>
<span class="ltx_bibblock">RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">arXiv:2312.10463</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023a.

</span>
<span class="ltx_bibblock">Is chatgpt a good recommender? a preliminary study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">arXiv:2304.10149</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, et al<span class="ltx_text" id="bib.bib88.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Llmrec: Benchmarking large language models on recommendation task.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.4.1">arXiv:2308.12241</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Zijian Zhang, Feng Tian, and Yefeng Zheng. 2024b.

</span>
<span class="ltx_bibblock">Large Language Model Distilling Medication Recommendation Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">arXiv:2402.02803</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Tie-Yan Liu et al<span class="ltx_text" id="bib.bib90.3.1">.</span> 2009.

</span>
<span class="ltx_bibblock">Learning to rank for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.4.1">Foundations and Trends® in Information Retrieval</em> 3, 3 (2009), 225–331.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yuqing Liu, Yu Wang, Lichao Sun, and Philip S Yu. 2024a.

</span>
<span class="ltx_bibblock">Rec-GPT4V: Multimodal Recommendation with Large Vision-Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">arXiv:2402.08670</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou. 2024.

</span>
<span class="ltx_bibblock">Generative Multi-Modal Knowledge Retrieval with Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">arXiv preprint arXiv:2401.08206</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao Liao, and Xing Xie. 2024.

</span>
<span class="ltx_bibblock">Aligning Large Language Models for Controllable Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">arXiv:2403.05063</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou, Zongpeng Li, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023.

</span>
<span class="ltx_bibblock">RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">arXiv:2312.16018</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Sichun Luo, Yuxuan Yao, Bowei He, Yinya Huang, Aojun Zhou, Xinyi Zhang, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2024b.

</span>
<span class="ltx_bibblock">Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">arXiv:2401.13870</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Weiqing Luo, Chonggang Song, Lingling Yi, and Gong Cheng. 2024a.

</span>
<span class="ltx_bibblock">KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">arXiv:2403.06642</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. 2023.

</span>
<span class="ltx_bibblock">Llm-rec: Personalized recommendation via prompting large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">arXiv:2307.15780</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning llama for multi-stage text retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">arXiv preprint arXiv:2310.08319</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q Tran, Jinfeng Rao, Marc Najork, Emma Strubell, and Donald Metzler. 2022.

</span>
<span class="ltx_bibblock">DSI++: Updating Transformer Memory with New Documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">arXiv preprint arXiv:2212.09744</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mei and Zhang (2023)</span>
<span class="ltx_bibblock">
Kai Mei and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">LightLM: A Lightweight Deep and Narrow Language Model for Generative Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">arXiv:2310.17488</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024.

</span>
<span class="ltx_bibblock">Generative representational instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">arXiv preprint arXiv:2402.09906</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeem et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Usama Nadeem, Noah Ziems, and Shaoen Wu. 2022.

</span>
<span class="ltx_bibblock">Codedsi: Differentiable code search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">arXiv preprint arXiv:2210.00328</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nallapati (2004)</span>
<span class="ltx_bibblock">
Ramesh Nallapati. 2004.

</span>
<span class="ltx_bibblock">Discriminative models for information retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</em>. 64–71.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Palangi et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab Ward. 2016.

</span>
<span class="ltx_bibblock">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 24, 4 (2016), 694–707.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petrov and Macdonald (2023)</span>
<span class="ltx_bibblock">
Aleksandr V Petrov and Craig Macdonald. 2023.

</span>
<span class="ltx_bibblock">Generative sequential recommendation with gptrec.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">arXiv:2306.11114</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ronak Pradeep, Kai Hui, Jai Gupta, Adam D Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler, and Vinh Q Tran. 2023.

</span>
<span class="ltx_bibblock">How Does Generative Retrieval Scale to Millions of Passages?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">arXiv preprint arXiv:2305.11841</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Tao Qin, Xu-Dong Zhang, Ming-Feng Tsai, De-Sheng Wang, Tie-Yan Liu, and Hang Li. 2008.

</span>
<span class="ltx_bibblock">Query-level loss functions for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">Information Processing &amp; Management</em> 44, 2 (2008), 838–855.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Tao Qin, Xu-Dong Zhang, De-Sheng Wang, Tie-Yan Liu, Wei Lai, and Hang Li. 2007.

</span>
<span class="ltx_bibblock">Ranking with multiple hyperplanes. In <em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</em>. 279–286.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al<span class="ltx_text" id="bib.bib109.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Large language models are effective text rankers with pairwise ranking prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.4.1">arXiv preprint arXiv:2306.17563</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junyan Qiu, Haitao Wang, Zhaolin Hong, Yiping Yang, Qiang Liu, and Xingxing Wang. 2023.

</span>
<span class="ltx_bibblock">ControlRec: Bridging the semantic gap between language model and personalized recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">arXiv:2311.16441</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2020.

</span>
<span class="ltx_bibblock">RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">arXiv preprint arXiv:2010.08191</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.3.1">Journal of machine learning research</em> 21, 140 (2020), 1–67.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajput et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al<span class="ltx_text" id="bib.bib113.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Recommender Systems with Generative Retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib113.4.1">NeurIPS</em>. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ruiyang Ren, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023b.

</span>
<span class="ltx_bibblock">TOME: A Two-stage Approach for Model-based Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.3.1">arXiv preprint arXiv:2305.11161</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023a.

</span>
<span class="ltx_bibblock">Representation learning with large language models for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">arXiv:2310.15950</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle (2010)</span>
<span class="ltx_bibblock">
Steffen Rendle. 2010.

</span>
<span class="ltx_bibblock">Factorization Machines. In <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">ICDM</em>. IEEE, 995–1000.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009.

</span>
<span class="ltx_bibblock">BPR: Bayesian personalized ranking from implicit feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib117.3.1">UAI</em>. AUAI Press, 452–461.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarwar et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2001)</span>
<span class="ltx_bibblock">
Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001.

</span>
<span class="ltx_bibblock">Item-based collaborative filtering recommendation algorithms. In <em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">WWW</em>. ACM, 285–295.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scarselli et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008.

</span>
<span class="ltx_bibblock">The graph neural network model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">IEEE transactions on neural networks</em> 20, 1 (2008), 61–80.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shashua and Levin (2002)</span>
<span class="ltx_bibblock">
Amnon Shashua and Anat Levin. 2002.

</span>
<span class="ltx_bibblock">Ranking with large margin principle: Two approaches.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">Advances in neural information processing systems</em> 15 (2002).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, and Xi Xiao. 2024.

</span>
<span class="ltx_bibblock">PMG: Personalized Multimodal Generation with Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">WWW</em>. ACM.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014.

</span>
<span class="ltx_bibblock">A latent semantic model with convolutional-pooling structure for information retrieval. In <em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">Proceedings of the 23rd ACM international conference on conference on information and knowledge management</em>. 101–110.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zihua Si, Zhongxiang Sun, Jiale Chen, Guozhang Chen, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu. 2023.

</span>
<span class="ltx_bibblock">Generative Retrieval with Semantic Tree-Structured Item Identifiers via Contrastive Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">arXiv:2309.13375</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Socher et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013.

</span>
<span class="ltx_bibblock">Reasoning with neural tensor networks for knowledge base completion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.3.1">Advances in neural information processing systems</em> 26 (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib125.3.1">CIKM</em>. ACM, 1441–1450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan, Zheng Chen, Shuaiqiang Wang, Haichao Zhu, Pengjie Ren, Zhumin Chen, Dawei Yin, Maarten de Rijke, and Zhaochun Ren. 2023b.

</span>
<span class="ltx_bibblock">Learning to Tokenize for Generative Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.3.1">arXiv preprint arXiv:2304.04171</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023c.

</span>
<span class="ltx_bibblock">Is chatgpt good at search? investigating large language models as re-ranking agent.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">arXiv preprint arXiv:2304.09542</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhu Sun, Hongyang Liu, Xinghua Qu, Kaidong Feng, Yan Wang, and Yew-Soon Ong. 2023a.

</span>
<span class="ltx_bibblock">Large Language Models for Intent-Driven Session Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">arXiv:2312.07552</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhongxiang Sun, Zihua Si, Xiaoxue Zang, Kai Zheng, Yang Song, Xiao Zhang, and Jun Xu. 2024.

</span>
<span class="ltx_bibblock">Large Language Models Enhanced Collaborative Filtering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.3.1">arXiv:2403.17688</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, and Yongfeng Zhang. 2024.

</span>
<span class="ltx_bibblock">Towards LLM-RecSys Alignment with Textual ID Learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">arXiv:2403.19021</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang and Wang (2018)</span>
<span class="ltx_bibblock">
Jiaxi Tang and Ke Wang. 2018.

</span>
<span class="ltx_bibblock">Personalized top-n sequential recommendation via convolutional sequence embedding. In <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">WSDM</em>. ACM, 565–573.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yubao Tang, Ruqing Zhang, Jiafeng Guo, Jiangui Chen, Zuowei Zhu, Shuaiqiang Wang, Dawei Yin, and Xueqi Cheng. 2023.

</span>
<span class="ltx_bibblock">Semantic-Enhanced Differentiable Search Index Inspired by Learning Strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.3.1">arXiv preprint arXiv:2305.15115</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Yubao Tang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Wei Chen, and Xueqi Cheng. [n. d.].

</span>
<span class="ltx_bibblock">Listwise Generative Retrieval Models via a Sequential Learning Process.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">ACM Transactions on Information Systems</em> ([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al<span class="ltx_text" id="bib.bib134.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Transformer memory as a differentiable search index.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib134.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 21831–21843.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al<span class="ltx_text" id="bib.bib135.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.4.1">arXiv:2302.13971</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsai et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Ming-Feng Tsai, Tie-Yan Liu, Tao Qin, Hsin-Hsi Chen, and Wei-Ying Ma. 2007.

</span>
<span class="ltx_bibblock">Frank: a ranking method with fidelity loss. In <em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</em>. 383–390.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Uyangoda et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Lasitha Uyangoda, Supunmali Ahangama, and Tharindu Ranasinghe. 2018.

</span>
<span class="ltx_bibblock">User profile feature-based approach to address the cold start problem in collaborative filtering for personalized movie recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib137.3.1">ICDIM</em>. IEEE, 24–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Den Oord et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Aaron Van Den Oord, Oriol Vinyals, et al<span class="ltx_text" id="bib.bib138.3.1">.</span> 2017.

</span>
<span class="ltx_bibblock">Neural discrete representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.4.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vats et al<span class="ltx_text" id="bib.bib140.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Arpita Vats, Vinija Jain, Rahul Raja, and Aman Chadha. 2024.

</span>
<span class="ltx_bibblock">Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.3.1">arXiv preprint arXiv:2402.18590</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Dui Wang, Xiangyu Hou, Xiaohui Yang, Bo Zhang, Renbing Chen, and Daiyue Xue. 2023a.

</span>
<span class="ltx_bibblock">Multiple Key-value Strategy in Recommendation Systems Incorporating Large Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">arXiv:2310.16409</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Hanbing Wang, Xiaorui Liu, Wenqi Fan, Xiangyu Zhao, Venkataramana Kini, Devendra Yadav, Fei Wang, Zhen Wen, Jiliang Tang, and Hui Liu. 2024a.

</span>
<span class="ltx_bibblock">Rethinking Large Language Model Architectures for Sequential Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.3.1">arXiv:2402.09543</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Lim (2023)</span>
<span class="ltx_bibblock">
Lei Wang and Ee-Peng Lim. 2023.

</span>
<span class="ltx_bibblock">Zero-Shot Next-Item Recommendation using Large Pretrained Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">arXiv:2304.03153</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Liang Wang, Nan Yang, and Furu Wei. 2023d.

</span>
<span class="ltx_bibblock">Query2doc: Query expansion with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.3.1">arXiv preprint arXiv:2303.07678</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua. 2023b.

</span>
<span class="ltx_bibblock">Generative recommendation: Towards next-generation recommender paradigm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">arXiv:2304.03516</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.

</span>
<span class="ltx_bibblock">Neural Graph Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">SIGIR</em>. ACM, 165–174.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xuanhui Wang, Cheng Li, Nadav Golbandi, Michael Bendersky, and Marc Najork. 2018.

</span>
<span class="ltx_bibblock">The lambdaloss framework for ranking metric optimization. In <em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">Proceedings of the 27th ACM international conference on information and knowledge management</em>. 1313–1322.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yujing Wang, Yingyan Hou, Haonan Wang, Ziming Miao, Shibin Wu, Qi Chen, Yuqing Xia, Chengmin Chi, Guoshuai Zhao, Zheng Liu, et al<span class="ltx_text" id="bib.bib148.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">A neural corpus indexer for document retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 25600–25614.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Yu Wang, Zhiwei Liu, Jianguo Zhang, Weiran Yao, Shelby Heinecke, and Philip S Yu. 2023c.

</span>
<span class="ltx_bibblock">Drdt: Dynamic reflection with divergent thinking for llm-based sequential recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.3.1">arXiv:2312.11336</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yidan Wang, Zhaochun Ren, Weiwei Sun, Jiyuan Yang, Zhixiang Liang, Xin Chen, Ruobing Xie, Su Yan, Xu Zhang, Pengjie Ren, et al<span class="ltx_text" id="bib.bib150.3.1">.</span> 2024b.

</span>
<span class="ltx_bibblock">Enhanced Generative Recommendation via Content and Collaboration Integration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.4.1">arXiv:2403.18480</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Zihan Wang, Yujia Zhou, Yiteng Tu, and Zhicheng Dou. 2023e.

</span>
<span class="ltx_bibblock">NOVO: Learnable and Interpretable Document Identifiers for Model-Based IR. In <em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>. 2656–2665.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, et al<span class="ltx_text" id="bib.bib152.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Towards Universal Multi-Modal Personalization: A Language Model Empowered Generative Paradigm. In <em class="ltx_emph ltx_font_italic" id="bib.bib152.4.1">ICLR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al<span class="ltx_text" id="bib.bib153.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">A survey on large language models for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.4.1">arXiv preprint arXiv:2305.19860</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib154.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Wei Wu, Zhengdong Lu, and Hang Li. 2013.

</span>
<span class="ltx_bibblock">Learning Bilinear Model for Matching Queries and Documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.3.1">Journal of Machine Learning Research</em> 14 (2013).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023.

</span>
<span class="ltx_bibblock">Towards open-world recommendation with knowledge augmentation from large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.3.1">arXiv:2306.10933</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008.

</span>
<span class="ltx_bibblock">Listwise approach to learning to rank: theory and algorithm. In <em class="ltx_emph ltx_font_italic" id="bib.bib156.3.1">Proceedings of the 25th international conference on Machine learning</em>. 1192–1199.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib157.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jun Xu, Xiangnan He, and Hang Li. 2020.

</span>
<span class="ltx_bibblock">Deep Learning for Matching in Search and Recommendation.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yiyan Xu, Wenjie Wang, Fuli Feng, Yunshan Ma, Jizhi Zhang, and Xiangnan He. 2024.

</span>
<span class="ltx_bibblock">DiFashion: Towards Personalized Outfit Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib158.3.1">SIGIR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Hong-Jian Xue, Xinyu Dai, Jianbing Zhang, Shujian Huang, and Jiajun Chen. 2017.

</span>
<span class="ltx_bibblock">Deep matrix factorization models for recommender systems.. In <em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">IJCAI</em>, Vol. 17. Melbourne, Australia, 3203–3209.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Tianchi Yang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, and Qi Zhang. 2023.

</span>
<span class="ltx_bibblock">Auto Search Indexer for End-to-End Document Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">arXiv preprint arXiv:2310.12455</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bin Yin, Junjie Xie, Yu Qin, Zixiang Ding, Zhichao Feng, Xiang Li, and Wei Lin. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous knowledge fusion: A novel approach for personalized recommendation via llm. In <em class="ltx_emph ltx_font_italic" id="bib.bib161.3.1">RecSys</em>. 599–601.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, and Zhongrui Ma. 2024.

</span>
<span class="ltx_bibblock">RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib162.3.1">arXiv:2402.04527</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023.

</span>
<span class="ltx_bibblock">LlamaRec: Two-stage recommendation using large language models for ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.3.1">arXiv:2311.02089</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeghidour et al<span class="ltx_text" id="bib.bib164.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021.

</span>
<span class="ltx_bibblock">Soundstream: An end-to-end neural audio codec.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.3.1">TASLP</em> 30 (2021), 495–507.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib165.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hansi Zeng, Chen Luo, Bowen Jin, Sheikh Muhammad Sarwar, Tianxin Wei, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Scalable and Effective Generative Information Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib165.3.1">arXiv preprint arXiv:2311.09134</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianyang Zhai, Xiawu Zheng, Chang-Dong Wang, Hui Li, and Yonghong Tian. 2023.

</span>
<span class="ltx_bibblock">Knowledge prompt-tuning for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib166.3.1">MM</em>. ACM, 6451–6461.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib167.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hailin Zhang, Yujing Wang, Qi Chen, Ruiheng Chang, Ting Zhang, Ziming Miao, Yingyan Hou, Yang Ding, Xupeng Miao, Haonan Wang, et al<span class="ltx_text" id="bib.bib167.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Model-enhanced Vector Index.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib168.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023a.

</span>
<span class="ltx_bibblock">Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.3.1">arXiv:2305.07609</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib169.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023d.

</span>
<span class="ltx_bibblock">Recommendation as instruction following: A large language model empowered recommendation approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.3.1">arXiv:2305.07001</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib170.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Peitian Zhang, Zheng Liu, Yujia Zhou, Zhicheng Dou, and Zhao Cao. 2023c.

</span>
<span class="ltx_bibblock">Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search Engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib170.3.1">arXiv preprint arXiv:2305.13859</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Yongfeng Zhang, Qingyao Ai, Xu Chen, and W Bruce Croft. 2017.

</span>
<span class="ltx_bibblock">Joint representation learning for top-n recommendation with heterogeneous information sources. In <em class="ltx_emph ltx_font_italic" id="bib.bib171.3.1">CIKM</em>. 1449–1458.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021.

</span>
<span class="ltx_bibblock">Language models as recommender systems: Evaluations and limitations.

</span>
<span class="ltx_bibblock">(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib173.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023b.

</span>
<span class="ltx_bibblock">Collm: Integrating collaborative embeddings into large language models for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.3.1">arXiv:2310.19488</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">Adapting large language models by integrating collaborative semantics for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib174.3.1">arXiv:2311.09049</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhi Zheng, Wenshuo Chao, Zhaopeng Qiu, Hengshu Zhu, and Hui Xiong. 2024.

</span>
<span class="ltx_bibblock">Harnessing Large Language Models for Text-Rich Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib175.3.1">WWW</em>. ACM.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhiyuli et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Aakas Zhiyuli, Yanfang Chen, Xuan Zhang, and Xun Liang. 2023.

</span>
<span class="ltx_bibblock">BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.3.1">arXiv:2305.15673</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yujia Zhou, Zhicheng Dou, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">Enhancing Generative Retrieval with Reinforcement Learning from Relevance Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib177.3.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib178.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, and Ji-Rong Wen. 2022a.

</span>
<span class="ltx_bibblock">Dynamicretriever: A pre-training model-based IR system with neither sparse nor dense index.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.3.1">arXiv preprint arXiv:2203.00537</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib179.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yujia Zhou, Jing Yao, Zhicheng Dou, Ledell Wu, Peitian Zhang, and Ji-Rong Wen. 2022b.

</span>
<span class="ltx_bibblock">Ultron: An ultimate retriever on corpus with a model-based indexer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib179.3.1">arXiv preprint arXiv:2208.09257</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022.

</span>
<span class="ltx_bibblock">Bridging the gap between indexing and retrieval for differentiable search index with query generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.3.1">arXiv preprint arXiv:2206.10128</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Noah Ziems, Wenhao Yu, Zhihan Zhang, and Meng Jiang. 2023.

</span>
<span class="ltx_bibblock">Large Language Models are Built-in Autoregressive Search Engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib181.3.1">arXiv preprint arXiv:2305.09612</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr 30 18:30:43 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
