<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.14987] Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios</title><meta property="og:description" content="Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes.
The advancements in large language models (L…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.14987">

<!--Generated on Thu Feb 29 06:05:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Investigating Table-to-Text Generation Capabilities of LLMs in
<br class="ltx_break">Real-World Information Seeking Scenarios</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yilun Zhao  <sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">1</span></sup>  Haowei Zhang<math id="id2.2.m2.1" class="ltx_Math" alttext="{}^{*~{}2}" display="inline"><semantics id="id2.2.m2.1a"><msup id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml"><mi id="id2.2.m2.1.1a" xref="id2.2.m2.1.1.cmml"></mi><mrow id="id2.2.m2.1.1.1" xref="id2.2.m2.1.1.1.cmml"><mi id="id2.2.m2.1.1.1.2" xref="id2.2.m2.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.452em" id="id2.2.m2.1.1.1.1" xref="id2.2.m2.1.1.1.1.cmml">∗</mo><mn id="id2.2.m2.1.1.1.3" xref="id2.2.m2.1.1.1.3.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><apply id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"><apply id="id2.2.m2.1.1.1.cmml" xref="id2.2.m2.1.1.1"><times id="id2.2.m2.1.1.1.1.cmml" xref="id2.2.m2.1.1.1.1"></times><csymbol cd="latexml" id="id2.2.m2.1.1.1.2.cmml" xref="id2.2.m2.1.1.1.2">absent</csymbol><cn type="integer" id="id2.2.m2.1.1.1.3.cmml" xref="id2.2.m2.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">{}^{*~{}2}</annotation></semantics></math>  Shengyun Si<math id="id3.3.m3.1" class="ltx_Math" alttext="{}^{*~{}2}" display="inline"><semantics id="id3.3.m3.1a"><msup id="id3.3.m3.1.1" xref="id3.3.m3.1.1.cmml"><mi id="id3.3.m3.1.1a" xref="id3.3.m3.1.1.cmml"></mi><mrow id="id3.3.m3.1.1.1" xref="id3.3.m3.1.1.1.cmml"><mi id="id3.3.m3.1.1.1.2" xref="id3.3.m3.1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.452em" id="id3.3.m3.1.1.1.1" xref="id3.3.m3.1.1.1.1.cmml">∗</mo><mn id="id3.3.m3.1.1.1.3" xref="id3.3.m3.1.1.1.3.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="id3.3.m3.1b"><apply id="id3.3.m3.1.1.cmml" xref="id3.3.m3.1.1"><apply id="id3.3.m3.1.1.1.cmml" xref="id3.3.m3.1.1.1"><times id="id3.3.m3.1.1.1.1.cmml" xref="id3.3.m3.1.1.1.1"></times><csymbol cd="latexml" id="id3.3.m3.1.1.1.2.cmml" xref="id3.3.m3.1.1.1.2">absent</csymbol><cn type="integer" id="id3.3.m3.1.1.1.3.cmml" xref="id3.3.m3.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m3.1c">{}^{*~{}2}</annotation></semantics></math> 
<br class="ltx_break"><span id="id9.9.6" class="ltx_text ltx_font_bold">Linyong Nan<sup id="id9.9.6.1" class="ltx_sup"><span id="id9.9.6.1.1" class="ltx_text ltx_font_medium">1</span></sup>  Xiangru Tang<sup id="id9.9.6.2" class="ltx_sup"><span id="id9.9.6.2.1" class="ltx_text ltx_font_medium">1</span></sup>  Arman Cohan<sup id="id9.9.6.3" class="ltx_sup"><span id="id9.9.6.3.1" class="ltx_text ltx_font_medium ltx_font_italic">1,3</span></sup> 
<br class="ltx_break"><sup id="id9.9.6.4" class="ltx_sup"><span id="id9.9.6.4.1" class="ltx_text ltx_font_medium">1</span></sup>Yale University, <sup id="id9.9.6.5" class="ltx_sup"><span id="id9.9.6.5.1" class="ltx_text ltx_font_medium">2</span></sup>Technical University of Munich, <sup id="id9.9.6.6" class="ltx_sup"><span id="id9.9.6.6.1" class="ltx_text ltx_font_medium">3</span></sup>Allen Institute for AI
<br class="ltx_break"></span><span id="id11.11.id2" class="ltx_text ltx_font_typewriter">yilun.zhao@yale.edu</span><span id="id12.12.id3" class="ltx_text ltx_font_bold">  </span><span id="id13.13.id4" class="ltx_text ltx_font_typewriter">{haowei.zhang, shengyun.si}@tum.de</span><span id="id14.14.id5" class="ltx_text ltx_font_bold">
</span>
</span><span class="ltx_author_notes">  Equal Contributions.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id15.id1" class="ltx_p">Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes.
The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored.
In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the <span id="id15.id1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> and our newly-constructed <span id="id15.id1.2" class="ltx_text ltx_font_smallcaps">LoTNLG</span> datasets for <em id="id15.id1.3" class="ltx_emph ltx_font_italic">data insight generation</em>, along with the FeTaQA and our newly-constructed F2WTQ datasets for <em id="id15.id1.4" class="ltx_emph ltx_font_italic">query-based generation</em>.
We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively.
Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users’ information seeking purposes in real-world scenarios.
However, a significant performance gap still exists between other open-sourced LLMs (e.g., <span id="id15.id1.5" class="ltx_text ltx_font_smallcaps">Tülu</span> and LLaMA-2) and GPT-4 models. Our data and code are publicly available at <a target="_blank" href="https://github.com/yale-nlp/LLM-T2T" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/yale-nlp/LLM-T2T</a>.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2305.14987/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="751" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The real-world table information seeking scenarios and research questions investigated in this paper.</figcaption>
</figure>
<figure id="S0.T1" class="ltx_table">
<table id="S0.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S0.T1.1.1.1" class="ltx_tr">
<td id="S0.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S0.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S0.T1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S0.T1.1.1.1.2.1" class="ltx_text ltx_font_bold"># Table</span></td>
<td id="S0.T1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt"><span id="S0.T1.1.1.1.3.1" class="ltx_text ltx_font_bold"># Examples</span></td>
<td id="S0.T1.1.1.1.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S0.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Control Signal</span></td>
<td id="S0.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S0.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Rich in Reasoning?</span></td>
</tr>
<tr id="S0.T1.1.2.2" class="ltx_tr">
<td id="S0.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="5"><em id="S0.T1.1.2.2.1.1" class="ltx_emph ltx_font_italic">Data Insight Generation</em></td>
</tr>
<tr id="S0.T1.1.3.3" class="ltx_tr">
<td id="S0.T1.1.3.3.1" class="ltx_td ltx_align_left">
<span id="S0.T1.1.3.3.1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="S0.T1.1.3.3.2" class="ltx_td ltx_align_right">862</td>
<td id="S0.T1.1.3.3.3" class="ltx_td ltx_align_right">4,305</td>
<td id="S0.T1.1.3.3.4" class="ltx_td ltx_align_left">None</td>
<td id="S0.T1.1.3.3.5" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S0.T1.1.4.4" class="ltx_tr">
<td id="S0.T1.1.4.4.1" class="ltx_td ltx_align_left">
<span id="S0.T1.1.4.4.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">LoTNLG</span> (ours)</td>
<td id="S0.T1.1.4.4.2" class="ltx_td ltx_align_right">862</td>
<td id="S0.T1.1.4.4.3" class="ltx_td ltx_align_right">4,305</td>
<td id="S0.T1.1.4.4.4" class="ltx_td ltx_align_left">Reasoning type</td>
<td id="S0.T1.1.4.4.5" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S0.T1.1.5.5" class="ltx_tr">
<td id="S0.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t" colspan="5"><em id="S0.T1.1.5.5.1.1" class="ltx_emph ltx_font_italic">Query-based Generation</em></td>
</tr>
<tr id="S0.T1.1.6.6" class="ltx_tr">
<td id="S0.T1.1.6.6.1" class="ltx_td ltx_align_left">FeTaQA <cite class="ltx_cite ltx_citemacro_cite">Parikh et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="S0.T1.1.6.6.2" class="ltx_td ltx_align_right">2,003</td>
<td id="S0.T1.1.6.6.3" class="ltx_td ltx_align_right">2,003</td>
<td id="S0.T1.1.6.6.4" class="ltx_td ltx_align_left">User query</td>
<td id="S0.T1.1.6.6.5" class="ltx_td ltx_align_center">✗</td>
</tr>
<tr id="S0.T1.1.7.7" class="ltx_tr">
<td id="S0.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S0.T1.1.7.7.1.1" class="ltx_text ltx_font_bold">F2WTQ</span> (ours)</td>
<td id="S0.T1.1.7.7.2" class="ltx_td ltx_align_right ltx_border_bb">4,344</td>
<td id="S0.T1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_bb">4,344</td>
<td id="S0.T1.1.7.7.4" class="ltx_td ltx_align_left ltx_border_bb">User query</td>
<td id="S0.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_bb">✓</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Experimental dataset statistics for the test set. Examples of our newly-constructed <span id="S0.T1.3.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span> and F2WTQ datasets are displayed in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.1.2 LoTNLG Dataset ‣ 2.1 Data Insight Generation ‣ 2 Table Information Seeking Scenarios ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#S2.F3" title="Figure 3 ‣ 2.2.1 FeTaQA Dataset ‣ 2.2 Query-based Generation ‣ 2 Table Information Seeking Scenarios ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, respectively.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In an era where users interact with vast amounts of structured data every day for decision-making and information-seeking purposes, the need for intuitive, user-friendly interpretations has become paramount <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a href="#bib.bib47" title="" class="ltx_ref">2023</a>); Zha et al. (<a href="#bib.bib46" title="" class="ltx_ref">2023</a>); Li et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>. Given this emerging necessity, table-to-text generation techniques, which transform complex tabular data into comprehensible narratives tailored to users’ information needs, have drawn considerable attention <cite class="ltx_cite ltx_citemacro_cite">Parikh et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>); Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>); Nan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>); Zhao et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023c</a>)</cite>. These techniques can be incorporated into a broad range of applications, including but not limited to game strategy development, financial analysis, and human resources management. However, existing fine-tuned table-to-text generation models <cite class="ltx_cite ltx_citemacro_cite">Nan et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>); Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022b</a>, <a href="#bib.bib23" title="" class="ltx_ref">a</a>); Zhao et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite> are typically task-specific, limiting their adaptability to real-world applications.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The emergence and remarkable achievements of LLMs <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Scao et al. (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>); Scheurer et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); OpenAI (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>); Touvron et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023a</a>); Taori et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023</a>); Touvron et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023b</a>)</cite> have sparked a significant transformation in the field of controllable text generation and data interpretations <cite class="ltx_cite ltx_citemacro_cite">Nan et al. (<a href="#bib.bib29" title="" class="ltx_ref">2021</a>); Zhang et al. (<a href="#bib.bib48" title="" class="ltx_ref">2022</a>); Goyal et al. (<a href="#bib.bib13" title="" class="ltx_ref">2022</a>); Köksal et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Gao et al. (<a href="#bib.bib11" title="" class="ltx_ref">2023b</a>); Madaan et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Zhou et al. (<a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>.
As for table-based tasks, recent work <cite class="ltx_cite ltx_citemacro_cite">Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>); Ye et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>); Gemmell and Dalton (<a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> reveals that LLMs are capable of achieving competitive performance with state-of-the-art fine-tuned models on table question answering <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>); Nan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite> and table fact checking <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>); Gupta et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>. However, the potential of LLMs in generating text from tabular data for users’ information-seeking purposes remains largely underexplored.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we investigate the table-to-text generation capabilities of LLMs in two real-world table information seeking scenarios: 1) <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Data Insight Generation</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite>, where users aim to promptly derive significant facts from the table, anticipating the systems to offer several data insights; and 2) <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">Query-based Generation</span> <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>); Nan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite>, where users consult tables to answer specific questions.
To facilitate a rigorous evaluation of LLM performance, we also construct two new benchmarks: <span id="S1.p3.1.3" class="ltx_text ltx_font_bold ltx_font_smallcaps">LoTNLG</span> for data insight generation conditioned with specific logical reasoning types; and <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">F2WTQ</span> for free-form question answering that requires models to perform human-like reasoning over Wikipedia tables.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We provide an overview of table information seeking scenarios and our main research questions in Figure <a href="#S0.F1" title="Figure 1 ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and enumerate our findings as follows:</p>
</div>
<div id="S1.p5" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p"><span id="S1.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">RQ1</span>: <em id="S1.I1.ix1.p1.1.2" class="ltx_emph ltx_font_italic">How do LLMs perform in table-to-text generation tasks?</em> 
<br class="ltx_break"><span id="S1.I1.ix1.p1.1.3" class="ltx_text ltx_font_bold">Finding</span>: LLMs exhibit significant potential in generating coherent and faithful natural language statements based on the given table. For example, GPT-4 outperforms state-of-the-art fine-tuned models in terms of faithfulness during both automated and human evaluations. The statements generated by GPT-3.5 and GPT-4 are also preferred by human evaluators. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-* models, especially on our newly-constructed <span id="S1.I1.ix1.p1.1.4" class="ltx_text ltx_font_smallcaps">LoTNLG</span> and F2WTQ datasets.</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p"><span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">RQ2</span>: <em id="S1.I1.ix2.p1.1.2" class="ltx_emph ltx_font_italic">Can we use LLMs to assess factual consistency of table-to-text generation?</em> 
<br class="ltx_break"><span id="S1.I1.ix2.p1.1.3" class="ltx_text ltx_font_bold">Finding</span>: LLMs using chain-of-thought prompting can serve as reference-free metrics for table-to-text generation evaluation. These metrics demonstrate better alignment with human evaluation in terms of both fluency and faithfulness.</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p"><span id="S1.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">RQ3</span>: <em id="S1.I1.ix3.p1.1.2" class="ltx_emph ltx_font_italic">How can fine-tuned models benefit from LLMs’ strong table-to-text abilities?</em> 
<br class="ltx_break"><span id="S1.I1.ix3.p1.1.3" class="ltx_text ltx_font_bold">Finding</span>: LLMs that utilize chain-of-thought prompting can provide high-quality natural language feedback in terms of factuality, which includes explanations, corrective instructions, and edited statements for the output of other models. The edited statements are more factually consistent with the table compared to the initial ones.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Table Information Seeking Scenarios</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Table <a href="#S0.T1" title="Table 1 ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the data statistics for the four datasets used in the experiments. We investigate the performance of the LLM in the following two real-world table information-seeking scenarios.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data Insight Generation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Data insight generation is an essential task that involves generating meaningful and relevant insights from tables. By interpreting and explaining tabular data in natural language, LLMs can play a crucial role in assisting users with information seeking and decision making. This frees users from the need to manually comb through vast amounts of data. We use the following two datasets for evaluation.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span><span id="S2.SS1.SSS1.1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> Dataset</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">The task of <span id="S2.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> involves generating five logically consistent sentences from a given table. It aims to uncover intriguing facts from the table by applying various logical reasoning operations (e.g., count and comparison) across different table regions.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span><span id="S2.SS1.SSS2.1.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span> Dataset</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">Our preliminary experiments revealed that when applied to the <span id="S2.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> dataset, table-to-text generation systems tend to generate multiple sentences that employ the same logical reasoning operations. For instance, in a 0-shot setting, the GPT-3.5 model is more inclined to generate sentences involving numerical comparisons, while overlooking other compelling facts within tables. This lack of diversity in data insight generation poses a significant limitation because, in real-world information-seeking scenarios, users typically expect systems to offer a variety of perspectives on the tabular data. To address this issue, application developers could tailor the table-to-text generation systems to generate multiple insights that encompass different logical reasoning operations <cite class="ltx_cite ltx_citemacro_cite">Perlitz et al. (<a href="#bib.bib35" title="" class="ltx_ref">2022</a>); Zhao et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite>.
In order to foster a more rigorous evaluation of LLMs’ abilities to utilize a broader range of logical reasoning operations while generating insights from tables, we have developed a new dataset, <span id="S2.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_smallcaps">LoTNLG</span>, for logical reasoning type-conditioned table-to-text generation. In this setup, the model is tasked with generating a statement by performing the logical reasoning operations of the specified types on the tables.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2305.14987/assets/figures/figures/LOTNLG.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="478" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An example of <span id="S2.F2.2.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span>, where models are required to generate statements using the specified types of logical reasoning operations</figcaption>
</figure>
<section id="S2.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">
<span id="S2.SS1.SSS2.Px1.1.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span> Dataset Construction</h5>

<div id="S2.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS2.Px1.p1.1" class="ltx_p">Following <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>)</cite>, we have predefined nine types of common logical reasoning operations (e.g., count, comparative, and superlative), with detailed definitions provided in Appendix <a href="#A1.SS1" title="A.1 LoTNLG Dataset ‣ Appendix A Table-to-Text Generation Benchmarks ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>. We use examples from the <span id="S2.SS1.SSS2.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> test set to construct <span id="S2.SS1.SSS2.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">LoTNLG</span>. Specifically, for each statement from <span id="S2.SS1.SSS2.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">LogicNLG</span>, we assign two annotators to independently label the set of logical reasoning types used in that statement, ensuring that no more than two types were identified per statement. If there are discrepancies in the labels, an expert annotator is brought in to make the final decision. The distribution of logical reasoning types in <span id="S2.SS1.SSS2.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">LoTNLG</span> is illustrated in Figure <a href="#A1.F4" title="Figure 4 ‣ Logical Reasoning Type Definition ‣ A.1 LoTNLG Dataset ‣ Appendix A Table-to-Text Generation Benchmarks ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> in Appendix <a href="#A1.SS1" title="A.1 LoTNLG Dataset ‣ Appendix A Table-to-Text Generation Benchmarks ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
</section>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Query-based Generation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Query-based table-to-text generation pertains to producing detailed responses based on specific user queries in the context of a given table.
The ability to answer users’ queries accurately, coherently, and in a context-appropriate manner is crucial for LLMs in many real-world applications, such as customer data support and personal digital assistants. We utilize following two datasets to evaluate LLMs’ efficiency in interacting with users and their proficiency in table understanding and reasoning.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>FeTaQA Dataset</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Nan et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022b</a>)</cite> introduces a task of free-form table question answering. This task involves retrieving and aggregating information from Wikipedia tables, followed by generating coherent sentences based on the aggregated contents.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2305.14987/assets/x2.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="430" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An example of F2WTQ, where models need to perform human-like reasoning to generate response.</figcaption>
</figure>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>F2WTQ Dataset</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Queries in the FeTaQA dataset typically focus on <em id="S2.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">surface-level facts</em> (e.g., "Which country hosted the 2014 FIFA World Cup?"). However, in real-world information-seeking scenarios, users are likely to consult tables for more complex questions, which require models to perform human-like reasoning over tabular data. Therefore, we have constructed a new benchmark, named F2WTQ, for more challenging, free-form table question answering tasks.</p>
</div>
<section id="S2.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">F2WTQ Dataset Construction</h5>

<div id="S2.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS2.Px1.p1.1" class="ltx_p">We adopt the WTQ dataset <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>)</cite> as a basis to construct F2WTQ. The WTQ dataset is a short-form table question answering dataset, which includes human-annotated questions based on Wikipedia tables and requires complex reasoning. However, we do not directly use WTQ for LLM evaluation because, in real-world scenarios, users typically prefer a natural language response over a few words. In the development of F2WTQ, for each QA pair in the WTQ test set, we assign an annotator who assumes the role of an agent that analyzes the table and provides an expanded, sentence-long response. We found that the original questions in the WTQ dataset occasionally contained grammatical errors or lacked a natural linguistic flow. In these cases, the annotators are required to rewrite the question to ensure it was fluent and natural.</p>
</div>
</section>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation System</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Automated Evaluation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We adopt following popular evaluation metrics for automated evaluation:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">BLEU</span> <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a href="#bib.bib32" title="" class="ltx_ref">2002</a>)</cite> uses a precision-based approach, measuring the n-gram matches between the generated and reference statements.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">ROUGE</span> <cite class="ltx_cite ltx_citemacro_cite">Lin (<a href="#bib.bib22" title="" class="ltx_ref">2004</a>)</cite> uses a recall-based approach, and measures the percentage of overlapping words and phrases between the generated output and reference one.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">SP-Acc</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> extracts the meaning representation from the generated sentence and executes it against the table to verify correctness.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">NLI-Acc</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> uses TableBERT fine-tuned on the TabFact dataset <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>)</cite> as faithfulness classifier.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">TAPAS-Acc</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022a</a>)</cite> uses TAPAS <cite class="ltx_cite ltx_citemacro_cite">Herzig et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> fine-tuned on the TabFact dataset as the backbone.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p"><span id="S3.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">TAPEX-Acc</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022a</a>)</cite> employs TAPEX <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022b</a>)</cite> fine-tuned on the TabFact dataset as the backbone. Recent works <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022a</a>); Zhao et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite> have revealed that NLI-Acc and TAPAS-Acc is overly positive about the predictions, while TAPEX-Acc serves as a more reliable faithfulness-level metric.</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p"><span id="S3.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Exact Match &amp; F-Score for Logical Reasoning Type</span> For <span id="S3.I1.i7.p1.1.2" class="ltx_text ltx_font_smallcaps">LoTNLG</span> evaluation, the exact match measures the percentage of samples with all the labels classified correctly, while the F-Score provides a balanced metric that considers both type I and type II errors.</p>
</div>
</li>
<li id="S3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i8.p1" class="ltx_para">
<p id="S3.I1.i8.p1.1" class="ltx_p"><span id="S3.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">Answer Accuracy</span> refers to the proportion of correct predictions out of the total number of predictions in F2WTQ generation.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Human Evaluation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.4" class="ltx_p">To gain a more comprehensive understanding of the system’s performance, we also conduct human evaluation. Specifically, the generated statements from different models are evaluated by humans based on two criteria: <em id="S3.SS2.p1.4.1" class="ltx_emph ltx_font_italic">faithfulness</em> and <em id="S3.SS2.p1.4.2" class="ltx_emph ltx_font_italic">fluency</em>. For <em id="S3.SS2.p1.4.3" class="ltx_emph ltx_font_italic">faithfulness</em>, each sentence is scored <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> (refuted) or <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">1</annotation></semantics></math> (entailed). For <em id="S3.SS2.p1.4.4" class="ltx_emph ltx_font_italic">fluency</em>, scores range from <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1</annotation></semantics></math> (worst) to <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn type="integer" id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">5</annotation></semantics></math> (best). We average the scores across different human evaluators for each criterion. We do not apply more fine-grained scoring scales for <em id="S3.SS2.p1.4.5" class="ltx_emph ltx_font_italic">faithfulness</em>-level evaluation, as each statement in <span id="S3.SS2.p1.4.6" class="ltx_text ltx_font_smallcaps">LogicNLG</span> consists of only a single sentence.</p>
</div>
<figure id="S3.SS2.2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.SS2.2.3" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.SS2.2.3.1.1" class="ltx_tr">
<th id="S3.SS2.2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Type</th>
<th id="S3.SS2.2.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Models</th>
<td id="S3.SS2.2.3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">SP-Acc</td>
<td id="S3.SS2.2.3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">NLI-Acc</td>
<td id="S3.SS2.2.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">TAPAS-Acc</td>
<td id="S3.SS2.2.3.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">TAPEX-Acc</td>
</tr>
<tr id="S3.SS2.2.3.2.2" class="ltx_tr">
<th id="S3.SS2.2.3.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="S3.SS2.2.3.2.2.1.1" class="ltx_text">Fine-tuned</span></th>
<th id="S3.SS2.2.3.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT2-C2F</th>
<td id="S3.SS2.2.3.2.2.3" class="ltx_td ltx_align_center ltx_border_t">43.6</td>
<td id="S3.SS2.2.3.2.2.4" class="ltx_td ltx_align_center ltx_border_t">71.4</td>
<td id="S3.SS2.2.3.2.2.5" class="ltx_td ltx_align_center ltx_border_t">46.2</td>
<td id="S3.SS2.2.3.2.2.6" class="ltx_td ltx_align_center ltx_border_t">43.8</td>
</tr>
<tr id="S3.SS2.2.3.3.3" class="ltx_tr">
<th id="S3.SS2.2.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">R2D2</th>
<td id="S3.SS2.2.3.3.3.2" class="ltx_td ltx_align_center">53.2</td>
<td id="S3.SS2.2.3.3.3.3" class="ltx_td ltx_align_center">86.2</td>
<td id="S3.SS2.2.3.3.3.4" class="ltx_td ltx_align_center">60.2</td>
<td id="S3.SS2.2.3.3.3.5" class="ltx_td ltx_align_center">61.0</td>
</tr>
<tr id="S3.SS2.2.3.4.4" class="ltx_tr">
<th id="S3.SS2.2.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PLOG</th>
<td id="S3.SS2.2.3.4.4.2" class="ltx_td ltx_align_center">52.8</td>
<td id="S3.SS2.2.3.4.4.3" class="ltx_td ltx_align_center">84.2</td>
<td id="S3.SS2.2.3.4.4.4" class="ltx_td ltx_align_center">63.8</td>
<td id="S3.SS2.2.3.4.4.5" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.4.4.5.1" class="ltx_text ltx_font_bold">69.6</span></td>
</tr>
<tr id="S3.SS2.2.3.5.5" class="ltx_tr">
<th id="S3.SS2.2.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S3.SS2.2.3.5.5.1.1" class="ltx_text ltx_font_smallcaps">LoFT</span></th>
<td id="S3.SS2.2.3.5.5.2" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.5.5.2.1" class="ltx_text ltx_font_bold">53.8</span></td>
<td id="S3.SS2.2.3.5.5.3" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.5.5.3.1" class="ltx_text ltx_font_bold">86.6</span></td>
<td id="S3.SS2.2.3.5.5.4" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.5.5.4.1" class="ltx_text ltx_font_bold">67.4</span></td>
<td id="S3.SS2.2.3.5.5.5" class="ltx_td ltx_align_center">61.4</td>
</tr>
<tr id="S3.SS2.2.3.6.6" class="ltx_tr">
<th id="S3.SS2.2.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.SS2.2.3.6.6.1.1" class="ltx_text">0-shot*</span></th>
<th id="S3.SS2.2.3.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3.5</th>
<td id="S3.SS2.2.3.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.SS2.2.3.6.6.3.1" class="ltx_text ltx_font_bold">54.2</span></td>
<td id="S3.SS2.2.3.6.6.4" class="ltx_td ltx_align_center ltx_border_t">87.6</td>
<td id="S3.SS2.2.3.6.6.5" class="ltx_td ltx_align_center ltx_border_t">81.6</td>
<td id="S3.SS2.2.3.6.6.6" class="ltx_td ltx_align_center ltx_border_t">79.4</td>
</tr>
<tr id="S3.SS2.2.3.7.7" class="ltx_tr">
<th id="S3.SS2.2.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="S3.SS2.2.3.7.7.2" class="ltx_td ltx_align_center">43.2</td>
<td id="S3.SS2.2.3.7.7.3" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.7.7.3.1" class="ltx_text ltx_font_bold">90.4</span></td>
<td id="S3.SS2.2.3.7.7.4" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.7.7.4.1" class="ltx_text ltx_font_bold">91.8</span></td>
<td id="S3.SS2.2.3.7.7.5" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.7.7.5.1" class="ltx_text ltx_font_bold">91.0</span></td>
</tr>
<tr id="S3.SS2.2.3.8.8" class="ltx_tr">
<th id="S3.SS2.2.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="S3.SS2.2.3.8.8.1.1" class="ltx_text">1-shot Direct</span></th>
<th id="S3.SS2.2.3.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">GPT-3.5</th>
<td id="S3.SS2.2.3.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.SS2.2.3.8.8.3.1" class="ltx_text ltx_font_bold">60.2</span></td>
<td id="S3.SS2.2.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t">79.0</td>
<td id="S3.SS2.2.3.8.8.5" class="ltx_td ltx_align_center ltx_border_t">80.4</td>
<td id="S3.SS2.2.3.8.8.6" class="ltx_td ltx_align_center ltx_border_t">79.2</td>
</tr>
<tr id="S3.SS2.2.3.9.9" class="ltx_tr">
<th id="S3.SS2.2.3.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="S3.SS2.2.3.9.9.2" class="ltx_td ltx_align_center">57.6</td>
<td id="S3.SS2.2.3.9.9.3" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.9.9.3.1" class="ltx_text ltx_font_bold">82.0</span></td>
<td id="S3.SS2.2.3.9.9.4" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.9.9.4.1" class="ltx_text ltx_font_bold">87.6</span></td>
<td id="S3.SS2.2.3.9.9.5" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.9.9.5.1" class="ltx_text ltx_font_bold">88.0</span></td>
</tr>
<tr id="S3.SS2.2.3.10.10" class="ltx_tr">
<th id="S3.SS2.2.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" rowspan="2">
<span id="S3.SS2.2.3.10.10.1.1" class="ltx_ERROR undefined">\cdashline</span>1-6
<span id="S3.SS2.2.3.10.10.1.2" class="ltx_text">1-shot CoT</span>
</th>
<th id="S3.SS2.2.3.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-3.5</th>
<td id="S3.SS2.2.3.10.10.3" class="ltx_td ltx_align_center">51.6</td>
<td id="S3.SS2.2.3.10.10.4" class="ltx_td ltx_align_center">70.0</td>
<td id="S3.SS2.2.3.10.10.5" class="ltx_td ltx_align_center">81.8</td>
<td id="S3.SS2.2.3.10.10.6" class="ltx_td ltx_align_center">78.2</td>
</tr>
<tr id="S3.SS2.2.3.11.11" class="ltx_tr">
<th id="S3.SS2.2.3.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="S3.SS2.2.3.11.11.2" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.11.11.2.1" class="ltx_text ltx_font_bold">59.8</span></td>
<td id="S3.SS2.2.3.11.11.3" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.11.11.3.1" class="ltx_text ltx_font_bold">80.8</span></td>
<td id="S3.SS2.2.3.11.11.4" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.11.11.4.1" class="ltx_text ltx_font_bold">89.4</span></td>
<td id="S3.SS2.2.3.11.11.5" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.11.11.5.1" class="ltx_text ltx_font_bold">90.8</span></td>
</tr>
<tr id="S3.SS2.2.3.12.12" class="ltx_tr">
<th id="S3.SS2.2.3.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" rowspan="11"><span id="S3.SS2.2.3.12.12.1.1" class="ltx_text">2-shot Direct</span></th>
<th id="S3.SS2.2.3.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Pythia<span id="S3.SS2.2.3.12.12.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</th>
<td id="S3.SS2.2.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t">39.4</td>
<td id="S3.SS2.2.3.12.12.4" class="ltx_td ltx_align_center ltx_border_t">53.2</td>
<td id="S3.SS2.2.3.12.12.5" class="ltx_td ltx_align_center ltx_border_t">39.4</td>
<td id="S3.SS2.2.3.12.12.6" class="ltx_td ltx_align_center ltx_border_t">40.4</td>
</tr>
<tr id="S3.SS2.2.3.13.13" class="ltx_tr">
<th id="S3.SS2.2.3.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.13.13.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.13.13.2" class="ltx_td ltx_align_center">47.2</td>
<td id="S3.SS2.2.3.13.13.3" class="ltx_td ltx_align_center">58.4</td>
<td id="S3.SS2.2.3.13.13.4" class="ltx_td ltx_align_center">47.0</td>
<td id="S3.SS2.2.3.13.13.5" class="ltx_td ltx_align_center">43.2</td>
</tr>
<tr id="S3.SS2.2.3.14.14" class="ltx_tr">
<th id="S3.SS2.2.3.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.14.14.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</th>
<td id="S3.SS2.2.3.14.14.2" class="ltx_td ltx_align_center">38.6</td>
<td id="S3.SS2.2.3.14.14.3" class="ltx_td ltx_align_center">63.4</td>
<td id="S3.SS2.2.3.14.14.4" class="ltx_td ltx_align_center">45.8</td>
<td id="S3.SS2.2.3.14.14.5" class="ltx_td ltx_align_center">43.6</td>
</tr>
<tr id="S3.SS2.2.3.15.15" class="ltx_tr">
<th id="S3.SS2.2.3.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA2<span id="S3.SS2.2.3.15.15.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="S3.SS2.2.3.15.15.2" class="ltx_td ltx_align_center">56.0</td>
<td id="S3.SS2.2.3.15.15.3" class="ltx_td ltx_align_center">52.4</td>
<td id="S3.SS2.2.3.15.15.4" class="ltx_td ltx_align_center">54.6</td>
<td id="S3.SS2.2.3.15.15.5" class="ltx_td ltx_align_center">52.4</td>
</tr>
<tr id="S3.SS2.2.3.16.16" class="ltx_tr">
<th id="S3.SS2.2.3.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.16.16.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</th>
<td id="S3.SS2.2.3.16.16.2" class="ltx_td ltx_align_center">45.4</td>
<td id="S3.SS2.2.3.16.16.3" class="ltx_td ltx_align_center">55.8</td>
<td id="S3.SS2.2.3.16.16.4" class="ltx_td ltx_align_center">53.8</td>
<td id="S3.SS2.2.3.16.16.5" class="ltx_td ltx_align_center">53.0</td>
</tr>
<tr id="S3.SS2.2.3.17.17" class="ltx_tr">
<th id="S3.SS2.2.3.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Alpaca<span id="S3.SS2.2.3.17.17.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.17.17.2" class="ltx_td ltx_align_center">44.0</td>
<td id="S3.SS2.2.3.17.17.3" class="ltx_td ltx_align_center">70.6</td>
<td id="S3.SS2.2.3.17.17.4" class="ltx_td ltx_align_center">58.0</td>
<td id="S3.SS2.2.3.17.17.5" class="ltx_td ltx_align_center">54.6</td>
</tr>
<tr id="S3.SS2.2.3.18.18" class="ltx_tr">
<th id="S3.SS2.2.3.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.18.18.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</th>
<td id="S3.SS2.2.3.18.18.2" class="ltx_td ltx_align_center">52.2</td>
<td id="S3.SS2.2.3.18.18.3" class="ltx_td ltx_align_center">57.2</td>
<td id="S3.SS2.2.3.18.18.4" class="ltx_td ltx_align_center">58.4</td>
<td id="S3.SS2.2.3.18.18.5" class="ltx_td ltx_align_center">56.8</td>
</tr>
<tr id="S3.SS2.2.3.19.19" class="ltx_tr">
<th id="S3.SS2.2.3.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S3.SS2.2.3.19.19.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="S3.SS2.2.3.19.19.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.19.19.2" class="ltx_td ltx_align_center">44.4</td>
<td id="S3.SS2.2.3.19.19.3" class="ltx_td ltx_align_center">68.4</td>
<td id="S3.SS2.2.3.19.19.4" class="ltx_td ltx_align_center">63.4</td>
<td id="S3.SS2.2.3.19.19.5" class="ltx_td ltx_align_center">59.6</td>
</tr>
<tr id="S3.SS2.2.3.20.20" class="ltx_tr">
<th id="S3.SS2.2.3.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna<span id="S3.SS2.2.3.20.20.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.20.20.2" class="ltx_td ltx_align_center">51.8</td>
<td id="S3.SS2.2.3.20.20.3" class="ltx_td ltx_align_center">71.4</td>
<td id="S3.SS2.2.3.20.20.4" class="ltx_td ltx_align_center">66.2</td>
<td id="S3.SS2.2.3.20.20.5" class="ltx_td ltx_align_center">65.2</td>
</tr>
<tr id="S3.SS2.2.3.21.21" class="ltx_tr">
<th id="S3.SS2.2.3.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-3.5</th>
<td id="S3.SS2.2.3.21.21.2" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.21.21.2.1" class="ltx_text ltx_font_bold">64.0</span></td>
<td id="S3.SS2.2.3.21.21.3" class="ltx_td ltx_align_center">78.4</td>
<td id="S3.SS2.2.3.21.21.4" class="ltx_td ltx_align_center">78.8</td>
<td id="S3.SS2.2.3.21.21.5" class="ltx_td ltx_align_center">81.2</td>
</tr>
<tr id="S3.SS2.2.3.22.22" class="ltx_tr">
<th id="S3.SS2.2.3.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-4</th>
<td id="S3.SS2.2.3.22.22.2" class="ltx_td ltx_align_center">55.4</td>
<td id="S3.SS2.2.3.22.22.3" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.22.22.3.1" class="ltx_text ltx_font_bold">85.8</span></td>
<td id="S3.SS2.2.3.22.22.4" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.22.22.4.1" class="ltx_text ltx_font_bold">92.0</span></td>
<td id="S3.SS2.2.3.22.22.5" class="ltx_td ltx_align_center"><span id="S3.SS2.2.3.22.22.5.1" class="ltx_text ltx_font_bold">89.6</span></td>
</tr>
<tr id="S3.SS2.2.3.23.23" class="ltx_tr">
<th id="S3.SS2.2.3.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" rowspan="11">
<span id="S3.SS2.2.3.23.23.1.1" class="ltx_ERROR undefined">\cdashline</span>1-6
<span id="S3.SS2.2.3.23.23.1.2" class="ltx_text">2-shot CoT</span>
</th>
<th id="S3.SS2.2.3.23.23.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Pythia<span id="S3.SS2.2.3.23.23.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</th>
<td id="S3.SS2.2.3.23.23.3" class="ltx_td ltx_align_center">41.8</td>
<td id="S3.SS2.2.3.23.23.4" class="ltx_td ltx_align_center">54.0</td>
<td id="S3.SS2.2.3.23.23.5" class="ltx_td ltx_align_center">41.2</td>
<td id="S3.SS2.2.3.23.23.6" class="ltx_td ltx_align_center">42.8</td>
</tr>
<tr id="S3.SS2.2.3.24.24" class="ltx_tr">
<th id="S3.SS2.2.3.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.24.24.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</th>
<td id="S3.SS2.2.3.24.24.2" class="ltx_td ltx_align_center">38.0</td>
<td id="S3.SS2.2.3.24.24.3" class="ltx_td ltx_align_center">63.2</td>
<td id="S3.SS2.2.3.24.24.4" class="ltx_td ltx_align_center">48.0</td>
<td id="S3.SS2.2.3.24.24.5" class="ltx_td ltx_align_center">43.0</td>
</tr>
<tr id="S3.SS2.2.3.25.25" class="ltx_tr">
<th id="S3.SS2.2.3.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.25.25.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.25.25.2" class="ltx_td ltx_align_center">44.2</td>
<td id="S3.SS2.2.3.25.25.3" class="ltx_td ltx_align_center">53.2</td>
<td id="S3.SS2.2.3.25.25.4" class="ltx_td ltx_align_center">49.2</td>
<td id="S3.SS2.2.3.25.25.5" class="ltx_td ltx_align_center">48.6</td>
</tr>
<tr id="S3.SS2.2.3.26.26" class="ltx_tr">
<th id="S3.SS2.2.3.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.26.26.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</th>
<td id="S3.SS2.2.3.26.26.2" class="ltx_td ltx_align_center">45.0</td>
<td id="S3.SS2.2.3.26.26.3" class="ltx_td ltx_align_center">56.6</td>
<td id="S3.SS2.2.3.26.26.4" class="ltx_td ltx_align_center">60.8</td>
<td id="S3.SS2.2.3.26.26.5" class="ltx_td ltx_align_center">54.2</td>
</tr>
<tr id="S3.SS2.2.3.27.27" class="ltx_tr">
<th id="S3.SS2.2.3.27.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA<span id="S3.SS2.2.3.27.27.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</th>
<td id="S3.SS2.2.3.27.27.2" class="ltx_td ltx_align_center">48.0</td>
<td id="S3.SS2.2.3.27.27.3" class="ltx_td ltx_align_center">58.8</td>
<td id="S3.SS2.2.3.27.27.4" class="ltx_td ltx_align_center">57.4</td>
<td id="S3.SS2.2.3.27.27.5" class="ltx_td ltx_align_center">57.4</td>
</tr>
<tr id="S3.SS2.2.3.28.28" class="ltx_tr">
<th id="S3.SS2.2.3.28.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S3.SS2.2.3.28.28.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="S3.SS2.2.3.28.28.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.28.28.2" class="ltx_td ltx_align_center">46.0</td>
<td id="S3.SS2.2.3.28.28.3" class="ltx_td ltx_align_center">69.8</td>
<td id="S3.SS2.2.3.28.28.4" class="ltx_td ltx_align_center">61.6</td>
<td id="S3.SS2.2.3.28.28.5" class="ltx_td ltx_align_center">58.8</td>
</tr>
<tr id="S3.SS2.2.3.29.29" class="ltx_tr">
<th id="S3.SS2.2.3.29.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vicuna<span id="S3.SS2.2.3.29.29.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.29.29.2" class="ltx_td ltx_align_center">44.6</td>
<td id="S3.SS2.2.3.29.29.3" class="ltx_td ltx_align_center">70.8</td>
<td id="S3.SS2.2.3.29.29.4" class="ltx_td ltx_align_center">63.0</td>
<td id="S3.SS2.2.3.29.29.5" class="ltx_td ltx_align_center">61.6</td>
</tr>
<tr id="S3.SS2.2.3.30.30" class="ltx_tr">
<th id="S3.SS2.2.3.30.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Alpaca<span id="S3.SS2.2.3.30.30.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="S3.SS2.2.3.30.30.2" class="ltx_td ltx_align_center">45.4</td>
<td id="S3.SS2.2.3.30.30.3" class="ltx_td ltx_align_center">68.2</td>
<td id="S3.SS2.2.3.30.30.4" class="ltx_td ltx_align_center">64.0</td>
<td id="S3.SS2.2.3.30.30.5" class="ltx_td ltx_align_center">64.0</td>
</tr>
<tr id="S3.SS2.2.3.31.31" class="ltx_tr">
<th id="S3.SS2.2.3.31.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LLaMA2<span id="S3.SS2.2.3.31.31.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="S3.SS2.2.3.31.31.2" class="ltx_td ltx_align_center">52.6</td>
<td id="S3.SS2.2.3.31.31.3" class="ltx_td ltx_align_center">66.8</td>
<td id="S3.SS2.2.3.31.31.4" class="ltx_td ltx_align_center">69.4</td>
<td id="S3.SS2.2.3.31.31.5" class="ltx_td ltx_align_center">69.2</td>
</tr>
<tr id="S3.SS2.2.3.32.32" class="ltx_tr">
<th id="S3.SS2.2.3.32.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPT-3.5</th>
<td id="S3.SS2.2.3.32.32.2" class="ltx_td ltx_align_center">60.4</td>
<td id="S3.SS2.2.3.32.32.3" class="ltx_td ltx_align_center">70.2</td>
<td id="S3.SS2.2.3.32.32.4" class="ltx_td ltx_align_center">84.0</td>
<td id="S3.SS2.2.3.32.32.5" class="ltx_td ltx_align_center">83.4</td>
</tr>
<tr id="S3.SS2.2.3.33.33" class="ltx_tr">
<th id="S3.SS2.2.3.33.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">GPT-4</th>
<td id="S3.SS2.2.3.33.33.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.SS2.2.3.33.33.2.1" class="ltx_text ltx_font_bold">62.2</span></td>
<td id="S3.SS2.2.3.33.33.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.SS2.2.3.33.33.3.1" class="ltx_text ltx_font_bold">76.8</span></td>
<td id="S3.SS2.2.3.33.33.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.SS2.2.3.33.33.4.1" class="ltx_text ltx_font_bold">88.8</span></td>
<td id="S3.SS2.2.3.33.33.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.SS2.2.3.33.33.5.1" class="ltx_text ltx_font_bold">90.4</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Faithfulness-level automated evaluation results on the <span id="S3.SS2.2.6.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> dataset. Within each experimental setting, we used TAPEX-Acc as the ranking indicator of model performance. <sup id="S3.SS2.2.7.2" class="ltx_sup">∗</sup>: It is challenging for other LLMs to follow the instructions in 0-shot prompt to generate five statements for the input table.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4" class="ltx_section ltx_figure_panel">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In the following subsections, we discuss the three key research questions about adopting LLMs into real-world table information seeking scenarios. Specifically, we explore LLMs’ capabilities for table-to-text generation tasks, their ability to assess factual consistency, and whether they can benefit smaller fine-tuned models. The examined systems for each experiment are discussed in Appendix <a href="#A2" title="Appendix B Examined Systems ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>RQ1: How do LLMs perform in table-to-text generation tasks?</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We experiment with two in-context learning methods, <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">Direct Prediction</em> (Figure <a href="#A3.F5" title="Figure 5 ‣ Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> in Appendix) and <em id="S4.SS1.p1.1.2" class="ltx_emph ltx_font_italic">Chain of Thoughts</em> (CoT, Figure <a href="#A3.F6" title="Figure 6 ‣ Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> in Appendix), to solve the table-to-text generation tasks.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data Insight Generation Results</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">The results on the <span id="S4.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> dataset, as displayed in Table <a href="#S3.SS2" title="3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> and Table <a href="#S4.SS1.SSS0.Px2" title="Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>, indicate that GPT-* models generally surpass the current top-performing fine-tuned models (i.e., <span id="S4.SS1.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">LoFT</span> and PLOG) even in a 0-shot setting. Meanwhile, LLaMA-based models (e.g., LLaMA, Alpaca, Vicuna, <span id="S4.SS1.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">Tülu</span>) manage to achieve comparable performance to these top-performing fine-tuned models in a 2-shot setting. However, when it comes to the more challenging <span id="S4.SS1.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">LoTNLG</span> dataset, the automated evaluation result shows that only GPT-4 is capable of generating faithful statements that adhere to the specified logical reasoning types (Table <a href="#A3" title="Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> in Appendix).
Moreover, increasing the number of shots or applying chain-of-thought approach does not always yield a performance gain, motivating us to explore more advanced prompting methods for data insight generation in future work.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Query-based Generation Results</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">Table <a href="#A3" title="Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> and <a href="#A3" title="Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> in Appendix display the automated evaluation results for the FeTaQA and F2WTQ datasets, respectively. On FeTaQA, both LLaMA-based LLM and GPT-* models achieve comparable performance to the current top-performing fine-tuned models in a 2-shot setting, indicating the capability of LLMs to answer questions requiring surface-level facts from the table. However, a significant performance gap exists between other LLMs and GPT-* models on the more challenging F2WTQ dataset. Moreover, increasing the number of shots or applying the chain-of-thought approach can both yield performance gains for query-based generation.</p>
</div>
<figure id="S4.SS1.SSS0.Px2.tab1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S4.SS1.SSS0.Px2.tab1.1" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS1.SSS0.Px2.tab1.1.1.1" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Model</th>
<th id="S4.SS1.SSS0.Px2.tab1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Fluency (1-5)</th>
<th id="S4.SS1.SSS0.Px2.tab1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Faithfulness (0-1)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS1.SSS0.Px2.tab1.1.2.1" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT2-C2F</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">3.85</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">0.54</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.3.2" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">R2D2</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.29</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.72</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.4.3" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">PLOG</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.23</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.77</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.5.4" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.SS1.SSS0.Px2.tab1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">LoFT</span></th>
<td id="S4.SS1.SSS0.Px2.tab1.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.42</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.81</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.6.5" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.SS1.SSS0.Px2.tab1.1.6.5.1.1" class="ltx_ERROR undefined">\cdashline</span>1-3
GPT-4 0-shot</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.SS1.SSS0.Px2.tab1.1.6.5.2.1" class="ltx_text ltx_font_bold">4.82</span></td>
<td id="S4.SS1.SSS0.Px2.tab1.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.90</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.7.6" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna 2-shot Direct</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.7.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.69</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.7.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.71</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.8.7" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna 2-shot CoT</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.8.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.65</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.8.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.73</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.9.8" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2 2-shot Direct</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.9.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.75</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.9.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.79</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.10.9" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2 2-shot CoT</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.10.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.70</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.10.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.83</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.11.10" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4 2-shot Direct</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.11.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">4.71</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.11.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">0.89</td>
</tr>
<tr id="S4.SS1.SSS0.Px2.tab1.1.12.11" class="ltx_tr">
<th id="S4.SS1.SSS0.Px2.tab1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4 2-shot CoT</th>
<td id="S4.SS1.SSS0.Px2.tab1.1.12.11.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">4.77</td>
<td id="S4.SS1.SSS0.Px2.tab1.1.12.11.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.SS1.SSS0.Px2.tab1.1.12.11.3.1" class="ltx_text ltx_font_bold">0.92</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Human evaluation results on <span id="S4.SS1.SSS0.Px2.tab1.3.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span>.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4.SS2" class="ltx_subsection ltx_figure_panel">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>RQ2: Can we use LLMs to assess factual consistency of table-to-text generation?</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In RQ1, we demonstrate that LLMs can generate statements with comparative or even greater factual consistency than fine-tuned models. One natural follow-up question is whether we can employ LLMs to evaluate the faithfulness of table-to-text generation systems. This capability is crucial, as it ensures that tabular data is accurately interpreted for users, thereby preserving the credibility and reliability of real-world applications.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">As discussed in Section <a href="#S3.SS1" title="3.1 Automated Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, existing faithfulness-level NLI-based metrics are trained on the TabFact dataset <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>)</cite>. Recent work <cite class="ltx_cite ltx_citemacro_cite">Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> has revealed that large language models using chain-of-thought prompting can achieve competitive results on TabFact. Motivated by this finding, we use the same <em id="S4.SS2.p2.1.1" class="ltx_emph ltx_font_italic">2-shot chain-of-thought prompt</em> (Figure <a href="#A3.F7" title="Figure 7 ‣ Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> in Appendix) as <cite class="ltx_cite ltx_citemacro_citet">Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> to generate factual consistency scores (0 for refuted and 1 for entailed) for output sentences from LogicNLG. We use GPT-3.5 and GPT-4 as the backbones, as they outperforms other LLMs in RQ1 experiments. We refer to these new metrics as <em id="S4.SS2.p2.1.2" class="ltx_emph ltx_font_italic">CoT-3.5-Acc</em> and <em id="S4.SS2.p2.1.3" class="ltx_emph ltx_font_italic">CoT-4-Acc</em>, respectively.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">CoT-Acc Metrics Achieve Better Correlation with Human Judgement</h5>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We leverage the human evaluation results of models (excluding GPT-4 models) in RQ1 as the <em id="S4.SS2.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">human judgement</em>. We then compare the system-level Pearson’s correlation between each evaluation metric and this human judgement.
As shown in Table <a href="#S4.T4" title="Table 4 ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the proposed CoT-4-Acc and CoT-3.5-Acc metrics achieve the highest and third highest correlation with human judgement, respectively. This result demonstrates LLMs’ capabilities in assessing the faithfulness of table-to-text generation.
It’s worth noting that although TAPAS-Acc and TAPEX-Acc perform better than CoT-4-Acc on the TabFact dataset, they exhibit lower correlation with human judgement on table-to-text evaluation. We suspect that this can be largely attributed to over-fitting on the TabFact dataset, where negative examples are created by rewriting from the positive examples. We believe that future work can explore the development of a more robust faithfulness-level metric with better alignment to human evaluation.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Metric</th>
<th id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Acc on Tabfact</th>
<th id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Pearson’s correlation</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.2.1" class="ltx_tr">
<th id="S4.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">SP-Acc</th>
<td id="S4.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">63.5</td>
<td id="S4.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">.458</td>
</tr>
<tr id="S4.T4.1.3.2" class="ltx_tr">
<th id="S4.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">NLI-Acc</th>
<td id="S4.T4.1.3.2.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.1</td>
<td id="S4.T4.1.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">.526</td>
</tr>
<tr id="S4.T4.1.4.3" class="ltx_tr">
<th id="S4.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">TAPAS-Acc</th>
<td id="S4.T4.1.4.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">81.0</td>
<td id="S4.T4.1.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">.705</td>
</tr>
<tr id="S4.T4.1.5.4" class="ltx_tr">
<th id="S4.T4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">TAPEX-Acc</th>
<td id="S4.T4.1.5.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T4.1.5.4.2.1" class="ltx_text ltx_font_bold">84.2</span></td>
<td id="S4.T4.1.5.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">.804</td>
</tr>
<tr id="S4.T4.1.6.5" class="ltx_tr">
<th id="S4.T4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T4.1.6.5.1.1" class="ltx_text ltx_font_bold">CoT-3.5-Acc</span></th>
<td id="S4.T4.1.6.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">78.0</td>
<td id="S4.T4.1.6.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">.787</td>
</tr>
<tr id="S4.T4.1.7.6" class="ltx_tr">
<th id="S4.T4.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T4.1.7.6.1.1" class="ltx_text ltx_font_bold">CoT-4-Acc</span></th>
<td id="S4.T4.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">80.9</td>
<td id="S4.T4.1.7.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S4.T4.1.7.6.3.1" class="ltx_text ltx_font_bold">.816</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>System-level Pearson’s correlation bettwen each automated evaluation metric and human judgement. We also report the accuracy of automated evaluation metrics on the TabFact dataset for reference.</figcaption>
</figure>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities?</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In RQ1 and RQ2, we demonstrate the strong capability of state-of-the-art LLMs in table-to-text generation and evaluation. We next explore how fine-tuned smaller models can benefit from these abilities. We believe such exploration can provide insights for future work regarding the distillation of text generation capabilities from LLMs to smaller models <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023a</a>); Scheurer et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); Madaan et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>. This is essential as deploying smaller, yet performance-comparable models in real-world applications could save computational resources and inference time.</p>
</div>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Generating Feedback for Improving Factual Consistency</h5>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Utilizing human feedback to enhance neural models has emerged as a significant area of interest in contemporary research <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022c</a>); Gao et al. (<a href="#bib.bib10" title="" class="ltx_ref">2023a</a>); Scheurer et al. (<a href="#bib.bib38" title="" class="ltx_ref">2023</a>); Madaan et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>.
For example, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022c</a>)</cite> illustrates that human-written feedback can be leveraged to improve factual consistency of text summarization systems. <cite class="ltx_cite ltx_citemacro_citet">Madaan et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> demonstrates that LLMs can improve their initial outputs through iterative feedback and refinement.
This work investigates whether LLMs can provide human-like feedback for outputs from fine-tuned models. Following <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022c</a>)</cite>, we consider generating feedback with three components: 1) <em id="S4.SS3.SSS0.Px1.p1.1.1" class="ltx_emph ltx_font_italic">Explanation</em>, which determine whether the initial statement is factually consistent with the given table; 2) <em id="S4.SS3.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic">Corrective Instruction</em>, which provide instructions on how to correct the initial statement if it is detected as unfaithful; and 3) <em id="S4.SS3.SSS0.Px1.p1.1.3" class="ltx_emph ltx_font_italic">Edited Statement</em>, which edits the initial statement following the corrective instruction. Figure <a href="#A3.F8" title="Figure 8 ‣ Appendix C Experiments ‣ Ethical Consideration ‣ 6 Conclusion ‣ Large Language Models ‣ 5 Related Work ‣ Feedback from LLMs is of High Quality ‣ Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> in Appendix shows an example of <em id="S4.SS3.SSS0.Px1.p1.1.4" class="ltx_emph ltx_font_italic">2-shot chain-of-thought</em> prompts we use for feedback generation.</p>
</div>
<figure id="S4.SS3.SSS0.Px1.tab1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.SS3.SSS0.Px1.tab1.1" class="ltx_inline-block ltx_figure_panel ltx_transformed_outer" style="width:433.6pt;height:572.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(88.4pt,-116.7pt) scale(1.68887710317579,1.68887710317579) ;">
<table id="S4.SS3.SSS0.Px1.tab1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.1.1" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Models</th>
<th id="S4.SS3.SSS0.Px1.tab1.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPAS-Acc</th>
<th id="S4.SS3.SSS0.Px1.tab1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPEX-Acc</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.2.1" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT2-C2F</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">46.2</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">43.8</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.3.2" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by LLaMA2<span id="S4.SS3.SSS0.Px1.tab1.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.3.2.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">58.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.3.2.2.1" class="ltx_text" style="color:#008C00;">(+11.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">50.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.3.2.3.1" class="ltx_text" style="color:#008C00;">(+6.2)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.4.3" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-3.5</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.4.3.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">71.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.4.3.2.1" class="ltx_text" style="color:#008C00;">(+24.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.4.3.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">68.4 <span id="S4.SS3.SSS0.Px1.tab1.1.1.4.3.3.1" class="ltx_text" style="color:#008C00;">(+24.6)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.5.4" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-4</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.5.4.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">81.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.5.4.2.1" class="ltx_text" style="color:#008C00;">(+34.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.5.4.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">82.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.5.4.3.1" class="ltx_text" style="color:#008C00;">(+38.2)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.6.5" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.SS3.SSS0.Px1.tab1.1.1.6.5.1.1" class="ltx_ERROR undefined">\cdashline</span>1-5
R2D2</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.6.5.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">60.2</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.6.5.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">61.0</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.7.6" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by LLaMA2<span id="S4.SS3.SSS0.Px1.tab1.1.1.7.6.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.7.6.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">65.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.7.6.2.1" class="ltx_text" style="color:#008C00;">(+4.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.7.6.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">60.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.7.6.3.1" class="ltx_text" style="color:#FF0000;">(-1.0)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.8.7" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-3.5</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.8.7.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">74.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.8.7.2.1" class="ltx_text" style="color:#008C00;">(+13.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.8.7.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">74.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.8.7.3.1" class="ltx_text" style="color:#008C00;">(+13.0)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.9.8" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-4</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.9.8.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">87.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.9.8.2.1" class="ltx_text" style="color:#008C00;">(+26.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.9.8.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">89.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.9.8.3.1" class="ltx_text" style="color:#008C00;">(+28.0)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.10.9" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.SS3.SSS0.Px1.tab1.1.1.10.9.1.1" class="ltx_ERROR undefined">\cdashline</span>1-5
PLOG</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.10.9.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">63.8</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.10.9.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">69.6</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.11.10" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by LLaMA2<span id="S4.SS3.SSS0.Px1.tab1.1.1.11.10.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.11.10.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">75.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.11.10.2.1" class="ltx_text" style="color:#008C00;">(+11.2)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.11.10.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">66.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.11.10.3.1" class="ltx_text" style="color:#FF0000;">(-3.6)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.12.11" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-3.5</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.12.11.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">70.6 <span id="S4.SS3.SSS0.Px1.tab1.1.1.12.11.2.1" class="ltx_text" style="color:#008C00;">(+6.8)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.12.11.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">67.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.12.11.3.1" class="ltx_text" style="color:#008C00;">(-2.6)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.13.12" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-4</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.13.12.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">91.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.13.12.2.1" class="ltx_text" style="color:#008C00;">(+27.2)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.13.12.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">86.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.13.12.3.1" class="ltx_text" style="color:#008C00;">(+16.4)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.14.13" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S4.SS3.SSS0.Px1.tab1.1.1.14.13.1.1" class="ltx_ERROR undefined">\cdashline</span>1-5
<span id="S4.SS3.SSS0.Px1.tab1.1.1.14.13.1.2" class="ltx_text ltx_font_smallcaps">LoFT</span>
</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.14.13.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">67.4</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.14.13.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">61.4</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.15.14" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by LLaMA2<span id="S4.SS3.SSS0.Px1.tab1.1.1.15.14.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.15.14.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">72.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.15.14.2.1" class="ltx_text" style="color:#008C00;">(+4.6)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.15.14.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">64.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.15.14.3.1" class="ltx_text" style="color:#008C00;">(+2.6)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.16.15" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-3.5</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.16.15.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">70.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.16.15.2.1" class="ltx_text" style="color:#008C00;">(+2.6)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.16.15.3" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">65.6 <span id="S4.SS3.SSS0.Px1.tab1.1.1.16.15.3.1" class="ltx_text" style="color:#008C00;">(+4.2)</span>
</td>
</tr>
<tr id="S4.SS3.SSS0.Px1.tab1.1.1.17.16" class="ltx_tr">
<th id="S4.SS3.SSS0.Px1.tab1.1.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">Edit by GPT-4</th>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.17.16.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">81.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.17.16.2.1" class="ltx_text" style="color:#008C00;">(+13.6)</span>
</td>
<td id="S4.SS3.SSS0.Px1.tab1.1.1.17.16.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">86.0 <span id="S4.SS3.SSS0.Px1.tab1.1.1.17.16.3.1" class="ltx_text" style="color:#008C00;">(+24.6)</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Automated evaluation results on <span id="S4.SS3.SSS0.Px1.tab1.3.1" class="ltx_text ltx_font_smallcaps">LogicNLG</span> using statements pre-edited and post-edited by LLMs.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph ltx_figure_panel">
<h5 class="ltx_title ltx_title_paragraph">Feedback from LLMs is of High Quality</h5>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p">We assess the quality of generated feedback through automated evaluations.
Specifically, we examine the faithfulness scores of <em id="S4.SS3.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">Edited Statements</em> in the generated feedback, comparing these scores to those of the original statements.
We report TAPAS-Acc and TAPEX-Acc for experimental results, as these two metrics exhibit better alignment with human evaluation (Section <a href="#S4.SS2.SSS0.Px1" title="CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). As illustrated in Table <a href="#S4.SS3.SSS0.Px1" title="Generating Feedback for Improving Factual Consistency ‣ 4.3 RQ3: How can fine-tuned models benefit from LLMs’ strong table-to-text abilities? ‣ CoT-Acc Metrics Achieve Better Correlation with Human Judgement ‣ 4.2 RQ2: Can we use LLMs to assess factual consistency of table-to-text generation? ‣ Query-based Generation Results ‣ 4.1 RQ1: How do LLMs perform in table-to-text generation tasks? ‣ 4 Experiments ‣ 3.2 Human Evaluation ‣ 3 Evaluation System ‣ Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, LLMs can effectively edit statements to improve their faithfulness, particularly for outputs from lower-performance models, such as GPT2-C2F.</p>
</div>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Table-to-Text Generation</h5>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.1" class="ltx_p">Text generation from semi-structured knowledge sources, such as web tables, has been studied extensively in recent years <cite class="ltx_cite ltx_citemacro_cite">Parikh et al. (<a href="#bib.bib33" title="" class="ltx_ref">2020</a>); Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>); Cheng et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>); Zhao et al. (<a href="#bib.bib49" title="" class="ltx_ref">2023a</a>)</cite>. The goal of the table-to-text generation task is to generate natural language statements that faithfully describe information contained in the provided table region.
The most popular approach for table-to-text generation tasks is to fine-tune a pre-trained language model on a task-specific dataset <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>); Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022a</a>); Zhao et al. (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>); Nan et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>); Zhao et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite>.
To the best of our knowledge, we are the first to systematically evaluate the performance of LLMs on table-to-text generation tasks.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Large Language Models</h5>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.1" class="ltx_p">LLMs have demonstrated remarkable in-context learning capabilities <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Chowdhery et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>); Scao et al. (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>); Chung et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>); OpenAI (<a href="#bib.bib31" title="" class="ltx_ref">2023</a>)</cite>, where the model receives a task demonstration in natural language accompanied by a limited number of examples. The Chain-of-Thought prompting methods <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>); Wang et al. (<a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite> further empower LLMs to perform complex reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">Han et al. (<a href="#bib.bib15" title="" class="ltx_ref">2022</a>); Zhao et al. (<a href="#bib.bib52" title="" class="ltx_ref">2023c</a>); Ye et al. (<a href="#bib.bib45" title="" class="ltx_ref">2023</a>); Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>.
More recent works <cite class="ltx_cite ltx_citemacro_cite">Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>); Nan et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> investigate in-context learning capabilities of LLMs on table-based tasks, including table question answering <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>); Iyyer et al. (<a href="#bib.bib17" title="" class="ltx_ref">2017</a>); Zhong et al. (<a href="#bib.bib53" title="" class="ltx_ref">2018</a>)</cite> and table fact checking <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2020b</a>); Gupta et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>.
However, the potential of LLMs in generating text from tabular data remains underexplored.</p>
</div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper investigates the potential of applying LLMs in real-world table information seeking scenarios. We demonstrate their superiority in faithfulness, and their potential as evaluation systems. Further, we provide valuable insights into leveraging LLMs to generate high-fidelity natural language feedback. We believe that the findings of this study could benefit real-world applications, aimed at improving user efficiency in data analysis.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethical Consideration</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span> and F2WTQ were constructed upon the test set of <span id="Sx1.p1.1.2" class="ltx_text ltx_font_smallcaps">LogicNLG</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> and WTQ <cite class="ltx_cite ltx_citemacro_cite">Pasupat and Liang (<a href="#bib.bib34" title="" class="ltx_ref">2015</a>)</cite> datasets, which are publicly available under the licenses of MIT<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://opensource.org/licenses/MIT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://opensource.org/licenses/MIT</a></span></span></span> and CC BY-SA 4.0<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://creativecommons.org/licenses/by-sa/4.0/</a></span></span></span>, respectively. These licenses permit us to modify, publish, and distribute additional annotations upon the original dataset.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et al. (2023)</span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
Wal. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2304.01373" title="" class="ltx_ref ltx_href">Pythia: A suite for
analyzing large language models across training and scaling</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, et al. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 33, pages 1877–1901. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2023)</span>
<span class="ltx_bibblock">
Wenhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.findings-eacl.83" title="" class="ltx_ref ltx_href">Large
language models are few(1)-shot table reasoners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EACL 2023</em>, pages 1120–1130, Dubrovnik, Croatia. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020a)</span>
<span class="ltx_bibblock">
Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang.
2020a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.708" title="" class="ltx_ref ltx_href">Logical
natural language generation from open-domain tables</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7929–7942, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020b)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li,
Xiyou Zhou, and William Yang Wang. 2020b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=rkeJRhNYDH" title="" class="ltx_ref ltx_href">Tabfact: A
large-scale dataset for table-based fact verification</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. (2022)</span>
<span class="ltx_bibblock">
Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han,
Jian-Guang Lou, and Dongmei Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.78" title="" class="ltx_ref ltx_href">HiTab: A
hierarchical table dataset for question answering and natural language
generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1094–1110,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and
Eric P. Xing. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_href">Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2204.02311.pdf" title="" class="ltx_ref ltx_href">Palm: Scaling language
modeling with pathways</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2204.02311.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric
Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, et al.
2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2210.11416" title="" class="ltx_ref ltx_href">Scaling
instruction-finetuned language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2210.11416.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023a)</span>
<span class="ltx_bibblock">
Ge Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2305.12473" title="" class="ltx_ref ltx_href">Continually improving
extractive qa via human feedback</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023b)</span>
<span class="ltx_bibblock">
Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan.
2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2304.02554" title="" class="ltx_ref ltx_href">Human-like summarization
evaluation with chatgpt</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.02554</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemmell and Dalton (2023)</span>
<span class="ltx_bibblock">
Carlos Gemmell and Jeffrey Stephen Dalton. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2303.10138.pdf" title="" class="ltx_ref ltx_href">Generate, transform,
answer: Question specific tool synthesis for tabular data</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.10138.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2022)</span>
<span class="ltx_bibblock">
Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2209.12356" title="" class="ltx_ref ltx_href">News summarization and
evaluation in the era of gpt-3</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.12356</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2020)</span>
<span class="ltx_bibblock">
Vivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.210" title="" class="ltx_ref ltx_href">INFOTABS:
Inference on tables as semi-structured data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2309–2324, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2022)</span>
<span class="ltx_bibblock">
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke
Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng,
Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan,
Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fabbri, Wojciech
Kryscinski, Xi Victoria Lin, Caiming Xiong, and Dragomir R. Radev. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2209.00840.pdf" title="" class="ltx_ref ltx_href">Folio: Natural language
reasoning with first-order logic</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2209.00840.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno,
and Julian Eisenschlos. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.398" title="" class="ltx_ref ltx_href">TaPas:
Weakly supervised table parsing via pre-training</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 4320–4333, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iyyer et al. (2017)</span>
<span class="ltx_bibblock">
Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P17-1167" title="" class="ltx_ref ltx_href">Search-based neural
structured learning for sequential question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1821–1831,
Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2022)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.68" title="" class="ltx_ref ltx_href">OmniTab:
Pretraining with natural and synthetic data for few-shot table-based question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 932–942, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Köksal et al. (2023)</span>
<span class="ltx_bibblock">
Abdullatif Köksal, Timo Schick, Anna Korhonen, and Hinrich Schütze.
2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2209.12356" title="" class="ltx_ref ltx_href">Longform: Optimizing
instruction tuning for long text generation with corpus extraction</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08460</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.703" title="" class="ltx_ref ltx_href">BART:
Denoising sequence-to-sequence pre-training for natural language generation,
translation, and comprehension</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 7871–7880, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2305.19308.pdf" title="" class="ltx_ref ltx_href">Sheetcopilot: Bringing
software productivity to the next level through large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2305.19308.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/W04-1013" title="" class="ltx_ref ltx_href">ROUGE: A package for
automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Text Summarization Branches Out</em>, pages 74–81, Barcelona,
Spain. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022a)</span>
<span class="ltx_bibblock">
Ao Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and Dongmei Zhang.
2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.emnlp-main.373" title="" class="ltx_ref ltx_href">PLOG:
Table-to-logic pretraining for logical table-to-text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 5531–5546, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022b)</span>
<span class="ltx_bibblock">
Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and
Jian-Guang Lou. 2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=O50443AsCP" title="" class="ltx_ref ltx_href">TAPEX: Table
pre-training via learning a neural SQL executor</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022c)</span>
<span class="ltx_bibblock">
Yixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L Halfaker, Dragomir R. Radev,
and Ahmed Hassan Awadallah. 2022c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2212.09968.pdf" title="" class="ltx_ref ltx_href">On improving
summarization factual consistency from natural language feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2212.09968.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al. (2023)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.
2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.17651" title="" class="ltx_ref ltx_href">Self-refine: Iterative
refinement with self-feedback</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.17651</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan et al. (2022a)</span>
<span class="ltx_bibblock">
Linyong Nan, Lorenzo Jaime Flores, Yilun Zhao, Yixin Liu, Luke Benson, Weijin
Zou, and Dragomir Radev. 2022a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.emnlp-main.464" title="" class="ltx_ref ltx_href">R2D2:
Robust data-to-text with replacement detection</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 6903–6917, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan et al. (2022b)</span>
<span class="ltx_bibblock">
Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui
Zhang, Wojciech Kryściński, Hailey Schoelkopf, Riley Kong, Xiangru
Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree Bandaru, Jacob
Cunningham, Caiming Xiong, Dragomir Radev, and Dragomir Radev.
2022b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00446" title="" class="ltx_ref ltx_href">FeTaQA: Free-form
table question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
10:35–49.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan et al. (2021)</span>
<span class="ltx_bibblock">
Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad,
Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,
Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi,
Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan,
Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani.
2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.naacl-main.37" title="" class="ltx_ref ltx_href">DART:
Open-domain structured data record to text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 432–447, Online. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nan et al. (2023)</span>
<span class="ltx_bibblock">
Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang,
Arman Cohan, and Dragomir Radev. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.12586" title="" class="ltx_ref ltx_href">Enhancing few-shot
text-to-sql capabilities of large language models: A study on prompt design
strategies</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2303.08774.pdf" title="" class="ltx_ref ltx_href">Gpt-4 technical
report</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/1073083.1073135" title="" class="ltx_ref ltx_href">Bleu: a method for
automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania,
USA. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parikh et al. (2020)</span>
<span class="ltx_bibblock">
Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra,
Diyi Yang, and Dipanjan Das. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.89" title="" class="ltx_ref ltx_href">ToTTo: A
controlled table-to-text generation dataset</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 1173–1186, Online. Association
for Computational Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang (2015)</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.3115/v1/P15-1142" title="" class="ltx_ref ltx_href">Compositional semantic
parsing on semi-structured tables</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 1470–1480,
Beijing, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perlitz et al. (2022)</span>
<span class="ltx_bibblock">
Yotam Perlitz, Liat Ein-Dor, Dafna Sheinwald, Noam Slonim, and Michal
Shmueli-Scheuer. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2205.10938.pdf" title="" class="ltx_ref ltx_href">Diversity enhanced
table-to-text generation via type control</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2205.10938.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://jmlr.org/papers/v21/20-074.html" title="" class="ltx_ref ltx_href">Exploring the limits
of transfer learning with a unified text-to-text transformer</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Journal of Machine Learning Research</em>, 21(140):1–67.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et al. (2022)</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
Yvon, Matthias Gallé, et al. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2211.05100" title="" class="ltx_ref ltx_href">Bloom: A 176b-parameter
open-access multilingual language model</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scheurer et al. (2023)</span>
<span class="ltx_bibblock">
J’er’emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica
Chen, Kyunghyun Cho, and Ethan Perez. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.16755" title="" class="ltx_ref ltx_href">Training language models
with language feedback at scale</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2303.16755.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2302.13971" title="" class="ltx_ref ltx_href">Llama: Open and efficient
foundation language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi,
Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian
Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne
Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,
Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,
Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan
Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang,
Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2307.09288" title="" class="ltx_ref ltx_href">Llama 2: Open foundation
and fine-tuned chat models</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny
Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2203.11171.pdf" title="" class="ltx_ref ltx_href">Self-consistency
improves chain of thought reasoning in language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2203.11171.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,
Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2306.04751" title="" class="ltx_ref ltx_href">How far can camels go?
exploring the state of instruction tuning on open resources</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
Ed H. Chi, Quoc V Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=_VjQlMeSB_J" title="" class="ltx_ref ltx_href">Chain of thought
prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2301.13808.pdf" title="" class="ltx_ref ltx_href">Large language models
are versatile decomposers: Decompose evidence and questions for table-based
reasoning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2301.13808.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zha et al. (2023)</span>
<span class="ltx_bibblock">
Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing
Yuan, Changbao Su, Xiang Li, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou,
Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye, Yali Ye, Wentao Ye, Yiming Zhang,
Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2307.08674" title="" class="ltx_ref ltx_href">Tablegpt: Towards unifying
tables, nature language and commands into one gpt</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yue Ting Zhuang. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/pdf/2306.07209.pdf" title="" class="ltx_ref ltx_href">Data-copilot: Bridging
billions of data and humans with autonomous workflow</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2306.07209.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Yusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev,
Chenguang Zhu, Michael Zeng, and Rui Zhang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2211.05041" title="" class="ltx_ref ltx_href">Macsum: Controllable
summarization with mixed attributes</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.05041</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023a)</span>
<span class="ltx_bibblock">
Yilun Zhao, Boyu Mi, Zhenting Qi, Linyong Nan, Minghao Guo, Arman Cohan, and
Dragomir Radev. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2023.acl-demo.32" title="" class="ltx_ref ltx_href">OpenRT: An
open-source framework for reasoning over tabular data</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 3: System Demonstrations)</em>, pages
336–347, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2022)</span>
<span class="ltx_bibblock">
Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, and Dragomir Radev. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.emnlp-main.615" title="" class="ltx_ref ltx_href">ReasTAP:
Injecting table reasoning skills during pre-training via synthetic reasoning
examples</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 9006–9018, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023b)</span>
<span class="ltx_bibblock">
Yilun Zhao, Zhenting Qi, Linyong Nan, Lorenzo Jaime Flores, and Dragomir Radev.
2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2023.eacl-main.40" title="" class="ltx_ref ltx_href">LoFT:
Enhancing faithfulness and diversity for table-to-text generation via logic
form control</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th Conference of the European Chapter
of the Association for Computational Linguistics</em>, pages 554–561, Dubrovnik,
Croatia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023c)</span>
<span class="ltx_bibblock">
Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng
Han, Xiangru Tang, Yumo Xu, Arman Cohan, and Dragomir Radev.
2023c.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2305.14303" title="" class="ltx_ref ltx_href">Qtsumm: A new benchmark for
query-focused table summarization</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2018)</span>
<span class="ltx_bibblock">
Victor Zhong, Caiming Xiong, and Richard Socher. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Syx6bz-Ab" title="" class="ltx_ref ltx_href">Seq2SQL:
Generating structured queries from natural language using reinforcement
learning</a>.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2303.11315" title="" class="ltx_ref ltx_href">Context-faithful prompting
for large language models</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.11315</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Table-to-Text Generation Benchmarks</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span><span id="A1.SS1.1.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span> Dataset</h3>

<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Logical Reasoning Type Definition</h5>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para">
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">Aggregation: operations involving sum or average operation to summarize the overall statistics. Sentence: The total number of scores of xxx is xxx. The average value of xxx is xxx.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">Negation: operations to negate. Sentence: xxx did not get the first prize.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">Superlative: superlative operations to get the highest or lowest value. Sentence: xxx achieved the most scores.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">Count: operations to count the amount of entities that fulfil certain conditions. Sentence: There are 4 people born in xxx.</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">Comparative: operations to compare a specific aspect of two or more entities. Sentence: xxx is taller than xxx.</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">Ordinal: operations to identify the ranking of entities in a specific aspect. Sentence: xxx is the third youngest player in the game.</p>
</div>
</li>
<li id="A1.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i7.p1" class="ltx_para">
<p id="A1.I1.i7.p1.1" class="ltx_p">Unique: operations to identify different entities. Sentence: The players come from 7 different cities.</p>
</div>
</li>
<li id="A1.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i8.p1" class="ltx_para">
<p id="A1.I1.i8.p1.1" class="ltx_p">All: operations to summarize what all entities do/have in common. Sentence: All of the xxx are more expensive than $25.</p>
</div>
</li>
<li id="A1.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i9.p1" class="ltx_para">
<p id="A1.I1.i9.p1.1" class="ltx_p">Surface-Level: no logical reasoning type above. Sentence: xxx is moving to xxx.</p>
</div>
</li>
</ul>
</div>
<figure id="A1.F4" class="ltx_figure"><img src="/html/2305.14987/assets/figures/figures/type_distribution.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="325" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Distribution of logical reasoning types for the <span id="A1.F4.2.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span> dataset.</figcaption>
</figure>
</section>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Examined Systems</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Fine-tuned Models</h3>

<div id="A2.SS1.p1" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">BART</span> <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> is a pre-trained denoising autoencoder with transformer-based architecture and shows effectiveness in NLG tasks.
</p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Flan-T5</span> <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> enhances T5 <cite class="ltx_cite ltx_citemacro_cite">Raffel et al. (<a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> by scaling instruction fine-tuning and demonstrates better human-like reasoning abilities than the T5.</p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">GPT2-C2F</span> <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib4" title="" class="ltx_ref">2020a</a>)</cite> first generates a template which determines the global logical structure, and then produces the statement using the template as control.</p>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">R2D2</span> <cite class="ltx_cite ltx_citemacro_cite">Nan et al. (<a href="#bib.bib27" title="" class="ltx_ref">2022a</a>)</cite> trains a generative language model both as a generator and a faithfulness discriminator with additional replacement detection and unlikelihood learning tasks, to enhance the faithfulness of table-to-text generation.</p>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">TAPEX</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022b</a>)</cite> continues pre-training the BART model by using a large-scale corpus of synthetic SQL query execution data, showing better table understanding and reasoning abilities.</p>
</div>
</li>
<li id="A2.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i6.p1" class="ltx_para">
<p id="A2.I1.i6.p1.1" class="ltx_p"><span id="A2.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">OmniTab</span> <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite> uses the same backbone as TAPEX, and is further pre-trained on collected natural and synthetic Table QA examples.</p>
</div>
</li>
<li id="A2.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i7.p1" class="ltx_para">
<p id="A2.I1.i7.p1.1" class="ltx_p"><span id="A2.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">ReasTAP</span> <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite> enhances the table understanding and reasoning abilities of BART by pre-training on a synthetic Table QA corpus.</p>
</div>
</li>
<li id="A2.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i8.p1" class="ltx_para">
<p id="A2.I1.i8.p1.1" class="ltx_p"><span id="A2.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">PLOG</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2022a</a>)</cite> continues pre-training text generation models on a table-to-logic-form generation task (i.e., T5 model), improving the faithfulness of table-to-text generation.</p>
</div>
</li>
<li id="A2.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i9.p1" class="ltx_para">
<p id="A2.I1.i9.p1.1" class="ltx_p"><span id="A2.I1.i9.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">LoFT</span> <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a href="#bib.bib51" title="" class="ltx_ref">2023b</a>)</cite> utilizes logic forms as fact verifiers and content planners to control table-to-text generation, exhibiting improved faithfulness and text diversity.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Large Language Models</h3>

<div id="A2.SS2.p1" class="ltx_para">
<ul id="A2.I2" class="ltx_itemize">
<li id="A2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i1.p1" class="ltx_para">
<p id="A2.I2.i1.p1.1" class="ltx_p"><span id="A2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Pythia</span> <cite class="ltx_cite ltx_citemacro_cite">Biderman et al. (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> is a suite of 16 open-sourced LLMs all trained on public data in the exact same order and ranging in size from 70M to 12B parameters. This helps researchers to gain a better understanding of LLMs and their training dynamics.</p>
</div>
</li>
<li id="A2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i2.p1" class="ltx_para">
<p id="A2.I2.i2.p1.1" class="ltx_p"><span id="A2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">LLaMA</span> <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib40" title="" class="ltx_ref">2023a</a>, <a href="#bib.bib41" title="" class="ltx_ref">b</a>)</cite> is an open-source LLM trained on large-scale and publicly available datasets. We evaluate both LLaMA and LLaMA2 in this paper.</p>
</div>
</li>
<li id="A2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i3.p1" class="ltx_para">
<p id="A2.I2.i3.p1.1" class="ltx_p"><span id="A2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Alpaca</span> <cite class="ltx_cite ltx_citemacro_cite">Taori et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite> and <span id="A2.I2.i3.p1.1.2" class="ltx_text ltx_font_bold">Vicuna</span> <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> are fine-tuned from LLaMA with instruction-following data, exhibiting better instruction-following capabilities.</p>
</div>
</li>
<li id="A2.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i4.p1" class="ltx_para">
<p id="A2.I2.i4.p1.1" class="ltx_p"><span id="A2.I2.i4.p1.1.1" class="ltx_text ltx_font_bold ltx_font_smallcaps">Tülu</span> <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite> further trains LLaMA on 12 open-source instruction datasets, achieving better performance than LLaMA.</p>
</div>
</li>
<li id="A2.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I2.i5.p1" class="ltx_para">
<p id="A2.I2.i5.p1.1" class="ltx_p"><span id="A2.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">GPT</span> <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib2" title="" class="ltx_ref">2020</a>); Wei et al. (<a href="#bib.bib44" title="" class="ltx_ref">2022</a>)</cite> is a powerful large language model which is capable of generating human-like text and performing a wide range of NLP tasks in a few-shot setting. We use the OpenAI engines of <span id="A2.I2.i5.p1.1.2" class="ltx_text ltx_font_typewriter">gpt-3.5-0301</span> and <span id="A2.I2.i5.p1.1.3" class="ltx_text ltx_font_typewriter">gpt-4-0314</span> for GPT-3.5 and GPT-4 models, respectively.</p>
</div>
</li>
</ul>
</div>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.1" class="ltx_p">To formulate the prompt, we linearize the table as done in previous work on table reasoning <cite class="ltx_cite ltx_citemacro_cite">Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> and concatenate it with its corresponding reference statements as demonstrations. We use the table truncation strategy as proposed by <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a href="#bib.bib24" title="" class="ltx_ref">2022b</a>)</cite> to truncate large table and ensure that the prompts are within the maximum token limitation for each type of LLMs. For LLM parameter settings, we used a temperature of 0.7, maximum output length of 512, without any frequency or presence penalty.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Experiments</h2>

<figure id="A3.F5" class="ltx_figure"><img src="/html/2305.14987/assets/x3.png" id="A3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="366" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>An example of 1-shot <em id="A3.F5.3.1" class="ltx_emph ltx_font_italic">direct-prediction</em> prompting for the <span id="A3.F5.4.2" class="ltx_text ltx_font_smallcaps">LogicNLG</span> task.</figcaption>
</figure>
<figure id="A3.F6" class="ltx_figure"><img src="/html/2305.14987/assets/x4.png" id="A3.F6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="721" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An example of 1-shot <em id="A3.F6.3.1" class="ltx_emph ltx_font_italic">chain-of-thought</em> prompting for the <span id="A3.F6.4.2" class="ltx_text ltx_font_smallcaps">LogicNLG</span> task.</figcaption>
</figure>
<figure id="A3.3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A3.3.4" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.3.4.1.1" class="ltx_tr">
<td id="A3.3.4.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Type</td>
<td id="A3.3.4.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Models</td>
<td id="A3.3.4.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">SP-Acc</td>
<td id="A3.3.4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">NLI-Acc</td>
<td id="A3.3.4.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPAS-Acc</td>
<td id="A3.3.4.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPEX-Acc</td>
<td id="A3.3.4.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Type EM</td>
<td id="A3.3.4.1.1.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Type F1</td>
</tr>
<tr id="A3.3.4.2.2" class="ltx_tr">
<td id="A3.3.4.2.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="A3.3.4.2.2.1.1" class="ltx_text">0-shot*</span></td>
<td id="A3.3.4.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.4.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">51.2</td>
<td id="A3.3.4.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">77.2</td>
<td id="A3.3.4.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">70.8</td>
<td id="A3.3.4.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">66.8</td>
<td id="A3.3.4.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">59.2</td>
<td id="A3.3.4.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">43.8</td>
</tr>
<tr id="A3.3.4.3.3" class="ltx_tr">
<td id="A3.3.4.3.3.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.4.3.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.3.3.2.1" class="ltx_text ltx_font_bold">69.2</span></td>
<td id="A3.3.4.3.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.3.3.3.1" class="ltx_text ltx_font_bold">79.4</span></td>
<td id="A3.3.4.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.3.3.4.1" class="ltx_text ltx_font_bold">85.6</span></td>
<td id="A3.3.4.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.3.3.5.1" class="ltx_text ltx_font_bold">84.2</span></td>
<td id="A3.3.4.3.3.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.3.3.6.1" class="ltx_text ltx_font_bold">75.2</span></td>
<td id="A3.3.4.3.3.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.3.3.7.1" class="ltx_text ltx_font_bold">60.0</span></td>
</tr>
<tr id="A3.3.4.4.4" class="ltx_tr">
<td id="A3.3.4.4.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="A3.3.4.4.4.1.1" class="ltx_text">1-shot Direct</span></td>
<td id="A3.3.4.4.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">53.8</td>
<td id="A3.3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">75.6</td>
<td id="A3.3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">71.6</td>
<td id="A3.3.4.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">71.0</td>
<td id="A3.3.4.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">51.2</td>
<td id="A3.3.4.4.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">38.1</td>
</tr>
<tr id="A3.3.4.5.5" class="ltx_tr">
<td id="A3.3.4.5.5.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.4.5.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.5.5.2.1" class="ltx_text ltx_font_bold">60.2</span></td>
<td id="A3.3.4.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.5.5.3.1" class="ltx_text ltx_font_bold">72.8</span></td>
<td id="A3.3.4.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.5.5.4.1" class="ltx_text ltx_font_bold">83.8</span></td>
<td id="A3.3.4.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.5.5.5.1" class="ltx_text ltx_font_bold">84.2</span></td>
<td id="A3.3.4.5.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.5.5.6.1" class="ltx_text ltx_font_bold">76.6</span></td>
<td id="A3.3.4.5.5.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.5.5.7.1" class="ltx_text ltx_font_bold">63.0</span></td>
</tr>
<tr id="A3.3.4.6.6" class="ltx_tr">
<td id="A3.3.4.6.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="A3.3.4.6.6.1.1" class="ltx_ERROR undefined">\cdashline</span>1-8
<span id="A3.3.4.6.6.1.2" class="ltx_text">1-shot CoT</span>
</td>
<td id="A3.3.4.6.6.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.4.6.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.8</td>
<td id="A3.3.4.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.6.6.4.1" class="ltx_text ltx_font_bold">78.8</span></td>
<td id="A3.3.4.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">79.2</td>
<td id="A3.3.4.6.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">79.4</td>
<td id="A3.3.4.6.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.2</td>
<td id="A3.3.4.6.6.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">30.2</td>
</tr>
<tr id="A3.3.4.7.7" class="ltx_tr">
<td id="A3.3.4.7.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.4.7.7.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.7.7.2.1" class="ltx_text ltx_font_bold">59.2</span></td>
<td id="A3.3.4.7.7.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">74.8</td>
<td id="A3.3.4.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.7.7.4.1" class="ltx_text ltx_font_bold">84.4</span></td>
<td id="A3.3.4.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.7.7.5.1" class="ltx_text ltx_font_bold">85.8</span></td>
<td id="A3.3.4.7.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.7.7.6.1" class="ltx_text ltx_font_bold">70.0</span></td>
<td id="A3.3.4.7.7.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.7.7.7.1" class="ltx_text ltx_font_bold">51.6</span></td>
</tr>
<tr id="A3.3.4.8.8" class="ltx_tr">
<td id="A3.3.4.8.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="11"><span id="A3.3.4.8.8.1.1" class="ltx_text">2-shot Direct</span></td>
<td id="A3.3.4.8.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Pythia<span id="A3.3.4.8.8.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</td>
<td id="A3.3.4.8.8.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">44.2</td>
<td id="A3.3.4.8.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">60.6</td>
<td id="A3.3.4.8.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">41.8</td>
<td id="A3.3.4.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">43.0</td>
<td id="A3.3.4.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.0</td>
<td id="A3.3.4.8.8.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">12.2</td>
</tr>
<tr id="A3.3.4.9.9" class="ltx_tr">
<td id="A3.3.4.9.9.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.9.9.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</td>
<td id="A3.3.4.9.9.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">41.0</td>
<td id="A3.3.4.9.9.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.2</td>
<td id="A3.3.4.9.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.2</td>
<td id="A3.3.4.9.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.2</td>
<td id="A3.3.4.9.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.2</td>
<td id="A3.3.4.9.9.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.4</td>
</tr>
<tr id="A3.3.4.10.10" class="ltx_tr">
<td id="A3.3.4.10.10.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna<span id="A3.3.4.10.10.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.10.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.6</td>
<td id="A3.3.4.10.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.2</td>
<td id="A3.3.4.10.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.4</td>
<td id="A3.3.4.10.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.4</td>
<td id="A3.3.4.10.10.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">22.0</td>
<td id="A3.3.4.10.10.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.2</td>
</tr>
<tr id="A3.3.4.11.11" class="ltx_tr">
<td id="A3.3.4.11.11.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.11.11.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.11.11.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">44.6</td>
<td id="A3.3.4.11.11.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.4</td>
<td id="A3.3.4.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.8</td>
<td id="A3.3.4.11.11.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.8</td>
<td id="A3.3.4.11.11.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">22.6</td>
<td id="A3.3.4.11.11.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.8</td>
</tr>
<tr id="A3.3.4.12.12" class="ltx_tr">
<td id="A3.3.4.12.12.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Alpaca<span id="A3.3.4.12.12.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.12.12.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.2</td>
<td id="A3.3.4.12.12.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.8</td>
<td id="A3.3.4.12.12.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.8</td>
<td id="A3.3.4.12.12.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.0</td>
<td id="A3.3.4.12.12.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.8</td>
<td id="A3.3.4.12.12.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.8</td>
</tr>
<tr id="A3.3.4.13.13" class="ltx_tr">
<td id="A3.3.4.13.13.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2<span id="A3.3.4.13.13.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</td>
<td id="A3.3.4.13.13.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">44.2</td>
<td id="A3.3.4.13.13.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.0</td>
<td id="A3.3.4.13.13.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.0</td>
<td id="A3.3.4.13.13.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.0</td>
<td id="A3.3.4.13.13.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.2</td>
<td id="A3.3.4.13.13.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.8</td>
</tr>
<tr id="A3.3.4.14.14" class="ltx_tr">
<td id="A3.3.4.14.14.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.14.14.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</td>
<td id="A3.3.4.14.14.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">40.0</td>
<td id="A3.3.4.14.14.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.6</td>
<td id="A3.3.4.14.14.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">53.0</td>
<td id="A3.3.4.14.14.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.6</td>
<td id="A3.3.4.14.14.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.2</td>
<td id="A3.3.4.14.14.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.4</td>
</tr>
<tr id="A3.3.4.15.15" class="ltx_tr">
<td id="A3.3.4.15.15.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.15.15.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</td>
<td id="A3.3.4.15.15.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">46.2</td>
<td id="A3.3.4.15.15.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.8</td>
<td id="A3.3.4.15.15.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.0</td>
<td id="A3.3.4.15.15.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">51.8</td>
<td id="A3.3.4.15.15.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.0</td>
<td id="A3.3.4.15.15.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.2</td>
</tr>
<tr id="A3.3.4.16.16" class="ltx_tr">
<td id="A3.3.4.16.16.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.4.16.16.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="A3.3.4.16.16.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.16.16.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">44.2</td>
<td id="A3.3.4.16.16.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.8</td>
<td id="A3.3.4.16.16.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.8</td>
<td id="A3.3.4.16.16.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.8</td>
<td id="A3.3.4.16.16.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">26.6</td>
<td id="A3.3.4.16.16.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.4</td>
</tr>
<tr id="A3.3.4.17.17" class="ltx_tr">
<td id="A3.3.4.17.17.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.4.17.17.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">55.2</td>
<td id="A3.3.4.17.17.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.17.17.3.1" class="ltx_text ltx_font_bold">76.2</span></td>
<td id="A3.3.4.17.17.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">70.8</td>
<td id="A3.3.4.17.17.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.6</td>
<td id="A3.3.4.17.17.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.2</td>
<td id="A3.3.4.17.17.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">35.0</td>
</tr>
<tr id="A3.3.4.18.18" class="ltx_tr">
<td id="A3.3.4.18.18.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.4.18.18.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.18.18.2.1" class="ltx_text ltx_font_bold">61.4</span></td>
<td id="A3.3.4.18.18.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.2</td>
<td id="A3.3.4.18.18.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.18.18.4.1" class="ltx_text ltx_font_bold">84.6</span></td>
<td id="A3.3.4.18.18.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.18.18.5.1" class="ltx_text ltx_font_bold">83.2</span></td>
<td id="A3.3.4.18.18.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.18.18.6.1" class="ltx_text ltx_font_bold">73.4</span></td>
<td id="A3.3.4.18.18.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.18.18.7.1" class="ltx_text ltx_font_bold">54.8</span></td>
</tr>
<tr id="A3.3.4.19.19" class="ltx_tr">
<td id="A3.3.4.19.19.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="11">
<span id="A3.3.4.19.19.1.1" class="ltx_ERROR undefined">\cdashline</span>1-8
<span id="A3.3.4.19.19.1.2" class="ltx_text">2-shot CoT</span>
</td>
<td id="A3.3.4.19.19.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Pythia<span id="A3.3.4.19.19.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</td>
<td id="A3.3.4.19.19.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">42.0</td>
<td id="A3.3.4.19.19.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">53.8</td>
<td id="A3.3.4.19.19.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">41.2</td>
<td id="A3.3.4.19.19.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">41.0</td>
<td id="A3.3.4.19.19.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.2</td>
<td id="A3.3.4.19.19.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">11.6</td>
</tr>
<tr id="A3.3.4.20.20" class="ltx_tr">
<td id="A3.3.4.20.20.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.20.20.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</td>
<td id="A3.3.4.20.20.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">41.0</td>
<td id="A3.3.4.20.20.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.4</td>
<td id="A3.3.4.20.20.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.6</td>
<td id="A3.3.4.20.20.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">59.2</td>
<td id="A3.3.4.20.20.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">20.4</td>
<td id="A3.3.4.20.20.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.2</td>
</tr>
<tr id="A3.3.4.21.21" class="ltx_tr">
<td id="A3.3.4.21.21.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.21.21.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</td>
<td id="A3.3.4.21.21.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">37.6</td>
<td id="A3.3.4.21.21.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61.2</td>
<td id="A3.3.4.21.21.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.8</td>
<td id="A3.3.4.21.21.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.0</td>
<td id="A3.3.4.21.21.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">17.2</td>
<td id="A3.3.4.21.21.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.4</td>
</tr>
<tr id="A3.3.4.22.22" class="ltx_tr">
<td id="A3.3.4.22.22.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2<span id="A3.3.4.22.22.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</td>
<td id="A3.3.4.22.22.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.2</td>
<td id="A3.3.4.22.22.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.6</td>
<td id="A3.3.4.22.22.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.0</td>
<td id="A3.3.4.22.22.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.8</td>
<td id="A3.3.4.22.22.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">20.2</td>
<td id="A3.3.4.22.22.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">13.4</td>
</tr>
<tr id="A3.3.4.23.23" class="ltx_tr">
<td id="A3.3.4.23.23.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.23.23.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.23.23.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.0</td>
<td id="A3.3.4.23.23.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.6</td>
<td id="A3.3.4.23.23.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">51.2</td>
<td id="A3.3.4.23.23.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">51.2</td>
<td id="A3.3.4.23.23.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.8</td>
<td id="A3.3.4.23.23.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">14.0</td>
</tr>
<tr id="A3.3.4.24.24" class="ltx_tr">
<td id="A3.3.4.24.24.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.4.24.24.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</td>
<td id="A3.3.4.24.24.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.2</td>
<td id="A3.3.4.24.24.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.4</td>
<td id="A3.3.4.24.24.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">59.4</td>
<td id="A3.3.4.24.24.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.8</td>
<td id="A3.3.4.24.24.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.2</td>
<td id="A3.3.4.24.24.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.2</td>
</tr>
<tr id="A3.3.4.25.25" class="ltx_tr">
<td id="A3.3.4.25.25.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna<span id="A3.3.4.25.25.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.25.25.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.4</td>
<td id="A3.3.4.25.25.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.0</td>
<td id="A3.3.4.25.25.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.2</td>
<td id="A3.3.4.25.25.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61.0</td>
<td id="A3.3.4.25.25.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">18.4</td>
<td id="A3.3.4.25.25.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.0</td>
</tr>
<tr id="A3.3.4.26.26" class="ltx_tr">
<td id="A3.3.4.26.26.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Alpaca<span id="A3.3.4.26.26.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.26.26.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">40.4</td>
<td id="A3.3.4.26.26.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.6</td>
<td id="A3.3.4.26.26.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.4</td>
<td id="A3.3.4.26.26.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.8</td>
<td id="A3.3.4.26.26.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">23.0</td>
<td id="A3.3.4.26.26.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.2</td>
</tr>
<tr id="A3.3.4.27.27" class="ltx_tr">
<td id="A3.3.4.27.27.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.4.27.27.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="A3.3.4.27.27.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.4.27.27.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.8</td>
<td id="A3.3.4.27.27.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.8</td>
<td id="A3.3.4.27.27.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.8</td>
<td id="A3.3.4.27.27.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61.0</td>
<td id="A3.3.4.27.27.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">23.2</td>
<td id="A3.3.4.27.27.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">16.2</td>
</tr>
<tr id="A3.3.4.28.28" class="ltx_tr">
<td id="A3.3.4.28.28.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.4.28.28.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">49.2</td>
<td id="A3.3.4.28.28.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.28.28.3.1" class="ltx_text ltx_font_bold">74.4</span></td>
<td id="A3.3.4.28.28.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">77.2</td>
<td id="A3.3.4.28.28.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">75.4</td>
<td id="A3.3.4.28.28.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">49.4</td>
<td id="A3.3.4.28.28.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">35.0</td>
</tr>
<tr id="A3.3.4.29.29" class="ltx_tr">
<td id="A3.3.4.29.29.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.4.29.29.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.29.29.2.1" class="ltx_text ltx_font_bold">59.2</span></td>
<td id="A3.3.4.29.29.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">72.0</td>
<td id="A3.3.4.29.29.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.29.29.4.1" class="ltx_text ltx_font_bold">85.6</span></td>
<td id="A3.3.4.29.29.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.29.29.5.1" class="ltx_text ltx_font_bold">83.2</span></td>
<td id="A3.3.4.29.29.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.29.29.6.1" class="ltx_text ltx_font_bold">67.6</span></td>
<td id="A3.3.4.29.29.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.4.29.29.7.1" class="ltx_text ltx_font_bold">55.6</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Faithfulness-level automated evaluation results on <span id="A3.3.8.1" class="ltx_text ltx_font_smallcaps">LoTNLG</span>. We do not evaluate fine-tuned models as <span id="A3.3.9.2" class="ltx_text ltx_font_smallcaps">LoTNLG</span> does not contain a training set. <sup id="A3.3.10.3" class="ltx_sup">∗</sup>: It is challenging for other LLMs to follow the instructions in 0-shot prompt to generate a statement using the specified types of logical reasoning operations.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.3.3" class="ltx_table ltx_figure_panel">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="A3.3.3.1" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.3.3.1.1.1" class="ltx_tr">
<th id="A3.3.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Type</th>
<th id="A3.3.3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Models</th>
<td id="A3.3.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">BLEU-1/2/3</td>
<td id="A3.3.3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">ROUGE-1/2/L</td>
<td id="A3.3.3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPAS-Acc</td>
<td id="A3.3.3.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPEX-Acc</td>
</tr>
<tr id="A3.3.3.1.2.2" class="ltx_tr">
<th id="A3.3.3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="5"><span id="A3.3.3.1.2.2.1.1" class="ltx_text">Fine-tuned</span></th>
<th id="A3.3.3.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">BART</th>
<td id="A3.3.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">63.2/50.8/42.0</td>
<td id="A3.3.3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.2.2.4.1" class="ltx_text ltx_font_bold">67.6</span>/<span id="A3.3.3.1.2.2.4.2" class="ltx_text ltx_font_bold">46.0</span>/<span id="A3.3.3.1.2.2.4.3" class="ltx_text ltx_font_bold">57.2</span>
</td>
<td id="A3.3.3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">94.8</td>
<td id="A3.3.3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">68.8</td>
</tr>
<tr id="A3.3.3.1.3.3" class="ltx_tr">
<th id="A3.3.3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Flan-T5</th>
<td id="A3.3.3.1.3.3.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.2/49.6/41.0</td>
<td id="A3.3.3.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.8/45.0/56.2</td>
<td id="A3.3.3.1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">94.2</td>
<td id="A3.3.3.1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">69.2</td>
</tr>
<tr id="A3.3.3.1.4.4" class="ltx_tr">
<th id="A3.3.3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">OmniTab</th>
<td id="A3.3.3.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63.4/50.8/41.8</td>
<td id="A3.3.3.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.4/45.2/56.2</td>
<td id="A3.3.3.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">94.6</td>
<td id="A3.3.3.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.6</td>
</tr>
<tr id="A3.3.3.1.5.5" class="ltx_tr">
<th id="A3.3.3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">ReasTAP</th>
<td id="A3.3.3.1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.5.5.2.1" class="ltx_text ltx_font_bold">63.6</span>/<span id="A3.3.3.1.5.5.2.2" class="ltx_text ltx_font_bold">51.0</span>/<span id="A3.3.3.1.5.5.2.3" class="ltx_text ltx_font_bold">42.2</span>
</td>
<td id="A3.3.3.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.5.5.3.1" class="ltx_text ltx_font_bold">67.6</span>/45.8/<span id="A3.3.3.1.5.5.3.2" class="ltx_text ltx_font_bold">57.2</span>
</td>
<td id="A3.3.3.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">94.6</td>
<td id="A3.3.3.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.4</td>
</tr>
<tr id="A3.3.3.1.6.6" class="ltx_tr">
<th id="A3.3.3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">TAPEX</th>
<td id="A3.3.3.1.6.6.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.6.6.2.1" class="ltx_text ltx_font_bold">63.6</span>/50.8/42.0</td>
<td id="A3.3.3.1.6.6.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.4/45.0/56.2</td>
<td id="A3.3.3.1.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.6.6.4.1" class="ltx_text ltx_font_bold">96.2</span></td>
<td id="A3.3.3.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.6.6.5.1" class="ltx_text ltx_font_bold">73.0</span></td>
</tr>
<tr id="A3.3.3.1.7.7" class="ltx_tr">
<th id="A3.3.3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="A3.3.3.1.7.7.1.1" class="ltx_text">0-shot</span></th>
<th id="A3.3.3.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</th>
<td id="A3.3.3.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.7.7.3.1" class="ltx_text ltx_font_bold">56.4</span>/<span id="A3.3.3.1.7.7.3.2" class="ltx_text ltx_font_bold">42.6</span>/<span id="A3.3.3.1.7.7.3.3" class="ltx_text ltx_font_bold">33.4</span>
</td>
<td id="A3.3.3.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">60.6/38.0/49.4</td>
<td id="A3.3.3.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">92.4</td>
<td id="A3.3.3.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">72.8</td>
</tr>
<tr id="A3.3.3.1.8.8" class="ltx_tr">
<th id="A3.3.3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</th>
<td id="A3.3.3.1.8.8.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.4/40.2/31.8</td>
<td id="A3.3.3.1.8.8.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.8.8.3.1" class="ltx_text ltx_font_bold">63.8</span>/<span id="A3.3.3.1.8.8.3.2" class="ltx_text ltx_font_bold">40.4</span>/<span id="A3.3.3.1.8.8.3.3" class="ltx_text ltx_font_bold">51.6</span>
</td>
<td id="A3.3.3.1.8.8.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.8.8.4.1" class="ltx_text ltx_font_bold">94.0</span></td>
<td id="A3.3.3.1.8.8.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.8.8.5.1" class="ltx_text ltx_font_bold">74.4</span></td>
</tr>
<tr id="A3.3.3.1.9.9" class="ltx_tr">
<th id="A3.3.3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="A3.3.3.1.9.9.1.1" class="ltx_text">1-shot Direct</span></th>
<th id="A3.3.3.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</th>
<td id="A3.3.3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.9.9.3.1" class="ltx_text ltx_font_bold">56.8</span>/43.2/34.2</td>
<td id="A3.3.3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">63.0/39.8/51.4</td>
<td id="A3.3.3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">91.8</td>
<td id="A3.3.3.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.9.9.6.1" class="ltx_text ltx_font_bold">74.6</span></td>
</tr>
<tr id="A3.3.3.1.10.10" class="ltx_tr">
<th id="A3.3.3.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</th>
<td id="A3.3.3.1.10.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.4/<span id="A3.3.3.1.10.10.2.1" class="ltx_text ltx_font_bold">43.6</span>/<span id="A3.3.3.1.10.10.2.2" class="ltx_text ltx_font_bold">34.8</span>
</td>
<td id="A3.3.3.1.10.10.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.10.10.3.1" class="ltx_text ltx_font_bold">66.2</span>/<span id="A3.3.3.1.10.10.3.2" class="ltx_text ltx_font_bold">43.0</span>/<span id="A3.3.3.1.10.10.3.3" class="ltx_text ltx_font_bold">54.4</span>
</td>
<td id="A3.3.3.1.10.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.10.10.4.1" class="ltx_text ltx_font_bold">94.0</span></td>
<td id="A3.3.3.1.10.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.8</td>
</tr>
<tr id="A3.3.3.1.11.11" class="ltx_tr">
<th id="A3.3.3.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="A3.3.3.1.11.11.1.1" class="ltx_ERROR undefined">\cdashline</span>1-6
<span id="A3.3.3.1.11.11.1.2" class="ltx_text">1-shot CoT</span>
</th>
<th id="A3.3.3.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</th>
<td id="A3.3.3.1.11.11.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.2/32.4/25.2</td>
<td id="A3.3.3.1.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.4/35.8/46.8</td>
<td id="A3.3.3.1.11.11.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.11.11.5.1" class="ltx_text ltx_font_bold">94.2</span></td>
<td id="A3.3.3.1.11.11.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.0</td>
</tr>
<tr id="A3.3.3.1.12.12" class="ltx_tr">
<th id="A3.3.3.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</th>
<td id="A3.3.3.1.12.12.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.12.12.2.1" class="ltx_text ltx_font_bold">59.6</span>/<span id="A3.3.3.1.12.12.2.2" class="ltx_text ltx_font_bold">45.8</span>/<span id="A3.3.3.1.12.12.2.3" class="ltx_text ltx_font_bold">36.4</span>
</td>
<td id="A3.3.3.1.12.12.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.12.12.3.1" class="ltx_text ltx_font_bold">64.0</span>/<span id="A3.3.3.1.12.12.3.2" class="ltx_text ltx_font_bold">41.0</span>/<span id="A3.3.3.1.12.12.3.3" class="ltx_text ltx_font_bold">52.4</span>
</td>
<td id="A3.3.3.1.12.12.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">91.0</td>
<td id="A3.3.3.1.12.12.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.12.12.5.1" class="ltx_text ltx_font_bold">76.4</span></td>
</tr>
<tr id="A3.3.3.1.13.13" class="ltx_tr">
<th id="A3.3.3.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="11"><span id="A3.3.3.1.13.13.1.1" class="ltx_text">2-shot Direct</span></th>
<th id="A3.3.3.1.13.13.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Pythia<span id="A3.3.3.1.13.13.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</th>
<td id="A3.3.3.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">38.8/26.6/19.4</td>
<td id="A3.3.3.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">43.2/22.6/35.2</td>
<td id="A3.3.3.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">76.6</td>
<td id="A3.3.3.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">35.0</td>
</tr>
<tr id="A3.3.3.1.14.14" class="ltx_tr">
<th id="A3.3.3.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.14.14.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</th>
<td id="A3.3.3.1.14.14.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">40.6/28.6/21.4</td>
<td id="A3.3.3.1.14.14.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.2/26.6/39.0</td>
<td id="A3.3.3.1.14.14.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">86.2</td>
<td id="A3.3.3.1.14.14.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.8</td>
</tr>
<tr id="A3.3.3.1.15.15" class="ltx_tr">
<th id="A3.3.3.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.15.15.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.15.15.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.4/35.2/26.8</td>
<td id="A3.3.3.1.15.15.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">51.0/29.4/42.2</td>
<td id="A3.3.3.1.15.15.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">85.4</td>
<td id="A3.3.3.1.15.15.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.4</td>
</tr>
<tr id="A3.3.3.1.16.16" class="ltx_tr">
<th id="A3.3.3.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Alpaca<span id="A3.3.3.1.16.16.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.16.16.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.2/38.4/29.6</td>
<td id="A3.3.3.1.16.16.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.4/33.6/46.2</td>
<td id="A3.3.3.1.16.16.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">88.4</td>
<td id="A3.3.3.1.16.16.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.4</td>
</tr>
<tr id="A3.3.3.1.17.17" class="ltx_tr">
<th id="A3.3.3.1.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.17.17.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="A3.3.3.1.17.17.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.17.17.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.6/37.4/29.0</td>
<td id="A3.3.3.1.17.17.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.2/31.8/44.6</td>
<td id="A3.3.3.1.17.17.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">86.4</td>
<td id="A3.3.3.1.17.17.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.0</td>
</tr>
<tr id="A3.3.3.1.18.18" class="ltx_tr">
<th id="A3.3.3.1.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.18.18.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</th>
<td id="A3.3.3.1.18.18.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.4/37.0/28.2</td>
<td id="A3.3.3.1.18.18.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.2/33.2/45.4</td>
<td id="A3.3.3.1.18.18.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">87.0</td>
<td id="A3.3.3.1.18.18.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.2</td>
</tr>
<tr id="A3.3.3.1.19.19" class="ltx_tr">
<th id="A3.3.3.1.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna<span id="A3.3.3.1.19.19.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.19.19.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.0/42.2/32.8</td>
<td id="A3.3.3.1.19.19.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">59.0/36.2/48.0</td>
<td id="A3.3.3.1.19.19.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">87.6</td>
<td id="A3.3.3.1.19.19.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.4</td>
</tr>
<tr id="A3.3.3.1.20.20" class="ltx_tr">
<th id="A3.3.3.1.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.20.20.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</th>
<td id="A3.3.3.1.20.20.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">53.6/39.8/30.8</td>
<td id="A3.3.3.1.20.20.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.0/34.0/46.6</td>
<td id="A3.3.3.1.20.20.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">88.4</td>
<td id="A3.3.3.1.20.20.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63.0</td>
</tr>
<tr id="A3.3.3.1.21.21" class="ltx_tr">
<th id="A3.3.3.1.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2<span id="A3.3.3.1.21.21.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="A3.3.3.1.21.21.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.6/41.0/31.8</td>
<td id="A3.3.3.1.21.21.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.4/35.8/47.8</td>
<td id="A3.3.3.1.21.21.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">89.4</td>
<td id="A3.3.3.1.21.21.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.2</td>
</tr>
<tr id="A3.3.3.1.22.22" class="ltx_tr">
<th id="A3.3.3.1.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</th>
<td id="A3.3.3.1.22.22.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">55.0/<span id="A3.3.3.1.22.22.2.1" class="ltx_text ltx_font_bold">42.8</span>/<span id="A3.3.3.1.22.22.2.2" class="ltx_text ltx_font_bold">34.6</span>
</td>
<td id="A3.3.3.1.22.22.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.22.22.3.1" class="ltx_text ltx_font_bold">66.0</span>/<span id="A3.3.3.1.22.22.3.2" class="ltx_text ltx_font_bold">42.8</span>/<span id="A3.3.3.1.22.22.3.3" class="ltx_text ltx_font_bold">54.0</span>
</td>
<td id="A3.3.3.1.22.22.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.22.22.4.1" class="ltx_text ltx_font_bold">95.2</span></td>
<td id="A3.3.3.1.22.22.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">75.8</td>
</tr>
<tr id="A3.3.3.1.23.23" class="ltx_tr">
<th id="A3.3.3.1.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</th>
<td id="A3.3.3.1.23.23.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.23.23.2.1" class="ltx_text ltx_font_bold">55.8</span>/<span id="A3.3.3.1.23.23.2.2" class="ltx_text ltx_font_bold">42.8</span>/34.0</td>
<td id="A3.3.3.1.23.23.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63.2/40.0/51.6</td>
<td id="A3.3.3.1.23.23.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">92.2</td>
<td id="A3.3.3.1.23.23.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.23.23.5.1" class="ltx_text ltx_font_bold">76.0</span></td>
</tr>
<tr id="A3.3.3.1.24.24" class="ltx_tr">
<th id="A3.3.3.1.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="11">
<span id="A3.3.3.1.24.24.1.1" class="ltx_ERROR undefined">\cdashline</span>1-6
<span id="A3.3.3.1.24.24.1.2" class="ltx_text">2-shot CoT</span>
</th>
<th id="A3.3.3.1.24.24.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Pythia<span id="A3.3.3.1.24.24.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</th>
<td id="A3.3.3.1.24.24.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">38.8/25.4/17.8</td>
<td id="A3.3.3.1.24.24.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">39.2/18.8/32.2</td>
<td id="A3.3.3.1.24.24.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">69.0</td>
<td id="A3.3.3.1.24.24.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">36.2</td>
</tr>
<tr id="A3.3.3.1.25.25" class="ltx_tr">
<th id="A3.3.3.1.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.25.25.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</th>
<td id="A3.3.3.1.25.25.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">33.0/22.2/16.0</td>
<td id="A3.3.3.1.25.25.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">41.0/21.2/33.2</td>
<td id="A3.3.3.1.25.25.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">77.6</td>
<td id="A3.3.3.1.25.25.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">42.0</td>
</tr>
<tr id="A3.3.3.1.26.26" class="ltx_tr">
<th id="A3.3.3.1.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.26.26.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.26.26.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.2/30.4/22.6</td>
<td id="A3.3.3.1.26.26.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.4/25.2/37.6</td>
<td id="A3.3.3.1.26.26.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">82.0</td>
<td id="A3.3.3.1.26.26.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.8</td>
</tr>
<tr id="A3.3.3.1.27.27" class="ltx_tr">
<th id="A3.3.3.1.27.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Alpaca<span id="A3.3.3.1.27.27.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.27.27.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.4/34.4/26.2</td>
<td id="A3.3.3.1.27.27.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">51.4/30.0/42.0</td>
<td id="A3.3.3.1.27.27.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">82.8</td>
<td id="A3.3.3.1.27.27.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.4</td>
</tr>
<tr id="A3.3.3.1.28.28" class="ltx_tr">
<th id="A3.3.3.1.28.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.28.28.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="A3.3.3.1.28.28.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.28.28.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">37.0/25.8/18.8</td>
<td id="A3.3.3.1.28.28.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.6/24.0/35.2</td>
<td id="A3.3.3.1.28.28.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">86.2</td>
<td id="A3.3.3.1.28.28.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">55.8</td>
</tr>
<tr id="A3.3.3.1.29.29" class="ltx_tr">
<th id="A3.3.3.1.29.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.29.29.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</th>
<td id="A3.3.3.1.29.29.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">45.4/33.2/25.6</td>
<td id="A3.3.3.1.29.29.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">52.4/30.8/42.2</td>
<td id="A3.3.3.1.29.29.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">86.2</td>
<td id="A3.3.3.1.29.29.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">63.6</td>
</tr>
<tr id="A3.3.3.1.30.30" class="ltx_tr">
<th id="A3.3.3.1.30.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna<span id="A3.3.3.1.30.30.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</th>
<td id="A3.3.3.1.30.30.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.4/37.6/29.4</td>
<td id="A3.3.3.1.30.30.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">53.8/32.4/44.6</td>
<td id="A3.3.3.1.30.30.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">85.6</td>
<td id="A3.3.3.1.30.30.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.8</td>
</tr>
<tr id="A3.3.3.1.31.31" class="ltx_tr">
<th id="A3.3.3.1.31.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.1.31.31.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</th>
<td id="A3.3.3.1.31.31.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.2/37.0/28.4</td>
<td id="A3.3.3.1.31.31.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">54.8/32.8/44.6</td>
<td id="A3.3.3.1.31.31.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">87.8</td>
<td id="A3.3.3.1.31.31.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.0</td>
</tr>
<tr id="A3.3.3.1.32.32" class="ltx_tr">
<th id="A3.3.3.1.32.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2<span id="A3.3.3.1.32.32.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</th>
<td id="A3.3.3.1.32.32.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">53.8/40.2/31.4</td>
<td id="A3.3.3.1.32.32.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.4/34.8/47.0</td>
<td id="A3.3.3.1.32.32.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">89.2</td>
<td id="A3.3.3.1.32.32.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.2</td>
</tr>
<tr id="A3.3.3.1.33.33" class="ltx_tr">
<th id="A3.3.3.1.33.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</th>
<td id="A3.3.3.1.33.33.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">50.8/38.8/30.8</td>
<td id="A3.3.3.1.33.33.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.6/38.2/49.0</td>
<td id="A3.3.3.1.33.33.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.33.33.4.1" class="ltx_text ltx_font_bold">92.8</span></td>
<td id="A3.3.3.1.33.33.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">70.8</td>
</tr>
<tr id="A3.3.3.1.34.34" class="ltx_tr">
<th id="A3.3.3.1.34.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</th>
<td id="A3.3.3.1.34.34.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.34.34.2.1" class="ltx_text ltx_font_bold">62.2</span>/<span id="A3.3.3.1.34.34.2.2" class="ltx_text ltx_font_bold">48.6</span>/<span id="A3.3.3.1.34.34.2.3" class="ltx_text ltx_font_bold">39.2</span>
</td>
<td id="A3.3.3.1.34.34.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.1.34.34.3.1" class="ltx_text ltx_font_bold">65.8</span>/<span id="A3.3.3.1.34.34.3.2" class="ltx_text ltx_font_bold">42.8</span>/<span id="A3.3.3.1.34.34.3.3" class="ltx_text ltx_font_bold">54.4</span>
</td>
<td id="A3.3.3.1.34.34.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">91.2</td>
<td id="A3.3.3.1.34.34.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.1.34.34.5.1" class="ltx_text ltx_font_bold">79.2</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Automated evaluation results on the FeTaQA dataset.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.3.3.tab1" class="ltx_table ltx_figure_panel">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A3.3.3.tab1.1" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A3.3.3.tab1.1.1.1" class="ltx_tr">
<td id="A3.3.3.tab1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Type</td>
<td id="A3.3.3.tab1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Models</td>
<td id="A3.3.3.tab1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">BLEU-1/2/3</td>
<td id="A3.3.3.tab1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">ROUGE-1/2/L</td>
<td id="A3.3.3.tab1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPAS-Acc</td>
<td id="A3.3.3.tab1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">TAPEX-Acc</td>
<td id="A3.3.3.tab1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;">Accuracy</td>
</tr>
<tr id="A3.3.3.tab1.1.2.2" class="ltx_tr">
<td id="A3.3.3.tab1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="A3.3.3.tab1.1.2.2.1.1" class="ltx_text">0-shot</span></td>
<td id="A3.3.3.tab1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.3.tab1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.2.2.3.1" class="ltx_text ltx_font_bold">63.2</span>/<span id="A3.3.3.tab1.1.2.2.3.2" class="ltx_text ltx_font_bold">49.2</span>/<span id="A3.3.3.tab1.1.2.2.3.3" class="ltx_text ltx_font_bold">39.4</span>
</td>
<td id="A3.3.3.tab1.1.2.2.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">64.4/40.0/<span id="A3.3.3.tab1.1.2.2.4.1" class="ltx_text ltx_font_bold">56.4</span>
</td>
<td id="A3.3.3.tab1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">73.0</td>
<td id="A3.3.3.tab1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">74.6</td>
<td id="A3.3.3.tab1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">54.0</td>
</tr>
<tr id="A3.3.3.tab1.1.3.3" class="ltx_tr">
<td id="A3.3.3.tab1.1.3.3.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.3.tab1.1.3.3.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">60.6/46.8/37.4</td>
<td id="A3.3.3.tab1.1.3.3.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.3.3.3.1" class="ltx_text ltx_font_bold">64.6</span>/<span id="A3.3.3.tab1.1.3.3.3.2" class="ltx_text ltx_font_bold">40.4</span>/54.8</td>
<td id="A3.3.3.tab1.1.3.3.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.3.3.4.1" class="ltx_text ltx_font_bold">78.6</span></td>
<td id="A3.3.3.tab1.1.3.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.3.3.5.1" class="ltx_text ltx_font_bold">80.6</span></td>
<td id="A3.3.3.tab1.1.3.3.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.3.3.6.1" class="ltx_text ltx_font_bold">62.4</span></td>
</tr>
<tr id="A3.3.3.tab1.1.4.4" class="ltx_tr">
<td id="A3.3.3.tab1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="A3.3.3.tab1.1.4.4.1.1" class="ltx_text">1-shot Direct</span></td>
<td id="A3.3.3.tab1.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.3.tab1.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">62.0/48.4/39.0</td>
<td id="A3.3.3.tab1.1.4.4.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">64.0/40.0/56.8</td>
<td id="A3.3.3.tab1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">75.0</td>
<td id="A3.3.3.tab1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">73.2</td>
<td id="A3.3.3.tab1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">51.8</td>
</tr>
<tr id="A3.3.3.tab1.1.5.5" class="ltx_tr">
<td id="A3.3.3.tab1.1.5.5.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.3.tab1.1.5.5.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.5.5.2.1" class="ltx_text ltx_font_bold">63.2</span>/<span id="A3.3.3.tab1.1.5.5.2.2" class="ltx_text ltx_font_bold">49.8</span>/<span id="A3.3.3.tab1.1.5.5.2.3" class="ltx_text ltx_font_bold">40.4</span>
</td>
<td id="A3.3.3.tab1.1.5.5.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.5.5.3.1" class="ltx_text ltx_font_bold">66.2</span>/<span id="A3.3.3.tab1.1.5.5.3.2" class="ltx_text ltx_font_bold">42.6</span>/<span id="A3.3.3.tab1.1.5.5.3.3" class="ltx_text ltx_font_bold">58.0</span>
</td>
<td id="A3.3.3.tab1.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.5.5.4.1" class="ltx_text ltx_font_bold">78.4</span></td>
<td id="A3.3.3.tab1.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.5.5.5.1" class="ltx_text ltx_font_bold">79.0</span></td>
<td id="A3.3.3.tab1.1.5.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.5.5.6.1" class="ltx_text ltx_font_bold">66.0</span></td>
</tr>
<tr id="A3.3.3.tab1.1.6.6" class="ltx_tr">
<td id="A3.3.3.tab1.1.6.6.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2">
<span id="A3.3.3.tab1.1.6.6.1.1" class="ltx_ERROR undefined">\cdashline</span>1-7
<span id="A3.3.3.tab1.1.6.6.1.2" class="ltx_text">1-shot CoT</span>
</td>
<td id="A3.3.3.tab1.1.6.6.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.3.tab1.1.6.6.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">55.0/42.4/33.8</td>
<td id="A3.3.3.tab1.1.6.6.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">62.8/39.0/54.8</td>
<td id="A3.3.3.tab1.1.6.6.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.4</td>
<td id="A3.3.3.tab1.1.6.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.2</td>
<td id="A3.3.3.tab1.1.6.6.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">55.2</td>
</tr>
<tr id="A3.3.3.tab1.1.7.7" class="ltx_tr">
<td id="A3.3.3.tab1.1.7.7.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.3.tab1.1.7.7.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">62.2/49.0/39.6</td>
<td id="A3.3.3.tab1.1.7.7.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.7.7.3.1" class="ltx_text ltx_font_bold">66.2</span>/<span id="A3.3.3.tab1.1.7.7.3.2" class="ltx_text ltx_font_bold">42.2</span>/<span id="A3.3.3.tab1.1.7.7.3.3" class="ltx_text ltx_font_bold">58.4</span>
</td>
<td id="A3.3.3.tab1.1.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.7.7.4.1" class="ltx_text ltx_font_bold">78.2</span></td>
<td id="A3.3.3.tab1.1.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.7.7.5.1" class="ltx_text ltx_font_bold">78.6</span></td>
<td id="A3.3.3.tab1.1.7.7.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.7.7.6.1" class="ltx_text ltx_font_bold">69.8</span></td>
</tr>
<tr id="A3.3.3.tab1.1.8.8" class="ltx_tr">
<td id="A3.3.3.tab1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="11"><span id="A3.3.3.tab1.1.8.8.1.1" class="ltx_text">2-shot Direct</span></td>
<td id="A3.3.3.tab1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Pythia<span id="A3.3.3.tab1.1.8.8.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</td>
<td id="A3.3.3.tab1.1.8.8.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">12.4/7.6/5.2</td>
<td id="A3.3.3.tab1.1.8.8.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.6/9.2/17.4</td>
<td id="A3.3.3.tab1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">74.6</td>
<td id="A3.3.3.tab1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">62.4</td>
<td id="A3.3.3.tab1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">7.8</td>
</tr>
<tr id="A3.3.3.tab1.1.9.9" class="ltx_tr">
<td id="A3.3.3.tab1.1.9.9.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.9.9.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</td>
<td id="A3.3.3.tab1.1.9.9.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">14.4/9.6/6.8</td>
<td id="A3.3.3.tab1.1.9.9.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">26.2/13.4/23.0</td>
<td id="A3.3.3.tab1.1.9.9.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.8</td>
<td id="A3.3.3.tab1.1.9.9.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">53.0</td>
<td id="A3.3.3.tab1.1.9.9.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">19.0</td>
</tr>
<tr id="A3.3.3.tab1.1.10.10" class="ltx_tr">
<td id="A3.3.3.tab1.1.10.10.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.10.10.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.10.10.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">7.6/4.8/3.4</td>
<td id="A3.3.3.tab1.1.10.10.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">20.2/10.4/18.2</td>
<td id="A3.3.3.tab1.1.10.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">78.4</td>
<td id="A3.3.3.tab1.1.10.10.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.0</td>
<td id="A3.3.3.tab1.1.10.10.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">21.4</td>
</tr>
<tr id="A3.3.3.tab1.1.11.11" class="ltx_tr">
<td id="A3.3.3.tab1.1.11.11.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna<span id="A3.3.3.tab1.1.11.11.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.11.11.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">43.0/31.6/24.4</td>
<td id="A3.3.3.tab1.1.11.11.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">46.0/27.2/40.6</td>
<td id="A3.3.3.tab1.1.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">74.6</td>
<td id="A3.3.3.tab1.1.11.11.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.2</td>
<td id="A3.3.3.tab1.1.11.11.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">30.2</td>
</tr>
<tr id="A3.3.3.tab1.1.12.12" class="ltx_tr">
<td id="A3.3.3.tab1.1.12.12.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Alpaca<span id="A3.3.3.tab1.1.12.12.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.12.12.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">40.8/29.2/21.6</td>
<td id="A3.3.3.tab1.1.12.12.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">46.6/26.2/40.4</td>
<td id="A3.3.3.tab1.1.12.12.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">71.8</td>
<td id="A3.3.3.tab1.1.12.12.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">57.6</td>
<td id="A3.3.3.tab1.1.12.12.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">31.2</td>
</tr>
<tr id="A3.3.3.tab1.1.13.13" class="ltx_tr">
<td id="A3.3.3.tab1.1.13.13.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.13.13.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</td>
<td id="A3.3.3.tab1.1.13.13.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">34.0/24.4/18.2</td>
<td id="A3.3.3.tab1.1.13.13.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">44.6/25.0/39.8</td>
<td id="A3.3.3.tab1.1.13.13.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">74.0</td>
<td id="A3.3.3.tab1.1.13.13.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">61.0</td>
<td id="A3.3.3.tab1.1.13.13.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">31.8</td>
</tr>
<tr id="A3.3.3.tab1.1.14.14" class="ltx_tr">
<td id="A3.3.3.tab1.1.14.14.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.14.14.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="A3.3.3.tab1.1.14.14.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.14.14.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">49.6/36.4/28.0</td>
<td id="A3.3.3.tab1.1.14.14.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">51.4/29.4/45.8</td>
<td id="A3.3.3.tab1.1.14.14.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">78.8</td>
<td id="A3.3.3.tab1.1.14.14.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.4</td>
<td id="A3.3.3.tab1.1.14.14.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">33.8</td>
</tr>
<tr id="A3.3.3.tab1.1.15.15" class="ltx_tr">
<td id="A3.3.3.tab1.1.15.15.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.15.15.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</td>
<td id="A3.3.3.tab1.1.15.15.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">45.8/33.8/26.0</td>
<td id="A3.3.3.tab1.1.15.15.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">48.8/28.2/43.6</td>
<td id="A3.3.3.tab1.1.15.15.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.6</td>
<td id="A3.3.3.tab1.1.15.15.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">64.4</td>
<td id="A3.3.3.tab1.1.15.15.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">36.2</td>
</tr>
<tr id="A3.3.3.tab1.1.16.16" class="ltx_tr">
<td id="A3.3.3.tab1.1.16.16.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2<span id="A3.3.3.tab1.1.16.16.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</td>
<td id="A3.3.3.tab1.1.16.16.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">51.2/38.4/30.0</td>
<td id="A3.3.3.tab1.1.16.16.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">50.4/29.6/45.4</td>
<td id="A3.3.3.tab1.1.16.16.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.4</td>
<td id="A3.3.3.tab1.1.16.16.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">68.4</td>
<td id="A3.3.3.tab1.1.16.16.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">37.6</td>
</tr>
<tr id="A3.3.3.tab1.1.17.17" class="ltx_tr">
<td id="A3.3.3.tab1.1.17.17.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.3.tab1.1.17.17.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.17.17.2.1" class="ltx_text ltx_font_bold">63.4</span>/<span id="A3.3.3.tab1.1.17.17.2.2" class="ltx_text ltx_font_bold">49.8</span>/40.2</td>
<td id="A3.3.3.tab1.1.17.17.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">64.8/40.8/57.2</td>
<td id="A3.3.3.tab1.1.17.17.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">74.8</td>
<td id="A3.3.3.tab1.1.17.17.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.6</td>
<td id="A3.3.3.tab1.1.17.17.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">51.8</td>
</tr>
<tr id="A3.3.3.tab1.1.18.18" class="ltx_tr">
<td id="A3.3.3.tab1.1.18.18.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.3.tab1.1.18.18.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">62.8/49.2/39.6</td>
<td id="A3.3.3.tab1.1.18.18.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.18.18.3.1" class="ltx_text ltx_font_bold">65.8</span>/<span id="A3.3.3.tab1.1.18.18.3.2" class="ltx_text ltx_font_bold">41.8</span>/<span id="A3.3.3.tab1.1.18.18.3.3" class="ltx_text ltx_font_bold">57.6</span>
</td>
<td id="A3.3.3.tab1.1.18.18.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.18.18.4.1" class="ltx_text ltx_font_bold">78.6</span></td>
<td id="A3.3.3.tab1.1.18.18.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.18.18.5.1" class="ltx_text ltx_font_bold">81.4</span></td>
<td id="A3.3.3.tab1.1.18.18.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.18.18.6.1" class="ltx_text ltx_font_bold">63.6</span></td>
</tr>
<tr id="A3.3.3.tab1.1.19.19" class="ltx_tr">
<td id="A3.3.3.tab1.1.19.19.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="11">
<span id="A3.3.3.tab1.1.19.19.1.1" class="ltx_ERROR undefined">\cdashline</span>1-7
<span id="A3.3.3.tab1.1.19.19.1.2" class="ltx_text">2-shot CoT</span>
</td>
<td id="A3.3.3.tab1.1.19.19.2" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Pythia<span id="A3.3.3.tab1.1.19.19.2.1" class="ltx_text ltx_font_typewriter">-12b</span>
</td>
<td id="A3.3.3.tab1.1.19.19.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">27.2/18.0/12.8</td>
<td id="A3.3.3.tab1.1.19.19.4" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">35.6/17.4/31.4</td>
<td id="A3.3.3.tab1.1.19.19.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.0</td>
<td id="A3.3.3.tab1.1.19.19.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">48.8</td>
<td id="A3.3.3.tab1.1.19.19.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">15.8</td>
</tr>
<tr id="A3.3.3.tab1.1.20.20" class="ltx_tr">
<td id="A3.3.3.tab1.1.20.20.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.20.20.1.1" class="ltx_text ltx_font_typewriter">-7b</span>
</td>
<td id="A3.3.3.tab1.1.20.20.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">13.2/8.4/5.8</td>
<td id="A3.3.3.tab1.1.20.20.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">28.0/13.2/24.0</td>
<td id="A3.3.3.tab1.1.20.20.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.4</td>
<td id="A3.3.3.tab1.1.20.20.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">47.8</td>
<td id="A3.3.3.tab1.1.20.20.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">24.2</td>
</tr>
<tr id="A3.3.3.tab1.1.21.21" class="ltx_tr">
<td id="A3.3.3.tab1.1.21.21.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.21.21.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.21.21.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">22.2/14.8/10.4</td>
<td id="A3.3.3.tab1.1.21.21.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">35.2/18.0/31.4</td>
<td id="A3.3.3.tab1.1.21.21.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">74.0</td>
<td id="A3.3.3.tab1.1.21.21.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">56.2</td>
<td id="A3.3.3.tab1.1.21.21.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">26.2</td>
</tr>
<tr id="A3.3.3.tab1.1.22.22" class="ltx_tr">
<td id="A3.3.3.tab1.1.22.22.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Alpaca<span id="A3.3.3.tab1.1.22.22.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.22.22.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">33.2/23.6/17.8</td>
<td id="A3.3.3.tab1.1.22.22.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">47.6/26.4/41.2</td>
<td id="A3.3.3.tab1.1.22.22.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">75.0</td>
<td id="A3.3.3.tab1.1.22.22.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">55.4</td>
<td id="A3.3.3.tab1.1.22.22.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">32.2</td>
</tr>
<tr id="A3.3.3.tab1.1.23.23" class="ltx_tr">
<td id="A3.3.3.tab1.1.23.23.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.23.23.1.1" class="ltx_text ltx_font_typewriter">-30b</span>
</td>
<td id="A3.3.3.tab1.1.23.23.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">37.4/26.2/19.6</td>
<td id="A3.3.3.tab1.1.23.23.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">46.2/24.8/40.6</td>
<td id="A3.3.3.tab1.1.23.23.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.6</td>
<td id="A3.3.3.tab1.1.23.23.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">60.0</td>
<td id="A3.3.3.tab1.1.23.23.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">35.6</td>
</tr>
<tr id="A3.3.3.tab1.1.24.24" class="ltx_tr">
<td id="A3.3.3.tab1.1.24.24.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.24.24.1.1" class="ltx_text ltx_font_smallcaps">Tülu</span> <span id="A3.3.3.tab1.1.24.24.1.2" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.24.24.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">25.8/17.0/12.0</td>
<td id="A3.3.3.tab1.1.24.24.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">35.4/17.4/31.0</td>
<td id="A3.3.3.tab1.1.24.24.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.24.24.4.1" class="ltx_text ltx_font_bold">79.0</span></td>
<td id="A3.3.3.tab1.1.24.24.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">65.6</td>
<td id="A3.3.3.tab1.1.24.24.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">35.8</td>
</tr>
<tr id="A3.3.3.tab1.1.25.25" class="ltx_tr">
<td id="A3.3.3.tab1.1.25.25.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">Vicuna<span id="A3.3.3.tab1.1.25.25.1.1" class="ltx_text ltx_font_typewriter">-13b</span>
</td>
<td id="A3.3.3.tab1.1.25.25.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">45.2/33.2/25.4</td>
<td id="A3.3.3.tab1.1.25.25.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">53.6/31.2/47.6</td>
<td id="A3.3.3.tab1.1.25.25.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">75.6</td>
<td id="A3.3.3.tab1.1.25.25.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">62.2</td>
<td id="A3.3.3.tab1.1.25.25.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">38.6</td>
</tr>
<tr id="A3.3.3.tab1.1.26.26" class="ltx_tr">
<td id="A3.3.3.tab1.1.26.26.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA<span id="A3.3.3.tab1.1.26.26.1.1" class="ltx_text ltx_font_typewriter">-65b</span>
</td>
<td id="A3.3.3.tab1.1.26.26.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">51.2/37.8/29.0</td>
<td id="A3.3.3.tab1.1.26.26.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">51.6/29.4/45.6</td>
<td id="A3.3.3.tab1.1.26.26.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">75.6</td>
<td id="A3.3.3.tab1.1.26.26.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">67.6</td>
<td id="A3.3.3.tab1.1.26.26.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">41.6</td>
</tr>
<tr id="A3.3.3.tab1.1.27.27" class="ltx_tr">
<td id="A3.3.3.tab1.1.27.27.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">LLaMA2<span id="A3.3.3.tab1.1.27.27.1.1" class="ltx_text ltx_font_typewriter">-70b-chat</span>
</td>
<td id="A3.3.3.tab1.1.27.27.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">46.2/34.2/26.6</td>
<td id="A3.3.3.tab1.1.27.27.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">49.6/28.8/44.2</td>
<td id="A3.3.3.tab1.1.27.27.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">75.8</td>
<td id="A3.3.3.tab1.1.27.27.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">66.6</td>
<td id="A3.3.3.tab1.1.27.27.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">43.2</td>
</tr>
<tr id="A3.3.3.tab1.1.28.28" class="ltx_tr">
<td id="A3.3.3.tab1.1.28.28.1" class="ltx_td ltx_align_left" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-3.5</td>
<td id="A3.3.3.tab1.1.28.28.2" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">57.4/44.4/35.4</td>
<td id="A3.3.3.tab1.1.28.28.3" class="ltx_td ltx_align_right" style="padding-left:3.0pt;padding-right:3.0pt;">64.0/40.0/55.4</td>
<td id="A3.3.3.tab1.1.28.28.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">73.6</td>
<td id="A3.3.3.tab1.1.28.28.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">72.8</td>
<td id="A3.3.3.tab1.1.28.28.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">58.6</td>
</tr>
<tr id="A3.3.3.tab1.1.29.29" class="ltx_tr">
<td id="A3.3.3.tab1.1.29.29.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">GPT-4</td>
<td id="A3.3.3.tab1.1.29.29.2" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.29.29.2.1" class="ltx_text ltx_font_bold">63.0</span>/<span id="A3.3.3.tab1.1.29.29.2.2" class="ltx_text ltx_font_bold">49.6</span>/<span id="A3.3.3.tab1.1.29.29.2.3" class="ltx_text ltx_font_bold">40.0</span>
</td>
<td id="A3.3.3.tab1.1.29.29.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="A3.3.3.tab1.1.29.29.3.1" class="ltx_text ltx_font_bold">66.2</span>/<span id="A3.3.3.tab1.1.29.29.3.2" class="ltx_text ltx_font_bold">42.4</span>/<span id="A3.3.3.tab1.1.29.29.3.3" class="ltx_text ltx_font_bold">58.8</span>
</td>
<td id="A3.3.3.tab1.1.29.29.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;">76.4</td>
<td id="A3.3.3.tab1.1.29.29.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.29.29.5.1" class="ltx_text ltx_font_bold">79.6</span></td>
<td id="A3.3.3.tab1.1.29.29.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="A3.3.3.tab1.1.29.29.6.1" class="ltx_text ltx_font_bold">68.4</span></td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Automated evaluation results on the F2WTQ dataset. We do not evaluate fine-tuned models as F2WTQ does not contain a training set.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F7" class="ltx_figure ltx_figure_panel"><img src="/html/2305.14987/assets/x5.png" id="A3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="552" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>An example of 2-shot chain-of-thought prompting adopted from <cite class="ltx_cite ltx_citemacro_citet">Chen (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> for faithfulness-level automated evaluation.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A3.F8" class="ltx_figure ltx_figure_panel"><img src="/html/2305.14987/assets/x6.png" id="A3.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="322" height="413" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>An example of 2-shot <em id="A3.F8.3.1" class="ltx_emph ltx_font_italic">chain-of-thought</em> prompts for natural language feedback generation on <span id="A3.F8.4.2" class="ltx_text ltx_font_smallcaps">LogicNLG</span>.</figcaption>
</figure>
</div>
</div>
</figure>
</div>
</div>
</figure>
</div>
</div>
</figure>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.14986" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.14987" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.14987">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.14987" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.14988" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 06:05:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
