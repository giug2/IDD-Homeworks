<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MedMobile: A mobile-sized language model with expert-level clinical capabilities</title>
<!--Generated on Fri Oct 11 17:31:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.09019v1/"/></head>
<body>
<nav class="ltx_page_navbar">
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">MedMobile: A mobile-sized language model with expert-level clinical capabilities</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Krithik Vishwanath<sup class="ltx_sup" id="id3.1.id1">1,3</sup>, Jaden Stryker<sup class="ltx_sup" id="id4.2.id2">1</sup>, Anton Alaykin<sup class="ltx_sup" id="id5.3.id3">1,4</sup>, 
<br class="ltx_break"/>Daniel A. Alber<sup class="ltx_sup" id="id6.4.id4">1</sup>,
Eric K.
Oermann<sup class="ltx_sup" id="id7.5.id5">1,2,5</sup>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1"><span class="ltx_text" id="id1.1.1">Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for physicians (<math alttext="\sim" class="ltx_Math" display="inline" id="id1.1.1.m1.1"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><csymbol cd="latexml" id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="id1.1.1.m1.1d">∼</annotation></semantics></math>60%), and approaching the scores of models 100 times its size. We subsequently perform a careful set of ablations, and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements. 
<br class="ltx_break"/></span></p>
</div>
<div class="ltx_logical-block" id="id2">
<div class="ltx_para" id="id2.p1">
<p class="ltx_p ltx_align_center" id="id2.p1.1"><sup class="ltx_sup" id="id2.p1.1.1">1</sup>Department of Neurological Surgery, <sup class="ltx_sup" id="id2.p1.1.2">2</sup>Department of Radiology,</p>
<p class="ltx_p ltx_align_center" id="id2.p1.2">NYU Langone Medical Center, New York, New York, 10016</p>
<p class="ltx_p ltx_align_center" id="id2.p1.3"><sup class="ltx_sup" id="id2.p1.3.1">3</sup>Department of Aerospace Engineering and Engineering Mechanics,</p>
<p class="ltx_p ltx_align_center" id="id2.p1.4">The University of Texas at Austin, Austin, Texas, 78712</p>
<p class="ltx_p ltx_align_center" id="id2.p1.5"><sup class="ltx_sup" id="id2.p1.5.1">4</sup>Department of Neurosurgery,</p>
<p class="ltx_p ltx_align_center" id="id2.p1.6">Washington University School of Medicine in St. Louis, St. Louis, Missouri, 63110</p>
<p class="ltx_p ltx_align_center" id="id2.p1.7"><sup class="ltx_sup" id="id2.p1.7.1">5</sup>Center for Data Science,</p>
<p class="ltx_p ltx_align_center" id="id2.p1.8">New York University, New York, New York, 10016</p>
<p class="ltx_p ltx_align_center" id="id2.p1.9">Send correspondence to: eric.oermann@nyulangone.org, krithik.vish@utexas.edu</p>
</div>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1">Keywords: phi-3-mini, USMLE, MultiMedQA, medical Q&amp;A, knowledge distillation, low-cost, open-source 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="p2">
<p class="ltx_p" id="p2.1">GitHub: https://github.com/nyuolab/MedMobile 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text" id="p3.1.1" style="font-size:50%;">Preprint. Under review.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Main</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">In recent years, language models (LM) have shown notable promise in the medical domain for their quick decision-making and ability for reasoning and knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib3" title="">3</a>]</cite>. However, large-scale adaptation of LMs faces several barriers, including security risks and the significant computational costs of model serving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib5" title="">5</a>]</cite>. Furthermore, the most powerful large models are closed-source, hindering domain-specific adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib6" title="">6</a>]</cite>. To address these barriers, we fine-tune phi-3-mini, an open-sourced 3.8B parameter language model, on data from the medical domain. We name the resulting model MedMobile, as models of this size have been demonstrated to run on mobile devices and boast inexpensive inference costs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib7" title="">7</a>]</cite>. MedMobile was fine-tuned with manual data (curated by human experts) and synthetic data (artificially generated from GPT-4 and textbooks), demonstrating the ability of smaller language models to mimic specific tasks using synthetic data generated from larger models with high levels of accuracy. We chose to use synthetically generated data in line with the original phi work, which demonstrated the ability of smaller language models to develop reasoning with less data and parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib7" title="">7</a>]</cite>. To the best of our knowledge, MedMobile represents the smallest language model model to attain a passing score (<math alttext="\sim" class="ltx_Math" display="inline" id="Sx1.p1.1.m1.1"><semantics id="Sx1.p1.1.m1.1a"><mo id="Sx1.p1.1.m1.1.1" xref="Sx1.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx1.p1.1.m1.1b"><csymbol cd="latexml" id="Sx1.p1.1.m1.1.1.cmml" xref="Sx1.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="Sx1.p1.1.m1.1d">∼</annotation></semantics></math>60%) on the MedQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib8" title="">8</a>]</cite>, a large collection of USMLE-style questions, achieving an accuracy of 75.7%. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Enabling smaller language models to achieve superior performance on the USMLE-style and other medical tasks is an active area of research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib10" title="">10</a>]</cite>. Due to advancements in language model architecture, higher quality training data, and novel prompt engineering techniques, recent open-source models in the <math alttext="\sim" class="ltx_Math" display="inline" id="Sx1.p2.1.m1.1"><semantics id="Sx1.p2.1.m1.1a"><mo id="Sx1.p2.1.m1.1.1" xref="Sx1.p2.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="Sx1.p2.1.m1.1b"><csymbol cd="latexml" id="Sx1.p2.1.m1.1.1.cmml" xref="Sx1.p2.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p2.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="Sx1.p2.1.m1.1d">∼</annotation></semantics></math>7-8B range, such as Meerkat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib9" title="">9</a>]</cite> and UltraMedical Llama 3.1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib10" title="">10</a>]</cite>, have achieved a passing score on USMLE-style Q&amp;A (Fig 2A), even outperforming language models several times larger such as GPT-3.5 (175B) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib11" title="">11</a>]</cite>, the SOTA from two years ago. Meerkat
<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib9" title="">9</a>]</cite>, the first 7B parameter model to achieve such a distinction, focused on improving smaller models via synthetic textbook-based, USMLE-style questions generated by GPT-4.0. Another series of models, UltraMedical <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib10" title="">10</a>]</cite> expands this work, generating synthetic questions on a larger scale and across all question types in the MultiMedQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib12" title="">12</a>]</cite>. Generalist language models can be improved significantly with knowledge distillation via supervised fine-tuning (SFT) on synthetic data. In this regard, enhancing smaller language models with support from much larger models has emerged as a leading approach to achieving superior performance with low-compute requirements. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Since there is a significant loss of token generation speed and increase in power consumption on mobile devices after surpassing a model size of 5B parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib13" title="">13</a>]</cite>, we use the terminology ”mobile-size” to refer to LMs smaller than 5B parameters. In this context, MedMobile is the first mobile-sized model to pass the USMLE. To achieve this with a low parameter count, we choose phi-3-mini
as a backbone for our model as it exhibits enhanced reasoning capabilities relative to other models of the size <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib7" title="">7</a>]</cite>. Models that exhibit superior reasoning often utilize Chain-of-Thought (CoT), a technique that simulates human-like reasoning by using a sequence of logical steps to achieve an accurate final conclusion. Forefront language models, such as GPT-4, outputs a clear step-by-step process for arriving at its answer. By fine-tuning phi-3-mini using the CoT of GPT-4 (i.e., the logical process it uses to achieve it’s final conclusion), we retain generalist reasoning capabilities while gaining medical domain knowledge, partially distilling the advanced problem-solving process and knowledge of GPT-4 to phi-3-mini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib14" title="">14</a>]</cite>. Although MedMobile’s performance doesn’t exceed that of GPT-4, it marks a clinically significant advancement by enabling individuals to carry a board-certified clinical assistant in their pocket.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="Sx1.fig1">
<p class="ltx_p" id="Sx1.fig1.1"><span class="ltx_text ltx_inline-block" id="Sx1.fig1.1.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx1.fig1.1.1.1" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="Sx1.fig1.1.1.1.1"><span class="ltx_text ltx_inline-block" id="Sx1.fig1.1.1.1.1.1" style="width:0.0pt;"></span></span>
</span></span></span></p>
</figure>
<div class="ltx_para ltx_noindent" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx1.p4.1.1">Figure 1.</span> Overview of MedMobile. Panel A) shows components of MultiMedQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib12" title="">12</a>]</cite>, the evaluation tasks descriptions, and the number of questions in each data test set. MultiMedQA is a collection of 8 different datasets encompassing the medical domain. In tasks such as the MMLU, we test the model on its ability to perform complex reasoning tasks across medical and medical-adjacent domains. PubMedQA tests a model’s ability to perform reasoned conclusions based on research-grade text. Finally, MedQA (USMLE) and MedMCQA evaluates the model on its ability to answer standardized medical questions necessary to be a licensed physician. In Panel B), we present a framework of medical Q&amp;A evaluation and model building. MultiMedQA is used to evaluate a fine-tuned phi-3-mini model, MedMobile, and is optimized in its prompting using automatic differentiation with GPT-4 as described in TextGrad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib15" title="">15</a>]</cite>. Responses are then filtered via an ensemble approach, where the most common answer is selected as the model’s final answer. We also fine-tune our model on synthetic and manually medical questions, annotated with CoT by GPT-4. Panel C) exhibits a sample MedQA (USMLE) question and MedMobile’s response. Note that this is one of five responses generated before ensembling. MedMobile displays an ability to contextualize complex medical scenarios and develop expert-level conclusions. MedMobile’s output is shortened in parts for visual purposes.</p>
</div>
<figure class="ltx_figure" id="Sx1.fig2">
<p class="ltx_p" id="Sx1.fig2.1"><span class="ltx_text ltx_inline-block" id="Sx1.fig2.1.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx1.fig2.1.1.1" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="Sx1.fig2.1.1.1.1"><span class="ltx_text ltx_inline-block" id="Sx1.fig2.1.1.1.1.1" style="width:0.0pt;"></span></span>
</span></span></span></p>
</figure>
<div class="ltx_para ltx_noindent" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.2"><span class="ltx_text ltx_font_bold" id="Sx1.p5.2.1">Figure 2.</span>
Overview of language models and their performance on USMLE-style questions, contextualized over time. Panel A) shows the progession of smallest language model that is able to pass the USMLE, based on the MedQA. Panel B) displays MedMobile (red) compared to Llama-3 UltraMedical 8B (purple), and a baseline phi-3-mini (green) model on the entire MultiMedQA. MedMobile achieves almost identical or superior performance across the entirety of the MultiMedQA compared to the SOTA of <math alttext="&lt;" class="ltx_Math" display="inline" id="Sx1.p5.1.m1.1"><semantics id="Sx1.p5.1.m1.1a"><mo id="Sx1.p5.1.m1.1.1" xref="Sx1.p5.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Sx1.p5.1.m1.1b"><lt id="Sx1.p5.1.m1.1.1.cmml" xref="Sx1.p5.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p5.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Sx1.p5.1.m1.1d">&lt;</annotation></semantics></math>10B parameter language models (UltraMedical 8B), with a fraction of the parameters. Panel C) presents the relative accuracy of MedMobile to other language models on the MedQA. Current models range vastly in parameter size; with closed-source models such as Med-Palm 2 requiring 540B parameters <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib16" title="">16</a>]</cite>. Open-source models for the medical domain are led by Llama 3.1 UltraMedical 70B, which achieves an accuracy of 85.4%. In the sub-10B parameter range, Llama 3.1 UltraMedical 8B leads SOTA with an accuracy of 76.1%. In ”mobile-sized” models, which we define to have <math alttext="&lt;" class="ltx_Math" display="inline" id="Sx1.p5.2.m2.1"><semantics id="Sx1.p5.2.m2.1a"><mo id="Sx1.p5.2.m2.1.1" xref="Sx1.p5.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="Sx1.p5.2.m2.1b"><lt id="Sx1.p5.2.m2.1.1.cmml" xref="Sx1.p5.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p5.2.m2.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="Sx1.p5.2.m2.1d">&lt;</annotation></semantics></math>5B parameters due to the significantly higher quantization, slower token generation, and compute requirements after surpassing the 5B parameter threshold <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib13" title="">13</a>]</cite>, MedMobile beats previous SOTA by over 20% accuracy points. We note that there has not been significant development in the sub-5B parameter space for some time, and MedMobile is the first to surpass the passing score in this category. Panel D) shows a stepwise ablation study of components. We add individual components of the pipeline shown in Panel B and evaluate their impact on model accuracy before continuing. Through this method, we improve our accuracy from a baseline of 57.5% to a final accuracy of 75.7% on the MedQA test set.
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p6">
<p class="ltx_p" id="Sx1.p6.2">In the past few years, several techniques have demonstrated improvement in LMs’ Q&amp;A performance on various benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib17" title="">17</a>]</cite>. However, we find a lack of technique validation for our context, given that a technique, such as k-shot prompting, may only be valid for a specific size model, domain, or cocktail of techniques. To determine the positively contributing components of our pipeline, we add components one by one and evaluate after each addition (Fig. 2D). After component testing, we develop our final pipeline (Fig. 1B) built on SFT, CoT, response ensembling, and prompt optimization. While a baseline phi-3-mini at baseline achieves a score of 57.5% on the MedQA, adding CoT (+2.4%), ensembling responses (+7.4%), and conducting SFT (+8.4%) allows MedMobile to achieve an accuracy of 75.7%. In doing so, we noted several promising potential pipeline components did not favorably impact inference in medical Q&amp;A, such as k-shot prompting with examples (<math alttext="-" class="ltx_Math" display="inline" id="Sx1.p6.1.m1.1"><semantics id="Sx1.p6.1.m1.1a"><mo id="Sx1.p6.1.m1.1.1" xref="Sx1.p6.1.m1.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="Sx1.p6.1.m1.1b"><minus id="Sx1.p6.1.m1.1.1.cmml" xref="Sx1.p6.1.m1.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p6.1.m1.1c">-</annotation><annotation encoding="application/x-llamapun" id="Sx1.p6.1.m1.1d">-</annotation></semantics></math>9.4%) and retrieval-augmented generation (RAG) (<math alttext="-" class="ltx_Math" display="inline" id="Sx1.p6.2.m2.1"><semantics id="Sx1.p6.2.m2.1a"><mo id="Sx1.p6.2.m2.1.1" xref="Sx1.p6.2.m2.1.1.cmml">−</mo><annotation-xml encoding="MathML-Content" id="Sx1.p6.2.m2.1b"><minus id="Sx1.p6.2.m2.1.1.cmml" xref="Sx1.p6.2.m2.1.1"></minus></annotation-xml><annotation encoding="application/x-tex" id="Sx1.p6.2.m2.1c">-</annotation><annotation encoding="application/x-llamapun" id="Sx1.p6.2.m2.1d">-</annotation></semantics></math>12.6%) from high-quality sources (i.e., textbooks), perhaps due to an increased input token length. This improvement represents a substantial increase from the next best sub-5B parameter language model, VOD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib18" title="">18</a>]</cite>, at an accuracy of 55.0% on the MedQA. MedMobile’s accuracy on the entirety of the MultiMedQA is comparable to the SOTA models in the medical domain with over double the number of parameters. In fact, MedMobile beats or matches UltraMedical 8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib10" title="">10</a>]</cite>, the current model with the highest accuracy in the sub-10B parameter space, in 6 out of 9 evaluation tasks in the MultiMedQA (Fig. 2B). To the best of our knowledge, MedMobile is also the smallest model to achieve the distinction of passing USMLE-like questions on the MedQA.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p7">
<p class="ltx_p" id="Sx1.p7.1">MedMobile displays an ability to develop explainable responses to complex medical scenarios, carefully accounting for a complex combination of patient symptoms that may or may not influence treatment (Fig. 1C). There is a clear delineation of step-by-step logical CoT responses, evidence of medical knowledge distillation from GPT-4’s and retention of reasoning capabilities. However, we note a decrease in performance based on token output length (see Supplemental Figure 1B). This can potentially be attributed to a loss of model CoT when crafting longer responses as fine-tuned smaller models tend to have reduced reasoning capabilities and weaker CoT. This can be compared to phi-3-mini baseline, which has a more consistent accuracy across the different token outputs it contains. However, across almost all bins, irrespective of output length, MedMobile outperforms phi-3-mini, highlighting the gain of domain-specific knowledge and the resultant gain in MedMobile’s performance on medical tasks.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p8">
<p class="ltx_p" id="Sx1.p8.1">Contrary to popular literature, we do not utilize many of the prompt engineering approaches that are common for large language models including retrieval augmented generation (RAG) and k-shot prompting. We implemented these techniques (see Supplemental Figure 3), but they did not lead to any significant degree of improvement. We hypothesize that this is mainly driven by the context-window limitations small language models have, and note these are interesting barriers to tackle for future research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib7" title="">7</a>]</cite>. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p9">
<p class="ltx_p" id="Sx1.p9.1">There are several limitations that apply to our work. While we demonstrate a significant improvement over previous open-source models of MedMobile’s size, any-size models still demonstrate superior performance on medical tasks. Thus, ignoring the barriers of subscription fees and issues with uploading classified patient health information, GPT-4 can be used for quick and reliable online inference. We also note that real-world clinical and patient-facing deployment of MedMobile is yet to be evaluated, and is left for future works. Finally, MedMobile, in this work, is trained only on language and cannot receive image input. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p10">
<p class="ltx_p" id="Sx1.p10.1">This work can be easily expanded to vision-language models (VLMs) by building upon Phi-3-vision using this pipeline. VLMs have shown promise for superhuman predictive power and novel pattern recognition but notoriously require extensive training and inference costs due to the larger data sizes associated with high-resolution imaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib20" title="">20</a>]</cite>. Using a smaller, domain-specific model, such as MedMobile, serves to combat these rising computational costs. Alongside this rise, we also note the increase in novel imaging methods that provide new dimensional data that machine learning models can leverage, such as photoacoustic imaging providing spectral information to individual voxels or shear wave elasticity imaging providing information about tissue stiffness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib22" title="">22</a>]</cite>. In light of the increase in imaging data from new dimensions (e.g., spectral data from photoacoustic imaging or tissue stiffness data from shearwave imaging) in these modalities, smaller language models may serve to foster new, cutting-edge insights and patterns that otherwise are hidden from humans, while bolstering quick compute times. In tandem with improvements of imaging modalities, VLM pattern recognition, and the increase in mobile-based ML platforms, such as Apple’s new Apple Intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib23" title="">23</a>]</cite>, we envision a method of use for mobile-sized VLMs centered around accessibility, where doctors and patients can take images with their iPhone and receive insights from a expert-level, fine-tuned LLM, without comprising personal security or requiring extensive computing power.

<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p11">
<p class="ltx_p" id="Sx1.p11.1">Recent studies in other domains have also demonstrated effective improvements to benchmark accuracy when using multi-LM agent-based systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib24" title="">24</a>]</cite>. A promising avenue for future research could be utilizing MedMobile as part of a multi-LM system, where problem-solving is divided into multiple iterations of MedMobile. Further distillation of GPT-4 on each agent in such an ensemble may allow for significant improvements to accuracy. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p12">
<p class="ltx_p" id="Sx1.p12.1">Highly accurate, expert-level mobile-sized language models, such as MedMobile, hold promise in low and middle-resource settings due to their reduced compute requirements and quicker inference times <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib27" title="">27</a>]</cite> and also serve to democratize access to the technical capabilities of LLMs beyond the domain of large technology companies and groups with substantial computing budgets. While we develop this work primarily for its impact in the medical domain, mobile-size language models and the related techniques in this work can be applied to any domain to train expert-level mobile assistants. We hope this work, and our open-source codebase, will contribute to the clinically meaningful development of mobile-sized language models that benefit physicians and patients.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Methods</h2>
<div class="ltx_para ltx_noindent" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1"><span class="ltx_text ltx_font_bold" id="Sx2.p1.1.1">Evaluation Data</span>
<br class="ltx_break"/>To determine an LM’s ability in the medical domain, we evaluate the model on the MultiMedQA, a multi-dataset of medical questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib12" title="">12</a>]</cite>. The MultiMedQA is composed of 8 individual datasets ranging from USMLE-style questions (MedQA) to College Biology (MMLU College Biology) and is outlined in Figure 1A. We choose to evaluate on these datasets due to the expert level of medical reasoning and knowledge required for USMLE-style questions, and to test the model’s ability against the range of medical tasks with the other datasets. Testing on the PubMedQA also demonstrates MedMobile’s ability to perform on research-related medical inquiries. These results are displayed in Supplemental Table 1. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1"><span class="ltx_text ltx_font_bold" id="Sx2.p2.1.1">Supervised Fine-Tuning (SFT)</span>
<br class="ltx_break"/>To train phi-3-mini’s baseline parameters to the medical domain, we utilize the UltraMedical dataset, a collection of over 400K synthetic and manual-curated multiple-choice questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib10" title="">10</a>]</cite>. In particular, we instruction-fine-tune phi-3-mini using CoT responses from GPT-4 for each of these questions, allowing for knowledge distillation from GPT-4’s much larger parameter set. To perform SFT, we train phi-3-mini for 3 epochs on 4 A100 nodes for 83 hours on the UltraMedical dataset. We also utilize a learning rate of <math alttext="1\times 10^{-4}" class="ltx_Math" display="inline" id="Sx2.p2.1.m1.1"><semantics id="Sx2.p2.1.m1.1a"><mrow id="Sx2.p2.1.m1.1.1" xref="Sx2.p2.1.m1.1.1.cmml"><mn id="Sx2.p2.1.m1.1.1.2" xref="Sx2.p2.1.m1.1.1.2.cmml">1</mn><mo id="Sx2.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="Sx2.p2.1.m1.1.1.1.cmml">×</mo><msup id="Sx2.p2.1.m1.1.1.3" xref="Sx2.p2.1.m1.1.1.3.cmml"><mn id="Sx2.p2.1.m1.1.1.3.2" xref="Sx2.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="Sx2.p2.1.m1.1.1.3.3" xref="Sx2.p2.1.m1.1.1.3.3.cmml"><mo id="Sx2.p2.1.m1.1.1.3.3a" xref="Sx2.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="Sx2.p2.1.m1.1.1.3.3.2" xref="Sx2.p2.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx2.p2.1.m1.1b"><apply id="Sx2.p2.1.m1.1.1.cmml" xref="Sx2.p2.1.m1.1.1"><times id="Sx2.p2.1.m1.1.1.1.cmml" xref="Sx2.p2.1.m1.1.1.1"></times><cn id="Sx2.p2.1.m1.1.1.2.cmml" type="integer" xref="Sx2.p2.1.m1.1.1.2">1</cn><apply id="Sx2.p2.1.m1.1.1.3.cmml" xref="Sx2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="Sx2.p2.1.m1.1.1.3.1.cmml" xref="Sx2.p2.1.m1.1.1.3">superscript</csymbol><cn id="Sx2.p2.1.m1.1.1.3.2.cmml" type="integer" xref="Sx2.p2.1.m1.1.1.3.2">10</cn><apply id="Sx2.p2.1.m1.1.1.3.3.cmml" xref="Sx2.p2.1.m1.1.1.3.3"><minus id="Sx2.p2.1.m1.1.1.3.3.1.cmml" xref="Sx2.p2.1.m1.1.1.3.3"></minus><cn id="Sx2.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="Sx2.p2.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.p2.1.m1.1c">1\times 10^{-4}</annotation><annotation encoding="application/x-llamapun" id="Sx2.p2.1.m1.1d">1 × 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math> and an effective batch size of 32. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx2.p3">
<p class="ltx_p" id="Sx2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx2.p3.1.1">Prompt Optimization</span>
<br class="ltx_break"/>To ensure streamlined and favorable prompting, we utilize TextGrad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib15" title="">15</a>]</cite>, a multi-LLM system for improving prompting verbiage using. TextGrad automatically develops improvements to smaller language models’ prompts by utilizing a much stronger model (in this case, we use GPT-4). GPT-4, as an optimizer model, generates new prompting templates. Then, a loss is calculated based on the accuracy generated by MedMobile on the prompt. While TextGrad finds an improved prompt verbiage for phi-3-mini baseline, it also supports that MedMobile does best with no additional prompting instructions. Due to the limited context window capabilities of a model of this size, it is likely that the additional text only hinders the model from domain-specific tasks that it is already trained to reason within. By utilizing the CoT responses of GPT-4 to fine-tune MedMobile, MedMobile exhibits high levels of medical reasoning without the necessity of additional prompting. 
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx2.p4">
<p class="ltx_p" id="Sx2.p4.1"><span class="ltx_text ltx_font_bold" id="Sx2.p4.1.1">Other Techniques</span>
<br class="ltx_break"/>MedMobile was also assessed with other prompting methods such as k-shot prompting, retrieval augmented generation, BM-25 searching, and additional prompting techniques. To develop retrieval-based prompting methods, we feed in paragraphs from Harrison’s Principles of Internal Medicine, 21e <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib28" title="">28</a>]</cite>. We attempt a variety of retrieval-based scenarios, such as the lucine implementation of BM-25, RAG based on cosine similarity with questions and paragraphs embedded with MedCPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib29" title="">29</a>]</cite>, and combination methods that use both scores to select the best contextual paragraphs related to the question. However, we note that these additions did not broadly improve model performance.

<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para ltx_noindent" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">EKO is supported by the National Cancer Institute’s Early Surgeon Scientist Program (3P30CA016087-41S1) and the W.M. Keck Foundation. We would like to acknowledge Nader Mherabi and Dafna Bar-Sagi, Ph.D., for their continued support of medical AI research at NYU. We thank Michael Constantino, Kevin Yie, and the NYU Langone High-Performance Computing (HPC) Team for supporting computing resources fundamental to our work.</p>
</div>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>
<div class="ltx_para ltx_noindent" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">EKO conceptualized and supervised the study. KV designed the MedMobile LLM pipeline. KV, JS, and AA implemented and trained the LLM. KV evaluated and tested the LLM. JS aided with LLM serving and deployment. KV wrote the initial draft of the manuscript. KV, AA, DAA, EKO edited the manuscript. All authors revised and approved the manuscript.</p>
</div>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Competing Interests</h2>
<div class="ltx_para ltx_noindent" id="Sx5.p1">
<p class="ltx_p" id="Sx5.p1.1">Disclosures: EKO and DA report consulting income with Sofinnova Partners. EKO reports equity in Eikon Therapeutics, Artisight Incorporate. The other authors have no personal, financial, or institutional interest pertinent to this article.</p>
</div>
</section>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Data availability</h2>
<div class="ltx_para ltx_noindent" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">The datasets generated or analysed during the current study are available in the nyuolab/MedMobile repository, https://github.com/nyuolab/MedMobile. The model weights are available on 
<br class="ltx_break"/>https://huggingface.co/KrithikV/MedMobile.</p>
</div>
</section>
<section class="ltx_section" id="Sx7">
<h2 class="ltx_title ltx_title_section">Code availability</h2>
<div class="ltx_para ltx_noindent" id="Sx7.p1">
<p class="ltx_p" id="Sx7.p1.1">Our code is shared publicly on GitHub upon publication of this work and can be found at 
<br class="ltx_break"/>https://github.com/nyuolab/MedMobile.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al.

</span>
<span class="ltx_bibblock">Can generalist foundation models outcompete special-purpose tuning? case study in medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2311.16452</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al.

</span>
<span class="ltx_bibblock">Capabilities of gemini models in medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2404.18416</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony Riina, Ilya Laufer, Paawan Punjabi, et al.

</span>
<span class="ltx_bibblock">Health system-scale language models are all-purpose prediction engines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Nature</span>, 619(7969):357–362, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Ehsan Ullah, Anil Parwani, Mirza Mansoor Baig, and Rajendra Singh.

</span>
<span class="ltx_bibblock">Challenges and barriers of using large language models (llm) such as chatgpt for diagnostic medicine with a focus on digital pathology–a recent scoping review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Diagnostic pathology</span>, 19(1):43, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Xiuquan Li and Tao Zhang.

</span>
<span class="ltx_bibblock">An exploration on artificial intelligence application: From security, privacy and ethic perspective.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">2017 IEEE 2nd International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)</span>, pages 416–420, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ruei-Shan Lu, Ching-Chang Lin, and Hsiu-Yuan Tsao.

</span>
<span class="ltx_bibblock">Empowering large language models to leverage domain-specific knowledge in e-learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Applied Sciences</span>, 14(12):5264, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al.

</span>
<span class="ltx_bibblock">Phi-3 technical report: A highly capable language model locally on your phone.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2404.14219</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Applied Sciences</span>, 11(14):6421, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">Small language models learn enhanced reasoning skills from medical textbooks, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, Xingtai Lv, Hu Jinfang, Zhiyuan Liu, and Bowen Zhou.

</span>
<span class="ltx_bibblock">Ultramedical: Building specialized generalists in biomedicine, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Openai OpenAI.

</span>
<span class="ltx_bibblock">Openai: Introducing chatgpt.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">URL https://openai. com/blog/chatgpt</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Nature</span>, 620(7972):172–180, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Stefanos Laskaridis, Kleomenis Kateveas, Lorenzo Minto, and Hamed Haddadi.

</span>
<span class="ltx_bibblock">Melting point: Mobile evaluation of language transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2403.12844</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou.

</span>
<span class="ltx_bibblock">A survey on knowledge distillation of large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2402.13116</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou.

</span>
<span class="ltx_bibblock">Textgrad: Automatic ”differentiation” via text, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al.

</span>
<span class="ltx_bibblock">Palm 2 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2305.10403</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha.

</span>
<span class="ltx_bibblock">A systematic survey of prompt engineering in large language models: Techniques and applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2402.07927</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Valentin Liévin, Andreas Geert Motzfeldt, Ida Riis Jensen, and Ole Winther.

</span>
<span class="ltx_bibblock">Variational open-domain question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">International Conference on Machine Learning</span>, pages 20950–20977. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Long-short transformer: Efficient transformers for language and vision.

</span>
<span class="ltx_bibblock">In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</span>, volume 34, pages 17723–17736. Curran Associates, Inc., 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Yixing Jiang, Jesutofunmi A Omiye, Cyril Zakka, Michael Moor, Haiwen Gui, Shayan Alipour, Seyed Shahabeddin Mousavi, Jonathan H Chen, Pranav Rajpurkar, and Roxana Daneshjou.

</span>
<span class="ltx_bibblock">Evaluating general vision-language models for clinical medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">medRxiv</span>, pages 2024–04, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Armen P Sarvazyan, Oleg V Rudenko, Scott D Swanson, J Brian Fowlkes, and Stanislav Y Emelianov.

</span>
<span class="ltx_bibblock">Shear wave elasticity imaging: a new ultrasonic technology of medical diagnostics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Ultrasound in medicine &amp; biology</span>, 24(9):1419–1435, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Nilesh Mathuria, Krithik Vishwanath, Blake C. Fallon, Antonio Martino, Giorgio Brero, Richard R Willson, Miguel Valderrabano, Carly S. Filgueira, and Richard R. Bouchard.

</span>
<span class="ltx_bibblock">In vivo assessment of cardiac radiofrequency ablation in a large-animal model using photoacoustic-ultrasound imaging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">medRxiv</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al.

</span>
<span class="ltx_bibblock">Apple intelligence foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2407.21075</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang.

</span>
<span class="ltx_bibblock">Small llms are weak tool learners: A multi-llm agent.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2401.07324</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Nethra Venkatayogi, Maanas Gupta, Alaukik Gupta, Shreya Nallaparaju, Nithya Cheemalamarri, Krithika Gilari, Shireen Pathak, Krithik Vishwanath, Carel Soney, Tanisha Bhattacharya, et al.

</span>
<span class="ltx_bibblock">From seeing to knowing with artificial intelligence: A scoping review of point-of-care ultrasound in low-resource settings.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Applied Sciences</span>, 13(14):8427, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Diego M López, Carolina Rico-Olarte, Bernd Blobel, and Carol Hullin.

</span>
<span class="ltx_bibblock">Challenges and solutions for transforming health ecosystems in low-and middle-income countries through artificial intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Frontiers in Medicine</span>, 9:958097, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Tadeusz Ciecierski-Holmes, Ritvij Singh, Miriam Axt, Stephan Brenner, and Sandra Barteit.

</span>
<span class="ltx_bibblock">Artificial intelligence for strengthening healthcare systems in low-and middle-income countries: a systematic scoping review.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">npj Digital Medicine</span>, 5(1):162, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
E Silverman, J Crapo, B Make, J Jameson, A Fauci, D Kasper, S Hauser, D Longo, and J Loscalzo.

</span>
<span class="ltx_bibblock">Harrison’s principles of internal medicine 21e, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and Zhiyong Lu.

</span>
<span class="ltx_bibblock">Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Bioinformatics</span>, 39(11):btad651, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx8">
<h2 class="ltx_title ltx_title_section">Supplementary Material</h2>
<figure class="ltx_figure" id="Sx8.fig1">
<p class="ltx_p" id="Sx8.fig1.1"><span class="ltx_text ltx_inline-block" id="Sx8.fig1.1.1" style="width:433.6pt;">
<span class="ltx_inline-block ltx_transformed_outer" id="Sx8.fig1.1.1.1" style="width:0.0pt;height:0pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="Sx8.fig1.1.1.1.1"><span class="ltx_text" id="Sx8.fig1.1.1.1.1.1"></span></span>
</span></span></span></p>
</figure>
<div class="ltx_para ltx_noindent" id="Sx8.p1">
<p class="ltx_p" id="Sx8.p1.1"><span class="ltx_text ltx_font_bold" id="Sx8.p1.1.1">Supplemental Figure 1.</span> Comparison of number of output tokens in a response and accuracy on MedQA questions. Each question of the MedQA test set is represented 5x in this figure due to the ensemble performed. Some questions are not included in the plots (<math alttext="&lt;20" class="ltx_Math" display="inline" id="Sx8.p1.1.m1.1"><semantics id="Sx8.p1.1.m1.1a"><mrow id="Sx8.p1.1.m1.1.1" xref="Sx8.p1.1.m1.1.1.cmml"><mi id="Sx8.p1.1.m1.1.1.2" xref="Sx8.p1.1.m1.1.1.2.cmml"></mi><mo id="Sx8.p1.1.m1.1.1.1" xref="Sx8.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="Sx8.p1.1.m1.1.1.3" xref="Sx8.p1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx8.p1.1.m1.1b"><apply id="Sx8.p1.1.m1.1.1.cmml" xref="Sx8.p1.1.m1.1.1"><lt id="Sx8.p1.1.m1.1.1.1.cmml" xref="Sx8.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="Sx8.p1.1.m1.1.1.2.cmml" xref="Sx8.p1.1.m1.1.1.2">absent</csymbol><cn id="Sx8.p1.1.m1.1.1.3.cmml" type="integer" xref="Sx8.p1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx8.p1.1.m1.1c">&lt;20</annotation><annotation encoding="application/x-llamapun" id="Sx8.p1.1.m1.1d">&lt; 20</annotation></semantics></math>) as model response exceeded maximum generation output and an accuracy could not be evaluated. Top panel is a CoT enhanced baseline phi-3-mini model, whereas the bottom panel is our fine-tuned model, MedMobile.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="Sx8.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="581" id="Sx8.1.g1" src="x1.png" width="830"/>
</figure>
<div class="ltx_para ltx_noindent" id="Sx8.p2">
<p class="ltx_p" id="Sx8.p2.1"><span class="ltx_text ltx_font_bold" id="Sx8.p2.1.1">Supplemental Figure 2.</span>
Comparison of number of input tokens in a response and accuracy on MedQA questions. Each question of the MedQA test set is represented 5x in this figure due to the ensemble performed. Some questions are not included in the plots (<math alttext="&lt;20" class="ltx_Math" display="inline" id="Sx8.p2.1.m1.1"><semantics id="Sx8.p2.1.m1.1a"><mrow id="Sx8.p2.1.m1.1.1" xref="Sx8.p2.1.m1.1.1.cmml"><mi id="Sx8.p2.1.m1.1.1.2" xref="Sx8.p2.1.m1.1.1.2.cmml"></mi><mo id="Sx8.p2.1.m1.1.1.1" xref="Sx8.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="Sx8.p2.1.m1.1.1.3" xref="Sx8.p2.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx8.p2.1.m1.1b"><apply id="Sx8.p2.1.m1.1.1.cmml" xref="Sx8.p2.1.m1.1.1"><lt id="Sx8.p2.1.m1.1.1.1.cmml" xref="Sx8.p2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="Sx8.p2.1.m1.1.1.2.cmml" xref="Sx8.p2.1.m1.1.1.2">absent</csymbol><cn id="Sx8.p2.1.m1.1.1.3.cmml" type="integer" xref="Sx8.p2.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx8.p2.1.m1.1c">&lt;20</annotation><annotation encoding="application/x-llamapun" id="Sx8.p2.1.m1.1d">&lt; 20</annotation></semantics></math>) as model response exceeded maximum generation output and an accuracy could not be evaluated. Top panel is a CoT enhanced baseline phi-3-mini model, whereas the bottom panel is our trained model, MedMobile.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="Sx8.2"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="239" id="Sx8.2.g1" src="extracted/5886025/Figures/Supp3.png" width="598"/>
</figure>
<div class="ltx_para ltx_noindent" id="Sx8.p3">
<p class="ltx_p" id="Sx8.p3.1"><span class="ltx_text ltx_font_bold" id="Sx8.p3.1.1">Supplemental Figure 3.</span>
Panel A) depicts the accuracy of MedMobile on the MedQA relative to the number of k-shot prompting (i.e., number of examples given to the model alongside the evaluation question). Panel B) shows different forms of retrieval for RAG and their resultant effects on the accuracy of MedMobile on the MedQA dataset. To conduct RAG based on vector embeddings, we compute cosine similarity based on MedCPT vectors generation between the question and paragraphs in the textbook. RAG built on BM-25 is developed through the lucine implementation, and selects the paragraph with the highest score for a particular question. While all forms of RAG achieve sub-optimal results, we note that BM25 seemed to affect the model least negatively with the addition of context. The source of information for these evaluations is from Harrison’s Principles of Internal Medicine, 21e <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib28" title="">28</a>]</cite>. 
<br class="ltx_break"/>
<br class="ltx_break"/></p>
</div>
<div class="ltx_para ltx_noindent" id="Sx8.p4">
<p class="ltx_p" id="Sx8.p4.1"><span class="ltx_text ltx_font_bold" id="Sx8.p4.1.1">Supplemental Table 1.</span>
Evaluation results across the MultiMedQA, for phi-3-mini, MedMobile, UltraMedical 8B, and Flan-Palm. Scores for UltraMedical 8B and Flan-Palm are sourced from literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09019v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="Sx8.tab1">
<div class="ltx_inline-block ltx_transformed_outer" id="Sx8.tab1.1" style="width:433.6pt;height:138.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-64.2pt,20.6pt) scale(0.771602468598324,0.771602468598324) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="Sx8.tab1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx8.tab1.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.1.1.2.1">phi-3-mini Baseline (3.8B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.1.1.3.1">MedMobile (3.8B)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.1.1.4.1">UltraMedical (8B)</span></td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.1.1.5.1">Flan-PaLM (540B)</span></td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="Sx8.tab1.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.2.2.1.1">MedQA (USMLE)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx8.tab1.1.1.2.2.2">57.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx8.tab1.1.1.2.2.3">75.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Sx8.tab1.1.1.2.2.4">76.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx8.tab1.1.1.2.2.5">67.6</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.3.3.1.1">MedMCQA Dev</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.3.3.2">56.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.3.3.3">63.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.3.3.4">63.8</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.3.3.5">57.6</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.4.4.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.4.4.1.1">PubMedQA Reasoning Required</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.4.4.2">75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.4.4.3">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.4.4.4">78.2</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.4.4.5">79</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.5.5.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.5.5.1.1">MMLU (Clinical Knowledge)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.5.5.2">75.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.5.5.3">81.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.5.5.4">77.4</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.5.5.5">80.4</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.6.6.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.6.6.1.1">MMLU (Medical Genetics)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.6.6.2">76</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.6.6.3">88</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.6.6.4">88</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.6.6.5">75</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.7.7.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.7.7.1.1">MMLU (Anatomy)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.7.7.2">63</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.7.7.3">75.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.7.7.4">74.8</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.7.7.5">63.7</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.8.8.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.8.8.1.1">MMLU (Professional Medicine)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.8.8.2">68.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.8.8.3">86.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.8.8.4">84.6</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.8.8.5">83.8</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.9.9.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.9.9.1.1">MMLU (College Biology)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.9.9.2">86.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.9.9.3">86.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.9.9.4">79.9</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.9.9.5">88.9</td>
</tr>
<tr class="ltx_tr" id="Sx8.tab1.1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx8.tab1.1.1.10.10.1"><span class="ltx_text ltx_font_bold" id="Sx8.tab1.1.1.10.10.1.1">MMLU (College Medicine)</span></th>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.10.10.2">63.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.10.10.3">78</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Sx8.tab1.1.1.10.10.4">75.1</td>
<td class="ltx_td ltx_align_center" id="Sx8.tab1.1.1.10.10.5">76.3</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 11 17:31:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
