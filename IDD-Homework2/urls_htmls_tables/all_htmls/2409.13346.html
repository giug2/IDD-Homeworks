<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Imagine yourself: Tuning-Free Personalized Image Generation</title>
<!--Generated on Fri Sep 20 09:17:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on September 20, 2024.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13346v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S1" title="In Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S2" title="In Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S2.SS1" title="In 2 Related Work ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Text-to-Image Diffusion Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S2.SS2" title="In 2 Related Work ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Tuning-based Personalization Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S2.SS3" title="In 2 Related Work ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Tuning-free Personalization Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3" title="In Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS1" title="In 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS2" title="In 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS3" title="In 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Synthetic Paired Data (<span class="ltx_text ltx_font_typewriter">SynPairs</span>)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4" title="In 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Model Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4.SSS1" title="In 3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Vision Encoder</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4.SSS2" title="In 3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Text Encoders</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4.SSS3" title="In 3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Fully Parallel Image-Text Fusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4.SSS4" title="In 3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>LoRA</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS5" title="In 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Multi-Stage Finetune</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS6" title="In 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Extension to Multi-Subject Personalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4" title="In Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS1" title="In 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Qualitative Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS2" title="In 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Quantitative Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS2.SSS1" title="In 4.2 Quantitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Evaluation Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS2.SSS2" title="In 4.2 Quantitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Benchmarked Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS2.SSS3" title="In 4.2 Quantitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Human Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS3" title="In 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS3.SSS1" title="In 4.3 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Impact of Multi-stage Finetune</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS3.SSS2" title="In 4.3 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Impact of Fully Parallel Attention</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.SS3.SSS3" title="In 4.3 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Impact of Synthetic Pairs</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S5" title="In Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S6" title="In Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Imagine yourself: Tuning-Free Personalized Image Generation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zecheng He<sup class="ltx_sup" id="id5.2.id1">‚àó</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Sun<sup class="ltx_sup" id="id6.2.id1">‚àó</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix Juefei-Xu<sup class="ltx_sup" id="id7.2.id1">‚àó</sup>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoyu Ma
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ankit Ramchandani
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vincent Cheung
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Siddharth Shah
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anmol Kalia
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Harihar Subramanyam
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alireza Zareian
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Li Chen
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ankit Jain
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ning Zhang
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peizhao Zhang
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roshan Sumbaly
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Peter Vajda
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Animesh Sinha<sup class="ltx_sup" id="id8.2.id1">‚àó</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">GenAI, Meta
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zechengh@meta.com">zechengh@meta.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(September 20, 2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, <em class="ltx_emph ltx_font_italic" id="id9.id1.1">e.g</em>.<span class="ltx_text" id="id9.id1.2"></span>, changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the model‚Äôs SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\correspondence</span>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup>* Core contributors</span></span></span>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large scale diffusion models have drawn significant attention. These models, trained on vast amounts of image-text pairs, showcase remarkable semantic understanding capabilities and are able to generate diverse, photo-realistic images based on textual prompts. Due to their unparalleled creative abilities, large-scale diffusion models have found applications across a spectrum of image-to-image tasks beyond the original text-to-image generation, <em class="ltx_emph ltx_font_italic" id="S1.p1.1.1">e.g</em>.<span class="ltx_text" id="S1.p1.1.2"></span>, image editing, image completion, style transfer, and controllable generation.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="581" id="S1.F1.g1" src="extracted/5866712/figures/kosher_figure/teaser_v11_kosher.png" width="580"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Generated results for the four reference images (depicted below) using Imagine yourself. The single reference image is used to generate those subjects in novel poses and styles.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Personalized image generation techniques have gained significant attention alongside large-scale diffusion models. These methods focus on tailoring image generation to individual preferences or specific user characteristics. By incorporating customization into the generation process, these techniques aim to create images that are more relevant and appealing to the individual user. One line of research tunes a text-to-image model to incorporate the identity <cite class="ltx_cite ltx_citemacro_citep">(Gal et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib4" title="">2022</a>; Ruiz et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib14" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib15" title="">b</a>)</cite> with a few reference images. However, these methods are not efficient or generalizable as they require a different model to be tuned for each new user.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recently, another effort has been proposed to obtain personalized diffusion models without subject-specific tuning. This direction of research extracts vision embedding from a reference images and inject it to the diffusion process <cite class="ltx_cite ltx_citemacro_citep">(Wei et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib20" title="">2023</a>; Li et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib6" title="">2023</a>; Chen et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib1" title="">2023</a>; Ye et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib22" title="">2023</a>; Wang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib19" title="">2024</a>; Zhang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib23" title="">2023</a>; Ostashev et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib8" title="">2024</a>)</cite>. While the previous work in this direction can achieve a personalized model generalizable to all users, it usually comes with a strong over-fitting behavior, <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">i.e</em>.<span class="ltx_text" id="S1.p3.1.2"></span>, a copy-paste effect, to the reference image. Thus, they can hardly generate images following prompts that require significant changes to the reference image, <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">e.g</em>.<span class="ltx_text" id="S1.p3.1.4"></span>, change facial expression, head and body poses, hence the diversity of the generated images is low. As a result, these models cannot preserve identity while following complex prompts at the same time.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we propose Imagine yourself, a state-of-the-art model for personalized image generation without subject-specific fine-tuning. Unlike previous tuning-based personalization techniques which require tuning for each user, Imagine yourself is a tuning-free model where all subjects share a single model. We investigated the key components that lead to the quality improvement of Imagine yourself: <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Identity preservation:</span> Trainable vision encoders with zero conv initialization, and masked vision embedding, <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">Visual quality:</span> multi-stage finetune and human-in-the-loop (HITL), <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">Text alignment:</span> Synthetic data and parallel attention. Meanwhile, we show that Imagine yourself outperforms the SOTA personalization models <cite class="ltx_cite ltx_citemacro_citep">(Ye et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib22" title="">2023</a>; Wang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib19" title="">2024</a>)</cite>, with significant margin in all aspects including identity preservation, visual quality, and text alignment, through large scale human evaluation. In particular, we won +27.8% in text alignment compared to SOTA on complex prompts.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions can be summarized as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose Imagine yourself, an innovative state-of-the-art model for personalized image generation. The proposed model can take any reference image as input for customized image generation and does not need tuning for each subject.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Imagine yourself incorporates new components and shows significant improvements over the existing models: a new synthetic paired data generation mechanism to encourage image diversity, a fully parallel attention with three text encoders and a fully trainable vision encoder architecture to improve the text faithfulness, and a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We provide comprehensive qualitative and quantitative evaluation results compared to the state-of-the-art models. We provide human annotations on thousands of test examples as a golden standard to demonstrate the superior performance of Imagine yourself in all aspects, including identity preservation, prompt alignment, and visual appeal.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S1.T1.4.1.1" style="font-size:129%;">Table 1</span>: </span><span class="ltx_text" id="S1.T1.5.2" style="font-size:129%;">Quantitative evaluation results of Imagine yourself vs. the SOTA control-based model and the SOTA adapter-based model under head-to-head human evaluation setting.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S1.T1.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T1.6.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S1.T1.6.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S1.T1.6.1.1.2">
<span class="ltx_text ltx_font_bold" id="S1.T1.6.1.1.2.1" style="font-size:70%;">Head-to-head</span><span class="ltx_text" id="S1.T1.6.1.1.2.2" style="font-size:70%;"> (win rate)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S1.T1.6.1.1.3">
<span class="ltx_text ltx_font_bold" id="S1.T1.6.1.1.3.1" style="font-size:70%;">Head-to-head</span><span class="ltx_text" id="S1.T1.6.1.1.3.2" style="font-size:70%;"> (win rate)</span>
</th>
</tr>
<tr class="ltx_tr" id="S1.T1.6.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S1.T1.6.2.2.1"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.1.1" style="font-size:70%;">Metrics</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.2.2.2"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.2.1" style="font-size:70%;">SOTA control-based model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.2.2.3"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.3.1" style="font-size:70%;">Imagine yourself</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S1.T1.6.2.2.4"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.4.1" style="font-size:70%;">Tie</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.2.2.5"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.5.1" style="font-size:70%;">SOTA adapter-based model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.2.2.6"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.6.1" style="font-size:70%;">Imagine yourself</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.6.2.2.7"><span class="ltx_text ltx_font_bold" id="S1.T1.6.2.2.7.1" style="font-size:70%;">Tie</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.6.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S1.T1.6.3.1.1"><span class="ltx_text" id="S1.T1.6.3.1.1.1" style="font-size:70%;">Prompt Alignment</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.6.3.1.2"><span class="ltx_text" id="S1.T1.6.3.1.2.1" style="font-size:70%;">1.2%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.6.3.1.3"><span class="ltx_text ltx_font_bold" id="S1.T1.6.3.1.3.1" style="font-size:70%;">46.3%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S1.T1.6.3.1.4"><span class="ltx_text" id="S1.T1.6.3.1.4.1" style="font-size:70%;">52.6%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.6.3.1.5"><span class="ltx_text" id="S1.T1.6.3.1.5.1" style="font-size:70%;">1.6%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.6.3.1.6"><span class="ltx_text ltx_font_bold" id="S1.T1.6.3.1.6.1" style="font-size:70%;">32.4%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.6.3.1.7"><span class="ltx_text" id="S1.T1.6.3.1.7.1" style="font-size:70%;">66.0%</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.6.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S1.T1.6.4.2.1"><span class="ltx_text" id="S1.T1.6.4.2.1.1" style="font-size:70%;">Identity Preservation</span></th>
<td class="ltx_td ltx_align_center" id="S1.T1.6.4.2.2"><span class="ltx_text ltx_font_bold" id="S1.T1.6.4.2.2.1" style="font-size:70%;">15.1%*</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.6.4.2.3"><span class="ltx_text" id="S1.T1.6.4.2.3.1" style="font-size:70%;">3.2%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S1.T1.6.4.2.4"><span class="ltx_text" id="S1.T1.6.4.2.4.1" style="font-size:70%;">81.7%</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.6.4.2.5"><span class="ltx_text" id="S1.T1.6.4.2.5.1" style="font-size:70%;">3.8%</span></td>
<td class="ltx_td ltx_align_center" id="S1.T1.6.4.2.6">
<span class="ltx_text ltx_font_bold" id="S1.T1.6.4.2.6.1" style="font-size:70%;">5.5</span><span class="ltx_text" id="S1.T1.6.4.2.6.2" style="font-size:70%;">%</span>
</td>
<td class="ltx_td ltx_align_center" id="S1.T1.6.4.2.7"><span class="ltx_text" id="S1.T1.6.4.2.7.1" style="font-size:70%;">90.7%</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.6.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S1.T1.6.5.3.1"><span class="ltx_text" id="S1.T1.6.5.3.1.1" style="font-size:70%;">Visual Appeal</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.6.5.3.2"><span class="ltx_text" id="S1.T1.6.5.3.2.1" style="font-size:70%;">11.5%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.6.5.3.3"><span class="ltx_text ltx_font_bold" id="S1.T1.6.5.3.3.1" style="font-size:70%;">31.6%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S1.T1.6.5.3.4"><span class="ltx_text" id="S1.T1.6.5.3.4.1" style="font-size:70%;">57.0%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.6.5.3.5"><span class="ltx_text" id="S1.T1.6.5.3.5.1" style="font-size:70%;">3.3%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.6.5.3.6">
<span class="ltx_text ltx_font_bold" id="S1.T1.6.5.3.6.1" style="font-size:70%;">4.2</span><span class="ltx_text" id="S1.T1.6.5.3.6.2" style="font-size:70%;">%</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S1.T1.6.5.3.7"><span class="ltx_text" id="S1.T1.6.5.3.7.1" style="font-size:70%;">92.5%</span></td>
</tr>
</tbody>
</table>
</figure><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">‚Ä†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‚Ä†</sup>* We observed the SOTA control-based model is better in identity preservation than Imagine yourself, due to its hard copy-pasting of the reference image at the center of the image, resulting in unnatural images despite the high identity metric.</span></span></span>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Text-to-Image Diffusion Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Text-to-image diffusion models represent a cutting-edge paradigm in the domain of deep learning, captivating researchers with their capacity to translate textual descriptions into vibrant visual representations. At their core, these models operate through an iterative refinement process, wherein an initial noise vector is progressively denoised based on text prompts, ultimately yielding the desired image output. A common practice is first translating images to a latent space and denoising in that space. Stable Diffusion <cite class="ltx_cite ltx_citemacro_citep">(Rombach et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib13" title="">2022</a>)</cite> and its variants SDXL <cite class="ltx_cite ltx_citemacro_citep">(Podell et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib9" title="">2023</a>)</cite>, Stable Diffusion Turbo <cite class="ltx_cite ltx_citemacro_citep">(Sauer et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib17" title="">2023</a>)</cite>, and Stable Diffusion-3 <cite class="ltx_cite ltx_citemacro_citep">(Esser et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib3" title="">2024</a>)</cite> follow this path by increasing the model size, distilling a large model to fewer denoise steps, and leveraging new transformer architectures, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Tuning-based Personalization Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Diffusion-based personalized image generation has indeed garnered increased attention in recent times. This approach involves leveraging diffusion models to generate high-quality and personalized images based on a given input or set of inputs. Technically two streams of personalization models have been proposed. One stream of research tunes a text-to-image model to incorporate the identity. Textual Inversion <cite class="ltx_cite ltx_citemacro_citep">(Gal et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib4" title="">2022</a>)</cite> finetunes a special text tokens for the new identity. DreamBooth <cite class="ltx_cite ltx_citemacro_citep">(Ruiz et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib14" title="">2023a</a>)</cite> leverages a few images from the same person as reference and a special text token to represent the identity. To accelerate the finetuning process, LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib5" title="">2021</a>)</cite> only tunes a light-weight low-rank adapter rather than the whole diffusion model. HyperDreambooth <cite class="ltx_cite ltx_citemacro_citep">(Ruiz et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib15" title="">2023b</a>)</cite> further predicts the initial weights of LoRA from the reference images. However, a major drawback of tuning-based personalization model is that the finetuned model becomes specific for the corresponding identity, and cannot be generalized to new identities. Furthermore, tuning for each user is costly and introduces a long waiting time.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Tuning-free Personalization Models</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To overcome the limitations of the tuning-based method, another line of research focuses on one generalized model without identity-specific finetuning. This direction of work extracts vision embedding from the reference image and injects it to the diffusion process. ELITE <cite class="ltx_cite ltx_citemacro_citep">(Wei et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib20" title="">2023</a>)</cite> extracts vision features from reference image and converts it to the text-embedding space through a local and a global mapping. PhotoMaker <cite class="ltx_cite ltx_citemacro_citep">(Li et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib6" title="">2023</a>)</cite> merges the vision and text tokens and replaces the original text tokens for cross-attention. PhotoVerse <cite class="ltx_cite ltx_citemacro_citep">(Chen et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib1" title="">2023</a>)</cite> incorporates an image adapter and a text adapter to merge the vision and language tokens, respectively. IP-Adapter-FaceID-Plus <cite class="ltx_cite ltx_citemacro_citep">(Ye et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib22" title="">2023</a>)</cite> leverages face embedding and clip vision encoder for identity preservation. InstantID is a control-based method that <cite class="ltx_cite ltx_citemacro_citep">(Wang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib19" title="">2024</a>)</cite> adds ControlNet <cite class="ltx_cite ltx_citemacro_citep">(Zhang et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib23" title="">2023</a>)</cite> to further control the pose and facial expression. MoA <cite class="ltx_cite ltx_citemacro_citep">(Ostashev et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib8" title="">2024</a>)</cite> proposes a mixture of attention architecture to better fuse the vision reference and the text prompts.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our proposed Imagine yourself takes a single face image of a specific subject, and generates visually appealing personalized images guided by text prompts. Our method can follow complex prompt guidance and generate images with diverse head and body poses, expressions, style, and layout.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To push the boundaries of personalized image generation, our approach begins by identifying three key facets crucial to eliciting a satisfying human visual experience: identity preservation, prompt alignment, and visual appeal (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS2" title="3.2 Overview ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">3.2</span></a>). We then introduce novel techniques tailored to enhance each of these aspects. Specifically, we propose a novel synthetic paired data generation mechanism (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS3" title="3.3 Synthetic Paired Data (SynPairs) ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">3.3</span></a>), new fully parallel architecture that incorporates three text encoders and a trainable vision encoder for optimizing identity preservation and text-alignment (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4" title="3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">3.4</span></a>), a novel coarse-to-fine multi-stage finetuning methodology designed to progressively enhance visual appeal, thereby pushing the visual appeal boundary of generated images. (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS5" title="3.5 Multi-Stage Finetune ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">3.5</span></a>). Finally, we demonstrate Imagine yourself is generalizable to multi-subjects personalization in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS6" title="3.6 Extension to Multi-Subject Personalization ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">3.6</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminary</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">Text-to-Image diffusion models gradually turn a noise <math alttext="\epsilon" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">œµ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">italic-œµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_œµ</annotation></semantics></math> to a clear image <math alttext="x_{0}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">x</mi><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ùë•</ci><cn id="S3.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>. While the diffusion process can happen in the pixel space <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib12" title="">2022</a>; Saharia et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib16" title="">2022</a>)</cite>, a common practice is to have latent diffusion models (LDM) perform diffusion process in a latent space <math alttext="\mathbf{z}=\mathcal{E}(x_{0})" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">ùê≥</mi><mo id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.3.cmml">‚Ñ∞</mi><mo id="S3.SS1.p1.3.m3.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p1.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml">x</mi><mn id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.SS1.p1.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"></eq><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">ùê≥</ci><apply id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.2"></times><ci id="S3.SS1.p1.3.m3.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3">‚Ñ∞</ci><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2">ùë•</ci><cn id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3">0</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathbf{z}=\mathcal{E}(x_{0})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">bold_z = caligraphic_E ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )</annotation></semantics></math>. During training, the LDM models optimize the reconstruction loss in the latent space:</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\mathrm{diffusion}}=\mathbb{E}_{z\sim\mathcal{E}(x_{0}),\epsilon%
\sim\mathbf{N}(0,1)}\|\epsilon-\epsilon_{\theta}(\mathbf{z}_{t}),t\|_{2}^{2}" class="ltx_math_unparsed" display="block" id="S3.E1.m1.5"><semantics id="S3.E1.m1.5a"><mrow id="S3.E1.m1.5b"><msub id="S3.E1.m1.5.6"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.6.2">‚Ñí</mi><mi id="S3.E1.m1.5.6.3">diffusion</mi></msub><mo id="S3.E1.m1.5.7">=</mo><msub id="S3.E1.m1.5.8"><mi id="S3.E1.m1.5.8.2">ùîº</mi><mrow id="S3.E1.m1.4.4.4.4"><mrow id="S3.E1.m1.3.3.3.3.1"><mi id="S3.E1.m1.3.3.3.3.1.3">z</mi><mo id="S3.E1.m1.3.3.3.3.1.2">‚àº</mo><mrow id="S3.E1.m1.3.3.3.3.1.1"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.3.3.1.1.3">‚Ñ∞</mi><mo id="S3.E1.m1.3.3.3.3.1.1.2">‚Å¢</mo><mrow id="S3.E1.m1.3.3.3.3.1.1.1.1"><mo id="S3.E1.m1.3.3.3.3.1.1.1.1.2" stretchy="false">(</mo><msub id="S3.E1.m1.3.3.3.3.1.1.1.1.1"><mi id="S3.E1.m1.3.3.3.3.1.1.1.1.1.2">x</mi><mn id="S3.E1.m1.3.3.3.3.1.1.1.1.1.3">0</mn></msub><mo id="S3.E1.m1.3.3.3.3.1.1.1.1.3" stretchy="false">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.4.4.4.4.3">,</mo><mrow id="S3.E1.m1.4.4.4.4.2"><mi id="S3.E1.m1.4.4.4.4.2.2">œµ</mi><mo id="S3.E1.m1.4.4.4.4.2.1">‚àº</mo><mrow id="S3.E1.m1.4.4.4.4.2.3"><mi id="S3.E1.m1.4.4.4.4.2.3.2">ùêç</mi><mo id="S3.E1.m1.4.4.4.4.2.3.1">‚Å¢</mo><mrow id="S3.E1.m1.4.4.4.4.2.3.3.2"><mo id="S3.E1.m1.4.4.4.4.2.3.3.2.1" stretchy="false">(</mo><mn id="S3.E1.m1.1.1.1.1">0</mn><mo id="S3.E1.m1.4.4.4.4.2.3.3.2.2">,</mo><mn id="S3.E1.m1.2.2.2.2">1</mn><mo id="S3.E1.m1.4.4.4.4.2.3.3.2.3" stretchy="false">)</mo></mrow></mrow></mrow></mrow></msub><mo id="S3.E1.m1.5.9" lspace="0em" rspace="0.167em">‚à•</mo><mi id="S3.E1.m1.5.10">œµ</mi><mo id="S3.E1.m1.5.11">‚àí</mo><msub id="S3.E1.m1.5.12"><mi id="S3.E1.m1.5.12.2">œµ</mi><mi id="S3.E1.m1.5.12.3">Œ∏</mi></msub><mrow id="S3.E1.m1.5.13"><mo id="S3.E1.m1.5.13.1" stretchy="false">(</mo><msub id="S3.E1.m1.5.13.2"><mi id="S3.E1.m1.5.13.2.2">ùê≥</mi><mi id="S3.E1.m1.5.13.2.3">t</mi></msub><mo id="S3.E1.m1.5.13.3" stretchy="false">)</mo></mrow><mo id="S3.E1.m1.5.14">,</mo><mi id="S3.E1.m1.5.5">t</mi><msubsup id="S3.E1.m1.5.15"><mo id="S3.E1.m1.5.15.2.2" lspace="0em">‚à•</mo><mn id="S3.E1.m1.5.15.2.3">2</mn><mn id="S3.E1.m1.5.15.3">2</mn></msubsup></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.5c">\mathcal{L}_{\mathrm{diffusion}}=\mathbb{E}_{z\sim\mathcal{E}(x_{0}),\epsilon%
\sim\mathbf{N}(0,1)}\|\epsilon-\epsilon_{\theta}(\mathbf{z}_{t}),t\|_{2}^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.5d">caligraphic_L start_POSTSUBSCRIPT roman_diffusion end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_z ‚àº caligraphic_E ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , italic_œµ ‚àº bold_N ( 0 , 1 ) end_POSTSUBSCRIPT ‚à• italic_œµ - italic_œµ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , italic_t ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.4">where <math alttext="\mathcal{L}_{\mathrm{diffusion}}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">‚Ñí</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">diffusion</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">‚Ñí</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">diffusion</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathcal{L}_{\mathrm{diffusion}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT roman_diffusion end_POSTSUBSCRIPT</annotation></semantics></math> is the diffusion loss. <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">œµ</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">Œ∏</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">italic-œµ</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">ùúÉ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_œµ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT</annotation></semantics></math> represents the diffusion model. <math alttext="\mathbf{z}_{t}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">ùê≥</mi><mi id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">ùê≥</ci><ci id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\mathbf{z}_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> is the noised input to the model at timestep <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">ùë°</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_t</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">It is a common practice to use text or other condition signals <math alttext="\mathbf{C}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.m1.1"><semantics id="S3.SS1.p4.1.m1.1a"><mi id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml">ùêÇ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><ci id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">ùêÇ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">\mathbf{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.m1.1d">bold_C</annotation></semantics></math> to guide the diffusion process. Thus, the conditioned diffusion process generates images following the condition signals. Usually, the text condition is incorporated with the diffusion model through cross-attention mechanism:</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{Attn}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{softmax}\left(\frac{%
\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d}}\right)\mathbf{V}" class="ltx_Math" display="block" id="S3.E2.m1.4"><semantics id="S3.E2.m1.4a"><mrow id="S3.E2.m1.4.5" xref="S3.E2.m1.4.5.cmml"><mrow id="S3.E2.m1.4.5.2" xref="S3.E2.m1.4.5.2.cmml"><mi id="S3.E2.m1.4.5.2.2" xref="S3.E2.m1.4.5.2.2.cmml">Attn</mi><mo id="S3.E2.m1.4.5.2.1" xref="S3.E2.m1.4.5.2.1.cmml">‚Å¢</mo><mrow id="S3.E2.m1.4.5.2.3.2" xref="S3.E2.m1.4.5.2.3.1.cmml"><mo id="S3.E2.m1.4.5.2.3.2.1" stretchy="false" xref="S3.E2.m1.4.5.2.3.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">ùêê</mi><mo id="S3.E2.m1.4.5.2.3.2.2" xref="S3.E2.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">ùêä</mi><mo id="S3.E2.m1.4.5.2.3.2.3" xref="S3.E2.m1.4.5.2.3.1.cmml">,</mo><mi id="S3.E2.m1.3.3" xref="S3.E2.m1.3.3.cmml">ùêï</mi><mo id="S3.E2.m1.4.5.2.3.2.4" stretchy="false" xref="S3.E2.m1.4.5.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.4.5.1" xref="S3.E2.m1.4.5.1.cmml">=</mo><mrow id="S3.E2.m1.4.5.3" xref="S3.E2.m1.4.5.3.cmml"><mi id="S3.E2.m1.4.5.3.2" xref="S3.E2.m1.4.5.3.2.cmml">softmax</mi><mo id="S3.E2.m1.4.5.3.1" xref="S3.E2.m1.4.5.3.1.cmml">‚Å¢</mo><mrow id="S3.E2.m1.4.5.3.3.2" xref="S3.E2.m1.4.4.cmml"><mo id="S3.E2.m1.4.5.3.3.2.1" xref="S3.E2.m1.4.4.cmml">(</mo><mfrac id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml"><msup id="S3.E2.m1.4.4.2" xref="S3.E2.m1.4.4.2.cmml"><mi id="S3.E2.m1.4.4.2.2" xref="S3.E2.m1.4.4.2.2.cmml">ùêêùêä</mi><mi id="S3.E2.m1.4.4.2.3" xref="S3.E2.m1.4.4.2.3.cmml">T</mi></msup><msqrt id="S3.E2.m1.4.4.3" xref="S3.E2.m1.4.4.3.cmml"><mi id="S3.E2.m1.4.4.3.2" xref="S3.E2.m1.4.4.3.2.cmml">d</mi></msqrt></mfrac><mo id="S3.E2.m1.4.5.3.3.2.2" xref="S3.E2.m1.4.4.cmml">)</mo></mrow><mo id="S3.E2.m1.4.5.3.1a" xref="S3.E2.m1.4.5.3.1.cmml">‚Å¢</mo><mi id="S3.E2.m1.4.5.3.4" xref="S3.E2.m1.4.5.3.4.cmml">ùêï</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.4b"><apply id="S3.E2.m1.4.5.cmml" xref="S3.E2.m1.4.5"><eq id="S3.E2.m1.4.5.1.cmml" xref="S3.E2.m1.4.5.1"></eq><apply id="S3.E2.m1.4.5.2.cmml" xref="S3.E2.m1.4.5.2"><times id="S3.E2.m1.4.5.2.1.cmml" xref="S3.E2.m1.4.5.2.1"></times><ci id="S3.E2.m1.4.5.2.2.cmml" xref="S3.E2.m1.4.5.2.2">Attn</ci><vector id="S3.E2.m1.4.5.2.3.1.cmml" xref="S3.E2.m1.4.5.2.3.2"><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">ùêê</ci><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">ùêä</ci><ci id="S3.E2.m1.3.3.cmml" xref="S3.E2.m1.3.3">ùêï</ci></vector></apply><apply id="S3.E2.m1.4.5.3.cmml" xref="S3.E2.m1.4.5.3"><times id="S3.E2.m1.4.5.3.1.cmml" xref="S3.E2.m1.4.5.3.1"></times><ci id="S3.E2.m1.4.5.3.2.cmml" xref="S3.E2.m1.4.5.3.2">softmax</ci><apply id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.5.3.3.2"><divide id="S3.E2.m1.4.4.1.cmml" xref="S3.E2.m1.4.5.3.3.2"></divide><apply id="S3.E2.m1.4.4.2.cmml" xref="S3.E2.m1.4.4.2"><csymbol cd="ambiguous" id="S3.E2.m1.4.4.2.1.cmml" xref="S3.E2.m1.4.4.2">superscript</csymbol><ci id="S3.E2.m1.4.4.2.2.cmml" xref="S3.E2.m1.4.4.2.2">ùêêùêä</ci><ci id="S3.E2.m1.4.4.2.3.cmml" xref="S3.E2.m1.4.4.2.3">ùëá</ci></apply><apply id="S3.E2.m1.4.4.3.cmml" xref="S3.E2.m1.4.4.3"><root id="S3.E2.m1.4.4.3a.cmml" xref="S3.E2.m1.4.4.3"></root><ci id="S3.E2.m1.4.4.3.2.cmml" xref="S3.E2.m1.4.4.3.2">ùëë</ci></apply></apply><ci id="S3.E2.m1.4.5.3.4.cmml" xref="S3.E2.m1.4.5.3.4">ùêï</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.4c">\mathrm{Attn}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{softmax}\left(\frac{%
\mathbf{Q}\mathbf{K}^{T}}{\sqrt{d}}\right)\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.4d">roman_Attn ( bold_Q , bold_K , bold_V ) = roman_softmax ( divide start_ARG bold_QK start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d end_ARG end_ARG ) bold_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.4">where <math alttext="\mathbf{K}=\mathbf{W}_{K}C" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">ùêä</mi><mo id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml"><msub id="S3.SS1.p6.1.m1.1.1.3.2" xref="S3.SS1.p6.1.m1.1.1.3.2.cmml"><mi id="S3.SS1.p6.1.m1.1.1.3.2.2" xref="S3.SS1.p6.1.m1.1.1.3.2.2.cmml">ùêñ</mi><mi id="S3.SS1.p6.1.m1.1.1.3.2.3" xref="S3.SS1.p6.1.m1.1.1.3.2.3.cmml">K</mi></msub><mo id="S3.SS1.p6.1.m1.1.1.3.1" xref="S3.SS1.p6.1.m1.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS1.p6.1.m1.1.1.3.3" xref="S3.SS1.p6.1.m1.1.1.3.3.cmml">C</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><eq id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1"></eq><ci id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2">ùêä</ci><apply id="S3.SS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3"><times id="S3.SS1.p6.1.m1.1.1.3.1.cmml" xref="S3.SS1.p6.1.m1.1.1.3.1"></times><apply id="S3.SS1.p6.1.m1.1.1.3.2.cmml" xref="S3.SS1.p6.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p6.1.m1.1.1.3.2.1.cmml" xref="S3.SS1.p6.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p6.1.m1.1.1.3.2.2.cmml" xref="S3.SS1.p6.1.m1.1.1.3.2.2">ùêñ</ci><ci id="S3.SS1.p6.1.m1.1.1.3.2.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3.2.3">ùêæ</ci></apply><ci id="S3.SS1.p6.1.m1.1.1.3.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3.3">ùê∂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">\mathbf{K}=\mathbf{W}_{K}C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">bold_K = bold_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT italic_C</annotation></semantics></math>, <math alttext="\mathbf{V}=\mathbf{W}_{V}C" class="ltx_Math" display="inline" id="S3.SS1.p6.2.m2.1"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mi id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">ùêï</mi><mo id="S3.SS1.p6.2.m2.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS1.p6.2.m2.1.1.3" xref="S3.SS1.p6.2.m2.1.1.3.cmml"><msub id="S3.SS1.p6.2.m2.1.1.3.2" xref="S3.SS1.p6.2.m2.1.1.3.2.cmml"><mi id="S3.SS1.p6.2.m2.1.1.3.2.2" xref="S3.SS1.p6.2.m2.1.1.3.2.2.cmml">ùêñ</mi><mi id="S3.SS1.p6.2.m2.1.1.3.2.3" xref="S3.SS1.p6.2.m2.1.1.3.2.3.cmml">V</mi></msub><mo id="S3.SS1.p6.2.m2.1.1.3.1" xref="S3.SS1.p6.2.m2.1.1.3.1.cmml">‚Å¢</mo><mi id="S3.SS1.p6.2.m2.1.1.3.3" xref="S3.SS1.p6.2.m2.1.1.3.3.cmml">C</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><eq id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1"></eq><ci id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2">ùêï</ci><apply id="S3.SS1.p6.2.m2.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3"><times id="S3.SS1.p6.2.m2.1.1.3.1.cmml" xref="S3.SS1.p6.2.m2.1.1.3.1"></times><apply id="S3.SS1.p6.2.m2.1.1.3.2.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.p6.2.m2.1.1.3.2.1.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS1.p6.2.m2.1.1.3.2.2.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2.2">ùêñ</ci><ci id="S3.SS1.p6.2.m2.1.1.3.2.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3.2.3">ùëâ</ci></apply><ci id="S3.SS1.p6.2.m2.1.1.3.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3.3">ùê∂</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">\mathbf{V}=\mathbf{W}_{V}C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.2.m2.1d">bold_V = bold_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT italic_C</annotation></semantics></math> represents transforms that map the condition <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.p6.3.m3.1"><semantics id="S3.SS1.p6.3.m3.1a"><mi id="S3.SS1.p6.3.m3.1.1" xref="S3.SS1.p6.3.m3.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.3.m3.1b"><ci id="S3.SS1.p6.3.m3.1.1.cmml" xref="S3.SS1.p6.3.m3.1.1">ùê∂</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.3.m3.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.3.m3.1d">italic_C</annotation></semantics></math> to the cross-attention key and values. <math alttext="\mathbf{Q}=\mathbf{W}_{Q}\phi(x_{t})" class="ltx_Math" display="inline" id="S3.SS1.p6.4.m4.1"><semantics id="S3.SS1.p6.4.m4.1a"><mrow id="S3.SS1.p6.4.m4.1.1" xref="S3.SS1.p6.4.m4.1.1.cmml"><mi id="S3.SS1.p6.4.m4.1.1.3" xref="S3.SS1.p6.4.m4.1.1.3.cmml">ùêê</mi><mo id="S3.SS1.p6.4.m4.1.1.2" xref="S3.SS1.p6.4.m4.1.1.2.cmml">=</mo><mrow id="S3.SS1.p6.4.m4.1.1.1" xref="S3.SS1.p6.4.m4.1.1.1.cmml"><msub id="S3.SS1.p6.4.m4.1.1.1.3" xref="S3.SS1.p6.4.m4.1.1.1.3.cmml"><mi id="S3.SS1.p6.4.m4.1.1.1.3.2" xref="S3.SS1.p6.4.m4.1.1.1.3.2.cmml">ùêñ</mi><mi id="S3.SS1.p6.4.m4.1.1.1.3.3" xref="S3.SS1.p6.4.m4.1.1.1.3.3.cmml">Q</mi></msub><mo id="S3.SS1.p6.4.m4.1.1.1.2" xref="S3.SS1.p6.4.m4.1.1.1.2.cmml">‚Å¢</mo><mi id="S3.SS1.p6.4.m4.1.1.1.4" xref="S3.SS1.p6.4.m4.1.1.1.4.cmml">œï</mi><mo id="S3.SS1.p6.4.m4.1.1.1.2a" xref="S3.SS1.p6.4.m4.1.1.1.2.cmml">‚Å¢</mo><mrow id="S3.SS1.p6.4.m4.1.1.1.1.1" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.cmml"><mo id="S3.SS1.p6.4.m4.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS1.p6.4.m4.1.1.1.1.1.1" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p6.4.m4.1.1.1.1.1.1.2" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.SS1.p6.4.m4.1.1.1.1.1.1.3" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS1.p6.4.m4.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.4.m4.1b"><apply id="S3.SS1.p6.4.m4.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1"><eq id="S3.SS1.p6.4.m4.1.1.2.cmml" xref="S3.SS1.p6.4.m4.1.1.2"></eq><ci id="S3.SS1.p6.4.m4.1.1.3.cmml" xref="S3.SS1.p6.4.m4.1.1.3">ùêê</ci><apply id="S3.SS1.p6.4.m4.1.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1.1"><times id="S3.SS1.p6.4.m4.1.1.1.2.cmml" xref="S3.SS1.p6.4.m4.1.1.1.2"></times><apply id="S3.SS1.p6.4.m4.1.1.1.3.cmml" xref="S3.SS1.p6.4.m4.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p6.4.m4.1.1.1.3.1.cmml" xref="S3.SS1.p6.4.m4.1.1.1.3">subscript</csymbol><ci id="S3.SS1.p6.4.m4.1.1.1.3.2.cmml" xref="S3.SS1.p6.4.m4.1.1.1.3.2">ùêñ</ci><ci id="S3.SS1.p6.4.m4.1.1.1.3.3.cmml" xref="S3.SS1.p6.4.m4.1.1.1.3.3">ùëÑ</ci></apply><ci id="S3.SS1.p6.4.m4.1.1.1.4.cmml" xref="S3.SS1.p6.4.m4.1.1.1.4">italic-œï</ci><apply id="S3.SS1.p6.4.m4.1.1.1.1.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p6.4.m4.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p6.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.2">ùë•</ci><ci id="S3.SS1.p6.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p6.4.m4.1.1.1.1.1.1.3">ùë°</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.4.m4.1c">\mathbf{Q}=\mathbf{W}_{Q}\phi(x_{t})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.4.m4.1d">bold_Q = bold_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT italic_œï ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )</annotation></semantics></math> represents the hidden state of the diffusion model.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Overview</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.F2" title="Figure 2 ‚Ä£ 3.2 Overview ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">2</span></a> provides an illustration of the proposed model architecture. The key of using diffusion models for personalized image generation is incorporating the reference identity as an additional control signal to the diffusion model. We propose to extract the identity information from the reference image through a trainable clip patch encoder. The identity vision signal is then added to the text signals through a parallel cross attention module. To better preserve the high visual quality of the foundation model, we leveraged low-rank adapters (LoRA) to freeze the self-attention and text cross-attention modules while only fine-tuning the adapters.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="488" id="S3.F2.g1" src="x1.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Overview of Imagine yourself model architecture. We introduced a fully parallel architecture
that incorporates three text encoders and a trainable vision encoder for optimizing identity preservation and text-alignment. We adopted LoRA on top of the self-attention layers and the text cross-attention layers to best preserve the foundation model‚Äôs image generation quality.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic Paired Data (<span class="ltx_text ltx_font_typewriter" id="S3.SS3.1.1">SynPairs</span>)</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We observed that one critical issue during training is the use of unpaired data, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.1">i.e</em>.<span class="ltx_text" id="S3.SS3.p1.1.2"></span>, the cropped image as input and the original image as target. It can introduce a severe copy-paste effect, making the model hard to learn the true identity relationship between input and output more than duplicating the reference image. Thus, the model is not able to generate images that follow hard prompts, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.3">e.g</em>.<span class="ltx_text" id="S3.SS3.p1.1.4"></span>, change expression or head orientation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To this end, we proposed a new synthetic data generation recipe to create high-quality paired data (same identity with varying expression, pose, and lighting conditions, <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.1">etc</em>.) for training. Compared to directly sourcing real paired data, which is not readily available, our study shows that curating the paired data synthetically allows us to retain higher quality data to further enhancing several aspects of the Imagine yourself model.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="492" id="S3.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.6.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.7.2" style="font-size:90%;">Generation pipeline for <span class="ltx_text ltx_font_typewriter" id="S3.F3.7.2.1">SynPairs</span> data. We first caption real images using multi-modal LLM and rewrite through a LLM rewriter. The prompt is fed into a text-to-image generation model to obtain high-quality synthetic images, and then refined with the reference image to better preserve identity. This results in high-quality paired data, <em class="ltx_emph ltx_font_italic" id="S3.F3.7.2.2">i.e</em>.<span class="ltx_text" id="S3.F3.7.2.3"></span>, same identity with varying expression, pose, and lighting conditions, <em class="ltx_emph ltx_font_italic" id="S3.F3.7.2.4">etc</em>.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To generate <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">SynPairs</span> data, we first obtain a dense image caption of the real reference image via a multi-modal LLM. The caption then flows through a caption rewrite stage based on Llama3 <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib7" title="">Meta AI, </a>)</cite> to inject more gaze and pose diversity in the caption. The rewritten caption is then fed to a text-to-image generation tool such as Emu <cite class="ltx_cite ltx_citemacro_citep">(Dai et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib2" title="">2023</a>)</cite> as the prompt to produce a high quality synthetic images. Next, we refine the generated image identity based on the reference image identity. After a large number of curated synthetic pairs is generated they go through an automatic filter based on similarity.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Model Architecture</h3>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Vision Encoder</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">We propose to use a trainable CLIP ViT-H patch vision encoder to extract the identity control signal from the reference image.
Unlike previous work that heavily relied on face embedding, we observed that a general trainable vision encoder can provide adequate information to preserve the identity.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">To further improve the identity preservation capability, we crop the face area and mask the corresponding background of the reference image to avoid the model attending to the non-critical areas, <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS1.p2.1.1">e.g</em>.<span class="ltx_text" id="S3.SS4.SSS1.p2.1.2"></span>, image background and non-face area in the cropped image. Figure 2 illustrates the vision embedding workflow. We also proposed to use <span class="ltx_text ltx_font_typewriter" id="S3.SS4.SSS1.p2.1.3">zero_conv</span> as initialization to avoid adding noisy control signals at the beginning of training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Text Encoders</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">We employ three distinct text-encoders: CLIP ViT-L <cite class="ltx_cite ltx_citemacro_citep">(Radford et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib10" title="">2021</a>)</cite> text encoder, UL2 <cite class="ltx_cite ltx_citemacro_citep">(Tay et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib18" title="">2022</a>)</cite>, and ByT5 <cite class="ltx_cite ltx_citemacro_citep">(Xue et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib21" title="">2022</a>)</cite>, as the text conditioning mechanisms. The selection of these encoders is driven by their respective strengths and suitability for specific tasks. The CLIP text encoder, for instance, shares a common space with the CLIP vision encoder, facilitating enhanced identity preservation. To capitalize on this alignment, we initialize the cross-attention module of the vision encoder with the pre-trained CLIP text encoder. Meanwhile, UL2 is specifically chosen for its proficiency in comprehending long and intricate text prompts, making it instrumental in handling complex input data. Furthermore, the ByT5 model is integrated for its supreme capability in encoding characters. We leverage ByT5 to improve visual text generating in the image, <em class="ltx_emph ltx_font_italic" id="S3.SS4.SSS2.p1.1.1">e.g</em>.<span class="ltx_text" id="S3.SS4.SSS2.p1.1.2"></span>, text on a signage.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Fully Parallel Image-Text Fusion</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">We investigated a parallel attention architecture to incorporate the vision and text conditions. Specifically, the newly added vision condition from the reference image and the spatial features fuse through a new vision cross-attention module. The output of the new vision cross-attention module is then added to the text cross-attention output. In our experiments, this design better balances the vision and text control than concatenating the text and vision controls.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="514" id="S3.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Fully parallel image-text fusion architecture. We employ three distinct text-encoders: CLIP ViT-L <cite class="ltx_cite ltx_citemacro_citep">(Radford et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib10" title="">2021</a>)</cite> text encoder, UL2 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib11" title="">2020</a>)</cite>, and ByT5 <cite class="ltx_cite ltx_citemacro_citep">(Xue et¬†al., <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#bib.bib21" title="">2022</a>)</cite>, as the text conditioning. They interact with a trainable CLIP vision encoder through fully parallel attention fusion.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>LoRA</h4>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p" id="S3.SS4.SSS4.p1.1">To preserve the visual quality from the foundation model, we leveraged low-rank adapters (LoRA) on top of the cross-attention module. The self-attention and text cross-attention modules in the foundation Unet are frozen. We observed that this design not only better preserves the foundation model‚Äôs image generation capability, but also accelerates the convergence speed by up to 5x.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Multi-Stage Finetune</h3>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="196" id="S3.F5.g1" src="extracted/5866712/figures/kosher_figure/Figure5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Training with real images has higher identity, training with synthetic images has higher prompt alignment. After an interleaved multi-staged training, identity and prompt alignment achieves best trade-off.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">We propose a multi-stage finetuning with interleaved real and synthetic data that help us achieve the best trade-off between editability and identity preservation. In the first two stages, we leverage large-scale data (nine millions) to pretrain the model to be able to condition on a reference identity. For the later stages, we finetune our pretrained checkpoint with high-quality, aesthetic images collected through Human-In-The-Loop (HITL). Empirically, we found training with real images gives the best identity preservation, while training with synthetic images gives better prompt alignment (editability). Synthetic images are generated from its respective prompt, therefore the image-text alignment is high and there is less noisy information during training, but the identity information is not as rich as in real data. This is why we adopt the interleaved training recipe, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.F5" title="Figure 5 ‚Ä£ 3.5 Multi-Stage Finetune ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">5</span></a>. After the 1st real data pretraining, the model is able to condition on image, after 2nd synthetic data pretraining, the prompt alignment is high but identity is not perfect, after the 3rd high-quality real-data finetune, the identity is good but prompt alignment drops, the 4th high-quality synthetic data finetune achieves the best trade-off between identity and editability.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Extension to Multi-Subject Personalization</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.6">The previously introduced fully parallel image-text fusion pipeline (Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.SS4.SSS3" title="3.4.3 Fully Parallel Image-Text Fusion ‚Ä£ 3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">3.4.3</span></a>) can be flexibly extended to accommodate multi-subject personalization. In the two-person scenario for example, instead of passing the global embedding and patch embedding of the single reference image into the <math alttext="\mathbf{K}" class="ltx_Math" display="inline" id="S3.SS6.p1.1.m1.1"><semantics id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1" xref="S3.SS6.p1.1.m1.1.1.cmml">ùêä</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">ùêä</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">\mathbf{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.1.m1.1d">bold_K</annotation></semantics></math> and <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S3.SS6.p1.2.m2.1"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1" xref="S3.SS6.p1.2.m2.1.1.cmml">ùêï</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">ùêï</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.2.m2.1d">bold_V</annotation></semantics></math> components as shown in the top left branch of Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S3.F4" title="Figure 4 ‚Ä£ 3.4.3 Fully Parallel Image-Text Fusion ‚Ä£ 3.4 Model Architecture ‚Ä£ 3 Method ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">4</span></a>, we can concatenate the vision embedding from both reference images and passing it into the <math alttext="\mathbf{K}" class="ltx_Math" display="inline" id="S3.SS6.p1.3.m3.1"><semantics id="S3.SS6.p1.3.m3.1a"><mi id="S3.SS6.p1.3.m3.1.1" xref="S3.SS6.p1.3.m3.1.1.cmml">ùêä</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m3.1b"><ci id="S3.SS6.p1.3.m3.1.1.cmml" xref="S3.SS6.p1.3.m3.1.1">ùêä</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m3.1c">\mathbf{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.3.m3.1d">bold_K</annotation></semantics></math> and <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S3.SS6.p1.4.m4.1"><semantics id="S3.SS6.p1.4.m4.1a"><mi id="S3.SS6.p1.4.m4.1.1" xref="S3.SS6.p1.4.m4.1.1.cmml">ùêï</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m4.1b"><ci id="S3.SS6.p1.4.m4.1.1.cmml" xref="S3.SS6.p1.4.m4.1.1">ùêï</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m4.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.4.m4.1d">bold_V</annotation></semantics></math> components. Given this setup, through training, the network learns how to map from reference<sub class="ltx_sub" id="S3.SS6.p1.6.1"><span class="ltx_text ltx_font_italic" id="S3.SS6.p1.6.1.1">i</span></sub> to subject<sub class="ltx_sub" id="S3.SS6.p1.6.2"><span class="ltx_text ltx_font_italic" id="S3.SS6.p1.6.2.1">j</span></sub> in the group photo while generating prompt-induced image context accordingly. Some examples of the two-person personalization results are shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.F11" title="Figure 11 ‚Ä£ 4.1 Qualitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we perform both qualitative and quantitative evaluations of our model. We also compare our model to the SOTA personalization models. Results show that our model outperforms the existing models on all axes setting the new state-of-the-art.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Qualitative Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We show examples of our model generated image in Figures <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.F6" title="Figure 6 ‚Ä£ 4.1 Qualitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">6</span></a>-<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.F10" title="Figure 10 ‚Ä£ 4.1 Qualitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">10</span></a>. Our model generates visually appealing images that both preserve the identity and follow the prompt faithfully.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="617" id="S4.F6.g1" src="extracted/5866712/figures/shine_1_clean.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">More visualizations (1/5) of generated personalized images using Imagine yourself.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="625" id="S4.F7.g1" src="extracted/5866712/figures/shine_2_clean.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">More visualizations (2/5) of generated personalized images using Imagine yourself.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="630" id="S4.F8.g1" src="extracted/5866712/figures/shine_3_clean.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">More visualizations (3/5) of generated personalized images using Imagine yourself.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="616" id="S4.F9.g1" src="extracted/5866712/figures/shine_4_clean.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.3.2" style="font-size:90%;">More visualizations (4/5) of generated personalized images using Imagine yourself.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="609" id="S4.F10.g1" src="extracted/5866712/figures/shine_5_clean.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.3.2" style="font-size:90%;">More visualizations (5/5) of generated personalized images using Imagine yourself.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1006" id="S4.F11.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S4.F11.3.2" style="font-size:90%;">Multi-player personalization using Imagine yourself.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Quantitative Evaluation</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Evaluation Dataset</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">To quantitatively evaluate Imagine yourself, we created an evaluation set consisting of two parts: (i) reference images, and (ii) eval prompts. To have a comprehensive comparison in all representative cases, we collected a total of 51 reference identities covering different gender, race, and skin tone. We created a list of 65 prompts to evaluate the model. It widely covers a wide range of usage scenarios, and also including hard prompts that require face expression or pose changes, camera motions, and stylization. These prompts help to assess the model‚Äôs ability to engage in more complex and nuanced interactions, diverse pose generation, and harmonization. Each identity is paired with all 65 prompts, so a total of 51x65=3315 generations for one round of human evaluation. The distributions of the prompts is shown in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.F12" title="Figure 12 ‚Ä£ 4.2.1 Evaluation Dataset ‚Ä£ 4.2 Quantitative Evaluation ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S4.F12.g1" src="x5.png" width="705"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S4.F12.3.2" style="font-size:90%;">Distribution of the evaluated prompts. It widely covers a wide range of usage scenarios, and also including hard prompts that require face expression or pose changes, camera motions, etc.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Benchmarked Methods</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We benchmarked the SOTA adapter-based personalization model and the SOTA control-based model. For the adapter-based method, we select the best one that strikes the best balance among visual appeal, identity preservation, and prompt alignment, the three axes that we evaluate our models on. For control-based method, we noticed that the choice of the pose image plays an important role in how the final generated image is composed, <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS2.p1.1.1">i.e</em>.<span class="ltx_text" id="S4.SS2.SSS2.p1.1.2"></span>, for some prompts, a carefully chosen pose image can make the generated images look better or worse. For a fair comparison, we use the reference image itself as the pose condition.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Human Evaluation</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">To evaluate the quality of the generated images, we conducted a large-scale annotation process that assessed various aspects of the images. We used human annotation as the gold standard to assess the model‚Äôs performance (standalone evaluation) and compare it with other models (head-to-head evaluation).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">In the standalone evaluation, we presented annotators with the input image, prompt, and a generated image and asked them three questions to rate on a scale of Strong Pass / Weak Pass / Fail. (1) <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.1.1">Identity Similarity</span>: Does the subject in the output image appear to have the same identity as the subject in the original image? (2) <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.1.2">Prompt Alignment</span>: Does the output image follow the personalization prompt faithfully? (3) <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS3.p2.1.3">Visual Appeal</span>: Is the output image visually appealing?
In the head-to-head model evaluation, we compared one model against another on the same three axes.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">As shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S1.T1" title="Table 1 ‚Ä£ 1 Introduction ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">1</span></a>, Imagine yourself outperforms the two state-of-the-art methods adapter-based model and control-based model by a significant margin in most axes. Specifically, Imagine yourself is significantly better in prompt alignment, with a +45.1% and +30.8% improvement over the SOTA adapter-based model and the SOTA control-based model, respectively. However, we observed that the control-based model is better in identity preservation than Imagine yourself, due to its hard copy-pasting of the reference image at the center of the image, resulting in unnatural images despite the high identity metric.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In our ablation study, we examined the effectiveness of various components within our proposed Imagine yourself. Main ablation results are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13346v1#S4.T2" title="Table 2 ‚Ä£ 4.3 Ablation Study ‚Ä£ 4 Experiments ‚Ä£ Imagine yourself: Tuning-Free Personalized Image Generation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.5.1.1" style="font-size:129%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.6.2" style="font-size:129%;">Ablation study of different component in Imagine yourself.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1">
<td class="ltx_td ltx_border_r ltx_border_tt" id="S4.T2.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S4.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1" style="font-size:70%;">Standalone Pass Rate (<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math>)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.1.1" style="font-size:70%;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.2"><span class="ltx_text" id="S4.T2.1.2.1.2.1" style="font-size:70%;">Prompt Alignment</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.2.1.3"><span class="ltx_text" id="S4.T2.1.2.1.3.1" style="font-size:70%;">Identity Preservation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4"><span class="ltx_text" id="S4.T2.1.2.1.4.1" style="font-size:70%;">Visual Appeal</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.1"><span class="ltx_text" id="S4.T2.1.3.2.1.1" style="font-size:70%;">Imagine yourself w/o multi-stage finetune</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.2"><span class="ltx_text" id="S4.T2.1.3.2.2.1" style="font-size:70%;">55.3%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.3.2.3"><span class="ltx_text" id="S4.T2.1.3.2.3.1" style="font-size:70%;">90.7%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.3.2.4"><span class="ltx_text" id="S4.T2.1.3.2.4.1" style="font-size:70%;">45.7%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3.1"><span class="ltx_text" id="S4.T2.1.4.3.1.1" style="font-size:70%;">Imagine yourself w/o fully parallel attention</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3.2"><span class="ltx_text" id="S4.T2.1.4.3.2.1" style="font-size:70%;">75.6%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.4.3.3"><span class="ltx_text" id="S4.T2.1.4.3.3.1" style="font-size:70%;">81.9%</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.4.3.4"><span class="ltx_text" id="S4.T2.1.4.3.4.1" style="font-size:70%;">65.7%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4.1">
<span class="ltx_text" id="S4.T2.1.5.4.1.1" style="font-size:70%;">Imagine yourself w/o </span><span class="ltx_text ltx_font_typewriter" id="S4.T2.1.5.4.1.2" style="font-size:70%;">SynPairs</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4.2"><span class="ltx_text" id="S4.T2.1.5.4.2.1" style="font-size:70%;">57.0%</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.5.4.3">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.3.1" style="font-size:70%;">95.5</span><span class="ltx_text" id="S4.T2.1.5.4.3.2" style="font-size:70%;">%</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.5.4.4"><span class="ltx_text" id="S4.T2.1.5.4.4.1" style="font-size:70%;">83.4%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.6.5.1"><span class="ltx_text" id="S4.T2.1.6.5.1.1" style="font-size:70%;">Imagine yourself</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.6.5.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.2.1" style="font-size:70%;">80.8</span><span class="ltx_text" id="S4.T2.1.6.5.2.2" style="font-size:70%;"> %</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.1.6.5.3"><span class="ltx_text" id="S4.T2.1.6.5.3.1" style="font-size:70%;">83.3%</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.6.5.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.4.1" style="font-size:70%;">87.7</span><span class="ltx_text" id="S4.T2.1.6.5.4.2" style="font-size:70%;">%</span>
</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Impact of Multi-stage Finetune</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">The ablation results highlight the impact of multi-stage fine-tuning. Reducing the multi-stage fine-tuning to a single stage significantly degrade all metrics, especially 25.5% in prompt alignment and 42.0% in visual appeal. Moreover, we observe that, the synthetic fine-tune stages provides better prompt alignment and the real data fine-tune stage improves the identity preservation capability.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Impact of Fully Parallel Attention</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">We ablate removing the full parallel attention to a standard token concatenate design to show the impact of the fully parallel attention architecture. We observed that all metrics, specifically 5.2% in prompt alignment, 1.4% in identity preservation, and 22.0% in visual appeal, respectively. This shows the importance incorporating all three text encoders and the vision encoder through fully parallel attention.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Impact of Synthetic Pairs</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1"><span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS3.p1.1.1">SynPairs</span> increase the diversity of the generated images by eliminating the copy-paste effect. Our ablation verifies this assumption and demonstrate better prompt-alignment compared to the model without synthetic paired training. We observed that it is especially effective for the complex prompts that require strong changes to the original images, <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS3.p1.1.2">e.g</em>.<span class="ltx_text" id="S4.SS3.SSS3.p1.1.3"></span>, expression change, covering the face, or turning head, <em class="ltx_emph ltx_font_italic" id="S4.SS3.SSS3.p1.1.4">etc</em>. However, we observed a regression in identity preservation with <span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS3.p1.1.5">SynPair</span> training because the faces in the corresponding reference and target pair are not exactly the same. Future work will focus on improving the face similarity of <span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS3.p1.1.6">SynPair</span> training data.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We would like to continue research and explore the following directions: 1) extend personalized image to video generation. The key is to consistently preserve the identity and scene in video generation. 2) While Imagine yourself has improved the prompt-alignment against existing models, we observed that it still has limitation in following prompts describing very complex poses, <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">e.g</em>.<span class="ltx_text" id="S5.p1.1.2"></span>, <span class="ltx_text ltx_font_italic" id="S5.p1.1.3">jumping from a mountain</span>. Future work will focus on improving the generated images‚Äô quality on these prompts.</p>
</div>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we introduce Imagine yourself, a pioneering model tailored for personalized image generation. Unlike traditional tuning-based approaches, Imagine yourself operates as a tuning-free solution, offering a shared framework accessible to all users without the need for individual adjustments. Imagine yourself overcomes the prior research limit in handling the intricate balance between preserving identity, following complex prompts, and maintaining visual quality by introducing 1) a novel synthetic paired data generation mechanism to foster image diversity, 2) a fully parallel attention architecture featuring three text encoders and a fully trainable vision encoder to enhance text faithfulness, and 3) a novel coarse-to-fine multi-stage fine-tuning methodology to progressively enhance visual quality. We perform large-scale human evaluation on thousands of examples and showcase that Imagine yourself outperforms state-of-the-art personalization models, demonstrating superior capabilities in identity preservation, visual quality, and text alignment.</p>
</div>
</section>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We extend our gratitude to the following people for their contributions: Xiaoliang Dai, Ji Hou, Kevin Chih-Yao Ma, Kunpeng Li, Sam Tsai, Jialiang Wang, Matthew Yu, Simran Motwani, Zijian He for text-to-image foundation model; Eric Alamillo, Xiao Chu, Yangwen Liu, Yan Yan, Jonas Kohler, Artsiom Sanakoyeu, Ali Thabet, Arantxa Casanova Paga, Zhipeng Fan for model evaluation and discussion; Aaron Nissenbaum, Becky McMahon, Mo Metanat, Danny Trinh, Jack Hanlon, Tali Zvi, Manohar Paluri, Prashant Ratanchandani, and Ahmad Al-Dahle for support and leadership.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et¬†al. (2023)</span>
<span class="ltx_bibblock">
Li¬†Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu¬†Wang, Hao Yang, Jing Liu, Kang Du, et¬†al.

</span>
<span class="ltx_bibblock">Photoverse: Tuning-free image customization with text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2309.05793</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et¬†al. (2023)</span>
<span class="ltx_bibblock">
Xiaoliang Dai, Ji¬†Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et¬†al.

</span>
<span class="ltx_bibblock">Emu: Enhancing image generation models using photogenic needles in a haystack.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2309.15807</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Esser et¬†al. (2024)</span>
<span class="ltx_bibblock">
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et¬†al.

</span>
<span class="ltx_bibblock">Scaling rectified flow transformers for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2403.03206</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal et¬†al. (2022)</span>
<span class="ltx_bibblock">
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or¬†Patashnik, Amit¬†H Bermano, Gal Chechik, and Daniel Cohen-Or.

</span>
<span class="ltx_bibblock">An image is worth one word: Personalizing text-to-image generation using textual inversion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2208.01618</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et¬†al. (2021)</span>
<span class="ltx_bibblock">
Edward¬†J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu¬†Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2106.09685</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. (2023)</span>
<span class="ltx_bibblock">
Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan.

</span>
<span class="ltx_bibblock">Photomaker: Customizing realistic human photos via stacked id embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2312.04461</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(7)</span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Introducing meta llama 3.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3/" title="">https://ai.meta.com/blog/meta-llama-3/</a>.

</span>
<span class="ltx_bibblock">Accessed on May 12, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ostashev et¬†al. (2024)</span>
<span class="ltx_bibblock">
Daniil Ostashev, Yuwei Fang, Sergey Tulyakov, Kfir Aberman, et¬†al.

</span>
<span class="ltx_bibblock">Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2404.11565</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Podell et¬†al. (2023)</span>
<span class="ltx_bibblock">
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna, and Robin Rombach.

</span>
<span class="ltx_bibblock">Sdxl: Improving latent diffusion models for high-resolution image synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2307.01952</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et¬†al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong¬†Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et¬†al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International conference on machine learning</em>, pages 8748‚Äì8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et¬†al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter¬†J Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Journal of machine learning research</em>, 21(140):1‚Äì67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et¬†al. (2022)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2204.06125</em>, 1(2):3, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et¬†al. (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 10684‚Äì10695, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz et¬†al. (2023a)</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 22500‚Äì22510, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruiz et¬†al. (2023b)</span>
<span class="ltx_bibblock">
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman.

</span>
<span class="ltx_bibblock">Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2307.06949</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saharia et¬†al. (2022)</span>
<span class="ltx_bibblock">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily¬†L Denton, Kamyar Ghasemipour, Raphael Gontijo¬†Lopes, Burcu Karagol¬†Ayan, Tim Salimans, et¬†al.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in neural information processing systems</em>, 35:36479‚Äì36494, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sauer et¬†al. (2023)</span>
<span class="ltx_bibblock">
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.

</span>
<span class="ltx_bibblock">Adversarial diffusion distillation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2311.17042</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et¬†al. (2022)</span>
<span class="ltx_bibblock">
Yi¬†Tay, Mostafa Dehghani, Vinh¬†Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung¬†Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, et¬†al.

</span>
<span class="ltx_bibblock">Ul2: Unifying language learning paradigms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2205.05131</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. (2024)</span>
<span class="ltx_bibblock">
Qixun Wang, Xu¬†Bai, Haofan Wang, Zekui Qin, and Anthony Chen.

</span>
<span class="ltx_bibblock">Instantid: Zero-shot identity-preserving generation in seconds.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2401.07519</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et¬†al. (2023)</span>
<span class="ltx_bibblock">
Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo.

</span>
<span class="ltx_bibblock">Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 15943‚Äì15953, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et¬†al. (2022)</span>
<span class="ltx_bibblock">
Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel.

</span>
<span class="ltx_bibblock">Byt5: Towards a token-free future with pre-trained byte-to-byte models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Transactions of the Association for Computational Linguistics</em>, 10:291‚Äì306, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et¬†al. (2023)</span>
<span class="ltx_bibblock">
Hu¬†Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang.

</span>
<span class="ltx_bibblock">Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2308.06721</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et¬†al. (2023)</span>
<span class="ltx_bibblock">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.

</span>
<span class="ltx_bibblock">Adding conditional control to text-to-image diffusion models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 3836‚Äì3847, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 20 09:17:08 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
