<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues</title>
<!--Generated on Thu Sep 19 08:26:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.12558v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S1" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S2" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Retrieval Augmented Dialogue Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS1" title="In 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Evaluated Abilities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS2" title="In 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Evaluator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3" title="In 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Benchmark Construction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS1" title="In 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Data Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS2" title="In 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Question Candidate Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS3" title="In 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Retrieved Context Integration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS4" title="In 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Question Candidates Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS5" title="In 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.5 </span>Reference Answers</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S4" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S4.SS1" title="In 4 Evaluation Results ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S4.SS2" title="In 4 Evaluation Results ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S4.SS3" title="In 4 Evaluation Results ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Correlation with Chatbot Arena</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S5" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#A1" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompts for Question Generation, Scoring, and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#A2" title="In RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Multi-turn dialogues examples for different scenarios</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Tzu-Lin Kuo<sup class="ltx_sup" id="id1.1.id1.1">†*</sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.id2">Feng-Ting Liao</span> <sup class="ltx_sup" id="id3.3.id3">*</sup>,
<span class="ltx_text ltx_font_bold" id="id4.4.id4">Mu-Wei Hsieh<sup class="ltx_sup" id="id4.4.id4.1"><span class="ltx_text ltx_font_medium" id="id4.4.id4.1.1">†</span></sup></span>,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.id5">Fu-Chieh Chang</span>,
<span class="ltx_text ltx_font_bold" id="id6.6.id6">Po-Chun Hsu</span>,
<span class="ltx_text ltx_font_bold" id="id7.7.id7">Da-Shan Shiu</span>
<br class="ltx_break"/>
<br class="ltx_break"/>MediaTek Research

<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.8.id8">{tzu-lin.kuo, ft.liao, morris-mw.hsieh,
<br class="ltx_break"/>mark-fc.chang, pochun.hsu, ds.shiu}@mtkresearch.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1"><span class="ltx_text" id="id9.id1.1" lang="en">In real-world applications with Large Language Models (LLMs), external retrieval mechanisms—such as Search-Augmented Generation (SAG), tool utilization, and Retrieval-Augmented Generation (RAG)—are often employed to enhance the quality of augmented generations in dialogues. These approaches often come with multi-turn dialogue, where each interaction is enriched by relevant information retrieved from external sources. Existing benchmarks either assess LLMs’ chat abilities in multi-turn dialogues or their use of retrieval for augmented responses in single-turn settings. However, there is a gap in evaluating LLMs’ ability to leverage retrieval for more precise responses across multiple turns.</span></p>
<p class="ltx_p" id="id10.id2"><span class="ltx_text" id="id10.id2.1" lang="en">To address this limitation, we introduce <span class="ltx_text ltx_font_bold" id="id10.id2.1.1">RAD</span>-Bench (<span class="ltx_text ltx_font_bold" id="id10.id2.1.2">R</span>etrieval <span class="ltx_text ltx_font_bold" id="id10.id2.1.3">A</span>ugmented <span class="ltx_text ltx_font_bold" id="id10.id2.1.4">D</span>ialogue), a benchmark designed to evaluate LLMs’ capabilities in multi-turn dialogues following retrievals, essential for their deployment in context-rich applications. RAD-Bench evaluates two key abilities of LLMs: <em class="ltx_emph ltx_font_italic" id="id10.id2.1.5">Retrieval Synthesis</em> and <em class="ltx_emph ltx_font_italic" id="id10.id2.1.6">Retrieval Reasoning</em>. These are measured using discriminative questions and retrieved contexts, and corresponding reference answers, assessing how effectively LLMs integrate and reason with context to maintain and enhance conversation quality over multiple turns. Our evaluation results on commonly used LLMs reveal that model performance deteriorates as additional layers of conditions or constraints are applied across conversation turns, even when accurate retrieved contexts are provided. Code and data are available at <a class="ltx_ref ltx_href ltx_font_typewriter" href="https://github.com/mtkresearch/RAD-Bench" title="">github.com/mtkresearch/RAD-Bench</a></span></p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\pdfcolInitStack</span>
<p class="ltx_p" id="p1.2">tcb@breakable
































































<span class="ltx_text" id="p1.2.1" lang="en"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p2" lang="en">
<div class="ltx_block ltx_align_bottom" id="p2.1">
<p class="ltx_p" id="p2.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.1.1">RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p2.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p2.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p2.1.2.1.1">
<span class="ltx_tr" id="p2.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p2.1.2.1.1.1.1.1">
Tzu-Lin Kuo<sup class="ltx_sup" id="p2.1.2.1.1.1.1.1.1">†*</sup>,
Feng-Ting Liao <sup class="ltx_sup" id="p2.1.2.1.1.1.1.1.2">*</sup>,
Mu-Wei Hsieh<sup class="ltx_sup" id="p2.1.2.1.1.1.1.1.3">†</sup>,</span></span></span>
<span class="ltx_tr" id="p2.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="p2.1.2.1.1.2.1.1">Fu-Chieh Chang</span>,
<span class="ltx_text ltx_font_bold" id="p2.1.2.1.1.2.1.2">Po-Chun Hsu</span>,
<span class="ltx_text ltx_font_bold" id="p2.1.2.1.1.2.1.3">Da-Shan Shiu</span></span></span>
<span class="ltx_tr" id="p2.1.2.1.1.3">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.3.1">MediaTek Research</span></span>
<span class="ltx_tr" id="p2.1.2.1.1.4">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.4.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.2.1.1.4.1.1">{tzu-lin.kuo, ft.liao, morris-mw.hsieh,</span></span></span>
<span class="ltx_tr" id="p2.1.2.1.1.5">
<span class="ltx_td ltx_align_center" id="p2.1.2.1.1.5.1"><span class="ltx_text ltx_font_typewriter" id="p2.1.2.1.1.5.1.1">mark-fc.chang, pochun.hsu, ds.shiu}@mtkresearch.com</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1" lang="en"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="footnotex1.1" style="width:433.6pt;">
<span class="ltx_p" id="footnotex1.1.1"><sup class="ltx_sup" id="footnotex1.1.1.1">†</sup>Work done during internship at MediaTek Research.</span>
<span class="ltx_p ltx_align_left" id="footnotex1.1.2"><sup class="ltx_sup" id="footnotex1.1.2.1">*</sup>Core contributors.</span>
</span></span></span></span>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="801" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.2.1">Evaluation Process in Retrieval Augmented Dialogue Benchmark</span>: At each turn, a user question paired with a retrieved context is presented to the LLM for augmented generation. The LLM’s response is scored on a scale of 1 to 10 using an LLM-as-a-Judge framework. This framework prompts the judge to assess how well the model utilized the given context to answer progressively changing questions, based on specific criteria, and compare it against a reference answer, ensuring accurate and consistent evaluations across different scenarios.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="612" id="S1.F2.g1" src="extracted/5859982/model_radar_chart.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S1.F2.2.1">Performance of evaluated LLMs</span></figcaption>
</figure>
<figure class="ltx_figure" id="S1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="479" id="S1.F3.g1" src="extracted/5859982/correlation_plots/spearman_correlation_hard-en.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S1.F3.8.1">Correlation between RAD-Bench and Chatbot Arena (Hard-En prompts)</span> <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib5" title="">2024</a>)</cite>. Models exhibiting similar level of multi-turn chat capability do not perform similarly when they are applied to dialogues from retrieval, as showcased by results from <span class="ltx_text ltx_font_typewriter" id="S1.F3.9.2">Llama3.1-8B</span> vs <span class="ltx_text ltx_font_typewriter" id="S1.F3.10.3">Mistral-Large</span>; from <span class="ltx_text ltx_font_typewriter" id="S1.F3.11.4">Llama3.1-70B</span> vs <span class="ltx_text ltx_font_typewriter" id="S1.F3.12.5">Deepseek-V2</span>; from <span class="ltx_text ltx_font_typewriter" id="S1.F3.13.6">Llama3.1-405B</span> vs <span class="ltx_text ltx_font_typewriter" id="S1.F3.14.7">GPT-4o</span>.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, Large Language Models (LLMs) have shown remarkable capabilities in a diverse set of tasks, including summarization <cite class="ltx_cite ltx_citemacro_citep">(Ziegler et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib46" title="">2019</a>)</cite>, question-answering <cite class="ltx_cite ltx_citemacro_citep">(Mitra et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib23" title="">2023</a>)</cite>, planning <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib34" title="">2024</a>)</cite>, coding <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib36" title="">2024</a>)</cite>, and tool using <cite class="ltx_cite ltx_citemacro_citep">(Qin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib30" title="">2024</a>)</cite>. Despite these advances, due to the propensity of LLMs for hallucination and internal biases acquired during training <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib38" title="">2024</a>)</cite>, as well as the high costs associated with updating knowledge through retraining, alternative methods have been proposed. Notably, these include LLMs with extended context windows <cite class="ltx_cite ltx_citemacro_citep">(Peng et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib28" title="">2024</a>)</cite>, which facilitate the caching of substantial context, and retrieval-augmented generation (RAG) techniques <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib14" title="">2020</a>; Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib20" title="">2023</a>)</cite>, which utilize an external knowledge database as long-term memory.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In real-world applications <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib25" title="">2023</a>; MediaTek, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib22" title="">2024</a>; Perplexity AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib29" title="">2024</a>)</cite>, augmenting LLM generation with retrieved context, such as retrieved documents <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib14" title="">2020</a>)</cite>, web-searched results <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib20" title="">2023</a>)</cite>, and retrieved knowledge graphs <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib37" title="">2024</a>)</cite>, has become a common practice to alleviate hallucination. Existing benchmarks for evaluating LLMs’ augmented generation following retrieved context, such as <cite class="ltx_cite ltx_citemacro_citet">Lyu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib21" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib4" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib39" title="">2024</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib37" title="">2024</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib41" title="">2024</a>)</cite>, focus on single-turn instructions, whereas real-world interactions involve multi-turn dialogues.
Meanwhile, benchmarks in evaluating LLMs’ chat capabilities in multi-turn dialogues,
such as <cite class="ltx_cite ltx_citemacro_citet">Finch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib9" title="">2022</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib42" title="">2023</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib2" title="">2024</a>)</cite>, neglect instruction-following with retrieved context.
Hence, to our best knowledge, there exists no benchmarks in evaluating models’ capabilities in <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">augmented generation with retrieved context under multi-turn instructions</em>, a common setting for completing tasks such as writing, customer support, search recommendations in context-rich applications.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the aforementioned gap, we propose Retrieval Augmented Dialogue Benchmark (RAD-Bench), a benchmark designed to measure LLMs’ ability to follow user instructions in multi-turn dialogue scenarios and effectively recall and utilize retrieved context to enhance their responses. Specifically, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">1</span></a>, each benchmark sample consists of three-turn questions with accompanied retrieved context at each turn. We curate sample scenarios from real-world multi-turn dialogue data where continuous context application is essential for generation augmentation. These scenarios are grouped into two major evaluated abilities of LLMs: <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">Retrieval Synthesis</em> and <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">Retrieval Reasoning</em>. We define Retrieval Synthesis the ability of LLMs to progressively integrate and utilize retrieved context for tasks such as summarization and article writing, enabling effective knowledge accumulation and synthesis. We define Retrieval Reasoning being whether LLMs can make reasonable inference when user intent changes or additional conditions are introduced across turns, utilizing context in each turn to refine and improve responses. For each evaluated abilities, we select three representative scenarios that exemplify multi-turn dialogues following retrievals.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Furthermore, to construct the RAD-Bench, we propose a pipeline that orchestrates LLMs to generate, select, and synthesize synthetic questions and retrieved contexts. With the collected documents from public websites and datasets for each scenario, we generate a diverse set of multi-turn candidates using BreeXe-8x7B, Llama3-70B, and Mixtral-8x22B with tailored generation prompts, paired with corresponding source documents. The generated candidates include questions and search queries for each turn, which are subsequently used to retrieve relevant context via Azure’s web search and RAG services. The candidate questions then are scored by BreeXe-8x7B with scoring prompts following criteria such as relevance, progression, and clarity. Lastly, the final benchmark samples including reference answers are extracted from top-scoring candidates, manually inspected to ensure the retrieved contexts and reference answers containing relevant information for a robust evaluation.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The benchmark comprises 89 multi-turn question samples, each consisting of 3 turns with accompanying retrieved context and reference answer. This results in a total of 267 turns for evaluation. The evaluation adopts LLM-as-a-Judge <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib42" title="">2023</a>)</cite> with judge prompts including scenario-specific criteria to serve as scoring guidelines for each scenario as inspired by <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib10" title="">2023</a>)</cite>. We evaluate and report evaluation results for multiple commonly used LLMs such as GPT-4o, Llama, Gemma, Mistral, Deepseek, and Breeze. Further analysis of the results reveals that model performance degrades despite relevant retrieved contexts are provided as new intents or conditions are introduced into the multi-turn instruction. Finally, by comparing the evaluation scores with Elo ratings from Chatbot Arena (Hard Prompts) <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib15" title="">2024a</a>; Chiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib5" title="">2024</a>; Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib16" title="">2024b</a>)</cite>, we demonstrate that RAD-Bench effectively discriminates LLMs in context-rich, augmented dialogues applications. This comparison reveals that models exhibiting similar performance in multi-turn conversations may not perform equally in dialogues that follow retrievals.</p>
</div>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SSx1">
<h3 class="ltx_title ltx_title_subsection">Multi-turn dialogue</h3>
<div class="ltx_para" id="S2.SSx1.p1">
<p class="ltx_p" id="S2.SSx1.p1.1">LLMs with instruction-following ability <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib43" title="">2023</a>)</cite> has been an active area of research. Early works by <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib3" title="">2020</a>; Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib35" title="">2022</a>)</cite> highlight the capabilities of LLMs in single-turn instruction. More recently, <cite class="ltx_cite ltx_citemacro_citet">Ouyang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib26" title="">2022</a>); Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib33" title="">2023</a>); Mitra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib23" title="">2023</a>)</cite> have enhanced multi-turn interactions by maintaining context and generating coherent responses over extended exchanges through human alignment <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib31" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SSx2">
<h3 class="ltx_title ltx_title_subsection">Retrieval augmented generation benchmarks</h3>
<div class="ltx_para" id="S2.SSx2.p1">
<p class="ltx_p" id="S2.SSx2.p1.1">Several research efforts have evaluated LLMs’ augmented generation ability with retrieved context. For instance, <cite class="ltx_cite ltx_citemacro_citet">Lyu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib21" title="">2024</a>)</cite> evaluates RAG applications in <span class="ltx_text ltx_font_typewriter" id="S2.SSx2.p1.1.1">Create, Read, Update, and Delete</span> scenarios, while <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib4" title="">2024</a>)</cite> measures the fundamental abilities of LLMs required for RAG. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib39" title="">2024</a>)</cite> comprehensively evaluate factual questions with context from documents, web searches, APIs, and knowledge graphs. Contexts from tools such as <span class="ltx_text ltx_font_typewriter" id="S2.SSx2.p1.1.2">Google Calendar</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SSx2.p1.1.3">FlightSearch</span> are provided by <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib37" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib41" title="">2024</a>)</cite> to LLMs for evaluating planning abilities. Most benchmarks, though, typically evaluate LLMs in single-turn instructions, whereas real-world applications often involve multi-turn dialogues to address accumulation of hypotheses, constraints, and evolving user intents, which are not captured in typical single-turn evaluations.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SSx3">
<h3 class="ltx_title ltx_title_subsection">Multi-turn benchmarks</h3>
<div class="ltx_para" id="S2.SSx3.p1">
<p class="ltx_p" id="S2.SSx3.p1.1">To evaluate LLMs’ ability to accurately adhere to instructions in multi-turn dialogues on open-ended tasks that correlate with human preferences, several benchmarks have been proposed. Early work by <cite class="ltx_cite ltx_citemacro_citet">Finch et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib9" title="">2022</a>)</cite> evaluates models using labor-intensive metrics. Recently, <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib42" title="">2023</a>)</cite> attempts to evaluate models across core abilities with LLM-as-a-Judge, while <cite class="ltx_cite ltx_citemacro_citet">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib2" title="">2024</a>)</cite> proposes fine-grained assessments of real-life dialogues. Additionally, <cite class="ltx_cite ltx_citemacro_citet">Dubois et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib8" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib17" title="">2024</a>)</cite> comprehensively evaluate models with human-chatbot conversation logs, although these contained only tasks with single-turn instructions. Nevertheless, these works neglect the evaluation of the instruction-following capability of LLMs in multi-turn conversations while external context is engaged. Therefore, in this work, we aim to bridge this gap and propose a benchmark for evaluating core scenarios in real-life context-rich applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Retrieval Augmented Dialogue Benchmark</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">RAD-Bench aims to evaluate the capabilities of LLMs in multi-turn dialogues following retrievals, as exemplified by real world application such as ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib25" title="">2023</a>)</cite>, Perplexity <cite class="ltx_cite ltx_citemacro_citep">(Perplexity AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib29" title="">2024</a>)</cite>, and MediaTek DaVinci <cite class="ltx_cite ltx_citemacro_citep">(MediaTek, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib22" title="">2024</a>)</cite>. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">1</span></a>, each sample in the benchmark consists of three-turn questions with accompanied retrieved context to simulate the retrieval augmented dialogues. Responses to the turn questions by an LLM are evaluated by a reference-guided-judge, and a point-wise evaluation score for the LLM is reported.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In the following section, we first introduce the two evaluated abilities in the benchmark: <em class="ltx_emph ltx_font_italic" id="S3.p2.1.1">Retrieval Synthesis</em> and <em class="ltx_emph ltx_font_italic" id="S3.p2.1.2">Retrieval Reasoning</em>, where each ability comes with three representative tasks serving as application scenarios. We conclude and define these tasks through analyzing chat dialogues from ShareGPT <cite class="ltx_cite ltx_citemacro_citep">(Dom Eccleston, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib7" title="">2024</a>)</cite>, and MediaTek DaVinci <cite class="ltx_cite ltx_citemacro_citep">(MediaTek, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib22" title="">2024</a>)</cite>. We then explain the reference-guided-judge for evaluating LLM in generating response for retrieval augmented dialogues. Lastly, we present details of a proposed pipeline that is used to construct the RAD-Bench.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluated Abilities</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="822" id="S3.F4.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Evaluated capabilities—<em class="ltx_emph ltx_font_italic" id="S3.F4.3.1">Retrieval Synthesis</em> and <em class="ltx_emph ltx_font_italic" id="S3.F4.4.2">Retrieval Reasoning</em>—across three concrete application scenarios each. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#A2" title="Appendix B Multi-turn dialogues examples for different scenarios ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">B</span></a> for examples of augmented dialogues following retrievals.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Retrieval Synthesis</h4>
<div class="ltx_para" id="S3.SS1.SSSx1.p1">
<p class="ltx_p" id="S3.SS1.SSSx1.p1.1">In the applications of RAG and SAG in chatbots <cite class="ltx_cite ltx_citemacro_citep">(Perplexity AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib29" title="">2024</a>; MediaTek, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib22" title="">2024</a>)</cite>, users can require LLMs to utilize retrieved context for answering queries related to completing tasks such as summarization, paragraph writing, and knowledge synthesis in multi-turn dialogues. To measure the capability of LLMs in completing such tasks, we define <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSSx1.p1.1.1">Retrieval Synthesis</em> (RS) as the ability of LLM in following user instructions across turns while extracting useful information from retrieved information and integrating the information progressively. The selected scenarios for measuring the RS ability of LLMs are as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx1.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">News TLDR(Too Long; Didn’t Read)</span> embodies the scenario of journalist writing articles. It consists of instructions requiring LLMs to write comprehensive news articles by integrating retrievals of related past events, statistics, expert opinions, and recent developments.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Education</span> represents the case where educators compose educational articles. It comprises queries instructing LLMs to create engaging materials with progressive depths and breadths from retrievals of diverse educational resources.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Academic Writing</span> exemplifies the scenario that researchers leveraging LLMs to draft and refine academic papers. It includes multi-turn prompts that guide LLMs to integrate retrieved information from relevant studies, data, and citations, progressively building content depth. This is particularly useful for sections such as related work and literature reviews.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Retrieval Reasoning</h4>
<div class="ltx_para" id="S3.SS1.SSSx2.p1">
<p class="ltx_p" id="S3.SS1.SSSx2.p1.1">Carrying out data analysis <cite class="ltx_cite ltx_citemacro_citep">(MediaTek, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib22" title="">2024</a>)</cite>, constructing customer support chatbots <cite class="ltx_cite ltx_citemacro_citep">(Pandya and Holia, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib27" title="">2023</a>)</cite>, or planning <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib37" title="">2024</a>)</cite> through utilizing external databases and RAG have become prevalent scenarios of LLMs. In these scenarios, users interact with LLMs through queries that involve diverse hypotheses, new conditions, or changing intents based on retrieved information. We define <em class="ltx_emph ltx_font_italic" id="S3.SS1.SSSx2.p1.1.1">Retrieval Reasoning</em> (RR), an ability of LLMs in adjusting responses using retrieved references to support logical reasoning and problem-solving across multiple dialogue turns. We select complex scenarios where understanding context and evolving conditions is crucial for measuring the RR ability of LLMs. These scenarios are described as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.SSSx2.p2">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">Customer Support</span> addresses the application of RAG techniques with LLMs to enhance the user experience of customer support chatbots. It consists of questions and retrieved contexts for evaluating LLMs in resolving customer inquiries and narrowing down solutions with the contexts as customers describe issues in more details progressively.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">Finance</span> exemplifies the task of financial analyst utilizing LLMs with RAG to carry out data analysis. Queries in this scenario include tasks such as comparison of assets and computing finance metrics from retrieved financial statements for consolidating financial outlooks of companies at the end of multi-turn dialogues.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Travel Planning</span> represents the case where LLMs act as travel planning assistants in suggesting travel itineraries based on external databases. Instructions in such scenario start from broad questions and move on to specific conditions, e.g., preferred destinations, budgets, accommodations, and activities, to test LLMs in reasoning through conditions with retreived contexts. Furthermore, conflicting and updates to conditions are presented in the multi-turn instructions to evaluate LLMs ability in correcting its advice.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Evaluator</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Following <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib42" title="">2023</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib10" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib18" title="">2023</a>)</cite>, we utilize LLM-as-a-Judge and prompt the judge to evaluate chatbot responses to benchmark questions. The judge takes in chat history, retrieved context, and current turn question and response as inputs and provide a point-wise score to model response for each turn. We devise evaluation criteria for judge prompts, inspired by <cite class="ltx_cite ltx_citemacro_citep">(Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib10" title="">2023</a>)</cite>. Each criterion is accompanied by tailored instructions to guide the LLM’s evaluation. For Retrieval Synthesis, we assess Consistency, Informativeness, and Coherence, while for Retrieval Reasoning, we evaluate Accuracy, Consistency, and Coherence. These specific instructions enable the LLM to conduct more precise assessments of each aspect. We implement reference-guided judges <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib42" title="">2023</a>)</cite> with audited reference answers (Section <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS5" title="3.3.5 Reference Answers ‣ 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">3.3.5</span></a>) for each evaluated turn in News TLDR, Education, Academic Writing, Customer Support, and Finance scenarios. For the Travel Planning scenario, we apply this approach only to the last turn. We utilize chain-of-thought to prompt the judge to generate analysis based on the criteria and the reference answer prior to generating the final point-wise score. For further details of the judge prompts, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#A1" title="Appendix A Prompts for Question Generation, Scoring, and Evaluation ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="453" id="S3.F5.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S3.F5.2.1">Data construction pipeline of RAD-Bench</span>: The blue dashed lines represent scenarios with predetermined context integration at each turn, while the red dashed lines indicate scenarios where context must be retrieved via SAG or RAG, requiring additional search queries during question candidate generation (Phase 2). Detailed explanations of each phase are provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3" title="3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">3.3</span></a>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Benchmark Construction</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To construct benchmark questions with auditable reference answers, we propose a data generation pipeline (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.F5" title="Figure 5 ‣ 3.2 Evaluator ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">5</span></a>) that generates questions synthetically. This process involves deconstructing the knowledge points of an article into multiple-turn questions for Retrieval Synthesis and breaking down the joint conditions of solved tasks into multiple-turn questions for Retrieval Reasoning. We leverage LLMs both as question generators to create a pool of synthetic candidates and as question scorers to select the most suitable synthetic candidates for multi-turn dialogues from the retrievals.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Data Collection</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">We collect source articles and datasets from public data to form the source documents for synthetic question generation.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.1">Retrieval Synthesis</span>: For <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.2">News TLDR</span> scenario, we selected news articles from BBC; for <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.3">Education</span> scenario, we sourced popular science paragraphs from Scientific American; for <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p2.1.4">Academic Writing</span> scenario, we selected related work sections from papers on Arxiv and further extracted papers that appeared in each related work section. We include only source materials published after June 2024 to reduce the likelihood of the materials being included in the training data of LLMs.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.1.1">Retrieval Reasoning</span>: For <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.1.2">Customer Support</span> scenario, we collected user manuals from ManyManuals website. For <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.1.3">Finance</span> scenario, we leveraged datasets from FinanceBench <cite class="ltx_cite ltx_citemacro_citep">(Islam et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib12" title="">2023</a>)</cite> as source documents. The benchmark dataset comprises 10,231 questions, answers, and evidence triplets. The evidence triplets are passages supporting answering of the question from finance report documents. We manually inspected and selected 15 triplets that involve multi-step reasoning process to get the final answer and collected corresponding source documents to serve as base data for further question candidate generation process. For <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS1.p3.1.4">Travel Planning</span> scenario, we utilized TravelPlanner dataset <cite class="ltx_cite ltx_citemacro_citep">(Xie et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib37" title="">2024</a>)</cite>, which comprises 1225 travel planning queries in total and leveled from simple to hard, as source documents. The hard questions in the dataset involved complicated and multiple constraints in a query, suitable for being decomposed into multi-step reasoning steps to construct instructions including constraints progressively in multi-turn dialogues. We therefore selected 15 hard questions from the training set which provides human-annotated plan as reference to serve as source data for further question candidates generation process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Question Candidate Generation</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">With the collected source documents, candidates of three-turn questions for each scenarios are generated by a question generator as realized by an LLM. Output of the generator for News TLDR, Education, Finance, and Customer Support scenarios for each turn includes a question and a search query. The search queries are used for retrieving relevant context as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.SS3.SSS3" title="3.3.3 Retrieved Context Integration ‣ 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">3.3.3</span></a>. As to Academic Writing and Travel Planning scenarios, outputs of the generator include only the questions. We craft step-by-step guidance as prompts to the generator for aligning the generated questions with the evaluated abilities. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#A1" title="Appendix A Prompts for Question Generation, Scoring, and Evaluation ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">A</span></a> for details of the guidance and the prompts. We used multiple LLMs (BreeXe-8x7B, Llama3-70B, and Mixtral-8x22B) as the generator and varied the generation temperature for generating a diverse set of candidates.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Retrieved Context Integration</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">In this phase, each candidate’s questions for each turn are supplemented with corresponding useful information, simulating the retrieval process. For the <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.1">News TLDR</span> and <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.2">Education</span> scenarios, the accompanied search queries as produced in the question candidate generation stage are passed to the Azure web search service to retrieve the top 5 documents as useful information. For the <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.3">Customer Support</span> and <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.4">Finance</span> scenarios, we input the turn questions and source documents into Azure’s RAG service to collect the retrieved contexts. For the <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.5">Academic</span> scenario, the information to be integrated is pre-determined. We identify referenced papers in the questions and extract the abstracts and introductions of these papers to serve as retrieved contexts for the corresponding turns. In the <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS3.p1.1.6">Travel Planning</span> scenario, each turn includes reference information from the TravelPlanner bench, such as flight details, cities, and attractions, without further modification.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Question Candidates Selection</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">We employ an LLM as a scorer to assist the filtering of question candidates. For each scenario, we design customized prompts following scoring criteria to score each candidate. The criteria include Relevance, Progression, Clarity, Support, Knowledge Points, and Medium Complexity as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#A1.F14" title="Figure 14 ‣ Appendix A Prompts for Question Generation, Scoring, and Evaluation ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">14</span></a>. The Support and Knowledge criteria prompt the scoring LLMs to examine whether the retrieved context from web search and RAG services contains relevant information for answering candidate questions. We scored candidates with BreeXe-8x7B, Llama3-70B, and Mixtral-8x22B. After conducting a human review of a subsampled set of scored candidates, we selected the scoring results from BreeXe-8x7B due to its preferable alignment with the established criteria. With the scored candidates of three-turn questions for each scenario, we then filtered out the top candidates and manually verified that the retrieved contexts contain informative and relevant information for answering the questions in each turn.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.5 </span>Reference Answers</h4>
<div class="ltx_para" id="S3.SS3.SSS5.p1">
<p class="ltx_p" id="S3.SS3.SSS5.p1.1">To ensure the robustness of RAD-Bench evaluation, following the reasoning tasks in <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib42" title="">2023</a>)</cite>, we provide reference answers to benchmark questions. For evaluating scenarios in <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS5.p1.1.1">Retrieval Synthesis</span>, we extract knowledge points - sets of factual statements <cite class="ltx_cite ltx_citemacro_citep">(Adams et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib1" title="">2023</a>)</cite> - from retrieved contexts using BreeXe-8x7B as references for the first and second turn questions, and use target paragraphs in source documents as references for the third turn. Such reference answers thereby provide evaluator baseline quality of responses by determining whether useful knowledge points are recalled and integrated into the model’s answer. For <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS5.p1.1.2">Retrieval Reasoning</span>, which involves cross-turn reasoning, we manually inspect the questions and extract evidence text from the retrieved context to fully support the answers for Customer Support and Finance scenarios. In the Travel Planning scenario, we do not include reference answers in the first two turns. Instead, for the final turn, we use an expert-annotated travel plan provided in TravelPlanner Bench as the reference answer. This allows the evaluator to assess the similarity and coverage between the model’s planned itinerary and the expert-annotated travel plan.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S3.T1.1" style="width:433.6pt;height:203pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-78.3pt,36.5pt) scale(0.734636831503755,0.734636831503755) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.1.1">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="7" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">RAD-Bench</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.1.1">Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.2.1">Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.2.3">
<span class="ltx_text" id="S3.T1.1.1.2.3.1"></span> <span class="ltx_text" id="S3.T1.1.1.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.2.3.2.1">
<span class="ltx_tr" id="S3.T1.1.1.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.2.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.3.2.1.1.1.1">Activated</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.2.3.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.3.2.1.2.1.1">Params.</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.2.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.2.4">
<span class="ltx_text" id="S3.T1.1.1.2.4.1"></span> <span class="ltx_text" id="S3.T1.1.1.2.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.1.2.4.2.1">
<span class="ltx_tr" id="S3.T1.1.1.2.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.2.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.4.2.1.1.1.1">Context</span></span></span>
<span class="ltx_tr" id="S3.T1.1.1.2.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.1.2.4.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.4.2.1.2.1.1">Length</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.1.2.4.3"></span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.5.1">Academic</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.2.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.6.1">News</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.1.1.2.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.7.1">Education</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.2.8"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.8.1">Finance</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.2.9"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.9.1">Customer</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.1.1.2.10"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.10.1">Travel</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.2.11"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.11.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.1" rowspan="3"><span class="ltx_text" id="S3.T1.1.1.3.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.1.1.3.1.1.1" style="width:6.9pt;height:23.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:23.4pt;transform:translate(-8.22pt,-8.22pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.1.1.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.1.1.1.1.1">Close</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2">GPT-4o</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.3.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.3.4">128k</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.3.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.3.5.1">8.77</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.3.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.3.6.1">8.68</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.1.1.3.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.3.7.1">8.95</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.3.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.3.8.1">9.00</span></td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.3.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.3.9.1">9.10</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.1.1.3.10">7.83</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.3.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.1.1.3.11.1">8.72</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.1">GPT-4o-mini</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.4.3">128k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.4.4">8.27</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.4.5">8.53</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.4.6">8.80</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.4.7">8.87</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.4.8">8.53</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.4.9">7.80</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.4.10">8.47</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.1">Mistral-Large</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.5.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.5.3">32k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.5.4">8.17</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.5.5">7.77</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.5.6">8.33</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.5.7">8.58</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.5.8">7.83</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.5.9">6.76</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.5.10">7.91</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6">
<td class="ltx_td ltx_border_r" id="S3.T1.1.1.6.1"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.2">GPT-3.5-Turbo</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.6.4">16k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.6.5">5.30</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.6.6">5.23</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.6.7">6.55</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.6.8">8.04</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.6.9">8.47</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.6.10">5.93</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.6.11">6.59</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S3.T1.1.1.7.1" rowspan="8"><span class="ltx_text" id="S3.T1.1.1.7.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S3.T1.1.1.7.1.1.1" style="width:8.8pt;height:23.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:23.6pt;transform:translate(-7.42pt,-6.44pt) rotate(-90deg) ;">
<span class="ltx_p" id="S3.T1.1.1.7.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.1.1.1.1.1">Open</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.2">Llama3.1-405B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.1.7.3">405B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.1.7.4">128k</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.7.5">7.90</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.7.6">8.07</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.1.1.7.7">8.25</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.7.8">8.22</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.7.9">7.63</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T1.1.1.7.10">7.21</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.1.7.11"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.7.11.1">7.88</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.1">Llama3.1-70B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.8.2">70B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.8.3">128k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.8.4">8.03</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.8.5">7.72</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.8.6">8.25</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.8.7">8.02</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.8.8">6.83</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.8.9">7.07</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.8.10">7.65</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.1">Mixtral-8x22b</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.9.2">39B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.9.3">64k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.9.4">7.70</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.9.5">7.47</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.9.6">7.97</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.9.7">8.22</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.9.8">8.10</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.9.9">5.79</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.9.10">7.54</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.1">Deepseek-v2</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.10.2">21B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.10.3">128k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.10.4">7.57</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.10.5">6.67</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.10.6">8.00</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.10.7"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.10.7.1">8.71</span></td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.10.8"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.10.8.1">8.27</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.10.9"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S3.T1.1.1.10.9.1">7.95</span></td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.10.10">7.86</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.1">BreeXe-8x7B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.11.2">13B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.11.3">8k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.11.4"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.11.4.1">8.47</span></td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.11.5"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.11.5.1">8.14</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.11.6"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.11.6.1">8.58</span></td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.11.7">7.56</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.11.8">7.63</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.11.9">5.74</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.11.10">7.69</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.12.1">Mistral-Nemo-12B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.12.2">12B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.12.3">128k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.12.4">7.20</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.12.5">6.84</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.12.6">7.42</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.12.7">7.33</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.12.8">7.47</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.12.9">3.55</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.12.10">6.63</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.13.1">Llama3.1-8B</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.13.2">8B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.1.1.13.3">128k</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.13.4">7.33</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.13.5">6.16</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.13.6">7.53</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.13.7">8.33</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.13.8">6.77</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S3.T1.1.1.13.9">5.17</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.13.10">6.88</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.14">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.1.14.1">Breeze-7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.14.2">7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.1.1.14.3">8k</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.1.14.4">7.47</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.1.14.5">7.33</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T1.1.1.14.6">7.80</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.1.14.7">6.93</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.1.14.8">7.13</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T1.1.1.14.9">4.83</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.1.14.10">6.92</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluated models in RAD-Bench. For each scenario, <span class="ltx_text ltx_font_bold" id="S3.T1.4.1">bold score</span> indicates the best open-weight model; <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.T1.5.2">underlined score</span> marks the best model overall. We report instruct versions of the open-weight models.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Evaluation Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We evaluated a series of models, including OpenAI GPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib25" title="">2023</a>)</cite>, Mistral <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib13" title="">2023</a>)</cite>, Gemma <cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib32" title="">2024</a>)</cite>, Llama <cite class="ltx_cite ltx_citemacro_citep">(Llama Team, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib19" title="">2024</a>)</cite>, DeepSeek <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib6" title="">2024</a>)</cite>, and BreeXe <cite class="ltx_cite ltx_citemacro_citep">(Hsu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib11" title="">2024</a>)</cite>, each available in multiple model sizes. All selected models have context windows more than 8k, suitable for RAD applications. We collected the response of close-source models in July 2024, through API calls, and carried out evaluation with <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.1">GPT-4o (2024-05-13)</span> with temperature set to 0 as evaluator.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="S4.F6.g1" src="extracted/5859982/combined_3_radar.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance of various LLMs by categories (Llama 3.1, Mistral, and Breeze/BreeXe)</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We show scores of evaluated models in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S3.T1" title="Table 1 ‣ 3.3.5 Reference Answers ‣ 3.3 Benchmark Construction ‣ 3 Retrieval Augmented Dialogue Benchmark ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">1</span></a> and in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">2</span></a>. Overall speaking, the closed-source models, particularly <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.1">GPT-4o</span> with average of 8.72, consistently outperformed the open-source models across most scenarios. As for the open-source models, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.2">Llama3.1-405B</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p1.1.3">Deepseek-v2</span> show strong performance with averages of 7.88 and 7.86, respectively. These two models stand out within the open-source category, though still trailing behind the top closed-source models.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Scenario-Specific Observations</h4>
<div class="ltx_para" id="S4.SS2.SSSx1.p1">
<p class="ltx_p" id="S4.SS2.SSSx1.p1.1">In the Retrieval Synthesis scenarios, we observe that <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.1">BreeXe-8x-7B</span> achieved impressive performance, closely rivaling the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.2">GPT-4o-mini</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.3">GPT-4o</span>. This outcome may be attributed to the fact that <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.4">BreeXe-8x-7B</span> was utilized as a question scorer, potentially leading to a bias in selecting questions that favor the model’s strengths and thus resulting in the model performing well on questions it is inherently suited to answer. Additionally, we identified that the Travel Planning scenario is the most challenging among the six evaluated scenarios. Notably, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.5">Deepseek-v2</span> outperformed <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.6">GPT-4o</span> and all other models in this scenario. We hypothesize that this success can be linked to <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.7">Deepseek-v2</span>’s two-stage reinforcement learning (RL) training strategy <cite class="ltx_cite ltx_citemacro_citep">(DeepSeek-AI, <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib6" title="">2024</a>)</cite>. Initially, the model undergoes optimization on code and math reasoning tasks (reasoning alignment), which enhances its reasoning capabilities. This is followed by further helpful, safety, and rule alignment adjustments. The similarity between travel planning tasks and coding or math tasks—particularly their shared emphasis on hypothesis formation and constraint modification—likely enabled <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSSx1.p1.1.8">Deepseek-v2</span> to excel in this scenario due to the benefits gained from its reasoning alignment RL process.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Performance Across Dialogue Turns</h3>
<div class="ltx_para" id="S4.SSx1.p1">
<p class="ltx_p" id="S4.SSx1.p1.1">To investigate the model performance across turns for different evaluated abilities, we calculate the average score for each dialogue turn as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S5.F7" title="Figure 7 ‣ 5 Discussion and Future Work ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">7</span></a>. In Retrieval Synthesis, the average performance of models tends to improve at the second turn and declines at the third turn. Upon inspecting the evaluation by the judge, we observe that the score increase can be attributed to models following the instructions in second turn question to expand the breadth or deepen the depth of the augmented generation; the decline in the third turn is due to the challenge in synthesizing drafted article and paragraphs from the previous turns with the retrieved context provided in the third turn. As to Retrieval Reasoning, average scores of models notably decrease as turn number increases. We analyzed the model responses and conclude that models tend to fail to comply with the new conditions or constraints as provided in the second and third turn questions, resulting in failures to answer Retrieval Reasoning questions. We hypothesize that the chain-of-density framework <cite class="ltx_cite ltx_citemacro_citep">(Adams et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib1" title="">2023</a>)</cite>, due to its effectiveness in progressively extracting and synthesizing knowledge points, can enhance model performance in Retrieval Synthesis scenarios; that the self-discovery framework <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib44" title="">2024</a>)</cite>—with its structured approach to reasoning strategies—can improve reasoning in the context of newly presented conditions during multi-turn questions following retrieval in Retrieval Reasoning. Investigations into these hypotheses are left for future work.</p>
</div>
<section class="ltx_subsubsection" id="S4.SSx1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Effect of Model Size</h4>
<div class="ltx_para" id="S4.SSx1.SSSx1.p1">
<p class="ltx_p" id="S4.SSx1.SSSx1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S4.F6" title="Figure 6 ‣ 4.1 Evaluation Setup ‣ 4 Evaluation Results ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the performance distribution of various model series across different scenarios as the model scale changes. It is evident that as the model size increases, there is a notable improvement in reasoning capabilities, with the most significant growth observed in the Travel Planning scenario. This observation aligns with findings of <cite class="ltx_cite ltx_citemacro_citet">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib2" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Mondorf and Plank (<a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib24" title="">2024</a>)</cite>, which emphasize that as model scale increases, the model’s ability to reason, employ strategies, and interact becomes more pronounced.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Correlation with Chatbot Arena</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To study whether industry chat benchmark is sufficient to represent the performance of LLMs in applications requiring augmented generations, we compare the evaluation results of models in the benchmark to Elo scores of models from Chatbot Arena, an industry benchmark for assessing LLMs’ chat capability <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib5" title="">2024</a>)</cite> through anonymous human evaluations. We include models appearing in the Chatbot Arena for comparison. Results in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#S1.F3" title="Figure 3 ‣ 1 Introduction ‣ RAD-Bench: Evaluating Large Language Models’ Capabilities in Retrieval Augmented Dialogues"><span class="ltx_text ltx_ref_tag">3</span></a> shows that RAD-Bench is discriminative. Models exhibiting similar level of chat capability, such as <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.1">GPT-4o</span> vs <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.2">Llama3.1-405B</span>; <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.3">Llama3.1-70B</span> vs <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.4">Deepseek-v2</span>; <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.5">Llama3.1-8B</span> vs <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.6">Mistral-Large</span>, do not perform equally well when the models are applied to scenarios with dialogues from retrieval.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Future Work</h2>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_portrait" height="1096" id="S5.F7.g1" src="extracted/5859982/scores_per_turn.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S5.F7.2.1">Model performance across dialogue turns</span>. (Top): Retrieval Synthesis; (Bottom) Retrieval Reasoning.</figcaption>
</figure>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In the construction and evaluation of RAD-Bench, several areas warrant further exploration to enhance the benchmark’s robustness and applicability.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">First, benchmark questions are generated sequentially by prompting an LLM to deconstruct an article’s knowledge points into multiple-turn questions for Retrieval Synthesis. Similarly, solved tasks’ joint conditions are broken down into multiple-turn questions for Retrieval Reasoning. This approach allows for the generation of auditable reference answers for evaluation. Although it enables the assessment of an LLM’s ability to handle changing user intents and additional constraints, it limits the interdependence of subsequent questions on earlier answers. We propose that designing follow-up questions based on the LLM’s responses could create a tighter connection between rounds. This enhanced design would be particularly beneficial for retrieval reasoning tasks and would more accurately simulate real-life scenarios where users engage in ongoing dialogues due to dissatisfaction with initial LLM responses.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Second, expanding the diversity of questions and scenarios within RAD-Bench is crucial. Currently, the benchmark covers a range of representative scenarios; however, including a broader spectrum of contexts and more varied user intents, such as that in <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib45" title="">2024</a>)</cite>, could improve its generalizability and challenge models in different ways.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Third, refining the granularity of scoring criteria in benchmark construction could yield more nuanced selection for multi-turn questions and retrieved context. While the current scoring prompts assess relevance, progression, and clarity, more detailed aspects of model performance could be examined. For instance, scoring could be expanded to score the subtleties of context integration and the complexity of reasoning across multiple turns of the generated three-turn questions, providing deeper insights into model capabilities and limitations at evaluation time. Furthermore, addressing potential biases in scoring is essential. Notably, BreeXe, which serves as both a scorer and a model in RAD-Bench, exhibits competitive evaluation scores. This raises concerns about potential biases in the evaluation, as the scorer’s performance could influence its own evaluation outcomes. To mitigate this, incorporating multiple scoring models or scoring prompts with few-shots aligning human evaluators could help ensure a more objective assessment of generated questions.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Forth, averaging evaluation scores from multiple judge models and improving the judge prompts at evaluation time through techniques like self-discovery could offer a more comprehensive assessment. Self-discovery methods <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib44" title="">2024</a>; Yao et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.12558v1#bib.bib40" title="">2022</a>)</cite>, which emphasize iterative reasoning and adaptive problem-solving, might provide fair evaluations to how models handle dynamic and evolving questions and retrieved contexts. Implementing such techniques could enhance the evaluation framework, offering a more comprehensive view of a model’s ability to adapt and reason in complex, multi-turn dialogue following retrieval.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">Lastly, RAD-Bench assesses the capabilities of LLMs in augmented generation under the constraint that the retrieved contexts are pre-specified. This represents a constrained scenario within the broader retrieval-augmented dialogue (RAD) pipeline encountered in real-world applications. Future research aimed at benchmarking the entire end-to-end RAD pipeline may provide insights into potential areas for comprehensive system improvements.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adams et al. (2023)</span>
<span class="ltx_bibblock">
Griffin Adams, Alex Fabbri, Faisal Ladhak, Eric Lehman, and Noémie Elhadad. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.newsum-1.7" title="">From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 4th New Frontiers in Summarization Workshop</em>, pages 68–74, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2024)</span>
<span class="ltx_bibblock">
Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.acl-long.401" title="">MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 7421–7454, Bangkok, Thailand. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="">Language Models are Few-Shot Learners</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">NeurIPS</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v38i16.29728" title="">Benchmarking Large Language Models in Retrieval-Augmented Generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">AAAI</em>, pages 17754–17762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=3MW8GKNyzI" title="">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Forty-first International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI (2024)</span>
<span class="ltx_bibblock">
DeepSeek-AI. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2405.04434" title="">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">CoRR</em>, abs/2405.04434.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dom Eccleston (2024)</span>
<span class="ltx_bibblock">
Dom Eccleston. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/domeccleston/sharegpt" title="">ShareGPT</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al. (2024)</span>
<span class="ltx_bibblock">
Yann Dubois, Percy Liang, and Tatsunori Hashimoto. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=CybBmzWBX0" title="">Length-Controlled AlpacaEval: A Simple Debiasing of Automatic Evaluators</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">First Conference on Language Modeling</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Finch et al. (2022)</span>
<span class="ltx_bibblock">
Sarah E. Finch, James D. Finch, and Jinho D. Choi. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:254854340" title="">Don’t Forget Your ABC’s: Evaluating the State-of-the-Art in Chat-Oriented Dialogue Systems</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2023)</span>
<span class="ltx_bibblock">
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.04166" title="">GPTScore: Evaluate as You Desire</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2302.04166.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2024)</span>
<span class="ltx_bibblock">
Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang Chen, and Da-Shan Shiu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.02712" title="">Breeze-7B Technical Report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2403.02712 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Islam et al. (2023)</span>
<span class="ltx_bibblock">
Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.11944" title="">FinanceBench: A New Benchmark for Financial Question Answering</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2311.11944 [cs, stat].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7B</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2310.06825 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html" title="">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">NeurIPS</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024a.

</span>
<span class="ltx_bibblock">From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline.

</span>
<span class="ltx_bibblock">_eprint: 2406.11939.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://lmsys.org/blog/2024-04-19-arena-hard/" title="">From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2406.04770" title="">WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2406.04770 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.153" title="">G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 2511–2522, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Llama Team (2024)</span>
<span class="ltx_bibblock">
AI @ Meta Llama Team. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2407.21783" title="">The Llama 3 Herd of Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2407.21783 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2023)</span>
<span class="ltx_bibblock">
Hongyin Luo, Tianhua Zhang, Yung-Sung Chuang, Yuan Gong, Yoon Kim, Xixin Wu, Helen M. Meng, and James R. Glass. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=noIvPGG8P1" title="">Search Augmented Instruction Learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2024)</span>
<span class="ltx_bibblock">
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen, Yi Luo, Peng Cheng, Haiying Deng, Zhonghao Wang, and Zijia Lu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.17043" title="">CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2401.17043 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MediaTek (2024)</span>
<span class="ltx_bibblock">
MediaTek. 2024.

</span>
<span class="ltx_bibblock">MediaTek Davinci (June 13 Version) [Generative AI Platform].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitra et al. (2023)</span>
<span class="ltx_bibblock">
Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, and Ahmed Awadallah. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.11045" title="">Orca 2: Teaching Small Language Models How to Reason</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2311.11045 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mondorf and Plank (2024)</span>
<span class="ltx_bibblock">
Philipp Mondorf and Barbara Plank. 2024.

</span>
<span class="ltx_bibblock">Comparing inferential strategies of humans and large language models in deductive reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2402.14856</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">ChatGPT (June 13 Version)[Large Language Model].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=TG8KACxEON" title="">Training language models to follow instructions with human feedback</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pandya and Holia (2023)</span>
<span class="ltx_bibblock">
Keivalya Pandya and Mehfuza Holia. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.05421" title="">Automating Customer Service using LangChain: Building custom open-source GPT Chatbot for organizations</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2310.05421 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2024)</span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=wHBfxhZu1u" title="">YaRN: Efficient Context Window Extension of Large Language Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perplexity AI (2024)</span>
<span class="ltx_bibblock">
Perplexity AI. 2024.

</span>
<span class="ltx_bibblock">perplexity (June 13 Version) [Generative AI Platform].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2024)</span>
<span class="ltx_bibblock">
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=dHng2O0Jjr" title="">ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=HPuSIXJaa9" title="">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2024)</span>
<span class="ltx_bibblock">
Gemma Team. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2408.00118" title="">Gemma 2: Improving Open Language Models at a Practical Size</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2408.00118 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, and Kevin Stone. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2403.05313" title="">RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">CoRR</em>, abs/2403.05313.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=_VjQlMeSB_J" title="">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and LINGMING ZHANG. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=XUeoOBid3x" title="">Magicoder: Empowering Code Generation with OSS-Instruct</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Forty-first International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2024)</span>
<span class="ltx_bibblock">
Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=l5XQzNkAOe" title="">TravelPlanner: A Benchmark for Real-World Planning with Language Agents</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Forty-first International Conference on Machine Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2024)</span>
<span class="ltx_bibblock">
Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.11817" title="">Hallucination is Inevitable: An Innate Limitation of Large Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2401.11817 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2406.04744" title="">CRAG – Comprehensive RAG Benchmark</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2406.04744 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2022)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=tvI4u1ylcqs" title="">ReAct: Synergizing Reasoning and Acting in Language Models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">NeurIPS 2022 Foundation Models for Decision Making Workshop</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2406.04520" title="">NATURAL PLAN: Benchmarking LLMs on Natural Language Planning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2406.04520 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=uccHPGDlao" title="">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.07911" title="">Instruction-Following Evaluation for Large Language Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2311.07911 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2024)</span>
<span class="ltx_bibblock">
Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.03620" title="">Self-Discover: Large Language Models Self-Compose Reasoning Structures</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2402.03620 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2024)</span>
<span class="ltx_bibblock">
Kunlun Zhu, Yifan Luo, Dingling Xu, Ruobing Wang, Shi Yu, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2408.01262" title="">RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">arXiv preprint</em>.

</span>
<span class="ltx_bibblock">ArXiv:2408.01262 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziegler et al. (2019)</span>
<span class="ltx_bibblock">
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.

</span>
<span class="ltx_bibblock">Fine-tuning language models from human preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:1909.08593</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompts for Question Generation, Scoring, and Evaluation </h2>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.1" style="width:433.6pt;">
<div class="ltx_para" id="A1.1.p1">
<p class="ltx_p ltx_align_center" id="A1.1.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.1.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.1.p1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.1.p1.1.1.1.1">system_prompt:</span> You are an experienced writer tasked with designing a series of connected queries to guide an AI in progressively summarizing, comparing, and analyzing key points of an event or story.
The goal is to integrate new context at each step, resulting in a comprehensive summarization (TL;DR, tables, bullet points, analysis, etc.) that can cover as many key points as possible from a source article. To complete this, follow the following instructions:</span>
<span class="ltx_p" id="A1.1.p1.1.1.2">[The Start of Instruction]</span>
<span class="ltx_p" id="A1.1.p1.1.1.3">1. Identify key knowledge points in the source article that are crucial for understanding the event or story.</span>
<span class="ltx_p" id="A1.1.p1.1.1.4">2. Design the first turn query: - Decide on the final output format (e.g., TL;DR, comparison table, bullet points).</span>
<span class="ltx_p" id="A1.1.p1.1.1.5">- Specify the desired length and structure of the output (e.g., word count, number of paragraphs).</span>
<span class="ltx_p" id="A1.1.p1.1.1.6">3. Design the second and third turn query:</span>
<span class="ltx_p" id="A1.1.p1.1.1.7">- Identify additional context or background information that can enhance the initial draft.</span>
<span class="ltx_p" id="A1.1.p1.1.1.8">- Guide the AI to integrate this new information into the existing draft.</span>
<span class="ltx_p" id="A1.1.p1.1.1.9">Guide the AI to incorporate this analysis into the current draft.</span>
<span class="ltx_p" id="A1.1.p1.1.1.10">- Include relevant web search queries to gather expert opinions and analysis</span>
<span class="ltx_p" id="A1.1.p1.1.1.11">[The End of Instruction]</span>
<span class="ltx_p" id="A1.1.p1.1.1.12">Below are some important requirements you need to strictly follow when generating the three-turn question set:</span>
<span class="ltx_p" id="A1.1.p1.1.1.13">[The Start of Important Requirements]</span>
<span class="ltx_p" id="A1.1.p1.1.1.14">1. In the first turn, the query needs to guide the AI to specify what the final output should look like. (e.g., writing comparison table, writing TL;DR, bullet points, …)</span>
<span class="ltx_p" id="A1.1.p1.1.1.15">2. In the second and third turn, do not specify the output format</span>
<span class="ltx_p" id="A1.1.p1.1.1.16">3. Emphasize the continuity of the questions, prompting the AI to keep working on the current draft and adding knowledge points progressively.</span>
<span class="ltx_p" id="A1.1.p1.1.1.17">4. Avoid asking the AI to generate a whole new article in each turn</span>
<span class="ltx_p" id="A1.1.p1.1.1.18">5. Ensure the tasks are diverse, such as generating a comparison table, creating bullet points, and writing a brief analysis, rather than just writing a TL;DR.</span>
<span class="ltx_p" id="A1.1.p1.1.1.19">6. Please Strictly follow the specified output JSON format (in the end of the instruction) for the three-turn question set you come up with.</span>
<span class="ltx_p" id="A1.1.p1.1.1.20">[The End of Important Requirements]</span>
<span class="ltx_p" id="A1.1.p1.1.1.21">For the design of a set with connected questions and relevant web search queries, you can refer to the following example:</span>
<span class="ltx_p" id="A1.1.p1.1.1.22">[The Start of Examples]
{few_shot_learning_text}
[The End of Examples]</span>
<span class="ltx_p" id="A1.1.p1.1.1.23"><span class="ltx_text ltx_font_bold" id="A1.1.p1.1.1.23.1">prompt_template:</span> The following is the article you need to carefully read and generate questions for: [The Start of The Article]
{source_doc}
[The End of The Article]</span>
<span class="ltx_p" id="A1.1.p1.1.1.24">Remember in the first turn’s query, you should specify what needs to be done by the AI (the final output, e.g., TL;DR summary, comparison table, bullet points, etc.). YOU CANNOT DESIGN QUESTIONS THAT ARE SIMILAR TO QUESTIONS GENERATED IN PREVIOUS ROUNDS.
As for the final question set output format, YOU SHOULD STRICTLY FOLLOW THE FOLLOWING OUTPUT JSON FORMAT:</span>
<span class="ltx_p" id="A1.1.p1.1.1.25">[The Start of the OUTPUT JSON FORMAT]
{output_format}</span>
<span class="ltx_p" id="A1.1.p1.1.1.26">[The End of the OUTPUT JSON FORMAT]</span>
<span class="ltx_p" id="A1.1.p1.1.1.27">You need to STRICTLY FOLLOW the specified output JSON format to serve as your FINAL OUTPUT!</span>
<span class="ltx_p" id="A1.1.p1.1.1.28"><span class="ltx_text ltx_font_bold" id="A1.1.p1.1.1.28.1">output_format:</span>
[{"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The prompt to generate questions of News TLDR scenario.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.2" style="width:433.6pt;">
<div class="ltx_para" id="A1.2.p1">
<p class="ltx_p" id="A1.2.p1.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="A1.2.p1.1.1" style="border-color: #000000;padding:5.0pt;">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="A1.2.p1.1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.2.p1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.2.p1.1.1.1.1.1">system_prompt:</span> "You are an experienced writer tasked with designing a series of connected queries to guide an AI in progressively generating a draft article. The goal is to integrate new context at each step, resulting in a comprehensive final article. Each query should focus on one main aspect, ensuring the AI can build upon the previous draft with new information. Include relevant web search queries to help gather necessary information for each turn. To achieve this, follow these steps:</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.2">1. Identify several main knowledge points in the provided article.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.3">2. Group the knowledge points into three main aspects .</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.4">3. Design each query to focus on one aspect at a time, ensuring that the AI can integrate new information progressively.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.5">4. Ensure each query builds upon the previous draft, adding layers of information from different references.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.6">5. Include a list of relevant web search query, each focuses on designing a web search query that can gather necessary information the turn needs for answering correctly. The search query list should have exactly 3 queries. Output the 3 connected queries in JSON format, where each query entry should include:</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.7">1. "query": The query for the AI to generate the draft article.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.8">2. "web_search_query": A list of highly relevant web search query to find articles that can help construct the specified draft article. What needs to be noticed is that the query should only focus on one aspect at a time, and DO NOT ask questions that involves multiple actions such as summarize and compare at the same time.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.9">[Important Requirements]</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.10">1. In the first turn, the first turn’s query needs to guide the AI to specify what the final output should look like (e.g., word count, paragraph count, what needs to be done, etc.) and include the instruction to follow the specified output format. For example, the first turn’s query can start with: "I want to write an article about … The draft should be around … paragraphs, … words, etc."</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.11">2. In the second and third turn, do not specify the output format!.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.12">3. Emphasize the continuity of the questions, prompting the AI to keep working on the current draft and adding knowledge points progressively.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.13">4. Avoid asking the AI to generate a whole new article in each turn.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.14">For the design of a set with connected questions and relevant web search queries, you can refer to the following example: [The Start of Examples] {few_shot_learning_text} [The End of Examples]</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.15"><span class="ltx_text ltx_font_bold" id="A1.2.p1.1.1.1.15.1">prompt_template:</span> The following is the article you need to carefully read and generate questions for: [The Start of The Article]{source_doc}
[The End of The Article]</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.16">You should strictly follow the following output JSON format: output_format.</span>
<span class="ltx_p" id="A1.2.p1.1.1.1.17"><span class="ltx_text ltx_font_bold" id="A1.2.p1.1.1.1.17.1">output_format:</span>
[{"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}]</span>
</span>
.</span></p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The prompt to generate questions of Education scenario.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.3" style="width:433.6pt;">
<div class="ltx_para" id="A1.3.p1">
<p class="ltx_p" id="A1.3.p1.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="A1.3.p1.1.1" style="border-color: #000000;padding:5.0pt;">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="A1.3.p1.1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.3.p1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.3.p1.1.1.1.1.1">system_prompt:</span> "You are an experienced academic writer with expertise in constructing "Related Work" sections for research papers. Now given a related work’s paragraph, what you need to do is to design a series of three connected queries that will guide an AI to reconstruct the related work section progressively, integrating new context at each step to build a comprehensive final draft. In this task, you need to focus on identifying several key information points, grouping them into three main aspects, and ensuring that each query explicitly prompts the AI to expand upon a working draft "Related Work" section based on new information gathered at each step. Each query should guide the AI to build further on the previous draft, connecting the three main aspects. Additionally, for each question, identify those references that can be used to support the content by providing a list of reference_id.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.2">To achieve this, follow these steps:</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.3">1. Identify several key information points in the provided related work section.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.4">2. Group the key information points into three main aspects.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.5">3. Design each query to focus on one aspect at a time, ensuring that the AI can integrate new information progressively.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.6">4. Ensure each query builds upon the previous draft, adding layers of information from different references.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.7">5. Include a list of relevant reference_ids for each query, ensuring that the references are used to support the content and are not empty.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.8">Output the 3 connected queries in JSON format, where each query entry should include:</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.9">1. "query": The query for the AI to generate the draft "Related Work" section.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.10">2. "reference_ids": A list of reference IDs that are mentioned in the query and can be used to support the question.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.11">Please make sure you directly output the JSON format but not one query at a time.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.3.p1.1.1.1.12.1">prompt_template:</span> As an experienced academic writer specializing in education and related fields, you are tasked with designing three connected queries that will guide an AI to progressively generate a draft "Related Work" section for a research paper. Each query should build upon the previous one by integrating new context and insights, ultimately creating a comprehensive and cohesive final draft. The following article is provided as a source document for you to carefully review and design the questions:
{source_doc}</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.13">YOU CANNOT DESIGN QUESTIONS THAT ARE SIMILAR TO QUESTIONS GENERATED IN PREVIOUS ROUNDS. YOU SHOULD STRICTLY FOLLOW THE FOLLOWING OUTPUT JSON FORMAT:
{output_format}</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.14">The above output is just for your reference, you really need to carefully generate the query and corresponding reference ids list for the query ensuring these ids are all valid and existed in the given related work section. Please make sure you directly output the JSON format but not one query at a time.</span>
<span class="ltx_p" id="A1.3.p1.1.1.1.15"><span class="ltx_text ltx_font_bold" id="A1.3.p1.1.1.1.15.1">output_format:</span>
[{"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}]</span>
</span>
</span></p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The prompt to generate questions of Academic scenario.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.4" style="width:433.6pt;">
<div class="ltx_para" id="A1.4.p1">
<p class="ltx_p ltx_align_center" id="A1.4.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.4.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.4.p1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.4.p1.1.1.1.1">system_prompt:</span> You are a helpful and logical assistant specialized in finance and data analysis. Your task is to help users break down complex finance-related questions into simpler, intermediate questions that logically lead to a final question. Ensure that the answers provided are accurate and based on the given evidence text. You will be provided with information texts, and you need to generate a sequence of three questions and answers that build up to the final correct question and answer with the appropriate evidence text. For the design of the three connected follow-up questions, you can refer to the following examples:
{few_shot_learning_text}.</span>
<span class="ltx_p" id="A1.4.p1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.4.p1.1.1.2.1">prompt_template:</span>
Given the following expert-designed finance question, answer, and evidence text, think step by step and generate three questions with their answers and evidence text that can be built to lead to the final correct question and correct answer with the correct evidence text.
[The Start of the Given Document] # source_doc # [The End of the Given Document]</span>
<span class="ltx_p" id="A1.4.p1.1.1.3">You need to follow the below instructions to construct the data:</span>
<span class="ltx_p" id="A1.4.p1.1.1.4">[The Start of Instruction ]</span>
<span class="ltx_p" id="A1.4.p1.1.1.5">1. Identify Key Components: Break down the main question into its key components (e.g., time periods, specific events, financial metrics).</span>
<span class="ltx_p" id="A1.4.p1.1.1.6">2. Logical Steps: Determine the logical steps required to answer the main question. Each step should build on the previous one and lead to the final question.</span>
<span class="ltx_p" id="A1.4.p1.1.1.7">3. Generate Intermediate Questions: Create intermediate questions that address each logical step. Ensure each question is neither too easy nor too difficult and that it logically connects to the next question.</span>
<span class="ltx_p" id="A1.4.p1.1.1.8">4. Reference Evidence Text: Ensure each question can be answered using the provided evidence text. Clearly reference the part of the text that supports the answer. It has to be clear and you need to really make sure the question you propose can be answered or inferred from the support text you extracted</span>
<span class="ltx_p" id="A1.4.p1.1.1.9">5. Final Question: Use the answers from the intermediate questions to generate the final question, ensuring it matches the provided final question and answer. The final question should be the same or very similar to the provided main question to ensure it is the most difficult part</span>
<span class="ltx_p" id="A1.4.p1.1.1.10">[The End of Instruction]</span>
<span class="ltx_p" id="A1.4.p1.1.1.11">You should strictly follow the following output JSON format: {output_format}.</span>
<span class="ltx_p" id="A1.4.p1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.4.p1.1.1.12.1">output_format:</span>
[{"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}, {"query": "….", "answer": "…", "referenced_information": "…"}]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The prompt to generate questions of Finance scenario.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.5" style="width:433.6pt;">
<div class="ltx_para" id="A1.5.p1">
<p class="ltx_p" id="A1.5.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.5.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.5.p1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.5.p1.1.1.1.1">system_prompt:</span>
You are an experienced customer support agent who can handle user queries effectively by progressively narrowing down the problem and using reasoning techniques to identify the root cause. You will be provided with a user manual containing common errors and solution suggestions. Your task is to design three connected dialogue turns that simulate a user talking to a customer support agent to solve problems they encounter. Each turn should include a user question, context that supports answering the question, and a precise agent answer. The questions should progressively scope down and test the agent’s ability to reason and figure out the root cause of the user’s problem. The initial query might be broad and vague, the second turn should follow the agent’s solution but still encounter some problems, and the final turn should further narrow down the possible cause by providing new evidence. The final turn should correctly identify the problem the user encounters. To achieve this, follow these steps:</span>
<span class="ltx_p" id="A1.5.p1.1.1.2">1. Identify a common error from the user manual and its suggested solutions.</span>
<span class="ltx_p" id="A1.5.p1.1.1.3">2. Create a broad initial user query based on the common error.</span>
<span class="ltx_p" id="A1.5.p1.1.1.4">3. Design the second user query to follow up on the agent’s initial response, indicating that the initial solution did not fully resolve the issue and providing additional details or symptoms.</span>
<span class="ltx_p" id="A1.5.p1.1.1.5">4. Design the third user query to provide new findings or evidence based on the previous troubleshooting steps, leading to a more specific troubleshooting step or final resolution.</span>
<span class="ltx_p" id="A1.5.p1.1.1.6">5. Ensure each agent answer is clear, precise, and directly addresses the user’s issue.</span>
<span class="ltx_p" id="A1.5.p1.1.1.7">6. Extract the context directly from the user manual to support each answer.</span>
<span class="ltx_p" id="A1.5.p1.1.1.8">Output the three connected dialogue turns in JSON format, where each entry should include:</span>
<span class="ltx_p" id="A1.5.p1.1.1.9">1. "query": The user’s question.</span>
<span class="ltx_p" id="A1.5.p1.1.1.10">2. "context": The extracted context from the user manual that supports answering the question.</span>
<span class="ltx_p" id="A1.5.p1.1.1.11">3. "answer": The agent’s response.</span>
<span class="ltx_p" id="A1.5.p1.1.1.12"><span class="ltx_text ltx_font_bold" id="A1.5.p1.1.1.12.1">prompt_template:</span>
Here is the provided user manual:
[The Start of Manual] {source_doc} [The End of Manual]. Read it carefully and try to identify a common error and its suggested solutions. Based on this, design three connected dialogue turns that simulate a user talking to a customer support agent to solve the problem they encounter. Each turn should include a user question, context that supports answering the question, and a precise and clear agent answer. The questions should progressively scope down and test the agent’s ability to reason and figure out the root cause of the user’s problem. The initial query might be broad and vague, the second turn should follow the agent’s solution but still encounter some problems, and the final turn should further narrow down the possible cause by providing new findings or evidence. The final turn should correctly identify the problem the user encounters. Output the three connected dialogue turns in JSON format, where each entry should include:</span>
<span class="ltx_p" id="A1.5.p1.1.1.13">1. "query": The user’s question.</span>
<span class="ltx_p" id="A1.5.p1.1.1.14">2. "context": The context that supports answering the question SHOULD BE DIRECTLY EXTRACTED FROM THE USER MANUAL, WHICH IS A PIECE OF INFORMATION IN THE MANUAL. YOU NEED TO MAKE SURE THE CONTEXT IS HELPFUL FOR ANSWERING THE QUESTIONS</span>
<span class="ltx_p" id="A1.5.p1.1.1.15">3. "answer": The agent’s response.</span>
<span class="ltx_p" id="A1.5.p1.1.1.16">REMEMBER: YOU CANNOT DESIGN QUESTIONS THAT ARE SIMILAR TO QUESTIONS GENERATED IN PREVIOUS ROUNDS. IT MEANS THAT YOU HAVE TO IDENTIFY NEW PROBLEMS AND TRY TO USE THAT FOR CONSTRUCTING THE THREE TURN QUESTION SET. IN THE END, YOU SHOULD STRICTLY FOLLOW THE FOLLOWING OUTPUT JSON FORMAT:
{output_format}</span>
<span class="ltx_p" id="A1.5.p1.1.1.17">Please Make sure you really directly output the JSON format but not one query at a time!</span>
<span class="ltx_p" id="A1.5.p1.1.1.18"><span class="ltx_text ltx_font_bold" id="A1.5.p1.1.1.18.1">output_format:</span> ["query": "….", "answer": "…", "context": "…", "query": "….", "answer": "…", "context": "…", "query": "….", "answer": "…", "context": "…"]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>The prompt to generate questions of Customer Support scenario.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.6" style="width:433.6pt;">
<div class="ltx_para" id="A1.6.p1">
<p class="ltx_p" id="A1.6.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.6.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.6.p1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.6.p1.1.1.1.1">system_prompt:</span> You are a helpful and logical assistant specialized in travel planning. Your task is to help users break down complex travel-related queries into simpler, intermediate queries that logically lead to a final, more complex query. Ensure that the plans provided are accurate and based on the given reference information. You will be provided with information texts, and you need to generate a sequence of three queries that build up to the final correct query with the appropriate reference information.</span>
<span class="ltx_p" id="A1.6.p1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.6.p1.1.1.2.1">prompt_template:</span> You are a helpful and logical assistant specialized in travel planning. Your task is to help users break down complex travel-related queries into simpler, intermediate queries that logically lead to a final, more complex query. Ensure that the plans provided are accurate and based on the given reference information. You will be provided with information texts, and you need to generate a sequence of three queries that build up to the final correct query with the appropriate reference information. You will be given the original complex query and corresponding annotated constraints. What you need to do is to generate a three-turn question set starting from basic requirements, progressively adding constraints to build up to the final turn containing all constraints. Each query should build on the previous one without repeating the requirements already mentioned. Each query should prompt the AI to generate a complete plan based on the given constraints. The queries should be natural and conversational, just like a user talking to a travel agent. You have to strictly follow the output format: {output_format}</span>
<span class="ltx_p" id="A1.6.p1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.6.p1.1.1.3.1">output_format:</span>
["query": "….", "constraints": ,"query": "….", "constraints": ,"query": "….", "constraints": 
]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F13">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The prompt to generate questions of Travel Planning scenario.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.7" style="width:433.6pt;">
<div class="ltx_para" id="A1.7.p1">
<p class="ltx_p" id="A1.7.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.7.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.7.p1.1.1.1">Please act as an impartial judge and evaluate the quality of the generated three-turn question set based on the source document provided. Your evaluation should consider factors such as relevance, progression, clarity, support, and knowledge points. The explanation of these factors are given below:</span>
<span class="ltx_p" id="A1.7.p1.1.1.2">- Relevance: How closely the questions align with the source document and the task prompt</span>
<span class="ltx_p" id="A1.7.p1.1.1.3">- Progression: How well each question builds upon the previous one to add new layers of information.</span>
<span class="ltx_p" id="A1.7.p1.1.1.4">- Clarity: The clarity and unambiguity of the questions</span>
<span class="ltx_p" id="A1.7.p1.1.1.5">- Support: The relevance and utility of the suggested web search queries or reference IDs</span>
<span class="ltx_p" id="A1.7.p1.1.1.6">- Knowledge Points: How well the key information retrieved from the specified web search queries can be utilized in the questions.</span>
<span class="ltx_p" id="A1.7.p1.1.1.7">- Medium Complexity: The question needs to be focused and do not involve too many perspectives in one time!! Simply to say, a good question should focus on certain aspects but never cover too many knowledge points. That is to say, if a question covers too many topics, aspects at a time, you should see this as a question that is too difficult and deduct some points.</span>
<span class="ltx_p" id="A1.7.p1.1.1.8">Now carefully review the source document provided and the answer generated:</span>
<span class="ltx_p" id="A1.7.p1.1.1.9">[The Start of Original Article]
{reference}
[The End of Original Article]</span>
<span class="ltx_p" id="A1.7.p1.1.1.10">[The Start of Three-Turn Question Set to be evaluated]: {answer} [The End of Three-Turn Question Set to be evaluated]</span>
<span class="ltx_p" id="A1.7.p1.1.1.11">Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your careful and comprehensive explanation, you must rate the question set on a scale of 1 to 5 by strictly following this format: "&lt;FINAL&gt;[[rating]]&lt;/FINAL&gt;", for example: "Rating: &lt;FINAL&gt;[[4]]&lt;/FINAL&gt;"</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F14">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The prompt for the scoring candidates.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.8" style="width:433.6pt;">
<div class="ltx_para" id="A1.8.p1">
<p class="ltx_p" id="A1.8.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.8.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.8.p1.1.1.1">[Instruction]</span>
<span class="ltx_p" id="A1.8.p1.1.1.2">Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.</span>
<span class="ltx_p" id="A1.8.p1.1.1.3">Your evaluation should consider helpfulness and Informativeness:</span>
<span class="ltx_p" id="A1.8.p1.1.1.4">[Helpfulness]</span>
<span class="ltx_p" id="A1.8.p1.1.1.5">you should evaluate the helpfulness of the assistant’s answer to the question of current turn.</span>
<span class="ltx_p" id="A1.8.p1.1.1.6">[Informativeness]</span>
<span class="ltx_p" id="A1.8.p1.1.1.7">You are given the assistant’s answer and reference knowledge points representing knowledge that should be mentioned, discussed, and covered in the assistant’s answer. You should evaluate how informativeness the assistant’s answer is in including the reference knowledge points appropriately.</span>
<span class="ltx_p" id="A1.8.p1.1.1.8">Begin your evaluation by comparing the assistant’s answer with the reference knowledge points. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example: "Rating: [[5]]".</span>
<span class="ltx_p" id="A1.8.p1.1.1.9">[Question]</span>
<span class="ltx_p" id="A1.8.p1.1.1.10">{question}</span>
<span class="ltx_p" id="A1.8.p1.1.1.11">[End of Question]</span>
<span class="ltx_p" id="A1.8.p1.1.1.12">[The Start of Reference Knowledge Points]</span>
<span class="ltx_p" id="A1.8.p1.1.1.13">{reference}</span>
<span class="ltx_p" id="A1.8.p1.1.1.14">[The End of Reference Knowledge Points]</span>
<span class="ltx_p" id="A1.8.p1.1.1.15">[The Start of Assistant’s Answer]</span>
<span class="ltx_p" id="A1.8.p1.1.1.16">{answer}</span>
<span class="ltx_p" id="A1.8.p1.1.1.17">[The End of Assistant’s Answer]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F15">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Prompt for evaluating the first turn of a scenario in Retrieval Synthesis.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.9" style="width:433.6pt;">
<div class="ltx_para" id="A1.9.p1">
<p class="ltx_p" id="A1.9.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.9.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.9.p1.1.1.1">[Instruction]</span>
<span class="ltx_p" id="A1.9.p1.1.1.2">Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below.</span>
<span class="ltx_p" id="A1.9.p1.1.1.3">Your evaluation should assess the helpfulness, coherence, adherence, and informativeness:</span>
<span class="ltx_p" id="A1.9.p1.1.1.4">[Helpfulness] you should evaluate the helpfulness of the assistant’s answer to the question of current turn.</span>
<span class="ltx_p" id="A1.9.p1.1.1.5">[Informativeness] You are given the assistant’s answer and reference knowledge points representing knowledge that should be mentioned, discussed, and covered in the assistant’s answer. You should evaluate how informativeness the assistant’s answer is in including the reference knowledge points appropriately.</span>
<span class="ltx_p" id="A1.9.p1.1.1.6">[Adherence] You are given question of the previous turn. Consider how well the assistant’s answer respects the user intents throughout the turns.</span>
<span class="ltx_p" id="A1.9.p1.1.1.7">[Coherence] you are given the user questions and reference knowledge points in the previous turns to serve as previous instructions. You should consider how well the assistant’s answer aligns with the knowledge points mentioned in the current turn’s reference knowledge points and how it respects or builds upon the focus and knowledge points from the previous turns.</span>
<span class="ltx_p" id="A1.9.p1.1.1.8">Begin your evaluation by comparing the assistant’s answer against the reference knowledge points from both previous and current turns. Be as objective as possible, and provide a detailed justification for your rating. After providing your explanation, you must rate the response on a scale of 1 to 10, strictly following this format: "Rating: [[rating]]", for example: "Rating: [[5]]".</span>
<span class="ltx_p" id="A1.9.p1.1.1.9">[The Start of Previous Questions and Reference Knowledge Points]</span>
<span class="ltx_p" id="A1.9.p1.1.1.10">Question: {question_1}</span>
<span class="ltx_p" id="A1.9.p1.1.1.11">Reference Knowledge Points: {reference_1}</span>
<span class="ltx_p" id="A1.9.p1.1.1.12">[The End of Previous Questions and Reference Knowledge Points]</span>
<span class="ltx_p" id="A1.9.p1.1.1.13">[The Start of Current Turn Question]</span>
<span class="ltx_p" id="A1.9.p1.1.1.14">{question}</span>
<span class="ltx_p" id="A1.9.p1.1.1.15">[The End of Current Turn Question]</span>
<span class="ltx_p" id="A1.9.p1.1.1.16">[The Start of Reference Knowledge Points]</span>
<span class="ltx_p" id="A1.9.p1.1.1.17">{reference}</span>
<span class="ltx_p" id="A1.9.p1.1.1.18">[The End of Reference Knowledge Points]</span>
<span class="ltx_p" id="A1.9.p1.1.1.19">[The Start of Assistant’s Answer]</span>
<span class="ltx_p" id="A1.9.p1.1.1.20">{answer}</span>
<span class="ltx_p" id="A1.9.p1.1.1.21">[The End of Assistant’s Answer]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F16">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Prompt for evaluating the second turn of a scenario in Retrieval Synthesis.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.10" style="width:433.6pt;">
<div class="ltx_para" id="A1.10.p1">
<p class="ltx_p ltx_align_center" id="A1.10.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.10.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.10.p1.1.1.1">[Instruction]</span>
<span class="ltx_p" id="A1.10.p1.1.1.2">Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should assess the correctness, helpfulness. Your evaluation should focus on the assistant’s answer to the question of current turn. You also need to evaluate the adherence of the assistant’s answer to previous instructions. You will be given the assistant’s answer and a reference answer. You will also be given the user questions and reference knowledge points in the previous turns to serve as previous instructions. You should consider how well the assistant’s answer captures the key information, knowledge points mentioned in the reference answer and how it respects or builds upon the focus and knowledge points from the previous turns.</span>
<span class="ltx_p" id="A1.10.p1.1.1.3">Your evaluation should assess the helpfulness, coherence, adherence, and informativeness:</span>
<span class="ltx_p" id="A1.10.p1.1.1.4">[Helpfulness]</span>
<span class="ltx_p" id="A1.10.p1.1.1.5">you should evaluate the helpfulness of the assistant’s answer to the question of current turn.</span>
<span class="ltx_p" id="A1.10.p1.1.1.6">[Informativeness]</span>
<span class="ltx_p" id="A1.10.p1.1.1.7">You are given the assistant’s answer and reference knowledge points representing knowledge that should be mentioned, discussed, and covered in the assistant’s answer. You should evaluate how informativeness the assistant’s answer is in including the reference knowledge points appropriately.</span>
<span class="ltx_p" id="A1.10.p1.1.1.8">[Adherence]</span>
<span class="ltx_p" id="A1.10.p1.1.1.9">You are given questions of the previous turns. Consider how well the assistant’s answer respects the user intents throughout the turns.</span>
<span class="ltx_p" id="A1.10.p1.1.1.10">[Coherence]</span>
<span class="ltx_p" id="A1.10.p1.1.1.11">you are given the user questions and reference knowledge points in the previous turns to serve as previous instructions. You should consider how well the assistant’s answer aligns with the knowledge points mentioned in the current turn’s reference knowledge points and how it respects or builds upon the focus and knowledge points from the previous turns.</span>
<span class="ltx_p" id="A1.10.p1.1.1.12">Begin your evaluation by comparing the assistant’s answer against the reference answer in this turn and reference knowledge points in previous turns. Be as objective as possible, and provide a detailed justification for your rating. After providing your explanation, you must rate the response on a scale of 1 to 10, strictly following this format: "Rating: [[rating]]," for example: "Rating: [[5]]".</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F17">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Prompt for evaluating the final turn of a scenario in Retrieval Synthesis.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.11" style="width:433.6pt;">
<div class="ltx_para" id="A1.11.p1">
<p class="ltx_p" id="A1.11.p1.1"><span class="ltx_text ltx_framed ltx_framed_rectangle" id="A1.11.p1.1.1" style="border-color: #000000;padding:5.0pt;">
<span class="ltx_inline-block ltx_parbox ltx_align_middle" id="A1.11.p1.1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.11.p1.1.1.1.1">[Instruction]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.2">Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness, helpfulness, and reasoning correctness. Additionally, you need to assess how effectively the assistant utilizes the given context to generate its response. The assistant’s answer should align with the provided context and avoid any factual inaccuracies or hallucinations that cannot be inferred from the given context. You will be given a reference answer representing a correct response, context the assistant needs to utilize and the assistant’s answer. Begin your evaluation by comparing the assistant’s answer with the reference answer and considering its adherence to the context.</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.3">Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: "Rating: [[rating]]", for example: "Rating: [[5]]".</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.4">[Question]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.5">{question}</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.6">[The Start of Context]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.7">{context}</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.8">[The End of Context]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.9">[The Start of Reference Answer]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.10">{reference}</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.11">[The End of Reference Answer]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.12">[The Start of Assistant’s Answer]</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.13">{answer}</span>
<span class="ltx_p" id="A1.11.p1.1.1.1.14">[The End of Assistant’s Answer]</span>
</span>
</span></p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F18">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Prompt for evaluating the first turn of a scenario in Retrieval Reasoning.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.12" style="width:433.6pt;">
<div class="ltx_para" id="A1.12.p1">
<p class="ltx_p" id="A1.12.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.12.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.12.p1.1.1.1">Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the question of current turn displayed below. Your evaluation should consider correctness, helpfulness, and reasoning correctness. Additionally, assess how effectively the assistant utilizes the given context and adheres to constraints from both the first and the current turn to generate its response. The assistant’s answer should align with the provided context from current turn and avoid any factual inaccuracies or hallucinations that cannot be inferred from the given context. You will be given a conversation history in previous turns to evaluate the adherence of the assistant’s answer in the current turn. You will also be given a reference answer representing a correct response, context the assistant needs to utilize and the assistant’s answer. Begin your evaluation by comparing the assistant’s answer with the reference answers from both turns and considering its adherence to the context and logical progression.</span>
<span class="ltx_p" id="A1.12.p1.1.1.2">Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: "Rating: [[rating]]", for example: "Rating: [[5]]".</span>
<span class="ltx_p" id="A1.12.p1.1.1.3">[The Start of Original Article]</span>
<span class="ltx_p" id="A1.12.p1.1.1.4">{reference}</span>
<span class="ltx_p" id="A1.12.p1.1.1.5">[The End of Original Article]</span>
<span class="ltx_p" id="A1.12.p1.1.1.6">[The Start of The Conversation History]</span>
<span class="ltx_p" id="A1.12.p1.1.1.7">User: {question_1}</span>
<span class="ltx_p" id="A1.12.p1.1.1.8">Assistant’s Answer: {reference_1}</span>
<span class="ltx_p" id="A1.12.p1.1.1.9">User: {question_2}</span>
<span class="ltx_p" id="A1.12.p1.1.1.10">Assistant’s Answer: {reference_2}</span>
<span class="ltx_p" id="A1.12.p1.1.1.11">[The End of The Conversation History]</span>
<span class="ltx_p" id="A1.12.p1.1.1.12">[The Start of Current Turn Question]</span>
<span class="ltx_p" id="A1.12.p1.1.1.13">{question}</span>
<span class="ltx_p" id="A1.12.p1.1.1.14">[The End of Current Turn Question]</span>
<span class="ltx_p" id="A1.12.p1.1.1.15">[The Start of Current Turn Context]</span>
<span class="ltx_p" id="A1.12.p1.1.1.16">{context}</span>
<span class="ltx_p" id="A1.12.p1.1.1.17">[The End of Current Turn Context]</span>
<span class="ltx_p" id="A1.12.p1.1.1.18">[The Start of Current Turn Reference Answer]</span>
<span class="ltx_p" id="A1.12.p1.1.1.19">{reference}</span>
<span class="ltx_p" id="A1.12.p1.1.1.20">[The End of Current Turn Reference Answer]</span>
<span class="ltx_p" id="A1.12.p1.1.1.21">[The Start of Assistant’s Answer]</span>
<span class="ltx_p" id="A1.12.p1.1.1.22">{answer}</span>
<span class="ltx_p" id="A1.12.p1.1.1.23">[The End of Assistant’s Answer]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F19">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Prompt for evaluating the second turn of a scenario in Retrieval Reasoning.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_logical-block ltx_minipage ltx_align_middle" id="A1.13" style="width:433.6pt;">
<div class="ltx_para" id="A1.13.p1">
<p class="ltx_p ltx_align_center" id="A1.13.p1.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="A1.13.p1.1.1" style="width:390.3pt;">
<span class="ltx_p" id="A1.13.p1.1.1.1">Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the question of current turn displayed below. Your evaluation should consider correctness, helpfulness, and reasoning correctness. Additionally, assess how effectively the assistant utilizes the given context and adheres to constraints from both the first and the current turn to generate its response. The assistant’s answer should align with the provided context from current turn and avoid any factual inaccuracies or hallucinations that cannot be inferred from the given context. You will be given a conversation history in previous turns to evaluate the adherence of the assistant’s answer in the current turn. You will also be given a reference answer representing a correct response, context the assistant needs to utilize and the assistant’s answer. Begin your evaluation by comparing the assistant’s answer with the reference answers from both turns and considering its adherence to the context and logical progression.</span>
<span class="ltx_p" id="A1.13.p1.1.1.2">Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: "Rating: [[rating]]", for example: "Rating: [[5]]".</span>
<span class="ltx_p" id="A1.13.p1.1.1.3">[The Start of Original Article]</span>
<span class="ltx_p" id="A1.13.p1.1.1.4">{reference}</span>
<span class="ltx_p" id="A1.13.p1.1.1.5">[The End of Original Article]</span>
<span class="ltx_p" id="A1.13.p1.1.1.6">[The Start of The Conversation History]</span>
<span class="ltx_p" id="A1.13.p1.1.1.7">User: {question_1}</span>
<span class="ltx_p" id="A1.13.p1.1.1.8">Assistant’s Answer: {reference_1}</span>
<span class="ltx_p" id="A1.13.p1.1.1.9">User: {question_2}</span>
<span class="ltx_p" id="A1.13.p1.1.1.10">Assistant’s Answer: {reference_2}</span>
<span class="ltx_p" id="A1.13.p1.1.1.11">[The End of The Conversation History]</span>
<span class="ltx_p" id="A1.13.p1.1.1.12">[The Start of Current Turn Question]</span>
<span class="ltx_p" id="A1.13.p1.1.1.13">{question}</span>
<span class="ltx_p" id="A1.13.p1.1.1.14">[The End of Current Turn Question]</span>
<span class="ltx_p" id="A1.13.p1.1.1.15">[The Start of Current Turn Context]</span>
<span class="ltx_p" id="A1.13.p1.1.1.16">{context}</span>
<span class="ltx_p" id="A1.13.p1.1.1.17">[The End of Current Turn Context]</span>
<span class="ltx_p" id="A1.13.p1.1.1.18">[The Start of Current Turn Reference Answer]</span>
<span class="ltx_p" id="A1.13.p1.1.1.19">{reference}</span>
<span class="ltx_p" id="A1.13.p1.1.1.20">[The End of Current Turn Reference Answer]</span>
<span class="ltx_p" id="A1.13.p1.1.1.21">[The Start of Assistant’s Answer]</span>
<span class="ltx_p" id="A1.13.p1.1.1.22">{answer}</span>
<span class="ltx_p" id="A1.13.p1.1.1.23">[The End of Assistant’s Answer]</span>
</span>
</p>
</div>
<figure class="ltx_figure ltx_align_center" id="A1.F20">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Prompt for evaluating the final turn of a scenario in Retrieval Reasoning.</figcaption>
</figure>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A2" lang="en">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Multi-turn dialogues examples for different scenarios</h2>
<figure class="ltx_figure" id="A2.F21">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F21.1">0=<span class="ltx_ERROR undefined" id="A2.F21.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F21.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F21.2">
<tr class="ltx_tr" id="A2.F21.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F21.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.1.1.1">
<span class="ltx_p" id="A2.F21.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.1.1.1.1.1">Question 1:</span> Help me write a related work section. Begin by discussing the trend of porting various data structures to become GPU-resident in recent years. Explain the general approach of optimizing memory layout and access patterns to fit the underlying GPU architecture. Mention some examples of GPU-resident indexes, such as hash tables, bloom filters, and quotient filters, and their typical use cases.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F21.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.2.1.1">
<span class="ltx_p" id="A2.F21.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.2.1.1.1.1">Context 1:</span> GPU hash tables are useful for interactive graphics applications… Properties such as an amortized constant time complexity for insertion and querying as well as a compact memory layout make them versatile associative data structures with manifold applications… More recent approaches including cuDF [8]…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F21.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.3.1.1">
<span class="ltx_p" id="A2.F21.2.3.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.3.1.1.1.1">Assistant:</span> In recent years, there has been a growing interest in adapting and optimizing various data structures to take advantage of the massive parallelism and fast memory interfaces available on modern GPUs…
Hash tables, in particular, have been widely used in various applications…
Several approaches have been proposed to accelerate hash map construction on GPUs…
In summary, the trend of porting various data structures to become GPU-resident has been driven by the need for accelerated data processing pipelines and the optimization of memory layout and access patterns to fit the underlying GPU architecture…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F21.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.4.1.1">
<span class="ltx_p" id="A2.F21.2.4.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.4.1.1.1.1">Question 2:</span> Next, expand the draft and focus on GPU-resident tree structures, such as radix trees and comparison-based trees. Highlight their capabilities, including range-lookup support. Mention that while the evaluation includes a state-of-the-art comparison-based tree B+, no code for the radix tree is available.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F21.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.5.1.1">
<span class="ltx_p" id="A2.F21.2.5.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.5.1.1.1.1">Context 2:</span> Application-specific data structures, such as acceleration tree data structures used in ray tracing [31], may also be useful for general-purpose tasks… Furthermore, B-Tree in- sertions are also faster than LSM and sorted array insertions…Particularly challenging is the development of dynamic (mu- table) data structures that can be built, queried, and updated on the GPU…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F21.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.6.1.1">
<span class="ltx_p" id="A2.F21.2.6.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.6.1.1.1.1">Assistant:</span> In the realm of GPU-resident tree structures, two main categories can be distinguished: radix trees and comparison-based trees…
Radix trees, also known as prefix trees or Patricia tries, are a versatile data structure used for storing a set of strings, providing efficient lookups and range queries…
Comparison-based trees, such as B-trees, red-black trees, and B+ trees, are another type of GPU-resident tree structure…
It is important to note that while our evaluation includes a state-of-the-art comparison-based tree B+, no code for the radix tree is available…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F21.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.7.1.1">
<span class="ltx_p" id="A2.F21.2.7.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.7.1.1.1.1">Question 3:</span> Finish the draft by discussing GPU-resident spatial indexes, such as R-Trees, and their potential benefits for constructing bounding volumes. Note the absence of available code for these indexes, which limits their use as baselines for comparisons.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F21.2.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.8.1.1">
<span class="ltx_p" id="A2.F21.2.8.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.8.1.1.1.1">Context 3:</span> An R-tree is a data structure for organizing and querying multi-dimensional non-uniform and overlapping data… R-Trees are popular spatial indexing techniques that have been widely adopted in many geospatial applications…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F21.2.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F21.2.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F21.2.9.1.1">
<span class="ltx_p" id="A2.F21.2.9.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F21.2.9.1.1.1.1">Assistant:</span> In conclusion, the development of GPU-resident spatial indexes, such as R-Trees, holds great potential for accelerating various computational tasks that involve the processing of multi-dimensional non-uniform and overlapping data…
R-Trees, in particular, have been demonstrated to deliver significant performance improvements when implemented on GPUs…
However, it is important to note that the absence of available code for these GPU-resident indexes, such as R-Trees, limits their use as baselines for comparisons…
As society continues to rely on geospatial applications and the demand for high-performance, GPU-accelerated applications grows, the development of efficient and adaptable GPU-resident spatial indexes is of paramount importance…</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Multi-turn dialogues example for Academic scenario</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A2.F22">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F22.1">0=<span class="ltx_ERROR undefined" id="A2.F22.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F22.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F22.2">
<tr class="ltx_tr" id="A2.F22.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F22.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.1.1.1">
<span class="ltx_p" id="A2.F22.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.1.1.1.1.1">Question 1:</span> Write a detailed article about the Chicago Teachers Union’s approach towards climate change in their contract negotiations. The article should be approximately 3-4 paragraphs long. Start by explaining the background of the CTU and their previous successes in negotiating contracts. Then, describe Stacy Davis Gates’ strategy of ’bargaining for the common good,’ including climate policies like installing solar panels and heat pumps in schools. Finally, touch upon the opposition’s viewpoints and the potential impact of the CTU’s efforts on the city and state budget.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F22.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F22.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.2.1.1">
<span class="ltx_p" id="A2.F22.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.2.1.1.1.1">Context 1:</span> The president of the Chicago Teachers Union explains how climate change became a pillar of the union’s contract demands… Stacy Davis Gates, president of the CTU since 2022, sees both of those strategies as pillars of winning a better contract for teachers. Climate impacts like extreme heat are becoming a classroom issue… Some opponents of the union say its climate proposals are an example of overreach, citing Davis Gates’ recent comments that the teachers contract would cost the city “$50 billion and three cents…”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F22.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F22.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.3.1.1">
<span class="ltx_p" id="A2.F22.2.3.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.3.1.1.1.1">Assistant:</span> The Chicago Teachers Union (CTU) recently made headlines for its innovative approach to contract negotiations, focusing on climate change… This shift in priority is a testament to the union’s commitment… Under the leadership of President Stacy Davis Gates, the CTU has taken its mission a step further… In the case of the CTU, its climate change proposals include installing solar panels and heat pumps in schools… The CTU’s recent contract negotiations have put these proposals front and center, a move that highlights the union’s determination… The CTU’s previous successes in negotiating contracts… have demonstrated its ability to influence policy and secure funding for its members and communities… the CTU is setting a powerful example for other unions to follow, demonstrating that labor activism can and must go beyond traditional demands to address the broader social and environmental issues…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F22.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F22.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.4.1.1">
<span class="ltx_p" id="A2.F22.2.4.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.4.1.1.1.1">Question 2:</span> Expand your previously written article by providing more details about the specific climate policies proposed by the CTU. Discuss the importance of addressing climate issues within educational institutions and explain how implementing these policies could benefit both students and the wider community. Also, explore the potential challenges faced by the CTU in achieving their goals due to financial constraints and political opposition.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F22.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F22.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.5.1.1">
<span class="ltx_p" id="A2.F22.2.5.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.5.1.1.1.1">Context 2:</span> …The UN Framework Convention on Climate Change (UNFCCC) assigns responsibility to Parties of the Convention to undertake educational and public awareness campaigns on climate change… UNICEF has tapped into the minds and imaginations of children around the world… Through its Climate Change Education for Sustainable Development programme, UNESCO aims to “help people understand the impact of global warming today and increase "climate literacy" among young people.” This programme and other innovative educational initiatives, including the Global Action Programme (GAP)… were presented and discussed at the COP 22…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F22.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F22.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.6.1.1">
<span class="ltx_p" id="A2.F22.2.6.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.6.1.1.1.1">Assistant:</span> To expand on the Chicago Teachers Union (CTU)’s approach towards climate change in their contract negotiations, it’s essential to delve into the specific climate proposals put forth by the union.
Among the CTU’s climate proposals,… By reducing the carbon footprint of schools, the CTU aims to create a more sustainable and healthier learning environment for students … Furthermore, incorporating climate education and awareness into the curriculum will empower the next generation with the knowledge and skills to tackle climate change…
One significant barrier is the potential financial constraints faced by the city and state budget, given the district’s nearly $400 million deficit… In addition to financial challenges, the CTU’s climate proposals have faced political opposition. Critics argue that the union’s demands are an example of overreach… Despite these challenges, the CTU remains committed to its climate proposals and their potential benefits for students, staff, and the wider community… the CTU is leading the way in demonstrating how labor unions can be instrumental in driving meaningful change…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F22.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F22.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F22.2.7.1.1">
<span class="ltx_p" id="A2.F22.2.7.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F22.2.7.1.1.1.1">Question 3:</span> Finish your article by analyzing the broader implications of the CTU’s climate-focused negotiations. Consider the potential ripple effects on other labor unions, local governments, and the overall push for climate action across the United States. Additionally, provide insight into the possible outcomes of the ongoing contract negotiations between the CTU and the city of Chicago, taking into account the power dynamics and the influence of the newly elected Mayor Brandon Johnson.</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Multi-turn dialogues example for Education scenario - part 1</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A2.F23">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F23.1">0=<span class="ltx_ERROR undefined" id="A2.F23.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F23.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F23.2">
<tr class="ltx_tr" id="A2.F23.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F23.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F23.2.1.1.1">
<span class="ltx_p" id="A2.F23.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F23.2.1.1.1.1.1">Context 3:</span> … on average, the 17 U.S. states with the highest union densities: have state minimum wages that are on average 19% higher than the national average and 40% higher than those in low-union-density states have median annual incomes $6,000 higher than the national average have higher-than-average unemployment insurance recipiency rates… Climate Action Is a Labor Issue for This Teachers’ Union’s Leaders… The president of the Chicago Teachers Union explains how climate change became a pillar of the union’s contract demands… The union has already shored up its position by helping to elect Brandon Johnson, a former CTU organizer, as mayor of Chicago… Our union doubles down in a way that most unions don’t. We changed our constitution to reflect social, racial and economic justice… CTU wants these talks to take place in public… This is our reciprocity to the communities that are already working on these issues…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F23.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F23.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F23.2.2.1.1">
<span class="ltx_p" id="A2.F23.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F23.2.2.1.1.1.1">Assistant:</span> The Chicago Teachers Union’s (CTU) climate-focused negotiations represent a significant shift in the labor movement’s approach to addressing environmental concerns and climate change. By prioritizing climate policies in their contract proposals, the CTU is sending a powerful message to labor unions across the country that environmental justice is an integral part of workers’ rights and well-being.</span>
<span class="ltx_p" id="A2.F23.2.2.1.1.2">The potential ripple effects of the CTU’s approach on other labor unions are profound. As a leading force in the labor movement, the CTU’s actions could inspire other unions to follow suit, incorporating climate policies into their contract negotiations. This could lead to a broader push for environmental justice and climate action within the labor sector, as unions recognize the interconnected nature of labor rights and environmental sustainability.</span>
<span class="ltx_p" id="A2.F23.2.2.1.1.3">Furthermore, the CTU’s climate-focused negotiations may also influence local governments and policymakers to prioritize climate action within their communities. By demonstrating the potential for positive change through contract negotiations, the CTU is providing a model for other cities and states to follow, highlighting the importance of addressing climate change and environmental justice within public policy.</span>
<span class="ltx_p" id="A2.F23.2.2.1.1.4">The ongoing contract negotiations between the CTU and the city of Chicago are set against a backdrop of financial constraints and political dynamics. With a nearly $400 million deficit, the city faces significant challenges in meeting the CTU’s demands. However, the election of Mayor Brandon Johnson, a former CTU organizer, may provide an opportunity for the union to secure a more favorable outcome. Johnson’s support for the CTU and its climate-focused negotiations could lead to a more collaborative approach to contract negotiations, potentially resulting in the inclusion of climate policies within the final agreement.</span>
<span class="ltx_p" id="A2.F23.2.2.1.1.5">Ultimately, the CTU’s climate-focused negotiations may serve as a turning point for labor unions and environmental activism. By combining the power of organized labor with the fight for climate action, the CTU is demonstrating that environmental justice is not just an issue for environmentalists but also a priority for the labor movement. The outcome of the ongoing contract negotiations could set a precedent for future labor contracts, encouraging other unions to adopt similar approaches and pushing local governments to prioritize climate action within their communities.</span>
<span class="ltx_p" id="A2.F23.2.2.1.1.6">In conclusion, the Chicago Teachers Union’s climate-focused negotiations represent a significant turning point in the labor movement’s approach to addressing climate change and environmental justice. By incorporating climate policies into their contract proposals, the CTU is setting a powerful example for other unions and local governments to follow. As contract negotiations continue, it remains to be seen whether the city of Chicago will rise to the challenge and prioritize climate action within its public policies. However, the CTU’s innovative approach to labor negotiations has already made a significant impact on the labor movement and the broader fight for climate action across the United States.</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Multi-turn dialogues example for Education scenario - part 2</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A2.F24">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F24.1">0=<span class="ltx_ERROR undefined" id="A2.F24.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F24.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F24.2">
<tr class="ltx_tr" id="A2.F24.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F24.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.1.1.1">
<span class="ltx_p" id="A2.F24.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.1.1.1.1.1">Question 1:</span> Write an article about the challenges faced by couples in Ukraine due to ongoing conflict and conscription policies. The article should begin with a description of Serhiy and Tania’s wedding experience and highlight the impact of conscription on their lives. Provide statistics regarding casualties and the number of police officers in Ukraine. Also, describe the experiences of their friends and relatives who fear being conscripted. The draft should be approximately 3-4 paragraphs.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F24.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.2.1.1">
<span class="ltx_p" id="A2.F24.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.2.1.1.1.1">Context 1:</span> Ukrainian president signs controversial law to boost conscription to fend off Russia’s aggression… Zelensky says 31,000 troops killed since Russia’s full-scale invasion…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F24.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.3.1.1">
<span class="ltx_p" id="A2.F24.2.3.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.3.1.1.1.1">Assistant:</span> Title: Love Amidst War: The Struggles of Couples in Ukraine</span>
<span class="ltx_p" id="A2.F24.2.3.1.1.2">In the midst of the ongoing conflict in Ukraine, love and marriage have taken on new dimensions. The Russia-Ukraine war, now in its third year, has not only disrupted the lives of millions but also altered the experiences of couples trying to get married amidst the chaos…
Serhiy, a 27-year-old IT professional, and Tania, a 25-year-old teacher, had been planning their wedding for months…For Serhiy and Tania, the decision to marry was not taken lightly. Their friends and family members, many of whom have lost loved ones in the conflict or have been directly affected by the ongoing war… In spite of the challenges, love and hope continue to persevere in Ukraine. Serhiy and Tania, along with countless other couples, are determined to build a life together in the face of adversity.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F24.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.4.1.1">
<span class="ltx_p" id="A2.F24.2.4.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.4.1.1.1.1">Question 2:</span> Continue the article by exploring the reasons behind the reluctance of Ukrainian citizens to join the military despite the ongoing conflict. Discuss the imbalance between the number of police officers and the general population, and provide perspectives from individuals like Maksym who choose not to participate in the fighting. Additionally, explain the consequences of evading conscription and the measures taken by authorities to enforce compliance. The draft should be approximately 2-3 paragraphs.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F24.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.5.1.1">
<span class="ltx_p" id="A2.F24.2.5.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.5.1.1.1.1">Context 2:</span> … We first saw millions of Ukrainians flee the country when the Russians invaded Ukraine in February this year. Now there are reports of hundreds of thousands of Russians fleeing their country in order to avoid Russia’s first mobilisation since the second world war… Racism, religious discrimination and homophobia are endemic to the Russian military, resulting in targeted mistreatment…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F24.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.6.1.1">
<span class="ltx_p" id="A2.F24.2.6.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.6.1.1.1.1">Assistant:</span> The ongoing conflict in Ukraine has taken a heavy toll on the country’s population, with millions of people displaced and thousands killed… Firstly, the fear of death or injury is a significant factor that deters many Ukrainians from entering the military… Secondly, the economic implications of joining the military are also a deterrent for many Ukrainians… Furthermore, a lack of trust in the government is another reason that some Ukrainians choose not to join the military… With over 31,000 Ukrainian soldiers killed during the conflict, the police force remains one of the country’s most stable institutions… The imbalance between the number of police officers and the general population underscores the challenges that the Ukrainian government faces in mobilizing its citizens for the ongoing conflict…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F24.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.7.1.1">
<span class="ltx_p" id="A2.F24.2.7.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.7.1.1.1.1">Question 3:</span> Finish the article by addressing the emotional toll that the conflict takes on families like Serhiy and Tania’s. Delve into the psychological effects of losing loved ones in combat and the fears associated with potential conscription. Offer insight into how these factors contribute to the difficulties faced by couples trying to maintain normalcy amidst the chaos of war. The draft should be approximately 2-3 paragraphs.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F24.2.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.8.1.1">
<span class="ltx_p" id="A2.F24.2.8.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.8.1.1.1.1">Context 3:</span> Killing in War Leaves Veterans with Lasting Psychological Scars…  Differential associations were found in child functioning when military-related variables (e.g., service component), sociodemographic, and family composition factors (number and age of the children in the home) were examined…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F24.2.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F24.2.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F24.2.9.1.1">
<span class="ltx_p" id="A2.F24.2.9.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F24.2.9.1.1.1.1">Assistant:</span> The ongoing conflict in Ukraine has taken a significant emotional toll on families like Serhiy and Tania’s, as well as their friends and loved ones… For Serhiy and Tania, the psychological impact of the conflict is evident in their daily lives… The fear of potential conscription not only affects the couples themselves but also their extended families, who are forced to contend with the possibility of losing multiple family members to the war… As the conflict continues, it is essential to recognize the emotional impact of the war on these families and to provide support and resources to help them navigate this difficult period.</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Multi-turn dialogues example for News TLDR scenario.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A2.F25">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F25.1">0=<span class="ltx_ERROR undefined" id="A2.F25.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F25.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F25.2">
<tr class="ltx_tr" id="A2.F25.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F25.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.1.1.1">
<span class="ltx_p" id="A2.F25.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.1.1.1.1.1">Question 1:</span> What were the main factors that increased the cost of products sold for JnJ in FY2022?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F25.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.2.1.1">
<span class="ltx_p" id="A2.F25.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.2.1.1.1.1">Context 1:</span> Analysis of Consolidated Earnings Before Provision for Taxes on Income Consolidated earnings before provision for taxes on income was $21.7 billion and $22.8 billion for the years 2022 and 2021, respectively… Research and development expense by segment of business was as follows… As a percent to sales, consolidated earnings before provision for taxes on income was 22.9% and 24.3%, in 2022 and 2021, respectively…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F25.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.3.1.1">
<span class="ltx_p" id="A2.F25.2.3.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.3.1.1.1.1">Assistant:</span> The main factors that increased the cost of products sold for Johnson &amp; Johnson (JnJ) in FY2022 were:
1. One-time COVID-19 vaccine manufacturing exit related costs.
2. Currency impacts in the Pharmaceutical segment.
3. Commodity inflation in the MedTech and Consumer Health segments.</span>
<span class="ltx_p" id="A2.F25.2.3.1.1.2">These factors were partially offset by supply chain benefits in the Consumer Health segment. Additionally, the intangible asset amortization expense included in cost of products sold was $4.3 billion for the fiscal year 2022.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F25.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.4.1.1">
<span class="ltx_p" id="A2.F25.2.4.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.4.1.1.1.1">Question 2:</span> What were the factors that partially offset the increase in the cost of products sold for JnJ in FY2022?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F25.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.5.1.1">
<span class="ltx_p" id="A2.F25.2.5.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.5.1.1.1.1">Context 2:</span> The increase in the income before tax as a percent of sales was primarily driven by the following… The Company continues to evaluate and monitor both its internal and external supply arrangements…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F25.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.6.1.1">
<span class="ltx_p" id="A2.F25.2.6.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.6.1.1.1.1">Assistant:</span> The factors that partially offset the increase in the cost of products sold for Johnson &amp; Johnson (JnJ) in FY2022 were:</span>
<span class="ltx_p" id="A2.F25.2.6.1.1.2">1. Supply chain benefits in the Consumer Health segment.
2. Lower intangible asset amortization expense, which was $4.3 billion for fiscal year 2022 compared to $4.7 billion for fiscal year 2021.</span>
<span class="ltx_p" id="A2.F25.2.6.1.1.3">These factors helped to mitigate the impact of one-time COVID-19 vaccine manufacturing exit related costs, currency impacts in the Pharmaceutical segment, and commodity inflation in the MedTech and Consumer Health segments on the cost of products sold for JnJ.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F25.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.7.1.1">
<span class="ltx_p" id="A2.F25.2.7.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.7.1.1.1.1">Question 3:</span> What drove gross margin change as of FY2022 for JnJ?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F25.2.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.8.1.1">
<span class="ltx_p" id="A2.F25.2.8.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.8.1.1.1.1">Context 3:</span> The increase in the income before tax as a percent of sales was primarily driven by the following… The Company continues to evaluate and monitor both its internal and external supply arrangements. In fiscal 2022, the COVID-19 Vaccine related costs (mentioned above) included the remaining commitments and obligations, including external manufacturing network exit and related inventory costs and required clinical trial expenses…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F25.2.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F25.2.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F25.2.9.1.1">
<span class="ltx_p" id="A2.F25.2.9.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F25.2.9.1.1.1.1">Assistant:</span> The gross margin change as of FY2022 for Johnson &amp; Johnson (JnJ) was driven by:</span>
<span class="ltx_p" id="A2.F25.2.9.1.1.2">1. One-time COVID-19 vaccine manufacturing exit related costs.
2. Currency impacts in the Pharmaceutical segment.
3. Commodity inflation in the MedTech and Consumer Health segments.</span>
<span class="ltx_p" id="A2.F25.2.9.1.1.3">These factors led to an increase in the cost of products sold, which in turn negatively impacted the gross margin. However, the impact was partially offset by supply chain benefits in the Consumer Health segment and a lower intangible asset amortization expense in 2022 compared to 2021.</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>Multi-turn dialogues example for Finance scenario.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A2.F26">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F26.1">0=<span class="ltx_ERROR undefined" id="A2.F26.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F26.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F26.2">
<tr class="ltx_tr" id="A2.F26.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F26.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.1.1.1">
<span class="ltx_p" id="A2.F26.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.1.1.1.1.1">Question 1:</span> My Nokia HS-4W headset is not connecting to my phone. What should I do?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F26.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.2.1.1">
<span class="ltx_p" id="A2.F26.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.2.1.1.1.1">Context 1:</span> file_name: manual…
- If you cannot connect the headset to the compatible phone, proceed as follows: Make sure that the Bluetooth feature is activated on the compatible phone. Make sure that the headset is switched on and paired with the compatible phone. Make sure that you have disconnected the previous Bluetooth connection from the phone. Check that the headset is within 30 feet (10 metres) of the phone and that there are no obstructions, such as walls, or other electronic devices between the headset and the phone.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F26.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.3.1.1">
<span class="ltx_p" id="A2.F26.2.3.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.3.1.1.1.1">Assistant:</span> If your Nokia HS-4W headset is not connecting to your phone, follow these steps to troubleshoot the issue:
1. Make sure that the Bluetooth feature is activated on the compatible phone.
2. Make sure that the headset is switched on and paired with the compatible phone.
3. Make sure that you have disconnected the previous Bluetooth connection from the phone…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F26.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.4.1.1">
<span class="ltx_p" id="A2.F26.2.4.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.4.1.1.1.1">Question 2:</span> I followed those steps, but my headset still won’t connect. The indicator light is blinking green.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F26.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.5.1.1">
<span class="ltx_p" id="A2.F26.2.5.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.5.1.1.1.1">Context 2:</span> file_name: manual…
If the headset cannot connect to either of the compatible phones, it remains discoverable for other phones for approximately 10 minutes, which is displayed by the blinking green indicator light…
- Reset the headset:
Press and hold the power key and the upper end of the volume control key for 10 seconds. After the headset has been reset, it beeps twice and the red and the green indicator lights alternate for several seconds.
- Switch on the headset: press and hold the power key for 2 seconds. The headset beeps and the green indicator light starts blinking. The indicator light changes to yellow when the headset is connected to a compatible phone…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F26.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.6.1.1">
<span class="ltx_p" id="A2.F26.2.6.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.6.1.1.1.1">Assistant:</span> If the headset is blinking green and still not connecting, try the following steps:
1. Make sure the compatible phone has Bluetooth enabled and that the headset is in range (within 30 feet or 10 meters) without any obstructions.
2. Check if the pairing information list of the headset is not full. The headset can store the information of up to 8 phones at a time. If the list becomes full, reset the headset to the original settings by pressing and holding the power key…
3. Ensure that the headset is fully charged. A low battery can cause connection issues….</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F26.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.7.1.1">
<span class="ltx_p" id="A2.F26.2.7.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.7.1.1.1.1">Question 3:</span> I tried that, but it still won’t connect. I am wondering if there is any possible with the pairing limit?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F26.2.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.8.1.1">
<span class="ltx_p" id="A2.F26.2.8.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.8.1.1.1.1">Context 3:</span> file_name: manual … Troubleshooting: If you cannot connect the headset to the compatible phone, proceed as follows:… Make sure the pairing information list of the headset is not full. The headset can store the information of up to 8 phones at a time. If the list becomes full, reset the headset to the original settings. See ’Reset the headset, ’ p.16. Press and hold the answer/end key if the headset does not automatically reconnect to the default or last used phone….</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F26.2.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F26.2.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F26.2.9.1.1">
<span class="ltx_p" id="A2.F26.2.9.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F26.2.9.1.1.1.1">Assistant:</span> Yes, the headset has a pairing limit. The Nokia HS-4W headset can store the information of up to 8 phones at a time. If you have reached this limit, the headset will not be able to pair with a new phone. To resolve this issue, simply reset the headset to its original settings by pressing and holding the power key and the upper end of the volume control key for 10 seconds. After the headset has been reset, it beeps twice and the red and green indicator lights alternate for several seconds. This will clear the pairing information list, allowing you to pair the headset with your phone again.
If you still face connection issues after resetting the headset, make sure the headset is within 30 feet (10 meters) of the phone, and follow the troubleshooting steps mentioned earlier in the conversation. If all else fails, contact Nokia customer support or your local dealer for further assistance.</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>Multi-turn dialogues example for customer scenario.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_figure" id="A2.F27">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="A2.F27.1">0=<span class="ltx_ERROR undefined" id="A2.F27.1.1">\CT@everycr</span><span class="ltx_ERROR undefined" id="A2.F27.1.2">\@rowc@lors</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle" id="A2.F27.2">
<tr class="ltx_tr" id="A2.F27.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A2.F27.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.1.1.1">
<span class="ltx_p" id="A2.F27.2.1.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.1.1.1.1.1">Question 1:</span> Can you suggest a 5-day itinerary for a group of 7 people traveling from Richmond to two cities in Florida between March 9th and 13th, 2022?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F27.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.2.1.1">
<span class="ltx_p" id="A2.F27.2.2.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.2.1.1.1.1">Context 1:</span> Name Latitude Longitude Address Phone Website City…
[http://www.jungleisland.com/]</span>
<span class="ltx_p" id="A2.F27.2.2.1.1.2">Skyviews Miami Observation Wheel…</span>
<span class="ltx_p" id="A2.F27.2.2.1.1.3">[Skyviews](https://www.skyviewsobservationwheels.com/?utm_source…)</span>
<span class="ltx_p" id="A2.F27.2.2.1.1.4">{’Description’: ’Flight from Richmond to Miami on 2022-03-09’, ’Content’: ’Flight Number… }</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F27.2.3.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.3.1.1">
<span class="ltx_p" id="A2.F27.2.3.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.3.1.1.1.1">Assistant:</span> Day 1: Richmond to Miami</span>
<span class="ltx_p" id="A2.F27.2.3.1.1.2">Option A: Flight
Richmond (RIC) to Miami (MIA) on Flight Number F3681647, departing at 06:28 and arriving at 08:53 on March 9th, with an actual elapsed time of 2 hours and 25 minutes. The flight costs $316 per person; Option B: Self-driving…</span>
<span class="ltx_p" id="A2.F27.2.3.1.1.3">Day 2: Miami
- Start your day with a visit to the Miami Seaquarium, followed by a shopping experience at Bayside Marketplace. For lunch, enjoy a meal at one of the many restaurants in the area…</span>
<span class="ltx_p" id="A2.F27.2.3.1.1.4">Day 3: Miami to Punta Gorda
- Drive from Miami to Punta Gorda, covering a distance of 291 km, with an estimated duration of 2 hours and 41 minutes…</span>
<span class="ltx_p" id="A2.F27.2.3.1.1.5">Day 4: Punta Gorda
- Begin your day with a visit to the Military Heritage Museum, followed by a trip to the Blanchard House Museum…</span>
<span class="ltx_p" id="A2.F27.2.3.1.1.6">Day 5: Punta Gorda to Richmond
- Drive from Punta Gorda to Richmond, covering a distance of 1,434 km…Upon arrival in Richmond, check into your accommodation and spend the evening relaxing after your adventures in Florida…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F27.2.4.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.4.1.1">
<span class="ltx_p" id="A2.F27.2.4.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.4.1.1.1.1">Question 2:</span> Our budget for this trip is $8,500. Can you include that in the plan?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F27.2.5.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.5.1.1">
<span class="ltx_p" id="A2.F27.2.5.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.5.1.1.1.1">Context 2:</span> {’Description’: ’Accommodations in Miami’, ’Content’: ’NAME price room type…}, {’Description’: ’Restaurants in Miami’, ’Content’: "Name Average Cost Cuisines Aggregate Rating City…}</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F27.2.6.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.6.1.1">
<span class="ltx_p" id="A2.F27.2.6.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.6.1.1.1.1">Assistant:</span> Given your budget of $8,500 for the entire group of 7 people, we can allocate the budget as follows:</span>
<span class="ltx_p" id="A2.F27.2.6.1.1.2">1. Accommodation:
- Miami (3 nights): $3,000
- Punta Gorda (2 nights): $2,000
2. Transportation:
- Flight from Richmond to Miami: $316 per person for one-way tickets. Total: $2,212…
3. Attractions and Activities:
We recommend selecting a mix of free and paid attractions to stay within your budget…
4. Dining:
- Use the provided list of restaurants in Miami and Punta Gorda to select budget-friendly options for your group…
5. Additional expenses:
- Budget for any additional expenses, such as travel insurance, tips, and souvenirs…</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.F27.2.7.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.7.1.1">
<span class="ltx_p" id="A2.F27.2.7.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.7.1.1.1.1">Question 3:</span> We require accommodations that allow visitors and should ideally be entire rooms. In regards to dining options, we prefer French, American, Mediterranean, and Italian cuisines. Can you make sure of that in the final plan? Propose the final plan for me, thanks!</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.F27.2.8.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.8.1.1">
<span class="ltx_p" id="A2.F27.2.8.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.8.1.1.1.1">Context 3:</span> {’Description’: ’Accommodations in Miami’, ’Content’: ’ NAME price room type…}</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.F27.2.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A2.F27.2.9.1">
<span class="ltx_inline-block ltx_align_top" id="A2.F27.2.9.1.1">
<span class="ltx_p" id="A2.F27.2.9.1.1.1" style="width:455.2pt;"><span class="ltx_text ltx_font_bold" id="A2.F27.2.9.1.1.1.1">Assistant:</span> To accommodate your preferences for accommodations that allow visitors and a mix of French, American, Mediterranean, and Italian cuisines, I have adjusted the 5-day itinerary…
**Day 1: Richmond to Miami**
- Option A: Flight from Richmond to Miami (Duration: 2 hours 25 minutes, Cost: $316 per person)…
**Day 2: Miami**
- Start your day with a visit to Vizcaya Museum &amp; Gardens
- For lunch, enjoy a meal at Urban Cuisine (cuisine: French, American, Mediterranean, and Italian, aggregate rating: 3.4).</span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 27: </span>Multi-turn dialogues example for Travel Planning scenario.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 08:26:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
