<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.09344] DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</title><meta property="og:description" content="Current perceptual similarity metrics operate at the level of pixels and patches.00footnotetext: ∗ Equal contribution, corresponding authors. Order decided by random seed. These metrics compare images in terms of their…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.09344">

<!--Generated on Wed Feb 28 23:51:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">DreamSim: Learning New Dimensions of 
<br class="ltx_break">Human Visual Similarity using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Stephanie Fu<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">∗1</span></sup>
     
Netanel Y. Tamir<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">∗2</span></sup>
     
Shobhita Sundaram<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">∗1</span></sup>
 
<br class="ltx_break"><span id="id4.4.1" class="ltx_text ltx_font_bold">Lucy Chai<sup id="id4.4.1.1" class="ltx_sup"><span id="id4.4.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>
     
<span id="id5.5.2" class="ltx_text ltx_font_bold">Richard Zhang<sup id="id5.5.2.1" class="ltx_sup"><span id="id5.5.2.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span>
     
<span id="id6.6.3" class="ltx_text ltx_font_bold">Tali Dekel<sup id="id6.6.3.1" class="ltx_sup"><span id="id6.6.3.1.1" class="ltx_text ltx_font_medium ltx_font_italic">2</span></sup></span>
     
<span id="id7.7.4" class="ltx_text ltx_font_bold">Phillip Isola<sup id="id7.7.4.1" class="ltx_sup"><span id="id7.7.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">1</span></sup></span>


<br class="ltx_break"><sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">1</span></sup>MIT    <sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">2</span></sup>Weizmann Institute of Science    <sup id="id16.16.id6" class="ltx_sup"><span id="id16.16.id6.1" class="ltx_text ltx_font_italic">3</span></sup>Adobe Research
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Current perceptual similarity metrics operate at the level of pixels and patches.<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">0</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">0</sup><span class="ltx_note_type">footnotetext: </span><sup id="footnotex1.1" class="ltx_sup">∗</sup> Equal contribution, corresponding authors. Order decided by random seed.</span></span></span> These metrics compare images in terms of their <em id="id17.id1.1" class="ltx_emph ltx_font_italic">low-level</em> colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, <em id="id17.id1.2" class="ltx_emph ltx_font_italic">DreamSim</em>, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks. 
<br class="ltx_break">Our project page: <a target="_blank" href="https://dreamsim-nights.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://dreamsim-nights.github.io/</a></p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2306.09344/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S0.F1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">What makes two images look similar? <span id="S0.F1.2.1.1" class="ltx_text ltx_font_medium">
We generate a new benchmark of synthetic image triplets that span a wide range of mid-level variations and gather human judgments, rating whether image A/B is more similar to the reference. Our benchmark spans various notions of similarity such as pose (top-left), perspective (top-mid), foreground color (mid-left), number of items (mid-right), and object shape (bottom-left). This allows us to learn a new metric (DreamSim) that better coincides with human judgments w.r.t. existing similarity metrics (LPIPS) or embedding-based metrics extracted from recent large vision models (DINO &amp; CLIP).

</span></span></figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text ltx_font_italic">“A sense of sameness is the very keel and backbone of our thinking” – William James, 1890</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Our understanding of the visual world hinges crucially on our ability to perceive the similarities between different images. Moreover, humans can reason about many notions of similarity, ranging from low-level perceptual properties such as color and texture, to higher-level concepts such as an object’s category or a scene’s emotional valence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>. This capacity to conduct meaningful visual comparisons underlies our ability to effortlessly transfer our knowledge to new environments, e.g., recognizing unseen or unfamiliar objects based on their relatedness to familiar ones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> .</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Computer vision has tried to capture this sense of similarity with low-level metrics like PSNR and SSIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, as well as learned perceptual metrics such as LPIPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> and DISTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Despite their utility, these metrics are limited in that they focus on the pixel or patch level and fail to capture higher-level structures. These limitations have motivated researchers to reach for <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">image-level</em> embeddings from large vision models such as DINO or CLIP to measure image-to-image distances in a large variety of applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. Recent studies have shown that these embeddings do well at capturing certain high-level similarity judgments, in particular, predicting which semantic categories will be considered alike by humans <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. It remains unclear, however, how well these models align with human perception of richer and more fine-grained visual structure.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we introduce a new perceptual metric, which bridges the gap between lower-level patch-based metrics and broad categorical comparisons. We collect a new dataset named NIGHTS – Novel Image Generations with Human-Tested Similarity – containing human similarity judgments over image triplets. Each triplet consists of a reference image and two perturbed versions, along with human judgments as to which version is most similar to the reference (Fig. <a href="#S0.F1" title="Figure 1 ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). We use iterative filtering together with recent diffusion models to collect our dataset, which is designed to capture image sets that are cognitively impenetrable (i.e. result in consistent decisions across different individuals) yet showcase rich variations. For example, our data contains images of similar object appearance, viewing angles, camera poses, overall layout, etc. This dataset differs qualitatively from prior low-level datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, which focused on perturbations like blurring and adding noise, and from previous high-level datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, which showed variation just at the level of categories (e.g. “is an image of a kitchen more like an image of a giraffe or an image of a beach”).</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">On the mid-level similarity task presented by our data, we find that features from recent large pre-trained vision models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> outperform the current set of standard perceptual metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite>. We further show that these large vision models can be tuned on our data to be substantially more human-aligned. Our resulting metric, DreamSim, can be dropped into existing pipelines and demonstrates high agreement with human visual perception in both quantitative assessments and qualitative comparisons using out-of-domain real images (e.g., image retrieval, image synthesis). We also analyze which features our metric is most sensitive to and find that, compared to previous perceptual metrics, it focuses relatively heavily on foreground objects, while compared to modern image embeddings, it does not neglect color and layout.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In summary, our contributions are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A new image similarity dataset, consisting of 20k synthetic image triplets designed to be cognitively impenetrable with human judgments as labels.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A tuned metric that captures how humans naturally perceive image similarity, achieving 96.16% accuracy in predicting human judgments on our dataset.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Analysis of the image properties that affect our model’s decisions.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Demonstration of downstream applications to image retrieval and synthesis.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.2" class="ltx_p"><span id="S2.p1.2.1" class="ltx_text ltx_font_bold">Perceptual similarity.</span> Classical metrics such as Manhattan <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S2.p1.1.m1.1a"><msub id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ℓ</ci><cn type="integer" id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\ell_{1}</annotation></semantics></math>, Euclidean <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S2.p1.2.m2.1a"><msub id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">ℓ</mi><mn id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ℓ</ci><cn type="integer" id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\ell_{2}</annotation></semantics></math>, MSE, and PSNR use point-wise difference to measure similarity, thus failing to capture important joint image statistics. Patch-based metrics, including SSIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>, FSIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>, and HDR-VDP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite> tackle this issue and are widely used in applications involving photometric distortions such as image quality assessment. However,
they do not capture the nuances of human vision when more structural ambiguity is present <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> and are not suited for more complex image generation tasks.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">With the deep learning revolution, classical metrics have been replaced by learning-based metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. These metrics are defined in the space of deep features extracted from pre-trained networks, such as VGG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> or AlexNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
Amir and Weiss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> demonstrate that even <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">untrained</span> networks can be adapted as perceptual metrics. Zhang <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">et al</em>. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> observe that feature-based metrics outperform classical metrics across different convolutional architectures and learning paradigms, suggesting that perceptual similarity is an emergent property in deep representations. Further tuning on the perceptual data yields improvements, such as in LPIPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, PIE-APP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite>, or DPAM in the audio domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. Further improvements include ensembling for robustness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, antialiasing for stability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, and global descriptors for texture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Muttenthaler <em id="S2.p2.1.3" class="ltx_emph ltx_font_italic">et al</em>.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> provide insight into <span id="S2.p2.1.4" class="ltx_text ltx_font_italic">high-level</span> human similarity by training on a subset of the THINGS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> dataset, focusing on concept similarity and omitting visual cues for images within a category.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">While strong computer vision features make for strong perceptual metrics, counterintuitively, they eventually become <span id="S2.p3.1.1" class="ltx_text ltx_font_italic">decorrelated</span> with perceptual similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Today, the predominantly-used perceptual metric is LPIPS, operating on 64<math id="S2.p3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.p3.1.m1.1a"><mo id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><times id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\times</annotation></semantics></math>64 patches.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_noindent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Recent foundation models as metrics.</span>
Foundation models provide strong pre-trained backbones for a variety of downstream tasks. These models primarily leverage the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> architecture and are trained through self-supervised learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> learns to map images and text captions into a shared embedding space, proving useful for many (often zero-shot) tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite>. CLIP has been employed as a perceptual metric to train models for semantic consistency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Another self-supervised ViT-based model, DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, extracts disentangled appearance and structure descriptors that can be employed in image generation pipelines. Amir <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">et al</em>.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> show that DINO encodes valuable semantic information about object parts. Our work aims to systematically evaluate such representations for perceptual similarity, also including OpenCLIP (an open-source implementation of CLIP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and pre-trained masked autoencoders (MAE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para ltx_noindent">
<p id="S2.p5.1" class="ltx_p"><span id="S2.p5.1.1" class="ltx_text ltx_font_bold">Perceptual tests.</span>
The two alternative forced choice (2AFC) test has historically been used by behavioral psychologists to study decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
Humans judge the similarity between two images by choosing to consider certain dimensions of similarity more than others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>.
Gathering judgments on ambiguous sets of images can be cognitively penetrable, calling upon a subject’s cognitive processes rather than a more automatic, “wired-in” sense that is stable across humans and over time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Previous studies have raised concerns about cognitive penetrability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>.
On the other hand, as a psychophysical measure, just noticeable difference experiments (JND) are thought to be independent of subjective biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. We follow best practices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> and collect judgments on both of these complementary perceptual tests.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Synthetic data.</span>
GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> have been used widely for dataset generation on tasks such as visual alignment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, face manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>, and adversarial training for image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>. In recent years, text-driven generative models (e.g., Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, Imagen <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite>, DALLE-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>, MUSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) have emerged as powerful tools for image synthesis. They have also been used to generate training data for a variety of downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Perceptual dataset collection</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">While previous datasets focus on <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">low-level</span>, patch-based distortions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> or <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">high-level</span>, categorical <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> changes, we aim to close the gap, capturing distortions including mid-level variations. We aim to produce images with an underlying semantic commonality, but variations in a diversity of factors, such as style, color, pose, and other details, so that a human can assess their visual relationship.
In Section <a href="#S3.SS1" title="3.1 Generating images with varied distortions ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, we
describe our data generation pipeline – we prompt Stable Diffusion for related images of a given category, leveraging its natural image prior for variations within the category. We then describe our mechanism for collecting cognitively impenetrable perceptual judgments in Section <a href="#S3.SS2" title="3.2 Human perceptual judgments ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generating images with varied distortions</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We leverage Stable Diffusion v1.4  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, which generates diverse and high-quality images that adhere to a given text prompt. We sample images with a prompt of the same category, using the structure
<span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">‘‘An image of a &lt;category&gt;’’</span>. The <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">&lt;category&gt;</span> is drawn from image labels in popular datasets: ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, CIFAR-10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, CIFAR-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, Oxford 102 Flower <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>, Food-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, and SUN397 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite>. While the ImageNet labels make up <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\sim\!55\%" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml"></mi><mo rspace="0.108em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">∼</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">55</mn><mo id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">absent</csymbol><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="latexml" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">55</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\sim\!55\%</annotation></semantics></math> of the categories, the superset of categories across these datasets encompass a wide range of objects and scenes.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">As illustrated in Figure <a href="#S0.F1" title="Figure 1 ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the resulting images display mid-level variations, such as pose, perspective, and shape, falling in-between pixel/patch-based distortions and categorical differences. Additional triplets and the full list of categories are in the Supplementary Materials (SM). We generate an initial set of 100,000 image triplets, which are later filtered during the labeling process. Each triplet consists of a reference image and two “distortions”, generated from the same prompt.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.09344/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S3.F2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Overview of 2AFC and JND data collection.<span id="S3.F2.2.1.1" class="ltx_text ltx_font_medium"> (Left) We collect up to 10 similarity judgments for each triplet in our dataset, and filter for unanimous examples. (Right) We validate our 2AFC results with a JND study. We flash two interleaved image pairs, and ask if the first &amp; third, and second &amp; fourth were the same. Interleaving the pairs ensures that users decide solely based on their initial reaction to each image from a standardized (500ms) viewing time. </span></span></figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.5.5" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:83.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-174.7pt,33.4pt) scale(0.553731858833501,0.553731858833501) ;">
<table id="S3.T1.5.5.5" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.5.5.5.6.1" class="ltx_tr">
<th id="S3.T1.5.5.5.6.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.1.1" class="ltx_text">Dataset</span></th>
<th id="S3.T1.5.5.5.6.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.2.1" class="ltx_text">
<span id="S3.T1.5.5.5.6.1.2.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.6.1.2.1.1.1" class="ltx_p">Perturbation</span>
<span id="S3.T1.5.5.5.6.1.2.1.1.2" class="ltx_p">Type</span>
</span></span></th>
<th id="S3.T1.5.5.5.6.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.3.1" class="ltx_text">Data Source</span></th>
<th id="S3.T1.5.5.5.6.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.4.1" class="ltx_text">
<span id="S3.T1.5.5.5.6.1.4.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.6.1.4.1.1.1" class="ltx_p">Input</span>
<span id="S3.T1.5.5.5.6.1.4.1.1.2" class="ltx_p">type</span>
</span></span></th>
<th id="S3.T1.5.5.5.6.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.5.1" class="ltx_text">
<span id="S3.T1.5.5.5.6.1.5.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.6.1.5.1.1.1" class="ltx_p"># Images</span>
<span id="S3.T1.5.5.5.6.1.5.1.1.2" class="ltx_p">/ Patches</span>
</span></span></th>
<th id="S3.T1.5.5.5.6.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.6.1" class="ltx_text"># Samples</span></th>
<th id="S3.T1.5.5.5.6.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.7.1" class="ltx_text">
<span id="S3.T1.5.5.5.6.1.7.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.6.1.7.1.1.1" class="ltx_p"># Judge</span>
<span id="S3.T1.5.5.5.6.1.7.1.1.2" class="ltx_p">per</span>
<span id="S3.T1.5.5.5.6.1.7.1.1.3" class="ltx_p">sample</span>
</span></span></th>
<th id="S3.T1.5.5.5.6.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.8.1" class="ltx_text">
<span id="S3.T1.5.5.5.6.1.8.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.6.1.8.1.1.1" class="ltx_p">Judgment</span>
<span id="S3.T1.5.5.5.6.1.8.1.1.2" class="ltx_p">type</span>
</span></span></th>
<th id="S3.T1.5.5.5.6.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.5.5.5.6.1.9.1" class="ltx_text">Examples</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.3.3.3.3" class="ltx_tr">
<th id="S3.T1.3.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.3.3.3.3.4.1" class="ltx_text">BAPPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite></span></th>
<td id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text">
<span id="S3.T1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.1.1.1.1.1.1.1.2" class="ltx_p">low-level</span>
<span id="S3.T1.1.1.1.1.1.1.1.1" class="ltx_p">(traditional <math id="S3.T1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="S3.T1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.1.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.1.1.m1.1b"><and id="S3.T1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.1.1.m1.1c">\&amp;</annotation></semantics></math></span>
<span id="S3.T1.1.1.1.1.1.1.1.3" class="ltx_p">CNN-based)</span>
</span></span></td>
<td id="S3.T1.3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.3.3.3.3.5.1" class="ltx_text">
<span id="S3.T1.3.3.3.3.5.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.3.3.3.3.5.1.1.1" class="ltx_p">MIT-Adobe5k,</span>
<span id="S3.T1.3.3.3.3.5.1.1.2" class="ltx_p">RAISE1k</span>
</span></span></td>
<td id="S3.T1.2.2.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.2.2.2.2.2.1" class="ltx_text">
<span id="S3.T1.2.2.2.2.2.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.2.2.2.2.2.1.1.1" class="ltx_p">64<math id="S3.T1.2.2.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.T1.2.2.2.2.2.1.1.1.m1.1a"><mo id="S3.T1.2.2.2.2.2.1.1.1.m1.1.1" xref="S3.T1.2.2.2.2.2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.2.1.1.1.m1.1b"><times id="S3.T1.2.2.2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T1.2.2.2.2.2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.2.1.1.1.m1.1c">\times</annotation></semantics></math>64</span>
<span id="S3.T1.2.2.2.2.2.1.1.2" class="ltx_p">patches</span>
</span></span></td>
<td id="S3.T1.3.3.3.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">563.1k</td>
<td id="S3.T1.3.3.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">187.7k</td>
<td id="S3.T1.3.3.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">2.6</td>
<td id="S3.T1.3.3.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;">2AFC</td>
<td id="S3.T1.3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.3.3.3.3.3.1" class="ltx_text"><img src="/html/2306.09344/assets/figures/bapps_ex.png" id="S3.T1.3.3.3.3.3.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="50" alt="[Uncaptioned image]"></span></td>
</tr>
<tr id="S3.T1.5.5.5.7.1" class="ltx_tr">
<td id="S3.T1.5.5.5.7.1.1" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">19.2k</td>
<td id="S3.T1.5.5.5.7.1.2" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">9.6k</td>
<td id="S3.T1.5.5.5.7.1.3" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">3.0</td>
<td id="S3.T1.5.5.5.7.1.4" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">JND</td>
</tr>
<tr id="S3.T1.4.4.4.4" class="ltx_tr">
<th id="S3.T1.4.4.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:5pt;padding-bottom:5pt;">
<span id="S3.T1.4.4.4.4.2.1" class="ltx_ERROR undefined">\cdashline</span>1-9
<span id="S3.T1.4.4.4.4.2.2" class="ltx_text">THINGS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite></span>
</th>
<td id="S3.T1.4.4.4.4.3" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.3.1" class="ltx_text">
<span id="S3.T1.4.4.4.4.3.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.4.4.4.4.3.1.1.1" class="ltx_p">high-level</span>
<span id="S3.T1.4.4.4.4.3.1.1.2" class="ltx_p">conceptual</span>
</span></span></td>
<td id="S3.T1.4.4.4.4.4" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.4.1" class="ltx_text">
<span id="S3.T1.4.4.4.4.4.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.4.4.4.4.4.1.1.1" class="ltx_p">Google, Bing,</span>
<span id="S3.T1.4.4.4.4.4.1.1.2" class="ltx_p">Ebay, Flickr</span>
<span id="S3.T1.4.4.4.4.4.1.1.3" class="ltx_p">scrape</span>
</span></span></td>
<td id="S3.T1.4.4.4.4.5" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.5.1" class="ltx_text">Images</span></td>
<td id="S3.T1.4.4.4.4.6" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.6.1" class="ltx_text">1.8k</span></td>
<td id="S3.T1.4.4.4.4.7" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.7.1" class="ltx_text">4.7M</span></td>
<td id="S3.T1.4.4.4.4.8" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.8.1" class="ltx_text">1</span></td>
<td id="S3.T1.4.4.4.4.9" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.9.1" class="ltx_text">Odd-one-out</span></td>
<td id="S3.T1.4.4.4.4.1" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;"><span id="S3.T1.4.4.4.4.1.1" class="ltx_text"><img src="/html/2306.09344/assets/figures/things_ex.png" id="S3.T1.4.4.4.4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="51" alt="[Uncaptioned image]"></span></td>
</tr>
<tr id="S3.T1.5.5.5.5" class="ltx_tr">
<th id="S3.T1.5.5.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2">
<span id="S3.T1.5.5.5.5.2.1" class="ltx_ERROR undefined">\cdashline</span>1-9
<span id="S3.T1.5.5.5.5.2.2" class="ltx_text">
<span id="S3.T1.5.5.5.5.2.2.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.5.2.2.1.1" class="ltx_p">NIGHTS</span>
<span id="S3.T1.5.5.5.5.2.2.1.2" class="ltx_p">(Ours)</span>
</span></span>
</th>
<td id="S3.T1.5.5.5.5.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.5.5.5.5.3.1" class="ltx_text">
<span id="S3.T1.5.5.5.5.3.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.5.3.1.1.1" class="ltx_p">low &amp;</span>
<span id="S3.T1.5.5.5.5.3.1.1.2" class="ltx_p">mid-level</span>
<span id="S3.T1.5.5.5.5.3.1.1.3" class="ltx_p">synthetic</span>
</span></span></td>
<td id="S3.T1.5.5.5.5.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.5.5.5.5.4.1" class="ltx_text">
<span id="S3.T1.5.5.5.5.4.1.1" class="ltx_inline-block ltx_align_center">
<span id="S3.T1.5.5.5.5.4.1.1.1" class="ltx_p">Diffusion-</span>
<span id="S3.T1.5.5.5.5.4.1.1.2" class="ltx_p">synthesized</span>
</span></span></td>
<td id="S3.T1.5.5.5.5.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.5.5.5.5.5.1" class="ltx_text">Images</span></td>
<td id="S3.T1.5.5.5.5.6" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">60.0k</td>
<td id="S3.T1.5.5.5.5.7" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">20.0k</td>
<td id="S3.T1.5.5.5.5.8" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">7.1</td>
<td id="S3.T1.5.5.5.5.9" class="ltx_td ltx_align_center" style="padding-top:5pt;padding-bottom:5pt;">2AFC</td>
<td id="S3.T1.5.5.5.5.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;" rowspan="2"><span id="S3.T1.5.5.5.5.1.1" class="ltx_text"><img src="/html/2306.09344/assets/figures/nights_ex.png" id="S3.T1.5.5.5.5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="157" height="50" alt="[Uncaptioned image]"></span></td>
</tr>
<tr id="S3.T1.5.5.5.8.2" class="ltx_tr">
<td id="S3.T1.5.5.5.8.2.1" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;">411</td>
<td id="S3.T1.5.5.5.8.2.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;">137</td>
<td id="S3.T1.5.5.5.8.2.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;">3.0</td>
<td id="S3.T1.5.5.5.8.2.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-top:5pt;padding-bottom:5pt;">JND</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S3.T1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Perceptual dataset comparison.<span id="S3.T1.7.1.1" class="ltx_text ltx_font_medium"> While previous work targets low-level and high-level variations, we leverage <span id="S3.T1.7.1.1.1" class="ltx_text ltx_font_italic">synthetic</span> data to generate diverse perturbations. In addition, while previous data gathers fewer judgments per sample, we filter for unanimous judgments, curating cleaner samples.</span></span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Human perceptual judgments</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We strive to gather perceptual judgments that are <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">automatic</span> (requiring little to no cognition), <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">stable</span> (invariant to changes in mental representation), and <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">shared</span> across humans, which requires a cognitively impenetrable judging process. Our main dataset consists of two alternative forced choice (2AFC) judgments on a triplet. Additionally, we collect just noticeable difference (JND) judgments on image pairs, corroborating our findings. Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Generating images with varied distortions ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of our dataset, compared to prior perceptual datasets.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.6" class="ltx_p"><span id="S3.SS2.p2.6.1" class="ltx_text ltx_font_bold">Two alternative forced choice (2AFC).</span>
Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Generating images with varied distortions ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (left) illustrates our data gathering setup. We collect judgments on the commonly-used Amazon Mechanical Turk (AMT) platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>.
We show participants triplets of images <math id="S3.SS2.p2.1.m1.3" class="ltx_Math" alttext="(x,\tilde{x}_{0},\tilde{x}_{1})" display="inline"><semantics id="S3.SS2.p2.1.m1.3a"><mrow id="S3.SS2.p2.1.m1.3.3.2" xref="S3.SS2.p2.1.m1.3.3.3.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.3.3.2.3" xref="S3.SS2.p2.1.m1.3.3.3.cmml">(</mo><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p2.1.m1.3.3.2.4" xref="S3.SS2.p2.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS2.p2.1.m1.2.2.1.1" xref="S3.SS2.p2.1.m1.2.2.1.1.cmml"><mover accent="true" id="S3.SS2.p2.1.m1.2.2.1.1.2" xref="S3.SS2.p2.1.m1.2.2.1.1.2.cmml"><mi id="S3.SS2.p2.1.m1.2.2.1.1.2.2" xref="S3.SS2.p2.1.m1.2.2.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p2.1.m1.2.2.1.1.2.1" xref="S3.SS2.p2.1.m1.2.2.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p2.1.m1.2.2.1.1.3" xref="S3.SS2.p2.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p2.1.m1.3.3.2.5" xref="S3.SS2.p2.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS2.p2.1.m1.3.3.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.cmml"><mover accent="true" id="S3.SS2.p2.1.m1.3.3.2.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.p2.1.m1.3.3.2.2.2.2" xref="S3.SS2.p2.1.m1.3.3.2.2.2.2.cmml">x</mi><mo id="S3.SS2.p2.1.m1.3.3.2.2.2.1" xref="S3.SS2.p2.1.m1.3.3.2.2.2.1.cmml">~</mo></mover><mn id="S3.SS2.p2.1.m1.3.3.2.2.3" xref="S3.SS2.p2.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS2.p2.1.m1.3.3.2.6" xref="S3.SS2.p2.1.m1.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.3b"><vector id="S3.SS2.p2.1.m1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑥</ci><apply id="S3.SS2.p2.1.m1.2.2.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1">subscript</csymbol><apply id="S3.SS2.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2"><ci id="S3.SS2.p2.1.m1.2.2.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2.1">~</ci><ci id="S3.SS2.p2.1.m1.2.2.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p2.1.m1.2.2.1.1.3.cmml" xref="S3.SS2.p2.1.m1.2.2.1.1.3">0</cn></apply><apply id="S3.SS2.p2.1.m1.3.3.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2">subscript</csymbol><apply id="S3.SS2.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.2"><ci id="S3.SS2.p2.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.2.1">~</ci><ci id="S3.SS2.p2.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p2.1.m1.3.3.2.2.3.cmml" xref="S3.SS2.p2.1.m1.3.3.2.2.3">1</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.3c">(x,\tilde{x}_{0},\tilde{x}_{1})</annotation></semantics></math> and ask whether <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\tilde{x}_{0}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p2.2.m2.1.1.2.1" xref="S3.SS2.p2.2.m2.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><ci id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.1">~</ci><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\tilde{x}_{0}</annotation></semantics></math> or <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\tilde{x}_{1}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mover accent="true" id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2.2" xref="S3.SS2.p2.3.m3.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p2.3.m3.1.1.2.1" xref="S3.SS2.p2.3.m3.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><ci id="S3.SS2.p2.3.m3.1.1.2.1.cmml" xref="S3.SS2.p2.3.m3.1.1.2.1">~</ci><ci id="S3.SS2.p2.3.m3.1.1.2.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\tilde{x}_{1}</annotation></semantics></math> (“distortions” of <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">x</annotation></semantics></math>) is more similar to the reference image <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">x</annotation></semantics></math>.
We collect judgments <math id="S3.SS2.p2.6.m6.2" class="ltx_Math" alttext="y\in\{0,1\}" display="inline"><semantics id="S3.SS2.p2.6.m6.2a"><mrow id="S3.SS2.p2.6.m6.2.3" xref="S3.SS2.p2.6.m6.2.3.cmml"><mi id="S3.SS2.p2.6.m6.2.3.2" xref="S3.SS2.p2.6.m6.2.3.2.cmml">y</mi><mo id="S3.SS2.p2.6.m6.2.3.1" xref="S3.SS2.p2.6.m6.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p2.6.m6.2.3.3.2" xref="S3.SS2.p2.6.m6.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.6.m6.2.3.3.2.1" xref="S3.SS2.p2.6.m6.2.3.3.1.cmml">{</mo><mn id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">0</mn><mo id="S3.SS2.p2.6.m6.2.3.3.2.2" xref="S3.SS2.p2.6.m6.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p2.6.m6.2.2" xref="S3.SS2.p2.6.m6.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p2.6.m6.2.3.3.2.3" xref="S3.SS2.p2.6.m6.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.2b"><apply id="S3.SS2.p2.6.m6.2.3.cmml" xref="S3.SS2.p2.6.m6.2.3"><in id="S3.SS2.p2.6.m6.2.3.1.cmml" xref="S3.SS2.p2.6.m6.2.3.1"></in><ci id="S3.SS2.p2.6.m6.2.3.2.cmml" xref="S3.SS2.p2.6.m6.2.3.2">𝑦</ci><set id="S3.SS2.p2.6.m6.2.3.3.1.cmml" xref="S3.SS2.p2.6.m6.2.3.3.2"><cn type="integer" id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">0</cn><cn type="integer" id="S3.SS2.p2.6.m6.2.2.cmml" xref="S3.SS2.p2.6.m6.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.2c">y\in\{0,1\}</annotation></semantics></math> denoting the more perceptually similar distortion.
This choice does not necessarily need to be easily articulated but should be cognitively impenetrable, that is, instinctive and consistent across humans.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.2" class="ltx_p">Additionally, we use sentinels to ensure high-quality responses.
Sentinels are designed as <math id="S3.SS2.p3.1.m1.3" class="ltx_Math" alttext="(x,x,v)" display="inline"><semantics id="S3.SS2.p3.1.m1.3a"><mrow id="S3.SS2.p3.1.m1.3.4.2" xref="S3.SS2.p3.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.SS2.p3.1.m1.3.4.2.1" xref="S3.SS2.p3.1.m1.3.4.1.cmml">(</mo><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p3.1.m1.3.4.2.2" xref="S3.SS2.p3.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS2.p3.1.m1.2.2" xref="S3.SS2.p3.1.m1.2.2.cmml">x</mi><mo id="S3.SS2.p3.1.m1.3.4.2.3" xref="S3.SS2.p3.1.m1.3.4.1.cmml">,</mo><mi id="S3.SS2.p3.1.m1.3.3" xref="S3.SS2.p3.1.m1.3.3.cmml">v</mi><mo stretchy="false" id="S3.SS2.p3.1.m1.3.4.2.4" xref="S3.SS2.p3.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.3b"><vector id="S3.SS2.p3.1.m1.3.4.1.cmml" xref="S3.SS2.p3.1.m1.3.4.2"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">𝑥</ci><ci id="S3.SS2.p3.1.m1.2.2.cmml" xref="S3.SS2.p3.1.m1.2.2">𝑥</ci><ci id="S3.SS2.p3.1.m1.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3">𝑣</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.3c">(x,x,v)</annotation></semantics></math>, where one “distortion” is the image itself and the other is from a completely different prompt.
Approximately <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mn id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">20</mn><mo id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">20\%</annotation></semantics></math> of participants failed at least one sentinel, and we discard all their responses.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.3" class="ltx_p">To collect cognitively impenetrable triplets, we design our 2AFC experiments to ensure that each triplet included in the dataset obtains a unanimous vote for either <math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\tilde{x}_{0}" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><msub id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mover accent="true" id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2.2" xref="S3.SS2.p4.1.m1.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p4.1.m1.1.1.2.1" xref="S3.SS2.p4.1.m1.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">subscript</csymbol><apply id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2"><ci id="S3.SS2.p4.1.m1.1.1.2.1.cmml" xref="S3.SS2.p4.1.m1.1.1.2.1">~</ci><ci id="S3.SS2.p4.1.m1.1.1.2.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\tilde{x}_{0}</annotation></semantics></math> or <math id="S3.SS2.p4.2.m2.1" class="ltx_Math" alttext="\tilde{x}_{1}" display="inline"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mover accent="true" id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2.2" xref="S3.SS2.p4.2.m2.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p4.2.m2.1.1.2.1" xref="S3.SS2.p4.2.m2.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><apply id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2"><ci id="S3.SS2.p4.2.m2.1.1.2.1.cmml" xref="S3.SS2.p4.2.m2.1.1.2.1">~</ci><ci id="S3.SS2.p4.2.m2.1.1.2.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p4.2.m2.1.1.3.cmml" xref="S3.SS2.p4.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\tilde{x}_{1}</annotation></semantics></math>. We divide our experiments into 10 rounds, starting with 100,000 triplets, advancing instances that maintain unanimous votes. Triplets retained through all 10 rounds can earn a maximum of 10 votes, but may have less as a result of the aforementioned sentinels. To maintain dataset quality, we only include triplets with <math id="S3.SS2.p4.3.m3.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S3.SS2.p4.3.m3.1a"><mo id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><geq id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">\geq</annotation></semantics></math>6 unanimous judgments in the final dataset.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Ultimately, our 2AFC dataset <math id="S3.SS2.p5.1.m1.3" class="ltx_Math" alttext="\mathcal{D}^{\text{2afc}}=\{(x,\tilde{x}_{0},\tilde{x}_{1}),y\}" display="inline"><semantics id="S3.SS2.p5.1.m1.3a"><mrow id="S3.SS2.p5.1.m1.3.3" xref="S3.SS2.p5.1.m1.3.3.cmml"><msup id="S3.SS2.p5.1.m1.3.3.3" xref="S3.SS2.p5.1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p5.1.m1.3.3.3.2" xref="S3.SS2.p5.1.m1.3.3.3.2.cmml">𝒟</mi><mtext id="S3.SS2.p5.1.m1.3.3.3.3" xref="S3.SS2.p5.1.m1.3.3.3.3a.cmml">2afc</mtext></msup><mo id="S3.SS2.p5.1.m1.3.3.2" xref="S3.SS2.p5.1.m1.3.3.2.cmml">=</mo><mrow id="S3.SS2.p5.1.m1.3.3.1.1" xref="S3.SS2.p5.1.m1.3.3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.3.3.1.1.2" xref="S3.SS2.p5.1.m1.3.3.1.2.cmml">{</mo><mrow id="S3.SS2.p5.1.m1.3.3.1.1.1.2" xref="S3.SS2.p5.1.m1.3.3.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p5.1.m1.3.3.1.1.1.2.3" xref="S3.SS2.p5.1.m1.3.3.1.1.1.3.cmml">(</mo><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p5.1.m1.3.3.1.1.1.2.4" xref="S3.SS2.p5.1.m1.3.3.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.2" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.1" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.3" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p5.1.m1.3.3.1.1.1.2.5" xref="S3.SS2.p5.1.m1.3.3.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.cmml"><mover accent="true" id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.cmml"><mi id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.2" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.2.cmml">x</mi><mo id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.1" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.1.cmml">~</mo></mover><mn id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.3" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS2.p5.1.m1.3.3.1.1.1.2.6" xref="S3.SS2.p5.1.m1.3.3.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS2.p5.1.m1.3.3.1.1.3" xref="S3.SS2.p5.1.m1.3.3.1.2.cmml">,</mo><mi id="S3.SS2.p5.1.m1.2.2" xref="S3.SS2.p5.1.m1.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS2.p5.1.m1.3.3.1.1.4" xref="S3.SS2.p5.1.m1.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.3b"><apply id="S3.SS2.p5.1.m1.3.3.cmml" xref="S3.SS2.p5.1.m1.3.3"><eq id="S3.SS2.p5.1.m1.3.3.2.cmml" xref="S3.SS2.p5.1.m1.3.3.2"></eq><apply id="S3.SS2.p5.1.m1.3.3.3.cmml" xref="S3.SS2.p5.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.3.3.3.1.cmml" xref="S3.SS2.p5.1.m1.3.3.3">superscript</csymbol><ci id="S3.SS2.p5.1.m1.3.3.3.2.cmml" xref="S3.SS2.p5.1.m1.3.3.3.2">𝒟</ci><ci id="S3.SS2.p5.1.m1.3.3.3.3a.cmml" xref="S3.SS2.p5.1.m1.3.3.3.3"><mtext mathsize="70%" id="S3.SS2.p5.1.m1.3.3.3.3.cmml" xref="S3.SS2.p5.1.m1.3.3.3.3">2afc</mtext></ci></apply><set id="S3.SS2.p5.1.m1.3.3.1.2.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1"><vector id="S3.SS2.p5.1.m1.3.3.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">𝑥</ci><apply id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2"><ci id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.1">~</ci><ci id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.1.1.3">0</cn></apply><apply id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.1.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2">subscript</csymbol><apply id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2"><ci id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.1.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.1">~</ci><ci id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.3.cmml" xref="S3.SS2.p5.1.m1.3.3.1.1.1.2.2.3">1</cn></apply></vector><ci id="S3.SS2.p5.1.m1.2.2.cmml" xref="S3.SS2.p5.1.m1.2.2">𝑦</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.3c">\mathcal{D}^{\text{2afc}}=\{(x,\tilde{x}_{0},\tilde{x}_{1}),y\}</annotation></semantics></math> consists of 20,019 triplets with an average of 7 unanimous votes each. We partition our resulting dataset into train, validation, and test components with a random 80/10/10 split. Our dataset is publicly available on our project page.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Just noticeable differences (JND). </span>
JND aims to characterize the boundary when a distortion becomes <span id="S3.SS2.p6.1.2" class="ltx_text ltx_font_italic">just</span> noticeable. Below a threshold, a small perturbation (e.g., a 1-pixel shift) appears identical to a human observer. This perceptual test provides a complementary signal for perceptual similarity and allows us to exploit uniform timing and the presence of a correct answer.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.5" class="ltx_p">Intuitively, an image pair that falls below the JND threshold should be more likely to be selected as similar in the 2AFC test; thus a high correlation between 2AFC and JND scores indicates that our judgments are reliable under multiple experimental settings.
To assess how well our perceptual tests agree, we collect judgments on the triplets from the 2AFC test set, filtering for triplets that “straddle” the JND threshold. Specifically, given triplet <math id="S3.SS2.p7.1.m1.3" class="ltx_Math" alttext="(x,\tilde{x}_{0},\tilde{x}_{1})\in\mathcal{D}^{\text{2afc}}" display="inline"><semantics id="S3.SS2.p7.1.m1.3a"><mrow id="S3.SS2.p7.1.m1.3.3" xref="S3.SS2.p7.1.m1.3.3.cmml"><mrow id="S3.SS2.p7.1.m1.3.3.2.2" xref="S3.SS2.p7.1.m1.3.3.2.3.cmml"><mo stretchy="false" id="S3.SS2.p7.1.m1.3.3.2.2.3" xref="S3.SS2.p7.1.m1.3.3.2.3.cmml">(</mo><mi id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p7.1.m1.3.3.2.2.4" xref="S3.SS2.p7.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p7.1.m1.2.2.1.1.1" xref="S3.SS2.p7.1.m1.2.2.1.1.1.cmml"><mover accent="true" id="S3.SS2.p7.1.m1.2.2.1.1.1.2" xref="S3.SS2.p7.1.m1.2.2.1.1.1.2.cmml"><mi id="S3.SS2.p7.1.m1.2.2.1.1.1.2.2" xref="S3.SS2.p7.1.m1.2.2.1.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p7.1.m1.2.2.1.1.1.2.1" xref="S3.SS2.p7.1.m1.2.2.1.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p7.1.m1.2.2.1.1.1.3" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p7.1.m1.3.3.2.2.5" xref="S3.SS2.p7.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p7.1.m1.3.3.2.2.2" xref="S3.SS2.p7.1.m1.3.3.2.2.2.cmml"><mover accent="true" id="S3.SS2.p7.1.m1.3.3.2.2.2.2" xref="S3.SS2.p7.1.m1.3.3.2.2.2.2.cmml"><mi id="S3.SS2.p7.1.m1.3.3.2.2.2.2.2" xref="S3.SS2.p7.1.m1.3.3.2.2.2.2.2.cmml">x</mi><mo id="S3.SS2.p7.1.m1.3.3.2.2.2.2.1" xref="S3.SS2.p7.1.m1.3.3.2.2.2.2.1.cmml">~</mo></mover><mn id="S3.SS2.p7.1.m1.3.3.2.2.2.3" xref="S3.SS2.p7.1.m1.3.3.2.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS2.p7.1.m1.3.3.2.2.6" xref="S3.SS2.p7.1.m1.3.3.2.3.cmml">)</mo></mrow><mo id="S3.SS2.p7.1.m1.3.3.3" xref="S3.SS2.p7.1.m1.3.3.3.cmml">∈</mo><msup id="S3.SS2.p7.1.m1.3.3.4" xref="S3.SS2.p7.1.m1.3.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.1.m1.3.3.4.2" xref="S3.SS2.p7.1.m1.3.3.4.2.cmml">𝒟</mi><mtext id="S3.SS2.p7.1.m1.3.3.4.3" xref="S3.SS2.p7.1.m1.3.3.4.3a.cmml">2afc</mtext></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.3b"><apply id="S3.SS2.p7.1.m1.3.3.cmml" xref="S3.SS2.p7.1.m1.3.3"><in id="S3.SS2.p7.1.m1.3.3.3.cmml" xref="S3.SS2.p7.1.m1.3.3.3"></in><vector id="S3.SS2.p7.1.m1.3.3.2.3.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2"><ci id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">𝑥</ci><apply id="S3.SS2.p7.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1">subscript</csymbol><apply id="S3.SS2.p7.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.2"><ci id="S3.SS2.p7.1.m1.2.2.1.1.1.2.1.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.2.1">~</ci><ci id="S3.SS2.p7.1.m1.2.2.1.1.1.2.2.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p7.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.p7.1.m1.2.2.1.1.1.3">0</cn></apply><apply id="S3.SS2.p7.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2.2">subscript</csymbol><apply id="S3.SS2.p7.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2.2.2"><ci id="S3.SS2.p7.1.m1.3.3.2.2.2.2.1.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2.2.2.1">~</ci><ci id="S3.SS2.p7.1.m1.3.3.2.2.2.2.2.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2.2.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p7.1.m1.3.3.2.2.2.3.cmml" xref="S3.SS2.p7.1.m1.3.3.2.2.2.3">1</cn></apply></vector><apply id="S3.SS2.p7.1.m1.3.3.4.cmml" xref="S3.SS2.p7.1.m1.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p7.1.m1.3.3.4.1.cmml" xref="S3.SS2.p7.1.m1.3.3.4">superscript</csymbol><ci id="S3.SS2.p7.1.m1.3.3.4.2.cmml" xref="S3.SS2.p7.1.m1.3.3.4.2">𝒟</ci><ci id="S3.SS2.p7.1.m1.3.3.4.3a.cmml" xref="S3.SS2.p7.1.m1.3.3.4.3"><mtext mathsize="70%" id="S3.SS2.p7.1.m1.3.3.4.3.cmml" xref="S3.SS2.p7.1.m1.3.3.4.3">2afc</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.3c">(x,\tilde{x}_{0},\tilde{x}_{1})\in\mathcal{D}^{\text{2afc}}</annotation></semantics></math>, we independently collect JND judgments on pairs <math id="S3.SS2.p7.2.m2.2" class="ltx_Math" alttext="(x,\tilde{x}_{0})" display="inline"><semantics id="S3.SS2.p7.2.m2.2a"><mrow id="S3.SS2.p7.2.m2.2.2.1" xref="S3.SS2.p7.2.m2.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p7.2.m2.2.2.1.2" xref="S3.SS2.p7.2.m2.2.2.2.cmml">(</mo><mi id="S3.SS2.p7.2.m2.1.1" xref="S3.SS2.p7.2.m2.1.1.cmml">x</mi><mo id="S3.SS2.p7.2.m2.2.2.1.3" xref="S3.SS2.p7.2.m2.2.2.2.cmml">,</mo><msub id="S3.SS2.p7.2.m2.2.2.1.1" xref="S3.SS2.p7.2.m2.2.2.1.1.cmml"><mover accent="true" id="S3.SS2.p7.2.m2.2.2.1.1.2" xref="S3.SS2.p7.2.m2.2.2.1.1.2.cmml"><mi id="S3.SS2.p7.2.m2.2.2.1.1.2.2" xref="S3.SS2.p7.2.m2.2.2.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p7.2.m2.2.2.1.1.2.1" xref="S3.SS2.p7.2.m2.2.2.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p7.2.m2.2.2.1.1.3" xref="S3.SS2.p7.2.m2.2.2.1.1.3.cmml">0</mn></msub><mo stretchy="false" id="S3.SS2.p7.2.m2.2.2.1.4" xref="S3.SS2.p7.2.m2.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.2b"><interval closure="open" id="S3.SS2.p7.2.m2.2.2.2.cmml" xref="S3.SS2.p7.2.m2.2.2.1"><ci id="S3.SS2.p7.2.m2.1.1.cmml" xref="S3.SS2.p7.2.m2.1.1">𝑥</ci><apply id="S3.SS2.p7.2.m2.2.2.1.1.cmml" xref="S3.SS2.p7.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m2.2.2.1.1.1.cmml" xref="S3.SS2.p7.2.m2.2.2.1.1">subscript</csymbol><apply id="S3.SS2.p7.2.m2.2.2.1.1.2.cmml" xref="S3.SS2.p7.2.m2.2.2.1.1.2"><ci id="S3.SS2.p7.2.m2.2.2.1.1.2.1.cmml" xref="S3.SS2.p7.2.m2.2.2.1.1.2.1">~</ci><ci id="S3.SS2.p7.2.m2.2.2.1.1.2.2.cmml" xref="S3.SS2.p7.2.m2.2.2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p7.2.m2.2.2.1.1.3.cmml" xref="S3.SS2.p7.2.m2.2.2.1.1.3">0</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.2c">(x,\tilde{x}_{0})</annotation></semantics></math> and <math id="S3.SS2.p7.3.m3.2" class="ltx_Math" alttext="(x,\tilde{x}_{1})" display="inline"><semantics id="S3.SS2.p7.3.m3.2a"><mrow id="S3.SS2.p7.3.m3.2.2.1" xref="S3.SS2.p7.3.m3.2.2.2.cmml"><mo stretchy="false" id="S3.SS2.p7.3.m3.2.2.1.2" xref="S3.SS2.p7.3.m3.2.2.2.cmml">(</mo><mi id="S3.SS2.p7.3.m3.1.1" xref="S3.SS2.p7.3.m3.1.1.cmml">x</mi><mo id="S3.SS2.p7.3.m3.2.2.1.3" xref="S3.SS2.p7.3.m3.2.2.2.cmml">,</mo><msub id="S3.SS2.p7.3.m3.2.2.1.1" xref="S3.SS2.p7.3.m3.2.2.1.1.cmml"><mover accent="true" id="S3.SS2.p7.3.m3.2.2.1.1.2" xref="S3.SS2.p7.3.m3.2.2.1.1.2.cmml"><mi id="S3.SS2.p7.3.m3.2.2.1.1.2.2" xref="S3.SS2.p7.3.m3.2.2.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p7.3.m3.2.2.1.1.2.1" xref="S3.SS2.p7.3.m3.2.2.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p7.3.m3.2.2.1.1.3" xref="S3.SS2.p7.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS2.p7.3.m3.2.2.1.4" xref="S3.SS2.p7.3.m3.2.2.2.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.2b"><interval closure="open" id="S3.SS2.p7.3.m3.2.2.2.cmml" xref="S3.SS2.p7.3.m3.2.2.1"><ci id="S3.SS2.p7.3.m3.1.1.cmml" xref="S3.SS2.p7.3.m3.1.1">𝑥</ci><apply id="S3.SS2.p7.3.m3.2.2.1.1.cmml" xref="S3.SS2.p7.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.2.2.1.1.1.cmml" xref="S3.SS2.p7.3.m3.2.2.1.1">subscript</csymbol><apply id="S3.SS2.p7.3.m3.2.2.1.1.2.cmml" xref="S3.SS2.p7.3.m3.2.2.1.1.2"><ci id="S3.SS2.p7.3.m3.2.2.1.1.2.1.cmml" xref="S3.SS2.p7.3.m3.2.2.1.1.2.1">~</ci><ci id="S3.SS2.p7.3.m3.2.2.1.1.2.2.cmml" xref="S3.SS2.p7.3.m3.2.2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p7.3.m3.2.2.1.1.3.cmml" xref="S3.SS2.p7.3.m3.2.2.1.1.3">1</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.2c">(x,\tilde{x}_{1})</annotation></semantics></math>, keeping triplets where humans find <span id="S3.SS2.p7.5.1" class="ltx_text ltx_font_italic">one and only one</span> to be identical. In total, our JND dataset contains 411 triplets <math id="S3.SS2.p7.4.m4.3" class="ltx_Math" alttext="\mathcal{D}^{\text{jnd}}=\{(x,\tilde{x}_{0},\tilde{x}_{1}),s\}" display="inline"><semantics id="S3.SS2.p7.4.m4.3a"><mrow id="S3.SS2.p7.4.m4.3.3" xref="S3.SS2.p7.4.m4.3.3.cmml"><msup id="S3.SS2.p7.4.m4.3.3.3" xref="S3.SS2.p7.4.m4.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p7.4.m4.3.3.3.2" xref="S3.SS2.p7.4.m4.3.3.3.2.cmml">𝒟</mi><mtext id="S3.SS2.p7.4.m4.3.3.3.3" xref="S3.SS2.p7.4.m4.3.3.3.3a.cmml">jnd</mtext></msup><mo id="S3.SS2.p7.4.m4.3.3.2" xref="S3.SS2.p7.4.m4.3.3.2.cmml">=</mo><mrow id="S3.SS2.p7.4.m4.3.3.1.1" xref="S3.SS2.p7.4.m4.3.3.1.2.cmml"><mo stretchy="false" id="S3.SS2.p7.4.m4.3.3.1.1.2" xref="S3.SS2.p7.4.m4.3.3.1.2.cmml">{</mo><mrow id="S3.SS2.p7.4.m4.3.3.1.1.1.2" xref="S3.SS2.p7.4.m4.3.3.1.1.1.3.cmml"><mo stretchy="false" id="S3.SS2.p7.4.m4.3.3.1.1.1.2.3" xref="S3.SS2.p7.4.m4.3.3.1.1.1.3.cmml">(</mo><mi id="S3.SS2.p7.4.m4.1.1" xref="S3.SS2.p7.4.m4.1.1.cmml">x</mi><mo id="S3.SS2.p7.4.m4.3.3.1.1.1.2.4" xref="S3.SS2.p7.4.m4.3.3.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.cmml"><mover accent="true" id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.cmml"><mi id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.2" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.2.cmml">x</mi><mo id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.1" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.1.cmml">~</mo></mover><mn id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.3" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.3.cmml">0</mn></msub><mo id="S3.SS2.p7.4.m4.3.3.1.1.1.2.5" xref="S3.SS2.p7.4.m4.3.3.1.1.1.3.cmml">,</mo><msub id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.cmml"><mover accent="true" id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.cmml"><mi id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.2" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.2.cmml">x</mi><mo id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.1" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.1.cmml">~</mo></mover><mn id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.3" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.SS2.p7.4.m4.3.3.1.1.1.2.6" xref="S3.SS2.p7.4.m4.3.3.1.1.1.3.cmml">)</mo></mrow><mo id="S3.SS2.p7.4.m4.3.3.1.1.3" xref="S3.SS2.p7.4.m4.3.3.1.2.cmml">,</mo><mi id="S3.SS2.p7.4.m4.2.2" xref="S3.SS2.p7.4.m4.2.2.cmml">s</mi><mo stretchy="false" id="S3.SS2.p7.4.m4.3.3.1.1.4" xref="S3.SS2.p7.4.m4.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.4.m4.3b"><apply id="S3.SS2.p7.4.m4.3.3.cmml" xref="S3.SS2.p7.4.m4.3.3"><eq id="S3.SS2.p7.4.m4.3.3.2.cmml" xref="S3.SS2.p7.4.m4.3.3.2"></eq><apply id="S3.SS2.p7.4.m4.3.3.3.cmml" xref="S3.SS2.p7.4.m4.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p7.4.m4.3.3.3.1.cmml" xref="S3.SS2.p7.4.m4.3.3.3">superscript</csymbol><ci id="S3.SS2.p7.4.m4.3.3.3.2.cmml" xref="S3.SS2.p7.4.m4.3.3.3.2">𝒟</ci><ci id="S3.SS2.p7.4.m4.3.3.3.3a.cmml" xref="S3.SS2.p7.4.m4.3.3.3.3"><mtext mathsize="70%" id="S3.SS2.p7.4.m4.3.3.3.3.cmml" xref="S3.SS2.p7.4.m4.3.3.3.3">jnd</mtext></ci></apply><set id="S3.SS2.p7.4.m4.3.3.1.2.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1"><vector id="S3.SS2.p7.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2"><ci id="S3.SS2.p7.4.m4.1.1.cmml" xref="S3.SS2.p7.4.m4.1.1">𝑥</ci><apply id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1">subscript</csymbol><apply id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2"><ci id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.1">~</ci><ci id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.2.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.3.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.1.1.3">0</cn></apply><apply id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.1.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2">subscript</csymbol><apply id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2"><ci id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.1.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.1">~</ci><ci id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.2.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.2.2">𝑥</ci></apply><cn type="integer" id="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.3.cmml" xref="S3.SS2.p7.4.m4.3.3.1.1.1.2.2.3">1</cn></apply></vector><ci id="S3.SS2.p7.4.m4.2.2.cmml" xref="S3.SS2.p7.4.m4.2.2">𝑠</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.4.m4.3c">\mathcal{D}^{\text{jnd}}=\{(x,\tilde{x}_{0},\tilde{x}_{1}),s\}</annotation></semantics></math>, where users chose one of <math id="S3.SS2.p7.5.m5.2" class="ltx_Math" alttext="s\in\{0,1\}" display="inline"><semantics id="S3.SS2.p7.5.m5.2a"><mrow id="S3.SS2.p7.5.m5.2.3" xref="S3.SS2.p7.5.m5.2.3.cmml"><mi id="S3.SS2.p7.5.m5.2.3.2" xref="S3.SS2.p7.5.m5.2.3.2.cmml">s</mi><mo id="S3.SS2.p7.5.m5.2.3.1" xref="S3.SS2.p7.5.m5.2.3.1.cmml">∈</mo><mrow id="S3.SS2.p7.5.m5.2.3.3.2" xref="S3.SS2.p7.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.p7.5.m5.2.3.3.2.1" xref="S3.SS2.p7.5.m5.2.3.3.1.cmml">{</mo><mn id="S3.SS2.p7.5.m5.1.1" xref="S3.SS2.p7.5.m5.1.1.cmml">0</mn><mo id="S3.SS2.p7.5.m5.2.3.3.2.2" xref="S3.SS2.p7.5.m5.2.3.3.1.cmml">,</mo><mn id="S3.SS2.p7.5.m5.2.2" xref="S3.SS2.p7.5.m5.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p7.5.m5.2.3.3.2.3" xref="S3.SS2.p7.5.m5.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.5.m5.2b"><apply id="S3.SS2.p7.5.m5.2.3.cmml" xref="S3.SS2.p7.5.m5.2.3"><in id="S3.SS2.p7.5.m5.2.3.1.cmml" xref="S3.SS2.p7.5.m5.2.3.1"></in><ci id="S3.SS2.p7.5.m5.2.3.2.cmml" xref="S3.SS2.p7.5.m5.2.3.2">𝑠</ci><set id="S3.SS2.p7.5.m5.2.3.3.1.cmml" xref="S3.SS2.p7.5.m5.2.3.3.2"><cn type="integer" id="S3.SS2.p7.5.m5.1.1.cmml" xref="S3.SS2.p7.5.m5.1.1">0</cn><cn type="integer" id="S3.SS2.p7.5.m5.2.2.cmml" xref="S3.SS2.p7.5.m5.2.2">1</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.5.m5.2c">s\in\{0,1\}</annotation></semantics></math> to be identical.</p>
</div>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.2" class="ltx_p">We show our JND procedure in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Generating images with varied distortions ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We interleave two images and their respective distortions into sequence <math id="S3.SS2.p8.1.m1.1" class="ltx_Math" alttext="x\rightarrow v\rightarrow\tilde{x}\rightarrow\tilde{v}" display="inline"><semantics id="S3.SS2.p8.1.m1.1a"><mrow id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml">x</mi><mo stretchy="false" id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml">→</mo><mi id="S3.SS2.p8.1.m1.1.1.4" xref="S3.SS2.p8.1.m1.1.1.4.cmml">v</mi><mo stretchy="false" id="S3.SS2.p8.1.m1.1.1.5" xref="S3.SS2.p8.1.m1.1.1.5.cmml">→</mo><mover accent="true" id="S3.SS2.p8.1.m1.1.1.6" xref="S3.SS2.p8.1.m1.1.1.6.cmml"><mi id="S3.SS2.p8.1.m1.1.1.6.2" xref="S3.SS2.p8.1.m1.1.1.6.2.cmml">x</mi><mo id="S3.SS2.p8.1.m1.1.1.6.1" xref="S3.SS2.p8.1.m1.1.1.6.1.cmml">~</mo></mover><mo stretchy="false" id="S3.SS2.p8.1.m1.1.1.7" xref="S3.SS2.p8.1.m1.1.1.7.cmml">→</mo><mover accent="true" id="S3.SS2.p8.1.m1.1.1.8" xref="S3.SS2.p8.1.m1.1.1.8.cmml"><mi id="S3.SS2.p8.1.m1.1.1.8.2" xref="S3.SS2.p8.1.m1.1.1.8.2.cmml">v</mi><mo id="S3.SS2.p8.1.m1.1.1.8.1" xref="S3.SS2.p8.1.m1.1.1.8.1.cmml">~</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><and id="S3.SS2.p8.1.m1.1.1a.cmml" xref="S3.SS2.p8.1.m1.1.1"></and><apply id="S3.SS2.p8.1.m1.1.1b.cmml" xref="S3.SS2.p8.1.m1.1.1"><ci id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3">→</ci><ci id="S3.SS2.p8.1.m1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.2">𝑥</ci><ci id="S3.SS2.p8.1.m1.1.1.4.cmml" xref="S3.SS2.p8.1.m1.1.1.4">𝑣</ci></apply><apply id="S3.SS2.p8.1.m1.1.1c.cmml" xref="S3.SS2.p8.1.m1.1.1"><ci id="S3.SS2.p8.1.m1.1.1.5.cmml" xref="S3.SS2.p8.1.m1.1.1.5">→</ci><share href="#S3.SS2.p8.1.m1.1.1.4.cmml" id="S3.SS2.p8.1.m1.1.1d.cmml" xref="S3.SS2.p8.1.m1.1.1"></share><apply id="S3.SS2.p8.1.m1.1.1.6.cmml" xref="S3.SS2.p8.1.m1.1.1.6"><ci id="S3.SS2.p8.1.m1.1.1.6.1.cmml" xref="S3.SS2.p8.1.m1.1.1.6.1">~</ci><ci id="S3.SS2.p8.1.m1.1.1.6.2.cmml" xref="S3.SS2.p8.1.m1.1.1.6.2">𝑥</ci></apply></apply><apply id="S3.SS2.p8.1.m1.1.1e.cmml" xref="S3.SS2.p8.1.m1.1.1"><ci id="S3.SS2.p8.1.m1.1.1.7.cmml" xref="S3.SS2.p8.1.m1.1.1.7">→</ci><share href="#S3.SS2.p8.1.m1.1.1.6.cmml" id="S3.SS2.p8.1.m1.1.1f.cmml" xref="S3.SS2.p8.1.m1.1.1"></share><apply id="S3.SS2.p8.1.m1.1.1.8.cmml" xref="S3.SS2.p8.1.m1.1.1.8"><ci id="S3.SS2.p8.1.m1.1.1.8.1.cmml" xref="S3.SS2.p8.1.m1.1.1.8.1">~</ci><ci id="S3.SS2.p8.1.m1.1.1.8.2.cmml" xref="S3.SS2.p8.1.m1.1.1.8.2">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">x\rightarrow v\rightarrow\tilde{x}\rightarrow\tilde{v}</annotation></semantics></math>, and ask whether the images in each pair were identical. This exploits masking, a perceptual phenomenon that occurs when an image is obscured by another. By masking with a time gap, we standardize viewing time across users and prompt for a decision based on an initial reaction and memory of the image. For each user, we show 48 distorted pairs, along with 24 identical pairs, to balance the expected responses. We collect 3 judgments per pair, with no person seeing a repeat, and take the majority vote as the label. Users mark 20.4<math id="S3.SS2.p8.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.SS2.p8.2.m2.1a"><mo id="S3.SS2.p8.2.m2.1.1" xref="S3.SS2.p8.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.2.m2.1b"><csymbol cd="latexml" id="S3.SS2.p8.2.m2.1.1.cmml" xref="S3.SS2.p8.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.2.m2.1c">\%</annotation></semantics></math> of distorted images being identical, indicating that some of our distortions indeed fall below the noticeable threshold.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Perceptual metric learning</h2>

<figure id="S4.F3" class="ltx_figure ltx_align_floatright"><img src="/html/2306.09344/assets/x3.png" id="S4.F3.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="138" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> <span id="S4.F3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training overview.<span id="S4.F3.3.1.1" class="ltx_text ltx_font_medium"> Our perceptual metric is an ensemble of backbones, concatenating the representations together and fine-tuning with LoRA. We evaluate how well cosine distance on candidate feature spaces aligns with perceptual judgments <math id="S4.F3.3.1.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.F3.3.1.1.m1.1b"><mi id="S4.F3.3.1.1.m1.1.1" xref="S4.F3.3.1.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.F3.3.1.1.m1.1c"><ci id="S4.F3.3.1.1.m1.1.1.cmml" xref="S4.F3.3.1.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.3.1.1.m1.1d">y</annotation></semantics></math>. </span></span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We next evaluate how well pretrained embeddings and learned metrics align with our data, and we investigate if alignment can be improved by fine-tuning.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Embeddings as a distance metric</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.6" class="ltx_p">We denote a distance between two images as <math id="S4.SS1.p1.1.m1.3" class="ltx_Math" alttext="D(\cdot,\cdot\hskip 1.42262pt;f_{\theta})" display="inline"><semantics id="S4.SS1.p1.1.m1.3a"><mrow id="S4.SS1.p1.1.m1.3.3" xref="S4.SS1.p1.1.m1.3.3.cmml"><mi id="S4.SS1.p1.1.m1.3.3.3" xref="S4.SS1.p1.1.m1.3.3.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.3.3.2" xref="S4.SS1.p1.1.m1.3.3.2.cmml">​</mo><mrow id="S4.SS1.p1.1.m1.3.3.1.1" xref="S4.SS1.p1.1.m1.3.3.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.1.m1.3.3.1.1.2" xref="S4.SS1.p1.1.m1.3.3.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">⋅</mo><mo rspace="0em" id="S4.SS1.p1.1.m1.3.3.1.1.3" xref="S4.SS1.p1.1.m1.3.3.1.2.cmml">,</mo><mo lspace="0em" rspace="0.140em" id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">⋅</mo><mo id="S4.SS1.p1.1.m1.3.3.1.1.4" xref="S4.SS1.p1.1.m1.3.3.1.2.cmml">;</mo><msub id="S4.SS1.p1.1.m1.3.3.1.1.1" xref="S4.SS1.p1.1.m1.3.3.1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.3.3.1.1.1.2" xref="S4.SS1.p1.1.m1.3.3.1.1.1.2.cmml">f</mi><mi id="S4.SS1.p1.1.m1.3.3.1.1.1.3" xref="S4.SS1.p1.1.m1.3.3.1.1.1.3.cmml">θ</mi></msub><mo stretchy="false" id="S4.SS1.p1.1.m1.3.3.1.1.5" xref="S4.SS1.p1.1.m1.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.3b"><apply id="S4.SS1.p1.1.m1.3.3.cmml" xref="S4.SS1.p1.1.m1.3.3"><times id="S4.SS1.p1.1.m1.3.3.2.cmml" xref="S4.SS1.p1.1.m1.3.3.2"></times><ci id="S4.SS1.p1.1.m1.3.3.3.cmml" xref="S4.SS1.p1.1.m1.3.3.3">𝐷</ci><vector id="S4.SS1.p1.1.m1.3.3.1.2.cmml" xref="S4.SS1.p1.1.m1.3.3.1.1"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">⋅</ci><ci id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">⋅</ci><apply id="S4.SS1.p1.1.m1.3.3.1.1.1.cmml" xref="S4.SS1.p1.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.3.3.1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.3.3.1.1.1.2">𝑓</ci><ci id="S4.SS1.p1.1.m1.3.3.1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.3.3.1.1.1.3">𝜃</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.3c">D(\cdot,\cdot\hskip 1.42262pt;f_{\theta})</annotation></semantics></math>, where <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="f_{\theta}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">f</mi><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝑓</ci><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">f_{\theta}</annotation></semantics></math> is a feature extractor. We evaluate a variety of state-of-the-art candidate embeddings and metrics. LPIPS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> and DISTS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> use CNN backbones, whereas DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and MAE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> use transformer-based backbones <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Following standard practice, distance <math id="S4.SS1.p1.3.m3.7" class="ltx_Math" alttext="D(x,\tilde{x};f_{\theta})=1-\text{cos}\big{(}f_{\theta}(x),f_{\theta}(\tilde{x})\big{)}" display="inline"><semantics id="S4.SS1.p1.3.m3.7a"><mrow id="S4.SS1.p1.3.m3.7.7" xref="S4.SS1.p1.3.m3.7.7.cmml"><mrow id="S4.SS1.p1.3.m3.5.5.1" xref="S4.SS1.p1.3.m3.5.5.1.cmml"><mi id="S4.SS1.p1.3.m3.5.5.1.3" xref="S4.SS1.p1.3.m3.5.5.1.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.5.5.1.2" xref="S4.SS1.p1.3.m3.5.5.1.2.cmml">​</mo><mrow id="S4.SS1.p1.3.m3.5.5.1.1.1" xref="S4.SS1.p1.3.m3.5.5.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.3.m3.5.5.1.1.1.2" xref="S4.SS1.p1.3.m3.5.5.1.1.2.cmml">(</mo><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">x</mi><mo id="S4.SS1.p1.3.m3.5.5.1.1.1.3" xref="S4.SS1.p1.3.m3.5.5.1.1.2.cmml">,</mo><mover accent="true" id="S4.SS1.p1.3.m3.2.2" xref="S4.SS1.p1.3.m3.2.2.cmml"><mi id="S4.SS1.p1.3.m3.2.2.2" xref="S4.SS1.p1.3.m3.2.2.2.cmml">x</mi><mo id="S4.SS1.p1.3.m3.2.2.1" xref="S4.SS1.p1.3.m3.2.2.1.cmml">~</mo></mover><mo id="S4.SS1.p1.3.m3.5.5.1.1.1.4" xref="S4.SS1.p1.3.m3.5.5.1.1.2.cmml">;</mo><msub id="S4.SS1.p1.3.m3.5.5.1.1.1.1" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1.cmml"><mi id="S4.SS1.p1.3.m3.5.5.1.1.1.1.2" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1.2.cmml">f</mi><mi id="S4.SS1.p1.3.m3.5.5.1.1.1.1.3" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1.3.cmml">θ</mi></msub><mo stretchy="false" id="S4.SS1.p1.3.m3.5.5.1.1.1.5" xref="S4.SS1.p1.3.m3.5.5.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.3.m3.7.7.4" xref="S4.SS1.p1.3.m3.7.7.4.cmml">=</mo><mrow id="S4.SS1.p1.3.m3.7.7.3" xref="S4.SS1.p1.3.m3.7.7.3.cmml"><mn id="S4.SS1.p1.3.m3.7.7.3.4" xref="S4.SS1.p1.3.m3.7.7.3.4.cmml">1</mn><mo id="S4.SS1.p1.3.m3.7.7.3.3" xref="S4.SS1.p1.3.m3.7.7.3.3.cmml">−</mo><mrow id="S4.SS1.p1.3.m3.7.7.3.2" xref="S4.SS1.p1.3.m3.7.7.3.2.cmml"><mtext id="S4.SS1.p1.3.m3.7.7.3.2.4" xref="S4.SS1.p1.3.m3.7.7.3.2.4a.cmml">cos</mtext><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.7.7.3.2.3" xref="S4.SS1.p1.3.m3.7.7.3.2.3.cmml">​</mo><mrow id="S4.SS1.p1.3.m3.7.7.3.2.2.2" xref="S4.SS1.p1.3.m3.7.7.3.2.2.3.cmml"><mo maxsize="120%" minsize="120%" id="S4.SS1.p1.3.m3.7.7.3.2.2.2.3" xref="S4.SS1.p1.3.m3.7.7.3.2.2.3.cmml">(</mo><mrow id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.cmml"><msub id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.cmml"><mi id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.2" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.2.cmml">f</mi><mi id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.3" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.1" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.1.cmml">​</mo><mrow id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.3.2" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.3.2.1" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.cmml">(</mo><mi id="S4.SS1.p1.3.m3.3.3" xref="S4.SS1.p1.3.m3.3.3.cmml">x</mi><mo stretchy="false" id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.3.2.2" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p1.3.m3.7.7.3.2.2.2.4" xref="S4.SS1.p1.3.m3.7.7.3.2.2.3.cmml">,</mo><mrow id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.cmml"><msub id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.cmml"><mi id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.2" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.2.cmml">f</mi><mi id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.3" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.3.cmml">θ</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.1" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.1.cmml">​</mo><mrow id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.3.2" xref="S4.SS1.p1.3.m3.4.4.cmml"><mo stretchy="false" id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.3.2.1" xref="S4.SS1.p1.3.m3.4.4.cmml">(</mo><mover accent="true" id="S4.SS1.p1.3.m3.4.4" xref="S4.SS1.p1.3.m3.4.4.cmml"><mi id="S4.SS1.p1.3.m3.4.4.2" xref="S4.SS1.p1.3.m3.4.4.2.cmml">x</mi><mo id="S4.SS1.p1.3.m3.4.4.1" xref="S4.SS1.p1.3.m3.4.4.1.cmml">~</mo></mover><mo stretchy="false" id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.3.2.2" xref="S4.SS1.p1.3.m3.4.4.cmml">)</mo></mrow></mrow><mo maxsize="120%" minsize="120%" id="S4.SS1.p1.3.m3.7.7.3.2.2.2.5" xref="S4.SS1.p1.3.m3.7.7.3.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.7b"><apply id="S4.SS1.p1.3.m3.7.7.cmml" xref="S4.SS1.p1.3.m3.7.7"><eq id="S4.SS1.p1.3.m3.7.7.4.cmml" xref="S4.SS1.p1.3.m3.7.7.4"></eq><apply id="S4.SS1.p1.3.m3.5.5.1.cmml" xref="S4.SS1.p1.3.m3.5.5.1"><times id="S4.SS1.p1.3.m3.5.5.1.2.cmml" xref="S4.SS1.p1.3.m3.5.5.1.2"></times><ci id="S4.SS1.p1.3.m3.5.5.1.3.cmml" xref="S4.SS1.p1.3.m3.5.5.1.3">𝐷</ci><vector id="S4.SS1.p1.3.m3.5.5.1.1.2.cmml" xref="S4.SS1.p1.3.m3.5.5.1.1.1"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑥</ci><apply id="S4.SS1.p1.3.m3.2.2.cmml" xref="S4.SS1.p1.3.m3.2.2"><ci id="S4.SS1.p1.3.m3.2.2.1.cmml" xref="S4.SS1.p1.3.m3.2.2.1">~</ci><ci id="S4.SS1.p1.3.m3.2.2.2.cmml" xref="S4.SS1.p1.3.m3.2.2.2">𝑥</ci></apply><apply id="S4.SS1.p1.3.m3.5.5.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.5.5.1.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.3.m3.5.5.1.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1.2">𝑓</ci><ci id="S4.SS1.p1.3.m3.5.5.1.1.1.1.3.cmml" xref="S4.SS1.p1.3.m3.5.5.1.1.1.1.3">𝜃</ci></apply></vector></apply><apply id="S4.SS1.p1.3.m3.7.7.3.cmml" xref="S4.SS1.p1.3.m3.7.7.3"><minus id="S4.SS1.p1.3.m3.7.7.3.3.cmml" xref="S4.SS1.p1.3.m3.7.7.3.3"></minus><cn type="integer" id="S4.SS1.p1.3.m3.7.7.3.4.cmml" xref="S4.SS1.p1.3.m3.7.7.3.4">1</cn><apply id="S4.SS1.p1.3.m3.7.7.3.2.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2"><times id="S4.SS1.p1.3.m3.7.7.3.2.3.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.3"></times><ci id="S4.SS1.p1.3.m3.7.7.3.2.4a.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.4"><mtext id="S4.SS1.p1.3.m3.7.7.3.2.4.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.4">cos</mtext></ci><interval closure="open" id="S4.SS1.p1.3.m3.7.7.3.2.2.3.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2"><apply id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1"><times id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.1.cmml" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.1"></times><apply id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.cmml" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.1.cmml" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.2.cmml" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.2">𝑓</ci><ci id="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.3.cmml" xref="S4.SS1.p1.3.m3.6.6.2.1.1.1.1.2.3">𝜃</ci></apply><ci id="S4.SS1.p1.3.m3.3.3.cmml" xref="S4.SS1.p1.3.m3.3.3">𝑥</ci></apply><apply id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2"><times id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.1.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.1"></times><apply id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.1.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2">subscript</csymbol><ci id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.2.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.2">𝑓</ci><ci id="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.3.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.2.3">𝜃</ci></apply><apply id="S4.SS1.p1.3.m3.4.4.cmml" xref="S4.SS1.p1.3.m3.7.7.3.2.2.2.2.3.2"><ci id="S4.SS1.p1.3.m3.4.4.1.cmml" xref="S4.SS1.p1.3.m3.4.4.1">~</ci><ci id="S4.SS1.p1.3.m3.4.4.2.cmml" xref="S4.SS1.p1.3.m3.4.4.2">𝑥</ci></apply></apply></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.7c">D(x,\tilde{x};f_{\theta})=1-\text{cos}\big{(}f_{\theta}(x),f_{\theta}(\tilde{x})\big{)}</annotation></semantics></math> is taken as the cosine distance between the <span id="S4.SS1.p1.6.1" class="ltx_text ltx_font_typewriter">CLS</span> tokens taken from the last layer for DINO and MAE (before and after the layer normalization, respectively), and the embedding vector for CLIP and OpenCLIP. For LPIPS and DISTS, <math id="S4.SS1.p1.4.m4.3" class="ltx_Math" alttext="D(x,\tilde{x};f_{\theta})" display="inline"><semantics id="S4.SS1.p1.4.m4.3a"><mrow id="S4.SS1.p1.4.m4.3.3" xref="S4.SS1.p1.4.m4.3.3.cmml"><mi id="S4.SS1.p1.4.m4.3.3.3" xref="S4.SS1.p1.4.m4.3.3.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.4.m4.3.3.2" xref="S4.SS1.p1.4.m4.3.3.2.cmml">​</mo><mrow id="S4.SS1.p1.4.m4.3.3.1.1" xref="S4.SS1.p1.4.m4.3.3.1.2.cmml"><mo stretchy="false" id="S4.SS1.p1.4.m4.3.3.1.1.2" xref="S4.SS1.p1.4.m4.3.3.1.2.cmml">(</mo><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">x</mi><mo id="S4.SS1.p1.4.m4.3.3.1.1.3" xref="S4.SS1.p1.4.m4.3.3.1.2.cmml">,</mo><mover accent="true" id="S4.SS1.p1.4.m4.2.2" xref="S4.SS1.p1.4.m4.2.2.cmml"><mi id="S4.SS1.p1.4.m4.2.2.2" xref="S4.SS1.p1.4.m4.2.2.2.cmml">x</mi><mo id="S4.SS1.p1.4.m4.2.2.1" xref="S4.SS1.p1.4.m4.2.2.1.cmml">~</mo></mover><mo id="S4.SS1.p1.4.m4.3.3.1.1.4" xref="S4.SS1.p1.4.m4.3.3.1.2.cmml">;</mo><msub id="S4.SS1.p1.4.m4.3.3.1.1.1" xref="S4.SS1.p1.4.m4.3.3.1.1.1.cmml"><mi id="S4.SS1.p1.4.m4.3.3.1.1.1.2" xref="S4.SS1.p1.4.m4.3.3.1.1.1.2.cmml">f</mi><mi id="S4.SS1.p1.4.m4.3.3.1.1.1.3" xref="S4.SS1.p1.4.m4.3.3.1.1.1.3.cmml">θ</mi></msub><mo stretchy="false" id="S4.SS1.p1.4.m4.3.3.1.1.5" xref="S4.SS1.p1.4.m4.3.3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.3b"><apply id="S4.SS1.p1.4.m4.3.3.cmml" xref="S4.SS1.p1.4.m4.3.3"><times id="S4.SS1.p1.4.m4.3.3.2.cmml" xref="S4.SS1.p1.4.m4.3.3.2"></times><ci id="S4.SS1.p1.4.m4.3.3.3.cmml" xref="S4.SS1.p1.4.m4.3.3.3">𝐷</ci><vector id="S4.SS1.p1.4.m4.3.3.1.2.cmml" xref="S4.SS1.p1.4.m4.3.3.1.1"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">𝑥</ci><apply id="S4.SS1.p1.4.m4.2.2.cmml" xref="S4.SS1.p1.4.m4.2.2"><ci id="S4.SS1.p1.4.m4.2.2.1.cmml" xref="S4.SS1.p1.4.m4.2.2.1">~</ci><ci id="S4.SS1.p1.4.m4.2.2.2.cmml" xref="S4.SS1.p1.4.m4.2.2.2">𝑥</ci></apply><apply id="S4.SS1.p1.4.m4.3.3.1.1.1.cmml" xref="S4.SS1.p1.4.m4.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.3.3.1.1.1.1.cmml" xref="S4.SS1.p1.4.m4.3.3.1.1.1">subscript</csymbol><ci id="S4.SS1.p1.4.m4.3.3.1.1.1.2.cmml" xref="S4.SS1.p1.4.m4.3.3.1.1.1.2">𝑓</ci><ci id="S4.SS1.p1.4.m4.3.3.1.1.1.3.cmml" xref="S4.SS1.p1.4.m4.3.3.1.1.1.3">𝜃</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.3c">D(x,\tilde{x};f_{\theta})</annotation></semantics></math> is simply the distance metric itself.
Given a triplet <math id="S4.SS1.p1.5.m5.3" class="ltx_Math" alttext="(x,\tilde{x}_{0},\tilde{x}_{1})" display="inline"><semantics id="S4.SS1.p1.5.m5.3a"><mrow id="S4.SS1.p1.5.m5.3.3.2" xref="S4.SS1.p1.5.m5.3.3.3.cmml"><mo stretchy="false" id="S4.SS1.p1.5.m5.3.3.2.3" xref="S4.SS1.p1.5.m5.3.3.3.cmml">(</mo><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">x</mi><mo id="S4.SS1.p1.5.m5.3.3.2.4" xref="S4.SS1.p1.5.m5.3.3.3.cmml">,</mo><msub id="S4.SS1.p1.5.m5.2.2.1.1" xref="S4.SS1.p1.5.m5.2.2.1.1.cmml"><mover accent="true" id="S4.SS1.p1.5.m5.2.2.1.1.2" xref="S4.SS1.p1.5.m5.2.2.1.1.2.cmml"><mi id="S4.SS1.p1.5.m5.2.2.1.1.2.2" xref="S4.SS1.p1.5.m5.2.2.1.1.2.2.cmml">x</mi><mo id="S4.SS1.p1.5.m5.2.2.1.1.2.1" xref="S4.SS1.p1.5.m5.2.2.1.1.2.1.cmml">~</mo></mover><mn id="S4.SS1.p1.5.m5.2.2.1.1.3" xref="S4.SS1.p1.5.m5.2.2.1.1.3.cmml">0</mn></msub><mo id="S4.SS1.p1.5.m5.3.3.2.5" xref="S4.SS1.p1.5.m5.3.3.3.cmml">,</mo><msub id="S4.SS1.p1.5.m5.3.3.2.2" xref="S4.SS1.p1.5.m5.3.3.2.2.cmml"><mover accent="true" id="S4.SS1.p1.5.m5.3.3.2.2.2" xref="S4.SS1.p1.5.m5.3.3.2.2.2.cmml"><mi id="S4.SS1.p1.5.m5.3.3.2.2.2.2" xref="S4.SS1.p1.5.m5.3.3.2.2.2.2.cmml">x</mi><mo id="S4.SS1.p1.5.m5.3.3.2.2.2.1" xref="S4.SS1.p1.5.m5.3.3.2.2.2.1.cmml">~</mo></mover><mn id="S4.SS1.p1.5.m5.3.3.2.2.3" xref="S4.SS1.p1.5.m5.3.3.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S4.SS1.p1.5.m5.3.3.2.6" xref="S4.SS1.p1.5.m5.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.3b"><vector id="S4.SS1.p1.5.m5.3.3.3.cmml" xref="S4.SS1.p1.5.m5.3.3.2"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑥</ci><apply id="S4.SS1.p1.5.m5.2.2.1.1.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.2.2.1.1.1.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1">subscript</csymbol><apply id="S4.SS1.p1.5.m5.2.2.1.1.2.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.2"><ci id="S4.SS1.p1.5.m5.2.2.1.1.2.1.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.2.1">~</ci><ci id="S4.SS1.p1.5.m5.2.2.1.1.2.2.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S4.SS1.p1.5.m5.2.2.1.1.3.cmml" xref="S4.SS1.p1.5.m5.2.2.1.1.3">0</cn></apply><apply id="S4.SS1.p1.5.m5.3.3.2.2.cmml" xref="S4.SS1.p1.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.3.3.2.2.1.cmml" xref="S4.SS1.p1.5.m5.3.3.2.2">subscript</csymbol><apply id="S4.SS1.p1.5.m5.3.3.2.2.2.cmml" xref="S4.SS1.p1.5.m5.3.3.2.2.2"><ci id="S4.SS1.p1.5.m5.3.3.2.2.2.1.cmml" xref="S4.SS1.p1.5.m5.3.3.2.2.2.1">~</ci><ci id="S4.SS1.p1.5.m5.3.3.2.2.2.2.cmml" xref="S4.SS1.p1.5.m5.3.3.2.2.2.2">𝑥</ci></apply><cn type="integer" id="S4.SS1.p1.5.m5.3.3.2.2.3.cmml" xref="S4.SS1.p1.5.m5.3.3.2.2.3">1</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.3c">(x,\tilde{x}_{0},\tilde{x}_{1})</annotation></semantics></math>, and a feature extractor <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="f_{\theta}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><msub id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">f</mi><mi id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">𝑓</ci><ci id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">f_{\theta}</annotation></semantics></math>, the model vote is calculated as:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.7" class="ltx_Math" alttext="\hat{y}=\begin{cases}1,&amp;d_{1}&lt;d_{0}\\
0,&amp;d_{0}&lt;d_{1}\end{cases},\text{where}\hskip 2.84526ptd_{0}=D(x,\tilde{x}_{0};f_{\theta})\hskip 2.84526pt\text{and}\hskip 2.84526ptd_{1}=D(x,\tilde{x}_{1};f_{\theta})." display="block"><semantics id="S4.E1.m1.7a"><mrow id="S4.E1.m1.7.7.1"><mrow id="S4.E1.m1.7.7.1.1.2" xref="S4.E1.m1.7.7.1.1.3.cmml"><mrow id="S4.E1.m1.7.7.1.1.1.1" xref="S4.E1.m1.7.7.1.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.7.7.1.1.1.1.2" xref="S4.E1.m1.7.7.1.1.1.1.2.cmml"><mi id="S4.E1.m1.7.7.1.1.1.1.2.2" xref="S4.E1.m1.7.7.1.1.1.1.2.2.cmml">y</mi><mo id="S4.E1.m1.7.7.1.1.1.1.2.1" xref="S4.E1.m1.7.7.1.1.1.1.2.1.cmml">^</mo></mover><mo id="S4.E1.m1.7.7.1.1.1.1.1" xref="S4.E1.m1.7.7.1.1.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.4.4" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mo id="S4.E1.m1.4.4.5" xref="S4.E1.m1.7.7.1.1.1.1.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E1.m1.4.4.4" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mtr id="S4.E1.m1.4.4.4a" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4b" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mrow id="S4.E1.m1.1.1.1.1.1.1.3" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mn id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">1</mn><mo id="S4.E1.m1.1.1.1.1.1.1.3.1" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4c" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mrow id="S4.E1.m1.2.2.2.2.2.1" xref="S4.E1.m1.2.2.2.2.2.1.cmml"><msub id="S4.E1.m1.2.2.2.2.2.1.2" xref="S4.E1.m1.2.2.2.2.2.1.2.cmml"><mi id="S4.E1.m1.2.2.2.2.2.1.2.2" xref="S4.E1.m1.2.2.2.2.2.1.2.2.cmml">d</mi><mn id="S4.E1.m1.2.2.2.2.2.1.2.3" xref="S4.E1.m1.2.2.2.2.2.1.2.3.cmml">1</mn></msub><mo id="S4.E1.m1.2.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.2.1.1.cmml">&lt;</mo><msub id="S4.E1.m1.2.2.2.2.2.1.3" xref="S4.E1.m1.2.2.2.2.2.1.3.cmml"><mi id="S4.E1.m1.2.2.2.2.2.1.3.2" xref="S4.E1.m1.2.2.2.2.2.1.3.2.cmml">d</mi><mn id="S4.E1.m1.2.2.2.2.2.1.3.3" xref="S4.E1.m1.2.2.2.2.2.1.3.3.cmml">0</mn></msub></mrow></mtd></mtr><mtr id="S4.E1.m1.4.4.4d" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4e" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mrow id="S4.E1.m1.3.3.3.3.1.1.3" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mn id="S4.E1.m1.3.3.3.3.1.1.1" xref="S4.E1.m1.3.3.3.3.1.1.1.cmml">0</mn><mo id="S4.E1.m1.3.3.3.3.1.1.3.1" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E1.m1.4.4.4f" xref="S4.E1.m1.7.7.1.1.1.1.3.1.cmml"><mrow id="S4.E1.m1.4.4.4.4.2.1" xref="S4.E1.m1.4.4.4.4.2.1.cmml"><msub id="S4.E1.m1.4.4.4.4.2.1.2" xref="S4.E1.m1.4.4.4.4.2.1.2.cmml"><mi id="S4.E1.m1.4.4.4.4.2.1.2.2" xref="S4.E1.m1.4.4.4.4.2.1.2.2.cmml">d</mi><mn id="S4.E1.m1.4.4.4.4.2.1.2.3" xref="S4.E1.m1.4.4.4.4.2.1.2.3.cmml">0</mn></msub><mo id="S4.E1.m1.4.4.4.4.2.1.1" xref="S4.E1.m1.4.4.4.4.2.1.1.cmml">&lt;</mo><msub id="S4.E1.m1.4.4.4.4.2.1.3" xref="S4.E1.m1.4.4.4.4.2.1.3.cmml"><mi id="S4.E1.m1.4.4.4.4.2.1.3.2" xref="S4.E1.m1.4.4.4.4.2.1.3.2.cmml">d</mi><mn id="S4.E1.m1.4.4.4.4.2.1.3.3" xref="S4.E1.m1.4.4.4.4.2.1.3.3.cmml">1</mn></msub></mrow></mtd></mtr></mtable></mrow></mrow><mo id="S4.E1.m1.7.7.1.1.2.3" xref="S4.E1.m1.7.7.1.1.3a.cmml">,</mo><mrow id="S4.E1.m1.7.7.1.1.2.2" xref="S4.E1.m1.7.7.1.1.2.2.cmml"><mrow id="S4.E1.m1.7.7.1.1.2.2.6" xref="S4.E1.m1.7.7.1.1.2.2.6.cmml"><mtext id="S4.E1.m1.7.7.1.1.2.2.6.2" xref="S4.E1.m1.7.7.1.1.2.2.6.2a.cmml">where</mtext><mo lspace="0.280em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.2.6.1" xref="S4.E1.m1.7.7.1.1.2.2.6.1.cmml">​</mo><msub id="S4.E1.m1.7.7.1.1.2.2.6.3" xref="S4.E1.m1.7.7.1.1.2.2.6.3.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.6.3.2" xref="S4.E1.m1.7.7.1.1.2.2.6.3.2.cmml">d</mi><mn id="S4.E1.m1.7.7.1.1.2.2.6.3.3" xref="S4.E1.m1.7.7.1.1.2.2.6.3.3.cmml">0</mn></msub></mrow><mo id="S4.E1.m1.7.7.1.1.2.2.7" xref="S4.E1.m1.7.7.1.1.2.2.7.cmml">=</mo><mrow id="S4.E1.m1.7.7.1.1.2.2.2" xref="S4.E1.m1.7.7.1.1.2.2.2.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.2.4" xref="S4.E1.m1.7.7.1.1.2.2.2.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.2.2.3" xref="S4.E1.m1.7.7.1.1.2.2.2.3.cmml">​</mo><mrow id="S4.E1.m1.7.7.1.1.2.2.2.2.2" xref="S4.E1.m1.7.7.1.1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E1.m1.7.7.1.1.2.2.2.2.2.3" xref="S4.E1.m1.7.7.1.1.2.2.2.2.3.cmml">(</mo><mi id="S4.E1.m1.5.5" xref="S4.E1.m1.5.5.cmml">x</mi><mo id="S4.E1.m1.7.7.1.1.2.2.2.2.2.4" xref="S4.E1.m1.7.7.1.1.2.2.2.2.3.cmml">,</mo><msub id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.2" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.2.cmml">x</mi><mo id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.1" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.1.cmml">~</mo></mover><mn id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.3" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.E1.m1.7.7.1.1.2.2.2.2.2.5" xref="S4.E1.m1.7.7.1.1.2.2.2.2.3.cmml">;</mo><msub id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.2" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.2.cmml">f</mi><mi id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.3" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.3.cmml">θ</mi></msub><mo stretchy="false" id="S4.E1.m1.7.7.1.1.2.2.2.2.2.6" xref="S4.E1.m1.7.7.1.1.2.2.2.2.3.cmml">)</mo></mrow><mo lspace="0.280em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.2.2.3a" xref="S4.E1.m1.7.7.1.1.2.2.2.3.cmml">​</mo><mtext id="S4.E1.m1.7.7.1.1.2.2.2.5" xref="S4.E1.m1.7.7.1.1.2.2.2.5a.cmml">and</mtext><mo lspace="0.280em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.2.2.3b" xref="S4.E1.m1.7.7.1.1.2.2.2.3.cmml">​</mo><msub id="S4.E1.m1.7.7.1.1.2.2.2.6" xref="S4.E1.m1.7.7.1.1.2.2.2.6.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.2.6.2" xref="S4.E1.m1.7.7.1.1.2.2.2.6.2.cmml">d</mi><mn id="S4.E1.m1.7.7.1.1.2.2.2.6.3" xref="S4.E1.m1.7.7.1.1.2.2.2.6.3.cmml">1</mn></msub></mrow><mo id="S4.E1.m1.7.7.1.1.2.2.8" xref="S4.E1.m1.7.7.1.1.2.2.8.cmml">=</mo><mrow id="S4.E1.m1.7.7.1.1.2.2.4" xref="S4.E1.m1.7.7.1.1.2.2.4.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.4.4" xref="S4.E1.m1.7.7.1.1.2.2.4.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.7.7.1.1.2.2.4.3" xref="S4.E1.m1.7.7.1.1.2.2.4.3.cmml">​</mo><mrow id="S4.E1.m1.7.7.1.1.2.2.4.2.2" xref="S4.E1.m1.7.7.1.1.2.2.4.2.3.cmml"><mo stretchy="false" id="S4.E1.m1.7.7.1.1.2.2.4.2.2.3" xref="S4.E1.m1.7.7.1.1.2.2.4.2.3.cmml">(</mo><mi id="S4.E1.m1.6.6" xref="S4.E1.m1.6.6.cmml">x</mi><mo id="S4.E1.m1.7.7.1.1.2.2.4.2.2.4" xref="S4.E1.m1.7.7.1.1.2.2.4.2.3.cmml">,</mo><msub id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.cmml"><mover accent="true" id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.2" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.2.cmml">x</mi><mo id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.1" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.1.cmml">~</mo></mover><mn id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.3" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.3.cmml">1</mn></msub><mo id="S4.E1.m1.7.7.1.1.2.2.4.2.2.5" xref="S4.E1.m1.7.7.1.1.2.2.4.2.3.cmml">;</mo><msub id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.cmml"><mi id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.2" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.2.cmml">f</mi><mi id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.3" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.3.cmml">θ</mi></msub><mo stretchy="false" id="S4.E1.m1.7.7.1.1.2.2.4.2.2.6" xref="S4.E1.m1.7.7.1.1.2.2.4.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" id="S4.E1.m1.7.7.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.7b"><apply id="S4.E1.m1.7.7.1.1.3.cmml" xref="S4.E1.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.3a.cmml" xref="S4.E1.m1.7.7.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E1.m1.7.7.1.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.1.1"><eq id="S4.E1.m1.7.7.1.1.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.1.1.1"></eq><apply id="S4.E1.m1.7.7.1.1.1.1.2.cmml" xref="S4.E1.m1.7.7.1.1.1.1.2"><ci id="S4.E1.m1.7.7.1.1.1.1.2.1.cmml" xref="S4.E1.m1.7.7.1.1.1.1.2.1">^</ci><ci id="S4.E1.m1.7.7.1.1.1.1.2.2.cmml" xref="S4.E1.m1.7.7.1.1.1.1.2.2">𝑦</ci></apply><apply id="S4.E1.m1.7.7.1.1.1.1.3.1.cmml" xref="S4.E1.m1.4.4"><csymbol cd="latexml" id="S4.E1.m1.7.7.1.1.1.1.3.1.1.cmml" xref="S4.E1.m1.4.4.5">cases</csymbol><cn type="integer" id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1">1</cn><apply id="S4.E1.m1.2.2.2.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1"><lt id="S4.E1.m1.2.2.2.2.2.1.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.1"></lt><apply id="S4.E1.m1.2.2.2.2.2.1.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.2.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.2.1.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.2.2">𝑑</ci><cn type="integer" id="S4.E1.m1.2.2.2.2.2.1.2.3.cmml" xref="S4.E1.m1.2.2.2.2.2.1.2.3">1</cn></apply><apply id="S4.E1.m1.2.2.2.2.2.1.3.cmml" xref="S4.E1.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.2.1.3.1.cmml" xref="S4.E1.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.2.1.3.2.cmml" xref="S4.E1.m1.2.2.2.2.2.1.3.2">𝑑</ci><cn type="integer" id="S4.E1.m1.2.2.2.2.2.1.3.3.cmml" xref="S4.E1.m1.2.2.2.2.2.1.3.3">0</cn></apply></apply><cn type="integer" id="S4.E1.m1.3.3.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.3.3.1.1.1">0</cn><apply id="S4.E1.m1.4.4.4.4.2.1.cmml" xref="S4.E1.m1.4.4.4.4.2.1"><lt id="S4.E1.m1.4.4.4.4.2.1.1.cmml" xref="S4.E1.m1.4.4.4.4.2.1.1"></lt><apply id="S4.E1.m1.4.4.4.4.2.1.2.cmml" xref="S4.E1.m1.4.4.4.4.2.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.4.2.1.2.1.cmml" xref="S4.E1.m1.4.4.4.4.2.1.2">subscript</csymbol><ci id="S4.E1.m1.4.4.4.4.2.1.2.2.cmml" xref="S4.E1.m1.4.4.4.4.2.1.2.2">𝑑</ci><cn type="integer" id="S4.E1.m1.4.4.4.4.2.1.2.3.cmml" xref="S4.E1.m1.4.4.4.4.2.1.2.3">0</cn></apply><apply id="S4.E1.m1.4.4.4.4.2.1.3.cmml" xref="S4.E1.m1.4.4.4.4.2.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.4.4.2.1.3.1.cmml" xref="S4.E1.m1.4.4.4.4.2.1.3">subscript</csymbol><ci id="S4.E1.m1.4.4.4.4.2.1.3.2.cmml" xref="S4.E1.m1.4.4.4.4.2.1.3.2">𝑑</ci><cn type="integer" id="S4.E1.m1.4.4.4.4.2.1.3.3.cmml" xref="S4.E1.m1.4.4.4.4.2.1.3.3">1</cn></apply></apply></apply></apply><apply id="S4.E1.m1.7.7.1.1.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2"><and id="S4.E1.m1.7.7.1.1.2.2a.cmml" xref="S4.E1.m1.7.7.1.1.2.2"></and><apply id="S4.E1.m1.7.7.1.1.2.2b.cmml" xref="S4.E1.m1.7.7.1.1.2.2"><eq id="S4.E1.m1.7.7.1.1.2.2.7.cmml" xref="S4.E1.m1.7.7.1.1.2.2.7"></eq><apply id="S4.E1.m1.7.7.1.1.2.2.6.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6"><times id="S4.E1.m1.7.7.1.1.2.2.6.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.1"></times><ci id="S4.E1.m1.7.7.1.1.2.2.6.2a.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.2"><mtext id="S4.E1.m1.7.7.1.1.2.2.6.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.2">where</mtext></ci><apply id="S4.E1.m1.7.7.1.1.2.2.6.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.3"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.2.6.3.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.3">subscript</csymbol><ci id="S4.E1.m1.7.7.1.1.2.2.6.3.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.3.2">𝑑</ci><cn type="integer" id="S4.E1.m1.7.7.1.1.2.2.6.3.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.6.3.3">0</cn></apply></apply><apply id="S4.E1.m1.7.7.1.1.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2"><times id="S4.E1.m1.7.7.1.1.2.2.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.3"></times><ci id="S4.E1.m1.7.7.1.1.2.2.2.4.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.4">𝐷</ci><vector id="S4.E1.m1.7.7.1.1.2.2.2.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2"><ci id="S4.E1.m1.5.5.cmml" xref="S4.E1.m1.5.5">𝑥</ci><apply id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1">subscript</csymbol><apply id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2"><ci id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.1">~</ci><ci id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.1.1.1.1.3">0</cn></apply><apply id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.2">𝑓</ci><ci id="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.2.2.2.3">𝜃</ci></apply></vector><ci id="S4.E1.m1.7.7.1.1.2.2.2.5a.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.5"><mtext id="S4.E1.m1.7.7.1.1.2.2.2.5.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.5">and</mtext></ci><apply id="S4.E1.m1.7.7.1.1.2.2.2.6.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.6"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.2.2.6.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.6">subscript</csymbol><ci id="S4.E1.m1.7.7.1.1.2.2.2.6.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.6.2">𝑑</ci><cn type="integer" id="S4.E1.m1.7.7.1.1.2.2.2.6.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.2.6.3">1</cn></apply></apply></apply><apply id="S4.E1.m1.7.7.1.1.2.2c.cmml" xref="S4.E1.m1.7.7.1.1.2.2"><eq id="S4.E1.m1.7.7.1.1.2.2.8.cmml" xref="S4.E1.m1.7.7.1.1.2.2.8"></eq><share href="#S4.E1.m1.7.7.1.1.2.2.2.cmml" id="S4.E1.m1.7.7.1.1.2.2d.cmml" xref="S4.E1.m1.7.7.1.1.2.2"></share><apply id="S4.E1.m1.7.7.1.1.2.2.4.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4"><times id="S4.E1.m1.7.7.1.1.2.2.4.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.3"></times><ci id="S4.E1.m1.7.7.1.1.2.2.4.4.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.4">𝐷</ci><vector id="S4.E1.m1.7.7.1.1.2.2.4.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2"><ci id="S4.E1.m1.6.6.cmml" xref="S4.E1.m1.6.6">𝑥</ci><apply id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1">subscript</csymbol><apply id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2"><ci id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.1">~</ci><ci id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.3.1.1.1.3">1</cn></apply><apply id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2"><csymbol cd="ambiguous" id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.1.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2">subscript</csymbol><ci id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.2.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.2">𝑓</ci><ci id="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.3.cmml" xref="S4.E1.m1.7.7.1.1.2.2.4.2.2.2.3">𝜃</ci></apply></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.7c">\hat{y}=\begin{cases}1,&amp;d_{1}&lt;d_{0}\\
0,&amp;d_{0}&lt;d_{1}\end{cases},\text{where}\hskip 2.84526ptd_{0}=D(x,\tilde{x}_{0};f_{\theta})\hskip 2.84526pt\text{and}\hskip 2.84526ptd_{1}=D(x,\tilde{x}_{1};f_{\theta}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.3" class="ltx_p"><span id="S4.SS1.p2.3.1" class="ltx_text ltx_font_bold">Evaluating human-metric agreement.</span> Recall that for a triplet, we collect <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">y</annotation></semantics></math>, indicating which image a human selects as more similar. We evaluate how often each metric agrees with human judges as <math id="S4.SS1.p2.2.m2.2" class="ltx_Math" alttext="\texttt{Score}_{\texttt{2AFC}}(f_{\theta})=\mathds{E}_{\mathcal{D}^{\text{2afc}}}[\mathds{1}_{\hat{y}=y}]" display="inline"><semantics id="S4.SS1.p2.2.m2.2a"><mrow id="S4.SS1.p2.2.m2.2.2" xref="S4.SS1.p2.2.m2.2.2.cmml"><mrow id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml"><msub id="S4.SS1.p2.2.m2.1.1.1.3" xref="S4.SS1.p2.2.m2.1.1.1.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS1.p2.2.m2.1.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.1.3.2a.cmml">Score</mtext><mtext class="ltx_mathvariant_monospace" id="S4.SS1.p2.2.m2.1.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.1.3.3a.cmml">2AFC</mtext></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.2" xref="S4.SS1.p2.2.m2.1.1.1.2.cmml">​</mo><mrow id="S4.SS1.p2.2.m2.1.1.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p2.2.m2.1.1.1.1.1.2" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS1.p2.2.m2.1.1.1.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p2.2.m2.1.1.1.1.1.1.2" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.2.cmml">f</mi><mi id="S4.SS1.p2.2.m2.1.1.1.1.1.1.3" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.3.cmml">θ</mi></msub><mo stretchy="false" id="S4.SS1.p2.2.m2.1.1.1.1.1.3" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.p2.2.m2.2.2.3" xref="S4.SS1.p2.2.m2.2.2.3.cmml">=</mo><mrow id="S4.SS1.p2.2.m2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.2.cmml"><msub id="S4.SS1.p2.2.m2.2.2.2.3" xref="S4.SS1.p2.2.m2.2.2.2.3.cmml"><mi id="S4.SS1.p2.2.m2.2.2.2.3.2" xref="S4.SS1.p2.2.m2.2.2.2.3.2.cmml">𝔼</mi><msup id="S4.SS1.p2.2.m2.2.2.2.3.3" xref="S4.SS1.p2.2.m2.2.2.2.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.2.m2.2.2.2.3.3.2" xref="S4.SS1.p2.2.m2.2.2.2.3.3.2.cmml">𝒟</mi><mtext id="S4.SS1.p2.2.m2.2.2.2.3.3.3" xref="S4.SS1.p2.2.m2.2.2.2.3.3.3a.cmml">2afc</mtext></msup></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.2.2.2.2" xref="S4.SS1.p2.2.m2.2.2.2.2.cmml">​</mo><mrow id="S4.SS1.p2.2.m2.2.2.2.1.1" xref="S4.SS1.p2.2.m2.2.2.2.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.2.m2.2.2.2.1.1.2" xref="S4.SS1.p2.2.m2.2.2.2.1.2.1.cmml">[</mo><msub id="S4.SS1.p2.2.m2.2.2.2.1.1.1" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.cmml"><mn id="S4.SS1.p2.2.m2.2.2.2.1.1.1.2" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.2.cmml">𝟙</mn><mrow id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.cmml"><mover accent="true" id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.cmml"><mi id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.2" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.2.cmml">y</mi><mo id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.1" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.1.cmml">^</mo></mover><mo id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.1" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.1.cmml">=</mo><mi id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.3" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.3.cmml">y</mi></mrow></msub><mo stretchy="false" id="S4.SS1.p2.2.m2.2.2.2.1.1.3" xref="S4.SS1.p2.2.m2.2.2.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.2b"><apply id="S4.SS1.p2.2.m2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2"><eq id="S4.SS1.p2.2.m2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.2.2.3"></eq><apply id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.1.2"></times><apply id="S4.SS1.p2.2.m2.1.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.1.3.2a.cmml" xref="S4.SS1.p2.2.m2.1.1.1.3.2"><mtext class="ltx_mathvariant_monospace" id="S4.SS1.p2.2.m2.1.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.1.3.2">Score</mtext></ci><ci id="S4.SS1.p2.2.m2.1.1.1.3.3a.cmml" xref="S4.SS1.p2.2.m2.1.1.1.3.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS1.p2.2.m2.1.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.1.3.3">2AFC</mtext></ci></apply><apply id="S4.SS1.p2.2.m2.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S4.SS1.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.2">𝑓</ci><ci id="S4.SS1.p2.2.m2.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.1.1.3">𝜃</ci></apply></apply><apply id="S4.SS1.p2.2.m2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2"><times id="S4.SS1.p2.2.m2.2.2.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.2"></times><apply id="S4.SS1.p2.2.m2.2.2.2.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.2.2.2.3.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3">subscript</csymbol><ci id="S4.SS1.p2.2.m2.2.2.2.3.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3.2">𝔼</ci><apply id="S4.SS1.p2.2.m2.2.2.2.3.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.2.2.2.3.3.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3.3">superscript</csymbol><ci id="S4.SS1.p2.2.m2.2.2.2.3.3.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3.3.2">𝒟</ci><ci id="S4.SS1.p2.2.m2.2.2.2.3.3.3a.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3.3.3"><mtext mathsize="50%" id="S4.SS1.p2.2.m2.2.2.2.3.3.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.3.3.3">2afc</mtext></ci></apply></apply><apply id="S4.SS1.p2.2.m2.2.2.2.1.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.2.2.2.1.2.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.2">delimited-[]</csymbol><apply id="S4.SS1.p2.2.m2.2.2.2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.2.2.2.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1">subscript</csymbol><cn type="integer" id="S4.SS1.p2.2.m2.2.2.2.1.1.1.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.2">1</cn><apply id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3"><eq id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.1"></eq><apply id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2"><ci id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.1.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.1">^</ci><ci id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.2.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.2.2">𝑦</ci></apply><ci id="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.2.2.2.1.1.1.3.3">𝑦</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.2c">\texttt{Score}_{\texttt{2AFC}}(f_{\theta})=\mathds{E}_{\mathcal{D}^{\text{2afc}}}[\mathds{1}_{\hat{y}=y}]</annotation></semantics></math> and <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="\texttt{Score}_{\texttt{JND}}=\mathds{E}_{\mathcal{D}^{\text{jnd}}}[\mathds{1}_{\hat{y}=s}]" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><mrow id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><msub id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S4.SS1.p2.3.m3.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.3.2a.cmml">Score</mtext><mtext class="ltx_mathvariant_monospace" id="S4.SS1.p2.3.m3.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.3.3a.cmml">JND</mtext></msub><mo id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">=</mo><mrow id="S4.SS1.p2.3.m3.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.cmml"><msub id="S4.SS1.p2.3.m3.1.1.1.3" xref="S4.SS1.p2.3.m3.1.1.1.3.cmml"><mi id="S4.SS1.p2.3.m3.1.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.1.3.2.cmml">𝔼</mi><msup id="S4.SS1.p2.3.m3.1.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.3.m3.1.1.1.3.3.2" xref="S4.SS1.p2.3.m3.1.1.1.3.3.2.cmml">𝒟</mi><mtext id="S4.SS1.p2.3.m3.1.1.1.3.3.3" xref="S4.SS1.p2.3.m3.1.1.1.3.3.3a.cmml">jnd</mtext></msup></msub><mo lspace="0em" rspace="0em" id="S4.SS1.p2.3.m3.1.1.1.2" xref="S4.SS1.p2.3.m3.1.1.1.2.cmml">​</mo><mrow id="S4.SS1.p2.3.m3.1.1.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.SS1.p2.3.m3.1.1.1.1.1.2" xref="S4.SS1.p2.3.m3.1.1.1.1.2.1.cmml">[</mo><msub id="S4.SS1.p2.3.m3.1.1.1.1.1.1" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.cmml"><mn id="S4.SS1.p2.3.m3.1.1.1.1.1.1.2" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.2.cmml">𝟙</mn><mrow id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.cmml"><mover accent="true" id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.cmml"><mi id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.2" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.2.cmml">y</mi><mo id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.1" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.1.cmml">^</mo></mover><mo id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.1" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.1.cmml">=</mo><mi id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.3" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.3.cmml">s</mi></mrow></msub><mo stretchy="false" id="S4.SS1.p2.3.m3.1.1.1.1.1.3" xref="S4.SS1.p2.3.m3.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><eq id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2"></eq><apply id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.3.2a.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2"><mtext class="ltx_mathvariant_monospace" id="S4.SS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2">Score</mtext></ci><ci id="S4.SS1.p2.3.m3.1.1.3.3a.cmml" xref="S4.SS1.p2.3.m3.1.1.3.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S4.SS1.p2.3.m3.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3.3">JND</mtext></ci></apply><apply id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1"><times id="S4.SS1.p2.3.m3.1.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.2"></times><apply id="S4.SS1.p2.3.m3.1.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3">subscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3.2">𝔼</ci><apply id="S4.SS1.p2.3.m3.1.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.3.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3.3">superscript</csymbol><ci id="S4.SS1.p2.3.m3.1.1.1.3.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3.3.2">𝒟</ci><ci id="S4.SS1.p2.3.m3.1.1.1.3.3.3a.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3.3.3"><mtext mathsize="50%" id="S4.SS1.p2.3.m3.1.1.1.3.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.3.3.3">jnd</mtext></ci></apply></apply><apply id="S4.SS1.p2.3.m3.1.1.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.3.m3.1.1.1.1.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S4.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1">subscript</csymbol><cn type="integer" id="S4.SS1.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.2">1</cn><apply id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3"><eq id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.1"></eq><apply id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2"><ci id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.1.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.1">^</ci><ci id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.2.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.2.2">𝑦</ci></apply><ci id="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S4.SS1.p2.3.m3.1.1.1.1.1.1.3.3">𝑠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">\texttt{Score}_{\texttt{JND}}=\mathds{E}_{\mathcal{D}^{\text{jnd}}}[\mathds{1}_{\hat{y}=s}]</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Learning an improved metric</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.5" class="ltx_p"><span id="S4.SS2.p1.5.1" class="ltx_text ltx_font_bold">Objective.</span>
To better align a perceptual metric with human judgments, given triplet <math id="S4.SS2.p1.1.m1.3" class="ltx_Math" alttext="(x,\tilde{x}_{0},\tilde{x}_{1})" display="inline"><semantics id="S4.SS2.p1.1.m1.3a"><mrow id="S4.SS2.p1.1.m1.3.3.2" xref="S4.SS2.p1.1.m1.3.3.3.cmml"><mo stretchy="false" id="S4.SS2.p1.1.m1.3.3.2.3" xref="S4.SS2.p1.1.m1.3.3.3.cmml">(</mo><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">x</mi><mo id="S4.SS2.p1.1.m1.3.3.2.4" xref="S4.SS2.p1.1.m1.3.3.3.cmml">,</mo><msub id="S4.SS2.p1.1.m1.2.2.1.1" xref="S4.SS2.p1.1.m1.2.2.1.1.cmml"><mover accent="true" id="S4.SS2.p1.1.m1.2.2.1.1.2" xref="S4.SS2.p1.1.m1.2.2.1.1.2.cmml"><mi id="S4.SS2.p1.1.m1.2.2.1.1.2.2" xref="S4.SS2.p1.1.m1.2.2.1.1.2.2.cmml">x</mi><mo id="S4.SS2.p1.1.m1.2.2.1.1.2.1" xref="S4.SS2.p1.1.m1.2.2.1.1.2.1.cmml">~</mo></mover><mn id="S4.SS2.p1.1.m1.2.2.1.1.3" xref="S4.SS2.p1.1.m1.2.2.1.1.3.cmml">0</mn></msub><mo id="S4.SS2.p1.1.m1.3.3.2.5" xref="S4.SS2.p1.1.m1.3.3.3.cmml">,</mo><msub id="S4.SS2.p1.1.m1.3.3.2.2" xref="S4.SS2.p1.1.m1.3.3.2.2.cmml"><mover accent="true" id="S4.SS2.p1.1.m1.3.3.2.2.2" xref="S4.SS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S4.SS2.p1.1.m1.3.3.2.2.2.2" xref="S4.SS2.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mo id="S4.SS2.p1.1.m1.3.3.2.2.2.1" xref="S4.SS2.p1.1.m1.3.3.2.2.2.1.cmml">~</mo></mover><mn id="S4.SS2.p1.1.m1.3.3.2.2.3" xref="S4.SS2.p1.1.m1.3.3.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S4.SS2.p1.1.m1.3.3.2.6" xref="S4.SS2.p1.1.m1.3.3.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.3b"><vector id="S4.SS2.p1.1.m1.3.3.3.cmml" xref="S4.SS2.p1.1.m1.3.3.2"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑥</ci><apply id="S4.SS2.p1.1.m1.2.2.1.1.cmml" xref="S4.SS2.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S4.SS2.p1.1.m1.2.2.1.1">subscript</csymbol><apply id="S4.SS2.p1.1.m1.2.2.1.1.2.cmml" xref="S4.SS2.p1.1.m1.2.2.1.1.2"><ci id="S4.SS2.p1.1.m1.2.2.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.2.2.1.1.2.1">~</ci><ci id="S4.SS2.p1.1.m1.2.2.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2.1.1.2.2">𝑥</ci></apply><cn type="integer" id="S4.SS2.p1.1.m1.2.2.1.1.3.cmml" xref="S4.SS2.p1.1.m1.2.2.1.1.3">0</cn></apply><apply id="S4.SS2.p1.1.m1.3.3.2.2.cmml" xref="S4.SS2.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.3.3.2.2.1.cmml" xref="S4.SS2.p1.1.m1.3.3.2.2">subscript</csymbol><apply id="S4.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S4.SS2.p1.1.m1.3.3.2.2.2"><ci id="S4.SS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S4.SS2.p1.1.m1.3.3.2.2.2.1">~</ci><ci id="S4.SS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.3.3.2.2.2.2">𝑥</ci></apply><cn type="integer" id="S4.SS2.p1.1.m1.3.3.2.2.3.cmml" xref="S4.SS2.p1.1.m1.3.3.2.2.3">1</cn></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.3c">(x,\tilde{x}_{0},\tilde{x}_{1})</annotation></semantics></math> we maximize the difference between the perceptual distances <math id="S4.SS2.p1.2.m2.2" class="ltx_Math" alttext="d_{0},d_{1}" display="inline"><semantics id="S4.SS2.p1.2.m2.2a"><mrow id="S4.SS2.p1.2.m2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.3.cmml"><msub id="S4.SS2.p1.2.m2.1.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.1.1.2" xref="S4.SS2.p1.2.m2.1.1.1.1.2.cmml">d</mi><mn id="S4.SS2.p1.2.m2.1.1.1.1.3" xref="S4.SS2.p1.2.m2.1.1.1.1.3.cmml">0</mn></msub><mo id="S4.SS2.p1.2.m2.2.2.2.3" xref="S4.SS2.p1.2.m2.2.2.3.cmml">,</mo><msub id="S4.SS2.p1.2.m2.2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.2.2.cmml"><mi id="S4.SS2.p1.2.m2.2.2.2.2.2" xref="S4.SS2.p1.2.m2.2.2.2.2.2.cmml">d</mi><mn id="S4.SS2.p1.2.m2.2.2.2.2.3" xref="S4.SS2.p1.2.m2.2.2.2.2.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.2b"><list id="S4.SS2.p1.2.m2.2.2.3.cmml" xref="S4.SS2.p1.2.m2.2.2.2"><apply id="S4.SS2.p1.2.m2.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.2">𝑑</ci><cn type="integer" id="S4.SS2.p1.2.m2.1.1.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.1.1.3">0</cn></apply><apply id="S4.SS2.p1.2.m2.2.2.2.2.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.2.2.2.2.1.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S4.SS2.p1.2.m2.2.2.2.2.2.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.2">𝑑</ci><cn type="integer" id="S4.SS2.p1.2.m2.2.2.2.2.3.cmml" xref="S4.SS2.p1.2.m2.2.2.2.2.3">1</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.2c">d_{0},d_{1}</annotation></semantics></math>, with smaller distance associated with the distortion <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">y</annotation></semantics></math> humans voted to be most similar. To accomplish this, we map <math id="S4.SS2.p1.4.m4.4" class="ltx_Math" alttext="y\in\{0,1\}\rightarrow\bar{y}\in\{-1,1\}" display="inline"><semantics id="S4.SS2.p1.4.m4.4a"><mrow id="S4.SS2.p1.4.m4.4.4" xref="S4.SS2.p1.4.m4.4.4.cmml"><mi id="S4.SS2.p1.4.m4.4.4.3" xref="S4.SS2.p1.4.m4.4.4.3.cmml">y</mi><mo id="S4.SS2.p1.4.m4.4.4.4" xref="S4.SS2.p1.4.m4.4.4.4.cmml">∈</mo><mrow id="S4.SS2.p1.4.m4.4.4.5.2" xref="S4.SS2.p1.4.m4.4.4.5.1.cmml"><mo stretchy="false" id="S4.SS2.p1.4.m4.4.4.5.2.1" xref="S4.SS2.p1.4.m4.4.4.5.1.cmml">{</mo><mn id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml">0</mn><mo id="S4.SS2.p1.4.m4.4.4.5.2.2" xref="S4.SS2.p1.4.m4.4.4.5.1.cmml">,</mo><mn id="S4.SS2.p1.4.m4.2.2" xref="S4.SS2.p1.4.m4.2.2.cmml">1</mn><mo stretchy="false" id="S4.SS2.p1.4.m4.4.4.5.2.3" xref="S4.SS2.p1.4.m4.4.4.5.1.cmml">}</mo></mrow><mo stretchy="false" id="S4.SS2.p1.4.m4.4.4.6" xref="S4.SS2.p1.4.m4.4.4.6.cmml">→</mo><mover accent="true" id="S4.SS2.p1.4.m4.4.4.7" xref="S4.SS2.p1.4.m4.4.4.7.cmml"><mi id="S4.SS2.p1.4.m4.4.4.7.2" xref="S4.SS2.p1.4.m4.4.4.7.2.cmml">y</mi><mo id="S4.SS2.p1.4.m4.4.4.7.1" xref="S4.SS2.p1.4.m4.4.4.7.1.cmml">¯</mo></mover><mo id="S4.SS2.p1.4.m4.4.4.8" xref="S4.SS2.p1.4.m4.4.4.8.cmml">∈</mo><mrow id="S4.SS2.p1.4.m4.4.4.1.1" xref="S4.SS2.p1.4.m4.4.4.1.2.cmml"><mo stretchy="false" id="S4.SS2.p1.4.m4.4.4.1.1.2" xref="S4.SS2.p1.4.m4.4.4.1.2.cmml">{</mo><mrow id="S4.SS2.p1.4.m4.4.4.1.1.1" xref="S4.SS2.p1.4.m4.4.4.1.1.1.cmml"><mo id="S4.SS2.p1.4.m4.4.4.1.1.1a" xref="S4.SS2.p1.4.m4.4.4.1.1.1.cmml">−</mo><mn id="S4.SS2.p1.4.m4.4.4.1.1.1.2" xref="S4.SS2.p1.4.m4.4.4.1.1.1.2.cmml">1</mn></mrow><mo id="S4.SS2.p1.4.m4.4.4.1.1.3" xref="S4.SS2.p1.4.m4.4.4.1.2.cmml">,</mo><mn id="S4.SS2.p1.4.m4.3.3" xref="S4.SS2.p1.4.m4.3.3.cmml">1</mn><mo stretchy="false" id="S4.SS2.p1.4.m4.4.4.1.1.4" xref="S4.SS2.p1.4.m4.4.4.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.4b"><apply id="S4.SS2.p1.4.m4.4.4.cmml" xref="S4.SS2.p1.4.m4.4.4"><and id="S4.SS2.p1.4.m4.4.4a.cmml" xref="S4.SS2.p1.4.m4.4.4"></and><apply id="S4.SS2.p1.4.m4.4.4b.cmml" xref="S4.SS2.p1.4.m4.4.4"><in id="S4.SS2.p1.4.m4.4.4.4.cmml" xref="S4.SS2.p1.4.m4.4.4.4"></in><ci id="S4.SS2.p1.4.m4.4.4.3.cmml" xref="S4.SS2.p1.4.m4.4.4.3">𝑦</ci><set id="S4.SS2.p1.4.m4.4.4.5.1.cmml" xref="S4.SS2.p1.4.m4.4.4.5.2"><cn type="integer" id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">0</cn><cn type="integer" id="S4.SS2.p1.4.m4.2.2.cmml" xref="S4.SS2.p1.4.m4.2.2">1</cn></set></apply><apply id="S4.SS2.p1.4.m4.4.4c.cmml" xref="S4.SS2.p1.4.m4.4.4"><ci id="S4.SS2.p1.4.m4.4.4.6.cmml" xref="S4.SS2.p1.4.m4.4.4.6">→</ci><share href="#S4.SS2.p1.4.m4.4.4.5.cmml" id="S4.SS2.p1.4.m4.4.4d.cmml" xref="S4.SS2.p1.4.m4.4.4"></share><apply id="S4.SS2.p1.4.m4.4.4.7.cmml" xref="S4.SS2.p1.4.m4.4.4.7"><ci id="S4.SS2.p1.4.m4.4.4.7.1.cmml" xref="S4.SS2.p1.4.m4.4.4.7.1">¯</ci><ci id="S4.SS2.p1.4.m4.4.4.7.2.cmml" xref="S4.SS2.p1.4.m4.4.4.7.2">𝑦</ci></apply></apply><apply id="S4.SS2.p1.4.m4.4.4e.cmml" xref="S4.SS2.p1.4.m4.4.4"><in id="S4.SS2.p1.4.m4.4.4.8.cmml" xref="S4.SS2.p1.4.m4.4.4.8"></in><share href="#S4.SS2.p1.4.m4.4.4.7.cmml" id="S4.SS2.p1.4.m4.4.4f.cmml" xref="S4.SS2.p1.4.m4.4.4"></share><set id="S4.SS2.p1.4.m4.4.4.1.2.cmml" xref="S4.SS2.p1.4.m4.4.4.1.1"><apply id="S4.SS2.p1.4.m4.4.4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.4.4.1.1.1"><minus id="S4.SS2.p1.4.m4.4.4.1.1.1.1.cmml" xref="S4.SS2.p1.4.m4.4.4.1.1.1"></minus><cn type="integer" id="S4.SS2.p1.4.m4.4.4.1.1.1.2.cmml" xref="S4.SS2.p1.4.m4.4.4.1.1.1.2">1</cn></apply><cn type="integer" id="S4.SS2.p1.4.m4.3.3.cmml" xref="S4.SS2.p1.4.m4.3.3">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.4c">y\in\{0,1\}\rightarrow\bar{y}\in\{-1,1\}</annotation></semantics></math> and use a hinge loss, equivalent to triplet loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> between the embeddings, with a margin of <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="m=0.05" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><mrow id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">m</mi><mo id="S4.SS2.p1.5.m5.1.1.1" xref="S4.SS2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><eq id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1.1"></eq><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">𝑚</ci><cn type="float" id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">m=0.05</annotation></semantics></math>:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.5" class="ltx_Math" alttext="\mathcal{L}(y,\hat{y})=\max(0,m-\Delta d\cdot\bar{y}),\text{where}\hskip 2.84526pt\Delta d=\ d_{0}-d_{1}." display="block"><semantics id="S4.E2.m1.5a"><mrow id="S4.E2.m1.5.5.1"><mrow id="S4.E2.m1.5.5.1.1.2" xref="S4.E2.m1.5.5.1.1.3.cmml"><mrow id="S4.E2.m1.5.5.1.1.1.1" xref="S4.E2.m1.5.5.1.1.1.1.cmml"><mrow id="S4.E2.m1.5.5.1.1.1.1.3" xref="S4.E2.m1.5.5.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.5.5.1.1.1.1.3.2" xref="S4.E2.m1.5.5.1.1.1.1.3.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.1.1.1.1.3.1" xref="S4.E2.m1.5.5.1.1.1.1.3.1.cmml">​</mo><mrow id="S4.E2.m1.5.5.1.1.1.1.3.3.2" xref="S4.E2.m1.5.5.1.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S4.E2.m1.5.5.1.1.1.1.3.3.2.1" xref="S4.E2.m1.5.5.1.1.1.1.3.3.1.cmml">(</mo><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">y</mi><mo id="S4.E2.m1.5.5.1.1.1.1.3.3.2.2" xref="S4.E2.m1.5.5.1.1.1.1.3.3.1.cmml">,</mo><mover accent="true" id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml"><mi id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml">y</mi><mo id="S4.E2.m1.2.2.1" xref="S4.E2.m1.2.2.1.cmml">^</mo></mover><mo stretchy="false" id="S4.E2.m1.5.5.1.1.1.1.3.3.2.3" xref="S4.E2.m1.5.5.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.5.5.1.1.1.1.2" xref="S4.E2.m1.5.5.1.1.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.5.5.1.1.1.1.1.1" xref="S4.E2.m1.5.5.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml">max</mi><mo id="S4.E2.m1.5.5.1.1.1.1.1.1a" xref="S4.E2.m1.5.5.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S4.E2.m1.5.5.1.1.1.1.1.1.1" xref="S4.E2.m1.5.5.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E2.m1.5.5.1.1.1.1.1.1.1.2" xref="S4.E2.m1.5.5.1.1.1.1.1.2.cmml">(</mo><mn id="S4.E2.m1.4.4" xref="S4.E2.m1.4.4.cmml">0</mn><mo id="S4.E2.m1.5.5.1.1.1.1.1.1.1.3" xref="S4.E2.m1.5.5.1.1.1.1.1.2.cmml">,</mo><mrow id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">m</mi><mo id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.1" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml"><mrow id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml"><mi mathvariant="normal" id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.1" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.1.cmml">​</mo><mi id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3.cmml">d</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.1" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml">⋅</mo><mover accent="true" id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.2" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.2.cmml">y</mi><mo id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.1" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.1.cmml">¯</mo></mover></mrow></mrow><mo stretchy="false" id="S4.E2.m1.5.5.1.1.1.1.1.1.1.4" xref="S4.E2.m1.5.5.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E2.m1.5.5.1.1.2.3" xref="S4.E2.m1.5.5.1.1.3a.cmml">,</mo><mrow id="S4.E2.m1.5.5.1.1.2.2" xref="S4.E2.m1.5.5.1.1.2.2.cmml"><mrow id="S4.E2.m1.5.5.1.1.2.2.2" xref="S4.E2.m1.5.5.1.1.2.2.2.cmml"><mtext id="S4.E2.m1.5.5.1.1.2.2.2.2" xref="S4.E2.m1.5.5.1.1.2.2.2.2a.cmml">where</mtext><mo lspace="0.280em" rspace="0em" id="S4.E2.m1.5.5.1.1.2.2.2.1" xref="S4.E2.m1.5.5.1.1.2.2.2.1.cmml">​</mo><mi mathvariant="normal" id="S4.E2.m1.5.5.1.1.2.2.2.3" xref="S4.E2.m1.5.5.1.1.2.2.2.3.cmml">Δ</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.5.5.1.1.2.2.2.1a" xref="S4.E2.m1.5.5.1.1.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.5.5.1.1.2.2.2.4" xref="S4.E2.m1.5.5.1.1.2.2.2.4.cmml">d</mi></mrow><mo rspace="0.778em" id="S4.E2.m1.5.5.1.1.2.2.1" xref="S4.E2.m1.5.5.1.1.2.2.1.cmml">=</mo><mrow id="S4.E2.m1.5.5.1.1.2.2.3" xref="S4.E2.m1.5.5.1.1.2.2.3.cmml"><msub id="S4.E2.m1.5.5.1.1.2.2.3.2" xref="S4.E2.m1.5.5.1.1.2.2.3.2.cmml"><mi id="S4.E2.m1.5.5.1.1.2.2.3.2.2" xref="S4.E2.m1.5.5.1.1.2.2.3.2.2.cmml">d</mi><mn id="S4.E2.m1.5.5.1.1.2.2.3.2.3" xref="S4.E2.m1.5.5.1.1.2.2.3.2.3.cmml">0</mn></msub><mo id="S4.E2.m1.5.5.1.1.2.2.3.1" xref="S4.E2.m1.5.5.1.1.2.2.3.1.cmml">−</mo><msub id="S4.E2.m1.5.5.1.1.2.2.3.3" xref="S4.E2.m1.5.5.1.1.2.2.3.3.cmml"><mi id="S4.E2.m1.5.5.1.1.2.2.3.3.2" xref="S4.E2.m1.5.5.1.1.2.2.3.3.2.cmml">d</mi><mn id="S4.E2.m1.5.5.1.1.2.2.3.3.3" xref="S4.E2.m1.5.5.1.1.2.2.3.3.3.cmml">1</mn></msub></mrow></mrow></mrow><mo lspace="0em" id="S4.E2.m1.5.5.1.2">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.5b"><apply id="S4.E2.m1.5.5.1.1.3.cmml" xref="S4.E2.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.1.1.3a.cmml" xref="S4.E2.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S4.E2.m1.5.5.1.1.1.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1"><eq id="S4.E2.m1.5.5.1.1.1.1.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.2"></eq><apply id="S4.E2.m1.5.5.1.1.1.1.3.cmml" xref="S4.E2.m1.5.5.1.1.1.1.3"><times id="S4.E2.m1.5.5.1.1.1.1.3.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.3.1"></times><ci id="S4.E2.m1.5.5.1.1.1.1.3.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.3.2">ℒ</ci><interval closure="open" id="S4.E2.m1.5.5.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.3.3.2"><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">𝑦</ci><apply id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2"><ci id="S4.E2.m1.2.2.1.cmml" xref="S4.E2.m1.2.2.1">^</ci><ci id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2">𝑦</ci></apply></interval></apply><apply id="S4.E2.m1.5.5.1.1.1.1.1.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1"><max id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3"></max><cn type="integer" id="S4.E2.m1.4.4.cmml" xref="S4.E2.m1.4.4">0</cn><apply id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1"><minus id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.1"></minus><ci id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.2">𝑚</ci><apply id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3"><ci id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.1">⋅</ci><apply id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2"><times id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.1"></times><ci id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.2">Δ</ci><ci id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.2.3">𝑑</ci></apply><apply id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3"><ci id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.1">¯</ci><ci id="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S4.E2.m1.5.5.1.1.1.1.1.1.1.1.3.3.2">𝑦</ci></apply></apply></apply></apply></apply><apply id="S4.E2.m1.5.5.1.1.2.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2"><eq id="S4.E2.m1.5.5.1.1.2.2.1.cmml" xref="S4.E2.m1.5.5.1.1.2.2.1"></eq><apply id="S4.E2.m1.5.5.1.1.2.2.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2.2"><times id="S4.E2.m1.5.5.1.1.2.2.2.1.cmml" xref="S4.E2.m1.5.5.1.1.2.2.2.1"></times><ci id="S4.E2.m1.5.5.1.1.2.2.2.2a.cmml" xref="S4.E2.m1.5.5.1.1.2.2.2.2"><mtext id="S4.E2.m1.5.5.1.1.2.2.2.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2.2.2">where</mtext></ci><ci id="S4.E2.m1.5.5.1.1.2.2.2.3.cmml" xref="S4.E2.m1.5.5.1.1.2.2.2.3">Δ</ci><ci id="S4.E2.m1.5.5.1.1.2.2.2.4.cmml" xref="S4.E2.m1.5.5.1.1.2.2.2.4">𝑑</ci></apply><apply id="S4.E2.m1.5.5.1.1.2.2.3.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3"><minus id="S4.E2.m1.5.5.1.1.2.2.3.1.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.1"></minus><apply id="S4.E2.m1.5.5.1.1.2.2.3.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.1.1.2.2.3.2.1.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.2">subscript</csymbol><ci id="S4.E2.m1.5.5.1.1.2.2.3.2.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.2.2">𝑑</ci><cn type="integer" id="S4.E2.m1.5.5.1.1.2.2.3.2.3.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.2.3">0</cn></apply><apply id="S4.E2.m1.5.5.1.1.2.2.3.3.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.5.5.1.1.2.2.3.3.1.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.3">subscript</csymbol><ci id="S4.E2.m1.5.5.1.1.2.2.3.3.2.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.3.2">𝑑</ci><cn type="integer" id="S4.E2.m1.5.5.1.1.2.2.3.3.3.cmml" xref="S4.E2.m1.5.5.1.1.2.2.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.5c">\mathcal{L}(y,\hat{y})=\max(0,m-\Delta d\cdot\bar{y}),\text{where}\hskip 2.84526pt\Delta d=\ d_{0}-d_{1}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.4" class="ltx_p"><span id="S4.SS2.p2.4.1" class="ltx_text ltx_font_bold">Tuning. </span>
Next, we investigate the best method to tune or modify feature extractor <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="f_{\theta}" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><msub id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">f</mi><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑓</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">f_{\theta}</annotation></semantics></math>. A challenge is the large number of parameters (billions, for ViT backbones), compared to the relatively small amount of perceptual data. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>, we first try tuning via an MLP head, adding a 1-hidden-layer MLP with a residual connection on top of the pre-trained embeddings. We compare this to fine-tuning through the pre-trained backbones using Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, which we find to achieve better results than full fine-tuning. Using LoRA (<math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="r=16" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">r</mi><mo id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><eq id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></eq><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑟</ci><cn type="integer" id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">r=16</annotation></semantics></math>, dropout <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="p=0.3" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mrow id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml"><mi id="S4.SS2.p2.3.m3.1.1.2" xref="S4.SS2.p2.3.m3.1.1.2.cmml">p</mi><mo id="S4.SS2.p2.3.m3.1.1.1" xref="S4.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.3.m3.1.1.3" xref="S4.SS2.p2.3.m3.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><apply id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1"><eq id="S4.SS2.p2.3.m3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1.1"></eq><ci id="S4.SS2.p2.3.m3.1.1.2.cmml" xref="S4.SS2.p2.3.m3.1.1.2">𝑝</ci><cn type="float" id="S4.SS2.p2.3.m3.1.1.3.cmml" xref="S4.SS2.p2.3.m3.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">p=0.3</annotation></semantics></math>, <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mi id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">α</mi><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p2.4.m4.1.1.3" xref="S4.SS2.p2.4.m4.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><eq id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1"></eq><ci id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">𝛼</ci><cn type="float" id="S4.SS2.p2.4.m4.1.1.3.cmml" xref="S4.SS2.p2.4.m4.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\alpha=0.5</annotation></semantics></math>) we tune approximately 0.67% of each model’s parameters. In all experiments, LoRA significantly outperformed using the MLP head.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Feature concatenation.</span>
As multiple models may provide complementary features, we try combining them to boost performance. We concatenate features from the best-performing models on our dataset (DINO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite>, and OpenCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>) into one ensemble metric. By training just a handful of parameters in this ensemble (through LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>), we gain the benefits of each large model’s embedding space without sacrificing much computational load while fine-tuning.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.7" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:260.2pt;"><img src="/html/2306.09344/assets/x4.png" id="S4.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="300" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S4.F5.7.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Metrics performance on our benchmark.<span id="S4.F5.7.6.3.4" class="ltx_text ltx_font_medium"> Large vision models OpenCLIP and DINO outperform prior learned metrics LPIPS and DISTS (</span><span id="S4.F5.7.6.3.5" class="ltx_text" style="color:#E5A750;">orange</span><span id="S4.F5.5.4.1.1" class="ltx_text ltx_font_medium">) (chance = <math id="S4.F5.5.4.1.1.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S4.F5.5.4.1.1.m1.1b"><mrow id="S4.F5.5.4.1.1.m1.1.1" xref="S4.F5.5.4.1.1.m1.1.1.cmml"><mn id="S4.F5.5.4.1.1.m1.1.1.2" xref="S4.F5.5.4.1.1.m1.1.1.2.cmml">50</mn><mo id="S4.F5.5.4.1.1.m1.1.1.1" xref="S4.F5.5.4.1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.F5.5.4.1.1.m1.1c"><apply id="S4.F5.5.4.1.1.m1.1.1.cmml" xref="S4.F5.5.4.1.1.m1.1.1"><csymbol cd="latexml" id="S4.F5.5.4.1.1.m1.1.1.1.cmml" xref="S4.F5.5.4.1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S4.F5.5.4.1.1.m1.1.1.2.cmml" xref="S4.F5.5.4.1.1.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.5.4.1.1.m1.1d">50\%</annotation></semantics></math>). Further tuning on our perceptual data with an MLP (</span><span id="S4.F5.7.6.3.6" class="ltx_text" style="color:#56BCC6;">blue</span><span id="S4.F5.7.6.3.7" class="ltx_text ltx_font_medium">) improves performance over out-of-the-box features (</span><span id="S4.F5.7.6.3.8" class="ltx_text" style="color:#99BF63;">green</span><span id="S4.F5.7.6.3.9" class="ltx_text ltx_font_medium">); we find Low-rank LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> tuning boosts performance significantly (</span><span id="S4.F5.7.6.3.10" class="ltx_text" style="color:#EB85B7;">pink</span><span id="S4.F5.7.6.3.3" class="ltx_text ltx_font_medium">). Ensembling CLIP, OpenCLIP, and DINO models together improves performance as well. Our final model, <span id="S4.F5.7.6.3.3.1" class="ltx_text ltx_font_italic">DreamSim</span>, combines these insights and achieves high agreement with humans (96.16<math id="S4.F5.6.5.2.2.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.F5.6.5.2.2.m1.1b"><mo id="S4.F5.6.5.2.2.m1.1.1" xref="S4.F5.6.5.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.F5.6.5.2.2.m1.1c"><csymbol cd="latexml" id="S4.F5.6.5.2.2.m1.1.1.cmml" xref="S4.F5.6.5.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.6.5.2.2.m1.1d">\%</annotation></semantics></math>). Error bars represent a 95<math id="S4.F5.7.6.3.3.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.F5.7.6.3.3.m2.1b"><mo id="S4.F5.7.6.3.3.m2.1.1" xref="S4.F5.7.6.3.3.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.F5.7.6.3.3.m2.1c"><csymbol cd="latexml" id="S4.F5.7.6.3.3.m2.1.1.cmml" xref="S4.F5.7.6.3.3.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.7.6.3.3.m2.1d">\%</annotation></semantics></math> confidence interval.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F5.8" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" style="width:164.8pt;"><img src="/html/2306.09344/assets/x5.png" id="S4.F5.8.g1" class="ltx_graphics ltx_img_square" width="483" height="433" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S4.F5.8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Correlation between 2AFC &amp; JND.<span id="S4.F5.8.2.1.1" class="ltx_text ltx_font_medium">
We plot the agreement between human judgments and existing similarity metrics on two tasks, 2AFC and JND. The strong correlation between the metrics’ agreement with both tasks suggests that our dataset captures a general notion of similarity that can be replicated in two separate, independent human studies.
</span></span></figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>How well do existing metrics align with human judgments?</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Learning an improved metric ‣ 4 Perceptual metric learning ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we show how well metrics align with humans on our dataset, across low-level metrics (PSNR, SSIM), prior learned metrics (LPIPS, DISTS) and recent large vision model embeddings (MAE, CLIP, DINO).
The best-performing configurations of existing base models are DINO B/16, MAE B/16, CLIP B/32, and OpenCLIP B/32 (additional settings are in SM Section <a href="#A2.SS1" title="B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>).
However, existing metrics have significant misalignments with humans; for example, DINO has approximately 10% disagreement.
To improve performance, we finetune with LoRA (as described in Section <a href="#S4.SS2" title="4.2 Learning an improved metric ‣ 4 Perceptual metric learning ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), denoted as <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">Tuned - LoRA</span>, achieving significant alignment improvements over pre-trained baselines (Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Learning an improved metric ‣ 4 Perceptual metric learning ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>) and training an MLP head (<span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">Tuned - MLP</span>). Ensembling models by concatenating OpenCLIP, CLIP, and DINO features achieves the highest score across baselines (90.8%), MLP-tuned models (93.4%), and LoRA-tuned models (96.2%). We refer to the best LoRA-tuned ensemble model as <span id="S5.SS1.p1.1.3" class="ltx_text ltx_font_italic">DreamSim</span>, and use it for subsequent analysis and applications.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Do different ways of measuring human perception agree?</span>
We corroborate our results on our 2AFC test set by evaluating the models against JND perceptual judgments. The high correlation between 2AFC and JND scores indicates that our 2AFC judgments capture a general sense of similarity that can replicated across different settings (Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Learning an improved metric ‣ 4 Perceptual metric learning ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">Does improved mid-level perception lead to improved low-level and high-level perception?</span> We use the same models as in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Learning an improved metric ‣ 4 Perceptual metric learning ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> to examine how performance on our dataset corresponds to performance on two others: BAPPS, containing image triplets with low-level augmentations, and THINGS, containing triplets with categorical variations. Training on our dataset improves human alignment on BAPPS but <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">decreases</span> alignment on THINGS, suggesting that humans, when making an automatic judgment, tend to focus on appearance-based visual differences (captured in BAPPS and our dataset) rather than high-level object category. We additionally evaluate on the TID2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and KADID-10k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> image quality assessment (IQA) benchmarks as alternative low-level datasets. Similar to BAPPS, we find that our metric outperforms base models and is competitive with low-level metrics despite not training for low-level similarity. For full results, see SM Section <a href="#A2.SS1" title="B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>What image attributes affect similarity decisions?</h3>

<figure id="S5.F6" class="ltx_figure"><img src="/html/2306.09344/assets/x6.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S5.F6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sensitivity of our similarity metric to different image ablations.<span id="S5.F6.2.1.1" class="ltx_text ltx_font_medium"> (Left) We ablate elements of the image, assessing performance dropoff by changing the orientation, color, shading, structure, and foreground vs. background content. The model is fairly robust to orientation, and more impacted by changes in color, shading, or structure. Foreground content is more important for similarity recognition than background content.
(Right) For an example triplet (unmodified), we show visualizations of removing elements of the images.</span></span></figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_bold">Sensitivity to image ablations.</span> We study which image properties affect our model’s decisions by ablating properties of the image triplets, such as orientation, color, shading, structure, and foreground or background components. To do this, we modify the triplets to change or discard each attribute and compute the agreement between the model decisions on the modified and unmodified triplets.
This operation measures the model’s robustness when a given attribute is missing.
Note that this analysis aims to provide insight into how our model determines similarity between two images, rather than to compare to human judgments; it is possible that humans may be more sensitive to different image attributes.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">As shown in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, our model is least affected by orientation (tested by flipping the reference image), but ablating color or shading (the L or AB channels in LAB color space), structure  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, or both color and shading together using contours <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> has larger impacts.
We conclude that color, shading, and structure contain critical information for our model, but our model can tolerate differences in orientation.
To ablate foreground and background components, we segment the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and replace the corresponding pixels with random uniform noise. This operation removes the color, shading, and texture of the region, even though the same outline remains in both.
Removing the background has a smaller impact on model alignment compared to removing the foreground, suggesting that the foreground color and texture is more important for similarity recognition in our model compared to the background.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2306.09344/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="83" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="S5.F7.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Alignment with semantic and low-level metrics. <span id="S5.F7.2.1.1" class="ltx_text ltx_font_medium"> We select random triplets from the COCO dataset and assess how well low-level and semantic metrics predict model judgments. Compared to LPIPS &amp; DISTS, our model is more sensitive to objects that appear in individual instances (“things”) and the presence of people, and less sensitive to “stuff” categories, such as sky and road. Compared to OpenCLIP &amp; DINO, color explains more of our model’s decisions. We mark our model’s results with a dashed pink line for comparison.</span></span></figcaption>
</figure>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Alignment with semantic and low-level metrics.</span>
Given that some image attributes, like semantics, are difficult to ablate, we complement our ablation analysis by examining how well semantic and handcrafted low-level features align with model judgments. We use 10,000 random image triplets from MS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> annotated with individual object instances (“things”) and background categories such as grass and sky (“stuff”).
We compare our metric to LPIPS, DISTS, DINO, and OpenCLIP (Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>). Our model aligns less strongly with RGB-color histogram similarity (calculated using histogram intersection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>) compared to LPIPS and DISTS, however is more sensitive to color compared to other large vision models, while none align with depth map similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.
Next, we compute alignment with the things-category and stuff-category histograms, which summarize the area occupied by each semantic category.
Our model aligns better with <span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">things</span>-histogram similarity, whereas other models are comparatively more sensitive to <span id="S5.SS2.p3.1.3" class="ltx_text ltx_font_italic">stuff</span>. This result suggests that our model is relatively more sensitive to foreground object instances. Finally, we compute how often metrics align with the <span id="S5.SS2.p3.1.4" class="ltx_text ltx_font_italic">per-category</span> intersection of categorical histograms. Our model aligns best with the presence of people (additional results in SM Section <a href="#A2.SS1" title="B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>).</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2306.09344/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="S5.F8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Nearest-neighbor retrieval across different metrics.<span id="S5.F8.2.1.1" class="ltx_text ltx_font_medium"> Nearest neighbor retrieval on ImageNet-R (top) and COCO (bottom), comparing different metrics.
Although the datasets include images outside of our training domain, our model consistently retrieves neighbors with similar appearance and class to that of the query image. </span></span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Applications</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text ltx_font_bold">Image retrieval.</span>
We use our metric for image retrieval on two datasets: ImageNet-R <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, which contains 30K images of various renditions of ImageNet categories (e.g., sketches, cartoons), and a random subset of 30K images from MS-COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>, which contains diverse natural photos. Given a query image, we compute its similarity to the entire dataset and present the top-3 nearest images under each metric in Figure <a href="#S5.F8" title="Figure 8 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. As can be seen, our metric finds meaningful neighbors, demonstrating that it can generalize to diverse image domains. We compare to the best performing ViT embedding methods (OpenCLIP, DINO) and prior learned metrics (LPIPS, DISTS), as evaluated in Section <a href="#S5.SS1" title="5.1 How well do existing metrics align with human judgments? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. Compared to the baselines, which are either dominated by low-level similarity (e.g., background color in LPIPS/DISTS), or higher-level similarity (e.g., semantic class in OpenCLIP), our retrieved images exhibit similarity across a continuum of visual traits. See Section <a href="#A2.SS1" title="B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a> for more results and analysis on sketch-to-photo and photo-to-sketch domains.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.3" class="ltx_p">We confirm our retrieval quality with a user study. For each metric, we retrieve the top-10 neighbors for a set of query images. For ImageNet-R, users prefer our metric’s retrievals 36.8<math id="S6.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S6.p2.1.m1.1a"><mo id="S6.p2.1.m1.1.1" xref="S6.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S6.p2.1.m1.1b"><csymbol cd="latexml" id="S6.p2.1.m1.1.1.cmml" xref="S6.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.1.m1.1c">\%</annotation></semantics></math> of the time, followed by OpenCLIP (28.7<math id="S6.p2.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S6.p2.2.m2.1a"><mo id="S6.p2.2.m2.1.1" xref="S6.p2.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S6.p2.2.m2.1b"><csymbol cd="latexml" id="S6.p2.2.m2.1.1.cmml" xref="S6.p2.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.2.m2.1c">\%</annotation></semantics></math>) and DINO (19.5<math id="S6.p2.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S6.p2.3.m3.1a"><mo id="S6.p2.3.m3.1.1" xref="S6.p2.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S6.p2.3.m3.1b"><csymbol cd="latexml" id="S6.p2.3.m3.1.1.cmml" xref="S6.p2.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.p2.3.m3.1c">\%</annotation></semantics></math>). We observe similar trends in a study with COCO images (see Figure <a href="#S6.F9" title="Figure 9 ‣ 6 Applications ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
<figure id="S6.F9" class="ltx_figure"><img src="/html/2306.09344/assets/x9.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="S6.F9.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">User preferences for image retrieval results by metric.<span id="S6.F9.2.1.1" class="ltx_text ltx_font_medium"> We conduct a user study that collects preferences for retrieval results from LPIPS, DISTS, DINO, OpenCLIP, and DreamSim. We visualize one standard deviation above and below each bar. On ImageNet-R (top, orange), our metric is preferred by users 36.8% of the time, followed by OpenCLIP (28.7%). Similarly, on COCO (bottom, blue), users prefer our metric 33.6% of the time, with DINO being the second choice (28.1%).
</span></span></figcaption>
</figure>
<div id="S6.p3" class="ltx_para ltx_noindent">
<p id="S6.p3.1" class="ltx_p"><span id="S6.p3.1.1" class="ltx_text ltx_font_bold">Feature inversion.</span> To better understand the information captured by our metric, we apply existing feature inversion techniques, where the aim is to optimize for an image that is similar to a target image under some metric. We compare our metric to the DINO and OpenCLIP embeddings, as well as to an ensemble embedding that simply concatenates all features without finetuning.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p id="S6.p4.1" class="ltx_p">Figure <a href="#S6.F10" title="Figure 10 ‣ 6 Applications ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows the inversion results under a range of image priors. Under vanilla optimization, i.e., not using any image prior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, our metric’s results exhibit higher fidelity to the original colors, shapes, and semantics. Using Deep Image Prior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>, our metric reveals better global layout preservation and semantics. This is also evident when using our metric to guide recent generative models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, resulting in higher quality image samples that resemble the semantic concepts and appearance of the target image.</p>
</div>
<div id="S6.p5" class="ltx_para">
<p id="S6.p5.1" class="ltx_p">Consistent with Section <a href="#S5.SS2" title="5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> and the retrieval results, these visualizations demonstrate our metric’s sensitivity to color, layout, and semantics. The increased quality of our fine-tuned embeddings compared to just ensembling shows the effectiveness of tuning on our human-aligned dataset. See SM for more examples, and Section <a href="#A1.SS4" title="A.4 Feature Inversion ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a> for details on feature inversion methodology.</p>
</div>
<div id="S6.p6" class="ltx_para ltx_noindent">
<p id="S6.p6.1" class="ltx_p"><span id="S6.p6.1.1" class="ltx_text ltx_font_bold">k-NN Classification.</span> We evaluate our metric as a <span id="S6.p6.1.2" class="ltx_text ltx_font_italic">k</span>-Nearest Neighbors classifier, which requires the retrieval of images that are both visually and semantically relevant to a given query. We find that our metric outperforms all baselines on the ObjectNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> dataset and performs competitively with DINO on the ImageNet100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> dataset. For full details and results, refer to SM Section <a href="#A2.SS1" title="B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.1</span></a>.</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2306.09344/assets/x10.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="160" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="S6.F10.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Feature inversion across different metrics and image priors<span id="S6.F10.2.1.1" class="ltx_text ltx_font_medium">. Given a target image, we optimize for an image, where the objective is to match the target image embedding of a given backbone.
Without any image prior (Optimization), our metric better recovers the color, shape and semantics of the target image. With a weak image prior <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite> (DIP Inversion), our metric is able to reproduce scene structure and semantics. Using a diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> as a strong prior, our metric better captures overall semantics and scene appearance.
</span></span></figcaption>
</figure>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We expand the fundamental task of measuring perceptual image similarity to encompass factors beyond mere low-level similarity, spanning multiple notions of similarities (e.g., object poses, colors, shapes, and camera angles). By harnessing a state-of-the-art generative model, we overcome the lack of suitable data and introduce a new dataset consisting of human-judged synthetic image triplets. We evaluate candidate embeddings and propose a new similarity metric that better aligns with human judgments, demonstrating its generalization and effectiveness compared to existing metrics. We note that by using Stable Diffusion (SD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, our benchmark is exposed to potential preexisting biases and sensitive content in the model. Our perceptual model is also finetuned from existing pre-trained backbones, and thus may also inherit prior errors and biases. However, our work is a step in aligning representations to human preferences, and we believe our work can inspire further research in leveraging advances in generative image models for perceptual studies.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Acknowledgments.</span> We thank Jim DiCarlo, Liad Mudrik, Nitzan Censor, Michelle Li, and Narek Tumanyan for fruitful discussions throughout the project. This work was supported by a Packard Fellowship to P.I., Israeli Science Foundation (grant 2303/20) to T.D., Meta PhD Fellowship to L.C., and NSF GRFP Fellowship to S.S.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
CarveKit.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/OPHoperHPO/image-background-remove-tool/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OPHoperHPO/image-background-remove-tool/</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Image database tid2013: Peculiarities, results and perspectives.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Signal Processing: Image Communication</span>, 30:57–77, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Daniel E. Acuna, Max Berniker, Hugo L. Fernandes, and Konrad P. Kording.

</span>
<span class="ltx_bibblock">Using psychophysics to ask if the brain samples or maximizes.

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Journal of Vision</span>, 15(3):7–7, 03 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Dan Amir and Yair Weiss.

</span>
<span class="ltx_bibblock">Understanding and simplifying perceptual distances.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 12226–12235, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.

</span>
<span class="ltx_bibblock">Deep vit features as dense visual descriptors.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2112.05814</span>, 2(3):4, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and
David J. Fleet.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet
classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2304.08466, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
Gutfreund, Josh Tenenbaum, and Boris Katz.

</span>
<span class="ltx_bibblock">Objectnet: A large-scale bias-controlled dataset for pushing the
limits of object recognition models.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Advances in Neural
Information Processing Systems</span>, volume 32. Curran Associates, Inc., 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.

</span>
<span class="ltx_bibblock">Food-101–mining discriminative components with random forests.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part VI 13</span>, pages 446–461.
Springer, 2014.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tim Brooks, Aleksander Holynski, and Alexei A. Efros.

</span>
<span class="ltx_bibblock">Instructpix2pix: Learning to follow image editing instructions, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal,
Piotr Bojanowski, and Armand Joulin.

</span>
<span class="ltx_bibblock">Emerging properties in self-supervised vision transformers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF international conference on
computer vision</span>, pages 9650–9660, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Patrick Cavanagh.

</span>
<span class="ltx_bibblock">The cognitive impenetrability of cognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Behavioral and Brain Sciences</span>, 22(3):370–371, 1999.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Caroline Chan, Frédo Durand, and Phillip Isola.

</span>
<span class="ltx_bibblock">Learning to generate line drawings that convey geometry and
semantics.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 7915–7925, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang,
Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al.

</span>
<span class="ltx_bibblock">Muse: Text-to-image generation via masked generative transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.00704</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Bengio.

</span>
<span class="ltx_bibblock">Large scale online learning of image similarity through ranking.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 11(3), 2010.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hila Chefer, Shir Gur, and Lior Wolf.

</span>
<span class="ltx_bibblock">Transformer interpretability beyond attention visualization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 782–791, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">A simple framework for contrastive learning of visual
representations.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
1597–1607. PMLR, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel
Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.

</span>
<span class="ltx_bibblock">Reproducible scaling laws for contrastive language-image learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2212.07143, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Katherine Crowson.

</span>
<span class="ltx_bibblock">k-diffusion.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/crowsonkb/k-diffusion" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/crowsonkb/k-diffusion</a>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Michael R.W. Dawson.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Cognitive Impenetrability</span>, pages 1–3.

</span>
<span class="ltx_bibblock">Springer International Publishing, Cham, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
Alabdulmohsin, et al.

</span>
<span class="ltx_bibblock">Scaling vision transformers to 22 billion parameters.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.05442</span>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
34:8780–8794, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Prafulla Dhariwal and Alexander Nichol.

</span>
<span class="ltx_bibblock">Diffusion models beat gans on image synthesis.

</span>
<span class="ltx_bibblock">In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
volume 34, pages 8780–8794. Curran Associates, Inc., 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Simoncelli.

</span>
<span class="ltx_bibblock">Image quality assessment: Unifying structure and texture similarity.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
44(5):2567–2581, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy and Thomas Brox.

</span>
<span class="ltx_bibblock">Generating images with perceptual similarity metrics based on deep
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 29, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
David Eigen, Christian Puhrsch, and Rob Fergus.

</span>
<span class="ltx_bibblock">Depth map prediction from a single image using a multi-scale deep
network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">NIPS</span>, 2014.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Leon Gatys, Alexander S Ecker, and Matthias Bethge.

</span>
<span class="ltx_bibblock">Texture synthesis using convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Leon A Gatys, Alexander S Ecker, and Matthias Bethge.

</span>
<span class="ltx_bibblock">A neural algorithm of artistic style.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1508.06576</span>, 2015.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 63(11):139–144, 2020.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross
Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.

</span>
<span class="ltx_bibblock">Momentum contrast for unsupervised visual representation learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pages 9729–9738, 2020.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip H. S. Torr,
Song Bai, and Xiaojuan Qi.

</span>
<span class="ltx_bibblock">Is synthetic data from generative models ready for image recognition?

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2210.07574, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Martin N Hebart, Oliver Contier, Lina Teichmann, Adam H Rockter, Charles Y
Zheng, Alexis Kidder, Anna Corriveau, Maryam Vaziri-Pashkam, and Chris I
Baker.

</span>
<span class="ltx_bibblock">Things-data, a multimodal collection of large-scale datasets for
investigating object representations in human brain and behavior.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Elife</span>, 12:e82580, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Martin N Hebart, Adam H Dickter, Alexis Kidder, Wan Y Kwok, Anna Corriveau,
Caitlin Van Wicklin, and Chris I Baker.

</span>
<span class="ltx_bibblock">Things: A database of 1,854 object concepts and more than 26,000
naturalistic object images.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">PloS one</span>, 14(10):e0223792, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Olivier J Hénaff and Eero P Simoncelli.

</span>
<span class="ltx_bibblock">Geodesics of learned representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06394</span>, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
Steinhardt, and Justin Gilmer.

</span>
<span class="ltx_bibblock">The many faces of robustness: A critical analysis of
out-of-distribution generalization.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">ICCV</span>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.09685</span>, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas
Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John
Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.

</span>
<span class="ltx_bibblock">Openclip, July 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole.

</span>
<span class="ltx_bibblock">Zero-shot text-guided object generation with dream fields.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Ajay Jain, Matthew Tancik, and Pieter Abbeel.

</span>
<span class="ltx_bibblock">Putting nerf on a diet: Semantically consistent few-shot view
synthesis.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span>, pages 5885–5894, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
William James.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">The Principles of Psychology</span>, volume 1.

</span>
<span class="ltx_bibblock">Henry Holt, New York, 1890.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Justin Johnson, Alexandre Alahi, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Perceptual losses for real-time style transfer and super-resolution.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span>,
pages 694–711. Springer, 2016.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Tero Karras, Samuli Laine, and Timo Aila.

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative adversarial
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition</span>, pages 4401–4410, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Markus Kettunen, Erik Härkönen, and Jaakko Lehtinen.

</span>
<span class="ltx_bibblock">E-lpips: robust perceptual image similarity via random transformation
ensembles.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1906.03973</span>, 2019.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Neural Information Processing Systems</span>, 25, 01 2012.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Manoj Kumar, Neil Houlsby, Nal Kalchbrenner, and Ekin Dogus Cubuk.

</span>
<span class="ltx_bibblock">Do better imagenet classifiers assess perceptual similarity better?

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Transactions of Machine Learning Research</span>, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Hanhe Lin, Vlad Hosu, and Dietmar Saupe.

</span>
<span class="ltx_bibblock">Kadid-10k: A large-scale artificially distorted iqa database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">2019 Eleventh International Conference on Quality of
Multimedia Experience (QoMEX)</span>, pages 1–3, 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland, September 6-12, 2014, Proceedings, Part V 13</span>, pages 740–755.
Springer, 2014.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
S. W. Link and R. A. Heath.

</span>
<span class="ltx_bibblock">A sequential theory of psychological discrimination.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Psychometrika</span>, 40:77–105, 1975.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Aravindh Mahendran and Andrea Vedaldi.

</span>
<span class="ltx_bibblock">Understanding deep image representations by inverting them.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 5188–5196, 2015.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Pranay Manocha, Adam Finkelstein, Richard Zhang, Nicholas J Bryan, Gautham J
Mysore, and Zeyu Jin.

</span>
<span class="ltx_bibblock">A differentiable perceptual audio metric learned from just noticeable
differences.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.04460</span>, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Pranay Manocha, Zeyu Jin, Richard Zhang, and Adam Finkelstein.

</span>
<span class="ltx_bibblock">Cdpam: Contrastive learning for perceptual audio similarity.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</span>, pages 196–200. IEEE, 2021.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Rafał Mantiuk, Kil Joong Kim, Allan G Rempel, and Wolfgang Heidrich.

</span>
<span class="ltx_bibblock">Hdr-vdp-2: A calibrated visual metric for visibility and quality
predictions in all luminance conditions.

</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">ACM Transactions on graphics (TOG)</span>, 30(4):1–14, 2011.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Brian McElree and Barbara A. Dosher.

</span>
<span class="ltx_bibblock">Serial retrieval processes in the recovery of order information.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">Journal of Experimental Psychology: General</span>, 122:291–315,
1993.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Doug Medin, Robert Goldstone, and Dedre Gentner.

</span>
<span class="ltx_bibblock">Respects for similarity.

</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Psychological Review</span>, 100:254–278, 04 1993.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Clay Mullis.

</span>
<span class="ltx_bibblock">Clip guided diffusion.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/afiaka87/clip-guided-diffusion" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/afiaka87/clip-guided-diffusion</a>, 2022.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Lukas Muttenthaler, Jonas Dippel, Lorenz Linhardt, Robert A Vandermeulen, and
Simon Kornblith.

</span>
<span class="ltx_bibblock">Human alignment of neural network representations.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2211.01201</span>, 2022.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Maria-Elena Nilsback and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Automated flower classification over a large number of classes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">2008 Sixth Indian Conference on Computer Vision, Graphics &amp;
Image Processing</span>, pages 722–729. IEEE, 2008.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Chris Olah, Alexander Mordvintsev, and Ludwig Schubert.

</span>
<span class="ltx_bibblock">Feature visualization.

</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">Distill</span>, 2017.

</span>
<span class="ltx_bibblock">https://distill.pub/2017/feature-visualization.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.

</span>
<span class="ltx_bibblock">Semantic image synthesis with spatially-adaptive normalization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
William Peebles, Jun-Yan Zhu, Richard Zhang, Antonio Torralba, Alexei A Efros,
and Eli Shechtman.

</span>
<span class="ltx_bibblock">Gan-supervised dense visual alignment.

</span>
<span class="ltx_bibblock">In <span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 13470–13481, 2022.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Nikolay Ponomarenko, Lina Jin, Oleg Ieremeiev, Vladimir Lukin, Karen
Egiazarian, Jaakko Astola, Benoit Vozel, Kacem Chehdi, Marco Carli, Federica
Battisti, and C.-C. Jay Kuo.

</span>
<span class="ltx_bibblock">Image database tid2013.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">Image Commun.</span>, 30(C):57–77, jan 2015.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Ekta Prashnani, Hong Cai, Yasamin Mostofi, and Pradeep Sen.

</span>
<span class="ltx_bibblock">Pieapp: Perceptual image-error assessment through pairwise
preference.

</span>
<span class="ltx_bibblock">In <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 1808–1817, 2018.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2204.06125</span>, 2022.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.

</span>
<span class="ltx_bibblock">Hierarchical text-conditional image generation with clip latents.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">ArXiv</span>, abs/2204.06125, 2022.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen
Koltun.

</span>
<span class="ltx_bibblock">Towards robust monocular depth estimation: Mixing datasets for
zero-shot cross-dataset transfer.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI)</span>, 2020.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Philippe Rochat.

</span>
<span class="ltx_bibblock">What is it like to be a newborn?

</span>
<span class="ltx_bibblock">In Shaun Gallagher, editor, <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">The Oxford Handbook of the Self</span>.
Oxford University Press, 2011.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10684–10695, 2022.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim
Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi.

</span>
<span class="ltx_bibblock">Photorealistic text-to-image diffusion models with deep language
understanding.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, <span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, volume 35,
pages 36479–36494. Curran Associates, Inc., 2022.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Mehul P Sampat, Zhou Wang, Shalini Gupta, Alan Conrad Bovik, and Mia K Markey.

</span>
<span class="ltx_bibblock">Complex wavelet structural similarity: A new image similarity index.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">IEEE transactions on image processing</span>, 18(11):2385–2401, 2009.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Patsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays.

</span>
<span class="ltx_bibblock">The sketchy database: Learning to retrieve badly drawn bunnies.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (proceedings of SIGGRAPH)</span>, 2016.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Mert Bulent Sariyildiz, Alahari Karteek, Diane Larlus, and Yannis Kalantidis.

</span>
<span class="ltx_bibblock">Fake it till you make it: Learning transferable representations from
synthetic imagenet clones.

</span>
<span class="ltx_bibblock">2022.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Robert Schreuder, Giovanni B Flores d’Arcais, and Ge Glazenborg.

</span>
<span class="ltx_bibblock">Effects of perceptual and conceptual similarity in semantic priming.

</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">Psychological Research</span>, 45(4):339–354, 1984.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Joshua Susskind, Wenda Wang,
and Russell Webb.

</span>
<span class="ltx_bibblock">Learning from simulated and unsupervised images through adversarial
training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 2107–2116, 2017.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Karen Simonyan and Andrew Zisserman.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1409.1556</span>, 2014.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Dustin Stokes.

</span>
<span class="ltx_bibblock">Cognitive penetrability of perception.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">Philosophy Compass</span>, 8(7):646–663, 2013.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Michael J Swain and Dana H Ballard.

</span>
<span class="ltx_bibblock">Color indexing.

</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">International journal of computer vision</span>, 7(1):11–32, 1991.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Yonglong Tian, Dilip Krishnan, and Phillip Isola.

</span>
<span class="ltx_bibblock">Contrastive multiview coding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16</span>, pages 776–794.
Springer, 2020.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Yonglong Tian, Dilip Krishnan, and Phillip Isola.

</span>
<span class="ltx_bibblock">Contrastive multiview coding.

</span>
<span class="ltx_bibblock">In <span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">Computer Vision – ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XI</span>, page 776–794,
Berlin, Heidelberg, 2020. Springer-Verlag.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel.

</span>
<span class="ltx_bibblock">Splicing vit features for semantic appearance transfer.

</span>
<span class="ltx_bibblock">In <span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 10748–10757, 2022.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.

</span>
<span class="ltx_bibblock">Deep image prior.

</span>
<span class="ltx_bibblock">In <span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 9446–9454, 2018.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Andreas Veit, Serge Belongie, and Theofanis Karaletsos.

</span>
<span class="ltx_bibblock">Conditional similarity networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 830–838, 2017.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Yuri Viazovetskyi, Vladimir Ivashkin, and Evgeny Kashin.

</span>
<span class="ltx_bibblock">Stylegan2 distillation for feed-forward image manipulation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII 16</span>, pages 170–186.
Springer, 2020.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Roman Christian Bachmann,
Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir.

</span>
<span class="ltx_bibblock">Clipasso: Semantically-aware object sketching.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Graphics (TOG)</span>, 41(4):1–11, 2022.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli.

</span>
<span class="ltx_bibblock">Image quality assessment: from error visibility to structural
similarity.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">IEEE transactions on image processing</span>, 13(4):600–612, 2004.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and
Alexander M. Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-Art Natural Language Processing.

</span>
<span class="ltx_bibblock">pages 38–45. Association for Computational Linguistics, Oct. 2020.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Zongze Wu, Dani Lischinski, and Eli Shechtman.

</span>
<span class="ltx_bibblock">Stylespace analysis: Disentangled controls for stylegan image
generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 12863–12872, 2021.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba.

</span>
<span class="ltx_bibblock">Sun database: Large-scale scene recognition from abbey to zoo.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">2010 IEEE computer society conference on computer vision and
pattern recognition</span>, pages 3485–3492. IEEE, 2010.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Lin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang.

</span>
<span class="ltx_bibblock">Fsim: A feature similarity index for image quality assessment.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">IEEE transactions on Image Processing</span>, 20(8):2378–2386, 2011.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Richard Zhang.

</span>
<span class="ltx_bibblock">Making convolutional networks shift-invariant again.

</span>
<span class="ltx_bibblock">In <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
7324–7334. PMLR, 2019.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of deep features as a perceptual
metric.

</span>
<span class="ltx_bibblock">In <span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 586–595, 2018.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">In the supplemental materials, Sec. <a href="#A1" title="Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> contains additional methodological details on dataset collection and model training, Sec. <a href="#A2" title="Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> provides more analyses of our model, and Sec. <a href="#A3" title="Appendix C Discussion ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> discusses the broader impact of our work, limitations, and licensing. Additional qualitative results, such as additional triplets from our dataset and visualizations of image retrieval and image reconstruction, are available on our project page.</p>
</div>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Method</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>AMT Details</h3>

<div id="A1.SS1.p1" class="ltx_para ltx_noindent">
<p id="A1.SS1.p1.1" class="ltx_p"><span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_bold">User interface.</span>
During the 2AFC study, users are instructed with the following prompt:</p>
<p id="A1.SS1.p1.2" class="ltx_p ltx_align_center"><span id="A1.SS1.p1.2.1" class="ltx_text ltx_font_typewriter">You will see three images: one labeled "Reference", </span></p>
<p id="A1.SS1.p1.3" class="ltx_p ltx_align_center"><span id="A1.SS1.p1.3.1" class="ltx_text ltx_font_typewriter">one labeled "Image A", and one labeled "Image B".</span></p>
<p id="A1.SS1.p1.4" class="ltx_p ltx_align_center"><span id="A1.SS1.p1.4.1" class="ltx_text ltx_font_typewriter">Select whether Image A or B is more similar to the Reference.</span></p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">For each task, users see an image triplet with the reference in the center and distortions on either side (randomized). Each user completes 2 practice tasks, 50 real tasks, and 10 sentinels (randomly placed), averaging 3 minutes for the entire assignment. We discard responses from users who do not respond with 100<math id="A1.SS1.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A1.SS1.p2.1.m1.1a"><mo id="A1.SS1.p2.1.m1.1.1" xref="A1.SS1.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><csymbol cd="latexml" id="A1.SS1.p2.1.m1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">\%</annotation></semantics></math> sentinel accuracy. See Figure <a href="#A1.F11" title="Figure 11 ‣ A.1 AMT Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> (left) for an example of the user interface.</p>
</div>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.1" class="ltx_p">Instructions for JND are as follows:</p>
</div>
<div id="A1.SS1.1" class="ltx_logical-block">
<div id="A1.SS1.1.p1" class="ltx_para">
<p id="A1.SS1.1.p1.1" class="ltx_p ltx_align_center"><span id="A1.SS1.1.p1.1.1" class="ltx_text ltx_font_typewriter">You will see four images one after the other.</span></p>
<p id="A1.SS1.1.p1.2" class="ltx_p ltx_align_center"><span id="A1.SS1.1.p1.2.1" class="ltx_text ltx_font_typewriter">Determine whether the first and third images are identical, then whether the second and fourth images are identical.</span></p>
<p id="A1.SS1.1.p1.3" class="ltx_p ltx_align_center"><span id="A1.SS1.1.p1.3.1" class="ltx_text ltx_font_typewriter">Each correct answer earns 1 point.</span></p>
</div>
</div>
<div id="A1.SS1.p4" class="ltx_para">
<p id="A1.SS1.p4.1" class="ltx_p">Users are shown four images for 500 ms each with a 1 second gap inbetween, and are then prompted to answer the two questions in Figure <a href="#A1.F11" title="Figure 11 ‣ A.1 AMT Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> (right). They are given feedback and a score, though these have no bearing on the JND results themselves. Each user completes 2 practice tasks, 24 tasks with “different” pairs, and 12 tasks with “same” pairs, averaging 10 minutes for the entire assignment.</p>
</div>
<div id="A1.SS1.p5" class="ltx_para">
<p id="A1.SS1.p5.1" class="ltx_p">The object retrieval study instructs users with the following text:</p>
</div>
<div id="A1.SS1.2" class="ltx_logical-block">
<div id="A1.SS1.2.p1" class="ltx_para">
<p id="A1.SS1.2.p1.1" class="ltx_p ltx_align_center"><span id="A1.SS1.2.p1.1.1" class="ltx_text ltx_font_typewriter">You will see a reference image and five other images under it.</span></p>
<p id="A1.SS1.2.p1.2" class="ltx_p ltx_align_center"><span id="A1.SS1.2.p1.2.1" class="ltx_text ltx_font_typewriter">Pick the image that is most similar to the reference.</span></p>
</div>
</div>
<div id="A1.SS1.p6" class="ltx_para">
<p id="A1.SS1.p6.1" class="ltx_p">See Figure <a href="#A1.F12" title="Figure 12 ‣ A.1 AMT Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> for an example of an object retrieval task. Each user completes 2 practice tasks, 40 real tasks, and 5 sentinels (randomly placed), averaging 3 minutes for the entire assignment. We discard responses from users who do not respond with 100<math id="A1.SS1.p6.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="A1.SS1.p6.1.m1.1a"><mo id="A1.SS1.p6.1.m1.1.1" xref="A1.SS1.p6.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.p6.1.m1.1b"><csymbol cd="latexml" id="A1.SS1.p6.1.m1.1.1.cmml" xref="A1.SS1.p6.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p6.1.m1.1c">\%</annotation></semantics></math> sentinel accuracy.</p>
</div>
<figure id="A1.F11" class="ltx_figure"><img src="/html/2306.09344/assets/figures/afc_jnd_screenshot.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span> <span id="A1.F11.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">User interface for AMT studies. <span id="A1.F11.2.1.1" class="ltx_text ltx_font_medium"> (Left) One image triplet shown to a user in 2AFC, who is prompted to pick “Image A” or “Image B”. (Right) In each JND task, users are shown a sequence of images and asked whether the image pairs were identical. Upon answering, they are given the correct answers.
</span></span></figcaption>
</figure>
<figure id="A1.F12" class="ltx_figure"><img src="/html/2306.09344/assets/figures/nn_turk_screenshot.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="267" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span> <span id="A1.F12.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">User interface for image retrieval study. <span id="A1.F12.2.1.1" class="ltx_text ltx_font_medium"> To evaluate object retrieval performance, users are asked to pick the image (A-E) most similar to the reference.
</span></span></figcaption>
</figure>
<figure id="A1.T2" class="ltx_table">
<table id="A1.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T2.3.3" class="ltx_tr">
<th id="A1.T2.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T2.3.3.4.1" class="ltx_text ltx_font_bold">Round</span></th>
<th id="A1.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="A1.T2.1.1.1.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="A1.T2.1.1.1.m1.1a"><mi mathvariant="normal" id="A1.T2.1.1.1.m1.1.1" xref="A1.T2.1.1.1.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="A1.T2.1.1.1.m1.1b"><ci id="A1.T2.1.1.1.m1.1.1.cmml" xref="A1.T2.1.1.1.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.1.1.1.m1.1c">\#</annotation></semantics></math><span id="A1.T2.1.1.1.1" class="ltx_text ltx_font_bold"> unanimous</span>
</th>
<th id="A1.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="A1.T2.2.2.2.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="A1.T2.2.2.2.m1.1a"><mi mathvariant="normal" id="A1.T2.2.2.2.m1.1.1" xref="A1.T2.2.2.2.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="A1.T2.2.2.2.m1.1b"><ci id="A1.T2.2.2.2.m1.1.1.cmml" xref="A1.T2.2.2.2.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.2.2.2.m1.1c">\#</annotation></semantics></math><span id="A1.T2.2.2.2.1" class="ltx_text ltx_font_bold"> additional</span>
</th>
<th id="A1.T2.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="A1.T2.3.3.3.m1.1" class="ltx_Math" alttext="\#" display="inline"><semantics id="A1.T2.3.3.3.m1.1a"><mi mathvariant="normal" id="A1.T2.3.3.3.m1.1.1" xref="A1.T2.3.3.3.m1.1.1.cmml">#</mi><annotation-xml encoding="MathML-Content" id="A1.T2.3.3.3.m1.1b"><ci id="A1.T2.3.3.3.m1.1.1.cmml" xref="A1.T2.3.3.3.m1.1.1">#</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T2.3.3.3.m1.1c">\#</annotation></semantics></math><span id="A1.T2.3.3.3.1" class="ltx_text ltx_font_bold"> kept</span>
</th>
<td id="A1.T2.3.3.5" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="A1.T2.3.4.1" class="ltx_tr">
<td id="A1.T2.3.4.1.1" class="ltx_td"></td>
<th id="A1.T2.3.4.1.2" class="ltx_td ltx_th ltx_th_column"></th>
<th id="A1.T2.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A1.T2.3.4.1.3.1" class="ltx_text ltx_font_bold">sentinel failures</span></th>
<th id="A1.T2.3.4.1.4" class="ltx_td ltx_th ltx_th_column"></th>
<td id="A1.T2.3.4.1.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.5.2" class="ltx_tr">
<td id="A1.T2.3.5.2.1" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="A1.T2.3.5.2.2" class="ltx_td ltx_align_center ltx_border_t">100,000</td>
<td id="A1.T2.3.5.2.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="A1.T2.3.5.2.4" class="ltx_td ltx_align_center ltx_border_t">100,000</td>
<td id="A1.T2.3.5.2.5" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A1.T2.3.6.3" class="ltx_tr">
<td id="A1.T2.3.6.3.1" class="ltx_td ltx_align_center">2</td>
<td id="A1.T2.3.6.3.2" class="ltx_td ltx_align_center">74,346</td>
<td id="A1.T2.3.6.3.3" class="ltx_td ltx_align_center">6,750</td>
<td id="A1.T2.3.6.3.4" class="ltx_td ltx_align_center">81,096</td>
<td id="A1.T2.3.6.3.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.7.4" class="ltx_tr">
<td id="A1.T2.3.7.4.1" class="ltx_td ltx_align_center">3</td>
<td id="A1.T2.3.7.4.2" class="ltx_td ltx_align_center">63,423</td>
<td id="A1.T2.3.7.4.3" class="ltx_td ltx_align_center">2,411</td>
<td id="A1.T2.3.7.4.4" class="ltx_td ltx_align_center">65,834</td>
<td id="A1.T2.3.7.4.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.8.5" class="ltx_tr">
<td id="A1.T2.3.8.5.1" class="ltx_td ltx_align_center">4</td>
<td id="A1.T2.3.8.5.2" class="ltx_td ltx_align_center">51,097</td>
<td id="A1.T2.3.8.5.3" class="ltx_td ltx_align_center">592</td>
<td id="A1.T2.3.8.5.4" class="ltx_td ltx_align_center">51,689</td>
<td id="A1.T2.3.8.5.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.9.6" class="ltx_tr">
<td id="A1.T2.3.9.6.1" class="ltx_td ltx_align_center">5</td>
<td id="A1.T2.3.9.6.2" class="ltx_td ltx_align_center">43,615</td>
<td id="A1.T2.3.9.6.3" class="ltx_td ltx_align_center">289</td>
<td id="A1.T2.3.9.6.4" class="ltx_td ltx_align_center">43,904</td>
<td id="A1.T2.3.9.6.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.10.7" class="ltx_tr">
<td id="A1.T2.3.10.7.1" class="ltx_td ltx_align_center">6</td>
<td id="A1.T2.3.10.7.2" class="ltx_td ltx_align_center">37,696</td>
<td id="A1.T2.3.10.7.3" class="ltx_td ltx_align_center">113</td>
<td id="A1.T2.3.10.7.4" class="ltx_td ltx_align_center">37,809</td>
<td id="A1.T2.3.10.7.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.11.8" class="ltx_tr">
<td id="A1.T2.3.11.8.1" class="ltx_td ltx_align_center">7</td>
<td id="A1.T2.3.11.8.2" class="ltx_td ltx_align_center">30,420</td>
<td id="A1.T2.3.11.8.3" class="ltx_td ltx_align_center">16</td>
<td id="A1.T2.3.11.8.4" class="ltx_td ltx_align_center">30,436</td>
<td id="A1.T2.3.11.8.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.12.9" class="ltx_tr">
<td id="A1.T2.3.12.9.1" class="ltx_td ltx_align_center">8</td>
<td id="A1.T2.3.12.9.2" class="ltx_td ltx_align_center">25,079</td>
<td id="A1.T2.3.12.9.3" class="ltx_td ltx_align_center">0</td>
<td id="A1.T2.3.12.9.4" class="ltx_td ltx_align_center">25,079</td>
<td id="A1.T2.3.12.9.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.13.10" class="ltx_tr">
<td id="A1.T2.3.13.10.1" class="ltx_td ltx_align_center">9</td>
<td id="A1.T2.3.13.10.2" class="ltx_td ltx_align_center">22,098</td>
<td id="A1.T2.3.13.10.3" class="ltx_td ltx_align_center">0</td>
<td id="A1.T2.3.13.10.4" class="ltx_td ltx_align_center">22,098</td>
<td id="A1.T2.3.13.10.5" class="ltx_td"></td>
</tr>
<tr id="A1.T2.3.14.11" class="ltx_tr">
<td id="A1.T2.3.14.11.1" class="ltx_td ltx_align_center ltx_border_bb">10</td>
<td id="A1.T2.3.14.11.2" class="ltx_td ltx_align_center ltx_border_bb">20,019</td>
<td id="A1.T2.3.14.11.3" class="ltx_td ltx_align_center ltx_border_bb">0</td>
<td id="A1.T2.3.14.11.4" class="ltx_td ltx_align_center ltx_border_bb">20,019</td>
<td id="A1.T2.3.14.11.5" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="A1.T2.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Filtering for cognitively impenetrable triplets.<span id="A1.T2.5.1.1" class="ltx_text ltx_font_medium"> We start with 100K triplets, and advance triplets to the subsequent round if the human vote remains unanimous, or if the added vote came from a user who did not pass the sentinels and thus the vote is inconclusive (the vote from this user is discarded). </span></span></figcaption>
</figure>
<div id="A1.SS1.p7" class="ltx_para ltx_noindent">
<p id="A1.SS1.p7.1" class="ltx_p"><span id="A1.SS1.p7.1.1" class="ltx_text ltx_font_bold">Cost breakdown.</span>
Across 10 rounds of 2AFC studies, we show users 477,964 triplet instances (see Table <a href="#A1.T2" title="Table 2 ‣ A.1 AMT Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> for the full breakdown). Each user is paid $0.50 for one assignment consisting of 50 triplets, averaging $10.00/hr. In total, we pay users $4779.64 to collect 2AFC judgments on our dataset.</p>
</div>
<div id="A1.SS1.p8" class="ltx_para">
<p id="A1.SS1.p8.1" class="ltx_p">We run JND on image triplets in our test set that received <math id="A1.SS1.p8.1.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="A1.SS1.p8.1.m1.1a"><mo id="A1.SS1.p8.1.m1.1.1" xref="A1.SS1.p8.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.p8.1.m1.1b"><geq id="A1.SS1.p8.1.m1.1.1.cmml" xref="A1.SS1.p8.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p8.1.m1.1c">\geq</annotation></semantics></math>6 2AFC judgments. Each user is paid $2.00 for one assignment consisting of 48 image pairs, averaging $12.00/hr. In total, we pay users $156.00 to collect JND judgments on our test set.</p>
</div>
<div id="A1.SS1.p9" class="ltx_para">
<p id="A1.SS1.p9.1" class="ltx_p">Our object retrieval user study is conducted over 200 query images with 10 nearest neighbors. Users are paid $0.50 for one assignment consisting of 40 tasks, averaging $10.00/hr. In total, we pay users $40.50 to collect object retrieval judgments.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Additional Dataset Details</h3>

<div id="A1.SS2.p1" class="ltx_para ltx_noindent">
<p id="A1.SS2.p1.1" class="ltx_p"><span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_bold">Filtering text prompts.</span> As described in Sec. <a href="#S3.SS1" title="3.1 Generating images with varied distortions ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> we sample images using image labels drawn from popular image classification datasets. We perform basic filtering to avoid human categories that typically result in malformed generated images, such as “man” and “baby”. Thus, human faces only feature in a small percentage of our dataset (&lt;0.5%), typically in categories such as “lab coat” and “bonnet” that are still associated with humans. We note that even with some human categories excluded, our model is still sensitive to the presence of people, as shown in Fig. <a href="#S5.F7" title="Figure 7 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. For a full list of categories used, refer to our GitHub page.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para ltx_noindent">
<p id="A1.SS2.p2.1" class="ltx_p"><span id="A1.SS2.p2.1.1" class="ltx_text ltx_font_bold">2AFC task.</span>
Following Sec. <a href="#S3.SS2" title="3.2 Human perceptual judgments ‣ 3 Perceptual dataset collection ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> in the main text, we filter the images over 10 rounds to obtain cognitively impenetrable triplets where humans tend to vote the same way despite various differences between the images. Statistics for each round of filtering is reported in Table <a href="#A1.T2" title="Table 2 ‣ A.1 AMT Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, which leaves us with roughly 20% of the original triplets containing unanimous votes. We discard all votes from any worker who fails the sentinel task. As a result, not all triplets have the same number of votes. The resulting sizes of the train, validation, and test splits and additional statistics on each split are reported in Table <a href="#A1.T3" title="Table 3 ‣ A.2 Additional Dataset Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (top). In practice, we further discard triplets with five or fewer unanimous votes.</p>
</div>
<div id="A1.SS2.p3" class="ltx_para ltx_noindent">
<p id="A1.SS2.p3.1" class="ltx_p"><span id="A1.SS2.p3.1.1" class="ltx_text ltx_font_bold">JND task.</span> The JND triplets are meant to capture the decision boundary where two different images are similar enough to be confused as identical, in the presence of a masking image. Each triplet is divided into two pairs: Ref vs. A and Ref vs. B. These pairs are presented to different workers in different interleaving sequences. We collect three judgments for each pair and take the majority vote among the three judgments, thus collecting six judgments in total per triplet (Table <a href="#A1.T3" title="Table 3 ‣ A.2 Additional Dataset Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (bottom)).</p>
</div>
<figure id="A1.T3" class="ltx_table">
<table id="A1.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T3.1.1.1" class="ltx_tr">
<th id="A1.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="A1.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Split</span></th>
<th id="A1.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T3.1.1.1.3.1" class="ltx_text ltx_font_bold"># Samples</span></th>
<th id="A1.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T3.1.1.1.4.1" class="ltx_text ltx_font_bold">Avg # Votes</span></th>
<th id="A1.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T3.1.1.1.5.1" class="ltx_text ltx_font_bold">Consensus Type</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T3.1.2.1" class="ltx_tr">
<td id="A1.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="A1.T3.1.2.1.1.1" class="ltx_text">2AFC</span></td>
<td id="A1.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Train</td>
<td id="A1.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">15941</td>
<td id="A1.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">7.11</td>
<td id="A1.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">Unanimous</td>
</tr>
<tr id="A1.T3.1.3.2" class="ltx_tr">
<td id="A1.T3.1.3.2.1" class="ltx_td ltx_align_center">Validation</td>
<td id="A1.T3.1.3.2.2" class="ltx_td ltx_align_center">1958</td>
<td id="A1.T3.1.3.2.3" class="ltx_td ltx_align_center">7.07</td>
<td id="A1.T3.1.3.2.4" class="ltx_td ltx_align_center">Unanimous</td>
</tr>
<tr id="A1.T3.1.4.3" class="ltx_tr">
<td id="A1.T3.1.4.3.1" class="ltx_td ltx_align_center">Test</td>
<td id="A1.T3.1.4.3.2" class="ltx_td ltx_align_center">2120</td>
<td id="A1.T3.1.4.3.3" class="ltx_td ltx_align_center">7.04</td>
<td id="A1.T3.1.4.3.4" class="ltx_td ltx_align_center">Unanimous</td>
</tr>
<tr id="A1.T3.1.5.4" class="ltx_tr">
<td id="A1.T3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">JND</td>
<td id="A1.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Test</td>
<td id="A1.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">411</td>
<td id="A1.T3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">6</td>
<td id="A1.T3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Majority</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="A1.T3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset Statistics.<span id="A1.T3.3.1.1" class="ltx_text ltx_font_medium"> For each dataset, we report the number of samples and the average number of votes for each triplet after filtering for sentinel failures. Labels for the 2AFC dataset are based on a unanimous vote, while for JND we take the majority vote over three trials per pair (six trials per triplet). </span></span></figcaption>
</figure>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Model Training.</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.6" class="ltx_p">For all fine-tuned models (both <span id="A1.SS3.p1.6.1" class="ltx_text ltx_font_typewriter">Tuned - MLP</span> and <span id="A1.SS3.p1.6.2" class="ltx_text ltx_font_typewriter">Tuned - LoRA</span>) we use the NIGHTS training dataset, with an 80%-10%-10% split as described in Table <a href="#A1.T3" title="Table 3 ‣ A.2 Additional Dataset Details ‣ Appendix A Method ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, and images of size <math id="A1.SS3.p1.1.m1.1" class="ltx_Math" alttext="768\times 768" display="inline"><semantics id="A1.SS3.p1.1.m1.1a"><mrow id="A1.SS3.p1.1.m1.1.1" xref="A1.SS3.p1.1.m1.1.1.cmml"><mn id="A1.SS3.p1.1.m1.1.1.2" xref="A1.SS3.p1.1.m1.1.1.2.cmml">768</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS3.p1.1.m1.1.1.1" xref="A1.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="A1.SS3.p1.1.m1.1.1.3" xref="A1.SS3.p1.1.m1.1.1.3.cmml">768</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.1.m1.1b"><apply id="A1.SS3.p1.1.m1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1"><times id="A1.SS3.p1.1.m1.1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="A1.SS3.p1.1.m1.1.1.2.cmml" xref="A1.SS3.p1.1.m1.1.1.2">768</cn><cn type="integer" id="A1.SS3.p1.1.m1.1.1.3.cmml" xref="A1.SS3.p1.1.m1.1.1.3">768</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.1.m1.1c">768\times 768</annotation></semantics></math> resized to <math id="A1.SS3.p1.2.m2.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="A1.SS3.p1.2.m2.1a"><mrow id="A1.SS3.p1.2.m2.1.1" xref="A1.SS3.p1.2.m2.1.1.cmml"><mn id="A1.SS3.p1.2.m2.1.1.2" xref="A1.SS3.p1.2.m2.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="A1.SS3.p1.2.m2.1.1.1" xref="A1.SS3.p1.2.m2.1.1.1.cmml">×</mo><mn id="A1.SS3.p1.2.m2.1.1.3" xref="A1.SS3.p1.2.m2.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.2.m2.1b"><apply id="A1.SS3.p1.2.m2.1.1.cmml" xref="A1.SS3.p1.2.m2.1.1"><times id="A1.SS3.p1.2.m2.1.1.1.cmml" xref="A1.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="A1.SS3.p1.2.m2.1.1.2.cmml" xref="A1.SS3.p1.2.m2.1.1.2">224</cn><cn type="integer" id="A1.SS3.p1.2.m2.1.1.3.cmml" xref="A1.SS3.p1.2.m2.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.2.m2.1c">224\times 224</annotation></semantics></math>. We train on a single NVIDIA GeForce RTX 3090 or NVIDIA TITAN RTX GPU with an Adam optimizer, learning rate of <math id="A1.SS3.p1.3.m3.1" class="ltx_Math" alttext="3e-4" display="inline"><semantics id="A1.SS3.p1.3.m3.1a"><mrow id="A1.SS3.p1.3.m3.1.1" xref="A1.SS3.p1.3.m3.1.1.cmml"><mrow id="A1.SS3.p1.3.m3.1.1.2" xref="A1.SS3.p1.3.m3.1.1.2.cmml"><mn id="A1.SS3.p1.3.m3.1.1.2.2" xref="A1.SS3.p1.3.m3.1.1.2.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="A1.SS3.p1.3.m3.1.1.2.1" xref="A1.SS3.p1.3.m3.1.1.2.1.cmml">​</mo><mi id="A1.SS3.p1.3.m3.1.1.2.3" xref="A1.SS3.p1.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="A1.SS3.p1.3.m3.1.1.1" xref="A1.SS3.p1.3.m3.1.1.1.cmml">−</mo><mn id="A1.SS3.p1.3.m3.1.1.3" xref="A1.SS3.p1.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.3.m3.1b"><apply id="A1.SS3.p1.3.m3.1.1.cmml" xref="A1.SS3.p1.3.m3.1.1"><minus id="A1.SS3.p1.3.m3.1.1.1.cmml" xref="A1.SS3.p1.3.m3.1.1.1"></minus><apply id="A1.SS3.p1.3.m3.1.1.2.cmml" xref="A1.SS3.p1.3.m3.1.1.2"><times id="A1.SS3.p1.3.m3.1.1.2.1.cmml" xref="A1.SS3.p1.3.m3.1.1.2.1"></times><cn type="integer" id="A1.SS3.p1.3.m3.1.1.2.2.cmml" xref="A1.SS3.p1.3.m3.1.1.2.2">3</cn><ci id="A1.SS3.p1.3.m3.1.1.2.3.cmml" xref="A1.SS3.p1.3.m3.1.1.2.3">𝑒</ci></apply><cn type="integer" id="A1.SS3.p1.3.m3.1.1.3.cmml" xref="A1.SS3.p1.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.3.m3.1c">3e-4</annotation></semantics></math>, weight decay of 0, and batch size of 512 (non-ensemble models) and 16 (ensemble models). In MLP-tuned training, we use a width of 512. We tune the number of training epochs using the validation set; for the <span id="A1.SS3.p1.6.3" class="ltx_text ltx_font_typewriter">Tuned - LoRA</span> ensemble model (DreamSim) we train for 6 epochs. For <span id="A1.SS3.p1.6.4" class="ltx_text ltx_font_typewriter">Tuned - LoRA</span> models we use rank <math id="A1.SS3.p1.4.m4.1" class="ltx_Math" alttext="r=16" display="inline"><semantics id="A1.SS3.p1.4.m4.1a"><mrow id="A1.SS3.p1.4.m4.1.1" xref="A1.SS3.p1.4.m4.1.1.cmml"><mi id="A1.SS3.p1.4.m4.1.1.2" xref="A1.SS3.p1.4.m4.1.1.2.cmml">r</mi><mo id="A1.SS3.p1.4.m4.1.1.1" xref="A1.SS3.p1.4.m4.1.1.1.cmml">=</mo><mn id="A1.SS3.p1.4.m4.1.1.3" xref="A1.SS3.p1.4.m4.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.4.m4.1b"><apply id="A1.SS3.p1.4.m4.1.1.cmml" xref="A1.SS3.p1.4.m4.1.1"><eq id="A1.SS3.p1.4.m4.1.1.1.cmml" xref="A1.SS3.p1.4.m4.1.1.1"></eq><ci id="A1.SS3.p1.4.m4.1.1.2.cmml" xref="A1.SS3.p1.4.m4.1.1.2">𝑟</ci><cn type="integer" id="A1.SS3.p1.4.m4.1.1.3.cmml" xref="A1.SS3.p1.4.m4.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.4.m4.1c">r=16</annotation></semantics></math>, scaling <math id="A1.SS3.p1.5.m5.1" class="ltx_Math" alttext="\alpha=0.5" display="inline"><semantics id="A1.SS3.p1.5.m5.1a"><mrow id="A1.SS3.p1.5.m5.1.1" xref="A1.SS3.p1.5.m5.1.1.cmml"><mi id="A1.SS3.p1.5.m5.1.1.2" xref="A1.SS3.p1.5.m5.1.1.2.cmml">α</mi><mo id="A1.SS3.p1.5.m5.1.1.1" xref="A1.SS3.p1.5.m5.1.1.1.cmml">=</mo><mn id="A1.SS3.p1.5.m5.1.1.3" xref="A1.SS3.p1.5.m5.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.5.m5.1b"><apply id="A1.SS3.p1.5.m5.1.1.cmml" xref="A1.SS3.p1.5.m5.1.1"><eq id="A1.SS3.p1.5.m5.1.1.1.cmml" xref="A1.SS3.p1.5.m5.1.1.1"></eq><ci id="A1.SS3.p1.5.m5.1.1.2.cmml" xref="A1.SS3.p1.5.m5.1.1.2">𝛼</ci><cn type="float" id="A1.SS3.p1.5.m5.1.1.3.cmml" xref="A1.SS3.p1.5.m5.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.5.m5.1c">\alpha=0.5</annotation></semantics></math>, and dropout <math id="A1.SS3.p1.6.m6.1" class="ltx_Math" alttext="p=0.3" display="inline"><semantics id="A1.SS3.p1.6.m6.1a"><mrow id="A1.SS3.p1.6.m6.1.1" xref="A1.SS3.p1.6.m6.1.1.cmml"><mi id="A1.SS3.p1.6.m6.1.1.2" xref="A1.SS3.p1.6.m6.1.1.2.cmml">p</mi><mo id="A1.SS3.p1.6.m6.1.1.1" xref="A1.SS3.p1.6.m6.1.1.1.cmml">=</mo><mn id="A1.SS3.p1.6.m6.1.1.3" xref="A1.SS3.p1.6.m6.1.1.3.cmml">0.3</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.6.m6.1b"><apply id="A1.SS3.p1.6.m6.1.1.cmml" xref="A1.SS3.p1.6.m6.1.1"><eq id="A1.SS3.p1.6.m6.1.1.1.cmml" xref="A1.SS3.p1.6.m6.1.1.1"></eq><ci id="A1.SS3.p1.6.m6.1.1.2.cmml" xref="A1.SS3.p1.6.m6.1.1.2">𝑝</ci><cn type="float" id="A1.SS3.p1.6.m6.1.1.3.cmml" xref="A1.SS3.p1.6.m6.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.6.m6.1c">p=0.3</annotation></semantics></math>. Training time is approximately 30 min/epoch for LoRA-tuned models, and 15 min/epoch for MLP-tuned models.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Feature Inversion</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">In Fig. <a href="#S6.F10" title="Figure 10 ‣ 6 Applications ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> we show examples of using our metric to optimize for an image generation that is similar to some target image. We do so in three experiments, with increasingly strong generative priors.</p>
</div>
<div id="A1.SS4.p2" class="ltx_para ltx_noindent">
<p id="A1.SS4.p2.1" class="ltx_p"><span id="A1.SS4.p2.1.1" class="ltx_text ltx_font_bold">Optimization.</span> In this setup, we initialize an image randomly and update it iteratively using gradient descent with respect to the distance between it and the target image. Given a random crop of our working image and the target image we compute the distance between the two using a specific metric (DINO/OpenCLIP/Ensemble/DreamSim) and their corresponding embeddings. Note that in this setup there is no generator neural network.</p>
</div>
<div id="A1.SS4.p3" class="ltx_para ltx_noindent">
<p id="A1.SS4.p3.1" class="ltx_p"><span id="A1.SS4.p3.1.1" class="ltx_text ltx_font_bold">Deep Image Prior.</span> This technique involves beginning with a frozen random noise which serves as an input to a trainable U-Net. The U-Net is optimized iteratively by comparing the output of the U-Net with the target image using different metrics and their embeddings. This introduces a CNN prior on the generated image as described in the Deep Image Prior paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>.</p>
</div>
<div id="A1.SS4.p4" class="ltx_para ltx_noindent">
<p id="A1.SS4.p4.1" class="ltx_p"><span id="A1.SS4.p4.1.1" class="ltx_text ltx_font_bold">Guided Diffusion.</span> This technique involves using classifier guidance to guide the generation of a 256x256 unconditional ImageNet-trained diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> using an input prompt image. The guidance is done by computing the gradient of some loss function, given the estimated clean image and the target image, with respect to the current iteration. At every training step we compute the spherical distance loss between our model’s embeddings of 64 random crops of the input image and the synthesized image. We combine this distance with a total variation loss, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The gradient of this loss is then added to the mean of the current iteration, which pushes the generated image to be closer to the target image.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Experiments</h2>

<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Additional Evaluations</h3>

<div id="A2.SS1.p1" class="ltx_para ltx_noindent">
<p id="A2.SS1.p1.1" class="ltx_p"><span id="A2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Full Evaluation on Large Vision Models.</span>
In Section <a href="#S5.SS1" title="5.1 How well do existing metrics align with human judgments? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> and Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Learning an improved metric ‣ 4 Perceptual metric learning ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> in the main text we report results using the best-performing setting of various large vision model backbones. Table <a href="#A2.T4" title="Table 4 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the experimental variation over multiple runs, in which the LoRA variation consistently outperforms the MLP variation. Table <a href="#A2.T5" title="Table 5 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> evaluates additional model settings, spanning different ViT model sizes, patch sizes, and strides.</p>
</div>
<figure id="A2.T4" class="ltx_table">
<div id="A2.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:67.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.5pt,2.2pt) scale(0.937699816768292,0.937699816768292) ;">
<table id="A2.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T4.1.1.1.1" class="ltx_tr">
<td id="A2.T4.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="A2.T4.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="5">Independent training seeds</th>
<th id="A2.T4.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<td id="A2.T4.1.1.1.1.4" class="ltx_td ltx_border_tt"></td>
</tr>
<tr id="A2.T4.1.1.2.2" class="ltx_tr">
<td id="A2.T4.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Ensemble tuning</span></td>
<td id="A2.T4.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Trial 1</span></td>
<td id="A2.T4.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Trial 2</span></td>
<td id="A2.T4.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Trial 3</span></td>
<td id="A2.T4.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Trial 4</span></td>
<td id="A2.T4.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Trial 5</span></td>
<td id="A2.T4.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Avg.</span></td>
<td id="A2.T4.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T4.1.1.2.2.8.1" class="ltx_text ltx_font_bold">Stdev.</span></td>
</tr>
<tr id="A2.T4.1.1.3.3" class="ltx_tr">
<td id="A2.T4.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t">MLP</td>
<td id="A2.T4.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">93.4</td>
<td id="A2.T4.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">93.1</td>
<td id="A2.T4.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">93.1</td>
<td id="A2.T4.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
<td id="A2.T4.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">92.9</td>
<td id="A2.T4.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">93.3</td>
<td id="A2.T4.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">0.326</td>
</tr>
<tr id="A2.T4.1.1.4.4" class="ltx_tr">
<td id="A2.T4.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">LoRA</td>
<td id="A2.T4.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">96.2</td>
<td id="A2.T4.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">96.3</td>
<td id="A2.T4.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">95.1</td>
<td id="A2.T4.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">96.0</td>
<td id="A2.T4.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">95.8</td>
<td id="A2.T4.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">95.9</td>
<td id="A2.T4.1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.416</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span><span id="A2.T4.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Experimental variation for ensemble models.<span id="A2.T4.3.1.1" class="ltx_text ltx_font_medium"> We train the <span id="A2.T4.3.1.1.1" class="ltx_text ltx_font_typewriter">Tuned - MLP</span> and <span id="A2.T4.3.1.1.2" class="ltx_text ltx_font_typewriter">Tuned - LoRA</span> ensemble models on 5 seeds each and record their test accuracies at the same epoch as was recorded in the main paper. </span></span></figcaption>
</figure>
<figure id="A2.T5" class="ltx_table">
<div id="A2.T5.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:511.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.3pt,14.5pt) scale(0.946135752474492,0.946135752474492) ;">
<table id="A2.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T5.1.1.1.1" class="ltx_tr">
<td id="A2.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span id="A2.T5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="A2.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="A2.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Alignment</span></td>
</tr>
<tr id="A2.T5.1.1.2.2" class="ltx_tr">
<td id="A2.T5.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Model Class</span></td>
<td id="A2.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Model Name</span></td>
<td id="A2.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Model Type</span></td>
<td id="A2.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Feature</span></td>
<td id="A2.T5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Overall</span></td>
<td id="A2.T5.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.6.1" class="ltx_text ltx_font_bold">ImageNet</span></td>
<td id="A2.T5.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T5.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Non-ImageNet</span></td>
</tr>
<tr id="A2.T5.1.1.3.3" class="ltx_tr">
<td id="A2.T5.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A2.T5.1.1.3.3.1.1" class="ltx_text">
<span id="A2.T5.1.1.3.3.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="A2.T5.1.1.3.3.1.1.1.1" class="ltx_p"><span id="A2.T5.1.1.3.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Base</span></span>
<span id="A2.T5.1.1.3.3.1.1.1.2" class="ltx_p"><span id="A2.T5.1.1.3.3.1.1.1.2.1" class="ltx_text ltx_font_bold">Models</span></span>
</span></span></td>
<td id="A2.T5.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">PSNR</td>
<td id="A2.T5.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="A2.T5.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="A2.T5.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">57.1</td>
<td id="A2.T5.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">57.3</td>
<td id="A2.T5.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">57.0</td>
</tr>
<tr id="A2.T5.1.1.4.4" class="ltx_tr">
<td id="A2.T5.1.1.4.4.1" class="ltx_td ltx_align_center">SSIM</td>
<td id="A2.T5.1.1.4.4.2" class="ltx_td ltx_align_center">–</td>
<td id="A2.T5.1.1.4.4.3" class="ltx_td ltx_align_center">–</td>
<td id="A2.T5.1.1.4.4.4" class="ltx_td ltx_align_center">57.3</td>
<td id="A2.T5.1.1.4.4.5" class="ltx_td ltx_align_center">58.5</td>
<td id="A2.T5.1.1.4.4.6" class="ltx_td ltx_align_center">55.8</td>
</tr>
<tr id="A2.T5.1.1.5.5" class="ltx_tr">
<td id="A2.T5.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="A2.T5.1.1.5.5.1.1" class="ltx_text">
<span id="A2.T5.1.1.5.5.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="A2.T5.1.1.5.5.1.1.1.1" class="ltx_p"><span id="A2.T5.1.1.5.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Prior-Learned</span></span>
<span id="A2.T5.1.1.5.5.1.1.1.2" class="ltx_p"><span id="A2.T5.1.1.5.5.1.1.1.2.1" class="ltx_text ltx_font_bold">Metrics</span></span>
</span></span></td>
<td id="A2.T5.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">LPIPS</td>
<td id="A2.T5.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">AlexNet-Linear</td>
<td id="A2.T5.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">–</td>
<td id="A2.T5.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">70.7</td>
<td id="A2.T5.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">69.3</td>
<td id="A2.T5.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">72.7</td>
</tr>
<tr id="A2.T5.1.1.6.6" class="ltx_tr">
<td id="A2.T5.1.1.6.6.1" class="ltx_td ltx_align_center">DISTS</td>
<td id="A2.T5.1.1.6.6.2" class="ltx_td ltx_align_center">VGG16</td>
<td id="A2.T5.1.1.6.6.3" class="ltx_td ltx_align_center">–</td>
<td id="A2.T5.1.1.6.6.4" class="ltx_td ltx_align_center">86.0</td>
<td id="A2.T5.1.1.6.6.5" class="ltx_td ltx_align_center">87.1</td>
<td id="A2.T5.1.1.6.6.6" class="ltx_td ltx_align_center">84.5</td>
</tr>
<tr id="A2.T5.1.1.7.7" class="ltx_tr">
<td id="A2.T5.1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="14"><span id="A2.T5.1.1.7.7.1.1" class="ltx_text">
<span id="A2.T5.1.1.7.7.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="A2.T5.1.1.7.7.1.1.1.1" class="ltx_p"><span id="A2.T5.1.1.7.7.1.1.1.1.1" class="ltx_text ltx_font_bold">Base</span></span>
<span id="A2.T5.1.1.7.7.1.1.1.2" class="ltx_p"><span id="A2.T5.1.1.7.7.1.1.1.2.1" class="ltx_text ltx_font_bold">Models</span></span>
</span></span></td>
<td id="A2.T5.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="A2.T5.1.1.7.7.2.1" class="ltx_text">CLIP</span></td>
<td id="A2.T5.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">ViT B/16</td>
<td id="A2.T5.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">Embedding</td>
<td id="A2.T5.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">82.2</td>
<td id="A2.T5.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">82.6</td>
<td id="A2.T5.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_t">81.7</td>
</tr>
<tr id="A2.T5.1.1.8.8" class="ltx_tr">
<td id="A2.T5.1.1.8.8.1" class="ltx_td ltx_align_center">ViT B/32</td>
<td id="A2.T5.1.1.8.8.2" class="ltx_td ltx_align_center">Embedding</td>
<td id="A2.T5.1.1.8.8.3" class="ltx_td ltx_align_center">83.1</td>
<td id="A2.T5.1.1.8.8.4" class="ltx_td ltx_align_center">83.8</td>
<td id="A2.T5.1.1.8.8.5" class="ltx_td ltx_align_center">82.1</td>
</tr>
<tr id="A2.T5.1.1.9.9" class="ltx_tr">
<td id="A2.T5.1.1.9.9.1" class="ltx_td ltx_align_center">ViT L/14</td>
<td id="A2.T5.1.1.9.9.2" class="ltx_td ltx_align_center">Embedding</td>
<td id="A2.T5.1.1.9.9.3" class="ltx_td ltx_align_center">81.8</td>
<td id="A2.T5.1.1.9.9.4" class="ltx_td ltx_align_center">83.3</td>
<td id="A2.T5.1.1.9.9.5" class="ltx_td ltx_align_center">79.8</td>
</tr>
<tr id="A2.T5.1.1.10.10" class="ltx_tr">
<td id="A2.T5.1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="A2.T5.1.1.10.10.1.1" class="ltx_text">DINO</span></td>
<td id="A2.T5.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">ViT S/8</td>
<td id="A2.T5.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">CLS</td>
<td id="A2.T5.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">89.0</td>
<td id="A2.T5.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">89.7</td>
<td id="A2.T5.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t">88.0</td>
</tr>
<tr id="A2.T5.1.1.11.11" class="ltx_tr">
<td id="A2.T5.1.1.11.11.1" class="ltx_td ltx_align_center">ViT S/16</td>
<td id="A2.T5.1.1.11.11.2" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.11.11.3" class="ltx_td ltx_align_center">89.6</td>
<td id="A2.T5.1.1.11.11.4" class="ltx_td ltx_align_center">90.2</td>
<td id="A2.T5.1.1.11.11.5" class="ltx_td ltx_align_center">88.8</td>
</tr>
<tr id="A2.T5.1.1.12.12" class="ltx_tr">
<td id="A2.T5.1.1.12.12.1" class="ltx_td ltx_align_center">ViT B/8</td>
<td id="A2.T5.1.1.12.12.2" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.12.12.3" class="ltx_td ltx_align_center">88.6</td>
<td id="A2.T5.1.1.12.12.4" class="ltx_td ltx_align_center">88.6</td>
<td id="A2.T5.1.1.12.12.5" class="ltx_td ltx_align_center">88.5</td>
</tr>
<tr id="A2.T5.1.1.13.13" class="ltx_tr">
<td id="A2.T5.1.1.13.13.1" class="ltx_td ltx_align_center">ViT B/16</td>
<td id="A2.T5.1.1.13.13.2" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.13.13.3" class="ltx_td ltx_align_center">90.1</td>
<td id="A2.T5.1.1.13.13.4" class="ltx_td ltx_align_center">90.6</td>
<td id="A2.T5.1.1.13.13.5" class="ltx_td ltx_align_center">89.5</td>
</tr>
<tr id="A2.T5.1.1.14.14" class="ltx_tr">
<td id="A2.T5.1.1.14.14.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="A2.T5.1.1.14.14.1.1" class="ltx_text">MAE</span></td>
<td id="A2.T5.1.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t">ViT B/16</td>
<td id="A2.T5.1.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t">CLS</td>
<td id="A2.T5.1.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t">82.1</td>
<td id="A2.T5.1.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t">81.0</td>
<td id="A2.T5.1.1.14.14.6" class="ltx_td ltx_align_center ltx_border_t">83.5</td>
</tr>
<tr id="A2.T5.1.1.15.15" class="ltx_tr">
<td id="A2.T5.1.1.15.15.1" class="ltx_td ltx_align_center">ViT L/16</td>
<td id="A2.T5.1.1.15.15.2" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.15.15.3" class="ltx_td ltx_align_center">82.7</td>
<td id="A2.T5.1.1.15.15.4" class="ltx_td ltx_align_center">82.4</td>
<td id="A2.T5.1.1.15.15.5" class="ltx_td ltx_align_center">83.0</td>
</tr>
<tr id="A2.T5.1.1.16.16" class="ltx_tr">
<td id="A2.T5.1.1.16.16.1" class="ltx_td ltx_align_center">ViT H/14</td>
<td id="A2.T5.1.1.16.16.2" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.16.16.3" class="ltx_td ltx_align_center">83.5</td>
<td id="A2.T5.1.1.16.16.4" class="ltx_td ltx_align_center">83.2</td>
<td id="A2.T5.1.1.16.16.5" class="ltx_td ltx_align_center">83.8</td>
</tr>
<tr id="A2.T5.1.1.17.17" class="ltx_tr">
<td id="A2.T5.1.1.17.17.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="A2.T5.1.1.17.17.1.1" class="ltx_text">OpenCLIP</span></td>
<td id="A2.T5.1.1.17.17.2" class="ltx_td ltx_align_center ltx_border_t">ViT B/16</td>
<td id="A2.T5.1.1.17.17.3" class="ltx_td ltx_align_center ltx_border_t">Embedding</td>
<td id="A2.T5.1.1.17.17.4" class="ltx_td ltx_align_center ltx_border_t">87.1</td>
<td id="A2.T5.1.1.17.17.5" class="ltx_td ltx_align_center ltx_border_t">87.8</td>
<td id="A2.T5.1.1.17.17.6" class="ltx_td ltx_align_center ltx_border_t">86.2</td>
</tr>
<tr id="A2.T5.1.1.18.18" class="ltx_tr">
<td id="A2.T5.1.1.18.18.1" class="ltx_td ltx_align_center">ViT B/32</td>
<td id="A2.T5.1.1.18.18.2" class="ltx_td ltx_align_center">Embedding</td>
<td id="A2.T5.1.1.18.18.3" class="ltx_td ltx_align_center">87.6</td>
<td id="A2.T5.1.1.18.18.4" class="ltx_td ltx_align_center">87.5</td>
<td id="A2.T5.1.1.18.18.5" class="ltx_td ltx_align_center">87.6</td>
</tr>
<tr id="A2.T5.1.1.19.19" class="ltx_tr">
<td id="A2.T5.1.1.19.19.1" class="ltx_td ltx_align_center">ViT L/14</td>
<td id="A2.T5.1.1.19.19.2" class="ltx_td ltx_align_center">Embedding</td>
<td id="A2.T5.1.1.19.19.3" class="ltx_td ltx_align_center">85.9</td>
<td id="A2.T5.1.1.19.19.4" class="ltx_td ltx_align_center">86.7</td>
<td id="A2.T5.1.1.19.19.5" class="ltx_td ltx_align_center">84.9</td>
</tr>
<tr id="A2.T5.1.1.20.20" class="ltx_tr">
<td id="A2.T5.1.1.20.20.1" class="ltx_td ltx_align_center ltx_border_t">Ensemble</td>
<td id="A2.T5.1.1.20.20.2" class="ltx_td ltx_align_center ltx_border_t">ViT B/16</td>
<td id="A2.T5.1.1.20.20.3" class="ltx_td ltx_align_center ltx_border_t">Mixed</td>
<td id="A2.T5.1.1.20.20.4" class="ltx_td ltx_align_center ltx_border_t">90.8</td>
<td id="A2.T5.1.1.20.20.5" class="ltx_td ltx_align_center ltx_border_t">91.6</td>
<td id="A2.T5.1.1.20.20.6" class="ltx_td ltx_align_center ltx_border_t">89.8</td>
</tr>
<tr id="A2.T5.1.1.21.21" class="ltx_tr">
<td id="A2.T5.1.1.21.21.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="A2.T5.1.1.21.21.1.1" class="ltx_text">
<span id="A2.T5.1.1.21.21.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="A2.T5.1.1.21.21.1.1.1.1" class="ltx_p"><span id="A2.T5.1.1.21.21.1.1.1.1.1" class="ltx_text ltx_font_bold">Tuned</span></span>
<span id="A2.T5.1.1.21.21.1.1.1.2" class="ltx_p"><span id="A2.T5.1.1.21.21.1.1.1.2.1" class="ltx_text ltx_font_bold">MLP</span></span>
</span></span></td>
<td id="A2.T5.1.1.21.21.2" class="ltx_td ltx_align_center ltx_border_t">CLIP</td>
<td id="A2.T5.1.1.21.21.3" class="ltx_td ltx_align_center ltx_border_t">ViT B/32</td>
<td id="A2.T5.1.1.21.21.4" class="ltx_td ltx_align_center ltx_border_t">Embedding</td>
<td id="A2.T5.1.1.21.21.5" class="ltx_td ltx_align_center ltx_border_t">87.3</td>
<td id="A2.T5.1.1.21.21.6" class="ltx_td ltx_align_center ltx_border_t">88.2</td>
<td id="A2.T5.1.1.21.21.7" class="ltx_td ltx_align_center ltx_border_t">86.2</td>
</tr>
<tr id="A2.T5.1.1.22.22" class="ltx_tr">
<td id="A2.T5.1.1.22.22.1" class="ltx_td ltx_align_center">DINO</td>
<td id="A2.T5.1.1.22.22.2" class="ltx_td ltx_align_center">ViT B/16</td>
<td id="A2.T5.1.1.22.22.3" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.22.22.4" class="ltx_td ltx_align_center">91.2</td>
<td id="A2.T5.1.1.22.22.5" class="ltx_td ltx_align_center">91.8</td>
<td id="A2.T5.1.1.22.22.6" class="ltx_td ltx_align_center">90.3</td>
</tr>
<tr id="A2.T5.1.1.23.23" class="ltx_tr">
<td id="A2.T5.1.1.23.23.1" class="ltx_td ltx_align_center">MAE</td>
<td id="A2.T5.1.1.23.23.2" class="ltx_td ltx_align_center">ViT B/16</td>
<td id="A2.T5.1.1.23.23.3" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.23.23.4" class="ltx_td ltx_align_center">87.6</td>
<td id="A2.T5.1.1.23.23.5" class="ltx_td ltx_align_center">87.3</td>
<td id="A2.T5.1.1.23.23.6" class="ltx_td ltx_align_center">88.1</td>
</tr>
<tr id="A2.T5.1.1.24.24" class="ltx_tr">
<td id="A2.T5.1.1.24.24.1" class="ltx_td ltx_align_center">OpenCLIP</td>
<td id="A2.T5.1.1.24.24.2" class="ltx_td ltx_align_center">ViT B/32</td>
<td id="A2.T5.1.1.24.24.3" class="ltx_td ltx_align_center">Embedding</td>
<td id="A2.T5.1.1.24.24.4" class="ltx_td ltx_align_center">89.9</td>
<td id="A2.T5.1.1.24.24.5" class="ltx_td ltx_align_center">91.0</td>
<td id="A2.T5.1.1.24.24.6" class="ltx_td ltx_align_center">88.5</td>
</tr>
<tr id="A2.T5.1.1.25.25" class="ltx_tr">
<td id="A2.T5.1.1.25.25.1" class="ltx_td ltx_align_center">Ensemble</td>
<td id="A2.T5.1.1.25.25.2" class="ltx_td ltx_align_center">ViT B/16</td>
<td id="A2.T5.1.1.25.25.3" class="ltx_td ltx_align_center">Mixed</td>
<td id="A2.T5.1.1.25.25.4" class="ltx_td ltx_align_center">93.4</td>
<td id="A2.T5.1.1.25.25.5" class="ltx_td ltx_align_center">94.2</td>
<td id="A2.T5.1.1.25.25.6" class="ltx_td ltx_align_center">92.2</td>
</tr>
<tr id="A2.T5.1.1.26.26" class="ltx_tr">
<td id="A2.T5.1.1.26.26.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="5"><span id="A2.T5.1.1.26.26.1.1" class="ltx_text">
<span id="A2.T5.1.1.26.26.1.1.1" class="ltx_inline-block ltx_align_center">
<span id="A2.T5.1.1.26.26.1.1.1.1" class="ltx_p"><span id="A2.T5.1.1.26.26.1.1.1.1.1" class="ltx_text ltx_font_bold">Tuned</span></span>
<span id="A2.T5.1.1.26.26.1.1.1.2" class="ltx_p"><span id="A2.T5.1.1.26.26.1.1.1.2.1" class="ltx_text ltx_font_bold">LoRA</span></span>
</span></span></td>
<td id="A2.T5.1.1.26.26.2" class="ltx_td ltx_align_center ltx_border_t">CLIP</td>
<td id="A2.T5.1.1.26.26.3" class="ltx_td ltx_align_center ltx_border_t">ViT B/32</td>
<td id="A2.T5.1.1.26.26.4" class="ltx_td ltx_align_center ltx_border_t">Embedding</td>
<td id="A2.T5.1.1.26.26.5" class="ltx_td ltx_align_center ltx_border_t">93.9</td>
<td id="A2.T5.1.1.26.26.6" class="ltx_td ltx_align_center ltx_border_t">94.0</td>
<td id="A2.T5.1.1.26.26.7" class="ltx_td ltx_align_center ltx_border_t">93.6</td>
</tr>
<tr id="A2.T5.1.1.27.27" class="ltx_tr">
<td id="A2.T5.1.1.27.27.1" class="ltx_td ltx_align_center">DINO</td>
<td id="A2.T5.1.1.27.27.2" class="ltx_td ltx_align_center">ViT B/16</td>
<td id="A2.T5.1.1.27.27.3" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.27.27.4" class="ltx_td ltx_align_center">94.6</td>
<td id="A2.T5.1.1.27.27.5" class="ltx_td ltx_align_center">94.6</td>
<td id="A2.T5.1.1.27.27.6" class="ltx_td ltx_align_center">94.5</td>
</tr>
<tr id="A2.T5.1.1.28.28" class="ltx_tr">
<td id="A2.T5.1.1.28.28.1" class="ltx_td ltx_align_center">MAE</td>
<td id="A2.T5.1.1.28.28.2" class="ltx_td ltx_align_center">ViT B/16</td>
<td id="A2.T5.1.1.28.28.3" class="ltx_td ltx_align_center">CLS</td>
<td id="A2.T5.1.1.28.28.4" class="ltx_td ltx_align_center">90.5</td>
<td id="A2.T5.1.1.28.28.5" class="ltx_td ltx_align_center">90.6</td>
<td id="A2.T5.1.1.28.28.6" class="ltx_td ltx_align_center">90.3</td>
</tr>
<tr id="A2.T5.1.1.29.29" class="ltx_tr">
<td id="A2.T5.1.1.29.29.1" class="ltx_td ltx_align_center">OpenCLIP</td>
<td id="A2.T5.1.1.29.29.2" class="ltx_td ltx_align_center">ViT B/32</td>
<td id="A2.T5.1.1.29.29.3" class="ltx_td ltx_align_center">Embedding</td>
<td id="A2.T5.1.1.29.29.4" class="ltx_td ltx_align_center">95.5</td>
<td id="A2.T5.1.1.29.29.5" class="ltx_td ltx_align_center">96.5</td>
<td id="A2.T5.1.1.29.29.6" class="ltx_td ltx_align_center">94.1</td>
</tr>
<tr id="A2.T5.1.1.30.30" class="ltx_tr">
<td id="A2.T5.1.1.30.30.1" class="ltx_td ltx_align_center ltx_border_bb">Ensemble</td>
<td id="A2.T5.1.1.30.30.2" class="ltx_td ltx_align_center ltx_border_bb">ViT B/16</td>
<td id="A2.T5.1.1.30.30.3" class="ltx_td ltx_align_center ltx_border_bb">Mixed</td>
<td id="A2.T5.1.1.30.30.4" class="ltx_td ltx_align_center ltx_border_bb">96.2</td>
<td id="A2.T5.1.1.30.30.5" class="ltx_td ltx_align_center ltx_border_bb">96.6</td>
<td id="A2.T5.1.1.30.30.6" class="ltx_td ltx_align_center ltx_border_bb">95.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span><span id="A2.T5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Alignment on NIGHT test set.<span id="A2.T5.3.1.1" class="ltx_text ltx_font_medium"> We evaluate alignment on additional model settings, and separate the test set into ImageNet categories and non-ImageNet categories.</span></span></figcaption>
</figure>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">As some models are adapted from backbones trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> (including the prior learned metrics and DINO), we split our dataset into categories contained in ImageNet and those not in ImageNet, and evaluate alignment on each split. Performance on both splits is highly correlated (Figure <a href="#A2.F13" title="Figure 13 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>), suggesting that the notions of visual similarity are related regardless of whether or not the triplet was generated from an ImageNet category, and whether or not the model was trained only on ImageNet.</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<p id="A2.SS1.p3.1" class="ltx_p">We note that the MAE ViT B/16 checkpoint was taken from Hugging Face transformers repository <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, while the others were taken from the official Facebook repository <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<figure id="A2.F13" class="ltx_figure"><img src="/html/2306.09344/assets/x11.png" id="A2.F13.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span id="A2.F13.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Alignment on ImageNet and non-ImageNet triplets.<span id="A2.F13.2.1.1" class="ltx_text ltx_font_medium"> We split the test set into triplets generated from ImageNet categories and Non-ImageNet categories, as some model backbones are trained only on ImageNet images. For all models, alignment is highly correlated between the two splits.
</span></span></figcaption>
</figure>
<div id="A2.SS1.p4" class="ltx_para ltx_noindent">
<p id="A2.SS1.p4.1" class="ltx_p"><span id="A2.SS1.p4.1.1" class="ltx_text ltx_font_bold">Alignment on Alternative Datasets.</span>
As depicted in the left plot of Figure <a href="#A2.F14" title="Figure 14 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, training on our dataset (with either tuning method) indeed improves BAPPS metric-human alignment in nearly every model, suggesting that some of these patch-based distortions are implicitly still captured in our dataset.
We observe that MAE exhibits the best out-of-the-box performance, indicating a greater sensitivity to lower-level image distortions (e.g. color and shape) than DINO, CLIP or OpenCLIP. Surprisingly however, it is the only model whose performance decreases on BAPPS as it is further tuned. DINO, CLIP and OpenCLIP are not as sensitive to the image distortions in BAPPS, suggesting that before tuning, they are more attuned to higher-level image attributes that the dataset does not capture.</p>
</div>
<figure id="A2.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2306.09344/assets/x12.png" id="A2.F14.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="226" height="147" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2306.09344/assets/x13.png" id="A2.F14.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="226" height="147" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span><span id="A2.F14.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluation on existing low-level and high-level similarity datasets.<span id="A2.F14.2.1.1" class="ltx_text ltx_font_medium"> (Left) Despite never being trained for low-level similarity, LoRA-finetuned models on OpenCLIP, DINO, and Ensemble achieve similar human alignment to LPIPS, which was directly trained on the BAPPS dataset. (Right) The THINGS dataset measures high-level conceptual similarity, rather than appearance similarity. As such, we find that LoRA finetuning on our dataset degrades performance, as our triplets contain appearance similarity, by design.
</span></span></figcaption>
</figure>
<div id="A2.SS1.p5" class="ltx_para">
<p id="A2.SS1.p5.1" class="ltx_p">On THINGS, further training actually <span id="A2.SS1.p5.1.1" class="ltx_text ltx_font_italic">diminishes</span> alignment with humans (see right plot of Figure <a href="#A2.F14" title="Figure 14 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>). CLIP and OpenCLIP’s superior performance on this dataset supports our hypothesis that they are more well-adjusted to higher-level image attributes, which THINGS aims to capture, rather than appearance-level variations.</p>
</div>
<div id="A2.SS1.p6" class="ltx_para">
<p id="A2.SS1.p6.1" class="ltx_p">Our evaluations across these three datasets show that, as we train perceptual metrics that align more closely with human perceptual similarity, we also improve on low-level similarity but perform slightly worse on high-level image distortions. These results suggests that humans, when making an automatic judgment, are more inclined to focus on immediate visual differences (captured in BAPPS and our dataset) rather than the image’s category, context, or related words.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A2.T6" class="ltx_table">
<div id="A2.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:233.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-0.2pt,0.1pt) scale(0.999016598993397,0.999016598993397) ;">
<table id="A2.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T6.1.1.1.1" class="ltx_tr">
<th id="A2.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Metric</span></th>
<th id="A2.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Image Part</span></th>
<th id="A2.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Ours</span></th>
<th id="A2.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.4.1" class="ltx_text ltx_font_bold">DINO</span></th>
<th id="A2.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.5.1" class="ltx_text ltx_font_bold">OpenCLIP</span></th>
<th id="A2.T6.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.6.1" class="ltx_text ltx_font_bold">DISTS</span></th>
<th id="A2.T6.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.1.7.1" class="ltx_text ltx_font_bold">LPIPS</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T6.1.1.2.1" class="ltx_tr">
<td id="A2.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">Color (RGB)</td>
<td id="A2.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Foreground</td>
<td id="A2.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">58.3</td>
<td id="A2.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">53.7</td>
<td id="A2.T6.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">51.2</td>
<td id="A2.T6.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">59.4</td>
<td id="A2.T6.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">60.5</td>
</tr>
<tr id="A2.T6.1.1.3.2" class="ltx_tr">
<td id="A2.T6.1.1.3.2.1" class="ltx_td ltx_align_center">Color (RGB)</td>
<td id="A2.T6.1.1.3.2.2" class="ltx_td ltx_align_center">Background</td>
<td id="A2.T6.1.1.3.2.3" class="ltx_td ltx_align_center">58.7</td>
<td id="A2.T6.1.1.3.2.4" class="ltx_td ltx_align_center">57.2</td>
<td id="A2.T6.1.1.3.2.5" class="ltx_td ltx_align_center">55.5</td>
<td id="A2.T6.1.1.3.2.6" class="ltx_td ltx_align_center">64.1</td>
<td id="A2.T6.1.1.3.2.7" class="ltx_td ltx_align_center">64.7</td>
</tr>
<tr id="A2.T6.1.1.4.3" class="ltx_tr">
<td id="A2.T6.1.1.4.3.1" class="ltx_td ltx_align_center">Color (RGB)</td>
<td id="A2.T6.1.1.4.3.2" class="ltx_td ltx_align_center">Total</td>
<td id="A2.T6.1.1.4.3.3" class="ltx_td ltx_align_center">58.4</td>
<td id="A2.T6.1.1.4.3.4" class="ltx_td ltx_align_center">55.8</td>
<td id="A2.T6.1.1.4.3.5" class="ltx_td ltx_align_center">54.1</td>
<td id="A2.T6.1.1.4.3.6" class="ltx_td ltx_align_center">62.9</td>
<td id="A2.T6.1.1.4.3.7" class="ltx_td ltx_align_center">65.3</td>
</tr>
<tr id="A2.T6.1.1.5.4" class="ltx_tr">
<td id="A2.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t">Luminance</td>
<td id="A2.T6.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">Foreground</td>
<td id="A2.T6.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">54.5</td>
<td id="A2.T6.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">51.9</td>
<td id="A2.T6.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_t">49.9</td>
<td id="A2.T6.1.1.5.4.6" class="ltx_td ltx_align_center ltx_border_t">56.8</td>
<td id="A2.T6.1.1.5.4.7" class="ltx_td ltx_align_center ltx_border_t">55.6</td>
</tr>
<tr id="A2.T6.1.1.6.5" class="ltx_tr">
<td id="A2.T6.1.1.6.5.1" class="ltx_td ltx_align_center">Luminance</td>
<td id="A2.T6.1.1.6.5.2" class="ltx_td ltx_align_center">Background</td>
<td id="A2.T6.1.1.6.5.3" class="ltx_td ltx_align_center">55.5</td>
<td id="A2.T6.1.1.6.5.4" class="ltx_td ltx_align_center">54.3</td>
<td id="A2.T6.1.1.6.5.5" class="ltx_td ltx_align_center">54.1</td>
<td id="A2.T6.1.1.6.5.6" class="ltx_td ltx_align_center">57.7</td>
<td id="A2.T6.1.1.6.5.7" class="ltx_td ltx_align_center">54.4</td>
</tr>
<tr id="A2.T6.1.1.7.6" class="ltx_tr">
<td id="A2.T6.1.1.7.6.1" class="ltx_td ltx_align_center">Luminance</td>
<td id="A2.T6.1.1.7.6.2" class="ltx_td ltx_align_center">Total</td>
<td id="A2.T6.1.1.7.6.3" class="ltx_td ltx_align_center">54.1</td>
<td id="A2.T6.1.1.7.6.4" class="ltx_td ltx_align_center">52.9</td>
<td id="A2.T6.1.1.7.6.5" class="ltx_td ltx_align_center">51.5</td>
<td id="A2.T6.1.1.7.6.6" class="ltx_td ltx_align_center">56.9</td>
<td id="A2.T6.1.1.7.6.7" class="ltx_td ltx_align_center">55.6</td>
</tr>
<tr id="A2.T6.1.1.8.7" class="ltx_tr">
<th id="A2.T6.1.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Depth</th>
<th id="A2.T6.1.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Total</th>
<th id="A2.T6.1.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">54.2</th>
<th id="A2.T6.1.1.8.7.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">53.6</th>
<th id="A2.T6.1.1.8.7.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">53.3</th>
<th id="A2.T6.1.1.8.7.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">54.7</th>
<th id="A2.T6.1.1.8.7.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">55.8</th>
</tr>
<tr id="A2.T6.1.1.9.8" class="ltx_tr">
<td id="A2.T6.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_t">Category Histogram</td>
<td id="A2.T6.1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_t">Things</td>
<td id="A2.T6.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">58.7</td>
<td id="A2.T6.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_t">58.3</td>
<td id="A2.T6.1.1.9.8.5" class="ltx_td ltx_align_center ltx_border_t">55.1</td>
<td id="A2.T6.1.1.9.8.6" class="ltx_td ltx_align_center ltx_border_t">55.3</td>
<td id="A2.T6.1.1.9.8.7" class="ltx_td ltx_align_center ltx_border_t">53.8</td>
</tr>
<tr id="A2.T6.1.1.10.9" class="ltx_tr">
<td id="A2.T6.1.1.10.9.1" class="ltx_td ltx_align_center">Category Histogram</td>
<td id="A2.T6.1.1.10.9.2" class="ltx_td ltx_align_center">Stuff</td>
<td id="A2.T6.1.1.10.9.3" class="ltx_td ltx_align_center">59.5</td>
<td id="A2.T6.1.1.10.9.4" class="ltx_td ltx_align_center">58.8</td>
<td id="A2.T6.1.1.10.9.5" class="ltx_td ltx_align_center">57.9</td>
<td id="A2.T6.1.1.10.9.6" class="ltx_td ltx_align_center">62.5</td>
<td id="A2.T6.1.1.10.9.7" class="ltx_td ltx_align_center">61.3</td>
</tr>
<tr id="A2.T6.1.1.11.10" class="ltx_tr">
<td id="A2.T6.1.1.11.10.1" class="ltx_td ltx_align_center ltx_border_t">Presence of Person</td>
<td id="A2.T6.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="A2.T6.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t">55.3</td>
<td id="A2.T6.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t">52.6</td>
<td id="A2.T6.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t">55.2</td>
<td id="A2.T6.1.1.11.10.6" class="ltx_td ltx_align_center ltx_border_t">51.8</td>
<td id="A2.T6.1.1.11.10.7" class="ltx_td ltx_align_center ltx_border_t">53.8</td>
</tr>
<tr id="A2.T6.1.1.12.11" class="ltx_tr">
<td id="A2.T6.1.1.12.11.1" class="ltx_td ltx_align_center">Presence of Furniture</td>
<td id="A2.T6.1.1.12.11.2" class="ltx_td ltx_align_center">-</td>
<td id="A2.T6.1.1.12.11.3" class="ltx_td ltx_align_center">53.1</td>
<td id="A2.T6.1.1.12.11.4" class="ltx_td ltx_align_center">52.2</td>
<td id="A2.T6.1.1.12.11.5" class="ltx_td ltx_align_center">54.2</td>
<td id="A2.T6.1.1.12.11.6" class="ltx_td ltx_align_center">53.4</td>
<td id="A2.T6.1.1.12.11.7" class="ltx_td ltx_align_center">53.6</td>
</tr>
<tr id="A2.T6.1.1.13.12" class="ltx_tr">
<td id="A2.T6.1.1.13.12.1" class="ltx_td ltx_align_center ltx_border_bb">Presence of Textiles</td>
<td id="A2.T6.1.1.13.12.2" class="ltx_td ltx_align_center ltx_border_bb">-</td>
<td id="A2.T6.1.1.13.12.3" class="ltx_td ltx_align_center ltx_border_bb">52.8</td>
<td id="A2.T6.1.1.13.12.4" class="ltx_td ltx_align_center ltx_border_bb">52.4</td>
<td id="A2.T6.1.1.13.12.5" class="ltx_td ltx_align_center ltx_border_bb">53.1</td>
<td id="A2.T6.1.1.13.12.6" class="ltx_td ltx_align_center ltx_border_bb">51.6</td>
<td id="A2.T6.1.1.13.12.7" class="ltx_td ltx_align_center ltx_border_bb">53.6</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span><span id="A2.T6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Automated Metrics on COCO. <span id="A2.T6.3.1.1" class="ltx_text ltx_font_medium"> Alignment of hand-crafted metrics with model decisions on the COCO dataset, which provides ground-truth semantic labels.
</span></span></figcaption>
</figure>
<figure id="A2.T7" class="ltx_table">
<div id="A2.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:141.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-3.4pt,1.4pt) scale(0.980896469672014,0.980896469672014) ;">
<table id="A2.T7.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T7.1.1.1.1" class="ltx_tr">
<td id="A2.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Metric</span></td>
<td id="A2.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Image Part</span></td>
<td id="A2.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Ours</span></td>
<td id="A2.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">DINO</span></td>
<td id="A2.T7.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.5.1" class="ltx_text ltx_font_bold">OpenCLIP</span></td>
<td id="A2.T7.1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.6.1" class="ltx_text ltx_font_bold">DISTS</span></td>
<td id="A2.T7.1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T7.1.1.1.1.7.1" class="ltx_text ltx_font_bold">LPIPS</span></td>
</tr>
<tr id="A2.T7.1.1.2.2" class="ltx_tr">
<td id="A2.T7.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">Color (RGB)</td>
<td id="A2.T7.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">Foreground</td>
<td id="A2.T7.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">71.7</td>
<td id="A2.T7.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">70.0</td>
<td id="A2.T7.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">69.3</td>
<td id="A2.T7.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">68.7</td>
<td id="A2.T7.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t">60.6</td>
</tr>
<tr id="A2.T7.1.1.3.3" class="ltx_tr">
<td id="A2.T7.1.1.3.3.1" class="ltx_td ltx_align_center">Color (RGB)</td>
<td id="A2.T7.1.1.3.3.2" class="ltx_td ltx_align_center">Background</td>
<td id="A2.T7.1.1.3.3.3" class="ltx_td ltx_align_center">65.4</td>
<td id="A2.T7.1.1.3.3.4" class="ltx_td ltx_align_center">66.2</td>
<td id="A2.T7.1.1.3.3.5" class="ltx_td ltx_align_center">64.7</td>
<td id="A2.T7.1.1.3.3.6" class="ltx_td ltx_align_center">66.4</td>
<td id="A2.T7.1.1.3.3.7" class="ltx_td ltx_align_center">62.0</td>
</tr>
<tr id="A2.T7.1.1.4.4" class="ltx_tr">
<td id="A2.T7.1.1.4.4.1" class="ltx_td ltx_align_center">Color (RGB)</td>
<td id="A2.T7.1.1.4.4.2" class="ltx_td ltx_align_center">Total</td>
<td id="A2.T7.1.1.4.4.3" class="ltx_td ltx_align_center">69.8</td>
<td id="A2.T7.1.1.4.4.4" class="ltx_td ltx_align_center">67.9</td>
<td id="A2.T7.1.1.4.4.5" class="ltx_td ltx_align_center">67.6</td>
<td id="A2.T7.1.1.4.4.6" class="ltx_td ltx_align_center">66.0</td>
<td id="A2.T7.1.1.4.4.7" class="ltx_td ltx_align_center">64.3</td>
</tr>
<tr id="A2.T7.1.1.5.5" class="ltx_tr">
<td id="A2.T7.1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">Luminance</td>
<td id="A2.T7.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">Foreground</td>
<td id="A2.T7.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">63.1</td>
<td id="A2.T7.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">62.6</td>
<td id="A2.T7.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">62.2</td>
<td id="A2.T7.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">61.4</td>
<td id="A2.T7.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">56.1</td>
</tr>
<tr id="A2.T7.1.1.6.6" class="ltx_tr">
<td id="A2.T7.1.1.6.6.1" class="ltx_td ltx_align_center">Luminance</td>
<td id="A2.T7.1.1.6.6.2" class="ltx_td ltx_align_center">Background</td>
<td id="A2.T7.1.1.6.6.3" class="ltx_td ltx_align_center">59.3</td>
<td id="A2.T7.1.1.6.6.4" class="ltx_td ltx_align_center">59.5</td>
<td id="A2.T7.1.1.6.6.5" class="ltx_td ltx_align_center">58.8</td>
<td id="A2.T7.1.1.6.6.6" class="ltx_td ltx_align_center">60.3</td>
<td id="A2.T7.1.1.6.6.7" class="ltx_td ltx_align_center">56.8</td>
</tr>
<tr id="A2.T7.1.1.7.7" class="ltx_tr">
<td id="A2.T7.1.1.7.7.1" class="ltx_td ltx_align_center">Luminance</td>
<td id="A2.T7.1.1.7.7.2" class="ltx_td ltx_align_center">Total</td>
<td id="A2.T7.1.1.7.7.3" class="ltx_td ltx_align_center">59.4</td>
<td id="A2.T7.1.1.7.7.4" class="ltx_td ltx_align_center">60.6</td>
<td id="A2.T7.1.1.7.7.5" class="ltx_td ltx_align_center">59.8</td>
<td id="A2.T7.1.1.7.7.6" class="ltx_td ltx_align_center">59.2</td>
<td id="A2.T7.1.1.7.7.7" class="ltx_td ltx_align_center">56.5</td>
</tr>
<tr id="A2.T7.1.1.8.8" class="ltx_tr">
<td id="A2.T7.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Depth</td>
<td id="A2.T7.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">Total</td>
<td id="A2.T7.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.2</td>
<td id="A2.T7.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">53.6</td>
<td id="A2.T7.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">53.3</td>
<td id="A2.T7.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">54.7</td>
<td id="A2.T7.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span><span id="A2.T7.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Automated Metrics on NIGHTS. <span id="A2.T7.3.1.1" class="ltx_text ltx_font_medium"> Alignment of hand-crafted metrics with model decisions on our dataset. </span></span></figcaption>
</figure>
<div id="A2.SS1.p7" class="ltx_para ltx_noindent">
<p id="A2.SS1.p7.1" class="ltx_p"><span id="A2.SS1.p7.1.1" class="ltx_text ltx_font_bold">Alignment with low-level features.</span>
In Section <a href="#S5.SS2" title="5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> and Figure <a href="#S5.F7" title="Figure 7 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> of the main text we report results on the alignment between our metric, OpenCLIP, DINO, LPIPS, and DISTS with low-level and semantic metrics for the COCO dataset. In Table <a href="#A2.T6" title="Table 6 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we also report additional, fine-grained results for COCO triplets. We use CarveKit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> to segment out the foreground and background of each image, and then break down how well each metric agrees with RGB color histogram similarity, luminance histogram similarity, and depth map distance for foreground, background, and the full image. For color histograms we use 32 bins for each channel, and for luminance histograms we use 10 bins.</p>
</div>
<div id="A2.SS1.p8" class="ltx_para">
<p id="A2.SS1.p8.1" class="ltx_p">We also examine semantic features. For each image, we find the percentage of area that each semantic category occupies, and then compute the alignment between the difference in area for each category and perceptual metrics. Note that when the difference in area is the same for both pairs in a triplet, it is counted as 50% alignment. When the difference in area is smaller for the pair chosen by the perceptual metric, it is counted as 100% alignment (and 0% in the case of disagreement). In Table <a href="#A2.T6" title="Table 6 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> we show the five semantic categories most aligned with our metric. Our metric has a 55% alignment score with the “people” category, though does not align well above chance for other categories.</p>
</div>
<div id="A2.SS1.p9" class="ltx_para">
<p id="A2.SS1.p9.1" class="ltx_p">In Table <a href="#A2.T7" title="Table 7 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> we show alignment with low-level metrics for our dataset (which does not have semantic annotations). On our dataset there is higher alignment with color, luminance, and depth across all metrics, as compared to COCO triplets. This is likely because the images in each of our dataset’s triplets all share the same semantic category, making lower-level features more important than for the randomly-chosen COCO triplets. Our model aligns significantly better with foreground metrics – particularly foreground color – whereas LPIPS aligns slightly better with background.</p>
</div>
<div id="A2.SS1.p10" class="ltx_para ltx_noindent">
<p id="A2.SS1.p10.1" class="ltx_p"><span id="A2.SS1.p10.1.1" class="ltx_text ltx_font_bold">Sketch-photo image retrieval. </span>
To further analyze perceptual metric performance on out-of-domain data, we evaluate sketch-to-photo and photo-to-sketch image retrieval on the Sketchy database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>.
First, we assign sketches as queries and photos as the search space. As seen in Figure <a href="#A2.F16" title="Figure 16 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> (left), DINO, DISTS, and LPIPS perform poorly on this task compared to OpenCLIP and DreamSim because they focus on color similarity, retrieving photos with predominantly white/gray backgrounds. This sometimes appears in DreamSim’s results as well, hindering performance (Figure <a href="#A2.F15" title="Figure 15 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, top-left). Without this failure case, we return impressive results (Fig. <a href="#A2.F15" title="Figure 15 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, top-right) focusing on the subject appearance. We also evaluate retrieval in the other direction, using photos as queries and sketches as the search space. As shown in Figure <a href="#A2.F15" title="Figure 15 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> (bottom), DreamSim outperforms all other metrics in retrieving the main object while remaining sensitive to its size and location in the image.</p>
</div>
<figure id="A2.F15" class="ltx_figure"><img src="/html/2306.09344/assets/x14.png" id="A2.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="352" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span><span id="A2.F15.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sketch-photo nearest-neighbor retrieval.<span id="A2.F15.2.1.1" class="ltx_text ltx_font_medium"> Image retrieval on the Sketchy database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite>, with a sketch as query (top) and photo as query (bottom). OpenCLIP and DreamSim best preserve image content, returning photos of the subject itself (e.g. church, top right) rather than focusing on the black-and-white sketch style in the query. Our metric also best localizes the subject, returning a small boat in the corner while OpenCLIP and DINO both miss this spatial detail (bottom right).</span></span></figcaption>
</figure>
<figure id="A2.F16" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="A2.F16.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:212.5pt;">
<img src="/html/2306.09344/assets/x15.png" id="A2.F16.1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="299" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="A2.F16.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:212.5pt;">
<img src="/html/2306.09344/assets/x16.png" id="A2.F16.2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="299" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span><span id="A2.F16.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Sketch-photo retrieval recall scores. <span id="A2.F16.4.1.1" class="ltx_text ltx_font_medium"> When using real photos as the queries, our model outperforms all methods across # nearest neighbors, obtaining a Recall@10 score of 0.32, compared to the next best model, OpenCLIP, with Recall@10 = 0.20. When using sketches as the queries, we find that our model is competitive with OpenCLIP and outperforms the remaining models. </span></span></figcaption>
</figure>
<div id="A2.SS1.p11" class="ltx_para ltx_noindent">
<p id="A2.SS1.p11.1" class="ltx_p"><span id="A2.SS1.p11.1.1" class="ltx_text ltx_font_bold">Image Quality Assessment (IQA) benchmarks.</span>
We evaluate various metrics on the TID2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and KADID-10k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> image quality assessment (IQA) benchmarks and report results in Table <a href="#A2.T8" title="Table 8 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Following standard IQA literature, we measure the Spearman Rank Order Correlation Coefficient (SROCC) between metric similarities and Mean Opinion Score (MOS). While LPIPS and FSIM remain best-performing overall, DreamSim is still competitive in IQA, outperforming most low-level metrics and base ViT models. Notably, our metric improves SROCC by <math id="A2.SS1.p11.1.m1.1" class="ltx_Math" alttext="\sim 0.05" display="inline"><semantics id="A2.SS1.p11.1.m1.1a"><mrow id="A2.SS1.p11.1.m1.1.1" xref="A2.SS1.p11.1.m1.1.1.cmml"><mi id="A2.SS1.p11.1.m1.1.1.2" xref="A2.SS1.p11.1.m1.1.1.2.cmml"></mi><mo id="A2.SS1.p11.1.m1.1.1.1" xref="A2.SS1.p11.1.m1.1.1.1.cmml">∼</mo><mn id="A2.SS1.p11.1.m1.1.1.3" xref="A2.SS1.p11.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p11.1.m1.1b"><apply id="A2.SS1.p11.1.m1.1.1.cmml" xref="A2.SS1.p11.1.m1.1.1"><csymbol cd="latexml" id="A2.SS1.p11.1.m1.1.1.1.cmml" xref="A2.SS1.p11.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A2.SS1.p11.1.m1.1.1.2.cmml" xref="A2.SS1.p11.1.m1.1.1.2">absent</csymbol><cn type="float" id="A2.SS1.p11.1.m1.1.1.3.cmml" xref="A2.SS1.p11.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p11.1.m1.1c">\sim 0.05</annotation></semantics></math> after training on NIGHTS despite not having seen these low-level distortions. These results are consistent with those in Figure <a href="#A2.F14" title="Figure 14 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, where tuning on NIGHTS consistently improves human alignment on BAPPS.</p>
</div>
<figure id="A2.T8" class="ltx_table">
<div id="A2.T8.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:187.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.5pt,5.1pt) scale(0.948173366536946,0.948173366536946) ;">
<table id="A2.T8.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T8.3.1.1.1" class="ltx_tr">
<th id="A2.T8.3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A2.T8.3.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="A2.T8.3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T8.3.1.1.1.3.1" class="ltx_text ltx_font_bold">TID2013</span></td>
<td id="A2.T8.3.1.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt"><span id="A2.T8.3.1.1.1.4.1" class="ltx_text ltx_font_bold">KADID-10k</span></td>
</tr>
<tr id="A2.T8.3.1.2.2" class="ltx_tr">
<th id="A2.T8.3.1.2.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" rowspan="4"><span id="A2.T8.3.1.2.2.1.1" class="ltx_text ltx_font_bold">Low-level</span></th>
<th id="A2.T8.3.1.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">PSNR*</th>
<td id="A2.T8.3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.687</td>
<td id="A2.T8.3.1.2.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.676</td>
</tr>
<tr id="A2.T8.3.1.3.3" class="ltx_tr">
<th id="A2.T8.3.1.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">SSIM</th>
<td id="A2.T8.3.1.3.3.2" class="ltx_td ltx_align_center">0.720</td>
<td id="A2.T8.3.1.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center">0.724</td>
</tr>
<tr id="A2.T8.3.1.4.4" class="ltx_tr">
<th id="A2.T8.3.1.4.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">MS-SSIM</th>
<td id="A2.T8.3.1.4.4.2" class="ltx_td ltx_align_center">0.798</td>
<td id="A2.T8.3.1.4.4.3" class="ltx_td ltx_nopad_r ltx_align_center">0.802</td>
</tr>
<tr id="A2.T8.3.1.5.5" class="ltx_tr">
<th id="A2.T8.3.1.5.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">FSIM*</th>
<td id="A2.T8.3.1.5.5.2" class="ltx_td ltx_align_center"><span id="A2.T8.3.1.5.5.2.1" class="ltx_text ltx_font_bold">0.851</span></td>
<td id="A2.T8.3.1.5.5.3" class="ltx_td ltx_nopad_r ltx_align_center">0.854</td>
</tr>
<tr id="A2.T8.3.1.6.6" class="ltx_tr">
<th id="A2.T8.3.1.6.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" rowspan="2"><span id="A2.T8.3.1.6.6.1.1" class="ltx_text ltx_font_bold">Prior learned</span></th>
<th id="A2.T8.3.1.6.6.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">LPIPS</th>
<td id="A2.T8.3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T8.3.1.6.6.3.1" class="ltx_text ltx_framed ltx_framed_underline">0.838</span></td>
<td id="A2.T8.3.1.6.6.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><span id="A2.T8.3.1.6.6.4.1" class="ltx_text ltx_font_bold">0.883</span></td>
</tr>
<tr id="A2.T8.3.1.7.7" class="ltx_tr">
<th id="A2.T8.3.1.7.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">DISTS</th>
<td id="A2.T8.3.1.7.7.2" class="ltx_td ltx_align_center">0.794</td>
<td id="A2.T8.3.1.7.7.3" class="ltx_td ltx_nopad_r ltx_align_center">0.867</td>
</tr>
<tr id="A2.T8.3.1.8.8" class="ltx_tr">
<th id="A2.T8.3.1.8.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" rowspan="3"><span id="A2.T8.3.1.8.8.1.1" class="ltx_text ltx_font_bold">Base models</span></th>
<th id="A2.T8.3.1.8.8.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">DINO</th>
<td id="A2.T8.3.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t">0.722</td>
<td id="A2.T8.3.1.8.8.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">0.753</td>
</tr>
<tr id="A2.T8.3.1.9.9" class="ltx_tr">
<th id="A2.T8.3.1.9.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">OpenCLIP</th>
<td id="A2.T8.3.1.9.9.2" class="ltx_td ltx_align_center">0.764</td>
<td id="A2.T8.3.1.9.9.3" class="ltx_td ltx_nopad_r ltx_align_center">0.841</td>
</tr>
<tr id="A2.T8.3.1.10.10" class="ltx_tr">
<th id="A2.T8.3.1.10.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">Ensemble</th>
<td id="A2.T8.3.1.10.10.2" class="ltx_td ltx_align_center">0.757</td>
<td id="A2.T8.3.1.10.10.3" class="ltx_td ltx_nopad_r ltx_align_center">0.812</td>
</tr>
<tr id="A2.T8.3.1.11.11" class="ltx_tr">
<th id="A2.T8.3.1.11.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A2.T8.3.1.11.11.1.1" class="ltx_text ltx_font_bold">Ours</span></th>
<th id="A2.T8.3.1.11.11.2" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t">DreamSim</th>
<td id="A2.T8.3.1.11.11.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">0.814</td>
<td id="A2.T8.3.1.11.11.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T8.3.1.11.11.4.1" class="ltx_text ltx_framed ltx_framed_underline">0.868</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span><span id="A2.T8.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Evaluation on IQA datasets. <span id="A2.T8.2.1.1" class="ltx_text ltx_font_medium"> Alignment of various models on TID2013 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite> and KADID-10k <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> While LPIPS and FSIM are best-performing overall, DreamSim is still competitive in IQA despite not having been tuned on low-level distortions. <sup id="A2.T8.2.1.1.1" class="ltx_sup">∗</sup>While in some IQA literature PSNR/FSIM refer to computation on the luminance channel and PSNRc/FSIMc on color channels, we keep the metric names as-is and compute on color channels to be consistent with our PSNR labels elsewhere in the paper. </span></span></figcaption>
</figure>
<div id="A2.SS1.p12" class="ltx_para ltx_noindent">
<p id="A2.SS1.p12.1" class="ltx_p"><span id="A2.SS1.p12.1.1" class="ltx_text ltx_font_bold ltx_font_italic">k<span id="A2.SS1.p12.1.1.1" class="ltx_text ltx_font_upright">-Nearest Neighbors Classification.</span></span>
We evaluate our model as a <span id="A2.SS1.p12.1.2" class="ltx_text ltx_font_italic">k</span>-Nearest Neighbor (k-NN) classifier on the ObjectNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and ImageNet100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> datasets, both standard classification benchmarks. Results are shown in Figure <a href="#A2.F17" title="Figure 17 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>. Similarly to our other applications, we also evaluate OpenCLIP, DINO, and LPIPS as baselines (we were unable to evaluate DISTS due to computational constraints).</p>
</div>
<div id="A2.SS1.p13" class="ltx_para">
<p id="A2.SS1.p13.1" class="ltx_p">For ImageNet100 (a 100-category subset of ImageNet), we evaluate on the validation set and use the training set as the candidate search space. We note that DINO was trained using self-supervision on ImageNet, and has been previously shown to achieve strong k-NN results on ImageNet. DINO performs best, however our model still achieves competitive performance. For ObjectNet, we use an 80:20 split to randomly partition the dataset into a search-space and test set. Our model outperforms both OpenCLIP and DINO on top-1 classification across different values of <math id="A2.SS1.p13.1.m1.7" class="ltx_Math" alttext="k\in\{1,3,5,11,21,101,201\}" display="inline"><semantics id="A2.SS1.p13.1.m1.7a"><mrow id="A2.SS1.p13.1.m1.7.8" xref="A2.SS1.p13.1.m1.7.8.cmml"><mi id="A2.SS1.p13.1.m1.7.8.2" xref="A2.SS1.p13.1.m1.7.8.2.cmml">k</mi><mo id="A2.SS1.p13.1.m1.7.8.1" xref="A2.SS1.p13.1.m1.7.8.1.cmml">∈</mo><mrow id="A2.SS1.p13.1.m1.7.8.3.2" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml"><mo stretchy="false" id="A2.SS1.p13.1.m1.7.8.3.2.1" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">{</mo><mn id="A2.SS1.p13.1.m1.1.1" xref="A2.SS1.p13.1.m1.1.1.cmml">1</mn><mo id="A2.SS1.p13.1.m1.7.8.3.2.2" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">,</mo><mn id="A2.SS1.p13.1.m1.2.2" xref="A2.SS1.p13.1.m1.2.2.cmml">3</mn><mo id="A2.SS1.p13.1.m1.7.8.3.2.3" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">,</mo><mn id="A2.SS1.p13.1.m1.3.3" xref="A2.SS1.p13.1.m1.3.3.cmml">5</mn><mo id="A2.SS1.p13.1.m1.7.8.3.2.4" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">,</mo><mn id="A2.SS1.p13.1.m1.4.4" xref="A2.SS1.p13.1.m1.4.4.cmml">11</mn><mo id="A2.SS1.p13.1.m1.7.8.3.2.5" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">,</mo><mn id="A2.SS1.p13.1.m1.5.5" xref="A2.SS1.p13.1.m1.5.5.cmml">21</mn><mo id="A2.SS1.p13.1.m1.7.8.3.2.6" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">,</mo><mn id="A2.SS1.p13.1.m1.6.6" xref="A2.SS1.p13.1.m1.6.6.cmml">101</mn><mo id="A2.SS1.p13.1.m1.7.8.3.2.7" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">,</mo><mn id="A2.SS1.p13.1.m1.7.7" xref="A2.SS1.p13.1.m1.7.7.cmml">201</mn><mo stretchy="false" id="A2.SS1.p13.1.m1.7.8.3.2.8" xref="A2.SS1.p13.1.m1.7.8.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p13.1.m1.7b"><apply id="A2.SS1.p13.1.m1.7.8.cmml" xref="A2.SS1.p13.1.m1.7.8"><in id="A2.SS1.p13.1.m1.7.8.1.cmml" xref="A2.SS1.p13.1.m1.7.8.1"></in><ci id="A2.SS1.p13.1.m1.7.8.2.cmml" xref="A2.SS1.p13.1.m1.7.8.2">𝑘</ci><set id="A2.SS1.p13.1.m1.7.8.3.1.cmml" xref="A2.SS1.p13.1.m1.7.8.3.2"><cn type="integer" id="A2.SS1.p13.1.m1.1.1.cmml" xref="A2.SS1.p13.1.m1.1.1">1</cn><cn type="integer" id="A2.SS1.p13.1.m1.2.2.cmml" xref="A2.SS1.p13.1.m1.2.2">3</cn><cn type="integer" id="A2.SS1.p13.1.m1.3.3.cmml" xref="A2.SS1.p13.1.m1.3.3">5</cn><cn type="integer" id="A2.SS1.p13.1.m1.4.4.cmml" xref="A2.SS1.p13.1.m1.4.4">11</cn><cn type="integer" id="A2.SS1.p13.1.m1.5.5.cmml" xref="A2.SS1.p13.1.m1.5.5">21</cn><cn type="integer" id="A2.SS1.p13.1.m1.6.6.cmml" xref="A2.SS1.p13.1.m1.6.6">101</cn><cn type="integer" id="A2.SS1.p13.1.m1.7.7.cmml" xref="A2.SS1.p13.1.m1.7.7">201</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p13.1.m1.7c">k\in\{1,3,5,11,21,101,201\}</annotation></semantics></math>.</p>
</div>
<div id="A2.SS1.p14" class="ltx_para">
<p id="A2.SS1.p14.1" class="ltx_p">A common failure case for all three models is when the retrieved neighbors are visually similar to the query image but of a different category. We also note that ObjectNet is designed to differ from the ImageNet distribution, indicating that DreamSim may generalize well across different image distributions.</p>
</div>
<figure id="A2.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="A2.F17.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:203.8pt;">
<img src="/html/2306.09344/assets/x17.png" id="A2.F17.1.g1" class="ltx_graphics ltx_img_square" width="415" height="344" alt="Refer to caption">
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="A2.F17.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:212.5pt;">
<img src="/html/2306.09344/assets/x18.png" id="A2.F17.2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="330" alt="Refer to caption">
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span><span id="A2.F17.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">k-NN classification results.<span id="A2.F17.4.1.1" class="ltx_text ltx_font_medium"> When evaluated as a k-NN classifier on the ObjectNet dataset, our model outperforms all methods across different values of <math id="A2.F17.4.1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.F17.4.1.1.m1.1b"><mi id="A2.F17.4.1.1.m1.1.1" xref="A2.F17.4.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.F17.4.1.1.m1.1c"><ci id="A2.F17.4.1.1.m1.1.1.cmml" xref="A2.F17.4.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.F17.4.1.1.m1.1d">k</annotation></semantics></math>, with a maximum accuracy of 30.92%; the next best method, OpenCLIP, achieves 22.48%. When evaluating on ImageNet100, our model is competitive (1-2% difference) with DINO and outperforms other baselines. </span></span></figcaption>
</figure>
<div id="A2.SS1.p15" class="ltx_para ltx_noindent">
<p id="A2.SS1.p15.1" class="ltx_p"><span id="A2.SS1.p15.1.1" class="ltx_text ltx_font_bold">Dimensionality reduction with PCA.</span>
Our model consists of the concatenation of the DINO, OpenCLIP, and CLIP backbones, and therefore uses a higher-dimensional feature space to compute similarity compared to each of these models independently. To investigate whether the increased dimensionality is critical for improving human alignment, we ablate feature dimensions by applying PCA, taking a certain number of the top components, as seen in Figure <a href="#A2.F18" title="Figure 18 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a> and Table <a href="#A2.T9" title="Table 9 ‣ Figure 18 ‣ B.1 Additional Evaluations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We can achieve comparable performance using just 500 of the top components, similar to the 512 dimensions of the CLIP and OpenCLIP embedding outputs, suggesting that the improved alignment is not just due to the higher-dimensional feature space used to compute similarity, but rather the additional capacity and model priors obtained from ensembling different models.</p>
</div>
<figure id="A2.F18" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F18.1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:212.5pt;"><img src="/html/2306.09344/assets/x19.png" id="A2.F18.1.g1" class="ltx_graphics ltx_img_landscape" width="368" height="246" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span><span id="A2.F18.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Ablating Feature Dimension.<span id="A2.F18.1.2.1.1" class="ltx_text ltx_font_medium"> We apply a PCA decomposition to the output features of our model and vary the number of dimensions kept.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.F18.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom" style="width:212.5pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="A2.F18.fig1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.F18.fig1.1.1.1" class="ltx_tr">
<th id="A2.F18.fig1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.F18.fig1.1.1.1.1.1" class="ltx_text ltx_font_bold"># PCA Components</span></th>
<th id="A2.F18.fig1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.F18.fig1.1.1.1.2.1" class="ltx_text ltx_font_bold">2AFC Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.F18.fig1.1.2.1" class="ltx_tr">
<th id="A2.F18.fig1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="A2.F18.fig1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">63.4</td>
</tr>
<tr id="A2.F18.fig1.1.3.2" class="ltx_tr">
<th id="A2.F18.fig1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">512</th>
<td id="A2.F18.fig1.1.3.2.2" class="ltx_td ltx_align_center">95.7</td>
</tr>
<tr id="A2.F18.fig1.1.4.3" class="ltx_tr">
<th id="A2.F18.fig1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row">768</th>
<td id="A2.F18.fig1.1.4.3.2" class="ltx_td ltx_align_center">96.1</td>
</tr>
<tr id="A2.F18.fig1.1.5.4" class="ltx_tr">
<th id="A2.F18.fig1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">1792</th>
<td id="A2.F18.fig1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">96.2</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.T9" class="ltx_table ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span><span id="A2.T9.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Feature PCA Decomposition.<span id="A2.T9.2.1.1" class="ltx_text ltx_font_medium"> We list 2AFC scores as a function of the number of PCA components kept, beating both the CLIP/OpenCLIP dimensionality (512) and DINO (768). </span></span></figcaption>
</figure>
</div>
</div>
</figure>
</div>
</div>
</figure>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Additional Visualizations</h3>

<div id="A2.SS2.p1" class="ltx_para ltx_noindent">
<p id="A2.SS2.p1.1" class="ltx_p"><span id="A2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Qualitative metric comparisons.</span>
In Section <a href="#S5.SS2" title="5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a> and Figure <a href="#S5.F6" title="Figure 6 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>–<a href="#S5.F7" title="Figure 7 ‣ 5.2 What image attributes affect similarity decisions? ‣ 5 Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> of the main text we quantitatively analyze the differences between metrics. Here, we also provide a qualitative analysis by comparing image pairs that achieve high and low similarity scores for each metric.</p>
</div>
<figure id="A2.F19" class="ltx_figure"><img src="/html/2306.09344/assets/x20.png" id="A2.F19.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span><span id="A2.F19.2.1" class="ltx_text ltx_font_bold">Correlation between metric scores.</span> For image pairs from our dataset (above), and the COCO dataset (below), we plot similarity scores from our metric against similarity scores from DINO, OpenCLIP, LPIPS, and DISTS. Our metric’s scores are most correlated with other ViT-based metrics, and correlate better with DISTS than LPIPS.</figcaption>
</figure>
<figure id="A2.F20" class="ltx_figure"><img src="/html/2306.09344/assets/x21.png" id="A2.F20.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="447" height="251" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span><span id="A2.F20.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualizing differences between our model and other metrics for NIGHTS.<span id="A2.F20.2.1.1" class="ltx_text ltx_font_medium"> We show the examples where our metric most agrees and disagrees with other metrics. Primary differences are that our metric is more sensitive to major changes in pose, semantics, and color. It is less sensitive to granular changes in structure when overall appearance is preserved, such as the honeycomb example in the DISTS quadrant and the forest example in the DINO quadrant.</span></span></figcaption>
</figure>
<figure id="A2.F21" class="ltx_figure"><img src="/html/2306.09344/assets/x22.png" id="A2.F21.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="447" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span><span id="A2.F21.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualizing differences between our model and other metrics for COCO.<span id="A2.F21.2.1.1" class="ltx_text ltx_font_medium"> We show examples where our metric most agrees and disagrees with other metrics for pairs drawn from the COCO dataset. These pairs share fewer appearance similarities than pairs drawn from our dataset. Our metric seems particularly sensitive to foreground semantic similarities, such as the horse pair in in the OpenCLIP quadrant and the snowboarders in the LPIPS quadrant.</span></span></figcaption>
</figure>
<div id="A2.SS2.p2" class="ltx_para">
<p id="A2.SS2.p2.1" class="ltx_p">In Figure <a href="#A2.F19" title="Figure 19 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">19</span></a> we plot, for each image pair in our dataset, the similarity scores from our metric against similarity scores from DINO, OpenCLIP, DISTS, and LPIPS. We show the same for image pairs drawn from the COCO dataset. In Figure <a href="#A2.F20" title="Figure 20 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a> and Figure <a href="#A2.F21" title="Figure 21 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a> we show the pairs where our metric most agrees and most disagrees with DINO, OpenCLIP, DISTS, and LPIPS, for our test set and the COCO dataset. Note that for COCO we draw random pairs of images that share at least instance category so that pairs have some semantic commonality, enabling better visualization of qualitative differences between metrics.</p>
</div>
<div id="A2.SS2.p3" class="ltx_para">
<p id="A2.SS2.p3.1" class="ltx_p">Comparing DISTS, the top-performing prior learned similarity metric, to our model, we find that DISTS is sensitive to structural changes despite similar overall appearance (such as the width of the honeycomb or the position of similar objects), while our model rates these pairs as nearby. On the other hand, pairs that are far in our feature space but close in DISTS feature space have less appearance similarity (<span id="A2.SS2.p3.1.1" class="ltx_text ltx_font_italic">e</span>.<span id="A2.SS2.p3.1.2" class="ltx_text ltx_font_italic">g</span>. the houses and rooms of different colors). Comparing to deep ViT features (DINO, OpenCLIP) our model is more likely to rate pairs with similar foreground color/appearance as similar, and less likely for pairs that are similar semantically but not appearance-wise. For COCO pairs, where there are fewer appearance similarities than in our dataset, our model chooses pairs that are similar semantically first, and only then appearance-wise.</p>
</div>
<div id="A2.SS2.p4" class="ltx_para ltx_noindent">
<p id="A2.SS2.p4.1" class="ltx_p"><span id="A2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Attention Map Visualizations.</span>
As an alternate way of understanding our model’s similarity decisions, we visualize the transformer attention maps following Chefer et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Our model consists of finetuned versions of DINO, CLIP, and OpenCLIP backbones, and the resulting attention map places the largest activations on the finetuned DINO backbone (Figure <a href="#A2.F22" title="Figure 22 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a> (left)). Accordingly, the overall attention map looks largely similar to the attention map constructed from only the finetuned DINO branch. Compared to the pretrained versions of each model backbone, the finetuned model better captures full object identity (such as over the entire body of the fish), while also minimizing spurious attention values in the background (Figure <a href="#A2.F22" title="Figure 22 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a> (right)). Consistent with earlier analysis, this supports the fact that the foreground plays a larger role in the DreamSim similarily judgment than the background.</p>
</div>
<figure id="A2.F22" class="ltx_figure"><img src="/html/2306.09344/assets/x23.png" id="A2.F22.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="103" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span><span id="A2.F22.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Visualizing attention maps.<span id="A2.F22.2.1.1" class="ltx_text ltx_font_medium"> The finetuned DINO branch of our model has the largest contribution in the attention map <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, with the largest median activation compared to the CLIP and OpenCLIP branches (computed over 400 test set images). As such, in our model, the overall attention map is similar to the attention map from only the DINO branch. Compared to the pretrained backbones, our model better captures the entire object of interest (the fish body) while reducing spurious attention in background regions.</span></span></figcaption>
</figure>
<figure id="A2.F23" class="ltx_figure"><img src="/html/2306.09344/assets/x24.png" id="A2.F23.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span><span id="A2.F23.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Comparing our model and baseline attention maps from the DINO branch.<span id="A2.F23.2.1.1" class="ltx_text ltx_font_medium"> Focusing on the DINO branch, which has the largest contribution in our model’s attention map, we compare the attention maps of the pretrained and finetuning models. The finetuned attention map in our model better captures the foreground object or relevant regions of interest.
In all examples, the DINO baseline selects the A image, while humans and our model select the B image.
</span></span></figcaption>
</figure>
<div id="A2.SS2.p5" class="ltx_para">
<p id="A2.SS2.p5.1" class="ltx_p">As the DINO model has the largest contribution in the attention maps, Figure <a href="#A2.F23" title="Figure 23 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a> shows additional examples focusing on the difference between the finetuned DINO backbone within our model, and the pretrained DINO backbone prior to finetuning. This visualizes how the DINO backbone changes as it is finetuned on our dataset. Our finetuned model better captures the foreground object, while attention maps from the pretrained DINO backbone may only focus on small portions of the object. In the lobster example (Figure  <a href="#A2.F23" title="Figure 23 ‣ B.2 Additional Visualizations ‣ Appendix B Experiments ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a> (top-right)), our model places attention on the relevant parts of the object, such as the lobster body rather than the claws in the A image as the claws do not appear in the other two images.</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Discussion</h2>

<div id="A3.p1" class="ltx_para ltx_noindent">
<p id="A3.p1.1" class="ltx_p"><span id="A3.p1.1.1" class="ltx_text ltx_font_bold">Broader Impacts and Limitations.</span>
Our model and dataset are developed from pretrained Stable Diffusion, CLIP, OpenCLIP and DINO backbones. As such, our model can inherit and propagate biases existing in these models for decisions on downstream tasks.
Our dataset is generated using prompts that describe a single category, and we manually filter out categories that may generate sensitive content, such as violence or malformed human faces. As a result, the dataset has a large focus on object-centric domains, and content containing humans is considered out-of-domain. The resulting dataset and model does not capture the full range of human similarity judgements, but only the variations that we can capture in our synthetically-generated dataset.</p>
</div>
<div id="A3.p2" class="ltx_para ltx_noindent">
<p id="A3.p2.1" class="ltx_p"><span id="A3.p2.1.1" class="ltx_text ltx_font_bold">Licenses.</span>
The icons for Figure <a href="#S0.F1" title="Figure 1 ‣ DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> as well as the images for the inversion experiments are licensed by Adobe Stock, under the Adobe Stock Standard License, and by Pixabay, under their content license.</p>
</div>
<div id="A3.p3" class="ltx_para ltx_noindent">
<p id="A3.p3.1" class="ltx_p"><span id="A3.p3.1.1" class="ltx_text ltx_font_bold">IRB Disclosure.</span>
We received IRB approvals for our AMT experiments from all of the institutions involved. Accordingly, we took measures to ensure participant anonymity and refrained from showing them potentially offensive content.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.09343" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.09344" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.09344">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.09344" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.09345" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 23:51:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
