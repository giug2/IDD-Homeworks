<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.05173] Making Table Understanding Work in Practice</title><meta property="og:description" content="Understanding the semantics of tables at scale is crucial for tasks like data integration, preparation, and search. Table understanding methods aim at detecting a table’s topic, semantic column types, column relations,…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Making Table Understanding Work in Practice">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Making Table Understanding Work in Practice">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.05173">

<!--Generated on Tue Mar 19 13:58:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Making Table Understanding Work in Practice</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Madelon Hulsebos
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Sigma Computing</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">San Francisco</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:madelon@sigmacomputing.com">madelon@sigmacomputing.com</a>
</span><span id="id4.4.id1" class="ltx_text ltx_affiliation_institution">University of Amsterdam</span><span id="id5.5.id2" class="ltx_text ltx_affiliation_city">Amsterdam</span><span id="id6.6.id3" class="ltx_text ltx_affiliation_country">Netherlands</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sneha Gathani
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_affiliation_institution">Sigma Computing</span><span id="id8.2.id2" class="ltx_text ltx_affiliation_city">San Francisco</span><span id="id9.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sneha@sigmacomputing.com">sneha@sigmacomputing.com</a>
</span><span id="id10.4.id1" class="ltx_text ltx_affiliation_institution">University of Maryland</span><span id="id11.5.id2" class="ltx_text ltx_affiliation_city">College Park</span><span id="id12.6.id3" class="ltx_text ltx_affiliation_country">USA</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">James Gale
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Sigma Computing</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">San Francisco</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jlg@sigmacomputing.com">jlg@sigmacomputing.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Isil Dillig
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id16.1.id1" class="ltx_text ltx_affiliation_institution">University of Texas</span><span id="id17.2.id2" class="ltx_text ltx_affiliation_city">Austin</span><span id="id18.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:isil@cs.utexas.edu">isil@cs.utexas.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paul Groth
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id19.1.id1" class="ltx_text ltx_affiliation_institution">University of Amsterdam</span><span id="id20.2.id2" class="ltx_text ltx_affiliation_city">Amsterdam</span><span id="id21.3.id3" class="ltx_text ltx_affiliation_country">Netherlands</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:p.t.groth@uva.nl">p.t.groth@uva.nl</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Çağatay Demiralp
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id22.1.id1" class="ltx_text ltx_affiliation_institution">Sigma Computing</span><span id="id23.2.id2" class="ltx_text ltx_affiliation_city">San Francisco</span><span id="id24.3.id3" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:cagatay@sigmacomputing.com">cagatay@sigmacomputing.com</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id25.id1" class="ltx_p">Understanding the semantics of tables at scale is crucial for tasks like data integration, preparation, and search. Table understanding methods aim at detecting a table’s topic, semantic column types, column relations, or entities. With the rise of deep learning, powerful models have been developed for these tasks with excellent accuracy on benchmarks. However, we observe that there exists a gap between the performance of these models on these benchmarks and their applicability in practice. In this paper, we address the question: what do we need for these models to work in practice?</p>
<p id="id26.id2" class="ltx_p">We discuss three challenges of deploying table understanding models and propose a framework to address them. These challenges include 1) difficulty in customizing models to specific domains, 2) lack of training data for typical database tables often found in enterprises, and 3) lack of confidence in the inferences made by models. We present <span id="id26.id2.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> which implements this framework for the semantic column type detection task. <span id="id26.id2.2" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> encapsulates a hybrid model trained on GitTables and integrates a lightweight human-in-the-loop approach to customize the model. Lastly, we highlight avenues for future research that further close the gap towards making table understanding effective in practice.</p>
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>none</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>; Chaminade CA; USA</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The data used and generated by enterprise applications today are processed and stored overwhelmingly as relational tables in relational databases, including data warehouses. Table understanding aims at surfacing the semantics of tables in order to improve and help automate many tasks from data integration to data visualization. Recent developments in machine learning (ML), particularly deep representation and transfer learning, suggest that the time might be ripe for augmenting enterprise applications with computational table understanding. Table understanding <cite class="ltx_cite ltx_citemacro_citep">(Pujara
et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> in this context refers to tasks such as topic inference, table annotation, semantic column type detection, entity resolution, relation extraction, and learning contextual semantic representations of table elements.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Surfacing table semantics as metadata and effective representations (e.g., contextual semantic embeddings) can improve data exploration tasks such as search and data discovery in applications <cite class="ltx_cite ltx_citemacro_citep">(Cafarella et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2018</a>)</cite>.
For example, knowledge about semantic column types and inter-column relations is useful for automating data preparation <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2015</a>)</cite> and data visualization <cite class="ltx_cite ltx_citemacro_citep">(Hu
et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Dibia and
Demiralp, <a href="#bib.bib12" title="" class="ltx_ref">2019</a>; Zhou
et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>)</cite>. Knowledge of table schemas and entities, in turn, facilitates the integration of two or more data sources <cite class="ltx_cite ltx_citemacro_citep">(Stonebraker
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite> and can be used to construct data catalogs.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recent work has introduced deep learning models such as Sherlock <cite class="ltx_cite ltx_citemacro_citep">(Hulsebos et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite> and SATO <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite> to detect semantic column types. Using transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2017</a>)</cite> and building on BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, researchers have also proposed pretrained table models such as TURL <cite class="ltx_cite ltx_citemacro_citep">(Deng
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, TaBERT <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>, TaPaS <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al<span class="ltx_text">.</span>, <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>, TABBIE <cite class="ltx_cite ltx_citemacro_citep">(Iida
et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> and TUTA <cite class="ltx_cite ltx_citemacro_citep">(Wang
et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2021a</a>)</cite>
and successfully applied them in various downstream tasks for table understanding. While there is a heightened research interest in table understanding models, little is known about the feasibility of these
models in practice.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Commercial data systems such as Talend <cite class="ltx_cite ltx_citemacro_citep">(Talend, <a href="#bib.bib37" title="" class="ltx_ref">2021a</a>)</cite>, Trifacta <cite class="ltx_cite ltx_citemacro_citep">(Trifacta, <a href="#bib.bib41" title="" class="ltx_ref">2021a</a>)</cite>, and Tableau <cite class="ltx_cite ltx_citemacro_citep">(Tableau, <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>, in general, do not seem to deploy such models in practice.
Instead, they primarily rely on simpler methods like regular expression matching for detecting a limited set of semantic types <cite class="ltx_cite ltx_citemacro_citep">(Trifacta, <a href="#bib.bib42" title="" class="ltx_ref">2021b</a>; Talend, <a href="#bib.bib38" title="" class="ltx_ref">2021b</a>; Google, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>. Tamr <cite class="ltx_cite ltx_citemacro_citep">(Stonebraker et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2013</a>)</cite> made the step towards machine learning but uses
less data-heavy machine learning models due to the unavailability of training data and lack of domain expert time, among other things <cite class="ltx_cite ltx_citemacro_citep">(Stonebraker
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Motivated by the abundance of practical applications of table understanding, we shift our focus to making these models effective in practice. We identify three challenges in deploying table understanding models in practice.
First, early feedback from companies on semantic column type detection models such as Sherlock and SATO, highlighted the desire to customize models for different sets of tables and semantic types specific to domains. Doing so requires new large-scale datasets and semantic types that represent these domains well which often requires efficient labeling procedures and long retraining procedures. Although the pretraining/finetuning paradigm adopted for table understanding models <cite class="ltx_cite ltx_citemacro_citep">(Deng
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite> is meant to relieve these burdens, we observe that these models are not straightforward to finetune and still require a significant set of labeled data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Second, these models are pretrained on tables that poorly resemble typical database tables, as the training data mostly reflects tables found on the Web <cite class="ltx_cite ltx_citemacro_citep">(Langenecker et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Hulsebos
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. Therefore, tuning a pretrained model or retraining a task-specific model towards a representative data distribution and labels still takes many resources.
Lastly, the quality and confidence of the inferences made with these models are typically unstable across labels <cite class="ltx_cite ltx_citemacro_citep">(Hulsebos et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>, which limits the reliability of the model output in practice. Besides reliability, no finetuning procedures are proposed to reflect whether samples are far from the training set <cite class="ltx_cite ltx_citemacro_citep">(Dhamija
et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>. In practice, it is important to accurately reflect out-of-distribution data points to ensure high precision.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Based on these challenges, we propose a framework that provides a foundation for table understanding systems in practice. This framework is principled in enabling lightweight and interactive customization towards the target data domain, generalizing to typical database tables, and ensuring high precision and coverage. We present <span id="S1.p7.1.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span>, a system that implements the proposed framework for the semantic column type detection task but generalizes to other table understanding tasks.
<span id="S1.p7.1.2" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> incorporates a pretrained global model identically deployed across all customers, which combines heuristics with a learned model to establish high precision and semantic type coverage. This base model adapts iteratively to the user’s context based on user feedback. Building on the data programming by demonstration (DPBD) paradigm <cite class="ltx_cite ltx_citemacro_citep">(Evensen
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>, <span id="S1.p7.1.3" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> generates custom training data for the model leveraging the user’s feedback. <span id="S1.p7.1.4" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> is pretrained on GitTables providing representative tables as well as relevant semantic types.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In summary, our contributions are as follows: 1) the identification of three key challenges of table understanding systems in practice; 2) a framework for building table understanding systems; and 3) <span id="S1.p8.1.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span>, an implementation of this framework for semantic column type detection.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Challenges</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Deploying table understanding models introduces many challenges in practical applications. Here, we discuss three challenges deduced from our experience and the literature.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Unknown data domains</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2109.05173/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_img_landscape" width="191" height="67" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Models often fail when data-shift occurs. Data-shift comes in three flavors in practice: (a) covariate-shift, (b) label-shift, and (c) out-of-distribution data. Circles represent sample data points and their colors indicate the labels of the samples. The border of a circle indicates whether the corresponding sample stems from the training set (black) or target set (none).</span></figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">One system does not fit every context. In order to provide responsive and intelligent functionalities, the accuracy of a table understanding system is expected to converge to a user’s context and improve over time. If systems fail to do so, they might lose relevance to a user, leading to potential decreased satisfaction or engagement.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">There are three cases of shift that can occur between the training data and the data in the user context, the inference-time data. The first case is when the distribution of table values present in the training data differs from that of the inference-time
tables. This is also called covariate-shift <cite class="ltx_cite ltx_citemacro_citep">(Kouw and Loog, <a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> (Fig.  <a href="#S2.F1" title="Figure 1 ‣ 2.1. Unknown data domains ‣ 2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>a). For example, in the case of semantic column
type detection, a column with the name “ID” might contain values not previously seen for the semantic type <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_typewriter">ID</span>.
The second case is when table values associated with a certain label in the training dataset correspond to a different label within the user’s context. This is also known as label-shift (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1. Unknown data domains ‣ 2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>b) <cite class="ltx_cite ltx_citemacro_citep">(Rabanser
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>.
For example, a column with predicted semantic type <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">ID</span> might actually correspond to the <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">phone number</span> type within the user’s context. The third is the case when we encounter out-of-distribution data (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1. Unknown data domains ‣ 2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>c). A user might have tables and labels far from the training distribution <cite class="ltx_cite ltx_citemacro_citep">(Rabanser
et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Unrepresentative training data</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">With the availability of large-scale table resources like WebTables <cite class="ltx_cite ltx_citemacro_citep">(WebDataCommons, <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite> the fruits of deep learning were leveraged to build deep learning models for semantic column type detection models <cite class="ltx_cite ltx_citemacro_citep">(Hulsebos et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2019</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2020</a>)</cite>. Succeeding methods extend the pretraining paradigm to develop task-agnostic pretrained table understanding models <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>; Deng
et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2021b</a>)</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Despite the abundance of pretrained table understanding models that have been published, they seem not widely adopted in practice. Business Intelligence systems like Tableau and Google Data Studio, for example, only detect a limited set of semantic types, seemingly with lookup functions or pattern matching <cite class="ltx_cite ltx_citemacro_citep">(Trifacta, <a href="#bib.bib42" title="" class="ltx_ref">2021b</a>; Google, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">We believe that this gap can be attributed to the datasets used to pretrain these models, which mainly represent tables from the Web. Such tables can only partially represent tables found in enterprise databases <cite class="ltx_cite ltx_citemacro_citep">(Hulsebos
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Langenecker et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Vogelsgesang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>. This affects the applicability of concurrent pretrained table models to downstream tasks on typical “offline” databases. To remove this barrier, we need models pretrained on relevant tables, resembling what one encounters in offline databases.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Unreliable inferences</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">A table understanding system is useless to users, and might even do more harm than good if it cannot accurately infer table semantics. For example, a column type detector should be able to accurately predict basic types such as <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_typewriter">date</span> and <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_typewriter">city</span> with high precision as a bare minimum, while more and specific semantics may be a nice-to-have. Balancing this precision with coverage, e.g. the number of labels a model is able to distinguish, is a hard challenge <cite class="ltx_cite ltx_citemacro_citep">(Deng
et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2010</a>)</cite> but finding the optimal operating point is critical to make a table understanding system effective.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Finally, in practice, and especially in applications with varying semantic contexts, we might encounter tables and target labels that are not covered in the training set (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1. Unknown data domains ‣ 2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>c). While high-capacity models exist, they do not extrapolate well and can hardly capture the entire universe. Upon encountering tables and labels that are far from the training data, the system should avoid inferring labels for it. This is mainly critical in exact or user-facing applications such as data integration, visualization recommendation, or data cataloging.
If a system fails in this regard, we risk losing a user’s trust in the system or produce critical errors. The question is, (how) do table understanding models recognize out-of-distribution tables? And how to handle situations where a model fails?</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2109.05173/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="390" height="117" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S2.F2.4.2" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">SigmaTyper<span id="S2.F2.4.2.1" class="ltx_text ltx_font_upright">: tables and semantic types are used to pretrain a global model. The local model adapts towards the user’s context through data programming by demonstration: based on the user’s feedback, labeling functions are inferred and used to generate new training data and function as weak predictors in the local model. The weight of the local model increases over time.</span></span></figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We propose a general framework to addresses these challenges, providing a foundation for implementing table understanding
systems in practice.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Interactive customization</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Table understanding systems should effectively adapt to the user’s context. Earlier information retrieval systems have shown that iteratively learning from interactive feedback is an efficient way to accomplish this <cite class="ltx_cite ltx_citemacro_citep">(Tamine-Lechani et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2010</a>; Joachims and
Radlinski, <a href="#bib.bib22" title="" class="ltx_ref">2007</a>)</cite>. Interactions for this end should be easy to use, taking minimal time and input, to maximize the effectiveness of the feedback cycle. Existing systems often, however, require complicated configurations <cite class="ltx_cite ltx_citemacro_citep">(Talend, <a href="#bib.bib38" title="" class="ltx_ref">2021b</a>; Google, <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In our vision, the user’s feedback, implicit or explicit, should be used to derive rules automatically (e.g. regular expressions or heuristics) following the programming by demonstration (or example) framework <cite class="ltx_cite ltx_citemacro_citep">(Lieberman, <a href="#bib.bib27" title="" class="ltx_ref">2001</a>)</cite>. For example, if we want to match tables describing similar products from different e-commerce platforms, we can extract rules that describe the distribution of prices across products. Such rules can be exploited in many ways to adapt the system to the user’s task and data context. We believe they can, at least, be used to 1) generate customized training data using data programming <cite class="ltx_cite ltx_citemacro_citep">(Ratner
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2016</a>)</cite>, and 2) as weak learners to make new inferences <cite class="ltx_cite ltx_citemacro_citep">(Zhou, <a href="#bib.bib51" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Relevant training data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Models are only as good as the data they are trained on. Unlike large corpora of text extracted from the Web which are shown to be instrumental for pretraining widely used language models <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Brown
et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, pretrained table models have shown less impact in this regard. In fact, the generalizability of models trained towards typical database tables is found to be limited <cite class="ltx_cite ltx_citemacro_citep">(Langenecker et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Hulsebos
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To deploy table understanding models in enterprises, we need training data relevant to a variety of domains. Typical corpora used for pretraining table models include WebTables <cite class="ltx_cite ltx_citemacro_citep">(WebDataCommons, <a href="#bib.bib47" title="" class="ltx_ref">2021</a>)</cite> and WikiTables <cite class="ltx_cite ltx_citemacro_citep">(Bhagavatula
et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2013</a>)</cite>. Tables from the Web are relatively small and homogeneous. Typical database tables, instead, are relatively large and heterogeneous <cite class="ltx_cite ltx_citemacro_citep">(Vogelsgesang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>. Table understanding models should reflect such data distributions and be pretrained on matching corpora and relevant labels.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Hybrid adaptive systems</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Systems should ensure high precision and high coverage, be assessed accordingly, and ensure reliability. We believe that systems should be hybrid, combining heuristics that are often pragmatic, fast, and transparent (e.g., easier to explain), with learned models that offer high capacity, flexibility, and semantic coverage.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Beyond accuracy and coverage, systems should learn to reflect their confidence about out-of-distribution samples by being trained specifically on these scenarios. Finally, the underlying pretrained models should adapt to new domains, and do so with minimal iterations.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>SigmaTyper</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span>, a system that implements the proposed framework for semantic column type detection
(annotation), which is a popular table understanding task. Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.3. Unreliable inferences ‣ 2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the high-level system architecture of our system.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Input data and labels</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As traditional large-scale table corpora do not extend well to database tables <cite class="ltx_cite ltx_citemacro_citep">(Langenecker et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Hulsebos
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>, newer data sources <cite class="ltx_cite ltx_citemacro_citep">(Herzig et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>; Hulsebos
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Vogelsgesang et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite> aim at capturing more database-like tables. Since <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> is intended to operate on enterprise tables, we use the GitTables <cite class="ltx_cite ltx_citemacro_citep">(Hulsebos
et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> corpus to train it. GitTables has been recently introduced to address the need for database tables to train models for enterprise
applications.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Predictions of a learned model should be relevant for the majority of its intended users and their application domains. For semantic column type detection, this implies that we need semantic types common in the enterprise, science, and medical domains, and beyond. Tables in GitTables come along with over 500 semantic types from DBpedia <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2007</a>)</cite> and Schema.org <cite class="ltx_cite ltx_citemacro_citep">(Guha
et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2016</a>)</cite>. We select the semantic types from the DBpedia ontology, given its broad semantic coverage and easy integration with the DBpedia Knowledge Base.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2109.05173/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="297" height="84" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S4.F3.7.2" class="ltx_text" style="font-size:90%;">Data pogramming by demonstration implementation of <span id="S4.F3.7.2.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span>. ① Based on the user’s correction of “Income” to type <span id="S4.F3.7.2.2" class="ltx_text ltx_font_typewriter">salary</span>, ② <span id="S4.F3.7.2.3" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> infers labeling functions (LFs) from the column using e.g. data profilers, and ③ uses the LFs to extract customized training data from the source corpus, ④ into customized weakly labeled training data for semantic type <span id="S4.F3.7.2.4" class="ltx_text ltx_font_typewriter">salary</span>.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>System adaptation by DPBD</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To stay useful, systems incorporating learned models should adapt to the user’s specific semantic context, without imposing time-consuming work on the user. Hand-labeling vast amounts of data to retrain machine learning models, especially deep learning models, is infeasible given budget constraints <cite class="ltx_cite ltx_citemacro_citep">(Stonebraker
et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite>. We build on the data programming by demonstration framework (DPBD) <cite class="ltx_cite ltx_citemacro_citep">(Evensen
et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite> to handle these challenges. That is, we infer labeling functions automatically from a table based on the user’s feedback, which may be implicit or explicit, and use these functions to generate new training data.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Considering the example in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1. Input data and labels ‣ 4. SigmaTyper ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the system yields initial column type predictions for each column in this table. The second column, labeled with type <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">revenue</span> is found to be incorrect by the user. The user explicitly relabels this column, originally named “Income”, with new type <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">salary</span>. Labeling functions are then inferred for the new type <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_typewriter">salary</span> and used to generate training data so that the model can be tuned to recognize the type in the future <cite class="ltx_cite ltx_citemacro_citep">(Ratner
et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2016</a>)</cite>. While the interface allows explicit approval of the type predictions, the user does not give this explicit feedback for the remaining columns but leaves those type predictions as-is and conducts an analysis on the table. The system automatically interprets the other types to be implicitly approved. The entire table with its labels is then added to the training data along with the newly generated training data.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> infers labeling functions from columns with textual and numeric data types. For numeric columns, it captures statistics of the data distribution using a data profiler, currently Great Expectations <cite class="ltx_cite ltx_citemacro_citep">(Superconductive, <a href="#bib.bib34" title="" class="ltx_ref">2020</a>)</cite>, and extracts textual features from textual columns, e.g. the most frequent values and the number of unique values. For both type of columns, <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> infers functions to indicate co-occurring columns based on the other detected types, e.g. the types of neighboring columns as in Figure <a href="#S4.F3" title="Figure 3 ‣ 4.1. Input data and labels ‣ 4. SigmaTyper ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.2" class="ltx_p">To accommodate customization without occluding the model for other customers, <span id="S4.SS2.p4.2.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> consists of a “global” and multiple “local” models, one for each customer. The newly generated training data is only used to adapt the local model. The influence of the global and local models on the final prediction is captured in weight vectors representing the influence of each model per type, i.e. <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\textbf{W}_{g}" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><msub id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS2.p4.1.m1.1.1.2" xref="S4.SS2.p4.1.m1.1.1.2a.cmml">W</mtext><mi id="S4.SS2.p4.1.m1.1.1.3" xref="S4.SS2.p4.1.m1.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><apply id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.1.m1.1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p4.1.m1.1.1.2a.cmml" xref="S4.SS2.p4.1.m1.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.SS2.p4.1.m1.1.1.2.cmml" xref="S4.SS2.p4.1.m1.1.1.2">W</mtext></ci><ci id="S4.SS2.p4.1.m1.1.1.3.cmml" xref="S4.SS2.p4.1.m1.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\textbf{W}_{g}</annotation></semantics></math> for the global model and <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="\textbf{W}_{l}" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><msub id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mtext class="ltx_mathvariant_bold" id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2a.cmml">W</mtext><mi id="S4.SS2.p4.2.m2.1.1.3" xref="S4.SS2.p4.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p4.2.m2.1.1.2a.cmml" xref="S4.SS2.p4.2.m2.1.1.2"><mtext class="ltx_mathvariant_bold" id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">W</mtext></ci><ci id="S4.SS2.p4.2.m2.1.1.3.cmml" xref="S4.SS2.p4.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">\textbf{W}_{l}</annotation></semantics></math> for the local model. Over time, the influence of the local model on the final prediction increases.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2109.05173/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="187" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Semantic column type detection model of <span id="S4.F4.4.2.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span>. It implements a 3-step pipeline to predict the semantic type of a column based on the column’s header and values along with an embedding of the entire table.</span></figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Semantic type detection pipeline</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p"><span id="S4.SS3.p1.3.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> implements a 3-step pipeline to predict the semantic types of columns based on the table header, column values, and an embedding of the entire table (Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.2. System adaptation by DPBD ‣ 4. SigmaTyper ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
To minimize overhead, each step in the pipeline is executed (potentially for a subset of columns) only if a preset confidence threshold <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">c</annotation></semantics></math> is not met by the prior step. The steps are executed in order of inference time, so that the slowest step is executed at last. Given a table <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mi id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><ci id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">T</annotation></semantics></math> the system yields the top-<math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mi id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><ci id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">k</annotation></semantics></math> semantic types for each column along with their confidence score.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.2" class="ltx_p">As the header of <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mi id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><ci id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">T</annotation></semantics></math> might already correspond to a semantic type, <span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> matches each column name to the labels in the type ontology using syntactic and semantic matching. For syntactic matching, the column header is compared to the semantic types in the ontology using fuzzy matching. If there is a syntactic match, the confidence score is set to the maximum being <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mrow id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml"><mn id="S4.SS3.p2.2.m2.1.1.2" xref="S4.SS3.p2.2.m2.1.1.2.cmml">100</mn><mo id="S4.SS3.p2.2.m2.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><apply id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS3.p2.2.m2.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p2.2.m2.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">100\%</annotation></semantics></math>.For semantic matching, embedding vectors for the column names as well as the target semantic types are computed using FastText <cite class="ltx_cite ltx_citemacro_citep">(Joulin et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2016</a>)</cite> first. Then the cosine similarity between the column names and semantic types are calculated using their respective embedding vectors. <span id="S4.SS3.p2.2.2" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> considers these similarity scores as prediction confidences and returns the target types with the highest confidences are returned as final predictions for each column.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">If the confidence of the header matching step does not surpass a preset threshold for a subset of columns, a lookup step is triggered to label them. This step leverages lookup rules, to match a sample of column values to semantic types from the ontology. These rules consist of 1) the labeling functions from the global model and the local model which are obtained through DBPD (Sec. <a href="#S4.SS2" title="4.2. System adaptation by DPBD ‣ 4. SigmaTyper ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>), 2) the DBpedia Knowledge Base <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2007</a>)</cite>, and 3) a set of regular expressions which might be expanded on user input as well <cite class="ltx_cite ltx_citemacro_citep">(Chen
et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. The fraction of values that matched a type, is returned as the confidence for that type. The types with the highest confidences are returned as final predictions for each column.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">If the confidence of prior steps for any column is low, the pipeline embeds <math id="S4.SS3.p4.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS3.p4.1.m1.1a"><mi id="S4.SS3.p4.1.m1.1.1" xref="S4.SS3.p4.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p4.1.m1.1b"><ci id="S4.SS3.p4.1.m1.1.1.cmml" xref="S4.SS3.p4.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p4.1.m1.1c">T</annotation></semantics></math> using a pretrained TaBERT model <cite class="ltx_cite ltx_citemacro_citep">(Yin
et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2020</a>)</cite>. We trained TaBERT’s parameters towards GitTables and finetuned it to enable semantic column type detection. To detect out-of-distribution samples (Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.1. Unknown data domains ‣ 2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>c) we train the model on a background dataset <cite class="ltx_cite ltx_citemacro_citep">(Reddy
et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> and add the semantic type <span id="S4.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">unknown</span> to match columns of unknown types. The table embedding model in the local model is iteratively finetuned based on the weakly labeled data generated using DPDB.</p>
</div>
<div id="S4.SS3.p5" class="ltx_para">
<p id="S4.SS3.p5.3" class="ltx_p">The final prediction for each column in <math id="S4.SS3.p5.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S4.SS3.p5.1.m1.1a"><mi id="S4.SS3.p5.1.m1.1.1" xref="S4.SS3.p5.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.1.m1.1b"><ci id="S4.SS3.p5.1.m1.1.1.cmml" xref="S4.SS3.p5.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.1.m1.1c">T</annotation></semantics></math> is the soft majority vote based on the concatenated confidence scores from each step. An optimal aggregation function can be learned as well. We infer a parameter <math id="S4.SS3.p5.2.m2.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS3.p5.2.m2.1a"><mi id="S4.SS3.p5.2.m2.1.1" xref="S4.SS3.p5.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.2.m2.1b"><ci id="S4.SS3.p5.2.m2.1.1.cmml" xref="S4.SS3.p5.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.2.m2.1c">\tau</annotation></semantics></math> and threshold predictions that are below <math id="S4.SS3.p5.3.m3.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S4.SS3.p5.3.m3.1a"><mi id="S4.SS3.p5.3.m3.1.1" xref="S4.SS3.p5.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p5.3.m3.1b"><ci id="S4.SS3.p5.3.m3.1.1.cmml" xref="S4.SS3.p5.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p5.3.m3.1c">\tau</annotation></semantics></math> such that the precision of the system is high, hence errors are minimized.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Future work</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Evaluation.</span>
While a first prototype of the system is built, we aim at integrating it into a product over the next weeks. We plan to investigate if and how accurately the system addresses the challenges discussed in section <a href="#S2" title="2. Challenges ‣ Making Table Understanding Work in Practice" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In particular, we are interested in understanding how well and rapidly the system adapts to new data domains, how it generalizes to database tables and semantics in general, and in what scenarios it fails to make correct predictions.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Adaptation from scratch.</span>
It may be necessary to tailor a model to a different table distribution or ontology from scratch. One possible route for adapting to new data distributions is to take inspiration from multilingual models and pretrain a model on samples from both “languages” <cite class="ltx_cite ltx_citemacro_citep">(Lample and
Conneau, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, tables with different data distributions in our case. This model can be fine-tuned towards the downstream task on a set of labeled tables from the source training data, which in turn can be used to infer the labels for the target table distribution. For a different ontology, we could leverage alternative inexpensive data sources, from a different modality, labeled with the desired ontology <cite class="ltx_cite ltx_citemacro_citep">(Suri et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Engaging users to provide feedback.</span>
An ongoing research topic is how users can be engaged with systems to improve the system for collective reasons. Users need to have individual incentives in order to give explicit feedback, or it should be made very easy to give feedback. Preferably, a system does not even need any explicit feedback and learns from implicit feedback directly. We foresee many possibilities in this space and plan to experiment with users to learn what works best.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Deeper analysis of table models.</span>
We also propose a further analysis of the boundaries of what these models can and cannot do to elicit further developments. Prior work reports model performances often measured by predictive accuracy and occasionally analyze on which labels they fail. But we want to structurally answer the question <span id="S5.p4.1.2" class="ltx_text ltx_font_italic">when</span> these models do not work, and <span id="S5.p4.1.3" class="ltx_text ltx_font_italic">why</span>. Analyzing these aspects, in-depth, will help understanding where the weaknesses of these models reside.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Table-specific representations.</span>
To achieve further performance in practice, it seems necessary to move beyond adapting natural language models towards tables. TCN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2021b</a>)</cite> and RPT <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> propose the first steps towards leveraging the structure of tables. TCN deploys a convolutional neural network, while RPT decouples a table into tuples of attribute rows. TCN confirms the value of developing methods to represent tables, as it demonstrates the largest performance lift compared to other methods that build on top of language models like BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin
et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. The latest model contribution, RPT, proposes a representation of row-wise tuples, solely leveraging row-wise relations. We believe these methods are a step in the right direction but in-depth analyses can help guide the next steps.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Semantics from queries.</span>
Getting table understanding models to work in practice does not only impose challenges but also presents opportunities. An important aspect of semantics that prior deep representation learning research for both languages and tables ignored is the action or interactive semantics. A table’s semantics is not only embodied by its values and surrounding context (schema, title, etc.) but also by what people (users) do with the table. An important class of actions on tables is SQL queries run by users on tables. We believe that deploying table understanding models within enterprise applications such as Sigma opens up opportunities to incorporate semantics implied by queries into table understanding models.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">At Sigma Computing, we are building the technical foundation to integrate table understanding models with our product that will improve almost every aspect of user experience. Here we introduce a facet of this vision.
Table understanding models have been demonstrated to yield promising performance for detecting column types, entities, relations, and table topics. We believe the time has come to make these advances accessible to the masses, and explore avenues to make table understanding models work in practice.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">To this end, we discuss the main challenges of deploying these models, being: unknown data domains, unrepresentative training data, and unreliable inferences. We propose a general framework to address these challenges in any table understanding system. We present <span id="S6.p2.1.1" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> which implements this framework. To accomplish model customization, we leverage feedback and use the data programming by demonstration framework to infer labeling functions from data used to generate new training data and prediction functions. <span id="S6.p2.1.2" class="ltx_text ltx_font_smallcaps">SigmaTyper</span> is trained on data common in enterprise data warehouses and implements a hybrid and adaptive model and detects out-of-distribution data.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">The framework introduces many avenues for future research. We are particularly interested in methods to adapt and enrich models based on the user’s explicit feedback and implicit signals. Besides the adaptive capabilities for table understanding models, we need to better understand how to best represent tables and develop models accordingly.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
Sören Auer, Christian
Bizer, Georgi Kobilarov, Jens Lehmann,
Richard Cyganiak, and Zachary Ives.
2007.

</span>
<span class="ltx_bibblock">DBpedia: A nucleus for a web of open data.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">ISWC</em> (2007),
722–735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagavatula
et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Chandra Sekhar Bhagavatula,
Thanapon Noraset, and Doug Downey.
2013.

</span>
<span class="ltx_bibblock">Methods for exploring and mining tables on
wikipedia. In <em id="bib.bib3.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM SIGKDD
workshop on interactive data exploration and analytics</em>.
18–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown
et al<span id="bib.bib4.3.3.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom B Brown, Benjamin
Mann, Nick Ryder, Melanie Subbiah,
Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell,
et al<span id="bib.bib4.4.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.14165</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cafarella et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Michael Cafarella, Alon
Halevy, Hongrae Lee, Jayant Madhavan,
Cong Yu, Daisy Zhe Wang, and
Eugene Wu. 2018.

</span>
<span class="ltx_bibblock">Ten years of webtables.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment</em>
11, 12 (2018),
2140–2149.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen
et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Qiaochu Chen, Xinyu Wang,
Xi Ye, Greg Durrett, and
Isil Dillig. 2020.

</span>
<span class="ltx_bibblock">Multi-modal synthesis of regular expressions. In
<em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 41st ACM SIGPLAN Conference on
Programming Language Design and Implementation</em>. 487–502.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Xu Chu, John Morcos,
Ihab F Ilyas, Mourad Ouzzani,
Paolo Papotti, Nan Tang, and
Yin Ye. 2015.

</span>
<span class="ltx_bibblock">Katara: A data cleaning system powered by knowledge
bases and crowdsourcing. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">Proceedings of the
2015 ACM SIGMOD international conference on management of data</em>.
1247–1261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng
et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Jia Deng, Alexander C
Berg, Kai Li, and Li Fei-Fei.
2010.

</span>
<span class="ltx_bibblock">What does classifying more than 10,000 image
categories tell us?. In <em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">European conference on
computer vision</em>. Springer, 71–84.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng
et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiang Deng, Huan Sun,
Alyssa Lees, You Wu, and
Cong Yu. 2020.

</span>
<span class="ltx_bibblock">TURL: Table Understanding through
Representation Learning.

</span>
<span class="ltx_bibblock">14, 3 (Nov.
2020), 307–319.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin
et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In
<em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1</em>. 4171–4186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamija
et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Akshay Raj Dhamija, Manuel
Günther, and Terrance E Boult.
2018.

</span>
<span class="ltx_bibblock">Reducing network agnostophobia. In
<em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 32nd International Conference on
Neural Information Processing Systems</em>. 9175–9186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dibia and
Demiralp (2019)</span>
<span class="ltx_bibblock">
Victor Dibia and
Çağatay Demiralp.
2019.

</span>
<span class="ltx_bibblock">Data2vis: Automatic generation of data
visualizations using sequence-to-sequence recurrent neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE computer graphics and applications</em>
39, 5 (2019),
33–46.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evensen
et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sara Evensen, Chang Ge,
Dongjin Choi, and Çağatay
Demiralp. 2020.

</span>
<span class="ltx_bibblock">Data Programming by Demonstration: A Framework for
Interactively Learning Labeling Functions.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2009.01444</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google (2021)</span>
<span class="ltx_bibblock">
Google. 2021.

</span>
<span class="ltx_bibblock">Google — Dverview Studio.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://developers.google.com/datastudio/connector/reference#semantictype" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://developers.google.com/datastudio/connector/reference#semantictype</a>

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guha
et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Ramanathan V Guha, Dan
Brickley, and Steve Macbeth.
2016.

</span>
<span class="ltx_bibblock">Schema. org: evolution of structured data on the
web.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">Commun. ACM</em> 59,
2 (2016), 44–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jonathan Herzig, Thomas
Müller, Syrine Krichene, and
Julian Martin Eisenschlos.
2021.

</span>
<span class="ltx_bibblock">Open Domain Question Answering over Tables via
Dense Retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2103.12011</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig et al<span id="bib.bib17.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jonathan Herzig,
Pawel Krzysztof Nowak, Thomas Mueller,
Francesco Piccinno, and Julian
Eisenschlos. 2020.

</span>
<span class="ltx_bibblock">TaPas: Weakly Supervised Table Parsing via
Pre-training. In <em id="bib.bib17.3.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu
et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kevin Hu, Michiel A
Bakker, Stephen Li, Tim Kraska, and
César Hidalgo. 2019.

</span>
<span class="ltx_bibblock">Vizml: A machine learning approach to visualization
recommendation. In <em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI
Conference on Human Factors in Computing Systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hulsebos
et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Madelon Hulsebos,
Çağatay Demiralp, and Paul
Groth. 2021.

</span>
<span class="ltx_bibblock">GitTables: A Large-Scale Corpus of Relational
Tables.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.07258</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hulsebos et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Madelon Hulsebos, Kevin
Hu, Michiel Bakker, Emanuel Zgraggen,
Arvind Satyanarayan, Tim Kraska,
Çagatay Demiralp, and César
Hidalgo. 2019.

</span>
<span class="ltx_bibblock">Sherlock: A deep learning approach to semantic data
type detection. In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>.
1500–1508.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Iida
et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Hiroshi Iida, Dung Thai,
Varun Manjunatha, and Mohit Iyyer.
2021.

</span>
<span class="ltx_bibblock">TABBIE: Pretrained Representations of Tabular
Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.02584</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joachims and
Radlinski (2007)</span>
<span class="ltx_bibblock">
Thorsten Joachims and
Filip Radlinski. 2007.

</span>
<span class="ltx_bibblock">Search engines that learn from implicit feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Computer</em> 40,
8 (2007), 34–40.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joulin et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Armand Joulin, Edouard
Grave, Piotr Bojanowski, Matthijs Douze,
Hérve Jégou, and Tomas
Mikolov. 2016.

</span>
<span class="ltx_bibblock">Fasttext. zip: Compressing text classification
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1612.03651</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kouw and Loog (2018)</span>
<span class="ltx_bibblock">
Wouter M Kouw and Marco
Loog. 2018.

</span>
<span class="ltx_bibblock">An introduction to domain adaptation and transfer
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.11806</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample and
Conneau (2019)</span>
<span class="ltx_bibblock">
Guillaume Lample and
Alexis Conneau. 2019.

</span>
<span class="ltx_bibblock">Cross-lingual language model pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.07291</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Langenecker et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sven Langenecker,
Christoph Sturm, Christian Schalles,
and Carsten Binnig. 2021.

</span>
<span class="ltx_bibblock">Towards Learned Metadata Extraction for Data
Lakes.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">BTW 2021</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lieberman (2001)</span>
<span class="ltx_bibblock">
Henry Lieberman.
2001.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Your wish is my command: Programming by
example</em>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pujara
et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jay Pujara, Pedro
Szekely, Huan Sun, and Muhao Chen.
2021.

</span>
<span class="ltx_bibblock">From Tables to Knowledge: Recent Advances in Table
Understanding. In <em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em> (Virtual Event,
Singapore) <em id="bib.bib28.4.2" class="ltx_emph ltx_font_italic">(KDD ’21)</em>.
Association for Computing Machinery,
4060–4061.

</span>
<span class="ltx_bibblock">

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabanser
et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Stephan Rabanser, Stephan
Günnemann, and Zachary C Lipton.
2018.

</span>
<span class="ltx_bibblock">Failing loudly: An empirical study of methods for
detecting dataset shift.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.11953</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ratner
et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Alexander J Ratner,
Christopher M De Sa, Sen Wu,
Daniel Selsam, and Christopher Ré.
2016.

</span>
<span class="ltx_bibblock">Data programming: Creating large training sets,
quickly.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 29 (2016),
3567–3575.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy
et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Revanth Reddy, Rahul
Ramesh, Ameet Deshpande, and Mitesh M
Khapra. 2019.

</span>
<span class="ltx_bibblock">Figurenet: A deep learning model for
question-answering on scientific plots. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">2019
International Joint Conference on Neural Networks (IJCNN)</em>. IEEE,
1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stonebraker et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Michael Stonebraker,
Daniel Bruckner, Ihab F Ilyas,
George Beskales, Mitch Cherniack,
Stanley B Zdonik, Alexander Pagan, and
Shan Xu. 2013.

</span>
<span class="ltx_bibblock">Data Curation at Scale: The Data Tamer System.. In
<em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">Cidr</em>, Vol. 2013. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stonebraker
et al<span id="bib.bib33.3.3.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Michael Stonebraker,
Ihab F Ilyas, et al<span id="bib.bib33.4.1" class="ltx_text">.</span>
2018.

</span>
<span class="ltx_bibblock">Data Integration: The Current Status and the Way
Forward.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.5.1" class="ltx_emph ltx_font_italic">IEEE Data Eng. Bull.</em> 41,
2 (2018), 3–9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Superconductive (2020)</span>
<span class="ltx_bibblock">
Superconductive.
2020.

</span>
<span class="ltx_bibblock">Great Expectations: Always know what to expect
from your data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://github.com/great-expectations/great_expectations" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/great-expectations/great_expectations</a>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suri et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sahaana Suri, Raghuveer
Chanda, Neslihan Bulut, Pradyumna
Narayana, Yemao Zeng, Peter Bailis,
Sugato Basu, Girija Narlikar,
Christopher Ré, and Abishek Sethi.
2020.

</span>
<span class="ltx_bibblock">Leveraging organizational resources to adapt models
to new data modalities.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.09983</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tableau (2021)</span>
<span class="ltx_bibblock">
Tableau. 2021.

</span>
<span class="ltx_bibblock">Tableau.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.tableau.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tableau.com</a>

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talend (2021a)</span>
<span class="ltx_bibblock">
Talend. 2021a.

</span>
<span class="ltx_bibblock">Talend.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.talend.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.talend.com</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talend (2021b)</span>
<span class="ltx_bibblock">
Talend. 2021b.

</span>
<span class="ltx_bibblock">Talend.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://help.talend.com/r/en-US/7.2/studio-user-guide-real-time-big-data-platform/list-of-indexes-and-regex-categories-used-in-semantic-aware-analysis" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://help.talend.com/r/en-US/7.2/studio-user-guide-real-time-big-data-platform/list-of-indexes-and-regex-categories-used-in-semantic-aware-analysis</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tamine-Lechani et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Lynda Tamine-Lechani,
Mohand Boughanem, and Mariam Daoud.
2010.

</span>
<span class="ltx_bibblock">Evaluation of contextual information retrieval
effectiveness: overview of issues and research.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Knowledge and Information Systems</em>
24, 1 (2010),
1–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Nan Tang, Ju Fan,
Fangyi Li, Jianhong Tu,
Xiaoyong Du, Guoliang Li,
Samuel Madden, and Mourad Ouzzani.
2021.

</span>
<span class="ltx_bibblock">RPT: Relational Pre-trained Transformer Is Almost
All You Need towards Democratizing Data Preparation.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">Proc. VLDB Endow.</em> 14,
8 (2021), 1254–1261.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trifacta (2021a)</span>
<span class="ltx_bibblock">
Trifacta.
2012–2021a.

</span>
<span class="ltx_bibblock">Trifacta.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.trifacta.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.trifacta.com</a>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trifacta (2021b)</span>
<span class="ltx_bibblock">
Trifacta.
2021b.

</span>
<span class="ltx_bibblock">Trifacta — Overview of the Type System.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://docs.trifacta.com/display/DP/Overview+of+the+Type+System" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://docs.trifacta.com/display/DP/Overview+of+the+Type+System</a>

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia
Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need. In
<em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em>. 5998–6008.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vogelsgesang et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Adrian Vogelsgesang,
Michael Haubenschild, Jan Finis,
Alfons Kemper, Viktor Leis,
Tobias Mühlbauer, Thomas Neumann,
and Manuel Then. 2018.

</span>
<span class="ltx_bibblock">Get real: How benchmarks fail to represent the real
world. In <em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Workshop on Testing
Database Systems</em>. 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Daheng Wang, Prashant
Shiralkar, Colin Lockard, Binxuan Huang,
Xin Luna Dong, and Meng Jiang.
2021b.

</span>
<span class="ltx_bibblock">TCN: Table Convolutional Network for Web Table
Interpretation.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.09460</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang
et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Zhiruo Wang, Haoyu Dong,
Ran Jia, Jia Li, Zhiyi
Fu, Shi Han, and Dongmei Zhang.
2021a.

</span>
<span class="ltx_bibblock">TUTA: Tree-based Transformers for Generally
Structured Table Pre-training. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">KDD</em>.
1780–1790.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">WebDataCommons (2021)</span>
<span class="ltx_bibblock">
WebDataCommons.
2021.

</span>
<span class="ltx_bibblock">WDC Web Table Corpus 2012.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="http://webdatacommons.org/webtables/2012/relationalStatistics.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://webdatacommons.org/webtables/2012/relationalStatistics.html</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin
et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Pengcheng Yin, Graham
Neubig, Wen-tau Yih, and Sebastian
Riedel. 2020.

</span>
<span class="ltx_bibblock">TaBERT: Pretraining for Joint Understanding of
Textual and Tabular Data. In <em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">ACL</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Dan Zhang, Yoshihiko
Suhara, Jinfeng Li, Madelon Hulsebos,
Çağatay Demiralp, and
Wang-Chiew Tan. 2020.

</span>
<span class="ltx_bibblock">Sato: contextual semantic type detection in
tables.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the VLDB Endowment</em>
13, 12 (2020),
1835–1848.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou
et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mengyu Zhou, Qingtao Li,
Yuejiang Li, Shi Han, and
Dongmei Zhang. 2020.

</span>
<span class="ltx_bibblock">Table2Charts: Learning Shared Representations for
Recommending Charts on Multi-dimensional Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.11015</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou (2021)</span>
<span class="ltx_bibblock">
Zhi-Hua Zhou.
2021.

</span>
<span class="ltx_bibblock">Ensemble learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Machine Learning</em>.
Springer, 181–210.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.05172" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.05173" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.05173">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.05173" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.05174" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 13:58:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
