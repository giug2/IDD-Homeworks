<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models</title>
<!--Generated on Sat Aug 24 09:19:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.13533v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx1" title="In Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx2" title="In Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx2.SSx1" title="In Related Work ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Retrieval-Augmented Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx2.SSx2" title="In Related Work ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Noise Injection in LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx3" title="In Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">A Taxonomy of RAG Noise</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx4" title="In Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Noise RAG Benchmark Construction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx4.SSx1" title="In Noise RAG Benchmark Construction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Data Construction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx4.SSx2" title="In Noise RAG Benchmark Construction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5" title="In Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx1" title="In Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx1.SSSx1" title="In Experiment Setup ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx1.SSSx2" title="In Experiment Setup ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Baseline Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx1.SSSx3" title="In Experiment Setup ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx2" title="In Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx2.SSSx1" title="In Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">The Role of Diverse RAG Noises</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx2.SSSx2" title="In Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Beneficial Noise Enhances Performance Across Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx2.SSSx3" title="In Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Beneficial Noise Remains Effective Under Other Noise Disturbances</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx2.SSSx4" title="In Beneficial Noise Remains Effective Under Other Noise Disturbances ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Beneficial Noise Is Statistically Significant</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx3" title="In Beneficial Noise Is Statistically Significant ‣ Beneficial Noise Remains Effective Under Other Noise Disturbances ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Exploring the Mechanisms Behind Beneficial Noise</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx3.SSSx1" title="In Exploring the Mechanisms Behind Beneficial Noise ‣ Beneficial Noise Is Statistically Significant ‣ Beneficial Noise Remains Effective Under Other Noise Disturbances ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Case Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.SSx3.SSSx2" title="In Exploring the Mechanisms Behind Beneficial Noise ‣ Beneficial Noise Is Statistically Significant ‣ Beneficial Noise Remains Effective Under Other Noise Disturbances ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Statistical Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx6" title="In Statistical Analysis ‣ Exploring the Mechanisms Behind Beneficial Noise ‣ Beneficial Noise Is Statistically Significant ‣ Beneficial Noise Remains Effective Under Other Noise Disturbances ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_title">Conclusion</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jinyang Wu<sup class="ltx_sup" id="id1.1.id1">1</sup>,
Feihu Che<sup class="ltx_sup" id="id2.2.id2">1</sup>,
Chuyuan Zhang<sup class="ltx_sup" id="id3.3.id3">1</sup>,
Jianhua Tao<sup class="ltx_sup" id="id4.4.id4">1,2,</sup>,
Shuai Zhang<sup class="ltx_sup" id="id5.5.id5">1</sup>,
Pengpeng Shao<sup class="ltx_sup" id="id6.6.id6">1</sup>
</span><span class="ltx_author_notes">Corresponding author</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Retrieval-Augmented Generation (RAG) has emerged as a crucial method for addressing hallucinations in large language models (LLMs). While recent research has extended RAG models to complex noisy scenarios, these explorations often confine themselves to limited noise types and presuppose that noise is inherently detrimental to LLMs, potentially deviating from real-world retrieval environments and restricting practical applicability. In this paper, we define seven distinct noise types from a linguistic perspective and establish a Noise RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing multiple datasets and reasoning tasks. Through empirical evaluation of eight representative LLMs with diverse architectures and scales, we reveal that these noises can be further categorized into two practical groups: noise that is beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs (aka harmful noise). While harmful noise generally impairs performance, beneficial noise may enhance several aspects of model capabilities and overall performance. Our analysis offers insights for developing more robust, adaptable RAG solutions and mitigating hallucinations across diverse retrieval scenarios.</p>
</div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Introduction</h2>
<figure class="ltx_figure" id="Sx1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="699" id="Sx1.F1.g1" src="x1.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example from NoiserBench illustrating effects of different RAG noises. Initially, the model is misled by counterfactual noise. Interestingly, upon introducing beneficial noise, it successfully discriminates between correct and incorrect information and produces the accurate answer ‘D’.</figcaption>
</figure>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Large language models (LLMs) <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib26" title="">2023</a>; Meta, AI <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib24" title="">2024</a>)</cite> have demonstrated remarkable proficiency across various tasks <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib3" title="">2023</a>)</cite>. Despite these impressive capabilities, LLMs face challenges such as reliance on outdated knowledge and hallucination <cite class="ltx_cite ltx_citemacro_citep">(Huang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib13" title="">2023</a>; Kandpal et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib18" title="">2023</a>)</cite>. Retrieval-Augmented Generation (RAG) has recently emerged as a promising approach to mitigate these limitations <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib10" title="">2024</a>)</cite>. RAG enhances LLM performance by augmenting inputs with additional information retrieved from external sources during inference.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">However, the internet is filled with various non-standard noises, including AI-generated fake news, outdated content, spelling errors, and data contamination, which may potentially influence model performance <cite class="ltx_cite ltx_citemacro_citep">(Shi et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib32" title="">2023</a>; Xie et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib43" title="">2024</a>)</cite>. It is crucial to explore how noise affects RAG systems and understand the underlying mechanisms.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Recently, several studies <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib5" title="">2024</a>; Xiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib42" title="">2024</a>)</cite> have attempted to extend RAG systems to complex real-world scenarios, investigating the impact of noisy documents and strategies to enhance the system’s robustness. For example, <cite class="ltx_cite ltx_citemacro_citet">Cuconasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib7" title="">2024</a>)</cite> defines three types of noise in retrieved documents and examines their impacts on LLM. Despite highlighting one noise’s positive effect, the study lacks a comprehensive noise definition and in-depth investigation of underlying principles. <cite class="ltx_cite ltx_citemacro_citet">Fang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib8" title="">2024</a>)</cite> applies adversarial training to dynamically adjust the model’s training process in response to retrieval noises. RobustRAG <cite class="ltx_cite ltx_citemacro_citep">(Xiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib42" title="">2024</a>)</cite> proposes a defense framework to improve the robustness of RAG models against retrieval corruption attacks. Nevertheless, these investigations typically focus on a limited number of noise types (usually no more than three) and lack clear classification, which fails to fully capture the complexity of real-world noise environments. Additionally, these studies often assume that noise is harmful, neglecting its potential positive effects and lacking systematic evaluation datasets. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, introducing beneficial noise allows the LLM to avoid the harmful effects of counterfactual noise, concentrate on the golden context, and produce accurate responses. Thus, there’s an urgent need to redefine and describe noise scenarios in RAG, and systematically explore the specific impacts of retrieval noises.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">In this paper, we conduct a comprehensive analysis to reveal the role of RAG noises in LLMs. We first define seven types of noise from a linguistic perspective. Based on this definition, we propose a systematic framework to create diverse noisy documents and establish NoiserBench, a novel noise RAG benchmark. Then, we evaluate eight representative LLMs with different architectures and scales. Extensive results show that RAG noises can be categorized into two practical groups: <span class="ltx_text ltx_font_italic" id="Sx1.p4.1.1">beneficial noise</span> (semantic, datatype, illegal sentence) and <span class="ltx_text ltx_font_italic" id="Sx1.p4.1.2">harmful noise</span> (counterfactual, supportive, orthographic, prior). While harmful noise impairs performance, beneficial noise surprisingly enhances model capabilities and leads to improved performance. Further analysis reveals that beneficial noise facilitates more standardized answer formats, clearer reasoning paths, and increased confidence in responses with golden context. These contrasting effects are analogous to <span class="ltx_text ltx_font_italic" id="Sx1.p4.1.3">opening Pandora’s Box</span> (harmful noise) versus <span class="ltx_text ltx_font_italic" id="Sx1.p4.1.4">unlocking Aladdin’s Lamp</span> (beneficial noise). We hope this study will advance efforts to mitigate harmful noise and leverage the positive effects of beneficial noise in future research. The main contributions are:</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">We define seven types of noise and categorize them into two groups: beneficial and harmful. This is the first comprehensive study to define and assess RAG noises from both linguistic and practical perspectives.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">We propose a novel framework for constructing diverse retrieval documents and create the noise RAG benchmark (NoiserBench), which effectively simulates the impact of real-world noise on RAG models.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1">Evaluated on eight datasets and representative LLMs, our results reveal that while some RAG noises (e.g. counterfactual) can open Pandora’s Box and cause errors, beneficial noise (e.g. datatype) has the potential to unlock the power of Aladdin’s Lamp and deliver positive effects.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i4.p1">
<p class="ltx_p" id="Sx1.I1.i4.p1.1">Our findings redefine retrieval noise and encourage researchers to explore methods that harness its beneficial properties while addressing its harmful effects.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Related Work</h2>
<section class="ltx_subsection" id="Sx2.SSx1">
<h3 class="ltx_title ltx_title_subsection">Retrieval-Augmented Generation</h3>
<div class="ltx_para" id="Sx2.SSx1.p1">
<p class="ltx_p" id="Sx2.SSx1.p1.1">By integrating external information, RAG methods enhance reasoning and generation process <cite class="ltx_cite ltx_citemacro_citep">(Gao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib10" title="">2024</a>; Zhao et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib49" title="">2024</a>)</cite>. Early work primarily focuses on improving retrieval model performance to obtain relevant documents for subsequent generation <cite class="ltx_cite ltx_citemacro_citep">(Qu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib29" title="">2021</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib37" title="">2023</a>; Zheng et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib50" title="">2024</a>)</cite>. Recent research has expanded RAG framework to real-world noisy scenarios, aiming to build robust RAG systems by enhancing the generator <cite class="ltx_cite ltx_citemacro_citep">(Fang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib8" title="">2024</a>; Xiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib42" title="">2024</a>)</cite>. For instance, Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib2" title="">2024</a>)</cite> employs four specialized tokens and GPT-4-generated instruction-tuning data to fine-tune the Llama2 model. RetRobust <cite class="ltx_cite ltx_citemacro_citep">(Yoran et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib48" title="">2024</a>)</cite> introduces an automated data generation method to fine-tune the generator to utilize retrieved passages against noise effectively. RobustRAG <cite class="ltx_cite ltx_citemacro_citep">(Xiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib42" title="">2024</a>)</cite> proposes a defense framework that enhances RAG model robustness against retrieval corruption attacks through an isolate-then-aggregate strategy, achieving certifiable robustness via secure text aggregation techniques. However, these investigations are constrained by their narrow focus on specific noise types and the inherent assumption that noise is harmful, potentially hindering method generalization. This paper aims to present a systematic analysis of RAG noise and reveal its role.
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_subsection" id="Sx2.SSx2">
<h3 class="ltx_title ltx_title_subsection">Noise Injection in LLMs</h3>
<div class="ltx_para" id="Sx2.SSx2.p1">
<p class="ltx_p" id="Sx2.SSx2.p1.1">Noise injection <cite class="ltx_cite ltx_citemacro_citep">(Grandvalet, Canu, and Boucheron <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib12" title="">1997</a>)</cite> in LLMs involves adding noise to inputs during training or inference, such as data augmentation <cite class="ltx_cite ltx_citemacro_citep">(Ye et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib47" title="">2024</a>)</cite>, adversarial training <cite class="ltx_cite ltx_citemacro_citep">(Fang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib8" title="">2024</a>)</cite>, and prompt perturbation <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib51" title="">2024</a>)</cite>. Recently, researchers have focused on noise injection in RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib5" title="">2024</a>)</cite>. For example, <cite class="ltx_cite ltx_citemacro_citet">Cuconasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib7" title="">2024</a>)</cite> classifies three retrieval noises and explores their effects on LLMs. <cite class="ltx_cite ltx_citemacro_citet">Fang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib8" title="">2024</a>)</cite> leverages adversarial training to dynamically adjust LLM’s training process in response to retrieval noises. However, these noise types are limited and may not reflect complex real-world scenarios. A comprehensive framework that simulates real-world noise is necessary.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">A Taxonomy of RAG Noise</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx3.F2" title="Figure 2 ‣ A Taxonomy of RAG Noise ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">2</span></a>, we categorize RAG noise into seven types from a linguistic perspective. They are further divided into beneficial (semantic, datatype, and illegal sentence) and harmful noise (counterfactual, supportive, orthographic, and prior) for practical applications. We will explain the reason behind this classification in the <span class="ltx_text ltx_font_italic" id="Sx3.p1.1.1">Experiments</span> section.</p>
</div>
<figure class="ltx_figure" id="Sx3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="424" id="Sx3.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="Sx3.F2.3.1">(A)</span> Seven types of noise in RAG comprehensively reflect real-world scenarios. <span class="ltx_text ltx_font_bold" id="Sx3.F2.4.2">(B)</span> This detailed illustration of diverse RAG noise intuitively showcases various types. Note that significant noise injections are highlighted in red.</figcaption>
</figure>
<div class="ltx_para" id="Sx3.p2">
<p class="ltx_p" id="Sx3.p2.1"><span class="ltx_text ltx_font_bold" id="Sx3.p2.1.1">Semantic Noise (SeN)</span> 
Retrieval documents may contain content with low semantic relevance to the query, often being off-topic or deviating from the intended meaning. Given that Warren Weaver originally defined semantic noise as ”the perturbations or distortions of sentence meaning” <cite class="ltx_cite ltx_citemacro_citep">(Shannon, Weaver, and Hockett <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib31" title="">1961</a>)</cite>, we classify off-topic, low-semantic-relevance documents as <span class="ltx_text ltx_font_italic" id="Sx3.p2.1.2">semantic noise</span>.</p>
</div>
<div class="ltx_para" id="Sx3.p3">
<p class="ltx_p" id="Sx3.p3.1"><span class="ltx_text ltx_font_bold" id="Sx3.p3.1.1">Datatype Noise (DN)</span> 
This type of noise refers to the mixing of different data types on the web, such as the blending of links and text on Wikipedia. In this paper, we consider three types of data: text, URLs, and code.</p>
</div>
<div class="ltx_para" id="Sx3.p4">
<p class="ltx_p" id="Sx3.p4.1"><span class="ltx_text ltx_font_bold" id="Sx3.p4.1.1">Illegal Sentence Noise (ISN)</span> 
Web content may include fragments that do not form grammatically correct sentences, such as “history transform cover managed that hand black”. We define this type of noise as <span class="ltx_text ltx_font_italic" id="Sx3.p4.1.2">illegal sentence noise</span>.</p>
</div>
<div class="ltx_para" id="Sx3.p5">
<p class="ltx_p" id="Sx3.p5.1"><span class="ltx_text ltx_font_bold" id="Sx3.p5.1.1">Counterfactual Noise (CN)</span> 
The internet contains abundant false information, including fake news and outdated knowledge <cite class="ltx_cite ltx_citemacro_citep">(Tumarkin and Whitelaw <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib35" title="">2001</a>; Olan et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib25" title="">2024</a>)</cite>, which poses significant challenges to RAG systems. Drawing from linguistics, where “counterfactual” denotes statements contrary to fact <cite class="ltx_cite ltx_citemacro_citep">(Feng and Yi <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib9" title="">2006</a>)</cite>, we introduce the term “<span class="ltx_text ltx_font_italic" id="Sx3.p5.1.2">counterfactual noise</span>” to characterize factual errors. This concept aligns with prior research <cite class="ltx_cite ltx_citemacro_citep">(Fang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib8" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx3.p6">
<p class="ltx_p" id="Sx3.p6.1"><span class="ltx_text ltx_font_bold" id="Sx3.p6.1.1">Supportive Noise (SuN)</span> 
Supportive evidence, known as positive evidence, is highly semantically relevant to a hypothesis and provides necessary information to support it <cite class="ltx_cite ltx_citemacro_citep">(Kertész and Rákosi <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib20" title="">2012</a>)</cite>. We introduce the term “<span class="ltx_text ltx_font_italic" id="Sx3.p6.1.2">supportive noise</span>” to describe documents that exhibit high semantic relevance but lack corresponding answer information.</p>
</div>
<div class="ltx_para" id="Sx3.p7">
<p class="ltx_p" id="Sx3.p7.1"><span class="ltx_text ltx_font_bold" id="Sx3.p7.1.1">Orthographic Noise (ON)</span> 
The word “orthography” originates from the Greek <span class="ltx_text ltx_font_italic" id="Sx3.p7.1.2">orthós</span> (meaning “correct”) and <span class="ltx_text ltx_font_italic" id="Sx3.p7.1.3">gráphein</span> (meaning “to write”), and refers to the way words are written in linguistics <cite class="ltx_cite ltx_citemacro_citep">(Skeat <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib33" title="">1993</a>; Aloufi <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib1" title="">2021</a>)</cite>. <span class="ltx_text ltx_font_italic" id="Sx3.p7.1.4">Orthographic noise</span>, on the other hand, can refer to writing errors such as spelling mistakes and word lengthening.</p>
</div>
<div class="ltx_para" id="Sx3.p8">
<p class="ltx_p" id="Sx3.p8.1"><span class="ltx_text ltx_font_bold" id="Sx3.p8.1.1">Prior Noise (PN)</span> 
In linguistics, prior knowledge refers to what a learner already knows before solving a problem <cite class="ltx_cite ltx_citemacro_citep">(Chafe <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib4" title="">1971</a>)</cite>. Our study defines <span class="ltx_text ltx_font_italic" id="Sx3.p8.1.2">prior noise</span> as questions based on false assumptions or premises. For example, the question “Who was the CEO of Google when they were restructured into Alphabet in 2017?” contains prior noise because the restructuring occurred in 2015, not 2017.</p>
</div>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Noise RAG Benchmark Construction</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">We discuss the data construction and evaluation metrics. The overall framework is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx4.F3" title="Figure 3 ‣ Noise RAG Benchmark Construction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="Sx4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="333" id="Sx4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The overall framework for simulating the impact of real-world noise on RAG models. Initially, we generate and obtain QA instances, utilizing ChatGPT to filter out ambiguous examples (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx4.F3.5.1">Step 1</span>). Then, we perform entailment verification using NLI models to maintain evidence quality (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx4.F3.6.2">Step 2</span>). After that, we use tools like search engines to create noisy documents (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx4.F3.7.3">Step 3</span>). Finally, we transform the free-form QA into a multiple-choice QA format by providing several answer options for convenient automatic evaluation (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx4.F3.8.4">Step 4</span>). All experiments are conducted in a zero-shot setting to avoid bias from demonstrations.</figcaption>
</figure>
<figure class="ltx_figure" id="Sx4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="704" id="Sx4.F4.g1" src="x4.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example LLM input for counterfactual evidence generation. The context of the prompt is composed of instruction, examples, and candidate counterfactual QA.</figcaption>
</figure>
<section class="ltx_subsection" id="Sx4.SSx1">
<h3 class="ltx_title ltx_title_subsection">Data Construction</h3>
<div class="ltx_para" id="Sx4.SSx1.p1">
<p class="ltx_p" id="Sx4.SSx1.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx4.F3" title="Figure 3 ‣ Noise RAG Benchmark Construction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">3</span></a> (A), our framework comprises four essential steps, including QA Instance Generation, Entailment Verification, Noise Introduction and Testbeds Construction.</p>
</div>
<div class="ltx_para ltx_indent" id="Sx4.SSx1.p2">
<p class="ltx_p" id="Sx4.SSx1.p2.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx1.p2.1.1">Step 1: QA Instance Generation</span> 
For prior noise, we collect article snippets from mainstream media and Wikipedia, covering various time periods and domains such as sports, politics, and finance. We then design prompts for ChatGPT to generate relevant events, questions, and answers for each snippet. Note that the generated questions contain prior noise (factual errors), which we manually review to ensure that they are reasonably answerable by LLMs. For the remaining six types of noise (SeN, DN, ISN, CN, SuN, ON, PN), we obtain question-answering (QA) pairs from existing datasets, following previous work <cite class="ltx_cite ltx_citemacro_citep">(Fang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib8" title="">2024</a>; Cuconasu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib7" title="">2024</a>; Yoran et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib48" title="">2024</a>)</cite>. After obtaining candidate QA pairs, we employ ChatGPT to remove ambiguous or difficult-to-assess pairs, followed by a manual review. For example, questions like “How many companies have a market capitalization of over <math alttext="\$" class="ltx_Math" display="inline" id="Sx4.SSx1.p2.1.m1.1"><semantics id="Sx4.SSx1.p2.1.m1.1a"><mo id="Sx4.SSx1.p2.1.m1.1.1" xref="Sx4.SSx1.p2.1.m1.1.1.cmml">$</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.p2.1.m1.1b"><csymbol cd="latexml" id="Sx4.SSx1.p2.1.m1.1.1.cmml" xref="Sx4.SSx1.p2.1.m1.1.1">currency-dollar</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.p2.1.m1.1c">\$</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx1.p2.1.m1.1d">$</annotation></semantics></math>25 billion and pledged to reduce greenhouse gas emissions?” should be excluded due to their broad potential answers and the dynamic market values of companies. Similar criteria are applied to other instances.</p>
</div>
<div class="ltx_para ltx_indent" id="Sx4.SSx1.p3">
<p class="ltx_p" id="Sx4.SSx1.p3.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx1.p3.1.1">Step 2: Entailment Verification</span> 
As illustrated in <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib43" title="">2024</a>); Yoran et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib48" title="">2024</a>)</cite>, effective evidence should strongly support its answer. For example, golden evidence about David Beckham should support the answer that he played for Real Madrid before joining LA Galaxy. Therefore, we use the natural language inference model bart-large-mnli-407M <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib23" title="">2019</a>)</cite> to ensure evidence properly entails the answer. Note that, we only keep those examples with an entailment probability <math alttext="\geq" class="ltx_Math" display="inline" id="Sx4.SSx1.p3.1.m1.1"><semantics id="Sx4.SSx1.p3.1.m1.1a"><mo id="Sx4.SSx1.p3.1.m1.1.1" xref="Sx4.SSx1.p3.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="Sx4.SSx1.p3.1.m1.1b"><geq id="Sx4.SSx1.p3.1.m1.1.1.cmml" xref="Sx4.SSx1.p3.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx1.p3.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="Sx4.SSx1.p3.1.m1.1d">≥</annotation></semantics></math> 0.8.</p>
</div>
<div class="ltx_para ltx_indent" id="Sx4.SSx1.p4">
<p class="ltx_p" id="Sx4.SSx1.p4.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx1.p4.1.1">Step 3: Noise Introduction</span> 
We construct diverse retrieval documents for noise testbeds. For counterfactual noise, we extract related entities and relations from Google search results to create counterfactual answers. ChatGPT is then employed to construct corresponding supportive evidence, followed by entailment verification. We present the prompts in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx4.F4" title="Figure 4 ‣ Noise RAG Benchmark Construction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>. For Supportive and semantic noise, we utilize the 2018 English Wikipedia dump <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib19" title="">2020</a>)</cite> as source documents, with off-the-shelf Contriever-MS MARCO model <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib14" title="">2022</a>)</cite> for retrieval and the lightweight text embedding model all-MiniLM-L6-v2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib36" title="">2021</a>)</cite> for semantic relevance filtering. To simulate illegal sentence noise, we construct meaningless sentences by randomly combining words from model vocabulary, mimicking real-world garbled text. Datatype noise is created by prompting ChatGPT to insert URLs or code snippets while preserving key answer information. Finally, orthographic noise is generated using the open-source textnoisr package <cite class="ltx_cite ltx_citemacro_citep">(Preligens Lab <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib27" title="">2023</a>)</cite>, which enables convenient noise introduction. Four types of “action” are implemented: insert, delete, substitute, and swap. In summary, this pipeline enables a comprehensive assessment of model performance across a range of noise scenarios.</p>
</div>
<div class="ltx_para ltx_indent" id="Sx4.SSx1.p5">
<p class="ltx_p" id="Sx4.SSx1.p5.1"><span class="ltx_text ltx_font_bold" id="Sx4.SSx1.p5.1.1">Step 4: Testbeds Construction</span> 
After obtaining high-quality QA instances and diverse retrieval documents, we build testbeds to evaluate model performance under various noise conditions. Given the challenges in automatically assessing LLM responses to open-ended QA tasks <cite class="ltx_cite ltx_citemacro_citep">(Xie et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib43" title="">2024</a>)</cite>, we convert free-form QA into a multiple-choice format. This constrains the response space and facilitates more accurate evaluation. Specifically, for each QA pair, LLMs choose from 4 options: the correct answer, two counterfactual alternatives, and “Uncertain”. The order of the golden option remains entirely random to avoid LLM sensitivity to option order <cite class="ltx_cite ltx_citemacro_citep">(Wu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib40" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="Sx4.SSx1.p6">
<p class="ltx_p" id="Sx4.SSx1.p6.1">Finally, eight datasets are obtained for NoiserBench. Following <cite class="ltx_cite ltx_citemacro_citep">(Yoran et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib48" title="">2024</a>; Wang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib38" title="">2024</a>)</cite>, we randomly select 500 samples from each dataset as test cases or use all samples if the dataset contains fewer than 500.</p>
</div>
</section>
<section class="ltx_subsection" id="Sx4.SSx2">
<h3 class="ltx_title ltx_title_subsection">Evaluation Metrics</h3>
<div class="ltx_para" id="Sx4.SSx2.p1">
<p class="ltx_p" id="Sx4.SSx2.p1.1">This benchmark aims to reveal the role that RAG noise plays on LLMs. We use accuracy as the primary metric and report the weighted average accuracy across datasets, calculated by aggregating accuracy for each dataset.</p>
</div>
<figure class="ltx_table" id="Sx4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Impact of diverse noise types on accuracy (<math alttext="\%" class="ltx_Math" display="inline" id="Sx4.T1.3.m1.1"><semantics id="Sx4.T1.3.m1.1b"><mo id="Sx4.T1.3.m1.1.1" xref="Sx4.T1.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx4.T1.3.m1.1c"><csymbol cd="latexml" id="Sx4.T1.3.m1.1.1.cmml" xref="Sx4.T1.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.3.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.3.m1.1e">%</annotation></semantics></math>) for Llama3-8B-Instruct and Qwen2-7b-Instruct across seven datasets. We assess performance across various retrieval scenarios: “Base” (no retrieval), “Golden Only” (only golden retrieval context), and “Golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx4.T1.4.m2.1"><semantics id="Sx4.T1.4.m2.1b"><mo id="Sx4.T1.4.m2.1.1" xref="Sx4.T1.4.m2.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx4.T1.4.m2.1c"><and id="Sx4.T1.4.m2.1.1.cmml" xref="Sx4.T1.4.m2.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.4.m2.1d">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.4.m2.1e">&amp;</annotation></semantics></math> XXX” (golden context + specific retrieval noises, including Counterfactual, Supportive, Orthographic, Semantic, Datatype, Illegal Sentence Noise). The <span class="ltx_text" id="Sx4.T1.50.1" style="color:#00FF00;">green</span> and <span class="ltx_text" id="Sx4.T1.51.2" style="color:#FF0000;">red</span> values indicate the performance gap from ”Golden Only”. We also provide the weighted average accuracy for each noise type. The best two results are shown in bold and underlined.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="Sx4.T1.47">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx4.T1.47.44.1">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="9" id="Sx4.T1.47.44.1.1">Llama3-8B-Instruct</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.45.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.45.2.1" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.45.2.1.1">
<span class="ltx_p" id="Sx4.T1.47.45.2.1.1.1" style="width:62.6pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.45.2.1.1.1.1">Scenario</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2" id="Sx4.T1.47.45.2.2"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.45.2.2.1">Single-hop</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="3" id="Sx4.T1.47.45.2.3"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.45.2.3.1">Multi-hop (Explicit)</span></td>
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="2" id="Sx4.T1.47.45.2.4"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.45.2.4.1">Multi-hop (Implicit)</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.45.2.5" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.45.2.5.1">
<span class="ltx_p" id="Sx4.T1.47.45.2.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.45.2.5.1.1.1">Average</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.46.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.46.3.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.1.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.1.1.1" style="width:39.8pt;">NQ</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.46.3.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.2.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.2.1.1" style="width:39.8pt;">RGB</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.46.3.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.3.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.3.1.1" style="width:42.7pt;">HotpotQA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.46.3.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.4.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.4.1.1" style="width:42.7pt;">2WikiMQA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.46.3.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.5.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.5.1.1" style="width:42.7pt;">Bamboogle</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.46.3.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.6.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.6.1.1" style="width:42.7pt;">StrategyQA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.46.3.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.46.3.7.1">
<span class="ltx_p" id="Sx4.T1.47.46.3.7.1.1" style="width:42.7pt;">TempQA</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.47.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.47.4.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.1.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.1.1.1" style="width:62.6pt;">Base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.47.4.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.2.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.2.1.1" style="width:39.8pt;">61.34</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.47.4.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.3.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.3.1.1" style="width:39.8pt;">47.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.47.4.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.4.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.4.1.1" style="width:42.7pt;">53.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.47.4.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.5.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.5.1.1" style="width:42.7pt;">34.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.47.4.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.6.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.6.1.1" style="width:42.7pt;">32.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.47.4.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.7.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.7.1.1" style="width:42.7pt;">58.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.47.4.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.8.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.8.1.1" style="width:42.7pt;">50.54</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.47.4.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.47.4.9.1">
<span class="ltx_p" id="Sx4.T1.47.47.4.9.1.1" style="width:42.7pt;">51.58</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.48.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.48.5.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.1.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.1.1.1" style="width:62.6pt;">Golden Only</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.48.5.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.2.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.2.1.1" style="width:39.8pt;">93.06</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.48.5.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.3.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.3.1.1" style="width:39.8pt;">80.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.48.5.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.4.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.4.1.1" style="width:42.7pt;">97.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.48.5.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.5.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.5.1.1" style="width:42.7pt;">79.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.48.5.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.6.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.6.1.1" style="width:42.7pt;">87.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.48.5.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.7.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.7.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.47.48.5.7.1.1.1">73.40</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.48.5.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.8.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.8.1.1" style="width:42.7pt;">91.94</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.48.5.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.48.5.9.1">
<span class="ltx_p" id="Sx4.T1.47.48.5.9.1.1" style="width:42.7pt;">86.57</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.5.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.5.1.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.2.1">
<span class="ltx_p" id="Sx4.T1.5.1.2.1.1" style="width:62.6pt;">Golden &amp; CN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.5.1.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.3.1">
<span class="ltx_p" id="Sx4.T1.5.1.3.1.1" style="width:39.8pt;">58.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.5.1.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.4.1">
<span class="ltx_p" id="Sx4.T1.5.1.4.1.1" style="width:39.8pt;">36.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.5.1.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.5.1">
<span class="ltx_p" id="Sx4.T1.5.1.5.1.1" style="width:42.7pt;">44.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.5.1.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.6.1">
<span class="ltx_p" id="Sx4.T1.5.1.6.1.1" style="width:42.7pt;">21.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.5.1.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.7.1">
<span class="ltx_p" id="Sx4.T1.5.1.7.1.1" style="width:42.7pt;">61.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.5.1.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.8.1">
<span class="ltx_p" id="Sx4.T1.5.1.8.1.1" style="width:42.7pt;">43.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.5.1.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.9.1">
<span class="ltx_p" id="Sx4.T1.5.1.9.1.1" style="width:42.7pt;">67.74</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.5.1.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.5.1.1.1">
<span class="ltx_p" id="Sx4.T1.5.1.1.1.1" style="width:42.7pt;"><math alttext="45.58_{{\color[rgb]{0,1,0}-40.99}}" class="ltx_Math" display="inline" id="Sx4.T1.5.1.1.1.1.m1.1"><semantics id="Sx4.T1.5.1.1.1.1.m1.1a"><msub id="Sx4.T1.5.1.1.1.1.m1.1.1" xref="Sx4.T1.5.1.1.1.1.m1.1.1.cmml"><mn id="Sx4.T1.5.1.1.1.1.m1.1.1.2" xref="Sx4.T1.5.1.1.1.1.m1.1.1.2.cmml">45.58</mn><mrow id="Sx4.T1.5.1.1.1.1.m1.1.1.3" xref="Sx4.T1.5.1.1.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.5.1.1.1.1.m1.1.1.3a" mathcolor="#00FF00" xref="Sx4.T1.5.1.1.1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.T1.5.1.1.1.1.m1.1.1.3.2" mathcolor="#00FF00" xref="Sx4.T1.5.1.1.1.1.m1.1.1.3.2.cmml">40.99</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.5.1.1.1.1.m1.1b"><apply id="Sx4.T1.5.1.1.1.1.m1.1.1.cmml" xref="Sx4.T1.5.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.5.1.1.1.1.m1.1.1.1.cmml" xref="Sx4.T1.5.1.1.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.5.1.1.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.5.1.1.1.1.m1.1.1.2">45.58</cn><apply id="Sx4.T1.5.1.1.1.1.m1.1.1.3.cmml" xref="Sx4.T1.5.1.1.1.1.m1.1.1.3"><minus id="Sx4.T1.5.1.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.5.1.1.1.1.m1.1.1.3"></minus><cn id="Sx4.T1.5.1.1.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.5.1.1.1.1.m1.1.1.3.2">40.99</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.5.1.1.1.1.m1.1c">45.58_{{\color[rgb]{0,1,0}-40.99}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.5.1.1.1.1.m1.1d">45.58 start_POSTSUBSCRIPT - 40.99 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.6.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.6.2.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.2.1">
<span class="ltx_p" id="Sx4.T1.6.2.2.1.1" style="width:62.6pt;">Golden &amp; SuN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.6.2.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.3.1">
<span class="ltx_p" id="Sx4.T1.6.2.3.1.1" style="width:39.8pt;">90.58</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.6.2.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.4.1">
<span class="ltx_p" id="Sx4.T1.6.2.4.1.1" style="width:39.8pt;">80.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.6.2.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.5.1">
<span class="ltx_p" id="Sx4.T1.6.2.5.1.1" style="width:42.7pt;">95.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.6.2.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.6.1">
<span class="ltx_p" id="Sx4.T1.6.2.6.1.1" style="width:42.7pt;">81.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.6.2.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.7.1">
<span class="ltx_p" id="Sx4.T1.6.2.7.1.1" style="width:42.7pt;">93.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.6.2.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.8.1">
<span class="ltx_p" id="Sx4.T1.6.2.8.1.1" style="width:42.7pt;">69.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.6.2.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.9.1">
<span class="ltx_p" id="Sx4.T1.6.2.9.1.1" style="width:42.7pt;">93.01</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.6.2.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.6.2.1.1">
<span class="ltx_p" id="Sx4.T1.6.2.1.1.1" style="width:42.7pt;"><math alttext="85.37_{{\color[rgb]{0,1,0}-1.20}}" class="ltx_Math" display="inline" id="Sx4.T1.6.2.1.1.1.m1.1"><semantics id="Sx4.T1.6.2.1.1.1.m1.1a"><msub id="Sx4.T1.6.2.1.1.1.m1.1.1" xref="Sx4.T1.6.2.1.1.1.m1.1.1.cmml"><mn id="Sx4.T1.6.2.1.1.1.m1.1.1.2" xref="Sx4.T1.6.2.1.1.1.m1.1.1.2.cmml">85.37</mn><mrow id="Sx4.T1.6.2.1.1.1.m1.1.1.3" xref="Sx4.T1.6.2.1.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.6.2.1.1.1.m1.1.1.3a" mathcolor="#00FF00" xref="Sx4.T1.6.2.1.1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.T1.6.2.1.1.1.m1.1.1.3.2" mathcolor="#00FF00" xref="Sx4.T1.6.2.1.1.1.m1.1.1.3.2.cmml">1.20</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.6.2.1.1.1.m1.1b"><apply id="Sx4.T1.6.2.1.1.1.m1.1.1.cmml" xref="Sx4.T1.6.2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.6.2.1.1.1.m1.1.1.1.cmml" xref="Sx4.T1.6.2.1.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.6.2.1.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.6.2.1.1.1.m1.1.1.2">85.37</cn><apply id="Sx4.T1.6.2.1.1.1.m1.1.1.3.cmml" xref="Sx4.T1.6.2.1.1.1.m1.1.1.3"><minus id="Sx4.T1.6.2.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.6.2.1.1.1.m1.1.1.3"></minus><cn id="Sx4.T1.6.2.1.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.6.2.1.1.1.m1.1.1.3.2">1.20</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.6.2.1.1.1.m1.1c">85.37_{{\color[rgb]{0,1,0}-1.20}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.6.2.1.1.1.m1.1d">85.37 start_POSTSUBSCRIPT - 1.20 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.7.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.7.3.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.2.1">
<span class="ltx_p" id="Sx4.T1.7.3.2.1.1" style="width:62.6pt;">Golden &amp; ON</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.7.3.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.3.1">
<span class="ltx_p" id="Sx4.T1.7.3.3.1.1" style="width:39.8pt;">93.31</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.7.3.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.4.1">
<span class="ltx_p" id="Sx4.T1.7.3.4.1.1" style="width:39.8pt;">75.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.7.3.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.5.1">
<span class="ltx_p" id="Sx4.T1.7.3.5.1.1" style="width:42.7pt;">96.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.7.3.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.6.1">
<span class="ltx_p" id="Sx4.T1.7.3.6.1.1" style="width:42.7pt;">78.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.7.3.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.7.1">
<span class="ltx_p" id="Sx4.T1.7.3.7.1.1" style="width:42.7pt;">89.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.7.3.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.8.1">
<span class="ltx_p" id="Sx4.T1.7.3.8.1.1" style="width:42.7pt;">63.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.7.3.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.9.1">
<span class="ltx_p" id="Sx4.T1.7.3.9.1.1" style="width:42.7pt;">90.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.7.3.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.7.3.1.1">
<span class="ltx_p" id="Sx4.T1.7.3.1.1.1" style="width:42.7pt;"><math alttext="83.99_{{\color[rgb]{0,1,0}-2.58}}" class="ltx_Math" display="inline" id="Sx4.T1.7.3.1.1.1.m1.1"><semantics id="Sx4.T1.7.3.1.1.1.m1.1a"><msub id="Sx4.T1.7.3.1.1.1.m1.1.1" xref="Sx4.T1.7.3.1.1.1.m1.1.1.cmml"><mn id="Sx4.T1.7.3.1.1.1.m1.1.1.2" xref="Sx4.T1.7.3.1.1.1.m1.1.1.2.cmml">83.99</mn><mrow id="Sx4.T1.7.3.1.1.1.m1.1.1.3" xref="Sx4.T1.7.3.1.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.7.3.1.1.1.m1.1.1.3a" mathcolor="#00FF00" xref="Sx4.T1.7.3.1.1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.T1.7.3.1.1.1.m1.1.1.3.2" mathcolor="#00FF00" xref="Sx4.T1.7.3.1.1.1.m1.1.1.3.2.cmml">2.58</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.7.3.1.1.1.m1.1b"><apply id="Sx4.T1.7.3.1.1.1.m1.1.1.cmml" xref="Sx4.T1.7.3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.7.3.1.1.1.m1.1.1.1.cmml" xref="Sx4.T1.7.3.1.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.7.3.1.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.7.3.1.1.1.m1.1.1.2">83.99</cn><apply id="Sx4.T1.7.3.1.1.1.m1.1.1.3.cmml" xref="Sx4.T1.7.3.1.1.1.m1.1.1.3"><minus id="Sx4.T1.7.3.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.7.3.1.1.1.m1.1.1.3"></minus><cn id="Sx4.T1.7.3.1.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.7.3.1.1.1.m1.1.1.3.2">2.58</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.7.3.1.1.1.m1.1c">83.99_{{\color[rgb]{0,1,0}-2.58}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.7.3.1.1.1.m1.1d">83.99 start_POSTSUBSCRIPT - 2.58 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.14.10" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.14.10.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.14.10.8.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.14.10.8.1.1" style="width:62.6pt;">Golden &amp; SeN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.8.4.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.8.4.1.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.8.4.1.1.1" style="width:39.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.8.4.1.1.1.1">96.53</span><sub class="ltx_sub" id="Sx4.T1.8.4.1.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.8.4.1.1.1.2.1">+0.47</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.9.5.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.9.5.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.9.5.2.1.1" style="width:39.8pt;">81.33<sub class="ltx_sub" id="Sx4.T1.9.5.2.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.9.5.2.1.1.1.1">+1.33</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.10.6.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.10.6.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.10.6.3.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.10.6.3.1.1.1">98.40</span><sub class="ltx_sub" id="Sx4.T1.10.6.3.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.10.6.3.1.1.2.1">+0.60</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.11.7.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.11.7.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.11.7.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.11.7.4.1.1.1">87.20</span><sub class="ltx_sub" id="Sx4.T1.11.7.4.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.11.7.4.1.1.2.1">+7.40</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.12.8.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.12.8.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.12.8.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.12.8.5.1.1.1">93.60</span><sub class="ltx_sub" id="Sx4.T1.12.8.5.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.12.8.5.1.1.2.1">+6.40</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.14.10.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.14.10.9.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.14.10.9.1.1" style="width:42.7pt;">68.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.13.9.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.13.9.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.13.9.6.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.13.9.6.1.1.1">96.24</span><sub class="ltx_sub" id="Sx4.T1.13.9.6.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.13.9.6.1.1.2.1">+4.30</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.14.10.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.14.10.7.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.14.10.7.1.1" style="width:42.7pt;"><math alttext="88.73_{{\color[rgb]{1,0,0}+2.16}}" class="ltx_Math" display="inline" id="Sx4.T1.14.10.7.1.1.m1.1"><semantics id="Sx4.T1.14.10.7.1.1.m1.1a"><msub id="Sx4.T1.14.10.7.1.1.m1.1.1" xref="Sx4.T1.14.10.7.1.1.m1.1.1.cmml"><mn id="Sx4.T1.14.10.7.1.1.m1.1.1.2" mathbackground="#E6E6E6" xref="Sx4.T1.14.10.7.1.1.m1.1.1.2.cmml">88.73</mn><mrow id="Sx4.T1.14.10.7.1.1.m1.1.1.3" xref="Sx4.T1.14.10.7.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.14.10.7.1.1.m1.1.1.3a" mathbackground="#E6E6E6" mathcolor="#FF0000" xref="Sx4.T1.14.10.7.1.1.m1.1.1.3.cmml">+</mo><mn id="Sx4.T1.14.10.7.1.1.m1.1.1.3.2" mathbackground="#E6E6E6" mathcolor="#FF0000" xref="Sx4.T1.14.10.7.1.1.m1.1.1.3.2.cmml">2.16</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.14.10.7.1.1.m1.1b"><apply id="Sx4.T1.14.10.7.1.1.m1.1.1.cmml" xref="Sx4.T1.14.10.7.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.14.10.7.1.1.m1.1.1.1.cmml" xref="Sx4.T1.14.10.7.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.14.10.7.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.14.10.7.1.1.m1.1.1.2">88.73</cn><apply id="Sx4.T1.14.10.7.1.1.m1.1.1.3.cmml" xref="Sx4.T1.14.10.7.1.1.m1.1.1.3"><plus id="Sx4.T1.14.10.7.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.14.10.7.1.1.m1.1.1.3"></plus><cn id="Sx4.T1.14.10.7.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.14.10.7.1.1.m1.1.1.3.2">2.16</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.14.10.7.1.1.m1.1c">88.73_{{\color[rgb]{1,0,0}+2.16}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.14.10.7.1.1.m1.1d">88.73 start_POSTSUBSCRIPT + 2.16 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.21.17" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.21.17.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.21.17.8.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.21.17.8.1.1" style="width:62.6pt;">Golden &amp; DN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.15.11.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.15.11.1.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.15.11.1.1.1" style="width:39.8pt;">93.19<sub class="ltx_sub" id="Sx4.T1.15.11.1.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.15.11.1.1.1.1.1">+0.13</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.16.12.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.16.12.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.16.12.2.1.1" style="width:39.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.16.12.2.1.1.1">81.67</span><sub class="ltx_sub" id="Sx4.T1.16.12.2.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.16.12.2.1.1.2.1">+1.67</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.21.17.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.21.17.9.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.21.17.9.1.1" style="width:42.7pt;">95.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.17.13.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.17.13.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.17.13.3.1.1" style="width:42.7pt;">82.00<sub class="ltx_sub" id="Sx4.T1.17.13.3.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.17.13.3.1.1.1.1">+2.20</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.18.14.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.18.14.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.18.14.4.1.1" style="width:42.7pt;">88.00<sub class="ltx_sub" id="Sx4.T1.18.14.4.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.18.14.4.1.1.1.1">+0.80</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.19.15.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.19.15.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.19.15.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.19.15.5.1.1.1">73.60<sub class="ltx_sub" id="Sx4.T1.19.15.5.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.19.15.5.1.1.1.1.1">+0.20</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.20.16.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.20.16.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.20.16.6.1.1" style="width:42.7pt;">94.62<sub class="ltx_sub" id="Sx4.T1.20.16.6.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.20.16.6.1.1.1.1">+2.68</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.21.17.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.21.17.7.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.21.17.7.1.1" style="width:42.7pt;"><math alttext="86.91_{{\color[rgb]{1,0,0}+0.34}}" class="ltx_Math" display="inline" id="Sx4.T1.21.17.7.1.1.m1.1"><semantics id="Sx4.T1.21.17.7.1.1.m1.1a"><msub id="Sx4.T1.21.17.7.1.1.m1.1.1" xref="Sx4.T1.21.17.7.1.1.m1.1.1.cmml"><mn id="Sx4.T1.21.17.7.1.1.m1.1.1.2" mathbackground="#E6E6E6" xref="Sx4.T1.21.17.7.1.1.m1.1.1.2.cmml">86.91</mn><mrow id="Sx4.T1.21.17.7.1.1.m1.1.1.3" xref="Sx4.T1.21.17.7.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.21.17.7.1.1.m1.1.1.3a" mathbackground="#E6E6E6" mathcolor="#FF0000" xref="Sx4.T1.21.17.7.1.1.m1.1.1.3.cmml">+</mo><mn id="Sx4.T1.21.17.7.1.1.m1.1.1.3.2" mathbackground="#E6E6E6" mathcolor="#FF0000" xref="Sx4.T1.21.17.7.1.1.m1.1.1.3.2.cmml">0.34</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.21.17.7.1.1.m1.1b"><apply id="Sx4.T1.21.17.7.1.1.m1.1.1.cmml" xref="Sx4.T1.21.17.7.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.21.17.7.1.1.m1.1.1.1.cmml" xref="Sx4.T1.21.17.7.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.21.17.7.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.21.17.7.1.1.m1.1.1.2">86.91</cn><apply id="Sx4.T1.21.17.7.1.1.m1.1.1.3.cmml" xref="Sx4.T1.21.17.7.1.1.m1.1.1.3"><plus id="Sx4.T1.21.17.7.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.21.17.7.1.1.m1.1.1.3"></plus><cn id="Sx4.T1.21.17.7.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.21.17.7.1.1.m1.1.1.3.2">0.34</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.21.17.7.1.1.m1.1c">86.91_{{\color[rgb]{1,0,0}+0.34}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.21.17.7.1.1.m1.1d">86.91 start_POSTSUBSCRIPT + 0.34 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.28.24" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.28.24.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.28.24.8.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.28.24.8.1.1" style="width:62.6pt;">Golden &amp; ISN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.22.18.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.22.18.1.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.22.18.1.1.1" style="width:39.8pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.22.18.1.1.1.1">96.65<sub class="ltx_sub" id="Sx4.T1.22.18.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.22.18.1.1.1.1.1.1">+0.65</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.23.19.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.23.19.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.23.19.2.1.1" style="width:39.8pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.23.19.2.1.1.1">83.00<sub class="ltx_sub" id="Sx4.T1.23.19.2.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.23.19.2.1.1.1.1.1">+1.33</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.24.20.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.24.20.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.24.20.3.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.24.20.3.1.1.1">98.80<sub class="ltx_sub" id="Sx4.T1.24.20.3.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.24.20.3.1.1.1.1.1">+1.00</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.25.21.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.25.21.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.25.21.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.25.21.4.1.1.1">87.40<sub class="ltx_sub" id="Sx4.T1.25.21.4.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.25.21.4.1.1.1.1.1">+7.60</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.26.22.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.26.22.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.26.22.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.26.22.5.1.1.1">94.40<sub class="ltx_sub" id="Sx4.T1.26.22.5.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.26.22.5.1.1.1.1.1">+7.20</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.28.24.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.28.24.9.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.28.24.9.1.1" style="width:42.7pt;">72.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.27.23.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.27.23.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.27.23.6.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.27.23.6.1.1.1">97.85<sub class="ltx_sub" id="Sx4.T1.27.23.6.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.27.23.6.1.1.1.1.1">+5.91</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.28.24.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.28.24.7.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.28.24.7.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.28.24.7.1.1.1">89.89<sub class="ltx_sub" id="Sx4.T1.28.24.7.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.28.24.7.1.1.1.1.1">+3.32</span></sub></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.49.6">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_t" colspan="9" id="Sx4.T1.47.49.6.1">Qwen2-7B-Instruct</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.50.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.50.7.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.1.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.1.1.1" style="width:62.6pt;">Base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.50.7.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.2.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.2.1.1" style="width:39.8pt;">58.24</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.50.7.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.3.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.3.1.1" style="width:39.8pt;">31.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.50.7.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.4.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.4.1.1" style="width:42.7pt;">50.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.50.7.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.5.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.5.1.1" style="width:42.7pt;">22.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.50.7.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.6.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.6.1.1" style="width:42.7pt;">31.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.50.7.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.7.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.7.1.1" style="width:42.7pt;">42.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx4.T1.47.50.7.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.8.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.8.1.1" style="width:42.7pt;">40.86</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx4.T1.47.50.7.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.50.7.9.1">
<span class="ltx_p" id="Sx4.T1.47.50.7.9.1.1" style="width:42.7pt;">43.01</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.51.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.51.8.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.1.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.1.1.1" style="width:62.6pt;">Golden Only</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.51.8.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.2.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.2.1.1" style="width:39.8pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.51.8.2.1.1.1">97.03</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.51.8.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.3.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.3.1.1" style="width:39.8pt;">76.33</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.51.8.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.4.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.4.1.1" style="width:42.7pt;">98.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.51.8.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.5.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.5.1.1" style="width:42.7pt;">78.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.51.8.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.6.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.6.1.1" style="width:42.7pt;">94.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.51.8.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.7.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.7.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.47.51.8.7.1.1.1">67.00</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.47.51.8.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.8.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.8.1.1" style="width:42.7pt;">94.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.47.51.8.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.51.8.9.1">
<span class="ltx_p" id="Sx4.T1.47.51.8.9.1.1" style="width:42.7pt;">86.46</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.29.25">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.29.25.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.2.1">
<span class="ltx_p" id="Sx4.T1.29.25.2.1.1" style="width:62.6pt;">Golden &amp; CN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.29.25.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.3.1">
<span class="ltx_p" id="Sx4.T1.29.25.3.1.1" style="width:39.8pt;">41.88</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.29.25.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.4.1">
<span class="ltx_p" id="Sx4.T1.29.25.4.1.1" style="width:39.8pt;">26.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.29.25.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.5.1">
<span class="ltx_p" id="Sx4.T1.29.25.5.1.1" style="width:42.7pt;">38.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.29.25.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.6.1">
<span class="ltx_p" id="Sx4.T1.29.25.6.1.1" style="width:42.7pt;">12.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.29.25.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.7.1">
<span class="ltx_p" id="Sx4.T1.29.25.7.1.1" style="width:42.7pt;">39.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.29.25.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.8.1">
<span class="ltx_p" id="Sx4.T1.29.25.8.1.1" style="width:42.7pt;">37.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.29.25.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.9.1">
<span class="ltx_p" id="Sx4.T1.29.25.9.1.1" style="width:42.7pt;">45.16</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.29.25.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.29.25.1.1">
<span class="ltx_p" id="Sx4.T1.29.25.1.1.1" style="width:42.7pt;"><math alttext="33.96_{{\color[rgb]{0,1,0}-52.50}}" class="ltx_Math" display="inline" id="Sx4.T1.29.25.1.1.1.m1.1"><semantics id="Sx4.T1.29.25.1.1.1.m1.1a"><msub id="Sx4.T1.29.25.1.1.1.m1.1.1" xref="Sx4.T1.29.25.1.1.1.m1.1.1.cmml"><mn id="Sx4.T1.29.25.1.1.1.m1.1.1.2" xref="Sx4.T1.29.25.1.1.1.m1.1.1.2.cmml">33.96</mn><mrow id="Sx4.T1.29.25.1.1.1.m1.1.1.3" xref="Sx4.T1.29.25.1.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.29.25.1.1.1.m1.1.1.3a" mathcolor="#00FF00" xref="Sx4.T1.29.25.1.1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.T1.29.25.1.1.1.m1.1.1.3.2" mathcolor="#00FF00" xref="Sx4.T1.29.25.1.1.1.m1.1.1.3.2.cmml">52.50</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.29.25.1.1.1.m1.1b"><apply id="Sx4.T1.29.25.1.1.1.m1.1.1.cmml" xref="Sx4.T1.29.25.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.29.25.1.1.1.m1.1.1.1.cmml" xref="Sx4.T1.29.25.1.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.29.25.1.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.29.25.1.1.1.m1.1.1.2">33.96</cn><apply id="Sx4.T1.29.25.1.1.1.m1.1.1.3.cmml" xref="Sx4.T1.29.25.1.1.1.m1.1.1.3"><minus id="Sx4.T1.29.25.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.29.25.1.1.1.m1.1.1.3"></minus><cn id="Sx4.T1.29.25.1.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.29.25.1.1.1.m1.1.1.3.2">52.50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.29.25.1.1.1.m1.1c">33.96_{{\color[rgb]{0,1,0}-52.50}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.29.25.1.1.1.m1.1d">33.96 start_POSTSUBSCRIPT - 52.50 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.30.26">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.30.26.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.2.1">
<span class="ltx_p" id="Sx4.T1.30.26.2.1.1" style="width:62.6pt;">Golden &amp; SuN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.30.26.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.3.1">
<span class="ltx_p" id="Sx4.T1.30.26.3.1.1" style="width:39.8pt;">90.46</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.30.26.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.4.1">
<span class="ltx_p" id="Sx4.T1.30.26.4.1.1" style="width:39.8pt;">74.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.30.26.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.5.1">
<span class="ltx_p" id="Sx4.T1.30.26.5.1.1" style="width:42.7pt;">96.40</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.30.26.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.6.1">
<span class="ltx_p" id="Sx4.T1.30.26.6.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.30.26.6.1.1.1">80.40</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.30.26.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.7.1">
<span class="ltx_p" id="Sx4.T1.30.26.7.1.1" style="width:42.7pt;">92.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.30.26.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.8.1">
<span class="ltx_p" id="Sx4.T1.30.26.8.1.1" style="width:42.7pt;">64.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.30.26.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.9.1">
<span class="ltx_p" id="Sx4.T1.30.26.9.1.1" style="width:42.7pt;">90.32</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.30.26.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.30.26.1.1">
<span class="ltx_p" id="Sx4.T1.30.26.1.1.1" style="width:42.7pt;"><math alttext="83.65_{{\color[rgb]{0,1,0}-2.81}}" class="ltx_Math" display="inline" id="Sx4.T1.30.26.1.1.1.m1.1"><semantics id="Sx4.T1.30.26.1.1.1.m1.1a"><msub id="Sx4.T1.30.26.1.1.1.m1.1.1" xref="Sx4.T1.30.26.1.1.1.m1.1.1.cmml"><mn id="Sx4.T1.30.26.1.1.1.m1.1.1.2" xref="Sx4.T1.30.26.1.1.1.m1.1.1.2.cmml">83.65</mn><mrow id="Sx4.T1.30.26.1.1.1.m1.1.1.3" xref="Sx4.T1.30.26.1.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.30.26.1.1.1.m1.1.1.3a" mathcolor="#00FF00" xref="Sx4.T1.30.26.1.1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.T1.30.26.1.1.1.m1.1.1.3.2" mathcolor="#00FF00" xref="Sx4.T1.30.26.1.1.1.m1.1.1.3.2.cmml">2.81</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.30.26.1.1.1.m1.1b"><apply id="Sx4.T1.30.26.1.1.1.m1.1.1.cmml" xref="Sx4.T1.30.26.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.30.26.1.1.1.m1.1.1.1.cmml" xref="Sx4.T1.30.26.1.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.30.26.1.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.30.26.1.1.1.m1.1.1.2">83.65</cn><apply id="Sx4.T1.30.26.1.1.1.m1.1.1.3.cmml" xref="Sx4.T1.30.26.1.1.1.m1.1.1.3"><minus id="Sx4.T1.30.26.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.30.26.1.1.1.m1.1.1.3"></minus><cn id="Sx4.T1.30.26.1.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.30.26.1.1.1.m1.1.1.3.2">2.81</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.30.26.1.1.1.m1.1c">83.65_{{\color[rgb]{0,1,0}-2.81}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.30.26.1.1.1.m1.1d">83.65 start_POSTSUBSCRIPT - 2.81 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.31.27">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.31.27.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.2.1">
<span class="ltx_p" id="Sx4.T1.31.27.2.1.1" style="width:62.6pt;">Golden &amp; ON</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.31.27.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.3.1">
<span class="ltx_p" id="Sx4.T1.31.27.3.1.1" style="width:39.8pt;">95.66</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.31.27.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.4.1">
<span class="ltx_p" id="Sx4.T1.31.27.4.1.1" style="width:39.8pt;">74.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.31.27.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.5.1">
<span class="ltx_p" id="Sx4.T1.31.27.5.1.1" style="width:42.7pt;">97.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.31.27.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.6.1">
<span class="ltx_p" id="Sx4.T1.31.27.6.1.1" style="width:42.7pt;">80.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.31.27.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.7.1">
<span class="ltx_p" id="Sx4.T1.31.27.7.1.1" style="width:42.7pt;">91.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.31.27.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.8.1">
<span class="ltx_p" id="Sx4.T1.31.27.8.1.1" style="width:42.7pt;">54.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.31.27.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.9.1">
<span class="ltx_p" id="Sx4.T1.31.27.9.1.1" style="width:42.7pt;">94.62</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.31.27.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.31.27.1.1">
<span class="ltx_p" id="Sx4.T1.31.27.1.1.1" style="width:42.7pt;"><math alttext="83.82_{{\color[rgb]{0,1,0}-2.64}}" class="ltx_Math" display="inline" id="Sx4.T1.31.27.1.1.1.m1.1"><semantics id="Sx4.T1.31.27.1.1.1.m1.1a"><msub id="Sx4.T1.31.27.1.1.1.m1.1.1" xref="Sx4.T1.31.27.1.1.1.m1.1.1.cmml"><mn id="Sx4.T1.31.27.1.1.1.m1.1.1.2" xref="Sx4.T1.31.27.1.1.1.m1.1.1.2.cmml">83.82</mn><mrow id="Sx4.T1.31.27.1.1.1.m1.1.1.3" xref="Sx4.T1.31.27.1.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.31.27.1.1.1.m1.1.1.3a" mathcolor="#00FF00" xref="Sx4.T1.31.27.1.1.1.m1.1.1.3.cmml">−</mo><mn id="Sx4.T1.31.27.1.1.1.m1.1.1.3.2" mathcolor="#00FF00" xref="Sx4.T1.31.27.1.1.1.m1.1.1.3.2.cmml">2.64</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.31.27.1.1.1.m1.1b"><apply id="Sx4.T1.31.27.1.1.1.m1.1.1.cmml" xref="Sx4.T1.31.27.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.31.27.1.1.1.m1.1.1.1.cmml" xref="Sx4.T1.31.27.1.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.31.27.1.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.31.27.1.1.1.m1.1.1.2">83.82</cn><apply id="Sx4.T1.31.27.1.1.1.m1.1.1.3.cmml" xref="Sx4.T1.31.27.1.1.1.m1.1.1.3"><minus id="Sx4.T1.31.27.1.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.31.27.1.1.1.m1.1.1.3"></minus><cn id="Sx4.T1.31.27.1.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.31.27.1.1.1.m1.1.1.3.2">2.64</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.31.27.1.1.1.m1.1c">83.82_{{\color[rgb]{0,1,0}-2.64}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.31.27.1.1.1.m1.1d">83.82 start_POSTSUBSCRIPT - 2.64 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.36.32" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.36.32.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.36.32.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.36.32.6.1.1" style="width:62.6pt;">Golden &amp; SeN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.36.32.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.36.32.7.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.36.32.7.1.1" style="width:39.8pt;">96.53</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.32.28.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.32.28.1.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.32.28.1.1.1" style="width:39.8pt;">77.67<sub class="ltx_sub" id="Sx4.T1.32.28.1.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.32.28.1.1.1.1.1">+1.34</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.33.29.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.33.29.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.33.29.2.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.33.29.2.1.1.1">98.80</span><sub class="ltx_sub" id="Sx4.T1.33.29.2.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.33.29.2.1.1.2.1">+0.40</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.36.32.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.36.32.8.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.36.32.8.1.1" style="width:42.7pt;">77.00</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.34.30.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.34.30.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.34.30.3.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.34.30.3.1.1.1">96.80<sub class="ltx_sub" id="Sx4.T1.34.30.3.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.34.30.3.1.1.1.1.1">+2.40</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.36.32.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.36.32.9.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.36.32.9.1.1" style="width:42.7pt;">66.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.35.31.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.35.31.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.35.31.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.35.31.4.1.1.1">97.31</span><sub class="ltx_sub" id="Sx4.T1.35.31.4.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.35.31.4.1.1.2.1">+2.69</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.36.32.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.36.32.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.36.32.5.1.1" style="width:42.7pt;"><math alttext="86.60_{{\color[rgb]{1,0,0}+0.14}}" class="ltx_Math" display="inline" id="Sx4.T1.36.32.5.1.1.m1.1"><semantics id="Sx4.T1.36.32.5.1.1.m1.1a"><msub id="Sx4.T1.36.32.5.1.1.m1.1.1" xref="Sx4.T1.36.32.5.1.1.m1.1.1.cmml"><mn id="Sx4.T1.36.32.5.1.1.m1.1.1.2" mathbackground="#E6E6E6" xref="Sx4.T1.36.32.5.1.1.m1.1.1.2.cmml">86.60</mn><mrow id="Sx4.T1.36.32.5.1.1.m1.1.1.3" xref="Sx4.T1.36.32.5.1.1.m1.1.1.3.cmml"><mo id="Sx4.T1.36.32.5.1.1.m1.1.1.3a" mathbackground="#E6E6E6" mathcolor="#FF0000" xref="Sx4.T1.36.32.5.1.1.m1.1.1.3.cmml">+</mo><mn id="Sx4.T1.36.32.5.1.1.m1.1.1.3.2" mathbackground="#E6E6E6" mathcolor="#FF0000" xref="Sx4.T1.36.32.5.1.1.m1.1.1.3.2.cmml">0.14</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx4.T1.36.32.5.1.1.m1.1b"><apply id="Sx4.T1.36.32.5.1.1.m1.1.1.cmml" xref="Sx4.T1.36.32.5.1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx4.T1.36.32.5.1.1.m1.1.1.1.cmml" xref="Sx4.T1.36.32.5.1.1.m1.1.1">subscript</csymbol><cn id="Sx4.T1.36.32.5.1.1.m1.1.1.2.cmml" type="float" xref="Sx4.T1.36.32.5.1.1.m1.1.1.2">86.60</cn><apply id="Sx4.T1.36.32.5.1.1.m1.1.1.3.cmml" xref="Sx4.T1.36.32.5.1.1.m1.1.1.3"><plus id="Sx4.T1.36.32.5.1.1.m1.1.1.3.1.cmml" xref="Sx4.T1.36.32.5.1.1.m1.1.1.3"></plus><cn id="Sx4.T1.36.32.5.1.1.m1.1.1.3.2.cmml" type="float" xref="Sx4.T1.36.32.5.1.1.m1.1.1.3.2">0.14</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.T1.36.32.5.1.1.m1.1c">86.60_{{\color[rgb]{1,0,0}+0.14}}</annotation><annotation encoding="application/x-llamapun" id="Sx4.T1.36.32.5.1.1.m1.1d">86.60 start_POSTSUBSCRIPT + 0.14 end_POSTSUBSCRIPT</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.41.37" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.41.37.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.41.37.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.41.37.6.1.1" style="width:62.6pt;">Golden &amp; DN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.41.37.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.41.37.7.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.41.37.7.1.1" style="width:39.8pt;">96.03</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.37.33.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.37.33.1.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.37.33.1.1.1" style="width:39.8pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.37.33.1.1.1.1">84.33<sub class="ltx_sub" id="Sx4.T1.37.33.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.37.33.1.1.1.1.1.1">+9.00</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.41.37.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.41.37.8.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.41.37.8.1.1" style="width:42.7pt;">98.20</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.38.34.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.38.34.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.38.34.2.1.1" style="width:42.7pt;">79.60<sub class="ltx_sub" id="Sx4.T1.38.34.2.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.38.34.2.1.1.1.1">+1.60</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.41.37.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.41.37.9.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.41.37.9.1.1" style="width:42.7pt;">93.60</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.39.35.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.39.35.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.39.35.3.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.39.35.3.1.1.1">71.80<sub class="ltx_sub" id="Sx4.T1.39.35.3.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.39.35.3.1.1.1.1.1">+4.80</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="Sx4.T1.40.36.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.40.36.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.40.36.4.1.1" style="width:42.7pt;">95.70<sub class="ltx_sub" id="Sx4.T1.40.36.4.1.1.1"><span class="ltx_text ltx_font_italic" id="Sx4.T1.40.36.4.1.1.1.1">+1.08</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="Sx4.T1.41.37.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.41.37.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.41.37.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.41.37.5.1.1.1">88.11<sub class="ltx_sub" id="Sx4.T1.41.37.5.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.41.37.5.1.1.1.1.1">+1.65</span></sub></span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx4.T1.47.43" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="Sx4.T1.47.43.7">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.43.7.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.47.43.7.1.1" style="width:62.6pt;">Golden &amp; ISN</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="Sx4.T1.47.43.8">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.43.8.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.47.43.8.1.1" style="width:39.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.47.43.8.1.1.1">96.65</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="Sx4.T1.42.38.1">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.42.38.1.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.42.38.1.1.1" style="width:39.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="Sx4.T1.42.38.1.1.1.1">80.00</span><sub class="ltx_sub" id="Sx4.T1.42.38.1.1.1.2"><span class="ltx_text ltx_font_italic" id="Sx4.T1.42.38.1.1.1.2.1">+3.67</span></sub></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="Sx4.T1.43.39.2">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.43.39.2.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.43.39.2.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.43.39.2.1.1.1">99.00<sub class="ltx_sub" id="Sx4.T1.43.39.2.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.43.39.2.1.1.1.1.1">+0.60</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="Sx4.T1.44.40.3">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.44.40.3.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.44.40.3.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.44.40.3.1.1.1">83.80<sub class="ltx_sub" id="Sx4.T1.44.40.3.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.44.40.3.1.1.1.1.1">+5.80</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="Sx4.T1.45.41.4">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.45.41.4.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.45.41.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.45.41.4.1.1.1">96.80<sub class="ltx_sub" id="Sx4.T1.45.41.4.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.45.41.4.1.1.1.1.1">+2.40</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="Sx4.T1.47.43.9">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.43.9.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.47.43.9.1.1" style="width:42.7pt;">66.80</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" id="Sx4.T1.46.42.5">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.46.42.5.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.46.42.5.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.46.42.5.1.1.1">97.85<sub class="ltx_sub" id="Sx4.T1.46.42.5.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.46.42.5.1.1.1.1.1">+1.23</span></sub></span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="Sx4.T1.47.43.6">
<span class="ltx_inline-block ltx_align_top" id="Sx4.T1.47.43.6.1" style="background-color:#E6E6E6;">
<span class="ltx_p" id="Sx4.T1.47.43.6.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx4.T1.47.43.6.1.1.1">88.11<sub class="ltx_sub" id="Sx4.T1.47.43.6.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="Sx4.T1.47.43.6.1.1.1.1.1">+1.65</span></sub></span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="Sx5">
<h2 class="ltx_title ltx_title_section">Experiments</h2>
<section class="ltx_subsection" id="Sx5.SSx1">
<h3 class="ltx_title ltx_title_subsection">Experiment Setup</h3>
<section class="ltx_subsubsection" id="Sx5.SSx1.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Datasets</h4>
<div class="ltx_para" id="Sx5.SSx1.SSSx1.p1">
<p class="ltx_p" id="Sx5.SSx1.SSSx1.p1.1">We experiment with multiple QA datasets, which are categorized into four types based on the required reasoning skills:</p>
<ul class="ltx_itemize" id="Sx5.I2">
<li class="ltx_item" id="Sx5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I2.i1.p1">
<p class="ltx_p" id="Sx5.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.I2.i1.p1.1.1">Single-hop</span>: Questions requiring one-step reasoning. We evaluate using the Natural Questions (NQ) <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib22" title="">2019</a>)</cite> and RGB <cite class="ltx_cite ltx_citemacro_citep">(Chen et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib5" title="">2024</a>)</cite> datasets.</p>
</div>
</li>
<li class="ltx_item" id="Sx5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I2.i2.p1">
<p class="ltx_p" id="Sx5.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.I2.i2.p1.1.1">Explicit Multi-hop</span>: Questions where multiple reasoning steps are explicitly expressed. We utilize HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib46" title="">2018</a>)</cite>, 2WIKIMQA <cite class="ltx_cite ltx_citemacro_citep">(Welbl, Stenetorp, and Riedel <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib39" title="">2018</a>)</cite> and Bamboogle dataset <cite class="ltx_cite ltx_citemacro_citep">(Press et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib28" title="">2022</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Sx5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I2.i3.p1">
<p class="ltx_p" id="Sx5.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.I2.i3.p1.1.1">Implicit Multi-hop</span>: Questions where intermediate steps are not explicitly stated, often requiring commonsense knowledge for implicit reasoning. We employ StrategyQA <cite class="ltx_cite ltx_citemacro_citep">(Geva et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib11" title="">2021</a>)</cite> and TempQA <cite class="ltx_cite ltx_citemacro_citep">(Jia et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib15" title="">2018</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="Sx5.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I2.i4.p1">
<p class="ltx_p" id="Sx5.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="Sx5.I2.i4.p1.1.1">Mixed-Hop</span>: Questions requiring single- or multi-hop reasoning. We use our constructed dataset, PriorQA.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="Sx5.SSx1.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Baseline Models</h4>
<div class="ltx_para" id="Sx5.SSx1.SSSx2.p1">
<p class="ltx_p" id="Sx5.SSx1.SSSx2.p1.1">We evaluate LLMs of different architectures and scales: Llama3-Instruct (8B, 70B) <cite class="ltx_cite ltx_citemacro_citep">(Meta, AI <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib24" title="">2024</a>)</cite>, Qwen2-7B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib45" title="">2024</a>)</cite>, Mistral (7B, 8x7B) <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib16" title="">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib17" title="">2024</a>)</cite>, Vicuna-13B-v1.5 <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib6" title="">2023</a>)</cite>, Llama2-13B <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib34" title="">2023</a>)</cite>, and Baichuan2-13B <cite class="ltx_cite ltx_citemacro_citep">(Yang et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib44" title="">2023</a>)</cite>. This enables a comprehensive assessment of noise across various dimensions. Detailed descriptions of each model are provided in the official websites or the corresponding Huggingface repository<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://huggingface.co/models</span></span></span></span>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx5.SSx1.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Implementation Details</h4>
<div class="ltx_para" id="Sx5.SSx1.SSSx3.p1">
<p class="ltx_p" id="Sx5.SSx1.SSSx3.p1.1">We execute the experiments using the following compute specifications.</p>
<ul class="ltx_itemize" id="Sx5.I3">
<li class="ltx_item" id="Sx5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I3.i1.p1">
<p class="ltx_p" id="Sx5.I3.i1.p1.1">NVIDIA A100 80 GB GPU <math alttext="\times" class="ltx_Math" display="inline" id="Sx5.I3.i1.p1.1.m1.1"><semantics id="Sx5.I3.i1.p1.1.m1.1a"><mo id="Sx5.I3.i1.p1.1.m1.1.1" xref="Sx5.I3.i1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="Sx5.I3.i1.p1.1.m1.1b"><times id="Sx5.I3.i1.p1.1.m1.1.1.cmml" xref="Sx5.I3.i1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="Sx5.I3.i1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="Sx5.I3.i1.p1.1.m1.1d">×</annotation></semantics></math> 2</p>
</div>
</li>
<li class="ltx_item" id="Sx5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I3.i2.p1">
<p class="ltx_p" id="Sx5.I3.i2.p1.1">256 GB RAM</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx5.SSx1.SSSx3.p2">
<p class="ltx_p" id="Sx5.SSx1.SSSx3.p2.1">We use Python 3.10.0 and speed up inference using vllm<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/vllm-project/vllm</span></span></span></span>, a fast and easy-to-use library.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="Sx5.SSx2">
<h3 class="ltx_title ltx_title_subsection">Main Results</h3>
<div class="ltx_para" id="Sx5.SSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.p1.1">Firstly, we discuss the role of diverse RAG noises. While prior work has studied the harmful effects of RAG noise, we focus on beneficial noise. Specifically, after revealing the role of noises, we evaluate the effectiveness of beneficial noise across multiple dimensions, including model architectures, scales, and RAG system designs. Then, we investigate whether beneficial noise improves performance amidst other noise types and verify its effectiveness statistically.</p>
</div>
<figure class="ltx_table" id="Sx5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Effects of prior noise measured by accuracy (<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.T2.2.m1.1"><semantics id="Sx5.T2.2.m1.1b"><mo id="Sx5.T2.2.m1.1.1" xref="Sx5.T2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.T2.2.m1.1c"><csymbol cd="latexml" id="Sx5.T2.2.m1.1.1.cmml" xref="Sx5.T2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.T2.2.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.T2.2.m1.1e">%</annotation></semantics></math>). ‘Base’ indicates the scenario with no retrieval. ‘Misleading’ refers to counterfactual content associated with prior noise. ‘Background’ denotes multiple retrieval results obtained after decomposing the query into its constituent entities.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx5.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="Sx5.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx5.T2.3.1.1.1.1">Models</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx5.T2.3.1.1.2.1">Base</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T2.3.1.1.3.1">Misleading</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T2.3.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx5.T2.3.1.1.4.1">Background</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.T2.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="Sx5.T2.3.2.1.1">Llama3-8B</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T2.3.2.1.2">93.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T2.3.2.1.3">47.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T2.3.2.1.4">90.00</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.3.2.1">Qwen2-7B</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.3.2.2">94.20</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.3.2.3">28.20</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.3.2.4">98.20</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.4.3.1">Mistral-7B</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.4.3.2">96.60</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.4.3.3">28.60</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.4.3.4">99.20</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.5.4.1">Llama2-13B</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.5.4.2">21.00</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.5.4.3">5.60</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.5.4.4">61.60</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.6.5.1">Vicuna-13b</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.6.5.2">91.00</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.6.5.3">25.80</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.6.5.4">99.20</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.7.6.1">Baichuan2-13b</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.7.6.2">90.00</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.7.6.3">45.20</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.7.6.4">96.40</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.8.7.1">Llama3-70b</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.8.7.2">99.00</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.8.7.3">78.40</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.8.7.4">99.80</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T2.3.9.8.1">Mixtral-8x7b</th>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.9.8.2">91.20</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.9.8.3">39.00</td>
<td class="ltx_td ltx_align_center" id="Sx5.T2.3.9.8.4">99.60</td>
</tr>
<tr class="ltx_tr" id="Sx5.T2.3.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="Sx5.T2.3.10.9.1">Average</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T2.3.10.9.2">79.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T2.3.10.9.3">34.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T2.3.10.9.4">88.47</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="Sx5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="490" id="Sx5.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Impact of illegal sentence noise (ISN) on the average accuracy of eight representative LLMs on RGB. ‘Golden’, ‘ON’, ‘CN’, and ‘DN’ represent golden context only, golden context with orthographic, counterfactual, and datatype noise, respectively. The mean is marked by a red solid line and the median by a purple dashed line.</figcaption>
</figure>
<section class="ltx_subsubsection" id="Sx5.SSx2.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">The Role of Diverse RAG Noises</h4>
<div class="ltx_para" id="Sx5.SSx2.SSSx1.p1">
<p class="ltx_p" id="Sx5.SSx2.SSSx1.p1.1">Table 1 illustrates the impact of diverse noise types (the first six) on two state-of-the-art open-source models: Llama3-8B-Instruct and Qwen2-7B-Instruct. We observe consistent performance trends across multiple datasets and retrieval noises. Based on these trends, we can categorize retrieval noises into two types: <span class="ltx_text ltx_font_italic" id="Sx5.SSx2.SSSx1.p1.1.1">harmful noise</span> (counterfactual, supportive, and orthographic) and <span class="ltx_text ltx_font_italic" id="Sx5.SSx2.SSSx1.p1.1.2">beneficial noise</span> (semantic, datatype, and illegal sentence). We find that:</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.SSSx1.p2">
<p class="ltx_p" id="Sx5.SSx2.SSSx1.p2.1">(1) For harmful noise, counterfactual noise impacts model performance most significantly by disrupting accurate fact discernment and answer generation. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">1</span></a>, the false statement “Beckham was a prominent player for Manchester United” leads the model to disregard correct information and respond erroneously.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.SSSx1.p3">
<p class="ltx_p" id="Sx5.SSx2.SSSx1.p3.2">(2) For beneficial noise, illegal sentence noise exhibits the most notable improvement in model performance. It improves accuracy by an average of 3.32<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx1.p3.1.m1.1"><semantics id="Sx5.SSx2.SSSx1.p3.1.m1.1a"><mo id="Sx5.SSx2.SSSx1.p3.1.m1.1.1" xref="Sx5.SSx2.SSSx1.p3.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx1.p3.1.m1.1b"><csymbol cd="latexml" id="Sx5.SSx2.SSSx1.p3.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx1.p3.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx1.p3.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx1.p3.1.m1.1d">%</annotation></semantics></math> and 1.65<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx1.p3.2.m2.1"><semantics id="Sx5.SSx2.SSSx1.p3.2.m2.1a"><mo id="Sx5.SSx2.SSSx1.p3.2.m2.1.1" xref="Sx5.SSx2.SSSx1.p3.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx1.p3.2.m2.1b"><csymbol cd="latexml" id="Sx5.SSx2.SSSx1.p3.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx1.p3.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx1.p3.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx1.p3.2.m2.1d">%</annotation></semantics></math> for two models, respectively, and consistently achieves powerful performance across diverse datasets.</p>
</div>
<figure class="ltx_figure" id="Sx5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="581" id="Sx5.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Impact of three noise types on accuracy (<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.F6.4.m1.1"><semantics id="Sx5.F6.4.m1.1b"><mo id="Sx5.F6.4.m1.1.1" xref="Sx5.F6.4.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.F6.4.m1.1c"><csymbol cd="latexml" id="Sx5.F6.4.m1.1.1.cmml" xref="Sx5.F6.4.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F6.4.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.F6.4.m1.1e">%</annotation></semantics></math>) on RGB. We assess performance across various retrieval scenarios: “Golden Only” (only golden retrieval context), “Golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.F6.5.m2.1"><semantics id="Sx5.F6.5.m2.1b"><mo id="Sx5.F6.5.m2.1.1" xref="Sx5.F6.5.m2.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.F6.5.m2.1c"><and id="Sx5.F6.5.m2.1.1.cmml" xref="Sx5.F6.5.m2.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F6.5.m2.1d">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.F6.5.m2.1e">&amp;</annotation></semantics></math> ON” (golden context + orthographic noise), and “Golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.F6.6.m3.1"><semantics id="Sx5.F6.6.m3.1b"><mo id="Sx5.F6.6.m3.1.1" xref="Sx5.F6.6.m3.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.F6.6.m3.1c"><and id="Sx5.F6.6.m3.1.1.cmml" xref="Sx5.F6.6.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F6.6.m3.1d">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.F6.6.m3.1e">&amp;</annotation></semantics></math> SeN” (golden context + semantic noise).</figcaption>
</figure>
<div class="ltx_para" id="Sx5.SSx2.SSSx1.p4">
<p class="ltx_p" id="Sx5.SSx2.SSSx1.p4.2">For prior noise, we assess eight LLMs on our dataset, PriorQA. Questions in PriorQA contain factual errors, such as “Which country hosted 1980 FIFA World Cup?” (1980 FIFA World Cup was not held). Accuracy is measured by whether LLMs correctly identify and respond with “The question is factually incorrect”. As shown in Table 2, results show an average accuracy of 79.93<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx1.p4.1.m1.1"><semantics id="Sx5.SSx2.SSSx1.p4.1.m1.1a"><mo id="Sx5.SSx2.SSSx1.p4.1.m1.1.1" xref="Sx5.SSx2.SSSx1.p4.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx1.p4.1.m1.1b"><csymbol cd="latexml" id="Sx5.SSx2.SSSx1.p4.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx1.p4.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx1.p4.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx1.p4.1.m1.1d">%</annotation></semantics></math> across eight LLMs when handling prior noise. However, when models fail to identify prior errors and continue retrieval, performance drops significantly to 34.20<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx1.p4.2.m2.1"><semantics id="Sx5.SSx2.SSSx1.p4.2.m2.1a"><mo id="Sx5.SSx2.SSSx1.p4.2.m2.1.1" xref="Sx5.SSx2.SSSx1.p4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx1.p4.2.m2.1b"><csymbol cd="latexml" id="Sx5.SSx2.SSSx1.p4.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx1.p4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx1.p4.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx1.p4.2.m2.1d">%</annotation></semantics></math>. This underscores the need to detect prior errors in user queries before answering.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx5.SSx2.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Beneficial Noise Enhances Performance Across Models</h4>
<div class="ltx_para" id="Sx5.SSx2.SSSx2.p1">
<p class="ltx_p" id="Sx5.SSx2.SSSx2.p1.1">We consider both model architectures (Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.F5" title="Figure 5 ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>) and RAG system designs (Table 3) to demonstrate the positive effects of beneficial noise across various models. We present results for illegal sentence noise here. Additionally, since prior research has highlighted the positive effect of semantic noise <cite class="ltx_cite ltx_citemacro_citep">(Cuconasu et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib7" title="">2024</a>)</cite>, our subsequent discussion will focus on two types: datatype noise and illegal sentence noise.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.SSSx2.p2">
<p class="ltx_p" id="Sx5.SSx2.SSSx2.p2.2"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx2.p2.2.1">(1) Results across various architectures and scales</span> As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.F5" title="Figure 5 ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">5</span></a>, we evaluate the impact of illegal sentence noise (ISN) on eight LLMs (different architectures and scales) by calculating average accuracy across scenarios with no noise, harmful noise (e.g. CN, ON), and beneficial noise (e.g. DN).
We apply proportional scaling to CN data to make a clearer illustration within one figure while maintaining consistent conclusions. The results indicate that ISN significantly enhances model performance in all scenarios, with the most substantial improvement under harmful noise. To better illustrate the impacts of certain noise types, which may not be immediately apparent in tabular form, we plot their performance across multiple models using line graphs (Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.F6" title="Figure 6 ‣ The Role of Diverse RAG Noises ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">6</span></a>) under three conditions: golden only, golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx2.p2.1.m1.1"><semantics id="Sx5.SSx2.SSSx2.p2.1.m1.1a"><mo id="Sx5.SSx2.SSSx2.p2.1.m1.1.1" xref="Sx5.SSx2.SSSx2.p2.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx2.p2.1.m1.1b"><and id="Sx5.SSx2.SSSx2.p2.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx2.p2.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx2.p2.1.m1.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx2.p2.1.m1.1d">&amp;</annotation></semantics></math> orthographic noise, and golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx2.p2.2.m2.1"><semantics id="Sx5.SSx2.SSSx2.p2.2.m2.1a"><mo id="Sx5.SSx2.SSSx2.p2.2.m2.1.1" xref="Sx5.SSx2.SSSx2.p2.2.m2.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx2.p2.2.m2.1b"><and id="Sx5.SSx2.SSSx2.p2.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx2.p2.2.m2.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx2.p2.2.m2.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx2.p2.2.m2.1d">&amp;</annotation></semantics></math> semantic noise. These visualizations clearly demonstrate the negative effect of orthographic noise and the slight performance boost provided by semantic noise.</p>
</div>
<figure class="ltx_figure" id="Sx5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="318" id="Sx5.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Results for the impact of illegal sentence noise on the Llama3-8B-instruct and Qwen2-7B-instruct models when exposed to five typical noise categories across four datasets, including both single-hop (S) and multi-hop (explicit: EM, implicit: IM) reasoning tasks. The bar charts show performance differences upon introducing illegal sentence noise. The line graphs illustrate the average accuracy improvement across noise types per dataset.</figcaption>
</figure>
<figure class="ltx_table" id="Sx5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Effects of beneficial noise on Self-RAG (13B). We assess performance through enhanced accuracy ratios (<math alttext="\%" class="ltx_Math" display="inline" id="Sx5.T3.3.m1.1"><semantics id="Sx5.T3.3.m1.1b"><mo id="Sx5.T3.3.m1.1.1" xref="Sx5.T3.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.T3.3.m1.1c"><csymbol cd="latexml" id="Sx5.T3.3.m1.1.1.cmml" xref="Sx5.T3.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.T3.3.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.T3.3.m1.1e">%</annotation></semantics></math>), and the weighted average values (WA, <math alttext="\%" class="ltx_Math" display="inline" id="Sx5.T3.4.m2.1"><semantics id="Sx5.T3.4.m2.1b"><mo id="Sx5.T3.4.m2.1.1" xref="Sx5.T3.4.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="Sx5.T3.4.m2.1c"><csymbol cd="latexml" id="Sx5.T3.4.m2.1.1.cmml" xref="Sx5.T3.4.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="Sx5.T3.4.m2.1d">\%</annotation><annotation encoding="application/x-llamapun" id="Sx5.T3.4.m2.1e">%</annotation></semantics></math>) are also provided.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx5.T3.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.T3.5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="Sx5.T3.5.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx5.T3.5.1.1.1.1">Scenario</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T3.5.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx5.T3.5.1.1.2.1">NQ</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T3.5.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T3.5.1.1.3.1">RGB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T3.5.1.1.4"><span class="ltx_text ltx_font_bold" id="Sx5.T3.5.1.1.4.1">StrategyQA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T3.5.1.1.5"><span class="ltx_text ltx_font_bold" id="Sx5.T3.5.1.1.5.1">WA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.T3.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="Sx5.T3.5.2.1.1">Golden only</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.5.2.1.2">+3.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.5.2.1.3">+1.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.5.2.1.4">+18.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T3.5.2.1.5">+7.77</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="Sx5.T3.5.3.2.1">Golden &amp; DN</th>
<td class="ltx_td ltx_align_center" id="Sx5.T3.5.3.2.2">+1.84</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.5.3.2.3">+1.96</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.5.3.2.4">+13.50</td>
<td class="ltx_td ltx_align_center" id="Sx5.T3.5.3.2.5">+5.49</td>
</tr>
<tr class="ltx_tr" id="Sx5.T3.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="Sx5.T3.5.4.3.1">Golden &amp; ON</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T3.5.4.3.2">+1.76</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T3.5.4.3.3">+3.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T3.5.4.3.4">+10.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T3.5.4.3.5">+4.67</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Sx5.SSx2.SSSx2.p3">
<p class="ltx_p" id="Sx5.SSx2.SSSx2.p3.1"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx2.p3.1.1">(2) Noise effects on specialized RAG models</span> As illustrated in Table 3, introducing illegal sentence noise to the specialized RAG model Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al. <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib2" title="">2024</a>)</cite> consistently enhances model performance across various datasets (NQ, RGB, and StrategyQA) and scenarios (without noise, with harmful and beneficial noise). This further validates the positive effects of beneficial noise.</p>
</div>
<div class="ltx_para" id="Sx5.SSx2.SSSx2.p4">
<p class="ltx_p" id="Sx5.SSx2.SSSx2.p4.1">In conclusion, based on our comprehensive analysis, we can classify illegal sentence noise, datatype noise, and semantic noise as beneficial, while counterfactual, supportive, and orthographic noises are categorized as harmful.</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx5.SSx2.SSSx3">
<h4 class="ltx_title ltx_title_subsubsection">Beneficial Noise Remains Effective Under Other Noise Disturbances</h4>
<div class="ltx_para" id="Sx5.SSx2.SSSx3.p1">
<p class="ltx_p" id="Sx5.SSx2.SSSx3.p1.3">To illustrate the impact of beneficial noise under other noise disturbances, we analyze the effect of illegal sentence noise (ISN) in five scenarios: no noise (i.e., Golden only), harmful noise (i.e., Golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.p1.1.m1.1"><semantics id="Sx5.SSx2.SSSx3.p1.1.m1.1a"><mo id="Sx5.SSx2.SSSx3.p1.1.m1.1.1" xref="Sx5.SSx2.SSSx3.p1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.p1.1.m1.1b"><and id="Sx5.SSx2.SSSx3.p1.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx3.p1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.p1.1.m1.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.p1.1.m1.1d">&amp;</annotation></semantics></math> Counterfactual, Counterfactual only and Golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.p1.2.m2.1"><semantics id="Sx5.SSx2.SSSx3.p1.2.m2.1a"><mo id="Sx5.SSx2.SSSx3.p1.2.m2.1.1" xref="Sx5.SSx2.SSSx3.p1.2.m2.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.p1.2.m2.1b"><and id="Sx5.SSx2.SSSx3.p1.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx3.p1.2.m2.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.p1.2.m2.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.p1.2.m2.1d">&amp;</annotation></semantics></math> Orthographic), and beneficial noise (i.e., Golden <math alttext="\&amp;" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.p1.3.m3.1"><semantics id="Sx5.SSx2.SSSx3.p1.3.m3.1a"><mo id="Sx5.SSx2.SSSx3.p1.3.m3.1.1" xref="Sx5.SSx2.SSSx3.p1.3.m3.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.p1.3.m3.1b"><and id="Sx5.SSx2.SSSx3.p1.3.m3.1.1.cmml" xref="Sx5.SSx2.SSSx3.p1.3.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.p1.3.m3.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.p1.3.m3.1d">&amp;</annotation></semantics></math> Datatype). Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.F7" title="Figure 7 ‣ Beneficial Noise Enhances Performance Across Models ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">7</span></a> shows the model’s accuracy gains after introducing ISN in these scenarios. We find that ISN generally enhances performance across all datasets, particularly when combined with harmful noise like counterfactual noise, with average accuracy improvements exceeding 10 percentage points. The consistent positive effects of ISN in various real-world scenarios underscore its potential significance for future RAG research.</p>
</div>
<figure class="ltx_table" id="Sx5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Statistical significance of differences between scenarios with and without beneficial noises.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Sx5.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="Sx5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.1.1.1">Noise</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.1.2.1">Llama3-8B-Instruct</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Sx5.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="Sx5.T4.1.1.1.3.1">Qwen2-7B-Instruct</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="Sx5.T4.1.2.1.1">ISN</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.2.1.2">4.10e-5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Sx5.T4.1.2.1.3">4.88e-3</td>
</tr>
<tr class="ltx_tr" id="Sx5.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="Sx5.T4.1.3.2.1">DN</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.3.2.2">1.71e-4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Sx5.T4.1.3.2.3">9.59e-4</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Sx5.SSx2.SSSx3.6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Examples of LLM outputs without and with beneficial noise (BN). The <span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.6.9.1" style="color:#0000FF;">blue</span> and <span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.6.10.2" style="color:#00FF00;">green</span> colors denote the correct and incorrect responses, respectively. Upon introducing BN, LLMs exhibit clearer reasoning processes, more standardized response formats, and increased focus on golden context.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="Sx5.SSx2.SSSx3.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Sx5.SSx2.SSSx3.3.3.3">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="Sx5.SSx2.SSSx3.3.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.3.3.3.4.1">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.3.3.3.4.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.3.3.3.4.1.1.1">Retrieved Context  wo BN</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="Sx5.SSx2.SSSx3.3.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.3.3.3.3.3">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3" style="width:432.5pt;">[1] Paul R. Ehrlich, author of ”The Population Bomb,” was born on <span class="ltx_text" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.2" style="color:#FF0000;">June 14, 1931</span>. Ehrlich, a renowned biologist, published his influential book on population control in 1968, which sparked <math alttext="\ldots" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1"><semantics id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1a"><mi id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1b"><ci id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.1.1.1.1.1.1.m1.1d">…</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.3" style="color:#FF0000;">[Counterfactual]
<br class="ltx_break"/></span>[2] The author of The Population Bomb, Paul Ehrlich, was born on <span class="ltx_text" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.4" style="color:#FF0000;">May 29, 1932</span>. He is an American biologist and educator, known for his warnings about the consequences of overpopulation <math alttext="\ldots" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1"><semantics id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1a"><mi id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1.1" mathvariant="normal" xref="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1b"><ci id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.2.2.2.2.2.2.m2.1d">…</annotation></semantics></math>    <span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1" style="color:#FF0000;"> [Golden Context]
<br class="ltx_break"/><math alttext="\ldots" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1"><semantics id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1a"><mi id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1.1" mathcolor="#000000" mathvariant="normal" xref="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1.1.cmml">…</mi><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1b"><ci id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1.1">…</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1c">\ldots</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.3.3.3.3.3.3.1.m1.1d">…</annotation></semantics></math></span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Sx5.SSx2.SSSx3.6.6.7.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx5.SSx2.SSSx3.6.6.7.1.1">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.6.6.7.1.1.1">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.6.6.7.1.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.6.6.7.1.1.1.1.1">Question</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="Sx5.SSx2.SSSx3.6.6.7.1.2">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.6.6.7.1.2.1">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.6.6.7.1.2.1.1" style="width:432.5pt;">When was the author of The Population Bomb born?
<br class="ltx_break"/>‘A’: June 14, 1931 ‘B’: Uncertain <span class="ltx_text" id="Sx5.SSx2.SSSx3.6.6.7.1.2.1.1.1" style="color:#FF0000;">‘C’: May 29, 1932 (Ground Truth)</span> ‘D’: April 22, 1934</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx5.SSx2.SSSx3.5.5.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="Sx5.SSx2.SSSx3.5.5.5.3">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.5.5.5.3.1">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.5.5.5.3.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.5.5.5.3.1.1.1">Answer wo BN</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="Sx5.SSx2.SSSx3.5.5.5.2">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.5.5.5.2.2">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2" style="width:432.5pt;"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.1" style="color:#00FF00;">A</span> <math alttext="\#\#\#" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1"><semantics id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1a"><mrow id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.cmml"><mi id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.2" mathvariant="normal" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.2.cmml">#</mi><mo id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.1" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.3" mathvariant="normal" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.3.cmml">#</mi><mo id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.1a" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.4" mathvariant="normal" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.4.cmml">#</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1b"><apply id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1"><times id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.1.cmml" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.1"></times><ci id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.2.cmml" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.2">#</ci><ci id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.3.cmml" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.3">#</ci><ci id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.4.cmml" xref="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1.1.4">#</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1c">\#\#\#</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.4.4.4.1.1.1.m1.1d"># # #</annotation></semantics></math>\n\n<math alttext="\#\#\#" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1"><semantics id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1a"><mrow id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.cmml"><mi id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.2" mathvariant="normal" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.2.cmml">#</mi><mo id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.1" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.3" mathvariant="normal" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.3.cmml">#</mi><mo id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.1a" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.4" mathvariant="normal" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.4.cmml">#</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1b"><apply id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1"><times id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.1.cmml" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.1"></times><ci id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.2.cmml" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.2">#</ci><ci id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.3.cmml" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.3">#</ci><ci id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.4.cmml" xref="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1.1.4">#</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1c">\#\#\#</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.m2.1d"># # #</annotation></semantics></math> Explanation:\n The correct answer is ‘A’: June 14, 1931. According to the information provided, Paul R. Ehrlich, the author of “The Population Bomb” was born on <span class="ltx_text" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.2" style="color:#00FF00;">June 14, 1931</span>. The other options are incorrect, as they provide different birth dates for the author.  <span class="ltx_ERROR undefined" id="Sx5.SSx2.SSSx3.5.5.5.2.2.2.3">\faTimes</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="Sx5.SSx2.SSSx3.6.6.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t" id="Sx5.SSx2.SSSx3.6.6.6.2">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.6.6.6.2.1">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.6.6.6.2.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.6.6.6.2.1.1.1">Answer w BN</span></span>
</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="Sx5.SSx2.SSSx3.6.6.6.1">
<span class="ltx_inline-block ltx_align_top" id="Sx5.SSx2.SSSx3.6.6.6.1.1">
<span class="ltx_p" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1" style="width:432.5pt;"><span class="ltx_text ltx_font_bold" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.1" style="color:#0000FF;">C</span>\n<math alttext="\#\#\#" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1"><semantics id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1a"><mrow id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.cmml"><mi id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.2" mathvariant="normal" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.2.cmml">#</mi><mo id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.1" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.3" mathvariant="normal" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.3.cmml">#</mi><mo id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.1a" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.4" mathvariant="normal" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.4.cmml">#</mi></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1b"><apply id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1"><times id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.1.cmml" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.1"></times><ci id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.2.cmml" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.2">#</ci><ci id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.3.cmml" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.3">#</ci><ci id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.4.cmml" xref="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1.1.4">#</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1c">\#\#\#</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.m1.1d"># # #</annotation></semantics></math> Explanation: According to information [2], Paul Ehrlich, the author of The Population Bomb, was born on <span class="ltx_text" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.2" style="color:#0000FF;">May 29, 1932</span>. Therefore, the correct answer is ‘C’. Information [1] provides a different birthdate, but it is not the correct answer. The other options are not mentioned in the given information. <span class="ltx_ERROR undefined" id="Sx5.SSx2.SSSx3.6.6.6.1.1.1.3">\faCheck</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section class="ltx_subsubsection ltx_figure_panel" id="Sx5.SSx2.SSSx4">
<h4 class="ltx_title ltx_title_subsubsection">Beneficial Noise Is Statistically Significant</h4>
<div class="ltx_para" id="Sx5.SSx2.SSSx4.p1">
<p class="ltx_p" id="Sx5.SSx2.SSSx4.p1.3">To statistically evaluate the differences between scenarios with and without beneficial noise, we apply the nonparametric Wilcoxon signed-rank test <cite class="ltx_cite ltx_citemacro_citep">(Kotz and Johnson <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib21" title="">1992</a>)</cite>. This method effectively measures the magnitudes of differences and detects statistical significance between two conditions. We test the null hypothesis of no significant difference (<math alttext="H_{0}:difference=0" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx4.p1.1.m1.1"><semantics id="Sx5.SSx2.SSSx4.p1.1.m1.1a"><mrow id="Sx5.SSx2.SSSx4.p1.1.m1.1.1" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.cmml"><msub id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.cmml"><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.2" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.2.cmml">H</mi><mn id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.3" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.3.cmml">0</mn></msub><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.1" lspace="0.278em" rspace="0.278em" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.1.cmml">:</mo><mrow id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.cmml"><mrow id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.cmml"><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.2" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.2.cmml">d</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.3" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.3.cmml">i</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1a" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.4" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.4.cmml">f</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1b" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.5" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.5.cmml">f</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1c" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.6" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.6.cmml">e</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1d" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.7" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.7.cmml">r</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1e" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.8" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.8.cmml">e</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1f" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.9" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.9.cmml">n</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1g" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.10" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.10.cmml">c</mi><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1h" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.11" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.11.cmml">e</mi></mrow><mo id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.1" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.1.cmml">=</mo><mn id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.3" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.3.cmml">0</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx4.p1.1.m1.1b"><apply id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1"><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.1.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.1">:</ci><apply id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.1.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2">subscript</csymbol><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.2.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.2">𝐻</ci><cn id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.3.cmml" type="integer" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.2.3">0</cn></apply><apply id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3"><eq id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.1.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.1"></eq><apply id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2"><times id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.1"></times><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.2.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.2">𝑑</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.3.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.3">𝑖</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.4.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.4">𝑓</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.5.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.5">𝑓</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.6.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.6">𝑒</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.7.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.7">𝑟</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.8.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.8">𝑒</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.9.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.9">𝑛</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.10.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.10">𝑐</ci><ci id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.11.cmml" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.2.11">𝑒</ci></apply><cn id="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.3.cmml" type="integer" xref="Sx5.SSx2.SSSx4.p1.1.m1.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx4.p1.1.m1.1c">H_{0}:difference=0</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx4.p1.1.m1.1d">italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT : italic_d italic_i italic_f italic_f italic_e italic_r italic_e italic_n italic_c italic_e = 0</annotation></semantics></math>) against the alternative hypothesis of a significant difference (<math alttext="H_{1}:difference\neq 0" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx4.p1.2.m2.1"><semantics id="Sx5.SSx2.SSSx4.p1.2.m2.1a"><mrow id="Sx5.SSx2.SSSx4.p1.2.m2.1.1" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.cmml"><msub id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.cmml"><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.2" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.2.cmml">H</mi><mn id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.3" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.3.cmml">1</mn></msub><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.1" lspace="0.278em" rspace="0.278em" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.1.cmml">:</mo><mrow id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.cmml"><mrow id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.cmml"><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.2" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.2.cmml">d</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.3" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.3.cmml">i</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1a" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.4" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.4.cmml">f</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1b" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.5" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.5.cmml">f</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1c" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.6" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.6.cmml">e</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1d" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.7" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.7.cmml">r</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1e" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.8" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.8.cmml">e</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1f" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.9" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.9.cmml">n</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1g" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.10" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.10.cmml">c</mi><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1h" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml">⁢</mo><mi id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.11" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.11.cmml">e</mi></mrow><mo id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.1" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.1.cmml">≠</mo><mn id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.3" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.3.cmml">0</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx4.p1.2.m2.1b"><apply id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1"><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.1.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.1">:</ci><apply id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.1.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2">subscript</csymbol><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.2.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.2">𝐻</ci><cn id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.3.cmml" type="integer" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.2.3">1</cn></apply><apply id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3"><neq id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.1.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.1"></neq><apply id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2"><times id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.1"></times><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.2.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.2">𝑑</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.3.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.3">𝑖</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.4.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.4">𝑓</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.5.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.5">𝑓</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.6.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.6">𝑒</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.7.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.7">𝑟</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.8.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.8">𝑒</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.9.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.9">𝑛</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.10.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.10">𝑐</ci><ci id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.11.cmml" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.2.11">𝑒</ci></apply><cn id="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.3.cmml" type="integer" xref="Sx5.SSx2.SSSx4.p1.2.m2.1.1.3.3">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx4.p1.2.m2.1c">H_{1}:difference\neq 0</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx4.p1.2.m2.1d">italic_H start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : italic_d italic_i italic_f italic_f italic_e italic_r italic_e italic_n italic_c italic_e ≠ 0</annotation></semantics></math>). Following <cite class="ltx_cite ltx_citemacro_citet">Seth et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib30" title="">2023</a>); Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#bib.bib41" title="">2023</a>)</cite>, we use a significance level of 0.05. As shown in Table 4, all p-values are below 0.05, leading us to reject the null hypothesis (<math alttext="H_{0}" class="ltx_Math" display="inline" id="Sx5.SSx2.SSSx4.p1.3.m3.1"><semantics id="Sx5.SSx2.SSSx4.p1.3.m3.1a"><msub id="Sx5.SSx2.SSSx4.p1.3.m3.1.1" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1.cmml"><mi id="Sx5.SSx2.SSSx4.p1.3.m3.1.1.2" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1.2.cmml">H</mi><mn id="Sx5.SSx2.SSSx4.p1.3.m3.1.1.3" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="Sx5.SSx2.SSSx4.p1.3.m3.1b"><apply id="Sx5.SSx2.SSSx4.p1.3.m3.1.1.cmml" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="Sx5.SSx2.SSSx4.p1.3.m3.1.1.1.cmml" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1">subscript</csymbol><ci id="Sx5.SSx2.SSSx4.p1.3.m3.1.1.2.cmml" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1.2">𝐻</ci><cn id="Sx5.SSx2.SSSx4.p1.3.m3.1.1.3.cmml" type="integer" xref="Sx5.SSx2.SSSx4.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx5.SSx2.SSSx4.p1.3.m3.1c">H_{0}</annotation><annotation encoding="application/x-llamapun" id="Sx5.SSx2.SSSx4.p1.3.m3.1d">italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math>). These results provide strong statistical evidence that beneficial noise improves model performance.</p>
</div>
<section class="ltx_subsection" id="Sx5.SSx3">
<h3 class="ltx_title ltx_title_subsection">Exploring the Mechanisms Behind Beneficial Noise</h3>
<div class="ltx_para" id="Sx5.SSx3.p1">
<p class="ltx_p" id="Sx5.SSx3.p1.1">We investigate why beneficial noise positively impacts RAG systems. We propose 3 hypotheses that beneficial noise contributes to:</p>
<ul class="ltx_itemize" id="Sx5.I4">
<li class="ltx_item" id="Sx5.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I4.i1.p1">
<p class="ltx_p" id="Sx5.I4.i1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.I4.i1.p1.1.1">H1: Clearer and more explicit reasoning process</span></p>
</div>
</li>
<li class="ltx_item" id="Sx5.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I4.i2.p1">
<p class="ltx_p" id="Sx5.I4.i2.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.I4.i2.p1.1.1">H2: More standardized response formats</span></p>
</div>
</li>
<li class="ltx_item" id="Sx5.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx5.I4.i3.p1">
<p class="ltx_p" id="Sx5.I4.i3.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.I4.i3.p1.1.1">H3: Increased confidence with golden context</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="Sx5.SSx3.p1.2">We confirm them through case study and statistical analysis.</p>
</div>
<figure class="ltx_figure" id="Sx5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="506" id="Sx5.F8.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The effects of beneficial noise on LLM output uncertainty (anti-confidence). ‘ISN’ and ‘DN’ denote Illegal Sentence Noise and Datatype Noise, respectively. The red star <math alttext="\star" class="ltx_Math" display="inline" id="Sx5.F8.2.m1.1"><semantics id="Sx5.F8.2.m1.1b"><mo id="Sx5.F8.2.m1.1.1" mathcolor="#FF0000" xref="Sx5.F8.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="Sx5.F8.2.m1.1c"><ci id="Sx5.F8.2.m1.1.1.cmml" xref="Sx5.F8.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx5.F8.2.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="Sx5.F8.2.m1.1e">⋆</annotation></semantics></math> represents the mean uncertainty rate (µ).</figcaption>
</figure>
<section class="ltx_subsubsection" id="Sx5.SSx3.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Case Study</h4>
<div class="ltx_para" id="Sx5.SSx3.SSSx1.p1">
<p class="ltx_p" id="Sx5.SSx3.SSSx1.p1.1">Table 5 presents the complete reasoning and generation process of Llama3-8B-instruct on the multi-hop dataset Bamboogle. When exposed to harmful noise without any beneficial noise, the model ignores correct information and exhibits logical flaws under the influence of counterfactual noise influence. This is exemplified by its erroneous statement: “The other options are incorrect, as they provide different birth dates for the author.” However, upon introducing beneficial noise, the model exhibits heightened attention to the golden context and successfully distinguishes between correct and incorrect information (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.SSx3.SSSx1.p1.1.1">H1</span>). We hypothesize that beneficial noise enhances the LLM’s ability to integrate its parameterized knowledge with retrieved information, thus improving its capacity to discern truth from falsehood. Furthermore, by comparing model outputs under two conditions, we observe that beneficial noise contributes to more standardized answer formats (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.SSx3.SSSx1.p1.1.2">H2</span>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="Sx5.SSx3.SSSx2">
<h4 class="ltx_title ltx_title_subsubsection">Statistical Analysis</h4>
<div class="ltx_para" id="Sx5.SSx3.SSSx2.p1">
<p class="ltx_p" id="Sx5.SSx3.SSSx2.p1.1">To verify three hypotheses statistically, we use a two-step process. We first gather model outputs from multiple datasets before and after introducing beneficial noise. Then, we randomly sample 100 examples per dataset to manually assess which condition produces more standardized output formats and clearer reasoning processes. Outputs are deemed similar if no significant difference exists between conditions with and without beneficial noise.
Results across seven datasets show that, on average, 37 samples with beneficial noise exhibit clearer reasoning compared to 31 without (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.SSx3.SSSx2.p1.1.1">H1</span>), while 26 samples with beneficial noise demonstrate better output formats versus 23 without (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.SSx3.SSSx2.p1.1.2">H2</span>).</p>
</div>
<div class="ltx_para" id="Sx5.SSx3.SSSx2.p2">
<p class="ltx_p" id="Sx5.SSx3.SSSx2.p2.1">Second, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.13533v1#Sx5.F8" title="Figure 8 ‣ Exploring the Mechanisms Behind Beneficial Noise ‣ Beneficial Noise Is Statistically Significant ‣ Beneficial Noise Remains Effective Under Other Noise Disturbances ‣ Main Results ‣ Experiments ‣ Pandora’s Box or Aladdin’s Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models"><span class="ltx_text ltx_ref_tag">8</span></a>, we analyze the impact of beneficial noise on LLM output uncertainty across four powerful LLMs. The results indicate indicate that when combined with beneficial noise (ISN or DN), LLMs generally exhibit lower uncertainty and increased confidence in their outputs. This suggests that LLMs pay more attention to the provided golden context and respond with greater confidence (<span class="ltx_text ltx_font_bold ltx_font_italic" id="Sx5.SSx3.SSSx2.p2.1.1">H3</span>).</p>
</div>
<section class="ltx_section" id="Sx6">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>
<div class="ltx_para" id="Sx6.p1">
<p class="ltx_p" id="Sx6.p1.1">In this paper, we provide clear definitions for seven types of RAG noise and categorize them into two groups: beneficial and harmful noise. This is the first comprehensive study to explore retrieval noise from both linguistic and practical perspectives. To conduct this evaluation, we propose a systematic framework for generating various retrieval documents and establish a novel noise benchmark, NoiserBench. Evaluated on eight representative LLMs, extensive experimental results reveal the role that different noise plays in RAG systems. The most surprising finding is that beneficial noise can act like the power of Aladdin’s Lamp and enhance model performance by leading to clearer reasoning paths, more standardized answers, and increased confidence. We anticipate that future research will propose methods to fully leverage beneficial mechanisms of noise while avoiding the negative effects of harmful noise.</p>
</div>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aloufi (2021)</span>
<span class="ltx_bibblock">
Aloufi, A. 2021.

</span>
<span class="ltx_bibblock">Language and Linguistic Orthography.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">English Language and Literature Studies</em>, 11(3).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2024)</span>
<span class="ltx_bibblock">
Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H. 2024.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; et al. 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4.

</span>
<span class="ltx_bibblock">arXiv:2303.12712.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chafe (1971)</span>
<span class="ltx_bibblock">
Chafe, W. L. 1971.

</span>
<span class="ltx_bibblock">Linguistics and human knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Monograph series on languages and linguistics</em>, (24): 57.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Chen, J.; Lin, H.; Han, X.; and Sun, L. 2024.

</span>
<span class="ltx_bibblock">Benchmarking large language models in retrieval-augmented generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 17754–17762. AAAI Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://lmsys.org/blog/2023-03-30-vicuna/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al. (2024)</span>
<span class="ltx_bibblock">
Cuconasu, F.; Trappolini, G.; Siciliano, F.; Filice, S.; Campagnano, C.; Maarek, Y.; et al. 2024.

</span>
<span class="ltx_bibblock">The power of noise: Redefining retrieval for rag systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 719–729. New York, NY, USA: Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. (2024)</span>
<span class="ltx_bibblock">
Fang, F.; Bai, Y.; Ni, S.; Yang, M.; Chen, X.; and Xu, R. 2024.

</span>
<span class="ltx_bibblock">Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training.

</span>
<span class="ltx_bibblock">arXiv:2405.20978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Yi (2006)</span>
<span class="ltx_bibblock">
Feng, G.; and Yi, L. 2006.

</span>
<span class="ltx_bibblock">What if Chinese had linguistic markers for counterfactual conditionals? Language and thought revisited.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the Annual Meeting of the Cognitive Science Society</em>, volume 28.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2024)</span>
<span class="ltx_bibblock">
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; Wang, M.; and Wang, H. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock">arXiv:2312.10997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al. (2021)</span>
<span class="ltx_bibblock">
Geva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Berant, J. 2021.

</span>
<span class="ltx_bibblock">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Transactions of the Association for Computational Linguistics</em>, 9: 346–361.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grandvalet, Canu, and Boucheron (1997)</span>
<span class="ltx_bibblock">
Grandvalet, Y.; Canu, S.; and Boucheron, S. 1997.

</span>
<span class="ltx_bibblock">Noise injection: Theoretical prospects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Neural Computation</em>, 9(5): 1093–1108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Huang, L.; Yu, W.; Ma, W.; Zhong, W.; Feng, Z.; et al. 2023.

</span>
<span class="ltx_bibblock">A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.

</span>
<span class="ltx_bibblock">arXiv:2311.05232.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2022)</span>
<span class="ltx_bibblock">
Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski, P.; Joulin, A.; and Grave, E. 2022.

</span>
<span class="ltx_bibblock">Unsupervised Dense Information Retrieval with Contrastive Learning.

</span>
<span class="ltx_bibblock">arXiv:2112.09118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2018)</span>
<span class="ltx_bibblock">
Jia, Z.; Abujabal, A.; Saha Roy, R.; Strötgen, J.; and Weikum, G. 2018.

</span>
<span class="ltx_bibblock">Tempquestions: A benchmark for temporal question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Companion Proceedings of the The Web Conference 2018</em>, 1057–1062. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; et al. 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock">arXiv:2310.06825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Jiang, A. Q.; Sablayrolles, A.; Roux, A.; Mensch, A.; et al. 2024.

</span>
<span class="ltx_bibblock">Mixtral of Experts.

</span>
<span class="ltx_bibblock">arXiv:2401.04088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kandpal et al. (2023)</span>
<span class="ltx_bibblock">
Kandpal, N.; Deng, H.; Roberts, A.; Wallace, E.; and Raffel, C. 2023.

</span>
<span class="ltx_bibblock">Large language models struggle to learn long-tail knowledge.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Conference on Machine Learning</em>, 15696–15707. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Karpukhin, V.; Oğuz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov, S.; Chen, D.; and tau Yih, W. 2020.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question Answering.

</span>
<span class="ltx_bibblock">arXiv:2004.04906.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kertész and Rákosi (2012)</span>
<span class="ltx_bibblock">
Kertész, A.; and Rákosi, C. 2012.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Data and evidence in linguistics: A plausible argumentation model</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotz and Johnson (1992)</span>
<span class="ltx_bibblock">
Kotz, S.; and Johnson, N. L., eds. 1992.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Breakthroughs in Statistics: Methodology and Distribution</em>.

</span>
<span class="ltx_bibblock">New York, NY: Springer New York.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.; Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin, J.; Lee, K.; et al. 2019.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Transactions of the Association for Computational Linguistics</em>, 7: 453–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2019)</span>
<span class="ltx_bibblock">
Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; et al. 2019.

</span>
<span class="ltx_bibblock">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.

</span>
<span class="ltx_bibblock">arXiv:1910.13461.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meta, AI (2024)</span>
<span class="ltx_bibblock">
Meta, AI. 2024.

</span>
<span class="ltx_bibblock">Introducing meta llama 3: The most capable openly available llm to date.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://ai.meta.com/blog/meta-llama-3/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olan et al. (2024)</span>
<span class="ltx_bibblock">
Olan, F.; Jayawickrama, U.; Arakpogun, E. O.; Suklan, J.; and Liu, S. 2024.

</span>
<span class="ltx_bibblock">Fake news on social media: the impact on society.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Information Systems Frontiers</em>, 26(2): 443–458.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">Introducing ChatGPT.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://openai.com/index/chatgpt/</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Preligens Lab (2023)</span>
<span class="ltx_bibblock">
Preligens Lab. 2023.

</span>
<span class="ltx_bibblock">Textnoisr: Adding random noise to a dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/preligens-lab/textnoisr</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. (2022)</span>
<span class="ltx_bibblock">
Press, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N. A.; and Lewis, M. 2022.

</span>
<span class="ltx_bibblock">Measuring and narrowing the compositionality gap in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2210.03350</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2021)</span>
<span class="ltx_bibblock">
Qu, Y.; Ding, Y.; Liu, J.; Liu, K.; Ren, R.; et al. 2021.

</span>
<span class="ltx_bibblock">RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 5835–5847. Online: Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seth et al. (2023)</span>
<span class="ltx_bibblock">
Seth, I.; Lim, B.; Xie, Y.; Cevik, J.; Rozen, W. M.; Ross, R. J.; and Lee, M. 2023.

</span>
<span class="ltx_bibblock">Comparing the efficacy of large language models ChatGPT, BARD, and Bing AI in providing information on rhinoplasty: an observational study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Aesthetic Surgery Journal Open Forum</em>, 5: ojad084.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shannon, Weaver, and Hockett (1961)</span>
<span class="ltx_bibblock">
Shannon, C.; Weaver, W.; and Hockett, C. 1961.

</span>
<span class="ltx_bibblock">The mathematical theory of communication.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Urbana: University of Illinois</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023)</span>
<span class="ltx_bibblock">
Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, E. H.; Schärli, N.; et al. 2023.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">International Conference on Machine Learning</em>, 31210–31227. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Skeat (1993)</span>
<span class="ltx_bibblock">
Skeat, W. W. 1993.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">The concise dictionary of English etymology</em>.

</span>
<span class="ltx_bibblock">Wordsworth Editions.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock">arXiv:2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tumarkin and Whitelaw (2001)</span>
<span class="ltx_bibblock">
Tumarkin, R.; and Whitelaw, R. F. 2001.

</span>
<span class="ltx_bibblock">News or noise? Internet postings and stock prices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Financial Analysts Journal</em>, 57(3): 41–51.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2021)</span>
<span class="ltx_bibblock">
Wang, W.; Bao, H.; Huang, S.; Dong, L.; and Wei, F. 2021.

</span>
<span class="ltx_bibblock">MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers.

</span>
<span class="ltx_bibblock">arXiv:2012.15828.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Wang, Y.; Li, P.; Sun, M.; and Liu, Y. 2023.

</span>
<span class="ltx_bibblock">Self-Knowledge Guided Retrieval Augmentation for Large Language Models.

</span>
<span class="ltx_bibblock">arXiv:2310.05002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Wang, Z.; Liu, A.; Lin, H.; Li, J.; Ma, X.; and Liang, Y. 2024.

</span>
<span class="ltx_bibblock">RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation.

</span>
<span class="ltx_bibblock">arXiv:2403.05313.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl, Stenetorp, and Riedel (2018)</span>
<span class="ltx_bibblock">
Welbl, J.; Stenetorp, P.; and Riedel, S. 2018.

</span>
<span class="ltx_bibblock">Constructing datasets for multi-hop reading comprehension across documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Transactions of the Association for Computational Linguistics</em>, 6: 287–302.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Wu, J.; Che, F.; Zheng, X.; Zhang, S.; Jin, R.; Nie, S.; Shao, P.; and Tao, J. 2024.

</span>
<span class="ltx_bibblock">Can large language models understand uncommon meanings of common words?

</span>
<span class="ltx_bibblock">arXiv:2405.05741.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Wu, J.; Ning, Z.; Ding, Y.; Wang, Y.; Peng, Q.; and Fu, L. 2023.

</span>
<span class="ltx_bibblock">KGETCDA: an efficient representation learning framework based on knowledge graph encoder from transformer for predicting circRNA-disease associations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Briefings in Bioinformatics</em>, 24(5): bbad292.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiang et al. (2024)</span>
<span class="ltx_bibblock">
Xiang, C.; Wu, T.; Zhong, Z.; Wagner, D.; Chen, D.; and Mittal, P. 2024.

</span>
<span class="ltx_bibblock">Certifiably Robust RAG against Retrieval Corruption.

</span>
<span class="ltx_bibblock">arXiv:2405.15556.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2024)</span>
<span class="ltx_bibblock">
Xie, J.; Zhang, K.; Chen, J.; Lou, R.; and Su, Y. 2024.

</span>
<span class="ltx_bibblock">Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts.

</span>
<span class="ltx_bibblock">arXiv:2305.13300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Yang, A.; Xiao, B.; Wang, B.; Zhang, B.; Bian, C.; Yin, C.; Lv, C.; Pan, D.; et al. 2023.

</span>
<span class="ltx_bibblock">Baichuan 2: Open Large-scale Language Models.

</span>
<span class="ltx_bibblock">arXiv:2309.10305.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.; et al. 2024.

</span>
<span class="ltx_bibblock">Qwen2 Technical Report.

</span>
<span class="ltx_bibblock">arXiv:2407.10671.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Yang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.; Salakhutdinov, R.; and Manning, C. D. 2018.

</span>
<span class="ltx_bibblock">HotpotQA: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock">arXiv:1809.09600.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2024)</span>
<span class="ltx_bibblock">
Ye, J.; Xu, N.; Wang, Y.; Zhou, J.; Zhang, Q.; Gui, T.; and Huang, X. 2024.

</span>
<span class="ltx_bibblock">LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition.

</span>
<span class="ltx_bibblock">arXiv:2402.14568.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et al. (2024)</span>
<span class="ltx_bibblock">
Yoran, O.; Wolfson, T.; Ram, O.; and Berant, J. 2024.

</span>
<span class="ltx_bibblock">Making Retrieval-Augmented Language Models Robust to Irrelevant Context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2024)</span>
<span class="ltx_bibblock">
Zhao, P.; Zhang, H.; Yu, Q.; Wang, Z.; Geng, Y.; Fu, F.; Yang, L.; Zhang, W.; Jiang, J.; and Cui, B. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for AI-Generated Content: A Survey.

</span>
<span class="ltx_bibblock">arXiv:2402.19473.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Zheng, X.; Che, F.; Wu, J.; Zhang, S.; Nie, S.; Liu, K.; and Tao, J. 2024.

</span>
<span class="ltx_bibblock">KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering.

</span>
<span class="ltx_bibblock">arXiv:2404.15660.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2024)</span>
<span class="ltx_bibblock">
Zhu, K.; Wang, J.; Zhou, J.; Wang, Z.; Chen, H.; Wang, Y.; Yang, L.; Ye, W.; et al. 2024.

</span>
<span class="ltx_bibblock">PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts.

</span>
<span class="ltx_bibblock">arXiv:2306.04528.

</span>
</li>
</ul>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Aug 24 09:19:39 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
