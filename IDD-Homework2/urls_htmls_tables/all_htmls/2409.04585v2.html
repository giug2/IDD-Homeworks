<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance</title>
<!--Generated on Sat Sep 21 05:52:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.04585v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S1" title="In CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S2" title="In CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>The CubicML Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3" title="In CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.SS1" title="In 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>ZeRO Sharding Optimization for Distributed Training of Recommendation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.SS2" title="In 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>ML Prediction of Distributed Training Performance of Large Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1" title="In CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS1" title="In Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Training details of the predictor and RL agent in CubicML</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS1.SSS1" title="In A.1 Training details of the predictor and RL agent in CubicML ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.1 </span>The Predictor and RL Agent when Optimizing Ads Recommendation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS1.SSS2" title="In A.1 Training details of the predictor and RL agent in CubicML ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1.2 </span>The Predictor when Predicting Large Language Models Training Speed</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS2" title="In Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Search Spaces in CubicML Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS2.SSS1" title="In A.2 Search Spaces in CubicML Experiments ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.1 </span>Ads Recommendation Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS2.SSS2" title="In A.2 Search Spaces in CubicML Experiments ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2.2 </span>Large Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS3" title="In Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Figures of Experiments for Large Language Models</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Wen, Quanyu Zhu, Weiwei Chu, Wen-Yen Chen, Jiyan Yang 
<br class="ltx_break"/>AI at Meta
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">{wewen, qyz, wchu, wychen, chocjy}@meta.com</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.1">Scaling up deep learning models has been proven effective to improve intelligence of machine learning (ML) models, especially for industry recommendation models and large language models. The co-design of large distributed ML systems and algorithms (to maximize training performance) plays a pivotal role for its success. As it scales, the number of co-design hyper-parameters grows rapidly which brings challenges to feasibly find the optimal setup for system performance maximization. In this paper, we propose CubicML which uses ML to automatically optimize training performance of large distributed ML systems. In CubicML, we use an ML model as a proxy to predict the training performance for search efficiency and performance modeling flexibility. We proved that CubicML can effectively optimize training speed of in-house ads recommendation models with <math alttext="73" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mn id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">73</mn><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><cn id="id1.1.m1.1.1.cmml" type="integer" xref="id1.1.m1.1.1">73</cn></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">73</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">73</annotation></semantics></math> billion parameters and large language models up to 405 billion parameters at Meta.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Scaling deep learning models and training data has become a standard solution to improve the intelligence of machine learning (ML) models, typically of generative AI models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib2" title="">2</a>]</cite> and industry-level recommendation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib5" title="">5</a>]</cite>.
To enable efficient large scale training, it is essential to co-design ML algorithms in distributed ML systems, such as data parallelism by ZeRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib7" title="">7</a>]</cite> and FSDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib8" title="">8</a>]</cite>, model parallelism in the forms of tensor parallelism and pipeline parallelism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib10" title="">10</a>]</cite>, low precision training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib12" title="">12</a>]</cite> and more.
While human experts are proficient at proposing these co-design algorithms, they face challenges to maximize training speed/performance by effectively selecting hyper-parameters of those co-design algorithms based on current model and system setup, such as, to maximize training performance under memory constraint by applying different layer-wise FSDP/ZeRO data parallelism strategies, to accelerate large language model (LLM) distributed training by selecting different parallelism strategies and their hyper-parameters according to the scale and architecture of LLM models, and so on.
This problem becomes bigger when ML systems scale up with more co-design hyper-parameters. Finding the best hyper-parameter for ML system efficiency/performance is beyond easy reach of human experts by manual tuning. Moreover, as the model architecture and system hardware keep involving, repeated tuning is required whenever a change happens, demanding enormous human resources during development.
This difficulty calls for a more principled and automated approach to search co-design hyper-parameters.
Meanwhile, automated machine learning (AutoML <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib13" title="">13</a>]</cite>) has been successfully applied to search ML algorithms, such as model architecture and optimization hyper-parameters, providing one promising solution we can adopt for distributed ML system co-design; however, AutoML, typically black-box optimization, has surprisingly limited applications to distributed ML system co-design, likely because search space was small and simple grid search was enough.
In modern distributed ML systems with growing co-design search space, AutoML becomes non-trivial.
On the other hand, performance modeling of distributed ML systems is a key component to optimize system efficiency.
A performance model is usually lightweight and run efficiently, such that it can be used as a proxy to optimize systems.
However, to build a performance model, enormous research and engineering efforts in ML systems and ML algorithms are required, including ML algorithm deep dive, system hardware understanding, profiling, testing and refining. An accurate performance model is only within reach of experts with high expertise of both ML systems and ML algorithms; moreover, the performance model lacks generalization and a redesign is demanded whenever the hardware system or model algorithm changes.
Instead of manually customizing a performance model, we show that simple ML models can accurately predict performance of distributed ML training, even for a very complex search space in LLMs. Moreover, this method does not require deep dive into the details of current ML systems and adapts well because of its online learning ability.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">There exist some work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib17" title="">17</a>]</cite> on auto-tuning system efficiency with performance modeling but they are highly tailored to specific co-design situations (i.e. a specific parallelism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib16" title="">16</a>]</cite> or ML compiler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib17" title="">17</a>]</cite>) in a small system <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib15" title="">15</a>]</cite>.
We target on a generic black-box AutoML solution to optimize large-scale distributed ML systems by building ML prediction of system performance.
In this paper, we introduce CubicML, an automated machine learning algorithm for distributed ML system co-design to optimize training efficiency. CubicML is first comprehensively evaluated in large-scale ads recommendation models when optimizing layer-wise ZeRO sharding strategy; and then CubicML is evaluated in large language models (LLM) when searching model architecture, data and model parallelism strategies, training precision (FP8 and BF16) and others in different distributed system infrastructure setup (e.g. the number of GPUs and hardware types).
In recommendation models, we compared CubicML with baselines designed by in-house engineers by sampling real profiling jobs as dataset to build a ML model for performance prediction; in LLM models, we used hundreds of LLM training jobs to prove the accurate prediction of training performance by CubicML at scale.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The CubicML Framework</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="408" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>CubicML framework overview.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S2.F1" title="Figure 1 ‣ 2 The CubicML Framework ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates CubicML, which adopts an algorithm close to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib19" title="">19</a>]</cite>. There are five key components: <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">(1) ML systems</span> which is the training cluster stack to launch ML training jobs. It takes co-design hyper-parameters as job configurations to launch a specific ML training. When the job completes, its metadata (e.g. the hyper-parameters, training speed, etc) is saved;
<span class="ltx_text ltx_font_bold" id="S2.p1.1.2">(2) historical job data</span> which stores the metadata of completed jobs. It is used as dataset to train a regression model (dubbed as “predictor”) to predict ML metrics (such as training speed) for co-design hyper-parameters;
<span class="ltx_text ltx_font_bold" id="S2.p1.1.3">(3) search space</span> which defines a set of co-design hyper-parameters with their value ranges that CubicML can tune;
<span class="ltx_text ltx_font_bold" id="S2.p1.1.4">(4) predictor</span> is a lightweight regression model such as a neural network or decision tree regressor to predict system performance (i.e. training speed) we target to optimize. Margin Ranking Loss is used to train the predictor;
<span class="ltx_text ltx_font_bold" id="S2.p1.1.5">(5) searcher</span> which defines a search algorithm to sample many sets of hyper-parameters from the search space, feed these hyper-parameters to the predictor to predict corresponding system metrics, select hyper-parameters with top metrics to launch real training jobs into the ML systems. The searcher can be any black box optimizer, such as random, reinforcement learning (RL), Bayesian method, evolutionary algorithms, and so on. We use RL with REINFORCE algorithm in CubicML. We perform multiple rounds of RL search with different seeds to increase the diversity of top co-design hyper-parameters predicted by the “predictor”.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>ZeRO Sharding Optimization for Distributed Training of Recommendation Models</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="S3.F2.g1" src="extracted/5869690/figures/qps_samples_ctrcvr.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) CubicML result (with predictor-based RL searcher) when optimizing QPS of an ads recommendation model by searching FSDP and other co-design hyper-parameters. x-axis: the number of configurations/jobs CubicML run. y-axis: <math alttext="90" class="ltx_Math" display="inline" id="S3.F2.3.m1.1"><semantics id="S3.F2.3.m1.1b"><mn id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><cn id="S3.F2.3.m1.1.1.cmml" type="integer" xref="S3.F2.3.m1.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">90</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.m1.1e">90</annotation></semantics></math>-th percentile QPS. In the plots, QPS values are normalized/divided by a constant. The same color illustrates the same round of search with dotted line indicating ground-truth QPS value per sample/configuration and solid line indicating maximal QPS frontier observed as each round proceeds. For random search round, we average maximal frontiers over <math alttext="100" class="ltx_Math" display="inline" id="S3.F2.4.m2.1"><semantics id="S3.F2.4.m2.1b"><mn id="S3.F2.4.m2.1.1" xref="S3.F2.4.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.F2.4.m2.1c"><cn id="S3.F2.4.m2.1.1.cmml" type="integer" xref="S3.F2.4.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m2.1d">100</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.m2.1e">100</annotation></semantics></math> perturbations. Note that in “round 1” predictor-based RL search, we only launch a few top jobs for a quick test during development, ending up a very short line. Jobs failed because of out-of-memory or infra failures are not plotted. (b) rank correlation of ground-truth QPS and predicted QPS by the “predictor”. A validation dataset is used here. Note that we use pairwise ranking loss to train the predictor and the absolute values of predicted QPS does not need to approximate the ground truth QPS as long as the rank correlation is high.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">ZeRO Sharding Optimization is a data parallelism to scale ML models. ZeRO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib7" title="">7</a>]</cite> reduces memory usage per GPU by sharding optimizer states (stage 1), gradients (stage 2) and parameters (stage 3) across GPUs. More aggressive sharding (i.e. later stage) can save more memory but paying more cost of communication slowing down training.
We use Pytorch FSDP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib8" title="">8</a>]</cite> implementation, a variant inspired by ZeRO, in our experiment.
Our training hardware is the Grand Teton platform <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/</span></span></span> with 128 NVIDIA H100 GPUs (8 GPUs per node).
The recommendation model is an ads multi-task model for both click through rate (CTR) and impression conversion rate (CVR) prediction, adopting the backbone of Wukong <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib3" title="">3</a>]</cite> with <math alttext="11" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">11</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn id="S3.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1">11</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">11</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">11</annotation></semantics></math> layers. The model has <math alttext="73" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">73</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">73</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">73</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">73</annotation></semantics></math> billion parameters excluding embeddings of category features.
To optimize training speed measured by example/query per second (QPS), we design a search space with <math alttext="5\times 3^{11}\times 10\approx 8.9\times 10^{6}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">5</mn><mo id="S3.SS1.p1.3.m3.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">×</mo><msup id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2.3.2" xref="S3.SS1.p1.3.m3.1.1.2.3.2.cmml">3</mn><mn id="S3.SS1.p1.3.m3.1.1.2.3.3" xref="S3.SS1.p1.3.m3.1.1.2.3.3.cmml">11</mn></msup><mo id="S3.SS1.p1.3.m3.1.1.2.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.3.m3.1.1.2.4" xref="S3.SS1.p1.3.m3.1.1.2.4.cmml">10</mn></mrow><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">≈</mo><mrow id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mn id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">8.9</mn><mo id="S3.SS1.p1.3.m3.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.3.1.cmml">×</mo><msup id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml"><mn id="S3.SS1.p1.3.m3.1.1.3.3.2" xref="S3.SS1.p1.3.m3.1.1.3.3.2.cmml">10</mn><mn id="S3.SS1.p1.3.m3.1.1.3.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.3.cmml">6</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><approx id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></approx><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><times id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.1"></times><cn id="S3.SS1.p1.3.m3.1.1.2.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2.2">5</cn><apply id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">superscript</csymbol><cn id="S3.SS1.p1.3.m3.1.1.2.3.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2.3.2">3</cn><cn id="S3.SS1.p1.3.m3.1.1.2.3.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2.3.3">11</cn></apply><cn id="S3.SS1.p1.3.m3.1.1.2.4.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2.4">10</cn></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><times id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.1"></times><cn id="S3.SS1.p1.3.m3.1.1.3.2.cmml" type="float" xref="S3.SS1.p1.3.m3.1.1.3.2">8.9</cn><apply id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">superscript</csymbol><cn id="S3.SS1.p1.3.m3.1.1.3.3.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.3.3.2">10</cn><cn id="S3.SS1.p1.3.m3.1.1.3.3.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.3.3.3">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">5\times 3^{11}\times 10\approx 8.9\times 10^{6}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">5 × 3 start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT × 10 ≈ 8.9 × 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT</annotation></semantics></math> configurations, covering layer-wise FSDP sharding strategy, batch size and more in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS2.SSS1" title="A.2.1 Ads Recommendation Models ‣ A.2 Search Spaces in CubicML Experiments ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">A.2.1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.4">We use CubicML to search a configuration to maximize QPS. To reduce QPS reading variance, we train each job for <math alttext="2000" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">2000</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn id="S3.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1">2000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">2000</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">2000</annotation></semantics></math> global mini-batches and use <math alttext="90" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn id="S3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">90</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">90</annotation></semantics></math>-th percentile as the QPS metric. The search result is plotted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.F2" title="Figure 2 ‣ 3.1 ZeRO Sharding Optimization for Distributed Training of Recommendation Models ‣ 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">2</span></a> (a).
Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS1.SSS1" title="A.1.1 The Predictor and RL Agent when Optimizing Ads Recommendation Models ‣ A.1 Training details of the predictor and RL agent in CubicML ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">A.1.1</span></a> includes details on how we trained the predictor and RL agent in CubicML.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.F2" title="Figure 2 ‣ 3.1 ZeRO Sharding Optimization for Distributed Training of Recommendation Models ‣ 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">2</span></a> (a), CubicML first randomly sampled around <math alttext="480" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mn id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">480</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><cn id="S3.SS1.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1">480</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">480</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">480</annotation></semantics></math> jobs to test the limit of random search method, and then iterated three rounds of the predictor-based RL search as explained in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS1.SSS1" title="A.1.1 The Predictor and RL Agent when Optimizing Ads Recommendation Models ‣ A.1 Training details of the predictor and RL agent in CubicML ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">A.1.1</span></a>.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.F2" title="Figure 2 ‣ 3.1 ZeRO Sharding Optimization for Distributed Training of Recommendation Models ‣ 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">2</span></a> (a), the random search performance saturates quickly which proves the challenge in tuning this search space; however, after random search, QPS jumps quickly in each new round which proves the QPS uplifting ability of the predictor-based RL search in our solution.
Compared with the human baseline tuned by in-house engineers, CubicML achieved <math alttext="10.3\%" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mrow id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mn id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">10.3</mn><mo id="S3.SS1.p2.4.m4.1.1.1" xref="S3.SS1.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1.1">percent</csymbol><cn id="S3.SS1.p2.4.m4.1.1.2.cmml" type="float" xref="S3.SS1.p2.4.m4.1.1.2">10.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">10.3\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">10.3 %</annotation></semantics></math> QPS boost which is significant in industry-scale recommendation models with power saving of MegaWatts.
Moreover, the whole process is automated without human tuning.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.3">In each predictor-based RL round, we rank configurations based on the predicted QPS and launch jobs with higher predicted QPS first. We find that the “max frontier” (in solid lines) of QPS jumps more often at the beginning and real-time QPS (in dotted lines) trends down (which is very obvious in the last round). Both observations demonstrate the accurate rank of configurations and the accuracy of the predictor.
The accuracy of the predictor is very essential in CubicML, because it models system performance and functions as the proxy that RL uses to propose top configurations.
We evaluate the rank correlation of the predictor in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.F2" title="Figure 2 ‣ 3.1 ZeRO Sharding Optimization for Distributed Training of Recommendation Models ‣ 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">2</span></a> (b).
The result shows that our predictor can accurately score the ground-truth QPS with Kendall Tau <math alttext="0.7" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">0.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn id="S3.SS1.p3.1.m1.1.1.cmml" type="float" xref="S3.SS1.p3.1.m1.1.1">0.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">0.7</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">0.7</annotation></semantics></math>, Pearson <math alttext="0.86" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><mn id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">0.86</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><cn id="S3.SS1.p3.2.m2.1.1.cmml" type="float" xref="S3.SS1.p3.2.m2.1.1">0.86</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">0.86</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">0.86</annotation></semantics></math> and Spearman <math alttext="0.86" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mn id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">0.86</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><cn id="S3.SS1.p3.3.m3.1.1.cmml" type="float" xref="S3.SS1.p3.3.m3.1.1">0.86</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">0.86</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">0.86</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ML Prediction of Distributed Training Performance of Large Language Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">As discussed, accurate prediction of system performance is essential in CubicML.
We have proved its efficacy when optimizing ZeRO, but the search space can grow exponentially as we scale up large language models (LLM) in distributed systems.
In this experiment, we evaluate how accurate the predictor can predict the training speed (words/tokens per second) when training LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.9">We use training speed WPS (words/tokens per second) as the performance metric that CubicML targets to model.
The search space includes configurations detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS2.SSS2" title="A.2.2 Large Language Models ‣ A.2 Search Spaces in CubicML Experiments ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">A.2.2</span></a>, including Transformer model architecture configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib21" title="">21</a>]</cite>, distributed training co-design configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib20" title="">20</a>]</cite> and system infra setup.
Each configuration has a wide range of values.
For example, the number of GPUs can be as small as <math alttext="8" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><cn id="S3.SS2.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">8</annotation></semantics></math> and up to <math alttext="16,384" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.2"><semantics id="S3.SS2.p2.2.m2.2a"><mrow id="S3.SS2.p2.2.m2.2.3.2" xref="S3.SS2.p2.2.m2.2.3.1.cmml"><mn id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">16</mn><mo id="S3.SS2.p2.2.m2.2.3.2.1" xref="S3.SS2.p2.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.2b"><list id="S3.SS2.p2.2.m2.2.3.1.cmml" xref="S3.SS2.p2.2.m2.2.3.2"><cn id="S3.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS2.p2.2.m2.1.1">16</cn><cn id="S3.SS2.p2.2.m2.2.2.cmml" type="integer" xref="S3.SS2.p2.2.m2.2.2">384</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.2c">16,384</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.2d">16 , 384</annotation></semantics></math>; the sequence length ranges between <math alttext="2048" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mn id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><cn id="S3.SS2.p2.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">2048</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">2048</annotation></semantics></math> and <math alttext="131,072" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.2"><semantics id="S3.SS2.p2.4.m4.2a"><mrow id="S3.SS2.p2.4.m4.2.3.2" xref="S3.SS2.p2.4.m4.2.3.1.cmml"><mn id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">131</mn><mo id="S3.SS2.p2.4.m4.2.3.2.1" xref="S3.SS2.p2.4.m4.2.3.1.cmml">,</mo><mn id="S3.SS2.p2.4.m4.2.2" xref="S3.SS2.p2.4.m4.2.2.cmml">072</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.2b"><list id="S3.SS2.p2.4.m4.2.3.1.cmml" xref="S3.SS2.p2.4.m4.2.3.2"><cn id="S3.SS2.p2.4.m4.1.1.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1">131</cn><cn id="S3.SS2.p2.4.m4.2.2.cmml" type="integer" xref="S3.SS2.p2.4.m4.2.2">072</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.2c">131,072</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.2d">131 , 072</annotation></semantics></math>; the number of parameters ranges from <math alttext="27" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mn id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><cn id="S3.SS2.p2.5.m5.1.1.cmml" type="integer" xref="S3.SS2.p2.5.m5.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">27</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">27</annotation></semantics></math> million up to <math alttext="405" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.1"><semantics id="S3.SS2.p2.6.m6.1a"><mn id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">405</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><cn id="S3.SS2.p2.6.m6.1.1.cmml" type="integer" xref="S3.SS2.p2.6.m6.1.1">405</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">405</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.1d">405</annotation></semantics></math> billion.
The predictor training details are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.SS1.SSS2" title="A.1.2 The Predictor when Predicting Large Language Models Training Speed ‣ A.1 Training details of the predictor and RL agent in CubicML ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">A.1.2</span></a>.
We accumulated <math alttext="568" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mn id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">568</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><cn id="S3.SS2.p2.7.m7.1.1.cmml" type="integer" xref="S3.SS2.p2.7.m7.1.1">568</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">568</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">568</annotation></semantics></math> LLM jobs over time during LLM development.
We use <math alttext="145" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m8.1"><semantics id="S3.SS2.p2.8.m8.1a"><mn id="S3.SS2.p2.8.m8.1.1" xref="S3.SS2.p2.8.m8.1.1.cmml">145</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m8.1b"><cn id="S3.SS2.p2.8.m8.1.1.cmml" type="integer" xref="S3.SS2.p2.8.m8.1.1">145</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m8.1c">145</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m8.1d">145</annotation></semantics></math> examples as validation dataset and the rest (<math alttext="423" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m9.1"><semantics id="S3.SS2.p2.9.m9.1a"><mn id="S3.SS2.p2.9.m9.1.1" xref="S3.SS2.p2.9.m9.1.1.cmml">423</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m9.1b"><cn id="S3.SS2.p2.9.m9.1.1.cmml" type="integer" xref="S3.SS2.p2.9.m9.1.1">423</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m9.1c">423</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m9.1d">423</annotation></semantics></math> examples) as training data. The predicted WPS versus actual WPS is plotted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.F3" title="Figure 3 ‣ A.3 Figures of Experiments for Large Language Models ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">3</span></a>, where we performed different training-validation split to simulate three different real-world use cases:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">using random split which simulates the use case where CubicML samples and profiles configurations in real-time (i.e. online) for optimization as model and systems evolve;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">using examples in older jobs as training dataset and validating the predictor by jobs launched later sorted by timestamps. Note that the timestamp gap between jobs can be as long as half a year. This simulates the use case where CubicML targets on reusing historical jobs to jump start search with the purpose of saving compute resources. It also tests the generalization of the predictor when the distributed ML systems evolve in the wild;</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.2">using examples at smaller scales (<math alttext="8\leq\#GPUs\leq 3072" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mn id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">8</mn><mo id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">≤</mo><mrow id="S3.I1.i3.p1.1.m1.1.1.4" xref="S3.I1.i3.p1.1.m1.1.1.4.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.4.2" mathvariant="normal" xref="S3.I1.i3.p1.1.m1.1.1.4.2.cmml">#</mi><mo id="S3.I1.i3.p1.1.m1.1.1.4.1" xref="S3.I1.i3.p1.1.m1.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.1.m1.1.1.4.3" xref="S3.I1.i3.p1.1.m1.1.1.4.3.cmml">G</mi><mo id="S3.I1.i3.p1.1.m1.1.1.4.1a" xref="S3.I1.i3.p1.1.m1.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.1.m1.1.1.4.4" xref="S3.I1.i3.p1.1.m1.1.1.4.4.cmml">P</mi><mo id="S3.I1.i3.p1.1.m1.1.1.4.1b" xref="S3.I1.i3.p1.1.m1.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.1.m1.1.1.4.5" xref="S3.I1.i3.p1.1.m1.1.1.4.5.cmml">U</mi><mo id="S3.I1.i3.p1.1.m1.1.1.4.1c" xref="S3.I1.i3.p1.1.m1.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.1.m1.1.1.4.6" xref="S3.I1.i3.p1.1.m1.1.1.4.6.cmml">s</mi></mrow><mo id="S3.I1.i3.p1.1.m1.1.1.5" xref="S3.I1.i3.p1.1.m1.1.1.5.cmml">≤</mo><mn id="S3.I1.i3.p1.1.m1.1.1.6" xref="S3.I1.i3.p1.1.m1.1.1.6.cmml">3072</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><and id="S3.I1.i3.p1.1.m1.1.1a.cmml" xref="S3.I1.i3.p1.1.m1.1.1"></and><apply id="S3.I1.i3.p1.1.m1.1.1b.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><leq id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3"></leq><cn id="S3.I1.i3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.I1.i3.p1.1.m1.1.1.2">8</cn><apply id="S3.I1.i3.p1.1.m1.1.1.4.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4"><times id="S3.I1.i3.p1.1.m1.1.1.4.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4.1"></times><ci id="S3.I1.i3.p1.1.m1.1.1.4.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4.2">#</ci><ci id="S3.I1.i3.p1.1.m1.1.1.4.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4.3">𝐺</ci><ci id="S3.I1.i3.p1.1.m1.1.1.4.4.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4.4">𝑃</ci><ci id="S3.I1.i3.p1.1.m1.1.1.4.5.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4.5">𝑈</ci><ci id="S3.I1.i3.p1.1.m1.1.1.4.6.cmml" xref="S3.I1.i3.p1.1.m1.1.1.4.6">𝑠</ci></apply></apply><apply id="S3.I1.i3.p1.1.m1.1.1c.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><leq id="S3.I1.i3.p1.1.m1.1.1.5.cmml" xref="S3.I1.i3.p1.1.m1.1.1.5"></leq><share href="https://arxiv.org/html/2409.04585v2#S3.I1.i3.p1.1.m1.1.1.4.cmml" id="S3.I1.i3.p1.1.m1.1.1d.cmml" xref="S3.I1.i3.p1.1.m1.1.1"></share><cn id="S3.I1.i3.p1.1.m1.1.1.6.cmml" type="integer" xref="S3.I1.i3.p1.1.m1.1.1.6">3072</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">8\leq\#GPUs\leq 3072</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">8 ≤ # italic_G italic_P italic_U italic_s ≤ 3072</annotation></semantics></math>) to predict WPS under larger scales (<math alttext="4096\leq\#GPUs\leq 16384" class="ltx_Math" display="inline" id="S3.I1.i3.p1.2.m2.1"><semantics id="S3.I1.i3.p1.2.m2.1a"><mrow id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mn id="S3.I1.i3.p1.2.m2.1.1.2" xref="S3.I1.i3.p1.2.m2.1.1.2.cmml">4096</mn><mo id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml">≤</mo><mrow id="S3.I1.i3.p1.2.m2.1.1.4" xref="S3.I1.i3.p1.2.m2.1.1.4.cmml"><mi id="S3.I1.i3.p1.2.m2.1.1.4.2" mathvariant="normal" xref="S3.I1.i3.p1.2.m2.1.1.4.2.cmml">#</mi><mo id="S3.I1.i3.p1.2.m2.1.1.4.1" xref="S3.I1.i3.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.2.m2.1.1.4.3" xref="S3.I1.i3.p1.2.m2.1.1.4.3.cmml">G</mi><mo id="S3.I1.i3.p1.2.m2.1.1.4.1a" xref="S3.I1.i3.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.2.m2.1.1.4.4" xref="S3.I1.i3.p1.2.m2.1.1.4.4.cmml">P</mi><mo id="S3.I1.i3.p1.2.m2.1.1.4.1b" xref="S3.I1.i3.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.2.m2.1.1.4.5" xref="S3.I1.i3.p1.2.m2.1.1.4.5.cmml">U</mi><mo id="S3.I1.i3.p1.2.m2.1.1.4.1c" xref="S3.I1.i3.p1.2.m2.1.1.4.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.2.m2.1.1.4.6" xref="S3.I1.i3.p1.2.m2.1.1.4.6.cmml">s</mi></mrow><mo id="S3.I1.i3.p1.2.m2.1.1.5" xref="S3.I1.i3.p1.2.m2.1.1.5.cmml">≤</mo><mn id="S3.I1.i3.p1.2.m2.1.1.6" xref="S3.I1.i3.p1.2.m2.1.1.6.cmml">16384</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><and id="S3.I1.i3.p1.2.m2.1.1a.cmml" xref="S3.I1.i3.p1.2.m2.1.1"></and><apply id="S3.I1.i3.p1.2.m2.1.1b.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><leq id="S3.I1.i3.p1.2.m2.1.1.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3"></leq><cn id="S3.I1.i3.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.I1.i3.p1.2.m2.1.1.2">4096</cn><apply id="S3.I1.i3.p1.2.m2.1.1.4.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4"><times id="S3.I1.i3.p1.2.m2.1.1.4.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4.1"></times><ci id="S3.I1.i3.p1.2.m2.1.1.4.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4.2">#</ci><ci id="S3.I1.i3.p1.2.m2.1.1.4.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4.3">𝐺</ci><ci id="S3.I1.i3.p1.2.m2.1.1.4.4.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4.4">𝑃</ci><ci id="S3.I1.i3.p1.2.m2.1.1.4.5.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4.5">𝑈</ci><ci id="S3.I1.i3.p1.2.m2.1.1.4.6.cmml" xref="S3.I1.i3.p1.2.m2.1.1.4.6">𝑠</ci></apply></apply><apply id="S3.I1.i3.p1.2.m2.1.1c.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><leq id="S3.I1.i3.p1.2.m2.1.1.5.cmml" xref="S3.I1.i3.p1.2.m2.1.1.5"></leq><share href="https://arxiv.org/html/2409.04585v2#S3.I1.i3.p1.2.m2.1.1.4.cmml" id="S3.I1.i3.p1.2.m2.1.1d.cmml" xref="S3.I1.i3.p1.2.m2.1.1"></share><cn id="S3.I1.i3.p1.2.m2.1.1.6.cmml" type="integer" xref="S3.I1.i3.p1.2.m2.1.1.6">16384</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">4096\leq\#GPUs\leq 16384</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.2.m2.1d">4096 ≤ # italic_G italic_P italic_U italic_s ≤ 16384</annotation></semantics></math>). This may simulate the random split use case but only profile small scale jobs to save compute and predict performance of large scale jobs. Note that this simulation may not fully fulfill its purpose because more smaller-scale jobs were launched earlier as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.F4" title="Figure 4 ‣ A.3 Figures of Experiments for Large Language Models ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">4</span></a> (left) introducing unattempted bias to the second use case.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.4">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.F3" title="Figure 3 ‣ A.3 Figures of Experiments for Large Language Models ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">3</span></a> (left), CubicML predictor achieves great rank correlation metrics of Kendall Tau <math alttext="0.88" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mn id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">0.88</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><cn id="S3.SS2.p3.1.m1.1.1.cmml" type="float" xref="S3.SS2.p3.1.m1.1.1">0.88</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">0.88</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">0.88</annotation></semantics></math>, Pearson <math alttext="0.97" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mn id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">0.97</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><cn id="S3.SS2.p3.2.m2.1.1.cmml" type="float" xref="S3.SS2.p3.2.m2.1.1">0.97</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">0.97</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">0.97</annotation></semantics></math> and Spearman <math alttext="0.97" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mn id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">0.97</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><cn id="S3.SS2.p3.3.m3.1.1.cmml" type="float" xref="S3.SS2.p3.3.m3.1.1">0.97</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">0.97</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">0.97</annotation></semantics></math>, proving the applicability of CubicML to optimize distributed LLM training performance.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.F3" title="Figure 3 ‣ A.3 Figures of Experiments for Large Language Models ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">3</span></a> (middle) shows correlation drop when generalizing from history data to future, which is expected because of the data distribution shift (caused by deprecation and update of models and systems in the development span); however, the rank correlation is still decent proving the value of reusing historical jobs in the wild to save compute resource when using CubicML.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.F3" title="Figure 3 ‣ A.3 Figures of Experiments for Large Language Models ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">3</span></a> (right) shows more challenges if we only profile small scale jobs and attempt to generalize to large scale jobs. However, the rank correlation is still better than random guess which should regress all correlation metrics to <math alttext="0.0" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><mn id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml">0.0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><cn id="S3.SS2.p3.4.m4.1.1.cmml" type="float" xref="S3.SS2.p3.4.m4.1.1">0.0</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">0.0</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">0.0</annotation></semantics></math>. We expect some real-time profiling jobs at large scale are required to correct the prediction when using CubicML in this use case.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.3">Last but not least, we evaluate how many examples (i.e. the number of jobs we need to profile) when building the predictor in CubicML in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#A1.F4" title="Figure 4 ‣ A.3 Figures of Experiments for Large Language Models ‣ Appendix A Appendix ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">4</span></a> (right). Random dataset split is used in this study. <math alttext="50" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mn id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><cn id="S3.SS2.p4.1.m1.1.1.cmml" type="integer" xref="S3.SS2.p4.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">50</annotation></semantics></math> examples already provides a decent prediction performance to start, and Pearson and Spearman reach <math alttext="\geq 0.9" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mrow id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml"></mi><mo id="S3.SS2.p4.2.m2.1.1.1" xref="S3.SS2.p4.2.m2.1.1.1.cmml">≥</mo><mn id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><geq id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1.1"></geq><csymbol cd="latexml" id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">absent</csymbol><cn id="S3.SS2.p4.2.m2.1.1.3.cmml" type="float" xref="S3.SS2.p4.2.m2.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\geq 0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">≥ 0.9</annotation></semantics></math> after <math alttext="150" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mn id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><cn id="S3.SS2.p4.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p4.3.m3.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">150</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">150</annotation></semantics></math> examples. This implies that a relatively small amount of profiling jobs is enough for CubicML to search. Combined with the fact that the GPU cost of each profiling job is low, CubicML can be an efficient automated solution to optimize distributed LLM training performance.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. [2020]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2001.08361</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peebles and Xie [2023]</span>
<span class="ltx_bibblock">
William Peebles and Saining Xie.

</span>
<span class="ltx_bibblock">Scalable diffusion models with transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pages 4195–4205, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024]</span>
<span class="ltx_bibblock">
Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao,
Shen Li, Yuchen Hao, Yantao Yao, et al.

</span>
<span class="ltx_bibblock">Wukong: Towards a scaling law for large-scale recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2403.02545</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anil et al. [2022]</span>
<span class="ltx_bibblock">
Rohan Anil, Sandra Gadanho, Da Huang, Nijith Jacob, Zhuoshu Li, Dong Lin, Todd
Phillips, Cristina Pop, Kevin Regan, Gil I Shamir, et al.

</span>
<span class="ltx_bibblock">On the factory floor: Ml engineering for industrial-scale ads
recommendation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2209.05310</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ardalani et al. [2022]</span>
<span class="ltx_bibblock">
Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, and Adnan
Aziz.

</span>
<span class="ltx_bibblock">Understanding scaling laws for recommendation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2208.08489</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et al. [2020]</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Zero: Memory optimizations toward training trillion parameter models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis</em>, pages 1–16. IEEE, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasley et al. [2020]</span>
<span class="ltx_bibblock">
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.

</span>
<span class="ltx_bibblock">Deepspeed: System optimizations enable training deep learning models
with over 100 billion parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery &amp; Data Mining</em>, pages 3505–3506, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023]</span>
<span class="ltx_bibblock">
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less
Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al.

</span>
<span class="ltx_bibblock">Pytorch fsdp: experiences on scaling fully sharded data parallel.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2304.11277</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoeybi et al. [2019]</span>
<span class="ltx_bibblock">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro.

</span>
<span class="ltx_bibblock">Megatron-lm: Training multi-billion parameter language models using
model parallelism.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:1909.08053</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayanan et al. [2021]</span>
<span class="ltx_bibblock">
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
Bernauer, Bryan Catanzaro, et al.

</span>
<span class="ltx_bibblock">Efficient large-scale language model training on gpu clusters using
megatron-lm.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis</em>, pages 1–15, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2017]</span>
<span class="ltx_bibblock">
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.

</span>
<span class="ltx_bibblock">Terngrad: Ternary gradients to reduce communication in distributed
deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in neural information processing systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2023]</span>
<span class="ltx_bibblock">
Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan
Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al.

</span>
<span class="ltx_bibblock">Fp8-lm: Training fp8 large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2310.18313</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2021]</span>
<span class="ltx_bibblock">
Xin He, Kaiyong Zhao, and Xiaowen Chu.

</span>
<span class="ltx_bibblock">Automl: A survey of the state-of-the-art.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Knowledge-based systems</em>, 212:106622, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2022]</span>
<span class="ltx_bibblock">
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al.

</span>
<span class="ltx_bibblock">Alpa: Automating inter-and <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib14.1.m1.1"><semantics id="bib.bib14.1.m1.1a"><mo id="bib.bib14.1.m1.1.1" stretchy="false" xref="bib.bib14.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.1.m1.1b"><ci id="bib.bib14.1.m1.1.1.cmml" xref="bib.bib14.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib14.1.m1.1d">{</annotation></semantics></math>Intra-Operator<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib14.2.m2.1"><semantics id="bib.bib14.2.m2.1a"><mo id="bib.bib14.2.m2.1.1" stretchy="false" xref="bib.bib14.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib14.2.m2.1b"><ci id="bib.bib14.2.m2.1.1.cmml" xref="bib.bib14.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib14.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib14.2.m2.1d">}</annotation></semantics></math> parallelism for
distributed deep learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">16th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 22)</em>, pages 559–578, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Shiwei Zhang, Lansong Diao, Siyu Wang, Zongyan Cao, Yiliang Gu, Chang Si, Ziji
Shi, Zhen Zheng, Chuan Wu, and Wei Lin.

</span>
<span class="ltx_bibblock">Auto-parallelizing large models with rhino: A systematic approach on
production ai platform.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2302.08141</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023]</span>
<span class="ltx_bibblock">
Hao Lin, Ke Wu, Jie Li, Jun Li, and Wu-Jun Li.

</span>
<span class="ltx_bibblock">Uniap: Unifying inter-and intra-layer automatic parallelism by mixed
integer quadratic programming.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2307.16375</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2018]</span>
<span class="ltx_bibblock">
Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen
Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib17.1.m1.1"><semantics id="bib.bib17.1.m1.1a"><mo id="bib.bib17.1.m1.1.1" stretchy="false" xref="bib.bib17.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib17.1.m1.1b"><ci id="bib.bib17.1.m1.1.1.cmml" xref="bib.bib17.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib17.1.m1.1d">{</annotation></semantics></math>TVM<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib17.2.m2.1"><semantics id="bib.bib17.2.m2.1a"><mo id="bib.bib17.2.m2.1.1" stretchy="false" xref="bib.bib17.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib17.2.m2.1b"><ci id="bib.bib17.2.m2.1.1.cmml" xref="bib.bib17.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib17.2.m2.1d">}</annotation></semantics></math>: An automated <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib17.3.m3.1"><semantics id="bib.bib17.3.m3.1a"><mo id="bib.bib17.3.m3.1.1" stretchy="false" xref="bib.bib17.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib17.3.m3.1b"><ci id="bib.bib17.3.m3.1.1.cmml" xref="bib.bib17.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib17.3.m3.1d">{</annotation></semantics></math>End-to-End<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib17.4.m4.1"><semantics id="bib.bib17.4.m4.1a"><mo id="bib.bib17.4.m4.1.1" stretchy="false" xref="bib.bib17.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib17.4.m4.1b"><ci id="bib.bib17.4.m4.1.1.cmml" xref="bib.bib17.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib17.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib17.4.m4.1d">}</annotation></semantics></math> optimizing compiler for
deep learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.5.1">13th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 18)</em>, pages 578–594, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al. [2024]</span>
<span class="ltx_bibblock">
Hang Yin, Kuang-Hung Liu, Mengying Sun, Yuxin Chen, Buyun Zhang, Jiang Liu,
Vivek Sehgal, Rudresh Rajnikant Panchal, Eugen Hotaj, Xi Liu, et al.

</span>
<span class="ltx_bibblock">Automl for large capacity modeling of meta’s ranking systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Companion Proceedings of the ACM on Web Conference 2024</em>,
pages 374–382, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. [2020]</span>
<span class="ltx_bibblock">
Wei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, and Pieter-Jan
Kindermans.

</span>
<span class="ltx_bibblock">Neural predictor for neural architecture search.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">European Conference on Computer Vision</em>, pages 660–676.
Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. [2024]</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Neural
Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al. [2019]</span>
<span class="ltx_bibblock">
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.

</span>
<span class="ltx_bibblock">On the convergence of adam and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:1904.09237</em>, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Training details of the predictor and RL agent in CubicML</h3>
<section class="ltx_subsubsection" id="A1.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.1 </span>The Predictor and RL Agent when Optimizing Ads Recommendation Models</h4>
<div class="ltx_para" id="A1.SS1.SSS1.p1">
<p class="ltx_p" id="A1.SS1.SSS1.p1.8">We train an ensemble of <math alttext="10" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.1.m1.1"><semantics id="A1.SS1.SSS1.p1.1.m1.1a"><mn id="A1.SS1.SSS1.p1.1.m1.1.1" xref="A1.SS1.SSS1.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.1.m1.1b"><cn id="A1.SS1.SSS1.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.1.m1.1d">10</annotation></semantics></math> neural networks with identical architecture as a predictor. To train a neural network, each feature (i.e. a configuration) is encoded as one-hot and the neural network has a single hidden layer with <math alttext="1600" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.2.m2.1"><semantics id="A1.SS1.SSS1.p1.2.m2.1a"><mn id="A1.SS1.SSS1.p1.2.m2.1.1" xref="A1.SS1.SSS1.p1.2.m2.1.1.cmml">1600</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.2.m2.1b"><cn id="A1.SS1.SSS1.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p1.2.m2.1.1">1600</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.2.m2.1c">1600</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.2.m2.1d">1600</annotation></semantics></math> dimensions with dropout rate <math alttext="0.5" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.3.m3.1"><semantics id="A1.SS1.SSS1.p1.3.m3.1a"><mn id="A1.SS1.SSS1.p1.3.m3.1.1" xref="A1.SS1.SSS1.p1.3.m3.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.3.m3.1b"><cn id="A1.SS1.SSS1.p1.3.m3.1.1.cmml" type="float" xref="A1.SS1.SSS1.p1.3.m3.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.3.m3.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.3.m3.1d">0.5</annotation></semantics></math>.
The predictor is trained by the AMSGrad <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib22" title="">22</a>]</cite> optimizer for <math alttext="200" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.4.m4.1"><semantics id="A1.SS1.SSS1.p1.4.m4.1a"><mn id="A1.SS1.SSS1.p1.4.m4.1.1" xref="A1.SS1.SSS1.p1.4.m4.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.4.m4.1b"><cn id="A1.SS1.SSS1.p1.4.m4.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p1.4.m4.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.4.m4.1c">200</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.4.m4.1d">200</annotation></semantics></math> epochs with batch size <math alttext="64" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.5.m5.1"><semantics id="A1.SS1.SSS1.p1.5.m5.1a"><mn id="A1.SS1.SSS1.p1.5.m5.1.1" xref="A1.SS1.SSS1.p1.5.m5.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.5.m5.1b"><cn id="A1.SS1.SSS1.p1.5.m5.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p1.5.m5.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.5.m5.1c">64</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.5.m5.1d">64</annotation></semantics></math>, learning rate <math alttext="0.001" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.6.m6.1"><semantics id="A1.SS1.SSS1.p1.6.m6.1a"><mn id="A1.SS1.SSS1.p1.6.m6.1.1" xref="A1.SS1.SSS1.p1.6.m6.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.6.m6.1b"><cn id="A1.SS1.SSS1.p1.6.m6.1.1.cmml" type="float" xref="A1.SS1.SSS1.p1.6.m6.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.6.m6.1c">0.001</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.6.m6.1d">0.001</annotation></semantics></math> and weight decay <math alttext="0.005" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.7.m7.1"><semantics id="A1.SS1.SSS1.p1.7.m7.1a"><mn id="A1.SS1.SSS1.p1.7.m7.1.1" xref="A1.SS1.SSS1.p1.7.m7.1.1.cmml">0.005</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.7.m7.1b"><cn id="A1.SS1.SSS1.p1.7.m7.1.1.cmml" type="float" xref="A1.SS1.SSS1.p1.7.m7.1.1">0.005</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.7.m7.1c">0.005</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.7.m7.1d">0.005</annotation></semantics></math>.
The margin is set to <math alttext="0.001" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p1.8.m8.1"><semantics id="A1.SS1.SSS1.p1.8.m8.1a"><mn id="A1.SS1.SSS1.p1.8.m8.1.1" xref="A1.SS1.SSS1.p1.8.m8.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p1.8.m8.1b"><cn id="A1.SS1.SSS1.p1.8.m8.1.1.cmml" type="float" xref="A1.SS1.SSS1.p1.8.m8.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p1.8.m8.1c">0.001</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p1.8.m8.1d">0.001</annotation></semantics></math> in Margin Ranking Loss.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS1.p2">
<p class="ltx_p" id="A1.SS1.SSS1.p2.3">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S3.F2" title="Figure 2 ‣ 3.1 ZeRO Sharding Optimization for Distributed Training of Recommendation Models ‣ 3 Experiment ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">2</span></a> (a), CubicML first randomly sample around <math alttext="480" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p2.1.m1.1"><semantics id="A1.SS1.SSS1.p2.1.m1.1a"><mn id="A1.SS1.SSS1.p2.1.m1.1.1" xref="A1.SS1.SSS1.p2.1.m1.1.1.cmml">480</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p2.1.m1.1b"><cn id="A1.SS1.SSS1.p2.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p2.1.m1.1.1">480</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p2.1.m1.1c">480</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p2.1.m1.1d">480</annotation></semantics></math> jobs to test the limit of random search method, and then iterates three rounds of the predictor-based RL search as explained in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#S2.F1" title="Figure 1 ‣ 2 The CubicML Framework ‣ CubicML: Automated ML for Large ML Systems Co-design with ML Prediction of Performance"><span class="ltx_text ltx_ref_tag">1</span></a>; that is, random search builds the initial historical job data which is later used to train the predictor to predict QPS of any configuration; then the RL searcher samples <math alttext="2000" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p2.2.m2.1"><semantics id="A1.SS1.SSS1.p2.2.m2.1a"><mn id="A1.SS1.SSS1.p2.2.m2.1.1" xref="A1.SS1.SSS1.p2.2.m2.1.1.cmml">2000</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p2.2.m2.1b"><cn id="A1.SS1.SSS1.p2.2.m2.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p2.2.m2.1.1">2000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p2.2.m2.1c">2000</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p2.2.m2.1d">2000</annotation></semantics></math> configurations to maximize the reward (i.e. predicted QPS by the predictor). The configurations with top <math alttext="50" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p2.3.m3.1"><semantics id="A1.SS1.SSS1.p2.3.m3.1a"><mn id="A1.SS1.SSS1.p2.3.m3.1.1" xref="A1.SS1.SSS1.p2.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p2.3.m3.1b"><cn id="A1.SS1.SSS1.p2.3.m3.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p2.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p2.3.m3.1c">50</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p2.3.m3.1d">50</annotation></semantics></math> rewards are launched as new jobs which are added to historical job data upon completion to retrain the predictor for a next round of sampling.</p>
</div>
<div class="ltx_para" id="A1.SS1.SSS1.p3">
<p class="ltx_p" id="A1.SS1.SSS1.p3.2">During a RL search, its agent parameters are optimized by Adam optimizer with learning rate <math alttext="0.01" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p3.1.m1.1"><semantics id="A1.SS1.SSS1.p3.1.m1.1a"><mn id="A1.SS1.SSS1.p3.1.m1.1.1" xref="A1.SS1.SSS1.p3.1.m1.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p3.1.m1.1b"><cn id="A1.SS1.SSS1.p3.1.m1.1.1.cmml" type="float" xref="A1.SS1.SSS1.p3.1.m1.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p3.1.m1.1c">0.01</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p3.1.m1.1d">0.01</annotation></semantics></math> and batch size <math alttext="30" class="ltx_Math" display="inline" id="A1.SS1.SSS1.p3.2.m2.1"><semantics id="A1.SS1.SSS1.p3.2.m2.1a"><mn id="A1.SS1.SSS1.p3.2.m2.1.1" xref="A1.SS1.SSS1.p3.2.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS1.p3.2.m2.1b"><cn id="A1.SS1.SSS1.p3.2.m2.1.1.cmml" type="integer" xref="A1.SS1.SSS1.p3.2.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS1.p3.2.m2.1c">30</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS1.p3.2.m2.1d">30</annotation></semantics></math>.
In our experiment, we found that a single RL search returned similar configurations around a local minimum, to resolve which, we rerun three RL search trials with different randomization seeds and select top configurations from all trials.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.1.2 </span>The Predictor when Predicting Large Language Models Training Speed</h4>
<div class="ltx_para" id="A1.SS1.SSS2.p1">
<p class="ltx_p" id="A1.SS1.SSS2.p1.1">To evaluate the ability to predict LLM training performance, we simply use gradient boosting regressor as the predictor.
Integer and floating-point configurations are encoded as numerical features, Boolean and object (e.g. string) configurations are encoded as categorical features, ending up with feature vector length of <math alttext="117" class="ltx_Math" display="inline" id="A1.SS1.SSS2.p1.1.m1.1"><semantics id="A1.SS1.SSS2.p1.1.m1.1a"><mn id="A1.SS1.SSS2.p1.1.m1.1.1" xref="A1.SS1.SSS2.p1.1.m1.1.1.cmml">117</mn><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS2.p1.1.m1.1b"><cn id="A1.SS1.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS1.SSS2.p1.1.m1.1.1">117</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS2.p1.1.m1.1c">117</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.SSS2.p1.1.m1.1d">117</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Search Spaces in CubicML Experiments</h3>
<section class="ltx_subsubsection" id="A1.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.1 </span>Ads Recommendation Models</h4>
<div class="ltx_para" id="A1.SS2.SSS1.p1">
<p class="ltx_p" id="A1.SS2.SSS1.p1.1">To optimize training speed measured by example/query per second (QPS), we design a search space with <math alttext="5\times 3^{11}\times 10\approx 8.9\times 10^{6}" class="ltx_Math" display="inline" id="A1.SS2.SSS1.p1.1.m1.1"><semantics id="A1.SS2.SSS1.p1.1.m1.1a"><mrow id="A1.SS2.SSS1.p1.1.m1.1.1" xref="A1.SS2.SSS1.p1.1.m1.1.1.cmml"><mrow id="A1.SS2.SSS1.p1.1.m1.1.1.2" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mn id="A1.SS2.SSS1.p1.1.m1.1.1.2.2" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">5</mn><mo id="A1.SS2.SSS1.p1.1.m1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.1.cmml">×</mo><msup id="A1.SS2.SSS1.p1.1.m1.1.1.2.3" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3.cmml"><mn id="A1.SS2.SSS1.p1.1.m1.1.1.2.3.2" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3.2.cmml">3</mn><mn id="A1.SS2.SSS1.p1.1.m1.1.1.2.3.3" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3.3.cmml">11</mn></msup><mo id="A1.SS2.SSS1.p1.1.m1.1.1.2.1a" lspace="0.222em" rspace="0.222em" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.1.cmml">×</mo><mn id="A1.SS2.SSS1.p1.1.m1.1.1.2.4" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.4.cmml">10</mn></mrow><mo id="A1.SS2.SSS1.p1.1.m1.1.1.1" xref="A1.SS2.SSS1.p1.1.m1.1.1.1.cmml">≈</mo><mrow id="A1.SS2.SSS1.p1.1.m1.1.1.3" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mn id="A1.SS2.SSS1.p1.1.m1.1.1.3.2" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">8.9</mn><mo id="A1.SS2.SSS1.p1.1.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">×</mo><msup id="A1.SS2.SSS1.p1.1.m1.1.1.3.3" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mn id="A1.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">10</mn><mn id="A1.SS2.SSS1.p1.1.m1.1.1.3.3.3" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml">6</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS1.p1.1.m1.1b"><apply id="A1.SS2.SSS1.p1.1.m1.1.1.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1"><approx id="A1.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.1"></approx><apply id="A1.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.2"><times id="A1.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.1"></times><cn id="A1.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.2">5</cn><apply id="A1.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3"><csymbol cd="ambiguous" id="A1.SS2.SSS1.p1.1.m1.1.1.2.3.1.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3">superscript</csymbol><cn id="A1.SS2.SSS1.p1.1.m1.1.1.2.3.2.cmml" type="integer" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3.2">3</cn><cn id="A1.SS2.SSS1.p1.1.m1.1.1.2.3.3.cmml" type="integer" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.3.3">11</cn></apply><cn id="A1.SS2.SSS1.p1.1.m1.1.1.2.4.cmml" type="integer" xref="A1.SS2.SSS1.p1.1.m1.1.1.2.4">10</cn></apply><apply id="A1.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.3"><times id="A1.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.1"></times><cn id="A1.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" type="float" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.2">8.9</cn><apply id="A1.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="A1.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3">superscript</csymbol><cn id="A1.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" type="integer" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3.2">10</cn><cn id="A1.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml" type="integer" xref="A1.SS2.SSS1.p1.1.m1.1.1.3.3.3">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS1.p1.1.m1.1c">5\times 3^{11}\times 10\approx 8.9\times 10^{6}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS1.p1.1.m1.1d">5 × 3 start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT × 10 ≈ 8.9 × 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT</annotation></semantics></math> configurations, generated from</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">FSDP sharding strategy <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy</span></span></span> per layer: <span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.1">FULL_SHARD</span> (ZeRO stage 3), <span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.2">SHARD_GRAD_OP</span> (ZeRO stage 2) and <span class="ltx_text ltx_font_typewriter" id="A1.I1.i1.p1.1.3">NO_SHARD</span> (standard distributed data parallelism)</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.3">Local batch size per GPU: minimum is <math alttext="1024" class="ltx_Math" display="inline" id="A1.I1.i2.p1.1.m1.1"><semantics id="A1.I1.i2.p1.1.m1.1a"><mn id="A1.I1.i2.p1.1.m1.1.1" xref="A1.I1.i2.p1.1.m1.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.1.m1.1b"><cn id="A1.I1.i2.p1.1.m1.1.1.cmml" type="integer" xref="A1.I1.i2.p1.1.m1.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.1.m1.1c">1024</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.1.m1.1d">1024</annotation></semantics></math> and maximum is <math alttext="1536" class="ltx_Math" display="inline" id="A1.I1.i2.p1.2.m2.1"><semantics id="A1.I1.i2.p1.2.m2.1a"><mn id="A1.I1.i2.p1.2.m2.1.1" xref="A1.I1.i2.p1.2.m2.1.1.cmml">1536</mn><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.2.m2.1b"><cn id="A1.I1.i2.p1.2.m2.1.1.cmml" type="integer" xref="A1.I1.i2.p1.2.m2.1.1">1536</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.2.m2.1c">1536</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.2.m2.1d">1536</annotation></semantics></math> with step size <math alttext="128" class="ltx_Math" display="inline" id="A1.I1.i2.p1.3.m3.1"><semantics id="A1.I1.i2.p1.3.m3.1a"><mn id="A1.I1.i2.p1.3.m3.1.1" xref="A1.I1.i2.p1.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A1.I1.i2.p1.3.m3.1b"><cn id="A1.I1.i2.p1.3.m3.1.1.cmml" type="integer" xref="A1.I1.i2.p1.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.I1.i2.p1.3.m3.1c">128</annotation><annotation encoding="application/x-llamapun" id="A1.I1.i2.p1.3.m3.1d">128</annotation></semantics></math>. These batch sizes were verified with minor impact on model accuracy, so we can focus on QPS optimization by short-term profiling jobs without worrying about long-term accuracy.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Storage reservation policy:</p>
<ul class="ltx_itemize" id="A1.I1.i3.I1">
<li class="ltx_item" id="A1.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="A1.I1.i3.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i3.I1.i1.p1.1">FixedPercentage: A fixed percentage of HBM reserved for dense paramemers and runtime memory, ranging from 0.77 to 0.85, each step is 0.01.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="A1.I1.i3.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i3.I1.i2.p1.1">MemoryBalanced: The embedding sharding planner will find the minimal HBM used for emebdding tables, leave rest for dense parameters and runtime memory.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="A1.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">A.2.2 </span>Large Language Models</h4>
<div class="ltx_para" id="A1.SS2.SSS2.p1">
<p class="ltx_p" id="A1.SS2.SSS2.p1.7">We use training speed WPS (words/tokens per second) as the performance metric that CubicML targets to model.
The input features were concatenation of configurations (which influences WPS) below:</p>
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1">Transformer model architecture configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib21" title="">21</a>]</cite>, such as the number of layers, the numbers of heads, model dimension, feed forward network dimensions, batch size, sequence length, etc</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1">Distributed training co-design configurations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.04585v2#bib.bib20" title="">20</a>]</cite>, such as which parallelism strategies (tensor parallelism, pipeline parallelism, context parallelism, data parallelism) should be selected and their configurations (e.g. group sizes), precision formats (FP8 and BF16), etc</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i3.p1">
<p class="ltx_p" id="A1.I2.i3.p1.1">Distributed system infrastructure info: the number of GPUs and hardware types</p>
</div>
</li>
</ul>
<p class="ltx_p" id="A1.SS2.SSS2.p1.6">Each configuration has a wide range of values.
For example, the number of GPUs can be as small as <math alttext="8" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.1.m1.1"><semantics id="A1.SS2.SSS2.p1.1.m1.1a"><mn id="A1.SS2.SSS2.p1.1.m1.1.1" xref="A1.SS2.SSS2.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS2.p1.1.m1.1b"><cn id="A1.SS2.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="A1.SS2.SSS2.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS2.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS2.p1.1.m1.1d">8</annotation></semantics></math> and up to <math alttext="16,384" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.2.m2.2"><semantics id="A1.SS2.SSS2.p1.2.m2.2a"><mrow id="A1.SS2.SSS2.p1.2.m2.2.3.2" xref="A1.SS2.SSS2.p1.2.m2.2.3.1.cmml"><mn id="A1.SS2.SSS2.p1.2.m2.1.1" xref="A1.SS2.SSS2.p1.2.m2.1.1.cmml">16</mn><mo id="A1.SS2.SSS2.p1.2.m2.2.3.2.1" xref="A1.SS2.SSS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="A1.SS2.SSS2.p1.2.m2.2.2" xref="A1.SS2.SSS2.p1.2.m2.2.2.cmml">384</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS2.p1.2.m2.2b"><list id="A1.SS2.SSS2.p1.2.m2.2.3.1.cmml" xref="A1.SS2.SSS2.p1.2.m2.2.3.2"><cn id="A1.SS2.SSS2.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS2.SSS2.p1.2.m2.1.1">16</cn><cn id="A1.SS2.SSS2.p1.2.m2.2.2.cmml" type="integer" xref="A1.SS2.SSS2.p1.2.m2.2.2">384</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS2.p1.2.m2.2c">16,384</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS2.p1.2.m2.2d">16 , 384</annotation></semantics></math>; the sequence length ranges between <math alttext="2048" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.3.m3.1"><semantics id="A1.SS2.SSS2.p1.3.m3.1a"><mn id="A1.SS2.SSS2.p1.3.m3.1.1" xref="A1.SS2.SSS2.p1.3.m3.1.1.cmml">2048</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS2.p1.3.m3.1b"><cn id="A1.SS2.SSS2.p1.3.m3.1.1.cmml" type="integer" xref="A1.SS2.SSS2.p1.3.m3.1.1">2048</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS2.p1.3.m3.1c">2048</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS2.p1.3.m3.1d">2048</annotation></semantics></math> and <math alttext="131,072" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.4.m4.2"><semantics id="A1.SS2.SSS2.p1.4.m4.2a"><mrow id="A1.SS2.SSS2.p1.4.m4.2.3.2" xref="A1.SS2.SSS2.p1.4.m4.2.3.1.cmml"><mn id="A1.SS2.SSS2.p1.4.m4.1.1" xref="A1.SS2.SSS2.p1.4.m4.1.1.cmml">131</mn><mo id="A1.SS2.SSS2.p1.4.m4.2.3.2.1" xref="A1.SS2.SSS2.p1.4.m4.2.3.1.cmml">,</mo><mn id="A1.SS2.SSS2.p1.4.m4.2.2" xref="A1.SS2.SSS2.p1.4.m4.2.2.cmml">072</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS2.p1.4.m4.2b"><list id="A1.SS2.SSS2.p1.4.m4.2.3.1.cmml" xref="A1.SS2.SSS2.p1.4.m4.2.3.2"><cn id="A1.SS2.SSS2.p1.4.m4.1.1.cmml" type="integer" xref="A1.SS2.SSS2.p1.4.m4.1.1">131</cn><cn id="A1.SS2.SSS2.p1.4.m4.2.2.cmml" type="integer" xref="A1.SS2.SSS2.p1.4.m4.2.2">072</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS2.p1.4.m4.2c">131,072</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS2.p1.4.m4.2d">131 , 072</annotation></semantics></math>; the number of parameters ranges from <math alttext="27" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.5.m5.1"><semantics id="A1.SS2.SSS2.p1.5.m5.1a"><mn id="A1.SS2.SSS2.p1.5.m5.1.1" xref="A1.SS2.SSS2.p1.5.m5.1.1.cmml">27</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS2.p1.5.m5.1b"><cn id="A1.SS2.SSS2.p1.5.m5.1.1.cmml" type="integer" xref="A1.SS2.SSS2.p1.5.m5.1.1">27</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS2.p1.5.m5.1c">27</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS2.p1.5.m5.1d">27</annotation></semantics></math> million up to <math alttext="405" class="ltx_Math" display="inline" id="A1.SS2.SSS2.p1.6.m6.1"><semantics id="A1.SS2.SSS2.p1.6.m6.1a"><mn id="A1.SS2.SSS2.p1.6.m6.1.1" xref="A1.SS2.SSS2.p1.6.m6.1.1.cmml">405</mn><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS2.p1.6.m6.1b"><cn id="A1.SS2.SSS2.p1.6.m6.1.1.cmml" type="integer" xref="A1.SS2.SSS2.p1.6.m6.1.1">405</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS2.p1.6.m6.1c">405</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS2.p1.6.m6.1d">405</annotation></semantics></math> billion.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Figures of Experiments for Large Language Models</h3>
<figure class="ltx_figure" id="A1.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="223" id="A1.F3.g1" src="extracted/5869690/figures/llm_prediction.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Predicted WPS versus actual WPS by the predictor in CubicML. Left: random split of dataset; middle: use examples in older jobs to predict newer jobs; right: use examples with smaller numbers of GPUs to predict larger scale with more GPUs. Note that all WPS values are normalized/divided by a constant in a plot.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="A1.F4.g1" src="extracted/5869690/figures/gpu_timestamp.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Left: the number of GPUs used in jobs sorted by timestamps. Note that timestamps are normalized/divided by a constant. Right: rank correlation versus the number of training examples by random split of dataset. Shading bands are <math alttext="\pm 2.0\times" class="ltx_math_unparsed" display="inline" id="A1.F4.2.m1.1"><semantics id="A1.F4.2.m1.1b"><mrow id="A1.F4.2.m1.1c"><mo id="A1.F4.2.m1.1.1">±</mo><mn id="A1.F4.2.m1.1.2">2.0</mn><mo id="A1.F4.2.m1.1.3" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="A1.F4.2.m1.1d">\pm 2.0\times</annotation><annotation encoding="application/x-llamapun" id="A1.F4.2.m1.1e">± 2.0 ×</annotation></semantics></math> standard deviation over ten random perturbations.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 21 05:52:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
